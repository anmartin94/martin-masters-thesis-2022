<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">University of Oslo</orgName>
								<orgName type="institution" key="instit2">Potsdam University</orgName>
								<orgName type="institution" key="instit3">Linköping University</orgName>
								<orgName type="institution" key="instit4">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Nuance Communications Aachen GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Nuance Communications Aachen GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Nuance Communications Aachen GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angelina</forename><surname>Ivanova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">University of Oslo</orgName>
								<orgName type="institution" key="instit2">Potsdam University</orgName>
								<orgName type="institution" key="instit3">Linköping University</orgName>
								<orgName type="institution" key="instit4">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task 8 at SemEval 2014 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background and Motivation</head><p>Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; <ref type="bibr" target="#b2">Buchholz &amp; Marsi, 2006;</ref><ref type="bibr" target="#b20">Nivre et al., 2007;</ref><ref type="bibr">inter alios)</ref>. From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path.</p><p>This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and the proceedings footer are added by the organizers: http:// creativecommons.org/licenses/by/4.0/.</p><p>Unfortunately, tree-oriented parsers are ill-suited for producing meaning representations, i.e. moving from the analysis of grammatical structure to sentence semantics. Even if syntactic parsing arguably can be limited to tree structures, this is not the case in semantic analysis, where a node will often be the argument of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs).</p><p>Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014), 1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; <ref type="bibr" target="#b15">Marcus et al., 1993)</ref>. These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure <ref type="figure" target="#fig_1">1</ref> below for the WSJ sentence:</p><p>(1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice.</p><p>Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vac-   uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees.</p><p>In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; <ref type="bibr" target="#b8">Gildea &amp; Jurafsky, 2002)</ref>. In much previous work, however, target representations typically draw on resources like PropBank and NomBank <ref type="bibr" target="#b22">(Palmer et al., 2005;</ref><ref type="bibr" target="#b17">Meyers et al., 2004)</ref>, which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomenafor example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction-typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task defini-tion does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words.</p><p>Finally, a third closely related area of much current interest is often dubbed 'semantic parsing', which <ref type="bibr" target="#b14">Kate and Wong (2010)</ref> define as "the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application." In contrast to most work in this tradition, our SDP target representations aim to be task-and domainindependent, though at least part of this generality comes at the expense of 'completeness' in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Target Representations</head><p>We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure <ref type="figure" target="#fig_1">1</ref>), showing what are called the DM, PAS, and PCEDT semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. In the following paragraphs, we provide some background on the 'pedigree' and linguistic characterization of these representations. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation of Sections 00-21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; <ref type="bibr" target="#b6">Flickinger, 2000)</ref>. Among other layers of linguistic annotation, this resourcedubbed DeepBank by <ref type="bibr" target="#b7">Flickinger et al. (2012)</ref>includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; <ref type="bibr" target="#b3">Copestake et al., 2005)</ref>. Our DM target representations are derived through a two-step 'lossy' conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; <ref type="bibr" target="#b21">Oepen &amp; Lønning, 2006)</ref>, then to 'pure' bi-lexical form-projecting some construction semantics onto word-to-word dependencies <ref type="bibr" target="#b13">(Ivanova et al., 2012)</ref>. In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of <ref type="bibr" target="#b18">Miyao et al. (2014)</ref>. For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure <ref type="figure" target="#fig_1">1</ref>. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAS: Enju Predicate-Argument Structures</head><p>The Enju parsing system is an HPSG-based parser for English. <ref type="bibr">3</ref> The grammar and the disambiguation model of this parser are derived from the Enju HPSG treebank, which is automatically converted from the phrase structure and predicate-argument structure annotations of the PTB. The PAS data set is extracted from the WSJ portion of the Enju HPSG treebank. While the Enju treebank is annotated with full HPSG-style structures, only its predicate-argument structures are converted into the SDP data format for use in this task. Top nodes in this representation denote semantic heads. Again, the system description of <ref type="bibr" target="#b18">Miyao et al. (2014)</ref> provides more technical detail on the conversion.  texts from the PTB, and their Czech translations.</p><p>Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees).</p><p>PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCEDT representations are best observed in the procedure that converts the original PCEDT data to the SDP data format; see <ref type="bibr" target="#b18">Miyao et al. (2014)</ref>. Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Representation</head><p>The SDP target representations can be characterized as labeled, directed graphs. Formally, a semantic dependency graph for a sentence</p><formula xml:id="formula_0">x = x 1 , . . . , x n is a structure G = (V, E, V , E ) where V = {1, .</formula><p>. . , n} is a set of nodes (which are in one-to-one correspondence with the tokens of the sentence); E ⊆ V × V is a set of edges; and V and E are mappings that assign labels (from some finite alphabet) to nodes and edges, respectively. More specifically for this task, the label V (i) of a node i is a tuple consisting of four components: its word form, lemma, part of speech, and a Boolean flag indicating whether the corresponding token represents a top predicate for the specific sentence.</p><p>The label E (i → j) of an edge i → j is a semantic relation that holds between i and j. The exact definition of what constitutes a top node and what semantic relations are available differs among our three target representations, but note that top nodes can have incoming edges. All data provided for the task uses a columnbased file format (dubbed the SDP data format) similar to the one of the 2009 CoNLL Shared Task <ref type="bibr" target="#b10">(Hajič et al., 2009)</ref>. As in that task, we assume goldstandard sentence and token segmentation. For ease of reference, each sentence is prefixed by a line with just a unique identifier, using the scheme 2SSDDIII, with a constant leading 2, two-digit section code, two-digit document code (within each section), and three-digit item number (within each document). For example, identifier 20200002 denotes the second sentence in the first file of PTB Section 02, the classic Ms. Haag plays Elianti. The annotation of this sentence is shown in Table <ref type="table" target="#tab_2">1</ref>.</p><p>With one exception, our fields (i.e. columns in the tab-separated matrix) are a subset of the CoNLL 2009 inventory: (1) id, (2) form, (3) lemma, and (4) pos characterize the current token, with token identifiers starting from 1 within each sentence. Besides the lemma and part-of-speech information, in the closed track of our task, there is no explicit analysis of syntax. Across the three target representations in the task, fields ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) are aligned and uniform, i.e. all representations annotate exactly the same text. On the other hand, fields ( <ref type="formula">3</ref>) and ( <ref type="formula">4</ref>) are representation-specific, i.e. there are different conventions for lemmatization, and part-of-speech assignments can vary (but all representations use the same PTB inventory of PoS tags).</p><p>The bi-lexical semantic dependency graph over tokens is represented by two or more columns starting with the obligatory, binary-valued fields (5) top and (6) pred. A positive value in the top column indicates that the node corresponding to this token is a top node (see Section 2 below). The pred column is a simplification of the corresponding field in earlier tasks, indicating whether or not this token represents a predicate, i.e. a node with outgoing dependency edges. With these minor differences to the CoNLL tradition, our file format can represent general, directed graphs, with designated top nodes. For example, there can be singleton nodes not connected to other parts of the graph, and in principle there can be multiple tops, or a non-predicate top node.</p><p>To designate predicate-argument relations, there are as many additional columns as there are predicates in the graph (i.e. tokens marked + in the pred column); these additional columns are called (7) arg1, (8) arg2, etc. These colums contain argument roles relative to the i-th predicate, i.e. a non-empty value in column arg1 indicates that the current token is an argument of the (linearly) first predicate in the sentence. In this format, graph reentrancies will lead to a token receiving argument roles for multiple predicates (i.e. non-empty arg i values in the same row). All tokens of the same sentence must always have all argument columns filled in, even on non-predicate words; in other words, all lines making up one block of tokens will have the same number n of fields, but n can differ across   <ref type="bibr" target="#b5">(Fares et al., 2013)</ref>. Finally, 232 of the graphs obtained through the above conversions were cyclic. In total, we were left with 34,004 sentences (or 745,543 tokens) as training data (Sections 00-20), and 1348 testing sentences (29,808 tokens), from Section 21.</p><p>Quantitative Comparison As a first attempt at contrasting our three target representations, Table <ref type="table" target="#tab_4">2</ref> shows some high-level statistics of the graphs comprising the training data. <ref type="bibr">5</ref> In terms of distinctions   <ref type="figure" target="#fig_1">1</ref>) include most punctuation marks in PCEDT and DM, but not PAS. Furthermore, PCEDT (unlike the other two) analyzes some high-frequency determiners as semantically vacuous. Conversely, PAS on average has more edges per (non-singleton) nodes than the other two (3), which likely reflects its approach to the analysis of functional words (see below). Judging from both the percentage of actual trees (4), the proportions of projective graphs (5), and the proportions of reentrant nodes (7), PCEDT is much more 'tree-oriented' than the other two, which at least in part reflects its approach to the analysis of modifiers and determiners (again, see below). We view the small percentages of graphs without at least one top node (8) and of graphs with at least two non-singleton components that are not interconnected (6) as tentative indicators of general well-formedness. Intuitively, there should always be a 'top' predicate, and the whole graph should 'hang together'. Only DM exhibits non-trivial (if small) degrees of topless and fragmented graphs, and these may indicate imperfections in the Deep-Bank annotations or room for improvement in the conversion from full MRSs to bi-lexical dependencies, but possibly also exceptions to our intuitions about semantic dependency graphs.</p><p>Finally, in Table <ref type="table" target="#tab_6">3</ref> we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F 1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PCEDT, even more such that i &lt; j &lt; k. so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PCEDT. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PCEDT on the other hand.</p><p>Linguistic Comparison Among other aspects, <ref type="bibr" target="#b13">Ivanova et al. (2012)</ref> categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure <ref type="figure" target="#fig_1">1</ref> and the discussion of Table <ref type="table" target="#tab_4">2</ref> above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks; 6 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PCEDT, they function as predicates in PAS, though do not always serve as 'local' top nodes (i.e. the semantic head of the corresponding sub-graph): For example, the infinitival marker in Figure <ref type="figure" target="#fig_1">1</ref> takes the verb as its argument, but the 'upstairs' predicate impossible links directly to the verb, rather than to the infinitival marker as an intermediate.</p><p>At the same time, DM and PAS pattern alike in their approach to modifiers, e.g. attributive adjectives, adverbs, and prepositional phrases. Unlike in PCEDT (or common syntactic dependency schemes), these are analyzed as semantic predicates and, thus, contribute to higher degrees of node reentrancy and non-top (structural) roots. Roughly the same holds for determiners, but here our PCEDT projection of Prague tectogrammatical trees onto bi-lexical dependencies leaves 'vanilla' articles (like a and the) as singleton nodes.</p><p>The analysis of coordination is distinct in the three representations, as also evident in Figure <ref type="figure" target="#fig_1">1</ref>. By design, DM opts for what is often called the Mel'čukian analysis of coordinate structures <ref type="bibr" target="#b16">(Mel'čuk, 1988)</ref>, with a chain of dependencies rooted at the first conjunct (which is thus considered the head, 'standing in' for the structure at large); in the DM approach, coordinating conjunctions are not integrated with the graph but rather contribute different types of dependencies. In PAS, the final coordinating conjunction is the head of the structure and each coordinating conjunction (or intervening punctuation mark that acts like one) is a two-place predicate, taking left and right conjuncts as its arguments. Conversely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph. <ref type="bibr">7</ref> A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure <ref type="figure" target="#fig_2">2</ref>. Much noun phrase-internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of <ref type="bibr" target="#b27">Vadas and Curran (2007)</ref>. In the four-way nominal compounding example of Figure <ref type="figure" target="#fig_2">2</ref>, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns an underspecified, construction-specific dependency type; its compound dependency, then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a preposition, as in the above paraphrase). The DM and PCEDT annotations of this specific example happen to diverge in their bracketing decisions, where the DM analysis corresponds to <ref type="bibr">[...]</ref> investments in stock for employees, i.e. grouping the concept employee stock (in contrast to 'common stock').</p><p>Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see <ref type="bibr" target="#b19">Nakov, 2013</ref>, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between 'sentence' (or 'conventional') meaning, on the one hand, and 'speaker' (or 'occasion') meaning, on the other hand <ref type="bibr" target="#b24">(Quine, 1960;</ref><ref type="bibr" target="#b9">Grice, 1968)</ref>. In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task Setup</head><p>Training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit-including graph input, basic statistics, and scoring-were released to candidate participants in early December 2013. In mid-January, a minor update to the training data and optional syntactic 'companion' analyses (see below) were provided, and in early February the description and evaluation of a simple baseline system (using tree approximations and the parser of <ref type="bibr" target="#b0">Bohnet, 2010)</ref>. Towards the end of March, an input-only version of the test data was released, with just columns (1) to (4) pre-filled; participants then had one week to run their systems on these inputs, fill in columns (5), (6), and upwards, and submit their results (from up to two different runs) for scoring. Upon completion of the testing phase, we have shared the gold-standard test data, official scores, and system results for all submissions with participants and are currently preparing all data for general release through the Linguistic Data Consortium.   <ref type="bibr">(top)</ref> and open tracks (bottom). For each system, the second column (LF) indicates the averaged LF score across all target representations), which was used to rank the systems.</p><p>Evaluation Systems participating in the task were evaluated based on the accuracy with which they can produce semantic dependency graphs for previously unseen text, measured relative to the gold-standard testing data. The key measures for this evaluation were labeled and unlabeled precision and recall with respect to predicted dependencies (predicate-role-argument triples) and labeled and unlabeled exact match with respect to complete graphs. In both contexts, identification of the top node(s) of a graph was considered as the identification of additional, 'virtual' dependencies from an artificial root node (at position 0). Below we abbreviate these metrics as (a) labeled precision, recall, and F 1 : LP, LR, LF; (b) unlabeled precision, recall, and F 1 : UP, UR, UF; and (c) labeled and unlabeled exact match: LM, UM.</p><p>The 'official' ranking of participating systems, in both the closed and the open tracks, is determined based on the arithmetic mean of the labeled dependency F 1 scores (i.e. the geometric mean of labeled precision and labeled recall) on the three target representations (DM, PAS, and PCEDT). Thus, to be considered for the final ranking, a system had to submit semantic dependencies for all three target representations.</p><p>Closed vs. Open Tracks The task was subdivided into a closed track and an open track, where systems in the closed track could only be trained on the gold-standard semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example-provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21.</p><p>This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use 'companion' syntactic analyses, sentence-and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of <ref type="bibr" target="#b23">Petrov et al. (2006)</ref> and Stanford Basic syntactic dependencies (de <ref type="bibr" target="#b4">Marneffe et al., 2006)</ref> produced by the parser of <ref type="bibr" target="#b1">Bohnet and Nivre (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Submissions and Results</head><p>From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table <ref type="table" target="#tab_8">4</ref>. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the table.  The scores for labeled exact match show a much larger variation across both target representations and systems. <ref type="bibr">8</ref> In the open track, we see very similar trends. The average LF scores across target representations range from 86.27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Overview of Approaches</head><p>Table <ref type="table" target="#tab_10">5</ref> shows a summary of the systems that submitted final results. Most of the systems took a strategy to use some algorithm to process (restricted types of) graph structures, and apply machine learning like structured perceptrons. The methods for processing graph structures are classified into three types. One is to transform graphs into trees in the preprocessing stage, and apply conventional dependency parsing systems (e.g. Mate; <ref type="bibr" target="#b0">Bohnet, 2010)</ref> to the converted trees. Some systems simply output the result of dependency parsing (which means they inherently lose some depen-8 Please see the task web page at the address indicated above for full labeled and unlabeled scores. dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of <ref type="bibr" target="#b25">Sagae &amp; Tsujii, 2008;</ref><ref type="bibr" target="#b26">Titov et al., 2009)</ref>. In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning-oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method.</p><p>It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency parsing was successful; for example, Peking built an ensemble of existing transition-based and graph-based dependency parsers, and Priberam extended an existing dependency parser. As we indicated in the task description, a novel feature of this task is that we have to compute graph structures, and cannot assume well-known properties like projectivity and lack of reentrancies. However, many of the participants found that our representations are mostly tree-like, and this fact motivated them to apply methods that have been well studied in the field of syntactic dependency parsing.</p><p>Finally, we observe that three teams participated in both the closed and open tracks, and all of them reported that adding external resources improved accuracy by a little more than one point. Systems with (only) open submissions extensively use syntactic features (e.g. dependency paths) from external resources, and they are shown effective even with simple machine learning models. Pre-existing, tree-oriented dependency parsers are relatively effective, especially when combined with graph-totree transformation. Comparing across our three target representations, system scores show a tendency PAS &gt; DM &gt; PCEDT, which can be taken as a tentative indicator of relative levels of 'parsability'. As suggested in Section 4, this variation most likely correlates at least in part with diverging design decisions, e.g. the inclusion of relatively local and deterministic dependencies involving function words in PAS, or the decision to annotate contextually determined speaker meaning (rather than 'mere' sentence meaning) in at least some constructions in PCEDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Outlook</head><p>We have described the motivation, design, and outcomes of the SDP 2014 task on semantic dependency parsing, i.e. retrieving bi-lexical predicateargument relations between all content words within an English sentence. We have converted to a common format three existing annotations (DM, PAS, and PCEDT) over the same text and have put this to use for the first time in training and testing data-driven semantic dependency parsers. Building on strong community interest already to date and our belief that graph-oriented dependency parsing will further gain importance in the years to come, we are preparing a similar (slightly modified) task for SemEval 2015. Candidate modifications and extensions will include cross-domain testing and evaluation at the level of 'complete' predications (in contrast to more lenient per-dependency F 1 used this year). As optional new sub-tasks, we plan on offering cross-linguistic variation and predicate (i.e. semantic frame) disambiguation for at least some of the target representations. To further probe the role of syntax in the recovery of semantic dependency relations, we will make available to participants a wider selection of syntactic analyses, as well as add a third (idealized) 'gold' track, where syntactic dependencies are provided directly from available syntactic annotations of the underlying treebanks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample semantic dependency graphs for Example (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice . Partial semantic dependencies in PropBank and NomBank.</figDesc><table><row><cell></cell><cell></cell><cell>A1</cell><cell></cell><cell>A2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">top (a) A similar technique is almost impossible ARG2 BV ARG1 ARG1</cell><cell>to ARG1</cell><cell>apply</cell><cell cols="3">to other crops, such ARG3 ARG1 mwe ARG1</cell><cell cols="3">as cotton, ARG2 implicit_conj soybeans and rice. _and_c</cell></row><row><cell></cell><cell cols="9">(b) DELPH-IN Minimal Recursion Semantics-derived bi-lexical dependencies (DM).</cell></row><row><cell></cell><cell>top</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ARG2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ARG1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ARG2</cell></row><row><cell>ARG1</cell><cell></cell><cell>ARG2</cell><cell>ARG2</cell><cell></cell><cell>ARG2</cell><cell cols="2">ARG1</cell><cell></cell><cell></cell><cell>ARG1</cell></row><row><cell>ARG1</cell><cell>ARG1</cell><cell>ARG1</cell><cell>ARG1</cell><cell>ARG1</cell><cell cols="2">ARG1 ARG1</cell><cell cols="2">ARG1</cell><cell>ARG1</cell><cell>ARG2</cell><cell>ARG2</cell></row><row><cell cols="6">A similar technique is almost impossible to apply to other</cell><cell>crops</cell><cell cols="4">, such as cotton , soybeans and rice</cell></row><row><cell></cell><cell></cell><cell cols="6">(c) Enju Predicate-Argument Structures (PAS).</cell><cell></cell><cell></cell></row></table><note>A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Tabular SDP data format (showing DM).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Contrastive high-level graph statistics. sentences, depending on the count of graph nodes. -one alignment of tokens could not be established across all three representations; or (c) at least one of the graphs was cyclic. Of the 43,746 sentences in these 22 first sections of WSJ text, Deep-Bank lacks analyses for close to 15%, and the Enju Treebank has gaps for a little more than four percent. Some 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB idiosyncrasies like G.m.b, H. , S.p, A. , and U.S., . , and introducing a few new ones</figDesc><table><row><cell>4 Data Sets</cell></row><row><cell>All three target representations are annotations of</cell></row><row><cell>the same text, Sections 00-21 of the WSJ Cor-</cell></row><row><cell>pus. For this task, we have synchronized these</cell></row><row><cell>resources at the sentence and tokenization levels</cell></row><row><cell>and excluded from the SDP 2014 training and test-</cell></row><row><cell>ing data any sentences for which (a) one or more of</cell></row><row><cell>the treebanks lacked a gold-standard analysis; (b) a</cell></row><row><cell>one-to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Pairwise F 1 similarities, including punctuation (upper right diagonals) or not (lower left).</figDesc><table><row><cell>drawn in dependency labels (1), there are clear dif-</cell></row><row><cell>ferences between the representations, with PCEDT</cell></row><row><cell>appearing linguistically most fine-grained, and PAS</cell></row><row><cell>showing the smallest label inventory. Unattached</cell></row><row><cell>singleton nodes (2) in our setup correspond to</cell></row><row><cell>tokens analyzed as semantically vacuous, which</cell></row><row><cell>(as seen in Figure</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05 Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42 Copenhagen-80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01 Malmö Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19 Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82 Linköping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01 House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">DM</cell><cell></cell><cell></cell><cell cols="2">PAS</cell><cell></cell><cell></cell><cell cols="2">PCEDT</cell></row><row><cell></cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell></row><row><cell>Peking</cell><cell cols="3">85.91 DM</cell><cell></cell><cell></cell><cell></cell><cell>PAS</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCEDT</cell></row><row><cell></cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell><cell>LP</cell><cell>LR</cell><cell>LF</cell><cell>LM</cell></row><row><cell cols="14">Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68</cell></row><row><cell>CMU</cell><cell cols="13">82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12</cell></row><row><cell>Turku</cell><cell cols="13">80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82</cell></row><row><cell cols="14">Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42</cell></row><row><cell>Alpage</cell><cell cols="13">78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60</cell></row><row><cell>In-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results of the closed</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Overview of submitted systems, high-level approaches, and additional resources used (if any).In the closed track, the average LF scores across target representations range from 85.91 to 72.20. Comparing the results for different target representations, the average LF scores across systems are 85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://alt.qcri.org/semeval2014/ task8/ for further technical details, information on how to obtain the data, and official results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">These statistics are obtained using the 'official' SDP toolkit. We refer to nodes that have neither incoming nor outgoing edges and are not marked as top nodes as singletons; these nodes are ignored in subsequent statistics, e.g. when determining the proportion of edges per node (3) or the percentages of rooted trees (4) and fragmented graphs (6). The notation '%n' denotes (non-singleton) node percentages, and '%g' percentages over all graphs. We consider a root node any (non-singleton) node that has no incoming edges; reentrant nodes have at least two incoming edges. Following<ref type="bibr" target="#b25">Sagae and Tsujii (2008)</ref>, we consider a graph projective when there are no crossing edges (in a left-to-right rendering of nodes) and no roots are 'covered', i.e. for any root j there is no edge i → k</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In all formats, punctuation marks like dashes, colons, and sometimes commas can be contentful, i.e. at times occur as both predicates, arguments, and top nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">As detailed by<ref type="bibr" target="#b18">Miyao et al. (2014)</ref>, individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a 'shared' modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PCEDT.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Željko Agić and Bernd Bohnet for consultation and assistance in preparing our baseline and companion parses, to the Linguistic Data Consortium (LDC) for support in distributing the SDP data to participants, as well as to Emily M. Bender and two anonymous reviewers for feedback on this manuscript. Data preparation was supported through access to the ABEL high-performance computing facilities at the University of Oslo, and we acknowledge the Scientific Computing staff at UiO, the Norwegian Metacenter for Computational Science, and the Norwegian tax payers. Part of this work has been supported by the infrastructural funding by the Ministry of Education, Youth and Sports of the Czech Republic (CEP ID LM2010013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
				<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Natural Language Learning</title>
				<meeting>the 10th Conference on Natural Language Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Minimal Recursion Semantics. An introduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="332" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">, M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
				<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning for high-quality tokenization. Replicating variable tokenization schemes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics and intelligent text processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="231" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On building a more efficient grammar by exploiting types. Natural Language Engineering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep-Bank. A dynamically annotated treebank of the Wall Street Journal</title>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories</title>
				<meeting>the 11th International Workshop on Treebanks and Linguistic Theories<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Edições Colibri</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Utterer&apos;s meaning, sentencemeaning, and word-meaning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="242" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The CoNLL-2009 Shared Task. syntactic and semantic dependencies in multiple languages</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Natural Language Learning</title>
				<meeting>the 13th Conference on Natural Language Learning<address><addrLine>Boulder, CO, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Announcing Prague Czech-English Dependency Treebank 2.0</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajičová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Žabokrtský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
				<meeting>the 8th International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Who did what to whom? A contrastive study of syntacto-semantic dependencies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Øvrelid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Linguistic Annotation Workshop</title>
				<meeting>the Sixth Linguistic Annotation Workshop<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic parsing. The task, the state of the art and the future</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpora of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dependency syntax. Theory and practice</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mel'čuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>SUNY Press</publisher>
			<pubPlace>Albany, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Annotating noun argument structure for NomBank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Language Resources and Evaluation</title>
				<meeting>the 4th International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="803" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In-house: An ensemble of pre-existing off-the-shelf parsers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the interpretation of noun compounds: Syntax, semantics, and entailment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="330" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminantbased MRS banking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lønning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
				<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1250" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Proposition Bank. A corpus annotated with semantic roles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Meeting of the Association for Computational Linguistics</title>
				<meeting>the 21st International Conference on Computational Linguistics and the 44th Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Word and object</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V O</forename><surname>Quine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shift-reduce dependency DAG parsing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
				<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online graph planarisation for synchronous parsing of semantic and syntactic dependencies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Joint Conference on Artifical Intelligence</title>
				<meeting>the 21st International Joint Conference on Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1562" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adding Noun Phrase Structure to the Penn Treebank</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Meeting of the Association for Computational Linguistics</title>
				<meeting>the 45th Meeting of the Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
