<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
							<email>carreras@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre Technical University of Catalonia (UPC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
							<email>lluism@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre Technical University of Catalonia (UPC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the few last years there has been an increasing interest in shallow semantic parsing of natural language, which is becoming an important component in all kind of NLP applications. As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence. In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.</p><p>Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information. In <ref type="bibr" target="#b1">(Carreras and Màrquez, 2004)</ref> one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004. Ten systems contributed to the task, which was evaluated using the PropBank corpus . The best results were around 70 in F 1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F 1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop <ref type="bibr" target="#b13">(Litkowski, 2004)</ref>. Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus <ref type="bibr" target="#b7">(Fillmore et al., 2001)</ref>. From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works <ref type="bibr" target="#b25">(Punyakanok et al., 2004;</ref><ref type="bibr" target="#b18">Moschitti, 2004;</ref><ref type="bibr" target="#b34">Xue and Palmer, 2004;</ref><ref type="bibr" target="#b23">Pradhan et al., 2005a)</ref>.</p><p>Following last year's initiative, the CoNLL-2005 shared task 1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are:</p><p>• Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.</p><p>• The training corpus has been substantially enlarged. This allows to test the scalability of learning-based SRL systems to big datasets and to compute learning curves to see how much data is necessary to train. Again, we concentrate on the PropBank corpus , which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate-argument structures.</p><p>• In order to test the robustness of the presented systems, a cross-corpora evaluation is performed using a fresh test set from the Brown corpus.</p><p>Regarding evaluation, two different settings were devised depending if the systems use the information strictly contained in the training data (closed challenge) or they make use of external sources of information and/or tools (open challenge). The closed setting allows to compare systems under strict conditions, while the open setting aimed at exploring the contributions of other sources of information and the limits of the current learning-based systems on the SRL task. At the end, all 19 systems took part in the closed challenge and none of them in the open challenge.</p><p>The rest of the paper is organized as follows. Section 2 describes the general setting of the task. Section 3 provides a detailed description of training, development and test data. Participant systems are described and compared in section 4. In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets. Finally, section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>As in the 2004 edition, the goal of the task was to develop a machine learning system to recognize arguments of verbs in a sentence, and label them with their semantic role. A verb and its set of arguments form a proposition in the sentence, and typically, a sentence contains a number of propositions.</p><p>There are two properties that characterize the structure of the arguments in a proposition. First, arguments do not overlap, and are organized sequentially. Second, an argument may appear split into a number of non-contiguous phrases. For instance, in the sentence "[ A1 The apple], said John, [ C−A1 is on the table]", the utterance argument (labeled with type A1) appears split into two phrases. Thus, there is a set of non-overlapping arguments labeled with semantic roles associated with each proposition. The set of arguments of a proposition can be seen as a chunking of the sentence, in which chunks are parts of the semantic roles of the proposition predicate.</p><p>In practice, number of target verbs are marked in a sentence, each governing one proposition. A system has to recognize and label the arguments of each target verb. To support the role labeling task, sentences contain input annotations, that consist of syntactic information and named entities. Section 3 describes in more detail the annotations of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation</head><p>Evaluation is performed on a collection of unseen test sentences, that are marked with target verbs and contain only predicted input annotations.</p><p>A system is evaluated with respect to precision, recall and the F 1 measure of the predicted arguments. Precision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Finally, the F 1 measure computes the harmonic mean of precision and recall, and is the final measure to compare the performance of systems. It is formulated as: F β=1 = 2pr/(p + r).</p><p>For an argument to be correctly recognized, the words spanning the argument as well as its semantic role have to be correct. <ref type="bibr">2</ref> As an exceptional case, the verb argument of each proposition is excluded from the evaluation. This argument is the lexicalization of the predicate of the proposition. Most of the time, the verb corresponds to the target verb of the proposition, which is provided as input, and only in few cases the verb participant spans more words than the target verb. Except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition over-estimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation.   <ref type="formula">2nd</ref>), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th column marks target verbs, and their propositions are found in remaining columns. According to the PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for intersperse (9th), A0 is the arranger, and A1 the entity interspersed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Closed Challenge Setting</head><p>The organization provided training, development and test sets derived from the standard sections of the Penn TreeBank <ref type="bibr" target="#b14">(Marcus et al., 1993)</ref> and Prop-Bank  corpora.</p><p>In the closed challenge, systems have to be built strictly with information contained in the training sections of the TreeBank and PropBank. Since this collection contains the gold reference annotations of both syntactic and predicate-argument structures, the closed challenge allows: (1) to make use of any preprocessing system strictly developed within this setting, and (2) to learn from scratch any annotation that is contained in the data. To support the former, the organization provided the output of state-of-theart syntactic preprocessors, described in Section 3.</p><p>The development set is used to tune the parameters of a system. The gold reference annotations are also available in this set, but only to evaluate the performance of different parametrizations of a system, and select the optimal one. Finally, the test set is used to evaluate the performance of a system. It is only allowed to use predicted annotations in this set.</p><p>Since all systems in this setting have had access to the same training and development data, the evaluation results on the test obtained by different systems are comparable in a fair manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The data consists of sections of the Wall Street Journal part of the Penn TreeBank <ref type="bibr" target="#b14">(Marcus et al., 1993)</ref>, with information on predicate-argument structures extracted from the PropBank corpus . In this edition of the CoNLL shared task, we followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus (namely, ck01-03). The predicateargument annotations of the latter test material were kindly provided by the PropBank team, and are very valuable, as they allow to evaluate learning systems on a portion of data that comes from a different source than training.</p><p>We first describe the annotations related to argument structures. Then, we describe the preprocessing systems that have been selected to predict the input part of the data. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of a fully-annotated sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PropBank</head><p>The Proposition Bank (PropBank)  annotates the Penn TreeBank with verb argument structure. The semantic roles covered by Prop-Bank are the following:</p><p>• Numbered arguments (A0-A5, AA): Arguments defining verb-specific roles. Their semantics depends on the verb and the verb usage in a sentence, or verb sense. The most frequent roles are A0 and A1 and, commonly, A0 stands for the agent and A1 corresponds to the patient or theme of the proposition. However, no consistent generalization can be made across different verbs or different senses of the same verb. PropBank takes the definition of verb senses from VerbNet, and for each verb and each sense defines the set of possible roles for that verb usage, called the roleset. The definition of rolesets is provided in the PropBank Frames files, which is made available for the shared task as an official resource to develop systems.</p><p>• The role of a reference is the same as the role of the referenced argument. The label is an R-tag prefixed to the label of the referent, e.g. R-A1.</p><p>• Verbs (V): Argument corresponding to the verb of the proposition. Each proposition has exaclty one verb argument.</p><p>We used PropBank-1.0. Most predicative verbs were annotated, although not all of them (for example, most of the occurrences of the verb "to have" and "to be" were not annotated). We applied procedures to check consistency of propositions, looking for overlapping arguments, and incorrect semantic role labels. Also, co-referenced arguments were annotated as a single item in PropBank, and we automatically distinguished between the referent and the reference with simple rules matching pronominal expressions, which were tagged as R arguments.   A total number of 80 propositions were not compliant with our procedures (one in the Brown files, the rest in WSJ) and were filtered out from the CoNLL data sets.</p><formula xml:id="formula_0">R-A3 28 0 1 0 R-A4 7 0 1 0 R-AA 2 0 0 0 R-AM-ADV 5 0 2 0 R-</formula><p>Table <ref type="table" target="#tab_2">1</ref> provides counts of the number of sentences, tokens, annotated propositions, distinct verbs, and arguments in the four data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing Systems</head><p>In this section we describe the selected processors that computed input annotations for the SRL systems. The annotations are: part-of-speech (PoS) tags, chunks, clauses, full syntactic trees and named entities. As it has been noted, participants were also allowed to use any processor developed within the same WSJ partition.</p><p>The preprocessors correspond to the following state-of-the-art systems:</p><p>• UPC processors, consisting of: -PoS tagger: <ref type="bibr" target="#b9">(Giménez and Màrquez, 2003)</ref>, based on Support Vector Machines, and trained on WSJ sections 02-21. -Base Chunker and Clause Recognizer: <ref type="bibr" target="#b0">(Carreras and Màrquez, 2003)</ref>, based on Voted Perceptrons, trained on WSJ sections 02-21. These two processors form a coherent partial syntax of a sentence, that is, chunks and clauses form a partial syntactic tree.</p><p>• Full parser of Collins ( <ref type="formula">1999</ref>), with "model 2". Predicts WSJ full parses, with information of the lexical head for each syntactic constituent.</p><p>The PoS tags (required by the parser) have been computed with <ref type="bibr" target="#b9">(Giménez and Màrquez, 2003)</ref>.</p><p>• Full parser of <ref type="bibr" target="#b2">Charniak (2000)</ref>. Jointly predicts PoS tags and full parses.</p><p>• Named Entities predicted with the Maximum-Entropy based tagger of <ref type="bibr" target="#b4">Chieu and Ng (2003)</ref>.</p><p>The tagger follows the CoNLL-2003 task setting <ref type="bibr" target="#b30">(Tjong Kim Sang and De Meulder, 2003)</ref>, and thus is not developed with WSJ data. However, we allowed its use because there is no available named entity recognizer developed with WSJ data. The reported performance on the CoNLL-2003 test is F 1 = 88.31, with Prec/Rec. at 88.12/88.51.</p><p>Tables <ref type="table" target="#tab_4">2 and 3</ref> summarize the performance of the syntactic processors on the development and test sets. The performance of full parsers on the WSJ test is lower than that reported in the corresponding papers. The reason is that our evaluation figures have been computed in a strict manner with respect to punctuation tokens, while the full parsing community usually does not penalize for punctuation wrongly placed in the tree. <ref type="bibr">3</ref>   served, the performance of all syntactic processors suffers a substantial loss in the Brown test set. Noticeably, the parser of <ref type="bibr" target="#b6">Collins (1999)</ref> seems to be the more robust when moving from WSJ to Brown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Review of Participant Systems</head><p>Nineteen systems participated in the CoNLL-2005 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning techniques</head><p>Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the "ML-method" column of table 4 for a summary of the following information. Log-linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits.</p><p>In particular, 8 teams used the Maximum Entropy (ME) statistical framework <ref type="bibr" target="#b3">(Che et al., 2005;</ref><ref type="bibr" target="#b10">Haghighi et al., 2005;</ref><ref type="bibr" target="#b21">Park and Rim, 2005;</ref><ref type="bibr" target="#b31">Tjong Kim Sang et al., 2005;</ref><ref type="bibr" target="#b29">Sutton and McCallum, 2005;</ref><ref type="bibr" target="#b32">Tsai et al., 2005;</ref><ref type="bibr" target="#b35">Yi and Palmer, 2005;</ref><ref type="bibr" target="#b33">Venkatapathy et al., 2005)</ref>. Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels <ref type="bibr" target="#b16">(Mitsumori et al., 2005;</ref><ref type="bibr" target="#b31">Tjong Kim Sang et al., 2005;</ref><ref type="bibr" target="#b32">Tsai et al., 2005;</ref><ref type="bibr" target="#b24">Pradhan et al., 2005b)</ref>, another one using Gaussian kernels <ref type="bibr" target="#b19">(Ozgencil and McCracken, 2005)</ref>, and a last group using tree-based kernels specifically designed for the task <ref type="bibr" target="#b17">(Moschitti et al., 2005)</ref>. Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators <ref type="bibr" target="#b26">(Punyakanok et al., 2005)</ref>.</p><p>Decision Tree learning (DT) was also represented Devel.</p><p>Test WSJ Test Brown P(%) R(%) F 1 P(%) R(  Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel-based linear discriminant inside the framework of Sparse Bayesian Learning <ref type="bibr" target="#b11">(Johansson and Nugues, 2005)</ref> and Tree Conditional Random Fields (T-CRF) <ref type="bibr" target="#b5">(Cohn and Blunsom, 2005)</ref>, that extend the sequential CRF model to tree structures. Finally, <ref type="bibr" target="#b12">Lin and Smith (2005)</ref> presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification.</p><p>From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of increasing the robustness and coverage of the systems, which are quite dependent on input parsing errors. The different outputs to combine are obtained by varying input information, changing learning algorithm, or considering n-best solution lists. The combination schemes presented include very simple voting-like combination heuristics, stacking of classifiers, and a global constraint satisfaction framework modeled with Integer Linear Programming. Global models trained to re-rank alternative outputs represent a very interesting alternative that has been proposed by two systems. All these issues are reviewed in detail in section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SRL approaches</head><p>SRL is a complex task, which may be decomposed into a number of simpler decisions and annotating schemes in order to be addressed by learning techniques. Table <ref type="table" target="#tab_8">4</ref> contains a summary of the main properties of the 19 systems presented. In this section we will explain the contents of that table by columns (from left-to-right).</p><p>One first issue to consider is the input structure to navigate in order to extract the constituents that will form labeled arguments. The majority of systems perform parse tree node labeling, searching for a one-to-one map between arguments and parse constituents. This information is summarized in the "synt" column of Table <ref type="table" target="#tab_8">4</ref>. "col", "cha", "upc" stand for the syntactic parse trees (the latter is partial) provided as input by the organization. Additionally, some teams used lists of n-best parsings generated by available tools ("n-cha" by Charniak parser; "nbikel" by Bikel's implementation of Collins parser). Interestingly, Yi and  retrained Ratnaparkhi's parser using the WSJ training sections enriched with semantic information coming from PropBank annotations. These are referred to as AN and AM parses. As it can be seen, Charniak parses were used by most of the systems. Collins parses were used also in some of the best performing systems based on combination.</p><p>The exceptions to the hierarchical processing are the systems by <ref type="bibr" target="#b24">Pradhan et al. (2005b)</ref> and <ref type="bibr" target="#b16">Mitsumori et al. (2005)</ref>, which perform a chunking-based sequential tokenization. As for the former, the system is the same than the one presented in the 2004 edition. The system by <ref type="bibr" target="#b15">Màrquez et al. (2005)</ref> explores hierarchical syntactic structures but selects, in a preprocess, a sequence of tokens to perform a sequential tagging afterwards.  In general, the presented systems addressed the SRL problem by applying different chained processes. In Table <ref type="table" target="#tab_8">4</ref> the column "pre" summarizes preprocessing. In most of the cases this corresponds to a pruning procedure to filter out constituents that are not likely to be arguments. As in feature development, the related bibliography has been followed for pruning. For instance, many systems used the pruning strategy described in <ref type="bibr" target="#b34">(Xue and Palmer, 2004)</ref> ("x&amp;p") and other systems used the soft pruning rules described in (Pradhan et al., 2005a) ("softp"). Remarkably, <ref type="bibr" target="#b21">Park and Rim (2005)</ref> parametrize the pruning procedure and then study the effect of being more or less aggressive at filtering constituents. In the case of <ref type="bibr" target="#b15">Màrquez et al. (2005)</ref>, pre-processing corresponds to a sequentialization of syntactic hierarchical structures. As a special case, <ref type="bibr" target="#b12">Lin and Smith (2005)</ref> used the GT-PARA analyzer for converting parse trees into a flat representation of all predicates including argument boundaries.</p><p>The second stage, reflected in column "label" of Table <ref type="table" target="#tab_8">4</ref>, is the proper labeling of selected candidates. Most of the systems used a two-step procedure consisting of first identifying arguments (e.g., with a binary "null" vs. "non-null" classifier) and then classifying them. This is referred to as "i+c" in the table. Some systems address this phase in a single classification step by adding a "null" category to the multiclass problem (referred to as "c'). The methods performing a sequential tagging use a BIO tagging scheme ("bio"). As a special case, <ref type="bibr" target="#b17">Moschitti et al. (2005)</ref> subdivide the "i+c" strategy into four phases: after identification, heuristics are applied to assure compatibility of identified arguments; and, before classifying arguments into roles, a preclassification into core vs. adjunct arguments is performed. <ref type="bibr" target="#b33">Venkatapathy et al. (2005)</ref> use three labels instead of two in the identification phase : "null", "mandatory", and "optional".</p><p>Since arguments in a solution do not embed and most systems identify arguments as nodes in a hierarchical structure, non-embedding constraints must be resolved in order to generate a coherent argument labeling. The "embed" column of Table <ref type="table" target="#tab_8">4</ref> accounts for this issue. The majority of systems applied specific greedy procedures that select a subset of consistent arguments. The families of heuristics to do that selection include prioritizing better scored constituents ("g-score"), or selecting the arguments that are first reached in a top-down exploration ("gtop"). Some probabilistic systems include the nonembedding constraints within the dynamic programming inference component, and thus calculate the most probable coherent labeling ("dp-prob"). The "defer" value means that this is a combination system and that coherence of the individual system predictions is not forced, but deferred to the later combination step. As a particular case, <ref type="bibr" target="#b33">Venkatapathy et al. (2005)</ref> use PropBank subcategorization frames to force a coherent solution. Note that tagging-based systems do not need to check non-embedding constraints ("!need" value).</p><p>The "glob" column of Table <ref type="table" target="#tab_8">4</ref> accounts for the locality/globality of the process used to calculate the output solution given the argument prediction candidates. Systems with a "yes" value in that column define some kind of scoring function (possibly probabilistic) that applies to complete candidate solutions, and then calculate the solution that maximizes the scoring using an optimization algorithm. Some systems use some kind of postprocessing to improve the final output of the system by correcting some systematic errors, or treating some types of simple adjunct arguments. This information is included in the "post" column of Table <ref type="table" target="#tab_8">4</ref>. In most of the cases, this postprocess is performed on the basis of simple ad-hoc rules. However, it is worth mentioning the work of <ref type="bibr" target="#b31">Tjong Kim Sang et al. (2005)</ref> in which spelling error correction techniques are adapted for improving the resulting role labeling. In that system, postprocessing is applied before system combination.</p><p>Most of the best performing systems included a combination of different base subsystems to increase robustness of the approach and to gain coverage and independence from parse errors. Last 2 columns of Table <ref type="table" target="#tab_8">4</ref> present this information. In the "comb" column the source of the combination is reported. Basically, the alternative outputs to combine can be generated by different input syntactic structures or nbest parse candidates, or by applying different learning algorithms to the same input information.</p><p>The type of combination is reported in the last column. <ref type="bibr" target="#b15">Màrquez et al. (2005)</ref> and <ref type="bibr" target="#b31">Tjong Kim Sang et al. (2005)</ref> performed a greedy merging of the arguments of base complete solutions ("s-join"). Yi and  did also a greedy merging of arguments but taking into account not complete solutions but all candidate arguments labeled by base systems ("ac-join"). In a more sophisticated way, <ref type="bibr" target="#b26">Punyakanok et al. (2005)</ref> and <ref type="bibr" target="#b32">Tsai et al. (2005)</ref> performed global inference as constraint satisfaction using Integer Linear Programming, also taking into account all candidate arguments ("ac-ILP"). It is worth noting that the generalized inference applied in those papers allows to include, jointly with the combination of outputs, a number of linguisticallymotivated constraints to obtain a coherent solution.</p><p>Pradhan et al. ( <ref type="formula">2005b</ref>) followed a stacking approach by learning a chunk-based SRL system including as features the outputs of two syntax-based systems. Finally, <ref type="bibr" target="#b10">Haghighi et al. (2005)</ref> and Sutton and McCallum ( <ref type="formula">2005</ref>) performed a different approach by learning a re-ranking function as a global model on top of the base SRL models. Actually, <ref type="bibr" target="#b10">Haghighi et al. (2005)</ref> performed a double selection step: an inner re-ranking of n-best solutions coming from the base system on a single tree; and an outer selection of the final solution among the candidate solutions coming from n-best parse trees. The reranking approach allows to define global complex features applying to complete candidate solutions to train the rankers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Features</head><p>Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task <ref type="bibr" target="#b8">(Gildea and Jurafsky, 2002;</ref><ref type="bibr" target="#b28">Surdeanu et al., 2003;</ref><ref type="bibr" target="#b23">Pradhan et al., 2005a;</ref><ref type="bibr" target="#b34">Xue and Palmer, 2004)</ref>. With no exception, all systems have made intensive use of syntax to extract features. While most systems work only on the output of a parser -Charniak's being the most preferred-some systems depend on many syntactic parsers. In the latter situation, either a system is a combination of many individual systems (each working with a different parser), or a system extracts features from many different parse trees while exploring the nodes of only one parse tree. Most systems have also considered named entities for extracting features.</p><p>The main types of features seen in this SRL edition can be divided into four general categories: ( <ref type="formula">1</ref>   (3) Features that capture the relation between the verb predicate and the constituent under consideration; and (4) Global features describing the complete argument labeling of a predicate. The rest of the section describes the most common feature types in each category. Table <ref type="table" target="#tab_10">5</ref> summarizes the type of features exploited by systems.</p><formula xml:id="formula_1">• + h,c • p,s • + • + + + w + • • + • ozgencil cha • + h • p • • • + + + • + + • • • johansson cha,upc + + h • • • • • + + + • + + • • • cohn col • + h + p,s • + • + + + w + • + + • park cha • + h,c • p • • • + + + • + • + • • mitsumori upc,cha + + • + t • • + + • + c,t • + • • • venkatapathy col + + h + • • • • + • + • + • • • • ponzetto col,upc + + h + • + • • + • • w,c,t • • + • • lin cha • + h + • • • • + • + w • • • • • sutton bik • + h + p,s • • • + • + • + • • • +</formula><p>To represent an argument itself, all systems make use of the syntactic type of the argument. Almost all teams used the heuristics of <ref type="bibr" target="#b6">Collins (1999)</ref> to extract the head word of the argument, and used features that capture the form, lemma and PoS tag of the head. In the same line, some systems also use features of the content words of the argument, using the heuristics of <ref type="bibr" target="#b28">Surdeanu et al. (2003)</ref>. Very generally also, many systems extract features from the first and last words of the argument. Regarding the syntactic elements surrounding the argument, many systems working on full trees have considered the parent and siblings of the argument, capturing their syntactic type and head word. Differently, other systems have captured features from the left/right tokens surrounding the argument, which are typically words, but can be chunks or general phrases in systems that sequentialize the task <ref type="bibr" target="#b15">(Màrquez et al., 2005;</ref><ref type="bibr" target="#b24">Pradhan et al., 2005b;</ref><ref type="bibr" target="#b16">Mitsumori et al., 2005)</ref>. Many systems use a variety of indicator features that capture properties of the argument structure and its local syntactic annotations. For example, indicators of the immediate syntactic types that form the argument, flags raised by punctuation tokens in or nearby the argument, or the governing category feature of <ref type="bibr" target="#b8">Gildea and Jurafsky (2002)</ref>. It is also somewhat gen-eral the use of specific features that apply when the constituent is a prepositional phrase, such as looking for the head word of the noun phrase within it. A few systems have also built semantic dictionaries from training data, that collect words appearing frequently in temporal, locative or other arguments.</p><p>To represent the predicate, all systems have used features codifying the form, lemma, PoS tag and voice of the verb. It is also of general use the subcategorization feature, capturing the syntactic rule that expands the parent of the predicate. Some systems captured statistics related to the frequency of a verb in training data (not in Table <ref type="table" target="#tab_10">5</ref>).</p><p>Regarding features related to an argument-verb pair, almost all systems use the simple feature describing the relative position between them. To a lesser degree, systems have computed distances from one to the other, based on the number of words or chunks between them, or based on the syntactic tree. Not surprisingly, all systems have extracted the path from the argument to the verb. While almost all systems use the standard path of <ref type="bibr" target="#b8">(Gildea and Jurafsky, 2002)</ref>, many have explored variations of it. A common one consists of the path from the argument to the lowest common ancestor of the verb and the argument. Another variation is the partial path, that is built of chunks and clauses only. Indicator features that capture scalar values of the path are also common, and concentrate mainly on looking at the common ancestor, capturing the difference of clausal levels, or looking for punctuation and other linguistic elements in the path. In this category, it is also noticeable the use of the syntactic frame feature, proposed by <ref type="bibr" target="#b34">Xue and Palmer (2004)</ref>.</p><p>Finally, in this edition two systems apply learning at a global context <ref type="bibr" target="#b10">(Haghighi et al., 2005;</ref><ref type="bibr" target="#b29">Sutton and McCallum, 2005)</ref> and, consequently, they are able to extract features from a complete labeling of a predicate. Basically, the central feature in this context extracts the sequential pattern of predicate arguments. Then, this pattern can be enriched with syntactic categories, broken down into role-specific indicator variables, or conjoined with the predicate lemma.</p><p>Apart from basic feature extraction, combination of features has also been explored in this edition. Many of the combinations depart from the manually selected conjunctions of <ref type="bibr" target="#b34">Xue and Palmer (2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>A baseline rate was computed for the task. It was produced using a system developed in the past shared task edition by Erik Tjong Kim Sang, from the University of Amsterdam, The Netherlands. The baseline processor finds semantic roles based on the following seven rules:</p><p>• Tag target verb and successive particles as V.</p><p>• Tag not and n't in target verb chunk as AM-NEG. • Tag modal verbs in target verb chunk as AM-MOD. • Tag first NP before target verb as A0.</p><p>• Tag first NP after target verb as A1.</p><p>• Tag that, which and who before target verb as R-A0.</p><p>• Switch A0 and A1, and R-A0 and R-A1 if the target verb is part of a passive VP chunk. A VP chunk is considered in passive voice if it contains a form of to be and the verb does not end in ing.</p><p>Table <ref type="table" target="#tab_12">6</ref> presents the overall results obtained by the nineteen systems plus the baseline, on the development and test sets (i.e., Development, Test WSJ, Test Brown, and Test WSJ+Brown). The systems are sorted by the performance on the combined WSJ+Brown test set.</p><p>As it can be observed, all systems clearly outperformed the baseline. There are seven systems with a final F 1 performance in the 75-78 range, seven more with performances in the 70-75 range, and five with a performance between 65 and 70. The best performance was obtained by <ref type="bibr" target="#b26">Punyakanok et al. (2005)</ref>, which almost reached an F 1 at 80 in the WSJ test set and almost 78 in the combined test. Their results on the WSJ test equal the best results published so far on this task and datasets <ref type="bibr" target="#b23">(Pradhan et al., 2005a)</ref>, though they are not directly comparable due to a different setting in defining arguments not perfectly matching the predicted parse constituents. Since the evaluation in the shared task setting is more strict, we believe that the best results obtained in the shared task represent a new breakthrough in the SRL task.</p><p>It is also quite clear that the systems using combination are better than the individuals. It is worth noting that the first 4 systems are combined. The  The best results in the CoNLL-2005 shared task are 10 points better than those of last year edition. This increase in performance should be attributed to a combination of the following factors: 1) training sets have been substantially enlarged; 2) predicted parse trees are available as input information; and 3) more sophisticated combination schemes have been implemented. In order to have a more clear idea of the impact of enriching the syntactic information, we refer to <ref type="bibr" target="#b15">(Màrquez et al., 2005)</ref>, who developed an individual system based only on partial parsing ("upc" input information). That system performed F 1 =73.57 on the development set, which is 2.18 points below the F 1 =75.75 obtained by the same architecture using full parsing, and 4.47 points below the best performing combined system on the development set <ref type="bibr" target="#b24">(Pradhan et al., 2005b)</ref>.</p><p>Comparing the results across development and WSJ test corpora, we find that, with two exceptions, all systems experienced a significant increase in performance (normally between 1 and 2 F 1 points). This fact may be attributed to the different levels of difficulty found across WSJ sections. The linguistic processors and parsers perform slightly worse in the development set. As a consequence, the matching between parse nodes and actual arguments is lower.</p><p>Regarding the evaluation using the Brown test set, all systems experienced a severe drop in performance (about 10 F 1 points), even though the baseline on the Brown test set is higher than that of the WSJ test set. As already said in previous sections, all the linguistic processors, from PoS tagging to full parsing, showed a much lower performance than in the WSJ test set, evincing that their performance cannot be extrapolated across corpora. Presumably, this fact is the main responsible of the performace drop, though we do not discard an additional overfitting effect due to the design of specific features that do not generalize well. More im-portantly, this results impose (again) a severe criticism on the current pipelined architecture for Natural Language Processing. Error propagation and amplification through the chained modules make the final output generalize very badly when changing the domain of application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have described the CoNLL-2005 shared task on semantic role labeling. Contrasting with the CoNLL-2004 edition, the current edition has incorporated the use of full syntax as input to the SRL systems, much larger training sets, and crosscorpora evaluation. The first two novelties have most likely contributed to an improvement of results. The latter has evinced a major drawback of natural language pipelined architectures.</p><p>Nineteen teams have participated to the task, contributing with a variety of learning algorithms, labeling strategies, feature design and experimentation. While, broadly, all systems make use of the same basic techniques described in existing SRL literature, some novel aspects have also been explored. A remarkable aspect, common in the four top-performing systems and many other, is that of combining many individual SRL systems, each working on different syntactic structures. Combining systems improves robustness, and overcomes the limitations in coverage that working with a single, non-correct syntactic structure imposes. The best system, presented by <ref type="bibr" target="#b26">Punyakanok et al. (2005)</ref>, achieves an F 1 at 79.44 on the WSJ test. This performance, of the same order than the best reported in literature, is still far from the desired behavior of a natural language analyzer. Furthermore, the performance of such SRL module in a real application will be about ten points lower, as demonstrated in the evaluation on the sentences from Brown.</p><p>We conclude with two open questions. First, what semantic knowledge is needed to improve the quality and performance of SRL systems. Second, beyond pipelines, what type of architectures and language learning methodology ensures a robust performance of processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st column), PoS tags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th column marks target verbs, and their propositions are found in remaining columns. According to the PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for intersperse (9th), A0 is the arranger, and A1 the entity interspersed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Adjuncts (AM-): General arguments that any verb may take optionally. There are 13 types of adjuncts: AM-ADV : general-purpose AM-MOD : modal verb AM-CAU : cause AM-NEG : negation marker AM-DIR : direction AM-PNC : purpose AM-DIS : discourse marker AM-PRD : predication AM-EXT : extent AM-REC : reciprocal AM-LOC : location AM-TMP : temporal AM-MNR : manner • References (R-): Arguments representing arguments realized in other parts of the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>col), UPC partial parsers (upc), Bikel's Collins model (bik) and/or argument-enriched parsers (an,am); ne: use of named entities. On the argument: at: argument type; aw: argument words, namely the head (h) and/or content words (c); ab: argument boundaries, i.e. form and PoS of first and/or last argument words; ac: argument context, capturing features of the parent (p) and/or left/right siblings (s), or the tokens surrounding the argument (t); ai: indicators of the structure of the argument (e,g., on internal constituents, surrounding/boundary punctuation, governing category, etc.); pp: specific features for prepositional phrases; sd: semantic dictionaries. On the verb: v: standard verb features (voice, word/lemma, PoS); sc: subcategorization. On the arg-verb relation: rp: relative position; di: distance, based on words (w), chunks (c) or the syntactic tree (t); ps: standard path; pv: path variations; pi: scalar indicator variables on the path (of chunks, clauses, or other phrase types), common ancestor, etc.; sf: syntactic frame<ref type="bibr" target="#b34">(Xue and Palmer, 2004)</ref>; On the complete proposition: as: sequence of arguments of a proposition. argument; (2) Features describing properties of the target verb predicate;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Counts on the data sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>As it can be ob-Dev. tWSJ tBrown UPC PoS-tagger 97.13 97.36 94.73 Charniak (2000) 92.01 92.29 87.89</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) of PoS taggers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>93.17 93.91 95.26 94.52 94.89 92.64 90.85 91.73 UPC Clauser 90.38 84.73 87.46 90.93 85.94 88.36 84.21 74.32 78.95 Collins (1999) 85.02 83.55 84.28 85.63 85.20 85.41 82.68 81.33 82.00 Charniak (2000) 87.60 87.38 87.49 88.20 88.30 88.25 80.54 81.15 80.84</figDesc><table><row><cell></cell><cell>%)</cell><cell>F 1</cell><cell>P(%) R(%)</cell><cell>F 1</cell></row><row><cell>UPC Chunker</cell><cell>94.66</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of the syntactic parsers on the development, and WSJ and Brown test sets. Unlike in full parsing, the figures have been computed on a strict evaluation basis with respect to punctuation.</figDesc><table><row><cell>by Ponzetto and Strube (2005), who used C4.5.</cell></row><row><cell>Ensembles of decision trees learned through the</cell></row><row><cell>AdaBoost algorithm (AB) were applied by Màrquez</cell></row><row><cell>et al. (2005) and Surdeanu and Turmo (2005). Tjong</cell></row><row><cell>Kim Sang et al. (2005) applied, among others,</cell></row><row><cell>Memory-Based Learning (MBL).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Main properties of the SRL strategies implemented by the participant teams, sorted by F 1 performance on the WSJ+Brown test set. synt stands for the syntactic structure explored; pre stands for pre-processing steps; label stands for the labeling strategy; embed stands for the technique to ensure nonembedding of arguments; glob stands for global optimization; post stands for post-processing; comb stands for system output combination, and type stands for the type of combination. Concrete values appearing in the table are explained in section 4.1. The symbol "?" stands for unknown values not reported by the system description papers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Main feature types used by the 19 participating systems in the CoNLL-2005 shared task, sorted by performance on the WSJ+Brown test set. Sources: synt: use of parsers, namely Charniak (cha), Collins</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>74.83 77.35 82.28 76.78 79.44 73.38 62.93 67.75 81.18 74.92 77.92 haghighi 77.66 75.72 76.68 79.54 77.39 78.45 70.24 65.37 67.71 78.34 75.78 77.04 marquez 78.39 75.53 76.93 79.55 76.45 77.97 70.79 64.35 67.42 78.44 74.83 76.59 pradhan 80.90 75.38 78.04 81.97 73.27 77.37 73.73 61.51 67.07 80.93 71.69 76.03 surdeanu 79.14 71.57 75.17 80.32 72.95 76.46 72.41 59.67 65.42 79.35 71.17 75.04 tsai 81.13 72.42 76.53 82.77 70.90 76.38 73.21 59.49 65.64 81.55 69.37 74.97 che 79.65 71.34 75.27 80.48 72.79 76.44 71.13 59.99 65.09 79.30 71.08 74.97 moschitti 74.95 73.10 74.01 76.55 75.24 75.89 65.92 61.83 63.81 75.19 73.45 74.31 tjongkimsang 76.79 70.01 73.24 79.03 72.03 75.37 70.45 60.13 64.88 77.94 70.44 74.00 yi 75.70 69.99 72.73 77.51 72.97 75.17 67.88 59.03 63.14 76.31 71.10 73.61 ozgencil 73.57 71.87 72.71 74.66 74.21 74.44 65.52 62.93 64.20 73.48 72.70 73.09 johansson 73.40 70.85 72.10 75.46 73.18 74.30 65.17 60.59 62.79 74.13 71.50 72.79 cohn 73.51 68.98 71.17 75.81 70.58 73.10 67.63 60.08 63.63 74.76 69.17 71.86 park 72.68 69.16 70.87 74.69 70.78 72.68 64.58 60.31 62.38 73.35 69.37 71.31 mitsumori 71.68 64.93 68.14 74.15 68.25 71.08 63.24 54.20 58.37 72.77 66.37 69.43 venkatapathy 71.88 64.76 68.14 73.76 65.52 69.40 65.25 55.72 60.11 72.66 64.21 68.17 ponzetto 71.82 61.60 66.32 75.05 64.81 69.56 66.69 52.14 58.52 74.02 63.12 68.13 lin 70.11 61.96 65.78 71.49 64.67 67.91 65.75 52.82 58.58 70.80 63.09 66.72 sutton 64.43 63.11 63.76 68.57 64.99 66.73 62.91 54.85 58.60 67.86 63.63 65.68 baseline 50.00 28.98 36.70 51.13 29.16 37.14 62.66 33.07 43.30 52.58 29.69 37.95</figDesc><table><row><cell></cell><cell>Development</cell><cell></cell><cell>Test WSJ</cell><cell></cell><cell>Test Brown</cell><cell></cell><cell cols="2">Test WSJ+Brown</cell></row><row><cell></cell><cell>P(%) R(%)</cell><cell>F1</cell><cell>P(%) R(%)</cell><cell>F1</cell><cell>P(%) R(%)</cell><cell>F1</cell><cell>P(%) R(%)</cell><cell>F1</cell></row><row><cell>punyakanok</cell><cell>80.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Overall precision, recall and F 1 rates obtained by the 19 participating systems in the CoNLL-2005 shared task on the development and test sets. Systems sorted by F 1 score on the WSJ+Brown test set.</figDesc><table><row><cell>best individual system on the task is that of Sur-</cell></row><row><cell>deanu and Turmo (2005), which obtained F 1 =75.04</cell></row><row><cell>on the combined test set, about 3 points below than</cell></row><row><cell>the best performing combined system. On the de-</cell></row><row><cell>velopment set, that system achieved a performace</cell></row><row><cell>of 75.17 (slightly below than the 75.27 reported by</cell></row><row><cell>Che et al. (2005) on the same dataset). Accord-</cell></row><row><cell>ing to the description papers, we find that other</cell></row><row><cell>individual systems, from which the combined sys-</cell></row><row><cell>tems are constructed, performed also very well. For</cell></row><row><cell>instance, Tsai et al. (2005) report F 1 =75.76 for a</cell></row><row><cell>base system on the development set, Màrquez et al.</cell></row><row><cell>(2005) report F 1 =75.75, Punyakanok et al. (2005)</cell></row><row><cell>report F 1 =74.76, and Haghighi et al. (2005) report</cell></row><row><cell>F 1 =74.52.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The official CoNLL-2005 shared task web page, including data, software and systems' outputs, is available at http://www.lsi.upc.edu/∼srlconll.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The srl-eval.pl program is the official program to evaluate the performance of a system. It is available at the Shared Task web page.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Before evaluating Collins', we raised punctuation to the highest point in the tree, using a script that is available at the shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Authors would like to thank the following people and institutions. The PropBank team, and specially Martha Palmer and Benjamin Snyder, for making available PropBank-1.0 and the prop-banked Brown files. The Linguistic Data Consortium, for issuing a free evaluation license for the shared task to use the TreeBank. Hai Leong Chieu and Hwee Tou Ng, for running their Named Entity tagger on the task data. Finally, the teams contributing to the shared task, for their great enthusiasm. This work has been partially funded by the European Community (Chil -IP506909; PASCAL -IST-2002-506778) and the Spanish Ministry of Science and Technology (Aliado, TIC2002-04447-C02).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Phrase recognition by filtering and ranking with perceptrons</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP-2003</title>
				<meeting>RANLP-2003<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2004</title>
				<meeting>CoNLL-2004</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A maximum-entropy inspired parser</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-2000</title>
				<meeting>NAACL-2000</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic role labeling system using maximum entropy classifier</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with a maximum entropy approach</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
				<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic role labelling with tree conditional random fields</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Head-driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large lexical databank which provides deep semantics</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Wooters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Asian Conference on Language, Informa tion and Computation</title>
				<meeting>the Pacific Asian Conference on Language, Informa tion and Computation<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and accurate part-of-speech tagging: The svm approach revisited</title>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP-2003</title>
				<meeting>RANLP-2003<address><addrLine>Borovets</addrLine></address></meeting>
		<imprint>
			<publisher>Bulgaria</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A joint model for semantic role labeling</title>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse bayesian classification of predicate arguments</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic role labeling via consensus in pattern-matching</title>
		<author>
			<persName><forename type="first">Chi-San</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Senseval-3 task: Automatic labeling of semantic roles</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Senseval-3 ACL-SIGLEX Workshop</title>
				<meeting>the Senseval-3 ACL-SIGLEX Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic role labeling as sequential tagging</title>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pere</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Jesús Giménez, and Neus Català</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic role labeling using support vector machines</title>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Mitsumori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasushi</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouichi</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirohumi</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical semantic role labeling</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Giuglea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonaventura</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study on convolution kernel for shallow semantic parsing</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-2004)</title>
				<meeting>the 42nd Annual Conference of the Association for Computational Linguistics (ACL-2004)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic role labeling using libSVM</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Necati Ercan Ozgencil</surname></persName>
		</author>
		<author>
			<persName><surname>Mccracken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum entropy based semantic role labeling</title>
		<author>
			<persName><forename type="first">Kyung-Mi</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hae-Chang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic role labeling using lexical statistical information</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector learning for semantic argument classification</title>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Krugler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning. Special issue on Speech and Natural Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic role chunking combining complementary syntactic views</title>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic role labeling via integer linear programming inference</title>
		<author>
			<persName><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dav</forename><surname>Zimak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
				<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized inference with multiple semantic role labeling systems</title>
		<author>
			<persName><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Koomen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic role labeling using complete syntactic analysis</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using predicate-argument structures for information extraction</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Aarseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2003</title>
				<meeting>ACL 2003<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint parsing and semantic role labeling</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
				<meeting>CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Applying spelling error correction techniques for improving semantic role labelling</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">Tjong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Canisius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Antal van den Bosch, and Toine Bogers</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming</title>
		<author>
			<persName><forename type="first">Tzong-Han</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Lian</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inferring semantic roles using subcategorization frames and maximum entropy model</title>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshar</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashanth</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Calibrating features for semantic role labeling</title>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The integration of syntactic parsing and semantic role labeling</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Szu-Ting Yi</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2005</title>
				<meeting>CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
