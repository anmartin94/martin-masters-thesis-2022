<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2020 Task 3: Graded Word Similarity in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carlos</forename><forename type="middle">S</forename><surname>Armendariz</surname></persName>
							<email>c.santosarmendariz@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Research Group</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
							<email>m.purver@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Research Group</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Senja</forename><surname>Pollak</surname></persName>
							<email>senja.pollak@ijs.si</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikola</forename><surname>Ljubešić</surname></persName>
							<email>nikola.ljubesic@ijs.si</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jožef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matej</forename><surname>Ulčar</surname></persName>
							<email>matej.ulcar@fri.uni-lj.si</email>
						</author>
						<author>
							<persName><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Tehran Institute for Advanced Studies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2020 Task 3: Graded Word Similarity in Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the Graded Word Similarity in Context (GWSC) task which asked participants to predict the effects of context on human perception of similarity in English, Croatian, Slovene and Finnish. We received 15 submissions and 11 system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contextualised word embeddings, produced by models such as ELMo <ref type="bibr" target="#b31">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 <ref type="bibr" target="#b13">(Finkelstein et al., 2002)</ref> and SimLex-999 <ref type="bibr" target="#b20">(Hill et al., 2015)</ref> were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS <ref type="bibr" target="#b21">(Huang et al., 2012)</ref>, <ref type="bibr">WiC (Pilehvar and Camacho-Collados, 2019)</ref>, and WSim <ref type="bibr" target="#b11">(Erk et al., 2013)</ref>, focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim <ref type="bibr" target="#b11">(Erk et al., 2013)</ref> focuses on separate sentential contexts only in the English language.</p><p>The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex <ref type="bibr" target="#b2">(Armendariz et al., 2020)</ref>, which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; specifically, each pair of words appears together in two different shared contexts (see Figure <ref type="figure" target="#fig_0">1</ref>). The task was designed to test the ability of participating systems to reflect human judgements of word meaning similarity in context, and crucially, the way in which this varies as context is changed. In addition, since CoSimLex takes the gradedness of human judgements into account, the task applies not only to polysemous words, or words with distinct senses, but to the phenomenon of context-dependency of word meaning in general. The dataset is also multi-lingual: besides English, it includes three less-resourced European languages, Croatian, Finnish, and Slovene.</p><p>Word1: man Word2: warrior SimLex: µ 4.72 σ 1.03 Context1 Context1: µ 7.88 σ 2.07 When Jaimal died in the war, Patta Sisodia took the command, but he too died in the battle. These young men displayed true Rajput chivalry. Akbar was so impressed with the bravery of these two warriors that he commissioned a statue of Jaimal and Patta riding on elephants at the gates of the Agra fort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context2</head><p>Context2: µ 3.27 σ 2.87 She has a dark past when her whole family was massacred, leaving her an orphan. By day, Shi Yeon is an employee at a natural history museum. By night, she's a top-ranking woman warrior in the Nine-Tailed Fox clan, charged with preserving the delicate balance between man and fox.</p><p>P-Value: 1.3 × 10 −6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our motivation lies in the cognitive and psychological mechanisms by which context affects our perception of word meaning. Here, we present two of the most prominent ideas that helped define the task and dataset, and explain why previous datasets for similarity in context are not well suited to test them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contextual Modulation</head><p>One debate in lexical semantics is whether the discreteness of lexical senses is fundamental or just a perception. <ref type="bibr" target="#b8">Cruse (1986)</ref> proposed a compromise, distinguishing two different manners in which sentential context modifies the meaning of a word. First, the context can select for different discrete senses; in this case, the word is described as ambiguous, and the process as contextual selection of senses (familiar from many word sense disambiguation tasks). Second, the context can modify meaning within the scope of a single sense by highlighting certain semantic traits and backgrounding others. This is described as contextual modulation of meaning, and the word as general with respect to the traits being modulated. This latter effect is not discrete, but continuous or graded; every word is general to some extent, and thus has a different meaning in every context in which it appears.</p><p>1. At this point, the bank was covered with brambles. 2. Sue is visiting her pregnant cousin. 3. Arthur poured the butter into a dish.</p><p>The main effect of the context in example ( <ref type="formula">1</ref>) is to select one of the discrete senses associated with the word bank. In contrast, in examples (2) and (3), the contexts modulate the meanings of the words cousin and butter: for cousin, promoting the "female" trait, and for butter, the "liquid" trait. This is possible because of the general quality of these words. Other traits could be promoted in different contexts: cousin includes male and female, but also tall, short, happy and sad cousins. Related traits can be promoted as a consequence of this modulation: we understand the butter as not only liquid, but warm. We expect this to affect similarity judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Salience Manipulation</head><p>In contrast to this purely linguistic view, we can take a cognitive perspective on language and meaning, seeing it as a more general expression of human cognition <ref type="bibr" target="#b12">(Evans and Green, 2018)</ref>. In this view, the units of interest are the conceptual structures associated with words or lexical units, rather than the words themselves. One approach is to see these in terms of conceptual spaces characterised by quality dimensions <ref type="bibr" target="#b15">(Gärdenfors, 2000;</ref><ref type="bibr" target="#b16">Gärdenfors, 2014)</ref>. These dimensions may be concrete (weight, temperature, brightness) or abstract (awkwardness, goodness), and concepts are defined as regions (usually convex) within the space. This space is not fixed: when we communicate we constantly re-negotiate the dimensions framing the conversation and their salience <ref type="bibr" target="#b39">(Warglien and Gärdenfors, 2015)</ref>. This salience manipulation changes their perceived importance. Priming effects are proposed as the main mechanism that facilitates this process <ref type="bibr" target="#b32">(Pickering and Garrod, 2004)</ref>. This type of semantic effect was first reported by <ref type="bibr" target="#b26">Meyer and Schvaneveldt (1971)</ref> when they found that their lexical decision task was responded to faster when the subjects were primed with words associated to the target words.</p><p>From this perspective, then, context affects meaning not via the presence of specific words, but via a change in the mental state of the hearer/reader. 1. My muffins were a failure, I should have used butter or margarine instead of olive oil. 2. Vegan chefs replace animal fats, like butter, with plant based ones like olive oil or margarine. 3. Vegan influencers believe the consumption of animal products is cruel and unnecessary.</p><p>In example (1), the context of baking increases the salience of dimensions related to physical properties of ingredients; butter and margarine (both solid) therefore seem more similar to each other than to olive oil (liquid). In contrast, example (2)'s context of veganism makes the animal vs. plant-based dimension very salient; margarine and olive oil now seem more similar to each other than to the animal-based butter.</p><p>The effects of salience manipulation and contextual modulation have important differences. The effect in example ( <ref type="formula">3</ref>) is introduced by the word poured and limited to the word butter, but the effect in example (1) seems more general: once a context triggers changes in the salience of conceptual dimensions, any word thereafter is affected. Our hypothesis is that the salience manipulation effect applies even when the target words are not present: a context like example (3) will impact later perceptions of similarity of butter, margarine and olive oil. We hope to test such predictions in later analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related Work</head><p>The Stanford Contextual Word Similarity (SCWS) dataset <ref type="bibr" target="#b21">(Huang et al., 2012)</ref>, and the similar USim dataset <ref type="bibr" target="#b11">(Erk et al., 2013)</ref> contain graded similarity judgements of pairs of words in the context of naturally occurring sentences (e.g., from Wikipedia with SCWS). However, the datasets were designed to evaluate a discrete multi-prototype model, so the focus was on contexts that select for discrete word senses, and each word in a pair was presented in its own distinct context. This prevents a systematic comparison of contextual effects on pairwise similarity. In addition, inter-rater agreement (IRA) on SCWS, measured as the Spearman correlation between different annotators, shows worryingly low scores. As Pilehvar and Camacho-Collados (2019) point out, the mean IRA between each annotator and the average of the rest, considered a human-level upper bound for model performance, is 0.52; while the performance of a simple context-independent model like word2vec <ref type="bibr" target="#b27">(Mikolov et al., 2013)</ref> is 0.65. Many scores also show a very large standard deviation, with annotators rating the same pair very differently. One possible reason may lie in the annotation design: the task itself does not directly enforce engagement with the context, and the target words were presented to annotators highlighted in boldface, making it easy to pick them out from the context without reading it. Some of these limitations were addressed by the more recent Words-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019). With a more direct and straightforward take on word sense disambiguation, each entry of the dataset is made of two lexicographer examples of the same word, and labelled as to whether the word sense in the two examples/contexts is the same or different. This forces engagement with the context; it also creates a task in which context-independent models like word2vec "would perform no better than a random baseline"; and inter-rater agreement scores are much more healthy. However, as the dataset focuses on discrete word senses, it cannot capture graded effects of context. These datasets are also available only in English. Multi-lingual similarity datasets exist: in SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity, Camacho-Collados et al.</p><p>(2017) used five different languages, and even used pairs in which each word was presented in a different language. A more recent Multi-SimLex dataset <ref type="bibr" target="#b38">(Vulić et al., 2020)</ref> comprises similarity ratings for 1,888 concept pairs aligned across 13 typologically diverse languages. However, the pairs in both datasets were annotated out of context, preventing analysis of contextual effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Our dataset is based on pairs of words from SimLex-999 <ref type="bibr" target="#b20">(Hill et al., 2015)</ref>. Each instance is a naturallyoccurring context, taken from Wikipedia, in which both words in the pair appear, labelled with a similarity score given by human annotators. For each pair, the dataset contains two different contexts (see Section 4 for more detail on dataset and choice of contexts). We proposed two different subtasks: first, to predict the change in similarity score between the two different contexts for each pair; second, to predict the similarity scores themselves. These are related but independent tasks that use the same input data, but each subtask had its own phases and leaderboards. Submissions for each subtask were independent and participants were able to use different models for each subtasks and each language. The tasks were unsupervised, and so no training data was released; However, we released a small practice kit which contained a practice dataset, a script to generate the baseline and evaluation scripts so participants could easily reproduce results, and understand how the dataset looked and how the task was evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subtask 1: Predicting Change</head><p>In the first subtask, participants were asked to predict the change in the similarity ratings of a pair of words when the human annotators are presented with the same word pair within two different contexts. This task directly addresses our main question. It evaluates how well systems are able to model the effect that context has in human perception of similarity. Theoretically a model could perform very well at modelling change without actually being able to accurately predict the ratings themselves. On the other hand, any context-independent model will predict no change and perform poorly in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subtask 2: Predicting Contextual Ratings</head><p>In the second subtask, participants were asked to predict the absolute similarity rating for each pair in each context. This is a more traditional task which evaluates systems' ability to model both similarity of words and the effect that context has on it. Good context-independent models could theoretically give reasonably competitive results in this task, however we still expect context-dependent models to have a considerable advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>CoSimLex <ref type="bibr" target="#b2">(Armendariz et al., 2020)</ref> is based on pairs of words from SimLex-999 <ref type="bibr" target="#b20">(Hill et al., 2015)</ref>; the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case. For Croatian and Finnish we use existing translations of SimLex-999 <ref type="bibr" target="#b30">(Mrkšić et al., 2017;</ref><ref type="bibr" target="#b37">Venekoski and Vankka, 2017;</ref><ref type="bibr" target="#b22">Kittask, 2019)</ref>. In the case of Slovene, we have produced our own new translation, 1 following <ref type="bibr" target="#b30">Mrkšić et al. (2017)</ref>'s methodology for Croatian.</p><p>The dataset consists of 340 pairs in English, 112 in Croatian, 111 in Slovene and 24 in Finnish. Each pair is rated within two different contexts, giving a total of 1174 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts; this task is even more challenging for languages with less resources, and as a result the selection of pairs is different for each language.</p><p>Each line of CoSimLex is made of a pair of words selected from SimLex-999; two different contexts extracted from Wikipedia in which these two words appear; two scores of similarity, each one related to one of the contexts, calculated as the mean of annotator ratings for that context; two scores of standard deviation; the p-value given by applying the Mann-Whitney U test to the two score distributions; and the four inflected forms of the words exactly as they appear in the contexts (including case; note that in the morphologically rich languages, many inflections are possible). To the best of our knowledge, this is the first reasonably sized dataset in which differences in contextual similarity between two words are supported with a test of statistical significance. Figure <ref type="figure" target="#fig_0">1</ref> shows an example from the English dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Context Selection</head><p>For each word pair we needed to find two suitable contexts. These contexts were extracted from each language's Wikipedia. They are made of three consecutive sentences and they needed to contain the pair of words, appearing only once each. English is by far the easiest language to work with, not only because of the amount and quality of the text contained in the English version of Wikipedia but because the other three languages are highly inflected (Croatian, Finnish and Slovene). To overcome this, we worked with data from <ref type="bibr" target="#b17">(Ginter et al., 2017)</ref> 2 which contains tokenised and lemmatised versions of Wikipedia for 45 languages.</p><p>The differences were expected to be small; to maximise the chance of finding contexts that produced different ratings of similarity, we used a dual process based on ELMo and BERT models. First, we used a model to rate the similarity between the target words within each of the candidate contexts; then selected the context in which it scored the pair as the most similar, and the context in which it scored them as most different. We repeated the process using both ELMo and BERT scores. This gave us 4 promising contexts. Then we added 4 randomly selected contexts for a total of 8 candidate contexts.</p><p>The final selection of two contexts was made by expert human annotators, one per language. Our experts were presented with 8 candidate contexts and asked to select the two that maximised the potential contrast in similarity. In the case of less-resourced languages, the smaller size and lower quality of the Wikipedia text resources required some extra steps to ensure the quality of the final annotation. A set of heuristic filters were used to try to remove badly constructed contexts. In addition we produce 16 candidates instead of 8 for the expert annotators to choose from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation</head><p>As starting point for our annotation methodology, we adapted the instructions used for SimLex-999. This way we benefited from its tested method of explaining how to focus on similarity rather than relatedness or association <ref type="bibr" target="#b20">(Hill et al., 2015)</ref>. As explained in their original paper, cup and mug are very similar, while coffee and cup are strongly related but not similar at all. For English we adopted their crowd-sourcing process: we used Amazon Mechanical Turk, with the same initial scoring scale (0 to 6), which is later transformed to a 0 to 10 scale. For the less-resourced languages, crowdsourcing is not a viable option due to lack of available speakers, and we recruited annotators directly. This means fewer annotators (for Croatian, Finnish and Slovene, 12 annotators vs 27 in English), however the average quality of annotation is higher and the data requires less post-processing.</p><p>In regards to the annotation process itself, our goal is to capture the kind of contextual phenomena discussed in Section 2: lexical meaning modulation and conceptual salience manipulation. In order to maximise our chances we defined three goals:</p><p>• Interaction with the context should be as natural as possible, so as to maximise priming effects and capture the potential change in the salience of conceptual dimensions. • Annotators should have the chance to account for lexical modulation within the sentence.</p><p>• The process should ensure that the annotators engage fully with the context. With these goals in mind we designed a two-step mixed annotation process. Our online survey interface is composed of two pages per pair of words and context (each annotator scores only one of the contexts). In the first page the annotators are presented with the context, and asked to read it and come up with two words "inspired by it". Once this is complete, the second page shown presents the context again, but with the target words now highlighted in bold; they are now asked to rate the similarity of target words within the sentence. Notice these target words are completely independent to the ones that were chosen as "inspired by the context" (see Apendix A for an example of the survey).</p><p>The second page is the main scoring task; it is designed to capture changes in scores of similarity due both to lexical modulation and -because we hope the annotators are still primed by their recent previous engagement with the context -the changes in the salience of conceptual dimensions. The separate task on the first page is intended to make annotators engage fully with the whole context, while maintaining a natural interaction with it to maximise any priming effects. One of the possible problems we identified in the previous SCWS annotation process is the fact that the words were always highlighted in bold, making it easy for annotators (Amazon Mechanical Turk workers) to just look at the pair of words in isolation and   to not read the rest of the contexts. Our initial task is designed to prevent this (the words are not in bold in the first page).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Post-Processing</head><p>Post-processing and cleaning the data is especially important when relying on crowd-sourcing platforms to source annotators. Reliability of annotation was ensured by an adapted version of SimLex-999's post-processing method, which includes rating calibration and the filtering of annotators with very low correlation to the rest, see the original paper for details <ref type="bibr" target="#b20">(Hill et al., 2015)</ref>. In addition, we were able to use responses to the first annotation question to check annotator engagement with the context. In English there were instances in which a block of annotations resulted in especially bad data. In those cases the only solution was repeating the annotation of the whole block. In our experience, obtaining good annotation using Amazon Mechanical Turk is not straightforward, but can be improved by a few strategies to attract good annotators. It is possible to engage with quality annotators and create private tasks for them inside the platform, which produces better data and allows higher payment for the worker. We encourage other researchers to use similar strategies when possible. This was not an issue with the rest of the languages, where annotators were sourced directly. After the post-processing steps the English dataset retained an average of 21 annotations per entry (from a starting point of 27) while the rest of the languages kept an average of 10 annotations (from the starting 12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Basic Analysis</head><p>The difficulty of finding contexts for the less-resourced languages restricted the selection of pairs available. As a consequence the overlap of pairs between different languages is smaller than originally intended (86 pairs appear in two languages, 12 in three and only 4 appear in all languages). However we were still able to replicate SimLex-999's proportions of nouns, verbs and adjectives (about two thirds nouns, two ninths verbs and one ninth adjectives). In English we checked other metrics, namely concreteness, standard deviation and out-of-context similarity. The first were kept in similar ranges to SimLex, however for out-of-context similarity we decided to lower the proportion of antonyms and low similarity score pairs, which as noted by <ref type="bibr" target="#b4">Camacho-Collados et al. (2017)</ref> were substantially overrepresented (see Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>We expected that the relative complexity of the annotation process and the increased confounding effects could affect inter-rater agreement; however, as we can see in Table <ref type="table" target="#tab_1">1</ref>, the different CoSimLex datasets show correlation scores very close to SimLex-999's IRA (ρ = 0.77 vs ρ = 0.78 in English). In the same table we can see the standard deviation is higher. Differences in the average similarity score are mainly due to the pair selection. After the post-processing and cleaning of the data both the crowdsourced and directly sourced annotation produced similar IRA and standard deviation. We wondered if the highly inflected nature of some of the languages might increase the contextual effects; but as can be seen in the table, the average change is very similar, even lower for Slovene and Finnish. However an interesting phenomenon seems to appear when we look at the distribution by part of speech; Chart (c) in Figure <ref type="figure" target="#fig_1">2</ref> suggest that verbs and adjectives in Croatian, Slovene and Finnish do see an increased effect of context compared with English ones. Importantly, the global percentage of statistically significant results is high (indeed, higher than we expected), with a global 62% of pairs showing statistically significant differences between contexts.</p><p>One potential confounding effect is the separation between words as presented in context (the number of intervening words between the target pair): it is possible this could affect annotators' perception of similarity. There is a very small negative correlation between similarity ratings and distance (Pearson r = -0.13). The source of this could be annotator bias, a linguistic effect or a combination of the two; but the effect seems small enough to ignore for current purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Metrics</head><p>The first subtask looked at the change in similarity between the two contexts, therefore it was important to preserve the difference between positive and negative values since it reflected in which of the two context the system believed the two words to be more or less similar. Consequently the most appropriate metric was Uncentered Pearson Correlation which looks at the deviation from zero instead of the mean.</p><formula xml:id="formula_0">CC uncentered = n i=1 (x i )(y i ) ( n i=1 x i ) 2 ( n i=1 y i ) 2</formula><p>For the second subtask, which looked at the more traditional absolute value of similarity in context, we followed <ref type="bibr" target="#b4">(Camacho-Collados et al., 2017)</ref> and used the harmonic mean of the Pearson and the Spearman correlations between the system's results and the average of the human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Baselines</head><p>Our task studies contextual effects in four different languages, which made Multiligual BERT the perfect candidate for our baseline. Released shortly after the original BERT model <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, it employs its same architecture while being trained in more than 100 different languages, our four languages between them. The original model introduced an innovative masking strategy that for the first time allowed for a bidirectional Transformer language model. BERT models are renowned for their ability to capture contextual effects, ability which is often blamed for an important part of their performance improvements. For the baseline of our task we used the uncased version of the model, and as a common strategy we used the contents of the last layer to form our embeddings. BERT creates sub-word tokens for the out of vocabulary words, in those cases our strategy was simply averaging the sub-word vectors to form a word embeddings.</p><p>Additionally, the results achieved by ELMo are added to Tables <ref type="table" target="#tab_3">2 and 3</ref> as a reference. This model precedes BERT and was one of the first to produce contextualised embeddings <ref type="bibr" target="#b31">(Peters et al., 2018)</ref>, in this case using a bidirectional LSTM. The original ELMo dataset was only trained in English, however we used ELMo models recently trained in Croatian, Slovene and Finnish <ref type="bibr" target="#b36">(Ulčar and Robnik-Šikonja, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Participants &amp; Results</head><p>The task received a total of 14 submissions for the first subtask and 15 submissions for the second. From those, 11 teams submitted system description papers for review. In order to be considered for the official rankings we asked participants to fill a form with some basic information about their systems.   The values are calculated as the harmonic mean of the Spearman and Pearson correlation between the system's scores and the average human annotation. It represents the system's ability to predict contextual human perception of similarity. Human performance is the average value when comparing each annotator against the average of the rest. JUSTMasters is not part of the official ranking since they were able to optimise their system with more than the competition's limit of 9 submissions.</p><p>neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables <ref type="table" target="#tab_3">2 and 3</ref>). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet <ref type="bibr" target="#b28">(Miller, 1995)</ref> as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in <ref type="bibr" target="#b24">(Loureiro and Jorge, 2019)</ref>, creating pretrained embeddings for each sense in WordNet, this time using XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2019)</ref> and SemCor augmented with their own UWA dataset <ref type="bibr" target="#b23">(Loureiro and Camacho-Collados, 2020)</ref>. This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH <ref type="bibr" target="#b35">(Tang, 2020)</ref> submitted (after the competition had ended) a system based on the original BERT sense embeddings created for <ref type="bibr" target="#b24">(Loureiro and Jorge, 2019)</ref> but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT <ref type="bibr" target="#b25">(Mahmoud and Torki, 2020)</ref> created new sense embeddings for the competition target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don't exist for most languages other than English.</p><p>Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores.</p><p>The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref>, Transformer-XL  and XLNet . Their final submission made use of stacked embeddings proposed by <ref type="bibr" target="#b0">Akbik et al. (2018)</ref>. They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team <ref type="bibr" target="#b29">(Morishita et al., 2020)</ref> looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, especially in the English Subtask 2. However, in order to optimise their system, they made many more submissions than allowed in the competition; we therefore leave them out of the official ranking.</p><p>With a more multilingual approach, BabelEncoding (Costella Pessutto et al., 2020) proposed a solution in which they translated the contexts and target words to many languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings. Their idea is that the translation not only brings new resources but the process itself can produce useful information, for example to disambiguate. The approach works very well for the less resourced languages, being clearly the best system in that category, in both Subtask 1 and 2. Their system won Subtask 1 and 2 for Croatian (by a healthy margin) and 2 for Slovene, ending third in the Slovene Subtask 1 and third and second in the two Finnish ones.</p><p>The MultiSem team (Soler and Apidianaki, 2020) collected 5 different datasets in order to fine-tune their BERT models, most of them automatically generated from previous datasets to increase contextual influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categories, and significantly improving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively.</p><p>Ferryman's focus <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene.</p><p>The starting point of CitiusNLP <ref type="bibr" target="#b14">(Gamallo, 2020)</ref> was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn't seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT's and their actual submissions are based on a standard BERT model.</p><p>Finally, the Will_Go team <ref type="bibr" target="#b3">(Bao et al., 2020)</ref> looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not described in their paper. The combination works well, they achieved a second place in the English Subtask 1 and won the Finnish Subtask 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We resented the SemEval-2020 Task on Graded Word Similarity in Context and introduced our new dataset CoSimLex. We provided the motivation behind their design choices and described the annotation process. The task received a good number of submissions and system description papers (15 and 11 respectively). We hope both the task and the dataset will be useful for researchers looking into how state-of-the-art systems capture context, and help promote the use of psychologically and cognitively inspired ideas in our field. Some of the interesting highlights were good performance of WordNet-based sense embeddings, the improvements achieved in less-resourced languages by simply translating the input, how the explicit feeding of an "old-fashioned" feature like TF-IDF improved a very modern system's performance, and the power of well designed, automatically created datasets for fine-tuning.</p><p>Additional and more detailed analyses of the dataset and task results will follow as part of future work. Areas to be investigated include the impact of different similarity ranges and degrees of polysemy, and more detailed qualitative analysis of the differences in annotation and between systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Finnish</head><p>Word1: rikos Word2: varkaus SimLex (English): µ 7.53 σ 1.32 Context1 Context1: µ 4.33 σ 2.38 Valistuksen vaikutuksesta häpeärangaistuksista vähitellen luovuttiin. Esimodernissa Euroopassa häpeärangaistuksiin johtivat etupäässä pienehköt rikokset, kuten solvaukset ja häiritsevä juopumus, mutta myös esimerkiksi aviorikos ja varkaus. Häpeärangaistuksien toteuttamistavat vaihtelivat alueellisesti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context2</head><p>Context2: µ 0 σ 0 Tekoja voidaan siis pitää pääosin laittomina, koska tuolloin ei ollut käytettävissä kuolemanrangaistuksen sallivaa, asianmukaista lainsäädäntöä. Sisällissodan jälkeen laaditulla armahduslailla vapautettiin myös valkoisen osapuolen edustajat vastuusta mahdollisesti tekemistään rikoksista, joten jonkinlainen ymmärrys teloitusten laittomuudesta oli ollut olemassa jo tuolloin. Kuolemantuomioiden langettamista jatkoi Varkauden kenttäoikeus, jonka lainmukaisuudesta voidaan olla myös hyvin erimielisiä.</p><p>P-Value: 3.3 × 10 −5</p><p>Figure <ref type="figure">7</ref>: Example from the Finnish dataset, showing a word pair with two contexts, each with mean and standard deviation of human similarity judgements. The P-Value shown is the result of a Mann-Whitney U test. This is a very particular example, while "rikos" translates as "crime" and "varkaus" as "theft", there is a town named "Varkaus", which is the meaning of the word in the second context. This is the reason why all the annotators, accurately scored the similarity of the two words as 0 in the second context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example from the English dataset, showing a word pair with two contexts, each with mean and standard deviation of human similarity judgements. The original SimLex values for the same word pair without context are shown for comparison. The P-Value shown is the result of a Mann-Whitney U test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) (b): Differences in the distribution of similarity between SimLex-999 and the English CoSimLex; (c): Change in the scoring of similarity between contexts categorized by language and part of speech</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example from the Slovene dataset, showing a word pair with two contexts, each with mean and standard deviation of human similarity judgements. The P-Value shown is the result of a Mann-Whitney U test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Similarity, standard deviation, Spearman's ρ and change are average values. The two rightmost columns denote the proportion of pairs whose differences of scores with the original values are statistically significant at p-value &lt; 0.1 and p-value &lt; 0.05.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Subtask 1 Final Ranking: The values are calculated as the Pearson Uncentered Correlation between the system's scores and the average human annotation. It represents the system's ability to predict the change in perception produced by the contexts.</figDesc><table><row><cell cols="7">Since different annotators looked at each context, human performance couldn't be calculated for this subtask. JUSTMasters and</cell></row><row><cell cols="7">UZH are not part of the official ranking since they were able to optimise their systems with more than the competition's limit of</cell></row><row><cell>9 submissions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SUBTASK 2</cell><cell></cell></row><row><cell>English</cell><cell></cell><cell>Croatian</cell><cell></cell><cell>Slovene</cell><cell></cell><cell>Finnish</cell></row><row><cell>MineriaUNAM</cell><cell cols="6">0.723 BabelEnconding 0.658 BabelEnconding 0.579 BRUMS</cell><cell>0.645</cell></row><row><cell>LMMS</cell><cell>0.72</cell><cell>Hitachi</cell><cell cols="2">0.616 BRUMS</cell><cell cols="2">0.573 BabelEnconding 0.611</cell></row><row><cell cols="3">AlexU-Aux-Bert 0.719 MineriaUNAM</cell><cell cols="2">0.613 CiTIUS-NLP</cell><cell cols="2">0.538 MineriaUNAM</cell><cell>0.597</cell></row><row><cell>MultiSem</cell><cell cols="2">0.718 LMMS</cell><cell cols="2">0.565 will_go</cell><cell cols="2">0.516 MultiSem</cell><cell>0.492</cell></row><row><cell>BRUMS</cell><cell cols="2">0.715 BRUMS</cell><cell cols="4">0.545 AlexU-Aux-Bert 0.516 Ferryman</cell><cell>0.357</cell></row><row><cell>will_go</cell><cell cols="2">0.695 CiTIUS-NLP</cell><cell cols="2">0.496 Hitachi</cell><cell cols="2">0.514 LMMS</cell><cell>0.354</cell></row><row><cell>Hitachi</cell><cell cols="4">0.695 AlexU-Aux-Bert 0.402 MineriaUNAM</cell><cell cols="2">0.487 will_go</cell><cell>0.35</cell></row><row><cell>CiTIUS-NLP</cell><cell cols="2">0.687 will_go</cell><cell cols="2">0.402 LMMS</cell><cell cols="2">0.483 Hitachi</cell><cell>0.335</cell></row><row><cell cols="3">BabelEnconding 0.634 Ferryman</cell><cell cols="2">0.397 Ferryman</cell><cell cols="2">0.345 CiTIUS-NLP</cell><cell>0.289</cell></row><row><cell>Ferryman</cell><cell cols="2">0.437 MultiSem</cell><cell>-</cell><cell>MultiSem</cell><cell>-</cell><cell>AlexU-Aux-Bert 0.289</cell></row><row><cell>JUSTMasters</cell><cell>0.725</cell><cell></cell><cell>0.443</cell><cell></cell><cell>0.44</cell><cell>0.68</cell></row><row><cell cols="2">mBERT_uncased 0.573</cell><cell></cell><cell>0.402</cell><cell></cell><cell>0.516</cell><cell>0.289</cell></row><row><cell>ELMo</cell><cell>0.510</cell><cell></cell><cell>0.529</cell><cell></cell><cell>0.407</cell><cell>0.516</cell></row><row><cell>Human</cell><cell>0.77</cell><cell></cell><cell>0.76</cell><cell></cell><cell>0.77</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Subtask 2 Final Ranking:</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available from http://hdl.handle.net/11356/1309</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Available from http://hdl.handle.net/11234/1-1989</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No 825153, project EMBEDDIA (Cross-Lingual Embeddings for Less-Represented Languages in European News Media). The results of this publication reflect only the authors' views and the Commission is not responsible for any use that may be made of the information it contains. Carlos S. Armendariz is also supported by the EPSRC and AHRC Centre for Doctoral Training in Media and Arts Technology (EP/L01632X/1); Matthew Purver is also supported by the EPSRC project Streamlining Social Decision Making for Improved Internet Standards (SoDe-Stream, EP/S033564/1). The work of Ivan Vulić is supported by the ERC Consolidator Grant LEXICAL (no. 648909).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix: Survey Example  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">JUSTMasters at SemEval-2020 Task 3: Multilingual deep learning model to predict the effect of context in word similarity</title>
		<author>
			<persName><forename type="first">Nour</forename><surname>Al-Khdour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutaz</forename><surname>Bni Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malak</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al-Smadi</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoSimLex: A resource for evaluating graded word similarity in context</title>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">S</forename><surname>Armendariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Ulčar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senja</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Granroth-Wilding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
				<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="5878" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Will_Go at SemEval-2020 Task 3: An accurate model for predicting the (graded) effect of context in word similarity based on BERT</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Hongshu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2017 Task 2: Multilingual and cross-lingual semantic word similarity</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ferryman at SemEval-2020 Task: BERT with TFIDF-weighting for predicting the effect of context in word similarity</title>
		<author>
			<persName><forename type="first">Weilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale. ArXiv, abs</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BabelEncoding at SemEval-2020 Task 3: Contextual similarity as a combination of multilingualism and language models</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costella</forename><surname>Pessutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viviane</surname></persName>
		</author>
		<author>
			<persName><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Altigran</forename><surname>Tiago De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename><surname>Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lexical semantics</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Cruse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring word meaning in context</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gaylord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="554" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cognitive Linguistics: An Introduction</title>
		<author>
			<persName><forename type="first">Vyvyan</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on information systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CitiusNLP at SemEval-2020 Task 3: Comparing two approaches for word vector contextualization</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gamallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conceptual Spaces: The Geometry of Thought</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gärdenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Geometry of Meaning: Semantics Based on Conceptual Spaces</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gärdenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<title level="m">Shared Taskautomatically annotated raw texts and word embeddings. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MineriaUNAM at SemEval-2020 Task 3: Predicting contextual word similarity using a centroid based approach and word embeddings</title>
		<author>
			<persName><forename type="first">Helena</forename><surname>Gomez-Adorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Bel-Enguix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Reyes-Magaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vargas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BRUMS at SemEval-2020 Task 3: Contextualised embeddings for predicting the (graded) effect of context in word similarity</title>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tharindu</forename><surname>Ranasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computational Models of Concept Similarity for the Estonian Language. Bachelor&apos;s thesis</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Kittask</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Tartu</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Don&apos;t neglect the obvious: On the role of unambiguous words in word sense disambiguation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language modelling makes sense: Propagating representations through WordNet for full-coverage word sense disambiguation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alípio</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5682" to="5691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AlexU-AUX-BERT at SemEval-2020 Task 3: Improving BERT contextual similarity using multiple auxiliary contexts</title>
		<author>
			<persName><forename type="first">Somaia</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwan</forename><surname>Torki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facilitation in recognizing pairs of words: evidence of a dependence between retrieval operations</title>
		<author>
			<persName><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">W</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Schvaneveldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">227</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
				<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hitachi at SemEval-2020 Task 3: Exploring the representation spaces of transformers for human sense word similarity</title>
		<author>
			<persName><forename type="first">Terufumi</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaku</forename><surname>Morio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshinori</forename><surname>Miyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic specialization of distributional word vector spaces using monolingual and crosslingual constraints</title>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward a mechanistic psychology of dialogue</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName><surname>Garrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="190" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">WiC: The word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1267" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MultiSem at SemEval-2020 Task 3: Fine-tuning BERT for lexical meaning</title>
		<author>
			<persName><forename type="first">Gari</forename><surname>Aina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><surname>Apidianaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UZH at SemEval-2020 Task 3: Combining BERT with WordNet sense embeddings to predict graded word similarity changes</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High quality ELMo embeddings for seven less-resourced languages</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Ulčar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
				<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="4731" to="4738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finnish resources for evaluating language model semantics</title>
		<author>
			<persName><forename type="first">Viljami</forename><surname>Venekoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouko</forename><surname>Vankka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa</title>
				<meeting>the 21st Nordic Conference on Computational Linguistics, NoDaLiDa<address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Linköping University Electronic Press</publisher>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
	<note type="report_type">Gothenburg</note>
	<note>Linköping Electronic Conference Proceedings. Linköpings universitet</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-SimLex: A large-scale evaluation of multilingual and cross-lingual lexical semantic similarity</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulla</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Petti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eden</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04866</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meaning negotiation</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Warglien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gärdenfors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of conceptual spaces</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="79" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">XL-Net: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">1 Croatian Word1: nov Word2: svjež SimLex (English): µ 6</title>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fridrik je zamolio svježe trupe iz Njemačke. Prije svega Henrik Lav kao najmoćniji knez i vladar Bavarske odbio je caru poslati nove vojnike uvjetujući to prepuštanjem Goslara s bogatim rudnicima srebra</title>
	</analytic>
	<monogr>
		<title level="m">U jesen 1175</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Proučavanje upalnih promjena dokazao je da ulaženje bijelih krvnih tjelešaca u tkivo uzrokuje gnojenje. Po njegovoj teoriji, rak nastaje iz emrionalnih stanica, razbacanih po organizmu. Uveo je nove metode istraživanja, npr. smrzavanje svježeg tkiva i pravljenje mirkoskopskih rezova</title>
		<imprint/>
	</monogr>
	<note>P-Value: 2.4 × 10 −5</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Example from the Croatian dataset, showing a word pair with two contexts, each with mean and standard deviation of human similarity judgements. The P-Value shown is the result of a Mann-Whitney U test</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">Slovene Word1: zgodba Word2: tema SimLex (English): µ 5 σ</title>
				<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">V zgodbiČajanka za psa mačka in papagaja, se cunjasta dvojčica Nina sooča s strahom. Ker je še majhna deklica se boji teme, toda na pomoč ji prihiti punčka in škratje Copatki, ki Nini predlagajo naj se poveselijo in priredijočajanko. Skupaj s papagajem</title>
		<imprint/>
	</monogr>
	<note>psom in mačkom priredijočajanko in pozabijo na strah</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cyrus sodeloval že pri plesu za pesem »Hoedown Throwdown«. Miley Cyrus in Jamal Sims sta skupaj sestavila koreografijo, ki bi se ujemala z zgodbo v pesmi, in nazadnje vse skupaj predstavila Robertu Halsu, ki si je »takoj zamislil, kako bo vse skupaj izgledalo«. V zvezi s temo videospota je Miley Cyrus povedala: »Mislim, da videospot razloži, da moje življenje ne izključuje življenj drugih ljudi</title>
		<author>
			<persName><surname>Koreografijo</surname></persName>
		</author>
		<author>
			<persName><surname>Sims</surname></persName>
		</author>
		<author>
			<persName><surname>Miley</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>P-Value: 5.1 × 10 −5</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
