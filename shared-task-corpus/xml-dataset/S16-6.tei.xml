<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 6: Detecting Stance in Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
							<email>saif.mohammad@nrc-cnrc.gc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
							<email>svetlana.kiritchenko@nrc-cnrc.gc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<email>xiaodan.zhu@nrc-cnrc.gc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
							<email>colin.cherry@nrc-cnrc.gc.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 6: Detecting Stance in Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Here for the first time we present a shared task on detecting stance from tweets: given a tweet and a target entity (person, organization, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely. The target of interest may or may not be referred to in the tweet, and it may or may not be the target of opinion. Two tasks are proposed. Task A is a traditional supervised classification task where 70% of the annotated data for a target is used as training and the rest for testing. For Task B, we use as test data all of the instances for a new target (not used in task A) and no training data is provided. Our shared task received submissions from 19 teams for Task A and from 9 teams for Task B. The highest classification F-score obtained was 67.82 for Task A and 56.28 for Task B. However, systems found it markedly more difficult to infer stance towards the target of interest from tweets that express opinion towards another entity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target. The target may be a person, an organization, a government policy, a movement, a product, etc. For example, one can infer from Barack Obama's speeches that he is in favor of stricter gun laws in the US. Similarly, people often express stance towards various target entities through posts on online forums, blogs, Twitter, Youtube, Instagram, etc. Automatically detecting stance has widespread applications in information retrieval, text summarization, and textual entailment.</p><p>The task we explore is formulated as follows: given a tweet text and a target entity <ref type="bibr">(person, organization, movement, policy, etc.)</ref>, automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely. For example, consider the target-tweet pair:</p><p>Target: legalization of abortion</p><p>(1) Tweet: The pregnant are more than walking incubators, and have rights!</p><p>We can deduce from the tweet that the tweeter is likely in favor of the target. <ref type="bibr">1</ref> We annotated 4870 English tweets for stance towards six commonly known targets in the United States. The data corresponding to five of the targets ('Atheism', 'Climate Change is a Real Concern', 'Feminist Movement', 'Hillary Clinton', and 'Legalization of Abortion') was used in a standard supervised stance detection task -Task A. About 70% of the tweets per target were used for training and the remaining for testing. All of the data corresponding to the target 'Donald Trump' was used as test set in a separate task -Task B. No training data labeled with stance towards 'Donald Trump' was provided. However, participants were free to use data from Task A to develop their models for Task B.</p><p>Task A received submissions from 19 teams, wherein the highest classification F-score obtained was 67.82. Task B, which is particularly challenging due to lack of training data, received submissions from 9 teams wherein the highest classification F-score obtained was 56.28. The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons. Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, for Task A, none of these systems surpassed a baseline SVM classifier that uses word and character n-grams as features <ref type="bibr" target="#b16">(Mohammad et al., 2016b)</ref>. Further, results are markedly worse for instances where the target of interest is not the target of opinion.</p><p>More gains can be expected in the future on both tasks, as researchers better understand this new task and data. All of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Subtleties of Stance Detection</head><p>In the sub-sections below we discuss some of the nuances of stance detection, including a discussion on neutral stance and the relationship between stance and sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neutral Stance</head><p>The classification task formulated here does not include an explicit neutral class. The lack of evidence for 'favor' or 'against' does not imply that the tweeter is neutral towards the target. It may just be that one cannot deduce stance from the tweet. In fact, this is fairly common. On the other hand, the number of tweets from which we can infer neutral stance is expected to be small. An example is shown below:</p><p>Target: Hillary Clinton</p><p>(2) Tweet: Hillary Clinton has some strengths and some weaknesses.</p><p>Thus, even though we obtain annotations for neutral stance, we eventually merge all classes other than 'favor' and 'against' into one 'neither' class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stance and Sentiment</head><p>Stance detection is related to, but different from, sentiment analysis. Sentiment analysis tasks are usually formulated as: determining whether a piece of text is positive, negative, or neutral, OR determining from text the speaker's opinion and the target of the opinion (the entity towards which opinion is expressed). However, in stance detection, systems are to determine favorability towards a given (prechosen) target of interest. The target of interest may not be explicitly mentioned in the text and it may not be the target of opinion in the text. For example, consider the target-tweet pair below:</p><p>Target: Donald Trump (3) Tweet: Jeb Bush is the only sane candidate in this republican lineup.</p><p>The target of opinion in the tweet is Jeb Bush, but the given target of interest is Donald Trump. Nonetheless, we can infer that the tweeter is likely to be unfavorable towards Donald Trump. Also note that in stance detection, the target can be expressed in different ways which impacts whether the instance is labeled favour or against. For example, the target in example 1 could have been phrased as 'pro-life movement', in which case the correct label for that instance is 'against'. Also, the same stance (favour or against) towards a given target can be deduced from positive tweets and negative tweets. See <ref type="bibr" target="#b16">Mohammad et al. (2016b)</ref> for a quantitative exploration of this interaction between stance and sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Dataset for Stance from Tweets</head><p>The stance annotations we use are described in detail in <ref type="bibr" target="#b15">Mohammad et al. (2016a)</ref>. The same dataset was subsequently also annotated for target of opinion and sentiment (in addition to stance towards a given target) <ref type="bibr" target="#b16">(Mohammad et al., 2016b)</ref>. These additional annotations are not part of the SemEval-2016 competition, but are made available for future research. We summarize below all relevant details for this shared task: how we compiled a set of tweets and targets for stance annotation (Section 3.1), the questionnaire and crowdsourcing setup used for stance annotation (Section 3.2), and an analysis of the stance annotations (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selecting the Tweet-Target Pairs</head><p>We wanted to create a dataset of stance-labeled tweet-target pairs with the following properties:</p><p>1: The tweet and target are commonly understood by a wide number of people in the US. (The data was also eventually annotated for stance by respondents living in the US.)</p><p>2: There must be a significant amount of data for the three classes: favor, against, and neither.</p><p>3: Apart from tweets that explicitly mention the target, the dataset should include a significant number of tweets that express opinion towards the target without referring to it by name.</p><p>4: Apart from tweets that express opinion towards the target, the dataset should include a significant number of tweets in which the target of opinion is different from the given target of interest. Downstream applications often require stance towards particular pre-chosen targets of interest (for example, a company might be interested in stance towards its product). Having data where the target of opinion is some other entity (for example, a competitor's product) helps test how well stance detection systems can cope with such instances.</p><p>To help with Property 1, the authors of this paper compiled a list of target entities commonly known in the United States. (See Table <ref type="table" target="#tab_1">1</ref> for the list.)</p><p>We created a small list of hashtags, which we will call query hashtags, that people use when tweeting about the targets. We split these hashtags into three categories: (1) favor hashtags: expected to occur in tweets expressing favorable stance towards the target (for example, #Hillary4President), (2) against hashtags: expected to occur in tweets expressing opposition to the target (for example, #HillNo), and (3) stance-ambiguous hashtags: expected to occur in tweets about the target, but are not explicitly indicative of stance (for example, #Hillary2016). Next, we polled the Twitter API to collect over two million tweets containing these query hashtags. We discarded retweets and tweets with URLs. We kept only those tweets where the query hashtags appeared at the end. We removed the query hashtags from the tweets to exclude obvious cues for the classification task. Since we only select tweets that have the query hashtag at the end, removing them from the tweet often still results in text that is understandable and grammatical.</p><p>Note that the presence of a stance-indicative hashtag is not a guarantee that the tweet will have the same stance. 3 Further, removal of query hashtags may result in a tweet that no longer expresses the same stance as with the query hashtag. Thus we manually annotate the tweet-target pairs after the pre-processing described above. For each target, we sampled an equal number of tweets pertaining to the favor hashtags, the against hashtags, and the stanceambiguous hashtags-up to 1000 tweets at most per target. This helps in obtaining a sufficient number of tweets pertaining to each of the stance categories (Property 2). Properties 3 and 4 are addressed to some extent by the fact that removing the query hashtag can sometimes result in tweets that do not explicitly mention the target. Consider: Target: Hillary Clinton (4) Tweet: Benghazi must be answered for #Jeb16</p><p>The query hashtags '#HillNo' was removed from the original tweet, leaving no mention of Hillary Clinton. Yet there is sufficient evidence (through references to Benghazi and #Jeb16) that the tweeter is likely against Hillary Clinton. Further, conceptual targets such as 'legalization of abortion' (much more so than person-name targets) have many instances where the target is not explicitly mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stance Annotation</head><p>The core instructions given to annotators for determining stance are shown below. 4 Additional descriptions within each option (not shown here) make clear that stance can be expressed in many different ways, for example by explicitly supporting or opposing the target, by supporting an entity aligned with or opposed to the target, by re-tweeting somebody else's tweet, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target of Interest: [target entity]</head><p>Tweet: [tweet with query hashtag removed] Q: From reading the tweet, which of the options below is most likely to be true about the tweeter's stance or outlook towards the target:</p><p>1. We can infer from the tweet that the tweeter supports the target 2. We can infer from the tweet that the tweeter is against the target 3. We can infer from the tweet that the tweeter has a neutral stance towards the target 4. There is no clue in the tweet to reveal the stance of the tweeter towards the target (support/against/neutral) Each of the tweet-target pairs selected for annotation was uploaded on CrowdFlower for annotation with the questionnaire shown above. <ref type="bibr">5</ref> Each instance was annotated by at least eight respondents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Stance Annotations</head><p>The number of instances that were marked as neutral stance (option 3) was less than 1%. Thus we merged options 3 and 4 into one 'neither in favor nor against' option ('neither' for short). The interannotator agreement was 73.1%. These statistics are for the complete annotated dataset, which include instances that were genuinely difficult to annotate for stance (possibly because the tweets were too ungrammatical or vague) and/or instances that received poor annotations from the crowd workers (possibly because the particular annotator did not understand the tweet or its context). We selected instances with agreement equal to or greater than 60% (at least 5 out of 8 annotators must agree) to create the test and training sets for this task. <ref type="bibr">6</ref> We will refer to this dataset as the Stance Dataset. The inter-annotator agreement on this set is 81.85%. The rest of the instances are kept aside for future investigation. We partitioned the Stance Dataset into training and test sets based on the timestamps of the tweets. All annotated tweets were ordered by their timestamps, and the first 70% of the tweets formed the training set and the last 30% formed the test set. Table <ref type="table" target="#tab_1">1</ref> shows the number and distribution of instances in the Stance Dataset. Inspection of the data revealed that often the target is not directly mentioned, and yet stance towards the target was determined by the annotators. About 30% of the 'Hillary Clinton' instances and 65% of the 'Legalization of Abortion' instances were found to be of this kind-they did not mention 'Hillary' or 'Clinton' and did not mention 'abortion', 'pro-life', and 'pro-choice', respectively (case insensitive; with or without hashtag; with or without hyphen). Examples ( <ref type="formula">1</ref>) and ( <ref type="formula">4</ref>) shown earlier are instances of this, and are taken from our dataset. An interactive visualization of the Stance Dataset that shows various statistics about the data is available at the task website. Note that it also shows sentiment and target of opinion annotations (in addition to stance). Clicking on various visualization elements filters the data. For example, clicking on 'Feminism' and 'Favor' will show information pertaining to only those tweets that express favor towards feminism. One can also use the check boxes on the left to view only test or training data, or data on particular targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Task Setup: Automatic Stance Classification</head><p>The Stance Dataset was partitioned so as to be used in two tasks described in the subsections below: Task A (supervised framework) and Task B (weakly supervised framework). Participants could provide submissions for either one of the tasks, or both tasks. Both tasks required classification of tweettarget pairs into exactly one of three classes:</p><p>• Favor: We can infer from the tweet that the tweeter supports the target (e.g., directly or indirectly by supporting someone/something, by opposing or criticizing someone/something opposed to the target, or by echoing the stance of somebody else).</p><p>• Against: We can infer from the tweet that the tweeter is against the target (e.g., directly or indirectly by opposing or criticizing someone/something, by supporting someone/something opposed to the target, or by echoing the stance of somebody else).</p><p>• Neither: none of the above.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task B: Weakly Supervised Framework</head><p>This task tested stance towards one target 'Donald Trump' in 707 tweets. Participants were not provided with any training data for this target. They were given about 78,000 tweets associated with 'Donald Trump' to various degrees -the domain corpus, but these tweets were not labeled for stance. These tweets were gathered by polling Twitter for hashtags associated with Donald Trump.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Common Evaluation Metric for Both Task A and Task B</head><p>We used the macro-average of the F1-score for 'favor' and the F1-score for 'against' as the bottom-line evaluation metric.</p><formula xml:id="formula_0">F avg = F f avor + F against 2 (1)</formula><p>where F f avor and F against are calculated as shown below:</p><formula xml:id="formula_1">F f avor = 2P f avor R f avor P f avor +R f avor (2) F against = 2P against R against P against +R against (3)</formula><p>Note that the evaluation measure does not disregard the 'neither' class. By taking the average F-score for only the 'favor' and 'against' classes, we treat 'neither' as a class that is not of interest-or 'negative' class in Information Retrieval (IR) terms. Falsely labeling negative class instances still adversely affects the scores of this metric. If one uses simple accuracy as the evaluation metric, and if the negative class is very dominant (as is the case in IR), then simply labeling every instance with the negative class will obtain very high scores.</p><p>If one randomly accesses tweets, then the probability that one can infer 'favor' or 'against' stance towards a pre-chosen target of interest is small. This has motivated the IR-like metric used in this competition, even though we worked hard to have marked amounts of 'favor' and 'against' data in our training and test sets. This metric is also similar to how sentiment prediction was evaluated in recent SemEval competitions.</p><p>This evaluation metric can be seen as a microaverage of F-scores across targets (F-microT). Alternatively, one could determine the mean of the F avg scores for each of the targets-the macro average across targets (F-macroT). Even though not the official competition metric, the F-macroT can easily be determined from the per-target F avg scores shown in the result tables of Section 5.</p><p>The participants were provided with an evaluation script so that they could check the format of their submission and determine performance when gold labels were available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Systems and Results for Task A</head><p>We now discuss various baseline systems and the official submissions to Task A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task A Baselines</head><p>Table <ref type="table" target="#tab_3">2</ref> presents the results obtained with several baseline classifiers first presented in <ref type="bibr" target="#b16">(Mohammad et al., 2016b)</ref>. Since the baseline system was developed by some of the organizers of this task, it was  not entered as part of the official competition. Baselines:</p><p>1. Majority class: a classifier that simply labels every instance with the majority class ('favor' or 'against') for the corresponding target; The SVM parameters were tuned using 5-fold crossvalidation on the training data. The first three columns of the table show the official competition metric (Overall F avg ) along with the two components that are averaged to obtain it (F f avor and F against ). The next five columns describe per-target results-the official metric as calculated over each of the targets individually.</p><p>Observe that the Overall F avg for the Majority class baseline is very high. This is mostly due to the differences in the class distributions for the five targets: for most of the targets the majority of the instances are labeled as 'against' whereas for target 'Climate Change is a Real Concern' most of the data are labeled as 'favor'. Therefore, the F-scores for the classes 'favor' and 'against' are more balanced over all targets than for just one target.</p><p>We can see that a supervised classifier using unigram features alone produces results markedly  above the majority baseline for most of the targets. Furthermore, employing higher-order n-gram features results in substantial improvements for all targets as well as for the Overall F avg . Training separate classifiers for each target seems a better solution than training a single classifier for all targets even though the combined classifier has access to significantly more data. As expected, the words and concepts used in tweets corresponding to the stance categories do not generalize well across the targets. However, there is one exception: the results for 'Climate Change' improve by over 5% when the combined classifier has access to the training data for other targets. This is probably because it has access to more balanced dataset and more representative instances for 'against' class. Most teams chose to train separate classifiers for different targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task A Participating Stance Systems</head><p>Nineteen teams competed in Task A on supervised stance detection. Table <ref type="table" target="#tab_3">2</ref> shows each team's performance, both in aggregate and in terms of individual targets. Teams are sorted in terms of their performance according to the official metric. The best results obtained by a participating system was an Overall F avg of 67.82 by MITRE. Their approach employed two recurrent neural network (RNN) classifiers: the first was trained to predict task-relevant hashtags on a very large unlabeled Twitter corpus. This network was used to initialize a second RNN classifier, which was trained with the provided Task A data. However, this result is not higher than the SVM-ngrams baseline.</p><p>In general, per-target results are lower than the Overall F avg . This is likely due to the fact that it is easier to balance 'favor' and 'against' classes over all targets than it is for exactly one target. That is, when dealing with all targets, one can use the natural abundance of tweets in favor of concern over climate change to balance against the fact that many of the other targets have a high proportion of tweets against them. Most systems were optimized for the competition metric, which allows cross-target balancing, and thus would naturally perform worse on per-target metrics. IDI@NTNU is an interesting exception, as their submission focused on the 'Climate Change' target, and they did succeed in producing the best result for that target.</p><p>We also calculated Task A results on two subsets of the test set: (1) a subset where opinion is expressed towards the target, (2) a subset where opinion is expressed towards some other entity. Table <ref type="table" target="#tab_5">3</ref> shows these results. It also shows results on the complete test set (All), for easy reference. Observe that the stance task is markedly more difficult when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). This is not surprising because it is a more challenging task, and because there has been very little work on this in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Most teams used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features such as those drawn from sentiment lexicons <ref type="bibr" target="#b10">(Kiritchenko et al., 2014b)</ref>. Some teams polled Twitter for stancebearing hashtags, creating additional noisy stance data. Three teams tried variants of this strategy: MITRE, DeepStance and nldsucsc. These teams are distributed somewhat evenly throughout the standings, and although MITRE did use extra data in its top-placing entry, pkudblab achieved nearly the same score with only the provided data.</p><p>Another possible differentiator would be the use of continuous word representations, derived either from extremely large sources such as Google News, directly from Twitter corpora, or as a by-product of training a neural network classifier. Nine of the nineteen entries used some form of word embedding, including the top three entries, but PKULCWM's fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons.</p><p>Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), <ref type="bibr">Hu and Liu Lexicon (Hu and Liu, 2004)</ref>, MPQA Subjectivity Lexicon <ref type="bibr" target="#b29">(Wilson et al., 2005)</ref>, and NRC Hashtag Lexicons <ref type="bibr" target="#b10">(Kiritchenko et al., 2014b)</ref>.</p><p>Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Systems and Results for Task B</head><p>The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target 'Donald Trump', and no training data for this target was provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task B Baselines</head><p>We calculated two baselines listed below:</p><p>1. Majority class: a classifier that simply labels every instance with the majority class ('favor' or 'against') for the corresponding target;  2. SVM-ngrams-comb: one SVM classifier trained on the combined (all 5 targets) Task A training set, using word n-grams (1-, 2-, and 3-gram) and character n-grams (2-, 3-, 4-, and 5-gram) features.</p><p>The results are presented in Table <ref type="table" target="#tab_7">4</ref>. Note that the class distribution for the target 'Donald Trump' is more balanced. Therefore, the F avg for the Majority baseline for this target is much lower than the corresponding values for other targets. Yet, the combined classifier trained on other targets could not beat the Majority baseline on this test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Task B Participating Stance Systems</head><p>Nine teams competed in Task B. Table <ref type="table" target="#tab_7">4</ref> shows each team's performance. Teams are sorted in terms of their performance according to the official metric. The best results obtained by a participating system was an F avg of 56.28 by pkudblab. They used a rule-based annotation of the domain corpus to train a deep convolutional neural network to differentiate 'favour' from 'against' instances. At test time, they combined their network's output with rules to produce predictions that include the 'neither' class.</p><p>In general, results for Task B are lower than those for Task A as one would expect, as we remove the benefit of direct supervision. However, they are perhaps not as low as we might have expected, with the best result of 56.28 actually beating the best result for the supervised 'Climate Change' task (54.86).  Table <ref type="table" target="#tab_9">5</ref> shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Some teams did very well detecting tweets in favor of Trump (ltl.uni-due), with most of the others performing best on tweets against Trump. This makes sense, as 'against' tweets made up the bulk of the Trump dataset. The top team, pkudblab, was the only one to successfully balance these two goals, achieving the best F f avor score and the second-best F against score.</p><p>The Task B teams varied wildly in terms of approaches to this problem. The top three teams all took the approach of producing noisy labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task.</p><p>Further details on the submissions can be found in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, <ref type="bibr" target="#b4">Dias and Becker (2016)</ref>   <ref type="bibr" target="#b27">Tutek et al. (2016)</ref> for TakeLab, <ref type="bibr" target="#b32">Yuki et al. (2016)</ref> for Tohoku, and <ref type="bibr" target="#b1">Augenstein et al. (2016)</ref> for USFD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Past work on stance detection includes that by <ref type="bibr" target="#b25">Somasundaran and Wiebe (2010)</ref>, <ref type="bibr" target="#b0">Anand et al. (2011</ref><ref type="bibr" target="#b8">), Faulkner (2014</ref>, <ref type="bibr" target="#b22">Rajadesingan and Liu (2014)</ref>, <ref type="bibr" target="#b5">Djemili et al. (2014</ref><ref type="bibr">), Boltuzic andŠnajder (2014</ref>, <ref type="bibr" target="#b3">Conrad et al. (2012)</ref>, <ref type="bibr" target="#b26">Sridhar et al. (2014)</ref>, <ref type="bibr" target="#b22">Rajadesingan and</ref><ref type="bibr" target="#b22">Liu (2014), and</ref><ref type="bibr" target="#b24">Sobhani et al. (2015)</ref>. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys <ref type="bibr" target="#b18">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b11">Liu and Zhang, 2012;</ref> and proceedings of recent shared task competitions <ref type="bibr" target="#b30">(Wilson et al., 2013;</ref><ref type="bibr" target="#b14">Mohammad et al., 2013;</ref><ref type="bibr" target="#b23">Rosenthal et al., 2015)</ref>. See <ref type="bibr" target="#b20">Pontiki et al. (2014)</ref>, <ref type="bibr" target="#b21">Pontiki et al. (2015)</ref>, and <ref type="bibr" target="#b10">Kiritchenko et al. (2014a)</ref> for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled data pertaining to the test data targets is available (Task B). We received 19 submissions for Task A and 9 for Task B, with systems utilizing a wide array of features and resources. Stance detection, especially as formulated for Task B, is still in its infancy, and we hope that the dataset made available as part of this task will foster further research not only on stance detection as proposed here, but also for related tasks such as exploring the different ways in which stance is conveyed, and how the distribution of stance towards a target changes over time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2.</head><label></label><figDesc>SVM-unigrams: five SVM classifiers (one per target) trained on the corresponding training set for the target using word unigram features; 3. SVM-ngrams: five SVM classifiers (one per target) trained on the corresponding training set for the target using word n-grams (1-, 2-, and 3-gram) and character n-grams (2-, 3-, 4-, and 5-gram) features; 4. SVM-ngrams-comb: one SVM classifier trained on the combined (all 5 targets) training set using word n-grams (1-, 2-, and 3-gram) and character n-grams (2-, 3-, 4-, and 5-gram) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Distribution of instances in the Stance Train and Test sets for Task A and Task B. with 2,914 labeled training data instances for the five targets. The test data included 1,249 instances.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for Task A, reporting the official competition metric as 'Overall Favg', along with F f avor and Fagainst over all targets and Favg for each individual target. The highest scores in each column among the baselines and among the participating systems are shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for Task A (the official competition metric Favg) on different subsets of the test data. The highest scores in each column among the baselines and among the participating systems are shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results for Task B, reporting the official competition metric as Favg, along with F f avor and Fagainst. The highest score in each column is shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results for Task B (the official competition metricFavg) on different subsets of the test data. The highest score in each column is shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab,</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we use 'tweet' to refer to the text of the tweet and not to its meta-information. In our annotation task, we asked respondents to label for stance towards a given target based on the tweet text alone. However, automatic systems may benefit from exploiting tweet meta-information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://alt.qcri.org/semeval2016/task6/ http://www.saifmohammad.com/WebPages/StanceDataset.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A tweet that has a seemingly favorable hashtag may in fact oppose the target; and this is not uncommon. Similarly unfavorable hashtags may occur in tweets that favor the target.4  The full set of instructions is made available on the shared task website (http://alt.qcri.org/semeval2016/task6/).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.crowdflower.com6  The 60% threshold is somewhat arbitrary, but it seemed appropriate in terms of balancing quality and quantity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Annotations of the Stance Dataset were funded by the National Research Council of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cats rule and dogs drool!: Classifying stance in online debate</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean E Fox</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robeson</forename><surname>Bowmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Minor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd workshop on computational approaches to subjectivity and sentiment analysis</title>
				<meeting>the 2nd workshop on computational approaches to subjectivity and sentiment analysis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">USFD at SemEval-2016 Task 6: Any-Target Stance Detection on Twitter with Autoencoders</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Back up your stance: Recognizing arguments in online discussions</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Boltuzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janšnajder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Argumentation Mining</title>
				<meeting>the First Workshop on Argumentation Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing arguing subjectivity and argument tags</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics</title>
				<meeting>the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">INF-UFRGS-OPINION-MINING at SemEval-2016 Task 6: Automatic Generation of a Training Corpus for Unsupervised Identification of Stance in Tweets</title>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What does Twitter have to say about ideology?</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Djemili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Longhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Marinica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Natural Language Processing for Computer-Mediated Communication/Social Media-Pre-conference workshop at Konvens</title>
				<meeting>the Natural Language Processing for Computer-Mediated Communication/Social Media-Pre-conference workshop at Konvens</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Dimitris Kotzinos, and Georges-Elia Sarfati</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Heba</forename><surname>Elfardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">CU-GWU at SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Task 6: Ideological Stance Detection in Informal Text</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated classification of stance in student essays: An approach using stance target information and the Wikipedia link-based measure</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Faulkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Flairs Conference</title>
				<meeting>the Twenty-Seventh International Flairs Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;14</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;14<address><addrLine>Dublin, Ireland, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
		</imprint>
	</monogr>
	<note>Sentiment analysis of short informal texts</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
			<persName><forename type="first">Chengxiang</forename><surname>Aggarwal</surname></persName>
			<persName><surname>Zhai</surname></persName>
		</editor>
		<meeting><address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="415" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">nldsucsc at SemEval-2016 Task 6: A Semi-Supervised Approach to Detecting Stance in Tweets</title>
		<author>
			<persName><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Handleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Se-mEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, Se-mEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
				<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NRC-Canada: Building the state-of-theart in sentiment analysis of tweets</title>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;13</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;13<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 10th edition of the the Language Resources and Evaluation Conference (LREC)</title>
				<meeting>10th edition of the the Language Resources and Evaluation Conference (LREC)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>A dataset for detecting stance in tweets</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stance and sentiment in tweets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parinaz</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sentiment analysis: Detecting valence, emotions, and other affectual states from text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">JU NLP at SemEval-2016 Task 6: Detecting Stance in Tweets using Support Vector Machines</title>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Braja Gopal Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;14</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;14<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
	<note>SemEval-2014 Task 4: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 12: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Papageogiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;15</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;15<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Suresh Manandhar, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identifying users with opposing opinions in Twitter debates</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Rajadesingan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Social Computing, Behavioral-Cultural Modeling and Prediction</title>
				<meeting>the Conference on Social Computing, Behavioral-Cultural Modeling and Prediction<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semeval-2015 Task 10: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluations</title>
				<meeting>the 9th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From argumentation mining to stance classification</title>
		<author>
			<persName><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Argumentation Mining</title>
				<meeting>the Workshop on Argumentation Mining<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="67" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognizing stances in ideological on-line debates</title>
		<author>
			<persName><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
				<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective stance classification of posts in online debate forums</title>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
				<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TakeLab at SemEval-2016 Task 6: Stance Classification in Tweets Using a Genetic Algorithm Based Ensemble</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Tutek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sekulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Gombar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Paljak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Filipčulinović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Boltužić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domagoj</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janšnajder ; Wan</forename><surname>Alagić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>pkudblab at SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Specific Convolutional Neural Network System for Effective Stance Detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 2: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;13</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;13<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ltl.uni-due at SemEval-2016 Task 6: Stance Detection in Social Media Using Stacked Classifiers</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tohoku at SemEval-2016 Task 6: Feature-based Model versus Convolutional Neural Network for Stance Detection</title>
		<author>
			<persName><forename type="first">Igarashi</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Komatsu</forename><surname>Hiroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobayashi</forename><surname>Sosuke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Okazaki</forename><surname>Naoaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inui</forename><surname>Kentaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Marsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
