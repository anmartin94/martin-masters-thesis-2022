<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2020 Task 6: Definition extraction from free text with the DEFT corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sasha</forename><surname>Spala</surname></persName>
							<email>sspala@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<addrLine>345 Park Ave</addrLine>
									<postCode>95110-2704</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
							<email>nimiller@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<addrLine>345 Park Ave</addrLine>
									<postCode>95110-2704</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
							<email>dernonco@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<addrLine>345 Park Ave</addrLine>
									<postCode>95110-2704</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Dockhorn</surname></persName>
							<email>cdockhorn@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<addrLine>345 Park Ave</addrLine>
									<postCode>95110-2704</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2020 Task 6: Definition extraction from free text with the DEFT corpus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered. In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a termdefinition pair corpus that reflects the complex reality of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks: 1) Sentence classification, 2) sequence labeling, and 3) relation extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Definition extraction as a complex, real-world task is currently an emerging field of study. Traditional definition extraction approaches mostly rely on simple, syntactically straight-forward examples with relatively little variance in vocabulary. Corpora, including the WCL  and ukWaC <ref type="bibr" target="#b3">(Ferraresi et al., 2008)</ref>, typically consist of "definition sentences" which follow a standard X is a (type) Y or X, such as Y syntactic structure. Many also contain definitors , such as "means", "is", or "is defined by". We provide a complete review of the existing state of definition extraction in <ref type="bibr" target="#b16">Spala et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The DeftEval task provided data from the DEFT corpus for training, development and testing. Introduced in <ref type="bibr" target="#b16">Spala et al (2019)</ref>, the DEFT corpus is currently the largest and most comprehensive corpus explicitly for definition extraction. In addition to the typical definition-type sentences discussed in the introduction, the corpus also contains sentence and "sentence windows" of wide variance in syntactic and semantic construction. Sentences for the corpus were retrieved from the open source textbook website cnx.org, as well as from various 2017 SEC contract filings from the publicly available US Securities and Exchange Commission EDGAR (SEC) database. For the DeftEval shared task, participants were only required to use sentences extracted from textbooks. EDGAR annotations were provided on request, but not required for participation in the shared task.</p><p>The DEFT corpus provides sentence "context windows" for textbook data from cnx.org to narrow the search space for possible definitions. A context window is defined as a set of three sentences: one sentence before a sentence containing a potential term (e.g. a bold word), the main sentence with the potential term, and one sentence after. Though it is certainly possible that some definitions may occur further away from the bolded mention of the term, for the purposes of the shared task, definitions that occur outside of the three-sentence window around a potential term are out of scope.</p><p>The corpus contains 9 different token labels (see Table1), and 5 different types of relations (see . This expanded annotation schema allows for a wide range of data collection, as well as a better understanding of "non-traditional" definition types (i.e. beyond the typical x is a y structure). With this   The scientific method is a method of research with defined steps that include experiments and careful observation. 1</p><p>Occasionally, terms may appear as verb phrases or other constituent phrases (e.g. adjective phrases):</p><p>Many crystals and solutions rotate the plane of polarization of light passing through them. Such substances are said to be optically active. 2</p><p>Not including minor tokenization and sentence break fixes made during the shared task, there were no major modifications to the dataset before use in the DeftEval task. Complete details on the dataset is discussed in <ref type="bibr" target="#b16">Spala et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeftEval Task</head><p>The DeftEval shared task focused on fostering an expanded understanding of definition extraction for complex data, while keeping results comparable to traditional definition extraction tasks. The shared task, which ran from 4 Sept 2019 to 13 March 2020, was comprised of three subtasks: sentence classification, sequence labeling, and relation extraction.</p><p>Participants were provided data in accordance with the DeftEval timeline: DeftEval was administered using Codalab Competitions, a free, open-source platform for machine learning competitions and data challenges. Instructions for participation, including details about the annotation schema and information about the evaluation process, were made available during the practice and training periods of the shared task. All data and scripts were disseminated through the DEFT Github repository 3 , and linked to on the Codalab landing page 4 . Inconsistencies in the data, bug fixes, and updates to tokenization errors were made regularly throughout the training period as participants surfaced questions and potential problems with the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subtask 1: Sentence classification</head><p>In keeping with previous definition extraction tasks, we believed it to be important that the DeftEval shared task included a sentence classification subtask. Participants were provided a script 5 to convert the existing dataset in its sequence-labeling format to individual sentences with a binary label indicating whether or not the sentence contained a definition. For the purposes of this subtask, only sentences which contained token sequences labeled as definition in the annotated data were considered "definition sentences". Sentences containing only secondary definitions were given a 0 label. Unlabeled test data for the evaluation period was provided pre-converted in the sentence classification format; participants were only required to predict the label of each sentence at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subtask 2: Sequence labeling</head><p>Because of the nature of the expanded annotation in the DEFT corpus in comparison to previous definition extraction corpora, we believed it to be particularly important that the DeftEval shared task include a sequence labeling task. The DEFT corpus contains a large amount of auxiliary labels not previously seen in existing corpora, and the ability to reliably extract these labels could indicate a model with a broader cabability to identify complex and lengthy definition relationships. Ultimately, we hope for models that could both extract and pair (as seen in Subtask 33.3) related term and definition labels together, but given the potential for these two tasks to be particularly difficult and the novel nature of the DEFT corpus, the tasks were split into two subtasks for DeftEval.</p><p>The default format of the DEFT corpus data is similar to the CoNLL 2003 format <ref type="bibr" target="#b18">(Tjong Kim Sang and De Meulder, 2003)</ref> and formatted as BIO data <ref type="bibr" target="#b10">(Ramshaw and Marcus, 1995)</ref>. Therefore, participants did not need to convert any of the data for Subtask 2. At test time, participants were provided sentences where each line contained a token, source text file, lower char bound and upper char bound. Participants were required to predict only the appropriate BIO tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subtask 3: Relation Extraction</head><p>The final DeftEval subtask focused on extracting individual term-definition pairs, as well as matching auxiliary labels to their appropriate root. Without having a precedent for this particular task in the definition extraction domain, it was difficult to predict the level of difficulty for this task. Given the nature of existing relation extraction tasks, we expected this task to be particularly complex.</p><p>Participants were again able use the default format of the DEFT data for training. At test time, participants were provided sentences where each line contained a token, source text file, lower char bound, upper char bound, BIO tag, and tag ID. Participants were required to predict both the tag ID of the sister relation and the relationship tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task Logistics</head><p>All participants were required to use the provided data for training, but were allowed to add additional resources to their training data (e.g. other definition extraction resources, such as WCL, or general purpose models such as BERT). Additional hand-labeled definition extraction data, proprietary datasets, or unpublished data not universally accessible was explicitly disallowed. Participants were required to use the DEFT data as intended, using dev and training data only for training and development. Though it was only made available during the evaluation periods, participants were also informed that test data was to be used solely for testing purposes.</p><p>During the training period, participants were allowed unlimited submissions to the Codalab practice and training phases. The practice phase evaluated against the sample "practice" data, consisting of three "dummy" files, one for each subtask. The training phase evaluated against the dev set data. Exact copies of these evaluation data were provided in the Github repository. Participants were able to submit as many subtasks together as desired, though the evaluation process required that all files in a particular subtask be present for evaluation. In other words, participants must submit all files in the subtask evaluation set in order to be evaluated.</p><p>Evaluation scripts were also made available on the Github repository so that participants had access to understand and test the evalaution methods. Participants were encouraged to test their models locally on their own machines, then compare those results to those provided by the Codalab portal to ensure the formatting of their submissions were consistent with the Codalab requirements. Participants were also provided access to sample submissions via the Github repository for both the practice and training phases.</p><p>Each evaluation period ran for 12 days in which participants were able to submit their results through the Codalab submission process. Because of the nature of the relation extraction task potentially providing data for the sequence labeling task, the evaluation periods for Subtask 1 and 2 ran concurrently for 12 days, followed by the 12 day evaluation period for Subtask 3. Each participant was allowed 5 submissions per subtask, primarily to allow for any complications in the submission process. The vast majority of participants who submitted to the evaluation period used all 5 submissions. We suspect that many participants used these 5 submission attempts strategically in order to achieve higher results on the test sets, which should be taken into account for future tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Shared Task Results</head><p>Over the course of the shared task competition period, 279 submissions were made through the Codalab training phase, with many participants additionally developing and testing their systems locally. The evaluation periods brought 217 submissions for Subtask 1, 220 for Subtask 2, and 113 for Subtask 3. Results are based on the evaluation metrics described in Section 3.</p><p>Participants in DeftEval 2020 used a various array of methods for the three subtasks. For Subtasks 1 and 2 (see Tables <ref type="table" target="#tab_5">3 and 4</ref>), many participants opted simply to use a pre-trained language model, especially BERT, RoBERTa, and XLNet (see <ref type="bibr" target="#b19">Xie et al. (2020)</ref>      Participants were also particularly interested in multi-task solutions for Subtasks 1 and 2, to varying degrees of success (see <ref type="bibr" target="#b19">Xie et al. (2020)</ref>, <ref type="bibr" target="#b0">Avram et al. (2020)</ref>, ?)). <ref type="bibr" target="#b0">Avram et al. (2020)</ref> used pre-trained embeddings, then "projected the contextualized embeddings generated by the RoBERTa model in three vectors", making predictions for all three subtasks with a cohesive architecture.</p><p>Some "classic" neural architectures were also used in participant submissions to Subtask 1, including Kannan and Ponnusamy (2020)'s LSTM, RNN, and "hybrid" CNN solution, which leverages "the implicit local feature-extraction performed by the convolutional layers to refine the final representations passed to the reccurant layer, which accounts for global features." Ranasinghe et al. (2020) also used CNN and RNN methods on Subtask 1, in addition to other pretrained architectures.</p><p>Subtask 3 submissions (see Table <ref type="table" target="#tab_6">5</ref>) were generally more simple in nature, perhaps due to the issues discussed in Section 5.2. <ref type="bibr" target="#b19">Xie et al. (2020)</ref> used a pre-trained BERT model with some rule-based layers, and <ref type="bibr" target="#b1">Caspani et al. (2020)</ref> used a random forest approach with additional post-processing.</p><p>In addition to an array of architectures, DeftEval also sparked conversation around the annotation schema. <ref type="bibr" target="#b5">Jeawak et al. (2020)</ref> point out that the complexity of the schema make this task, as expected, more difficult than previous definition extraction tasks. They also note that some annotations conflict with each other in the training and test sets, which may be due to the nature of the schema, as well as the complicated content of the documents annotated. We recognized this issue, and recommend that future definition extraction tasks and tasks using the DEFT data consider this. Others mentioned they found that their initial model explorations came up short; <ref type="bibr" target="#b13">Singh et al. (2020)</ref> mention that "owing to the complexity of the DEFT corpus" their "syntactically aware neural networks" performed lower than expected.</p><p>Overall, participants employed a variety of different models, especially those based on pre-trained transformer architectures for Subtasks 1 and 2 and more "classic" approaches for Subtask 3. Participants also pointed out discrepancies and issues with the source dataset, which will be accounted for in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ammendments to the DEFT Corpus</head><p>The source data for DEFT was extracted by an automatic script that collected and sentence tokenized content from cnx.org. While modern automatic tokenization methods generally work well in clean environments, content scraped from cnx.org often unintentionally includes raw urls, incorrectly parsed text, and errant html code. Because of this, the tokenization process for the textbook data was particularly er-ror prone. Though our annotators and developers spent significant time manually adjusting tokenization errors, many still existed at the time of release on Github. Many participants discovered these tokenziation errors during training, which were then verified and fixed via Github issues. Because of the mixed nature of the source problem for tokenization errors, many caused incorrect sentence tokenization, which in turn meant that the actual number of sentences in the corpus was different from the reported number in <ref type="bibr" target="#b16">(Spala et al., 2019)</ref>. With the existing tokenziation fixes at publication time, the textbook corpus contains 16056 sentences (695627 tokens) and 9439 2-3 sentence context windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subtask 3 difficulty</head><p>In organizing Subtask 3, we believed the task of matching term-definition pairs to be particularly difficult, especially considering the number of additional possible tags in the DEFT corpus. Given the precedence of previous performance on other more constrained relation extraction tasks (see also <ref type="bibr" target="#b7">Lin et al. (2016)</ref>, <ref type="bibr" target="#b12">Shen and Huang (2016)</ref>, <ref type="bibr" target="#b15">Sorokin and Gurevych (2017)</ref>, etc.), we predicted this task would see mostly low performing scores.</p><p>However, participants in the subtask received a median F1 score of 0.9043, with 16 of the 27 participants scoring above 0.9. Upon further inspection, we realize this is due to the narrow search space of the three-sentence context windows. In designing the subtasks, we decided to keep the context windows in the test set so that the format of the data remained the same between released training data and test data and to minimize confusion for an already complicated task. Unfortunately, most context windows only include one set of term, definition, and matching auxiliary tags, meaning that this task was in fact particularly easy. F1 scores of 1 and near 1 make sense given the fact that models had to predict scenarios which in many cases only had one possible answer. Only cases that had more than one term-definition pair had a search space larger than 1.</p><p>We recommend that future iterations of this particular task provide sentences in context with the entire document. Though this does not work with the current format of the DEFT corpus, providing annotated data in context with the rest of the source textbook is a trivial task. While context windows only show the range of three sentences and possible distance between term-definition pairs would remain within those three sentences, providing the full context of the textbook would mean that models would have to discern term-definition pairs that cross context windows or appear in neighboring sentences. Future tasks may also consider combining Subtasks 2 and 3 in order to create a more comprehensive and challenging task overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Concurrent subtask evaluation periods</head><p>The evaluation periods for Subtasks 1 and 2 were scheduled to run concurrently in order to maximize the amount of time alloted for each subtask without extending the official SemEval evaluation period and to account for the data needs of Subtask 3. While Codalab's interface allowed for scheduling of two separate phases which had concurrent dates, the implementation in fact only allows for one phase. As a solution, we merged the two subtask evaluation phases into one phase which accepted both subtasks' submissions. Unfortunately, this also meant that individuals were now only provided 5 total submissions for both subtasks. To handle this, participants were instructed to complete submissions for their Subtask 1 evaluation first, then contact the task organizers to open additional submissions for Subtask 2 evaluation, keeping their best scoring Subtask 1 results in their submission files so that both Subtask 1 and Subtask 2 scores would be available on the leaderboard. Participants were warned that if their submissions for Subtask 1 changed with their Subtask 2 data after their 5 alloted submissions, the results would be removed and they would be subject to removal from the leaderboard.</p><p>We recommend future tasks with multiple subtasks using Codalab to either run these subtasks on separate, non-overlapping dates, or to provide the total number of submissions needed for both subtasks in the phase instructions, and remove any submissions that exceed the alloted submissions for the individual subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Overall, we believe the DeftEval shared task was a successful first exploration of definition extraction with the DEFT corpus. With more than 400 submissions and 149 registered participants, definition extraction as a complex, real-world task is clearly of interest to many. We believe that the results from this shared task provide a baseline for expectations for more complicated definition extraction tasks.</p><p>Though this task does not test a production-ready or real-life scenario, where models may have to predict term-definition pairs in a large document or in a wider context, the task does provide an initial look at how models perform on broad, non-standard definition extraction annotations. We recommend that future iterations of this task pay close attention to the effect of the DEFT corpus' three sentence context windows at test time, especially as it relates to the differences between an artificially narrow search space and documents "in the wild". We also recommend that future research includes a critical examination of conflicting annotations in the DEFT corpus.</p><p>Overall, we believe DeftEval has surfaced questions and research directions relevant to expanding definition extraction into more complicated linguistic environments, where data is particularly messy and the natural language it stems from is particularly complex. DeftEval provided a launching point for the exploration of this type of problem in definition extraction, proved the field interest for such a task, and laid groundwork for future iterations of definition extraction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>,<ref type="bibr" target="#b0">Avram et al. (2020)</ref>,<ref type="bibr" target="#b13">Singh et al. (2020)</ref>,<ref type="bibr" target="#b5">Jeawak et al. (2020</ref><ref type="bibr" target="#b11">), Ranasinghe et al. (2020</ref>, and<ref type="bibr" target="#b2">Davletov et al. (2020)</ref>). Other implementations incorporated various LSTM layers in addition to the use of BERT or other transformer architecture (see<ref type="bibr" target="#b20">Zhang and Ren (2020)</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Subtask 1 results; 56 participants, 217 total successful submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Subtask 2 results; 51 participants, 220 total successful submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>less common name for the primary term. Links to a term tag.Ordered TermMultiple terms that have matching sets of definitions which cannot be separated from each other without creating an non-contiguous sequence of tokens. E.g. x and y represent positive and negative versions of the definition, respectively</figDesc><table><row><cell>Tag Name</cell><cell>Description</cell></row><row><cell>Term</cell><cell>A primary term</cell></row><row><cell cols="2">Alias Term A secondary, Referential Term An NP reference to a previously mentioned term tag.</cell><cell>Typically</cell></row><row><cell></cell><cell>this/that/these + NP</cell></row><row><cell>Definition</cell><cell cols="2">A primary definition of a term. May not exist without a matching term.</cell></row><row><cell>Secondary Definition</cell><cell cols="2">Supplemental information that may qualify as a definition sentence or</cell></row><row><cell></cell><cell>phrase, but crosses a sentence boundary.</cell></row><row><cell>Ordered Definition</cell><cell cols="2">Multiple definitions that have matching sets of terms which cannot be sep-</cell></row><row><cell></cell><cell>arated from each other. See Ordered Term.</cell></row><row><cell cols="3">Referential Definition NP reference to a previously mentioned definition tag. See Referential</cell></row><row><cell></cell><cell>Term.</cell></row><row><cell>Qualifier</cell><cell cols="2">A specific date, location, or condition under which the definition holds</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Tag schema</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Relation schema</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Subtask 1 submissions and system descriptions.</figDesc><table><row><cell cols="2">Rank Team Name</cell><cell>Submission</cell><cell>Reference</cell><cell>Models</cell></row><row><cell></cell><cell></cell><cell>Name</cell><cell></cell><cell></cell></row><row><cell>23</cell><cell>*</cell><cell>davletov-aa</cell><cell>Davletov et al. (2020)</cell><cell>BERT</cell></row><row><cell>27</cell><cell>defx</cell><cell>marchbnr</cell><cell>Hübner et al. (2020)</cell><cell>CRF tagger</cell></row><row><cell>34</cell><cell>RGCL</cell><cell>tharindu</cell><cell>Ranasinghe et al.</cell><cell>XLNet -large</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(2020)</cell><cell></cell></row><row><cell>37</cell><cell>UPB</cell><cell>andrei.avram</cell><cell>Avram et al. (2020)</cell><cell>RoBERTa + CRF with fine-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tuning</cell></row><row><cell></cell><cell></cell><cell cols="3">Table 4: Subtask 2 submissions and system descriptions.</cell></row><row><cell cols="4">Rank Team Name Submission Name Reference</cell><cell>Models</cell></row><row><cell>1</cell><cell cols="2">UNIXLONG unixlong</cell><cell>Xie et al. (2020)</cell><cell>BERT + hand crafted rules</cell></row><row><cell>4</cell><cell>ACNLP</cell><cell>caspa</cell><cell cols="2">Caspani et al. (2020) Random Forest</cell></row></table><note>* Team name not specified * Team name not specified</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Subtask 3 submissions and system descriptions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/adobe-research/deft corpus/blob/master/data/deft files/dev/t1 biology 0 0.deft 2 https://github.com/adobe-research/deft corpus/blob/master/data/deft files/dev/t3 physics 1 101.deft</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/adobe-research/deft corpus 4 https://competitions.codalab.org/competitions/22759 5 https://github.com/adobe-research/deft corpus/blob/master/task1 converter.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Upb at semeval-2020 task 6: Pretrained language models for definition extraction</title>
		<author>
			<persName><forename type="first">Andrei-Marius</forename><surname>Avram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru-Clementin</forename><surname>Cercel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costin</forename><surname>Chiru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnlp at semeval 2020 task 6: A supervised approach for definition extraction</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Caspani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pirashanth</forename><surname>Ratnamogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathis</forename><surname>Linger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mhamed</forename><surname>Hajaiej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task learning for definition extraction</title>
		<author>
			<persName><forename type="first">Adis</forename><surname>Davletov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shatilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Gordeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Rey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Introducing and evaluating ukwac, a very large web-derived corpus of english</title>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2020. marchbnr at semeval-2020 task 6: Joint extraction of concepts and relations for definition extraction</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schwarzenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cardiff university at semeval-2020 task 6: Fine-tuning bert for domain-specific definition classification</title>
		<author>
			<persName><forename type="first">Shelan</forename><surname>Jeawak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ui at sem-eval 2020 task 6 : Definition mining</title>
		<author>
			<persName><forename type="first">Madeeswaran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haemanth</forename><surname>Santhi Ponnusamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning word-class lattices for definition and hypernym extraction</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
				<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An annotated dataset for extracting definitions and hypernyms from the web</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juana</forename><surname>Ruiz-Martãnez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010</title>
				<meeting>the International Conference on Language Resources and Evaluation, LREC 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
				<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rgcl at semeval-2020 task 6: Neural approaches to definitionextraction</title>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Tharindu Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Plum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName><surname>Mitkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for semantic relation extraction</title>
		<author>
			<persName><forename type="first">Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dsc iit-ism at semeval-2020 task 6: Boosting bert with dependencies for definition extraction</title>
		<author>
			<persName><forename type="first">Aadarsh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deftpunk at semeval-2020 task 6: using rnn-ensemble for the sentence classification</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Kaparina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-aware representations for knowledge base relation extraction</title>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DEFT: A corpus for definition extraction in free-and semi-structured text</title>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Spala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Dockhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Linguistic Annotation Workshop</title>
				<meeting>the 13th Linguistic Annotation Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated detection and annotation of term definitions in german text corpora</title>
		<author>
			<persName><forename type="first">Angelika</forename><surname>Storrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wellinghoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Language Resources and Evaluation (LREC)</title>
				<meeting>the Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unixlong at semeval-2020 task6: A joint model for definition extraction</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Lianxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bertatde at semeval-2020 task 6: Extracting term-definition pairs in free text using pre-trained model</title>
		<author>
			<persName><forename type="first">Huihui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
				<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
