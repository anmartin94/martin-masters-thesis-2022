<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIGTYP 2020 Shared Task: Prediction of Typological Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
							<email>jbjerva@cs.aau.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">Aalborg University University of Copenhagen Johns Hopkins University University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aditi</forename><surname>Chaudhary</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Carnegie Mellon University Leipzig University ETH Zürich</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Celano</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Edoardo</forename><forename type="middle">M</forename><surname>Ponti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
							<email>augenstein@di.ku.dk</email>
						</author>
						<title level="a" type="main">SIGTYP 2020 Shared Task: Prediction of Typological Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typological knowledge bases (KBs) such as WALS <ref type="bibr" target="#b18">(Dryer and Haspelmath, 2013</ref>) contain information about linguistic properties of the world's languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linguistic typology is the study of structural properties of languages <ref type="bibr" target="#b10">(Comrie, 1988;</ref><ref type="bibr" target="#b14">Croft, 2002;</ref><ref type="bibr" target="#b39">Velupillai, 2012)</ref>. Approaches to the categorisation of the languages of the world according to their linguistic properties are represented by, e.g., typological features in databases such as WALS <ref type="bibr" target="#b18">(Dryer and Haspelmath, 2013)</ref>, URIEL , and AUTOTYP <ref type="bibr" target="#b29">(Nichols et al., 2013)</ref>, e.g. in terms of their syntax, morphology, and phonology. One example of such a typological feature is the basic word order feature in WALS. For instance, English is best described as a subject-verb-object (SVO) language, whereas Japanese is best described as a subject-object-verb (SOV) language.</p><p>Once a relatively niche topic in the NLP community, studying typological features has recently risen in popularity and importance for a number of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words <ref type="bibr" target="#b0">(Ammar et al., 2016;</ref><ref type="bibr" target="#b40">Wada et al., 2019)</ref> or, more recently, sentences <ref type="bibr" target="#b1">(Artetxe and Schwenk, 2019;</ref><ref type="bibr" target="#b17">Devlin et al., 2019;</ref><ref type="bibr" target="#b12">Conneau and Lample, 2019;</ref><ref type="bibr" target="#b11">Conneau et al., 2020)</ref>. This naturally raises the question of what these representations encode, and some have turned to typology for potential answers <ref type="bibr" target="#b8">(Choenni and Shutova, 2020;</ref><ref type="bibr">Zhao et al., 2020)</ref>. In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few-or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models <ref type="bibr" target="#b2">(Bjerva and Augenstein, 2018a;</ref><ref type="bibr" target="#b30">Nooralahzadeh et al., 2020;</ref><ref type="bibr">Zhao et al., 2020)</ref>.</p><p>In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing <ref type="bibr" target="#b23">(de Lhoneux et al., 2018)</ref>. Finally, the relationship between typological knowledge bases (KBs) such as WALS <ref type="bibr" target="#b18">(Dryer and Haspelmath, 2013)</ref> and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs <ref type="bibr" target="#b26">(Malaviya et al., 2017;</ref><ref type="bibr" target="#b27">Murawaki, 2017;</ref><ref type="bibr" target="#b2">Bjerva and Augenstein, 2018a;</ref><ref type="bibr" target="#b6">Bjerva et al., 2019c)</ref>, and that implications can be discovered in typological KBs <ref type="bibr" target="#b16">(Daumé III and Campbell, 2007;</ref><ref type="bibr" target="#b5">Bjerva et al., 2019b)</ref>.</p><p>The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for auto-Figure <ref type="figure">1</ref>: Shared task WALS data superimposed on a map showing one point per language with train, dev, and test splits; relative point sizes representing number of features for that language. matic knowledge base population are highly desirable. While past approaches have shown the feasibility of typological feature prediction, the considered evaluation setups have some flaws which led to overestimated performance. Some papers control for phylogenetic relationships between languages, e.g. not both training and testing on Slavic languages, but little-to-no work has considered controlling for geographical proximity. This is corrected for in this shared task.</p><p>The shared task attracted 8 system submissions from 5 teams for two subtasks (constrained and unconstrained resources). In general, the systems which make use of correlations between features, and exploit observed features during inference, perform better, whereas those that do not make use of observed features perform similarly to our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The SIGTYP 2020 shared task is concerned with predicting typological features from the World Atlas of Language Structures (WALS) <ref type="bibr" target="#b18">(Dryer and Haspelmath, 2013)</ref>. For the task, participants were invited to build systems to predict features for languages unseen at training time. The shared task consisted of two subtasks: 1) the constrained setting, for which only the provided training data may be used; 2) the unconstrained setting, for which training data may be extended with any external source of information (e.g. pre-trained embeddings, additional text, etc.) Data Format For each instance, the following information is provided: the language code, name, latitude, longitude, genus, family, country code, and features. At training time, both the feature names and feature values are given, while at test time, submitted systems are required to fill values for the requested features. An example of a test instance is given in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>WALS comprises 2679 languages and a total of 192 feature categories <ref type="bibr" target="#b18">(Dryer and Haspelmath, 2013)</ref>. However, the database is quite sparse in that many language-feature combinations lack annotation. Furthermore, it is a skewed database, in that a handful of languages have annotations for a large number of features, and some features are annotated for almost all languages, whereas some have very little coverage. In order to alleviate data sparsity in the shared task, only the subset of the languages in WALS with more than 3 features available are considered. Furthermore, of all the features of the languages so selected only those present in more than 9 languages have been retained. Most feature categories in WALS can take several feature values. For instance, the feature Tone can take one of the values: No tones, Simple tone system, or Complex tone system. This dataset has been divided into train set (90%), dev set (5%), and test set (5%).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Setup</head><p>While a substantial amount of previous work deals with feature prediction in typological databases such as WALS (e.g. <ref type="bibr" target="#b26">Malaviya et al. (2017)</ref>; Murawaki (2017); Bjerva and Augenstein (2018a); <ref type="bibr" target="#b6">Bjerva et al. (2019c)</ref>), most such work does not take into account that both phylogenetic and geographic proximity should be controlled for. Languages which have shared common ancestry will often have similar typological features, hence training and evaluating on the same language family will tend to inflate the expected performance of the model <ref type="bibr" target="#b4">(Bjerva et al., 2019a)</ref>. In the data for this shared task, we make sure to control for both of these factors.</p><p>Our evaluation setup is constructed as follows. We evaluate on a set of languages from small languages spread across the world, as defined by the WALS macroareas: Mayan (North America), Tucanoan (South America), Madang (Papuanesia), Mahakiranti (Eurasia), Northern Pama-Nyungan (Australia), and Nilotic (Africa). In addition, we include a subset of languages spoken around the world, by randomly sampling 10% of the available data in WALS. This yields two evaluation set-ups: one in which we evaluate on unobserved languages, controlling for both phylogenetic and geographic relationships, and one in which we perform a random evaluation as is common in previous work.</p><p>The languages in the test data vary in the number of removed and present feature values so that the blanking ratios are spread uniformly between 5% and 95%. This will allow our analysis to investigate whether some approaches benefit from observing a large number of features and whether some are robust to situations where only a small number of features are observed (subsection 5.4).</p><p>In order to control for phylogenetic and geographic effects, we remove all languages from the same language genus as the aforementioned languages from the training set, as well as all lan-guages which are spoken within 1,000km of any of these languages. 1 This reduces the number of languages in the training set to 1250. The task had participants run their systems on the partial feature information for our held-out languages and send us the outputs of their systems, i.e., the imputed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>We report macro-averaged accuracies, meaning that we first compute the average accuracy for each language, i.e., the fraction of to be imputed features correctly predicted by the participant's system, then average these language accuracies within each language genus, and finally report the average of these genus accuracies to rank participants as well as all these accuracies for each language genus (Section 5) to see whether systems behave differently on different language families. We judge statistical significance using a non-parametric two-tailed paired permutation test with 5k samples each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We provide two baselines. The first is a simple lower-bound baseline based on observing feature frequencies in WALS (Baseline frequency in Figure <ref type="figure" target="#fig_0">2</ref>). For each unobserved feature in the test set, we predict the most frequent feature value from the training set.</p><p>The second uses the k-nearest neighbours (k-NN) algorithm with a simple feature set to predict each unobserved feature, with k = 1 (Baseline knn-imputation in Figure <ref type="figure" target="#fig_0">2</ref>). Each language is represented by a language vector ( l ∈ R 64 ) trained as a part of a multilingual character-based language model (Östling and Tiedemann, 2017). During inference, for a language l and unobserved feature y, we find the nearest neighbour to l for which y has been observed, similar to <ref type="bibr">Bjerva and Augenstein (2018a,b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submissions</head><p>We received eight submissions from five teams across the constrained and unconstrained subtasks, as described below.</p><p>UFAL <ref type="bibr" target="#b38">(Vastl et al. (2020)</ref>, Charles University) submitted a constrained system which ensembled two approaches: first, estimating the correlation of feature values within languages enables missing feature prediction, and second, using a neural network to predict whether feature values match a specific language after training one network with all provided WALS feature values and pre-computed language embeddings. By ensembling both using confidence scores, they were able to improve on each individual approach and produce the best accuracy of all constrained and unconstrained submissions.</p><p>CrossLingference <ref type="bibr" target="#b21">(Jäger (2020)</ref>, University of Tübingen) submitted an unconstrained system using inferred phylogenetic trees. These were built with Continuous Time Markov Processes using Swadesh lists from the Automated Similarity Judgment Project (ASJP), with k-nearest neighbour estimations based on geographic information as backoff for test set languages not in both Glottolog and ASJP. Ancestral state reconstruction allows the inference of features for ancestral states from the provided surface features (WALS), and similarly, for this year's shared task, unknown feature values for non-ancestral languages can be inferred individually by rerooting the tree to a related language.</p><p>NUIG (Choudhary (2020), NUI Galway) submitted a constrained system with independent classifiers to predict each WALS feature. The outputs of independent classifiers are then fed into a shared encoder with feed-forward and self-attention layers in order to make use of feature correlations. Their model does not use other known features for WALS feature prediction at inference time, relying only on the 5-dimensional inputs of longitude, latitude, genus, family, and country-code.</p><p>NEMO <ref type="bibr" target="#b20">(Gutkin and Sproat (2020)</ref>, Google London and Tokyo) submitted constrained systems which first computed probabilities of represented feature values across each language's genetic (genus and family), and areal (features from languages within a 2,500 kilometre radius, computed from provided latitude and longitude with the Haversine formula), and implicational universals or rather, priors for certain features given commonly associated feature-value pairs in the data. They compared several classifiers' performance using these sparse features, ultimately submitting systems using ridge regression. The two submitted systems differ in whether these features were computed for the test set or only train and dev.</p><p>Panlingua <ref type="bibr" target="#b22">(Kumar et al., 2020)</ref>, a team effort across KMI, Panlingua, and IIT KGP, submitted constrained systems from three approaches: two rule-based systems (one statistical, and one frequency-based baseline) and one hybrid system. Their baseline is similar to the organizers' frequency-base baseline, except that it produces the most frequent value for a feature within a genus if available, backing off to language family, and then the overall most-frequent value. The hybrid system uses 180 different SVM classifiers for the 180 features which were present in the training set. The statistical system provides a two-step back off procedure if neither a feature has been seen for either a languages' genus or family in training: first, finding the most frequent values in nearby languages using Haversine distance, and if these are too distant, turning to nearby language families. This system performed best on the held-out data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the overall results and rankings for all shared task submissions. The rankings use macro-averaged accuracies as this equally weights the controlled genera (the exception is the comparison to micro-averaged accuracies in Figure <ref type="figure" target="#fig_1">3</ref>). This year's shared task was separated into two subtasks: constrained systems which used only the WALS features and data provided, and unconstrained systems, open to use of any data or pre-trained models. Accordingly, we have two winning systems:ÚFAL for constrained, and CrossLingference for unconstrained, withÚFAL producing the best results overall across both subtasks.</p><p>Results for each unobserved genus, shown in comparison to results across genera observed in training, may be found in Table <ref type="table" target="#tab_3">2</ref>.</p><p>WALS feature value formatting is not standardized and, unfortunately, the test data was released containing additional tabs within the feature values for some features, which adversely affected teams who may have used tab-separation for data preprocessing. Many teams accounted for this and submitted feature values for all 2417 features across the 149 languages in the test data, but for two teams this led to missing features in their submissions: CrossLingference was missing 7 features across 7 languages, affecting their results by 1%; Panlingua was missing 61 features across 15 languages in their rule-based submission and 57 across 11 for their additional two submissions, affecting their results by 2%. When evaluating without the affected features, rankings were not changed, nor were there significant differences between submitted systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subtask 1: Constrained Setting</head><p>The nine systems in the constrained setting used a diverse set of model features and architectures. When computing pairwise significances with a paired permutation test, we find that these systems cluster into three groups, within each the systems are not significantly different from each other: {1}, {2-3}, and {4-8}. Teams submitting multiple systems were able to improve their accuracy within their own submissions, but we did not find that their individual submissions were statistically significantly different. Similar differences in overall accuracy do not necessarily indicate statistically significant margins: for example, the 1st and 2nd systems have the same margin (0.05) as the 4th and 7th, but the latter are not significantly different while the former are.</p><p>Finer-grained analysis of results across controlled genera, and comparing results across different levels of representation in the training data, can be found in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subtask 2: Unconstrained Setting</head><p>CrossLingference submitted the only unconstrained system, which used additional data in the form of Swadesh lists to infer phylogenetic trees. This system outperforms the unconstrained knnimputation baseline on all evaluated conditions. When we contextualize this submission by comparing it to those in the constrained setting, we find that it joins the second cluster with the two submissions from NEMO; interestingly, when features are micro-averaged rather than macro-averaged, these teams reorder, with CrossLingference outperforming the two NEMO systems, seen in Figure <ref type="figure" target="#fig_1">3</ref>. This is somewhat counter-intuitive, given the way each system uses phylogenetic information. While CrossLingference explicitly models phylogenetic information through its model structure, NEMO takes a frequentist approach where the counts and probabilities of each feature within a language's genus, family, and geographic area are pre-computed and passed as sparse features to feature classifiers. One might expect the latter to perform better on a micro-average where overall data frequencies would be more heavily weighted than each genus, but this was not the case here. We explore this further in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Our test data was constructed to enable comparison across controlled phylogenetic and geographic relationships, and randomly sampled features from languages covered in training as is common in previous work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Results</head><p>Table <ref type="table" target="#tab_3">2</ref> compares submission accuracy on features from diverse WALS macroareas unobserved in training data, and other observed genera. We see that overall rankings hold when evaluated on observed languages. However, this is not the case for several of our unobserved genera. With respect to the shared task baselines, we find that the frequency baseline, which naively picks the most well-represented values for each feature, is most representative for the larger 'other genera' category, which represents the majority of the training data but does not account for the diversity of typological features and values across many languages. Nonetheless, for most of the unobserved genera, the frequency baseline performed better than the knn-imputation baseline, which was significantly better for Mahakiranti only, primarily due to correct prediction of "OV" ordering across multiple features. Interestingly, while the first 3 systems perform better on macro-averaged accuracy than microaveraged (Figure <ref type="figure" target="#fig_1">3</ref>), this is not true for all other systems, suggesting that they rely more on getting frequent and "easy" features right, relying on frequency in training data. Note that the six unobserved genera come from separate macroareas across six different continents, and have a more even distribution of feature values than the 'other genera.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Differences across Genera</head><p>Looking at specific genera, we see that Mayan caused the greatest split between submitted systems, with the first two clusters performing very well, and the frequency baseline performed bet-ter than the majority of systems. On the other end of the spectrum, certain genera (Tucanoan and Madang) with well-represented features were relative equalizers, with the least variance in results across the submitted systems.</p><p>Within those teams which submitted multiple systems, there were only certain cases in which these performed significantly differently from each other. Panlingua submitted three different systems; two rule-based (one statistical and one frequency-based), and one hybrid model. For most genera, these performed very similarly, with consistently better results from the statistical rulebased system than the others, though there were no statistically significant differences shown by paired permutation tests. However, this was not the case for Tucanoan, where the statistical (and to a less degree, hybrid) model significantly outperformed the other. These systems had equal performance on four of the Tucanoan languages {Cubeo,Secoya,Siona,Koreguaje}, but quite divergent on the remaining four languages {Desano,Retuarã,Tucano,Tuyuca}. This second set required predicting values for several features concerning the order of Subject, Object, Verb, which the statistical model was able to correctly predict through better back-off choices, but swayed by the more frequent SVO languages in training, their frequency baseline and SVM-based classifiers were not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Differences among Features</head><p>Table <ref type="table" target="#tab_5">3</ref> shows the features with the highest and lowest accuracies across all submissions. We find that the features with highest accuracies also have the most consistent performance across all systems, and typically have the most frequent values for  those features. The most difficult features, on the other hand, have the least frequently occurring values in the training data, and have higher varianceinterestingly, the top four systems were nonetheless able to achieve greater than 65% accuracy on these features, while the remaining systems' accuracies were ∼ 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Blanking Ratio</head><p>Since the languages in our test sets all remove and retain different numbers of features, we can see whether the blanking ratio, that is, the ratio of features that participants have to impute to the features listed for that language, correlates with the performance of the system in question on that language. Calculating Pearson's correlation coefficient for every system individually, we realize that they range from -0.23 (for a NEMO system) to 0.31 (for a Panlingua system), almost all of which statistically significantly different from 0. Why do these correlations differ so much from system to system? To answer that question, we plot these correlation coefficients as a function of the system's overall performance in Figure <ref type="figure" target="#fig_2">4</ref>. It turns out that a system's overall performance and how sensitive it is to the blanking ratio are highly correlated: the stronger systems are much more negatively affected by the removal of more features (their correlations are negative), weaker systems are not only not harmed, but seem to find the languages where only a few features are blanked harder still (having positive correlations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Previous work can be divided into research on predicting typological features automatically, crosslingual transfer learning which utilises typology to inform sharing, probing of representations for what typological knowledge they encode, and finally, work on how best to represent a language in terms of its typological features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Predicting Typological Features</head><p>Typological knowledge bases are both sparse and skewed in terms of language-feature annotations. They are sparse in the sense that most languages only have annotations for a handful of features and skewed in the sense that a few features have much wider coverage than others. Luckily, such features often correlate with one another, which allows for prediction of those features from others. For instance, languages where the verb precedes the object tend to have prepositions, e.g. Norwegian, whereas languages where the object precedes the verb word tend to have postpositions, e.g. Japanese.</p><p>A survey of approaches to prediction of features is provided in <ref type="bibr">Ponti et al. (2019a, § 4.3)</ref>. Some common approaches include prediction based on language representations learned as a by-product of model training <ref type="bibr">(Östling and Tiedemann, 2017;</ref><ref type="bibr" target="#b26">Malaviya et al., 2017;</ref><ref type="bibr" target="#b2">Bjerva and Augenstein, 2018a;</ref><ref type="bibr" target="#b6">Bjerva et al., 2019c)</ref> and matrix factorisation <ref type="bibr" target="#b27">(Murawaki, 2017;</ref><ref type="bibr" target="#b4">Bjerva et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Typologically Informed Sharing</head><p>Cross-lingual sharing informed by typology has been investigated for, among others, parsing <ref type="bibr" target="#b28">(Naseem et al., 2012;</ref><ref type="bibr" target="#b36">Täckström et al., 2013;</ref><ref type="bibr" target="#b41">Zhang and Barzilay, 2015;</ref><ref type="bibr" target="#b23">de Lhoneux et al., 2018)</ref>, language modeling <ref type="bibr" target="#b35">Ponti et al., 2019b)</ref>, machine translation <ref type="bibr" target="#b15">(Daiber et al., 2016;</ref>, and morphological inflection <ref type="bibr" target="#b7">(Chaudhary et al., 2019)</ref>. Many of these approaches use language embeddings with sparse features encoding WALS feature values. <ref type="bibr" target="#b31">Oncevay et al. (2020)</ref> find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Typological Probing</head><p>Several recent papers study typological feature prediction as a probing task for evaluating cross-lingual sentence encoders <ref type="bibr" target="#b8">(Choenni and Shutova, 2020;</ref><ref type="bibr" target="#b2">Bjerva and Augenstein, 2018a;</ref><ref type="bibr" target="#b30">Nooralahzadeh et al., 2020;</ref><ref type="bibr">Zhao et al., 2020)</ref>. Typically, hidden representations are probed for whether or not they might encode a typological feature by, e.g., using them in a separate classifier <ref type="bibr" target="#b26">(Malaviya et al., 2017;</ref><ref type="bibr" target="#b2">Bjerva and Augenstein, 2018a;</ref><ref type="bibr" target="#b30">Nooralahzadeh et al., 2020)</ref>.Östling and Tiedemann (2017) learn language representations during multilingual language modelling and find that the resulting representations can reproduce relatively credible phylogenetic trees. Bjerva and Augenstein (2018a) learn language representations under NLP tasks such as POS tagging and grapheme-to-phoneme conversion, and find that typological features related to the task at hand are sometimes encoded. <ref type="bibr" target="#b30">Nooralahzadeh et al. (2020)</ref> use a typological probing task in experiments for zero-and few-shot NLI and QA, finding that languages which share typological properties benefit from sharing. <ref type="bibr">Zhao et al. (2020)</ref> attempt to induce language-agnostic representations, e.g. by reducing the typological gaps between languages, and find that this is beneficial for NLI and MT. <ref type="bibr" target="#b19">Gerz et al. (2018)</ref> show that there is a correlation between typological features related to morphology and model performance in language modelling, and <ref type="bibr" target="#b13">Cotterell et al. (2018)</ref> further show that inflectional morphology affects performance in both n-gram and LSTM-based language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper documents the first SIGTYP shared task on prediction of typological features in WALS. The 8 system submissions from 5 teams showed that a variety of different methods can be applied to the task. Interestingly, the best system only achieved a macro-averaged accuracy of 75%, indicating that the task is far from solved. This further shows that the evaluation set-up in which we controlled for both phylogenetic relationships and geographic proximity is a challenging one. We expect that further exploration of unconstrained systems to have the most potential for predicting features in such cases, where little or nothing is known about a language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Macro-averaged rankings of all submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of macro-averaged and microaveraged accuracies across submissions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Correlation coefficients (between blanking ratio and language performance) for each system as a function of that system's performance. The correlation of R = −0.88 is significant at p &lt; .001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data format for two test instances of the SIGTYP 2020 shared task dataset</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Macro-averaged results across each unobserved genus, as compared to genera with languages observed in training with randomly sampled splits, shown with number of languages in each genus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Features with the highest and lowest overall accuracies across all submissions, with number of languages containing the feature in the test data (183 total languages)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Distances calculated with WALS language locations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1602.01925</idno>
	</analytic>
	<monogr>
		<title level="j">Massively Multilingual Word Embeddings. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From phonology to syntax: Unsupervised linguistic typology at different levels with language embeddings</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1083</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking Typological Traits of Uralic Languages in Distributed Language Representations</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages</title>
				<meeting>the Fourth International Workshop on Computational Linguistics of Uralic Languages<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A probabilistic generative model of linguistic typology</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1529" to="1540" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncovering probabilistic implications in typological knowledge bases</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3924" to="3930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Han</forename><surname>Robertöstling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Veiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00351</idno>
		<title level="m">What do language representations really represent? Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CMU-01 at the SIGMORPHON 2019 shared task on crosslinguality and context in morphology</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gayatri</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
				<meeting>the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">What does it mean to be language-agnostic? probing multilingual sentence encoders for typological properties</title>
		<author>
			<persName><forename type="first">Rochelle</forename><surname>Choenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NUIG: Multitasking Selfattention based approach to SigTyp 2020 Shared Task</title>
		<author>
			<persName><forename type="first">Chinmay</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</title>
				<meeting>the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic typology</title>
		<author>
			<persName><surname>Bernard Comrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Anthropology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslingual Language Model Pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are all languages equally hard to language-model?</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Typology and Universals. Cambridge University Press</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Universal reordering via linguistic typology</title>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Stanojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3167" to="3176" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Bayesian Model for Discovering Typological Implications</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">WALS Online. Max Planck Institute for Evolutionary Anthropology</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the relation between linguistic typology and (limitations of) multilingual language modeling</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="316" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NEMO: Frequentist Inference Approach to Constrained Linguistic Typology Feature Prediction in SIGTYP 2020 Shared Task</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gutkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</title>
				<meeting>the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imputing typological values via phylogenetic inference</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Jäger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Research in Linguistic Typology</title>
				<meeting>the Second Workshop on Computational Research in Linguistic Typology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">KMI-Panlingua-IITKGP at SIGTYP2020: Exploring rules and hybrid systems for automatic prediction of typological features</title>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Alok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bornini</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><surname>Ojha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</title>
				<meeting>the Second Workshop on Computational Research in Linguistic Typology. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parameter sharing between dependency parsers for related languages</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Søgaard</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1543</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on</title>
				<meeting>the 2018 Conference on</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">Natural Language Processing</title>
				<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4992" to="4997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Kairis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning language representations for typology prediction</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1268</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2529" to="2535" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Diachrony-aware induction of binary latent representations from typological features</title>
		<author>
			<persName><forename type="first">Yugo</forename><surname>Murawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
				<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="461" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective sharing for multilingual dependency parsing</title>
		<author>
			<persName><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="637" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The autotyp genealogy and geography database: 2013 release</title>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alena</forename><surname>Witzlack-Makarevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balthasar</forename><surname>Bickel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Zurich</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Zurich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-Shot Cross-Lingual Transfer with Meta Learning</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bridging linguistic typology and multilingual machine translation with multi-view language representations</title>
		<author>
			<persName><forename type="first">Arturo</forename><surname>Oncevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14923</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Association for Computational Linguistics</title>
				<meeting>EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous multilinguality with language vectors</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Robertöstling</surname></persName>
		</author>
		<author>
			<persName><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="644" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling language variation and universals: A survey on typological linguistics for natural language processing</title>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">O</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeni</forename><surname>'horan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00357</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Isomorphic transfer of syntactic structures in cross-lingual NLP</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Edoardo Maria Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><surname>Vulić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1531" to="1542" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards zero-shot language modeling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1288</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2900" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1061" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Polyglot neural language models: A case study in cross-lingual phonetic representation learning</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1357" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting Typological Features in WALS using Language Embeddings and Conditional Probabilities: UFAL Submission to the SIGTYP 2020 Shared Task</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vastl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Research in Linguistic Typology</title>
				<meeting>the Second Workshop on Computational Research in Linguistic Typology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An introduction to linguistic typology</title>
		<author>
			<persName><forename type="first">Viveka</forename><surname>Velupillai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Benjamins Publishing Company Amsterdam</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3113" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical low-rank tensors for multilingual transfer parsing</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09112</idno>
		<title level="m">Johannes Bjerva, and Isabelle Augenstein. 2020. Inducing Language-Agnostic Multilingual Representations</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
