<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
							<email>hajic@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
							<email>johansson@disi.unitn.it</email>
						</author>
						<author>
							<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Antònia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martí</forename><surname>⋆⋆</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
							<email>lluism@lsi.upc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Meyers</surname></persName>
							<email>meyers@cs.nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">⋆</forename><forename type="middle">‡</forename><surname>Joakim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nivre</forename><surname>⋆⋄</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
							<email>pado@ims.uni-stuttgart.de</email>
						</author>
						<author>
							<persName><forename type="first">⋄⋆</forename><surname>Janštěpánek</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
							<email>stranak@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName><forename type="first">⋆</forename><surname>Mihai</surname></persName>
							<email>mihais@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Surdeanu</forename><surname>‡⋆</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
							<email>xuen@brandeis.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<email>yzhang@coli.uni-sb.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Catalonia</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Uppsala University and Växjö University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) launches a competitive, open "Shared Task". A common ("shared") task is defined and datasets are provided for its participants. In 2004 and 2005, the shared tasks were dedicated to semantic role labeling (SRL) in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. In 2008, the shared task <ref type="bibr" target="#b24">(Surdeanu et al., 2008</ref>) used a unified dependencybased formalism, which modeled both syntactic dependencies and semantic roles for English. The CoNLL-2009 Shared Task has built on the 2008 results by providing data for six more languages (Catalan, Chinese, Czech, German, Japanese and Spanish) in addition to the original English 1 . It has thus naturally extended the path taken by the five most recent CoNLL shared tasks.</p><p>As in 2008, the CoNLL-2009 shared task combined dependency parsing and the task of identifying and labeling semantic arguments of verbs (and other parts of speech whenever available). Participants had to choose from two tasks:</p><p>• Joint task (syntactic dependency parsing and semantic role labeling), or</p><p>• SRL-only task (syntactic dependency parses have been provided by the organizers, using state-of-the art parsers for the individual languages).</p><p>In contrast to the previous year, the evaluation data indicated which words were to be dealt with (for the SRL task). In other words, (predicate) disambiguation was still part of the task, whereas the identification of argument-bearing words was not. This decision was made to compensate for the significant differences between languages and between the annotation schemes used.</p><p>The "closed" and "open" challenges have been kept from last year as well; participants could have chosen one or both. In the closed challenge, systems had to be trained strictly with information contained in the given training corpus; in the open challenge, systems could have been developed making use of any kind of external tools and resources. This paper is organized as follows. Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges. A substantial portion of the paper (Section 3) is devoted to the description of the conversion and development of the data sets in the additional languages. Section 4 shows the main results of the submitted systems in the Joint and SRL-only tasks. Section 5 summarizes the approaches implemented by participants. Section 6 concludes the paper. In all sections, we will mention some of the differences between last year's and this year's tasks while keeping the text self-contained whenever possible; for details and observations on the English data, please refer to the overview paper of the CoNLL-2008 Shared Task <ref type="bibr" target="#b24">(Surdeanu et al., 2008)</ref> and to the references mentioned in the sections describing the other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>In this section we provide the definition of the shared task; after introducing the two challenges and the two tasks the participants were to choose, we continue with the format of the shared task data, followed by a description of the evaluation metrics used.</p><p>For three of the languages (Czech, English and German), out-of-domain data (OOD) have also been prepared for the final evaluation, following the same guidelines and formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Closed and Open Challenges</head><p>Similarly to the <ref type="bibr">CoNLL-2005 and</ref><ref type="bibr" target="#b24">CoNLL-2008</ref> shared tasks, this shared task evaluation is separated into two challenges:</p><p>Closed Challenge The aim of this challenge was to compare performance of the participating systems in a fair environment. Systems had to be built strictly with information contained in the given training corpus, and tuned with the development section. In addition, the lexical frame files (such as the Prop-Bank and NomBank for English, the valency dictionary PDT-Vallex for Czech etc.) were provided and may have been used. These restrictions mean that outside parsers (not trained by the participants' systems) could not be used. However, we did provide the output of a single, state-of-the-art dependency parser for each language so that participants could build a SRL-only system (using the provided parses as inputs) within the closed challenge (as opposed to the 2008 shared task).</p><p>Open Challenge Systems could have been developed making use of any kind of external tools and resources. The only condition was that such tools or resources must not have been developed with the annotations of the test set, both for the input and output annotations of the data. In this challenge, we were interested in learning methods which make use of any tools or resources that might improve the performance. The comparison of different systems in this setting may not be fair, and thus ranking of systems is not necessarily important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint and SRL-only tasks</head><p>In 2008, systems participating in the open challenge could have used state-of-the-art parsers for the syntactic dependency part of the task. This year, we have provided the output of these parsers for all the languages in an uniform way, thus allowing an orthogonal combination of the two tasks and the two challenges. For the SRL-only task, participants in the closed challenge simply had to use the provided parses only.</p><p>Despite the provisions for the SRL-only task, we are more interested in the approaches and results of the Joint task. Therefore, primary system ranking is provided for the Joint task while additional measures are computed for various combinations of parsers and SRL methods across the tasks and challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Format</head><p>The data format used in this shared task has been based on the CoNLL-2008 shared task, with some differences. The data follows these general rules:</p><p>• The files contain sentences separated by a blank line.</p><p>• A sentence consists of one or more tokens and the information for each token is represented on a separate line.</p><p>• A token consists of at least 14 fields. The fields are separated by one or more whitespace characters (spaces or tabs). Whitespace characters are not allowed within fields.</p><p>The data is thus a large table with whitespaceseparated fields (columns). The fields provided in the data are described in Table <ref type="table" target="#tab_1">1</ref>. They are identical for all languages, but they may differ in contents; for example, some fields might not be filled for all the languages provided (such as the FEAT or PFEAT fields).</p><p>For the SRL-only task, participants have been provided will all the data but the PRED and APREDs, which they were supposed to fill in with their correct values. However, they did not have to determine which tokens are predicates (or more precisely, which are the argument-bearing tokens), since they were marked by 'Y' in the FILLPRED field.</p><p>For the Joint task, participants could not (in addition to the PRED and APREDs) see the goldstandard nor the predicted syntactic dependencies (HEAD, PHEAD) and their labels (DEPREL, PDE-PREL). These syntactic dependencies were also to be filled by participants' systems.</p><p>In both tasks, participants have been free to use any other data (columns) provided, except the LEMMA, POS and FEAT columns (to get more 'realistic' results using only their automatically predicted variants PLEMMA, PPOS and PFEAT).</p><p>Besides the corpus proper, predicate dictionaries have been provided to participants in order to be able to properly match the predicates to the tokens in the corpus; their contents could have been used e.g. as features for the PRED/APREDs predictions (or even for the syntactic dependencies, i.e., for filling in the PHEAD and PDEPREL fields).</p><p>The system of filling-in the APREDs follows the 2008 pattern; for each argument-bearing token (predicate), a new APREDn column is created in the order in which the predicate token is encountered within the sentence (i.e., based on its ID seen as a numerical value). Then, for each token in the sentence, the value in the intersection of the APREDn column and the token row is either left unfilled (if the token is not an argument), or a predicateargument label(s) is(are) filled in.</p><p>The differences between the English-only 2008 task and this year's multilingual task can be briefly summarized as follows:</p><p>• only "split" 2 lemmas and forms have been provided in the English datasets (for the other languages, original tokenization from the respective treebanks has been used);</p><p>• rich morphological features have been added wherever available;</p><p>• syntactic dependencies by state-of-the-art parsers have been provided (for the SRL-only task);</p><p>• multiple semantic labels for a single token have been allowed (and properly evaluated) in the APREDs columns;</p><p>• predicates have been pre-identified and marked in both the training and test data;</p><p>• some of the fields (e.g. the APREDx) and values (ARG0 → A0 etc.) have been renamed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation Measures</head><p>It was required that participants submit results in all seven languages in the chosen task and in any of (or both) the challenges. Submission of out-of-domain data files has been optional. The main evaluation measure, according to which systems are primarily compared, is the Joint task,  closed challenge, Macro F 1 score. However, scores can also be computed for a number of other conditions:</p><p>• Task: Joint or SRL-only Joint task participants are also evaluated separately on the syntactic dependency task (labeled attachment score, LAS). Finally, systems competing in both tasks are compared on semantic role labeling alone, to assess the impact of the the joint parsing/SRL task compared to an SRL-only task on preparsed data. Finally, as an explanatory measure, precision and recall of the semantic labeling task have been computed and tabulated.</p><p>We have decided to omit several evaluation figures that were reported in previous years, such as the percentage of completely correct sentences ("Exact Match"), unlabeled scores, etc. With seven languages, two tasks (plus two challenges, and the IDD/OOD distinction), there are enough results to get lost even as it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Syntactic Dependency Measures</head><p>The LAS score is defined similarly as in the previous shared tasks, as the percentage of tokens for which a system has predicted the correct HEAD and DEPREL columns. The unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD regardless if the DEPREL is correct, has not been officially computed this year. No precision and recall measures are applicable, since all systems are supposed to output a single dependency with a single label (see also below the footnote to the description of the combined score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Semantic Labeling Measures</head><p>The semantic propositions are evaluated by converting them to semantic dependencies, i.e., we create n semantic dependencies from every predicate to its n arguments. These dependencies are labeled with the labels of the corresponding arguments. Additionally, we create a semantic dependency from each predicate to a virtual ROOT node. The latter dependencies are labeled with the predicate senses. This approach guarantees that the semantic dependency structure conceptually forms a single-rooted, connected (but not necessarily acyclic) graph. More importantly, this scoring strategy implies that if a system assigns the incorrect predicate sense, it still receives some points for the arguments correctly assigned. For example, for the correct proposition: verb.01: A0, A1, AM-TMP the system that generates the following output for the same argument tokens: verb.02: A0, A1, AM-LOC receives a labeled precision score of 2/4 because two out of four semantic dependencies are incorrect: the dependency to ROOT is labeled 02 instead of 01 and the dependency to the AM-TMP is incorrectly labeled AM-LOC. Using this strategy we compute precision, recall, and F 1 scores for semantic dependencies (labeled only).</p><p>For some languages (Czech, Japanese) there may be more than one label in a given argument position; for example, this happens in Czech in special cases of reciprocity when the same token serves as two or more arguments to the same predicate. The scorer takes this into account and considers such cases to be (as if) multiple predicate-argument relations for the computation of the evaluation measures.</p><p>For example, for the correct proposition:</p><p>v1f1: ACT|EFF, ADDR the system that generates the following output for the same argument tokens:</p><p>v1f1: ACT, ADDR|PAT receives a labeled precision score of 3/4 because the PAT is incorrect and labeled recall 3/4 because the EFF is missing (should the ACT|EFF and ADDR|PAT be taken as atomic values, the scores would then be zero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Combined Syntactic and Semantic Score</head><p>We combine the syntactic and semantic measures into one global measure using macro averaging. We compute macro precision and recall scores by averaging the labeled precision and recall for semantic dependencies with the LAS for syntactic dependencies:</p><formula xml:id="formula_0">3 LM P = W sem * LP sem + (1 − W sem ) * LAS (1) LM R = W sem * LR sem + (1 − W sem ) * LAS (2)</formula><p>where LM P is the labeled macro precision and LP sem is the labeled precision for semantic dependencies. Similarly, LM R is the labeled macro recall and LR sem is the labeled recall for semantic dependencies. W sem is the weight assigned to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The unification of the data formats for the various languages appeared to be a challenge in itself. We will briefly describe the processes of the conversion of the existing treebanks in the seven languages of the CoNLL-2009 shared task. In many instances, the original treebanks had to be not only converted format-wise, but also merged with other resources in order to generate useful training and testing data that fit the task description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Input Corpora</head><p>The data used as the input for the transformations aimed at arriving at the data contents and format described in Sect. 2.3 are described in <ref type="bibr" target="#b25">(Taulé et al., 2008)</ref>, <ref type="bibr" target="#b28">(Xue and Palmer, 2009)</ref>, <ref type="bibr" target="#b10">(Hajič et al., 2006)</ref>, <ref type="bibr" target="#b24">(Surdeanu et al., 2008)</ref>, <ref type="bibr" target="#b2">(Burchardt et al., 2006)</ref> and <ref type="bibr" target="#b11">(Kawahara et al., 2002)</ref>.</p><p>In the subsequent sections, the procedures for the data conversion for the individual languages are described. The data has been collected by the main organization site and checked for format errors, and repackaged for distribution.</p><p>There were three packages of the data distributed to the participants: Trial, Training plus Development, and Evaluation. The Trial data were rather small, just to give the feeling of the format and languages involved. A visual representation of the Trial data was also created to make understanding of the data easier. Any data in the same format can be transformed and displayed in the Tree Editor TrEd 5 <ref type="bibr">(Pajas andŠtěpánek, 2008)</ref> with the CoNLL 2009 Shared Task extension that can be installed from within the editor. A sample visualization of an English sentence after its conversion to the shared task format (Sect. 2.3) is in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>Due to licensing requirements, every package of the data had to be split into two portions. One portion (Catalan, German, Japanese, and Spanish data) was published on the task's webpage for down-  load, the other portion (Czech, English, and Chinese data) was invoiced and distributed by the Linguistic Data Consortium under a special agreement free of charge. Distribution of the Evaluation package was a bit more complicated, because there were two types of the packages -one for the Joint task and one for the SRL-only task. Every participant had to subscribe to one of the two tasks; subsequently, they obtained the appropriate data (again, from the webpage and LDC).</p><p>Prior to release, each data file was checked to eliminate errors. The following test were carried out:</p><p>• For every sentence, number of PREDs rows matches the number of APREDs columns.</p><p>• The first line of each file is never empty, while the last line always is.</p><p>• The first character on a non-empty line is always a digit, the last one is never a whitespace.</p><p>• The number of empty lines (i.e. the number of sentences) equals the number of lines beginning with "1".</p><p>• The data contain no spaces nor double tabs.</p><p>Some statistics on the data can be seen in Tables 2, 3 and 4. Whereas the training sizes of the data have not been that different as they were e.g. for the 2007 shared task on multilingual dependency parsing <ref type="bibr" target="#b18">(Nivre et al., 2007)</ref> 6 , substantial differences existed in the distribution of the predicates and arguments, the input features, the out-of-vocabulary rates, and other statistical characteristics of the data.</p><p>Data sizes have been relatively uniform in all the datasets, with Japanese having the smallest dataset containing data for SRL annotation training. To compensate at least for the dependency parsing part, an additional, large Japanese corpus with syntactic dependency annotation has been provided.</p><p>The average sentence length, the vocabulary sizes for FORM and LEMMA fields and the OOV rates characterize quite naturally the properties of the respective languages (in the domain of the training and evaluation data). It is no surprise that the FORM OOV rate is the highest for Czech, a highly inflectional language, and that the LEMMA OOV rate is the highest for German (as a consequence of keeping compounds as a single lemma). The other statistics also reflect (to a large extent) the annotation specification and conventions used for the original treebanks and/or the result of the conversion process to the unified CoNLL-2009 Shared Task format.</p><p>Starting with the POS and FEAT fields, it can be seen that Catalan, Czech and Spanish use only the 12 major part-of-speech categories as values of the POS field (with richly populated FEAT field); English and Chinese are the opposite extreme, disregarding the use of the FEAT field completely and coding everything as a POS value. While for Chinese this is quite understandable, English follows the PTB tradition in this respect. German and Japanese use relatively rich set of values in both the POS and FEAT fields.</p><p>For the dependency relations (DEPREL), all the languages use a similarly-sized set except for Japanese, which only encodes the distinction between a root and a dependent node (and some infrequent special ones).</p><p>Evaluation data are over 10% of the size of the training data for Catalan, Chinese, Czech, Japanese and Spanish and roughly 5% for English and German.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows the distribution of the five most frequent dependency relations (determined as part of the subtask of syntactic parsing). With the exception of Japanese, which essentially does not label dependency relations at this level, all the other languages show little difference in this distribution. For example, the unconditioned probability of "subjects" is almost the same for all the six other languages (between 6 and 8 percent). The probability mass covered by the first five most frequent DEPRELs is also almost the same (again, except for Japanese), sug-gesting that the labeling task might have similar difficulty <ref type="bibr">7</ref> . The most skewed one is for Czech (after Japanese).</p><p>Table <ref type="table" target="#tab_6">4</ref> shows similar statistics for the argument labels (PRED/APREDs); it also adds the average number of arguments per "predicate" token, since this is part of the SRL task 8 . It is apparent from the comparison of the "Total" rows in this table and Table 3 that the first five argument labels cover more that their syntactic counterparts. For example, the arguments A0-A4 account for all but 3% of all arguments labels, whereas Spanish and Catalan have much more rich set of argument labels, with a high entropy of the most-frequent-label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Catalan and Spanish</head><p>The Catalan and Spanish datasets <ref type="bibr" target="#b25">(Taulé et al., 2008)</ref> were generated from the AnCora corpora 9 through an automatic conversion process from a constituentbased formalism to dependencies <ref type="bibr" target="#b4">(Civit et al., 2006)</ref>.</p><p>AnCora corpora contain about half million words for Catalan and Spanish annotated with syntactic and semantic information. Text sources for the Catalan corpus are EFE news agency (∼75Kw), ACN Catalan news agency (∼225Kw), and 'El Periódico' newspaper (∼200Kw). The Spanish corpus comes from the Lexesp Spanish balanced corpus (∼75Kw), the EFE Spanish news agency (∼225Kw), and the Spanish version of 'El Periódico' (∼200Kw). The subset from 'El Periódico' corresponds to the same news in Catalan and Spanish, spanning from January to December 2000.</p><p>Linguistic annotation is the same in both languages and includes: PoS tags with morphological features (gender, number, person, etc.), lemmatization, syntactic dependencies (syntactic functions), semantic dependencies (arguments and thematic roles), named entities and predicate semantic classes (Lexical Semantic Structure, LSS). Tag sets are shared by the two languages.</p><p>If we take into account the complete PoS tags,    An incremental process guided the annotation of AnCora, since semantics depends on morphosyntax, and syntax relies on morphology. This procedure made it possible to check, correct, and complete the previous annotations, thus guaranteeing the final quality of the corpora and minimizing the error rate. The annotation process was carried out sequentially from lower to upper layers of linguistic description. All resulting layers are independent of each other, thus making easier the data management. The initial annotation was performed manually for syntax, semiautomatically in the case of arguments and thematic roles, and fully automatically for PoS .</p><p>The Catalan and Spanish AnCora corpora were straightforwardly translated into the CoNLL-2009 shared task formatting (information about named entities was skipped in this process). The resulting Catalan corpus (including training, development and test partitions) contains 16,786 sentences with an average length of 29.59 lexical tokens per sentence. Long sentences abound in this corpus. For instance, 10.73% of the sentences are longer than 50 tokens, and 4.42% are longer than 60. The corpus contains 47,537 annotated predicates (2.83 predicates per sentence, on average) with 107,171 arguments (2.25 arguments per predicate, on average). From the latter, 73.89% correspond to core arguments and 26.11% to adjuncts. Numbers for the Spanish corpus are comparable in all aspects: 17,709 sentences with 29.84 lexical tokens on average (11.58% of the sentences longer than 50 tokens, 4.07% longer than 60); 54,075 predicates (3.05 per sentence, on average) and 122,478 arguments (2.26 per predicate, on average); 73.34% core arguments and 26.66% adjuncts.</p><p>The following are important features of the Catalan and Spanish corpora in the CoNLL-2009 shared task setting: (1) all dependency trees are projective;</p><p>(2) no word can be the argument of more than one predicate in a sentence; (3) semantic dependencies completely match syntactic dependency structures (i.e., no new edges are introduced by the semantic structure); (4) only verbal predicates are annotated (with exceptional cases referring to words that can be adjectives and past participles); (5) the corpus is segmented so multi-words, named entities, temporal expressions, compounds, etc. are grouped together; and (6) segmentation also accounts for elliptical pronouns (there are marked as empty lexical tokens '_' with a pronoun POS tag).</p><p>Finally, the predicted columns (PLEMMA, PPOS, and PFEAT) have been generated with the FreeLing Open source suite of Language Analyzers 10 . Accuracy in PLEMMA and PPOS columns is above 95% for the two languages. PHEAD and PDEPREL columns have been generated using MaltParser 11 . Parsing accuracy (LAS) is above 86% for the the two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chinese</head><p>The Chinese Corpus for the 2009 CoNLL Shared Task was generated by merging the Chinese Treebank <ref type="bibr" target="#b29">(Xue et al., 2005)</ref> and the Chinese Proposition Bank <ref type="bibr" target="#b28">(Xue and Palmer, 2009)</ref> and then converting the constituent structure to a dependency formalism as specified in the CoNLL Shared Task. The Chinese data used in the shared task is based on Chinese Treebank 6.0 and the Chinese Proposition Bank 2.0, both of which are publicly available via the Linguistic Data Consortium.</p><p>The Chinese Treebank Project originated at Penn and was later moved to University of Colorado at Boulder. Now it is the process of being to moved to Brandeis University. The data sources of the Chinese Treebank range from Xinhua newswire (mainland China), Hong Kong news, and Sinorama Magazine (Taiwan). More recently under DARPA GALE funding it has been expanded to include broadcast news, broadcast conversation, news groups and web log data. It currently has over one million words and is fully segmented, POS-tagged and annotated with phrase structure. The version of the Chinese Treebank used in this shared task, CTB 6.0, includes newswire, magazine articles, and transcribed broadcast news <ref type="bibr">12</ref> . The training set has 609,060 tokens, the development set has 49,620 tokens, and the test set has 73,153 tokens.</p><p>The Chinese Proposition Bank adds a layer of semantic annotation to the syntactic parses in the Chinese Treebank. This layer of semantic annotation mainly deals with the predicate-argument structure of Chinese verbs and their nominalizations. Each major sense (called frameset) of a predicate takes a number of core arguments annotated with numerical labels Arg0 through Arg5 which are defined in a predicate-specific manner. The Chinese Proposition Bank also annotates adjunctive arguments such as locative, temporal and manner modifiers of the predicate. The version of the Chinese Propbank used in this CoNLL Shared Task is CPB 2.0, but nominal predicates are excluded because the annotation is incomplete.</p><p>Since the Chinese Treebank is annotated with constituent structures, the conversion and merging procedure converts the constituent structures to dependencies by identifying the head for each constituent in a parse tree and making its sisters its dependents. The Chinese Propbank pointers are then shifted from the entire constituent to the head of that constituent. The conversion procedure identifies the head by first exploiting the structural information in the syntactic parse and detecting six broad categories of syntactic relations that hold between the head and its dependents <ref type="bibr">(predication, modification, complementation, coordination, auxiliary, and flat)</ref> and then designating the head based on these relations. In particular, the first conjunct of a coordina-tion structure is designated as the head and the heads of the other conjuncts are the conjunctions preceding them. The conjunctions all "modify" the first conjunct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Czech</head><p>For the training, development and evaluation data, Prague Dependency Treebank 2.0 was used <ref type="bibr" target="#b10">(Hajič et al., 2006)</ref>. For the out-of-domain evaluation data, part of the Czech side of the Prague Czech-English Dependency Treebank (version 2, under construction) was used 13 , see also <ref type="bibr">(Čmejrek et al., 2004)</ref>. For the OOD data, no manual annotation of LEMMA, POS, and FEAT existed, so the predicted values were used. The same conversion procedure has been applied to both sources.</p><p>The FORM column was created from the form element of the morphological layer, not from the "token" from the word-form layer. Therefore, most typos, errors in word segmentation and tokenization are corrected and numerals are normalized.</p><p>The LEMMA column was created from the lemma element of the morphological layer. Only the initial string of the element was used, so there is no distinction between homonyms. However, some components of the detailed lemma explanation were incorporated into the FEAT column (see below).</p><p>The POS column was created form the morphological tag element, its first character more precisely.</p><p>The FEAT column was created from the remaining characters of the tag element. In addition, the special feature "Sem" corresponds to a semantic feature of the lemma.</p><p>For the HEAD and DEPREL columns, the PDT analytical layer was used. The DEPREL was taken from the analytic function (the afun node attribtue). There are 27 possible values for afun element: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv, Atv, AtvV, Coord, Apos, ExD, and a number of auxiliary and "double-function" labels. The first nine of these are the "most interesting" from the point of view of the shared task, since they relate to semantics more closely than the rest (at least from the linguistic point of view). The HEAD is a pointer to its parent, which means the PDT's ord attribute (within-sentence ID / word position number) of the parent. If a node is a member of a coordination or apposition (is_member element), its DEPREL obtains the _M suffix. The parenthesis annotation (is_parenthesis_root element) was ignored.</p><p>The PRED and APREDs columns were created from the tectogrammatical layer of PDT 2.0 and the valency lexicon PDT-Vallex according to the following rules:</p><p>• Every line corresponding to an analytical node referenced by a lexical reference (a/lex.rf) from the tectogrammatical layer has a PRED value filled. If the referring non-generated tectogrammatical node (is_generated not equal to 1) has a valency frame assigned (val_frame.rf), the value of PRED is the identifier of the frame. Otherwise, it is set to the same value as the LEMMA column.</p><p>• For every tectogrammatical node, a corresponding analytical node is searched for:</p><p>1. If the tectogrammatical node is not generated and has a lexical reference (a/lex.rf), the referenced node is taken. 2. Otherwise, if the tectogrammatical node has a coreference (coref_text.rf or coref_gram.rf) or complement reference (compl.rf) to a node that has an analytical node assigned (by 1. or 2.), the assigned node is taken.</p><p>APRED columns are filled with respect to the following correspondence: for a tectogrammatical node P and its effective child C with functor F, the column for P's corresponding analytical node at the row for C's corresponding analytical node is filled with F. Some nodes can thus have several functors in one APRED column, separated by a vertical bar (see Sect. 2.4.2).</p><p>PLEMMA, PPOS and PFEAT were generated by the (cross-trained) morphological tagger <ref type="bibr">MORCE (Spoustová et al., 2009)</ref>, which gives full combined accuracy (PLEMMA+PPOS+PFEAT) slightly under 96%.</p><p>PHEAD and PDEPREL were generated by the (cross-trained) MST parser for Czech (Chu-Liu/Edmonds algorithm, <ref type="bibr" target="#b16">(McDonald et al., 2005)</ref>), which has typical dependency accuracy around 85%.</p><p>The valency lexicon, converted from <ref type="bibr" target="#b9">(Hajič et al., 2003)</ref>, has four columns:</p><p>1. lemma (can occur several times in the lexicon, with different frames)</p><p>2. frame identifier (as found in the PRED column)</p><p>3. list of space-separated actants and obligatory members of the frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">example(s)</head><p>The source of the out-of-domain data uses an extended valency lexicon (because of out-ofvocabulary entries). For simplicity, the extended lexicon was not provided; instead, such words were not marked as predicates in the OOD data (their FILLPRED was set to '_') and thus not evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">English</head><p>The English corpus is almost identical to the corpus used in the closed challenge in the CoNLL-2008 shared task evaluation <ref type="bibr" target="#b24">(Surdeanu et al., 2008)</ref>. This corpus was generated through a process that merges several input corpora and converts them from the constituent-based formalism to dependencies. The following corpora were used as input to the merging procedure:</p><p>• Penn Treebank 3 -The Penn Treebank 3 corpus <ref type="bibr" target="#b13">(Marcus et al., 1994)</ref>  • Proposition Bank I (PropBank) -The Prop-Bank annotation <ref type="bibr" target="#b21">(Palmer et al., 2005)</ref> classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.).</p><p>• NomBank -NomBank annotation <ref type="bibr" target="#b17">(Meyers et al., 2004)</ref> uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun and verb argument structure; differences in treatment of nouns and verbs in the Penn Treebank; and differences in the sophistication of previous research about noun and verb argument structure.</p><p>Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ArgM.</p><p>The complete merging process and the conversion from the constituent representation to dependencies is detailed in <ref type="bibr" target="#b24">(Surdeanu et al., 2008)</ref>.</p><p>The main difference between the 2008 and 2009 version of the corpora is the generation of word lemmas. In the 2008 version the only lemmas provided were predicted using the built-in lemmatizer in WordNet <ref type="bibr">(Fellbaum, 1998)</ref> based on the most frequent sense for the form and the predicted part-ofspeech tag. These lemmas are listed in the 2009 corpus under the PLEMMA column. The LEMMA column in the 2009 version of the corpus contains lemmas generated using the same algorithm but using the correct Treebank part-of-speech tags. Additionally, the PHEAD and PDEPREL columns were generated using MaltParser 14 , similarly to the open challenge corpus in the CoNLL 2008 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">German</head><p>The German in-domain dataset is based on the annotated verb instances of the SALSA corpus <ref type="bibr" target="#b2">(Burchardt et al., 2006)</ref>, a total of around 40k sentences <ref type="bibr">15</ref> . SALSA provides manual semantic role annotation on top of the syntactically annotated TIGER newspaper corpus, one of the standard German treebanks. The original SALSA corpus uses semantic roles in the FrameNet paradigm. We constructed mappings between FrameNet frame elements and PropBank argument positions at the level of frame-predicate pairs semi-automatically. For the frame elements of each frame-predicate pair, we first identified the semantically defined PropBank Arg-0 and Arg-1 positions. To do so, we annotated a small number of very abstract frame elements with these labels (Agent, Actor, Communicator as Arg-0, and Theme, Effect, Message as Arg-1) and percolated these labels through the FrameNet hierarchy, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles.</p><p>The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation <ref type="bibr" target="#b19">(Pado and Lapata, 2005)</ref>. It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation.</p><p>For both datasets, we converted the syntactic TIGER <ref type="bibr" target="#b1">(Brants et al., 2002)</ref> representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con-version of person names and coordinations) were made to achieve better consistency with datasets of other languages. Since the TIGER annotation allows non-contiguous constituents, the resulting dependencies can be non-projective. Secondary edges were discarded in the conversion. As for the automatically constructed features, we used Tree-Tagger <ref type="bibr" target="#b22">(Schmid, 1994)</ref> to produce the PLEMMA and PPOS columns, and the Morphisto morphology <ref type="bibr" target="#b30">(Zielinski and Simon, 2008)</ref> for PFEAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Japanese</head><p>For Japanese, we used the Kyoto University Text Corpus <ref type="bibr" target="#b11">(Kawahara et al., 2002)</ref>, which consists of approximately 40k sentences taken from Mainichi Newspapers. Out of them, approximately 5k sentences are annotated with syntactic and semantic dependencies, and are used the training, development and test data of this year's shared task. The remaining sentences, which are annotated with only syntactic dependencies, are provided for the training corpus of syntactic dependency parsers.</p><p>This corpus adopts a dependency structure representation, and thus the conversion to the CoNLL-2009 format was relatively straightforward. However, since the original dependencies are annotated on the basis of phrases (Japanese bunsetsu), we needed to automatically convert the original annotations to word-based ones using several criteria. We used the following basic criteria: the words except the last word in a phrase depend on the next (right) word, and the last word in a phrase basically depends on the head word of the governing phrase.</p><p>Semantic dependencies are annotated for both verbal predicates and nominal predicates. The semantic roles (APRED columns) consist of 41 surface cases, many of which are case-marking postpositions such as ga (nominative), wo (accusative) and ni (dative). Semantic frame discrimination is not annotated, and so the PRED column is the same as the LEMMA column. The original corpus contains coreference annotations and inter-sentential semantic dependencies, such as inter-sentential zero pronouns and bridging references, but we did not use these annotations, which are not the target of this year's shared task.</p><p>To produce the PLEMMA, PPOS and PFEAT columns, we used the morphological analyzer JU-MAN <ref type="bibr">16</ref> and the dependency and case structure analyzer KNP 17 . To produce the PHEAD and PDE-PREL columns, we used the MSTParser 18 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions and Results</head><p>Participants uploaded the results through the shared task website, and the official evaluation was performed centrally. Feedback was provided if any formal problems were encountered (for a list of checks, see the previous section). One submission had to be rejected because only English results were provided. After the evaluation period had passed, the results were anonymized and published on the web.</p><p>A total of 20 systems participated in the closed challenge; 13 of them in the Joint task and seven in the SRL-only task. Two systems participated in the open challenge (Joint task). Moreover, 17 systems provided output in the out-of-domain part of the task (11 in the OOD Joint task and six in the OOD SRLonly task).</p><p>The main results for the core task -the Joint task (dependency syntax and semantic relations) in the context of the closed challenge -are summarized and ranked in Table <ref type="table">5</ref>.</p><p>The largest number of systems can be compared in the SRL results table <ref type="table">(Table 6</ref>), where all the systems have been evaluated solely on the SRL performance regardless whether they participated in the Joint or SRL-only task. However, since the results might have been influenced by the supplied parser, separate ranking is provided for both types of the systems.</p><p>Additional breakdown of the results (open challenge, precision and recall tables for the semantic labeling task, etc.) are available from the CoNLL-2009 Shared Task website 19 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approaches</head><p>Table <ref type="table" target="#tab_9">7</ref> summarizes the properties of the systems that participated in the closed the open challenges.</p><p>Table <ref type="table">6</ref>: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name (first name added only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the semantic labeled F 1 score (closed challenge). Bold numbers denote the best result for a given language. Separate ranking is provided for SRL-only systems.</p><p>The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly.</p><p>It is perhaps not surprising that most of the observations from the 2008 shared task still hold; namely, the best systems overall do not use joint learning or optimization (the best such system was placed third in the Joint task, and there were only four systems where the learning methodology can be considered "joint").</p><p>Therefore, most of the observations and conclusions from 2008 shared task hold as well for the current results. For details, we will leave it to the reader to interpret the architectures and methods </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comb.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference c</head><p>Arch.  columns filled in. The systems are sorted by the semantic labeled F1 score averaged over all the languages (same as in Table <ref type="table">6</ref>). Only the systems that have a corresponding paper in the proceedings are included. Acronyms used: D -syntactic dependencies, P -predicate, A -argument, I -identification, C -classification. Overall arch. stands for the complete system architecture; D Arch. stands for the architecture of the syntactic parser; D Comb. indicates if the final parser output was generated using parser combination; D Inference stands for the type of inference used for syntactic parsing; PA Arch. stands the type of architecture used for PAIC; PA Comb. indicates if the PA output was generated through system combination; PA Inference stands for the the type of inference used for PAIC; Joint Learning/Opt. indicates if some form of joint learning or optimization was implemented for the syntactic + semantic global task; ML Methods lists the ML methods used throughout the complete system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comb.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>a Authors of two systems: "Brown" and "Lin" didn't submit a paper, so their systems' architectures are unknown. by <ref type="bibr" target="#b3">Carreras (2007)</ref>, MST E by <ref type="bibr" target="#b6">Eisner (2000)</ref>, MST HOE = MST E with higher-order features (siblings + all grandchildren).</p><formula xml:id="formula_1">d</formula><p>The system unifies the syntactic and semantic labels into one label, and trains classifiers over them. It is thus difficult to split the system characteristic into a "D"/"PA" part.</p><p>when comparing Table <ref type="table" target="#tab_9">7</ref> with the Tables <ref type="table">5 and 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This year's task has been demanding in several respects, but certainly the most difficulty came from the fact that participants had to tackle all seven languages. It is encouraging that despite this added affort the number of participating systems has been almost the same as last year <ref type="bibr">(20 vs. 22 in 2008)</ref>.</p><p>There are several positive outcomes from this year's enterprise:</p><p>• we have prepared a unified format and data for several very different lanaguages, as a basis for possible extensions towards other languages and unified treatment of syntactic depenndecies and semantic role labeling across natural languages;</p><p>• 20 participants have produced SRL results for all seven languages, using several different methods, giving hope for a combined system with even substantially better performance;</p><p>• initial results have been provided for three languages on out-of-domain data (being in fact quite close to the in-domain results).</p><p>Only four systems tried to apply what can be described as joint learning for the syntactic and semantic parts of the task. <ref type="bibr">(Morante et al., 2009</ref>) use a true joint learning formulation that phrases syntacticosemantic parsing as a series of classification where the class labels are concatenations of syntactic and semantic edge labels. They predict (a), the set of syntactico-semantic edge labels for each pair of tokens; (b), the set of incoming syntactico-semantic edge labels for each individual token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. <ref type="bibr" target="#b12">(Lluís et al., 2009)</ref> present a joint approach based on an extension of Eisner's parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan and Spanish. <ref type="bibr" target="#b8">(Gesmundo et al., 2009)</ref> use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. <ref type="bibr" target="#b5">(Dai et al., 2009</ref>) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks -syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For example, in the iterative steps, semantic information becomes available as features to syntactic parsing, so on and so forth.</p><p>Despite these results, it is still not clear whether joint learning has a significant advantage over other approaches (and if yes, then for what languages). It is thus necessary to carefully plan the next shared tasks; it might be advantageous to bring up a similar task in the future once again, and/or couple it with selected application(s). There, (we hope) the benefits of the dependency representation combined with semantic roles the way we have formulated it in 2008 and 2009 will really show up.</p><p>Tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Challenge: open or closed • Domain: in-domain data (IDD, separated from training corpus) or out-of-domain data (OOD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Visualisation of the English sentence "And sometimes a reputable charity with a houshold name gets used and doesn't even know it." (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic dependencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2 nd line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that multiple semantic dependencies (e.g., there are four for charity: A0 ← know, A1 ← gets, A1 ← used, A1 ← name) and self-dependencies (name) appear in this sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are</cell></row><row><cell>not provided in the evaluation data; for the Joint task, columns 9-12 are also empty in the evaluation data.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All evaluation data statistics are derived from the in-domain evaluation data.</figDesc><table><row><cell>DEPREL</cell><cell cols="2">Catalan</cell><cell cols="2">Chinese</cell><cell>Czech</cell><cell cols="2">English</cell><cell>German</cell><cell>Japanese</cell><cell>Spanish</cell></row><row><cell></cell><cell>sn</cell><cell cols="7">0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D</cell><cell>0.93 sn</cell><cell>0.16</cell></row><row><cell></cell><cell cols="5">spec 0.15 NMOD 0.14 AuxP 0.10 P</cell><cell></cell><cell cols="2">0.11 PUNC 0.14 ROOT 0.04 spec 0.15</cell></row><row><cell>Labels</cell><cell>f</cell><cell cols="7">0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P</cell><cell>0.03 f</cell><cell>0.12</cell></row><row><cell></cell><cell>sp</cell><cell cols="5">0.09 UNK 0.09 Obj 0.07 SBJ</cell><cell cols="2">0.07 SB</cell><cell>0.07 A</cell><cell>0.00 sp</cell><cell>0.08</cell></row><row><cell></cell><cell>suj</cell><cell cols="2">0.07 SBJ</cell><cell cols="3">0.08 Sb 0.06 OBJ</cell><cell cols="2">0.06 ROOT 0.06 I</cell><cell>0.00 suj</cell><cell>0.08</cell></row><row><cell>Total</cell><cell></cell><cell>0.58</cell><cell></cell><cell>0.62</cell><cell>0.59</cell><cell>0.61</cell><cell></cell><cell>0.70</cell><cell>1.00</cell><cell>0.59</cell></row></table><note>a There were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and vocabulary statistics are computed using this larger dataset. b Percentage of tokens with FILLPRED='Y'. c Percentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data. d OOV percentage for in-domain/out-of-domain data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009 Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.</figDesc><table><row><cell>APRED</cell><cell>Catalan</cell><cell>Chinese</cell><cell>Czech</cell><cell>English</cell><cell cols="2">German Japanese</cell><cell>Spanish</cell></row><row><cell></cell><cell>arg1-pat 0.22</cell><cell>A1 0.30</cell><cell>RSTR 0.30</cell><cell>A1 0.37</cell><cell cols="2">A0 0.40 GA 0.33</cell><cell>arg1-pat 0.20</cell></row><row><cell></cell><cell>arg0-agt 0.18</cell><cell>A0 0.27</cell><cell>PAT 0.18</cell><cell>A0 0.25</cell><cell cols="2">A1 0.39 WO 0.15</cell><cell>arg0-agt 0.19</cell></row><row><cell>Labels</cell><cell cols="3">arg1-tem 0.15 ADV 0.20 ACT 0.17</cell><cell>A2 0.12</cell><cell cols="2">A2 0.12 NO 0.15</cell><cell>arg1-tem 0.15</cell></row><row><cell></cell><cell cols="2">argM-tmp 0.08 TMP 0.07</cell><cell>APP 0.06</cell><cell cols="3">AM-TMP 0.06 A3 0.06 NI 0.09</cell><cell>arg2-atr 0.08</cell></row><row><cell></cell><cell>arg2-atr 0.08</cell><cell>DIS 0.04</cell><cell cols="5">LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08</cell></row><row><cell>Total</cell><cell>0.71</cell><cell>0.91</cell><cell>0.75</cell><cell>0.83</cell><cell>0.97</cell><cell>0.78</cell><cell>0.70</cell></row><row><cell>Avg.</cell><cell>2.25</cell><cell>2.26</cell><cell>0.88</cell><cell>2.20</cell><cell>1.97</cell><cell>1.71</cell><cell>2.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009 Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The "Avg." line shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by FILLPRED='Y').AnCora has 280 different labels. Considering only the main syntactic categories, the tag set is reduced to 47 tags. The syntactic tag set consists of 50 different syntactic functions. Regarding semantic arguments, we distinguish Arg0, Arg1, Arg2, Arg3, Arg4, ArgM, and ArgL. The first five tags are numbered from less to more obliqueness with respect to the verb, ArgM corresponds to adjuncts.</figDesc><table><row><cell>The</cell></row></table><note>(Final State), and ADV (Adverbial). Each argument position can map onto specific thematic roles. By way of example, Arg1 can be PAT, TEM or EXT. For Named Entities, we distinguish six types: Organization, Person, Location, Date, Number, and Others.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>NAME dependency from Mary to Smith given the NE mention Mary Smith.</figDesc><table><row><cell>dependencies between NE tokens, e.g., we cre-</cell></row><row><cell>ate a</cell></row><row><cell>consists of hand-</cell></row><row><cell>coded parses of the Wall Street Journal (test,</cell></row><row><cell>development and training) and a small subset</cell></row><row><cell>of the Brown corpus (W. N. Francis and H.</cell></row><row><cell>Kucera, 1964) (test only).</cell></row><row><cell>• BBN Pronoun Coreference and Entity Type Corpus -BBN's NE annotation of the Wall</cell></row><row><cell>Street Journal corpus (Weischedel and Brun-</cell></row><row><cell>stein, 2005) takes the form of SGML inline</cell></row><row><cell>markup of text, tokenized to be completely</cell></row><row><cell>compatible with the Penn Treebank annotation.</cell></row><row><cell>For the CoNLL-2008 shared task evaluation,</cell></row><row><cell>this corpus was extended by the task organizers</cell></row><row><cell>to cover the subset of the Brown corpus used as</cell></row><row><cell>a secondary testing dataset. From this corpus</cell></row><row><cell>we only used NE boundaries to derive NAME</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Learning/Opt. Methods</head><label></label><figDesc></figDesc><table><row><cell>ME</cell><cell></cell><cell>L2-regularized</cell><cell>lin. regression</cell><cell>ME</cell><cell>SVM, ME</cell><cell>ISBN</cell><cell></cell><cell>MIRA</cell><cell>SVM (MIRA)</cell><cell>perceptron</cell><cell>ME</cell><cell>MIRA, ME</cell><cell>ME</cell><cell>SVM, kNN, ME</cell><cell>MBL</cell><cell>CRF</cell><cell></cell><cell>Avg. Perceptron</cell><cell>SVM</cell><cell>SVM (Malt), ME</cell><cell>cooccurrence</cell></row><row><cell>(SRL-only)</cell><cell></cell><cell>(SRL-only)</cell><cell></cell><cell>no</cell><cell>no</cell><cell>synchronized</cell><cell>derivation</cell><cell>(SRL-only)</cell><cell>no</cell><cell>no</cell><cell>iterative</cell><cell>no</cell><cell>no</cell><cell>(SRL-only)</cell><cell>unified labels</cell><cell>(SRL-only)</cell><cell></cell><cell>yes, MST E</cell><cell>(SRL-only)</cell><cell>no</cell><cell>no</cell></row><row><cell>greedy/global</cell><cell>search</cell><cell>beam search +</cell><cell>reranking</cell><cell>greedy (?)</cell><cell>ILP</cell><cell>beam search</cell><cell></cell><cell>Cutting Plane</cell><cell>greedy</cell><cell>n-best relax.</cell><cell>prob</cell><cell>classification</cell><cell>greedy</cell><cell>greedy</cell><cell>reranking</cell><cell>CRF</cell><cell></cell><cell>MST E</cell><cell>greedy</cell><cell>greedy</cell><cell>greedy</cell></row><row><cell>(SRL-only) class no</cell><cell></cell><cell>(SRL-only) class no</cell><cell></cell><cell>MST CL/E class no</cell><cell>MST HOE class no</cell><cell>beam search trans no</cell><cell></cell><cell>(SRL-only) Markov LN no</cell><cell>MST C +rearrange class no</cell><cell>MST C class no</cell><cell>MST C class no</cell><cell>MST E class no</cell><cell>MST CL/E ,MST E class no</cell><cell>(SRL-only) class no</cell><cell>reranking class no</cell><cell>(SRL-only) class no</cell><cell></cell><cell>MST E graph no</cell><cell>(SRL-only) class no</cell><cell>greedy class no</cell><cell>greedy with class no</cell><cell>heuristics</cell></row><row><cell>(SRL-only)</cell><cell></cell><cell>(SRL-only)</cell><cell></cell><cell>partially</cell><cell>no</cell><cell>no</cell><cell></cell><cell>(SRL-only)</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>for each lang.</cell><cell>(SRL-only)</cell><cell>no</cell><cell>(SRL-only)</cell><cell></cell><cell>no</cell><cell>(SRL-only)</cell><cell>no</cell><cell>no</cell></row><row><cell>(SRL-only)</cell><cell></cell><cell>(SRL-only)</cell><cell></cell><cell>graph</cell><cell>graph</cell><cell>generative,</cell><cell>trans</cell><cell>(SRL-only)</cell><cell>graph</cell><cell>graph</cell><cell>graph</cell><cell>graph</cell><cell>graph</cell><cell>(SRL-only)</cell><cell>class</cell><cell>(SRL-only)</cell><cell></cell><cell>graph</cell><cell>(SRL-only)</cell><cell>trans</cell><cell>trans</cell></row><row><cell>Zhao PAIC</cell><cell></cell><cell>Nugues (PC+AI+AC) + AIC</cell><cell></cell><cell>Chen P + PC + AI + AC</cell><cell>Che D+PC+AIC</cell><cell>Merlo DPAIC+D</cell><cell></cell><cell>Meza-Ruiz PAIC</cell><cell>Bohnet D + AI + AC + PC</cell><cell>Asahara D + PIC + AIC</cell><cell>Dai D + PC + AC</cell><cell>Zhang D + AI + AC + PC</cell><cell cols="2">Lu Li D + (PC || AIC) Baoli Li PC + AIC</cell><cell>Vallejo d [D+P+A]C + DI</cell><cell>Moreau D + PI + Clustering +</cell><cell>AI + AC</cell><cell>Lluís D+DAIC+PC</cell><cell>Täckström D + PI + AI + AC +</cell><cell>Constraint Satisfaction</cell><cell>Ren D + PC + AIC</cell><cell>Zeman DI+DC+PC+AI+AC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Summary of system architectures for the CoNLL-2009 shared task; all systems are included. SRL-only systems do not have the D columns and the Joint Learing/Opt.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The symbol + indicates sequential processing (otherwise, parallel/joint). The || means that several different architectures spanning multiple subtasks ran in parallel.</figDesc><table><row><cell>as used by McDonald (2005), MST C</cell></row><row><cell>CL/E</cell></row></table><note>b c MST</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">There are some format changes and deviations from the 2008 task data specification; seeSect. 2.3   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Splitting of forms and lemmas in English has been introduced in the 2008 shared task to match the tokenization convention for the arguments in NomBank.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We can do this because the LAS for syntactic dependencies is a special case of precision and recall, where the predicted number of dependencies is equal to the number of gold dependencies. semantic task.4  The macro labeled F 1 score, which was used for the ranking of the participating systems, is computed as the harmonic mean of LM P and LM R.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We assign equal weight to the two tasks, i.e., Wsem = 0.5. 5 http://ufal.mff.cuni.cz/ ∼ pajas/tred</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://nextens.uvt.nl/depparse-wiki/ DataOverview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Yes, this is overgeneralization since this distribution does not condition on the features, dependencies etc. But as a rough measure, it often correlates well with the results.8 A number below 1 means there are some argument-bearing words (often nouns) which have no arguments in the particular sentence in which they appear.9 http://clic.ub.edu/ancora</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://www.lsi.upc.es/ ∼ nlp/freeling 11 http://w3.msi.vxu.se/ ∼ jha/maltparser</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">A small number of files were taken out of the CoNLL shared task data due to conversion problems and time constraints to fix them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">http://ufal.mff.cuni.cz/pedt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">http://w3.msi.vxu.se/ ∼ nivre/research/ MaltParser.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Note, however, that typically not all predicates in each sentence are annotated (cf. Table2).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/juman-e.html 17 http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/knp-e.html 18 http://sourceforge.net/projects/ mstparser 19 http://ufal.mff.cuni.cz/conll2009-st</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the Linguistic Data Consortium, mainly to Denise DiPersio, Tony Casteletto and Christopher Cieri for their help and handling of invoicing and distribution of the data for which LDC has a license. For all of the trial, training and evaluation data they had to act a very short notice. All the data has been at the participants' disposal (again) free of charge. We are grateful to all of them for LDC's continuing support of the CoNLL Shared We would also like to thank organizers of the previous four shared tasks: Sabine Buchholz, Xavier Carreras, Ryan McDonald, Amit Dubey, Johan Hall, Yuval Krymolowski, Sandra Kübler, Erwin Marsi, Jens Nilsson, Sebastian Riedel and Deniz Yuret. This shared task would not have been possible without their previous effort.</p><p>We also acknowledge the support of the MŠMT of the Czech Republic, projects MSM0021620838 and LC536; the Grant Agency of the Academy of sciences of the Czech Republic 1ET201120505 (for Jan Hajič, JanŠtěpánek and Pavel Straňák).</p><p>Lluís Màrquez and M. Antònia Martí participation was supported by the Spanish Ministry of Education and Science, through the OpenMT and TextMess research projects (TIN2006-15307-C03-02, TIN2006-15265-C06-06).</p><p>The following individuals directly contributed to the Chinese Treebank (in alphabetic order): Meiyu Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang, Tony Kroch, Martha Palmer, Mitch Marcus, Fei Xia, Nianwen Xue. The contributors to the Chinese Proposition Bank include (in alphabetic order): Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang, Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu, Hua Zhong. The Chinese Treebank and the Chinese Proposition Bank were funded by DOD, NSF and DARPA.</p><p>Adam Meyers' work on the shared task has been supported by the NSF Grant IIS-0534700 "Structure Alignment-based MT."</p><p>We thank the Mainichi Newspapers for the permission of distributing the sentences of the Kyoto University Text Corpus for this shared task.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Teams are denoted by the last name (first name added only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the language-averaged macro F 1 score on the closed challenge Joint task. Bold numbers denote the best result for a given language. Rank Rank in task System Average Catalan</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<editor>Chinese Czech English German Japanese Spanish</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Official results of the Joint task, closed challenge</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The TIGER treebank</title>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>References Sabine Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Treebanks and Linguistic Theories</title>
				<meeting>the Workshop on Treebanks and Linguistic Theories<address><addrLine>Sozopol</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The SALSA corpus: a German corpus resource for lexical semantics</title>
		<author>
			<persName><forename type="first">Aljoscha</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006)</title>
				<meeting>the 5th International Conference on Language Resources and Evaluation (LREC-2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experiments with a higherorder projective dependency parser</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
				<meeting>EMNLP-CoNLL<address><addrLine>June. Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="957" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cat3LB and Cast3LB: from constituents to dependencies</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Montserrat Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Núria</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><surname>Bufí</surname></persName>
		</author>
		<idno>LNAI 4139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Natural Language Processing, FinTAL</title>
				<meeting>the 5th International Conference on Natural Language Processing, FinTAL<address><addrLine>Turku, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="141" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An iterative approach for joint dependency parsing and semantic role labeling</title>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009)</title>
				<meeting>the 13th Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bilexical grammars and their cubictime parsing algorithms</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Probabilistic and Other Parsing Tehcnologies</title>
				<editor>
			<persName><forename type="first">Harry</forename><surname>Bunt</surname></persName>
			<persName><forename type="first">Anton</forename><surname>Nijholt</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent variable model of synchronous syntactic-semantic parsing for multiple languages</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009)</title>
				<meeting>the 13th Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PDT-VALLEX: Creating a Largecoverage Valency Lexicon for Treebank Annotation</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarmila</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdeňka</forename><surname>Urešová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alevtina</forename><surname>Bémová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Kolářová-Řezníčková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Pajas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Second Workshop on Treebanks and Linguistic Theories</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
			<persName><forename type="first">E</forename><surname>Hinrichs</surname></persName>
		</editor>
		<meeting>The Second Workshop on Treebanks and Linguistic Theories<address><addrLine>Vaxjo, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Vaxjo University Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarmila</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Hajičová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Pajas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Janštěpánek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Havelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdeněkžabokrtský</forename><surname>Mikulová</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Prague Dependency Treebank 2.0</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Construction of a Japanese relevance-tagged corpus</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kôiti</forename><surname>Hasida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002)</title>
				<meeting>the 3rd International Conference on Language Resources and Evaluation (LREC-2002)<address><addrLine>Las Palmas, Canary Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2008" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A second-order joint eisner model for syntactic and semantic dependency parsing</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009)</title>
				<meeting>the 13th Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2007 Task 09: Multilevel semantic annotation of catalan and spanish</title>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Villarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007)</title>
				<meeting>the 4th International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anotación semiautomática con papeles temáticos de los corpus CESS-ECE</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Bertran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
				<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algortihms</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT&apos;05</title>
				<meeting>NAACL-HLT&apos;05<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple generative pipeline approach to dependency parsing and semantic role labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
				<meeting>the 13th Conference on Computational Natural Language Learning<address><addrLine>Vincent Van Asch, and Antal van den Bosch; Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>NAACL/HLT 2004 Workshop Frontiers in Corpus Annotation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The conll 2007 shared task on dependency parsing</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johann</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mc-Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP-CoNLL 2007 Conference</title>
				<meeting>the EMNLP-CoNLL 2007 Conference<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual projection of role-semantic information</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005)</title>
				<meeting>the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005)<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="859" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recent advances in a feature-rich framework for treebank annotation</title>
		<author>
			<persName><forename type="first">Petr</forename><surname>Pajas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janštěpánek</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Computational Linguistics -Proceedings of the Conference (COL-ING&apos;08)</title>
				<imprint>
			<publisher>Manchester</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An annotated corpus of semantic roles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on New Methods in Language Processing</title>
				<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised training for the averaged perceptron POS tagger</title>
		<author>
			<persName><forename type="first">"johanka"</forename><surname>Drahomíra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Spoustová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><surname>Spousta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European ACL Cenference EACL&apos;09</title>
				<meeting>the European ACL Cenference EACL&apos;09<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Computational Natural Language Learning</title>
				<meeting>the 12th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prague Czech-English Dependency Treebank: Syntactically Anntoated Resources for Machine Translation</title>
		<author>
			<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008)</title>
				<meeting>the 6th International Conference on Language Resources and Evaluation (LREC-2008)<address><addrLine>Marrakesh, Morroco. MartinČmejrek; Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
	<note>Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Brown Corpus Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kucera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
	<note>available at www.clarinet/brown</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">BBN pronoun coreference and entity type corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Lin-guistic Data Consortium</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adding semantic roles to the Chinese Treebank</title>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="172" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus</title>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Morphisto: An open-source morphological analyzer for german</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Finite State Methods in Natural Language Processing</title>
				<meeting>the Conference on Finite State Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
