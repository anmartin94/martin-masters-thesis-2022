<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2021 Task 8: MeasEval -Extracting Counts and Measurements and their Related Contexts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Corey</forename><forename type="middle">A</forename><surname>Harper</surname></persName>
							<email>c.a.harper@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Elsevier Labs</orgName>
								<address>
									<addrLine>Suite 800, 230 Park Avenue</addrLine>
									<postCode>10169</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>94323, 1090</postCode>
									<settlement>Postbus, /, GH, Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Cox</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Elsevier Labs</orgName>
								<address>
									<addrLine>Suite 800, 230 Park Avenue</addrLine>
									<postCode>10169</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Curt</forename><surname>Kohler</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Elsevier Labs</orgName>
								<address>
									<addrLine>Suite 800, 230 Park Avenue</addrLine>
									<postCode>10169</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antony</forename><surname>Scerri</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Elsevier Labs</orgName>
								<address>
									<addrLine>Suite 800, 230 Park Avenue</addrLine>
									<postCode>10169</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><surname>Daniel</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Elsevier Labs</orgName>
								<address>
									<addrLine>Suite 800, 230 Park Avenue</addrLine>
									<postCode>10169</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Groth</surname></persName>
							<email>p.t.groth@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>94323, 1090</postCode>
									<settlement>Postbus, /, GH, Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2021 Task 8: MeasEval -Extracting Counts and Measurements and their Related Contexts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Counts and measurements are an important part of scientific discourse <ref type="bibr" target="#b30">(Rijgersberg et al., 2011)</ref>. It is relatively easy to find measurements in text <ref type="bibr" target="#b12">(Foppiano et al., 2019a)</ref>, but a bare measurement like 17mg is not informative without knowing what it is referring to. For example, it is important to know whether a quantity is 17mg of a medicine dosage or 17mg of concrete additive. Only recently have attempts been made to identify the named entity and property being measured <ref type="bibr" target="#b18">(Hundman and Maamann, 2017)</ref>. Extracting such information is challenging because the way scientists write can be ambiguous and inconsistent. Furthermore, the location of this information relative to the measurement can vary greatly, and might even be in a different sentence.</p><p>Being able to extract measurement information automatically can enable the construction of databases of measured properties. Such databases are important in biomedicine <ref type="bibr" target="#b16">(Hao et al., 2016)</ref>, engineering <ref type="bibr" target="#b12">(Foppiano et al., 2019a)</ref>, and other scientific disciplines <ref type="bibr" target="#b4">(Bergmann et al., 2017)</ref>, but the approaches used for populating these databases do not generalize widely. Furthermore, knowledge graphs <ref type="bibr">(Hogan et al., 2021)</ref> frequently aggregate quantitative data reported in the literature and are often built through a largely manual curation process. Examples include: LITTERBASE 1 <ref type="bibr" target="#b4">(Bergmann et al., 2017)</ref>, which aggregates observations of marine litter distribution; NeuroElectro 2 <ref type="bibr" target="#b35">(Tripathy et al., 2014)</ref>, which collects information on electrophysiological properties of neurons; and various model organism databases like the Zebrafish Information Network 3 <ref type="bibr" target="#b32">(Sprague, 2006)</ref>, which provide summaries of gene information.</p><p>Beyond knowledge graphs and curated databases, clinical health contexts often require extraction of measured values for lab results and patient observations. Moreover, scientific research frequently relies on precise measurements for reproducibility of experimental methods <ref type="bibr" target="#b20">(Kaiser, 2018)</ref>. Measured property extraction could be of value in many other contexts, such as fact checking and news validation or in statistical analysis for public policy <ref type="bibr" target="#b11">(Einav and Levin, 2014)</ref>.</p><p>Research in information extraction and knowledge graph creation has concentrated on forming triples by extracting entities and relations <ref type="bibr" target="#b23">(Konstantinova, 2014)</ref>. Little attention has been paid to the extraction of measured properties, entities, and conditions or contexts, yet these elements are needed to place measurements into a database and for their subsequent use in comparison and calculation. Units and measures are an important part of the semantic web, though research has largely been focused on ontology design <ref type="bibr" target="#b31">(Rijgersberg et al., 2013)</ref>. There is, thus, a need for understanding the state of the art on this important task.</p><p>The aim of this paper is to introduce the Mea-sEval shared task for the extraction of counts, measurements, and related context from Englishlanguage scientific documents, as well as to present an analysis of the results of participant systems on the task.</p><p>The rest of this paper is organized as follows. We begin with a description of related work. This is followed by the description of the task itself (Section 3) and the associated data and annotation procedure (Section 4). The evaluation regime is detailed in Section 5 including baselines. Subsequently, we present an analysis of the results of the systems on the task. Finally, we summarize the various participating systems approaches and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a substantial body of work discussing units of measurement, ontologies to describe them, systems designed to extract them, as well as related work on knowledge graphs of numerical attributes. Automated extraction of measured quantities, such as 520 +/-8 items/kg, is straightforward and many tools exist to perform this task <ref type="bibr" target="#b13">(Foppiano et al., 2019b;</ref><ref type="bibr" target="#b8">Deus et al., 2017;</ref><ref type="bibr" target="#b16">Hao et al., 2016)</ref>. To build a knowledge graph, we must put these measurements in context. We need to determine the properties being measured (e.g. abundances), the entities that exhibit those properties (e.g. the Maowei Sea), and possible qualifying conditions under which measurements are obtained (e.g. the date and depth of the sampling). These properties, entities, and conditions can then be mapped to those that are used in the knowledge graph, so that the measurements can be normalized into a common system.</p><p>There are a number of ontologies that cover units of measurement, such as Quantities, Units, Dimensions, and Types Ontologies (QUDT) 4 and the Ontology of Units of Measure and Related Concepts (OM) <ref type="bibr" target="#b31">(Rijgersberg et al., 2013)</ref>. These and others are discussed in a survey paper by <ref type="bibr" target="#b33">(Steinberg et al., 2017)</ref>. Most of these ontologies focus on conversion between different systems of measurement, and on classifying types of measurement or domain of application, but do not necessarily address the "thing" being measured. The Joint Committee for Guides in Metrology's (JCGM) International Vocabulary of Metrology covers this in slightly more depth, discussing measurement units and quantity values, then talking about quantities themselves, which it defines as a "property of a phenomenon, body, or substance" (Joint Committee for Guides in <ref type="bibr">Metrology, 2012)</ref>. We find that this nomenclature, while precise, is likely to be con-fusing to non-metrologists from both an evaluation and annotation perspective, so to support the data annotation process for this task we use a simplified nomenclature.</p><p>Metrology research in the Semantic Web community is often focused on ontology alignment for Units of Measurement ontologies. <ref type="bibr" target="#b21">Kaladevi et al. (2016)</ref> look at aligning unit ontologies to support merging data across many weather information systems, while <ref type="bibr" target="#b10">Do and Pauwels (2013)</ref> more generally look at using MathML for aligning unit ontologies. Efforts around designing linked data models for semantic sensor streams for the Internet of Things also utilize the Units of Measurement ontology for representing measurement information <ref type="bibr" target="#b2">(Barnaghi et al., 2013)</ref>. None of this work addresses extraction of measurements and their contexts nor building knowledge graphs from such information.</p><p>Other research explores creating databases of numeric attributes. <ref type="bibr" target="#b24">Kotnis and Garcıa-Duran (2019)</ref> infer new values using linear regression for neighboring entities in a knowledge graph. <ref type="bibr" target="#b15">Gupta et al. (2015)</ref> use a logistic regression with distributional vectors. <ref type="bibr" target="#b6">Davidov and Rappoport (2010)</ref> use a system of averages and boundary values to infer an estimated numeric attribute value. Rather than imputing new values from related entities, MeasEval starts from a value and puts it into the context of measured entities and measured properties, working toward a knowledge representation of numeric data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>MeasEval is an entity recognition and semantic relation extraction task focused on finding counts and measurements, attributes of those quantities, and additional information including measured entities, properties, and measurement contexts.</p><p>MeasEval is composed of five sub-tasks that cover span extraction, classification, and relation extraction, including cross-sentence relations. Given a paragraph from a scientific text:</p><p>• For each paragraph of text, identify all spans containing quantities (e.g. 12 kg). Quantities are treated as strings, and are not converted or normalized.</p><p>• For each identified Quantity, identify the Unit of Measurement (e.g. kg), if one exists. For each Quantity classify additional value Modifiers (e.g. count, range, approximate, mean, etc.) that apply to the Quantity.</p><p>• For each identified Quantity, identify the Measured Entity (e.g. bed inventory) it applies to (if one exists) and mark its span. If an associated Measured Property (e.g. concentration) also exists, identify it and mark its span.</p><p>• Identify and mark the span of any Qualifier (e.g. after incubation) that is needed to record additional related context to either validate or understand each identified Quantity.</p><p>• Identify relationships between Quantity, Measured Entity, Measured Property, and Qualifier spans using the HasQuantity, HasProperty, and Qualifies relation types.</p><p>More detailed definitions can be found be reviewing the MeasEval Annotation Guidelines. <ref type="bibr">5</ref> We describe each of the elements to be extracted in more detail in the next section.</p><p>4 Annotated Data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Model</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the MeasEval annotation model consists of Quantities, MeasuredEntities, MeasuredProperties, and Qualifiers. A Quantity can be either a count or a measurement, with measurements being composed of a Unit and a Value. Values also can have additional attributes such as "isMean", "isApproximate", or "isRange". Quantities can be directly related to a MeasuredEntity, or can be indirectly related to a MeasuredEntity via a MeasuredProperty. Qualifiers provide additional information that is required to interpret the measurement. These include things like the pressure at which a boiling point was observed, or the depth and location where an ocean sample was taken. Since texts may contain different parts of this information, all relationships are optional. A Mea-suredEntity can be related to a MeasuredProperty or a Quantity, a MeasuredProperty can be related a Quantity, and a Qualifier can have a relationship to any span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corpus and Annotations</head><p>Annotations are drawn from 110 CC-BY licensed articles that have been made previously available by Elsevier Labs. <ref type="bibr">6</ref> These articles were the ba- sis of a previous SemEval task for SemEval 2017 <ref type="bibr" target="#b0">(Augenstein et al., 2017)</ref>. These 110 articles are distributed evenly across 10 subject areas.</p><p>From these 110 articles, the MeasEval dataset includes 428 paragraphs containing 1663 Quantities. These are split into a training data set of 1164 Quantities (313 paragraphs) and an evaluation set of 499 Quantities (135 paragraphs).</p><p>All paragraphs were annotated by at least two annotators, then reviewed and reconciled during an adjudication meeting, often including a third annotator. The MeasEval data release included training data, as well as original annotations from multiple annotators for a 248 Quantity subset of the training data. This was to provide deep information on inter-annotator agreement, and also to allow participants to do their own analysis on how their algorithms perform relative to humans.</p><p>The inter-annotator agreement (IAA) shows some variation in interpretation when humans are performing this task. The review process serves to resolve much of the disagreement and to ensure that the data is as consistent as possible given the challenging nature of the task. For this subset of data in this IAA set, Table <ref type="table" target="#tab_1">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Formats</head><p>To increase the usability of the data, multiple formats are provided. The MeasEval data includes a text file and a set of annotations for each paragraph of scientific text. Annotations are provided in a tabseparated value (.tsv) file format, and in the BRAT annotation format. The BRAT format is for the purpose of visualization and review, but the official data format for the task is the .tsv, which is used for submissions and evaluation. For .tsv and .txt files, there is one file per paragraph of annotated text, and the .tsv file contains all annotations. For the BRAT files, there are one .ann and one .txt file per annotated Quantity. For example, given the BRAT annotations illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the data will have a raw text file (S0016236113008041-3153.txt), a BRAT annotation file per Quantity (S0016236113008041-3153-1.ann, and S0016236113008041-3153-2.ann), and a tab-separated file containing all annotations from each Quantity (S0016236113008041-3153.tsv).</p><p>More detail on each of these formats, including examples, as well as all MeasEval training and evaluation data, inter-annotator agreement annotations, and annotation guidelines can be found on the MeasEval Github repository. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Evaluation is scored by providing a single SQuADstyle <ref type="bibr" target="#b29">(Rajpurkar et al., 2016</ref>) F1 (Overlap) score for each submission, averaged across all nine components of the five subtasks. The 9 components are the Quantity, MeasuredProperty, MeasuredEntity, and Qualifier spans; the Modifier and Unit extensions to Quantity, and the HasQuantity, HasProperty, and Qualifies relationships. The evaluation script also provides a number of other metrics, described below.</p><p>In order to effectively evaluate all 9 components of the sub-tasks, it is necessary to first pin all Quantities in a submission to the corresponding Quantities in the gold data. As an example, consider the sentence "The dog weighed 25 pounds, while the average weight of the cats was 9 lbs." We want to avoid crediting correct MeasuredEntities if asso-7 https://github.com/harperco/MeasEval ciated with the wrong Quantity. For example, if a submission listed "dog" as the MeasuredEntity associated with the average weight of 9 lbs, this would be incorrect.</p><p>The first pass matches each submission "annot-Set" ID to a corresponding Gold Set annotationId, and propagates this matched identifier across all of the data.</p><p>From there, the script calculates Precision, Recall, F-measure, and an Exact Match and SQuADstyle F1 (overlap) score. Exact Match and F1 are averaged across the entire submission. Exact Match is a binary value of 0 or 1, while F1 is a token level overlap ratio of submission to gold spans, where tokenization is done using simple white space delimiters. For components that do not include a span, Exact Match and F1 scores are the same. Relations are also scored with a binary Exact Match and F1 score if the relation types match and both endpoints match either exactly or with some overlap.</p><p>Any span, unit, modifier, or relationship found in the gold data, but not the submission, or found in the submission, but not the gold data is included as a "penalty row" with a score of 0 in order to sufficiently penalize both false positives and false negatives when averaging scores. This calculation leads to very fine-grained differences in the distribution of scores in the results tables.</p><p>Although not used for calculating leaderboard rankings, the evaluation code can also provide all the same scores micro-averaged by scoring component, by subject area, or by paragraph for further analysis of error. Additional documentation as well as the evaluation code itself can be found on the MeasEval GitHub repository. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Models</head><p>MeasEval also includes two very similar baseline models. Baseline 1 is the best-performing of these, and scores an overall F1 (Overlap) of 0.239 in the evaluation as reported in Tables <ref type="table" target="#tab_3">2 and 3</ref>  Baseline 1 generates a deduplicated list of all units in the training data, and checks each Quantity against this list. If there are one or more matches in this comparison, the system returns the "longest last matching" unit, ensuring that cm would be preferred to m in "22 cm" and that s would be preferred to m in "approximately 22 s". The baseline does not attempt the Modifier component, though could be augmented with a set of regular expressions that search the Quantity string for key phrases and symbols, including "approximately", "between", "&gt;", and "∼".</p><p>Once the NER models and unit matching are completed, baseline 1 matches Quantities to Mea-suredEntities, MeasuredProperties, and Qualifiers using a knockout match algorithm based on proximity. So each MeasuredProperty matches the nearest Quantity, each MeasuredEntity matches the nearest MeasuredProperty or Quantity, and each Qualifier matches the nearest span of any type. Baseline 2 is a variant that does much simpler matching, taking each span in the order they appear in the data. Baseline 2 does not appear in the results tables, but scores an overall F1 (Overlap) of 0.223. The code for both baselines is available in a Jupyter notebook on the MeasEval Github repository. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>During the 21-day evaluation period (January 10 through 31, 2021), 26 CodaLab users submitted a total of 89 submissions, of which 77 passed validation and were successfully scored by the evaluation script. Given the complexity of the task, we opted to allow for five submissions total during the evaluation, although some collaboration between users meant that some teams were able to effectively submit more than five times. We note that submissions did not calculate scores on sub-tasks, thus making it difficult to overly optimize models using just the overall score. The relatively generous submission allowance does not seem to have presented too much of an over-fitting problem, as scores remain relatively low on all tasks, although the collaboration could have given some participants a slight advantage in the rankings.</p><p>Table <ref type="table" target="#tab_3">2</ref> shows the top submission from each of the 19 teams that submitted successfully, as well as the top performing baseline. 10 of the 19 exceed the benchmark of the baseline spaCy model. In addition to the overall F1 scores, Table <ref type="table" target="#tab_3">2</ref> shows micro-averaged F1 across the four annotation spans as well as Units and Modifiers. Table <ref type="table" target="#tab_5">3</ref> provides this same breakdown for each of the three relationship types. Team names marked with an asterisk (*) represent teams which have either submitted system description papers or responded to a request for system information.</p><p>The overall top-performing model was at least tied for top performance in five out of 6 of the component scores in Table <ref type="table" target="#tab_3">2</ref>, but interestingly, the second best and third best performing models varied across scoring component. Models that did particularly well at Quantities, Units, or Modifiers, may have had their overall performance reduced by lower performance at the MeasuredEntity and MeasuredProperty spans.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows the scores for the Relation Extraction subtasks: HasQuantity, HasProperty, and Qualifies. These largely align with the annotation span components of the scoring which they are dependent on. In both Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_5">3</ref> it is worth noting that only 7 teams attempted extraction of Qualifiers and the Qualifies relation, as these were the most difficult aspects of the task.   Figure <ref type="figure" target="#fig_2">3</ref> provides a visualization of the distribution of scores for each scoring component from Tables <ref type="table" target="#tab_3">2 and 3</ref>. From this, it is clear that Quantity and Unit are the easiest aspects of the shared task, which makes intuitive sense. The relatively high scores for Modifier is also of interest, as these are the components of the extraction that capture uncertainty and variance in value, which is an important part of the task and not one that we expected to see handled as well as it was. This clearly demonstrates that the various Quantity contextualization subtasks are far more difficult and more work is needed in how best to handle the extraction of related MeasuredEntities, MeasuredProperties, and Qualifiers.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> provides a visualization of the distribution of scores for 9 of the 10 subject areas in the corpus. The mathematics subject area has been omitted from this graphic due to under-representation in both the training and evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Impact of Duplicates</head><p>As noted previously, the MeasEval evaluation algorithm was designed toward lenience, and as a result sometimes inflates scores if multiple submission annotations match a single entry in the gold data. This was done to allow submissions to get credit for submitting multiple Quantity annotations that partially matched a single gold data span as well as to generally not penalize systems that might make multiple predictions pinned to the same numeric value.</p><p>However, allowing duplicates can in some cases result in inflated scores. This is especially evident in cases where submissions contained entries that duplicated entire annotations sets. For a small number of submissions that exhibited annotation set level duplicates, a post-processing routine removed all set level duplicates before final evaluation, resulting in the scores in Table <ref type="table" target="#tab_3">2 and Table 3.</ref> Additionally, Quantity-level duplicates can also potentially inflate Quantity scores, but should have a neutral impact on other components of scoring. For example, if a system identified the same Quantity two times, but found a different MeasuredEntity for each occurrence, the submission will score extra points associated with the Quantity, and potentially the Unit and Modifiers if those are also correct, but will only get points for the correct MeasuredEntity while being penalized for the incorrect Mea-suredEntity. An ablation analysis was performed for the eight submissions covered by system papers, assessing the impact of these duplicates on the Overall F1 (Overlap) metric.</p><p>Table <ref type="table" target="#tab_7">4</ref> gives the extent of duplication for these submissions, the initial overall score from Table <ref type="table" target="#tab_5">3</ref>, the overall score with exact quantity duplicates removed, and the overall score with both exact and overlapping duplicates removed. For example, if the gold data includes the Quantity "approximately 23 mm", and a submissions included annotation sets with both "23 mm" and "approximately 23 mm", the exact match duplicate removal would not drop either score, whereas the overlapping match score would drop whichever occurred last in the submission, whether or not it is the correct answer.  The general downward trajectory seen while removing duplicates that are not at the set level is informative. Partly this is due to declining Quantity score from duplicate removal, but some effect is attributable to the possibility that deduplication deletes a correct MeasuredProperty or Measure-dEntity and leaves an incorrect one, given that they may include different values. The ablation analysis simply removes all but the first occurrence, so there is no control over whether removed values are a closer match to the gold data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multiple Hypotheses Hypothesis</head><p>The results of de-duplication analysis, the relatively low inter-annotator agreement scores, and deeper consideration of the annotation guidelines present an interesting hypothesis. It could be that the different interpretations of the context of a measurement are not automatically right or wrong. It could be that different interpretations are useful in different downstream applications. While it is out of scope in for this task description, future work may look more closely at categorization of the areas where annotators disagreed and systems produced multiple interpretations, to see if there is alignment in the differences and whether there are patterns to the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary of Participating Systems</head><p>The MeasEval track at SemEval-2021 received nine system description paper submissions, eight of which are represented in the analysis in Table <ref type="table" target="#tab_7">4</ref>. A ninth paper formulated a new task from the MeasEval dataset focusing on just the relation extraction part of the problem. One system only attempted the Quantity, Unit, and Modifier parts of the task, while another did not submit any Qualifier spans or Qualifies relationships. Four of the nine systems have released code or models.</p><p>There are several points of similarity between the eight main submissions. All but one of the systems are based on the BERT architecture <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> or a derivative, such as SciBERT <ref type="bibr" target="#b3">(Beltagy et al., 2019)</ref>, BioBERT <ref type="bibr" target="#b26">(Lee et al., 2019)</ref>, or RoBERTa <ref type="bibr" target="#b28">(Liu et al., 2019)</ref>. All but one used a pipeline architecture, starting with Quantity extraction. All but one used sequence tagging with a BIO encoding scheme, and four followed the sequence tagger by a Conditional Random Fields (CRF) model to assemble tokens into spans and improve accuracy over simple token-level classifiers. Unit and Modifier extraction was either done using a character-level BiLSTMs, another BERT model, or a rule-based approach. Finally, it was common to see MeasuredEntity, MeasuredProperty, and Qualifier, and sometimes the relation extraction components, stacked together in a multi-task sequence tagging model as a final stage, taking both the original sentences and Quantity spans as input. One system diverged from the sequence tagging tagging approach and used templated question answering techniques to handle the relation extraction along with related spans. Table <ref type="table" target="#tab_9">5</ref> provides a high-level summary of frequency of architectures and techniques in use by more than one system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">System Specifics</head><p>Davletov et al. (2021) (LIORI), achieve their stateof-the-art performance using pre-trained models RoBERTa <ref type="bibr" target="#b28">(Liu et al., 2019)</ref> and LUKE <ref type="bibr" target="#b38">(Yamada et al., 2020)</ref>. They use LUKE to fine-tune an NER model for Quantity extraction, and a RoBERTabased multi-task model for all other spans. Mod-ifiers are predicted as Quantity-types. All other spans, including units, are extracted using Question Answering style sequence tagging (start/end logits) without question prompts for each annotation type queried for each extracted Quantity. This sequential ensemble approach of Quantitydetection followed by either "all-in-one-multi-task" extraction or a staged approach to one or more of the other subtasks proved very common among the top-performing systems. <ref type="bibr" target="#b5">Cao et al. (2021)</ref> (jarvis@tencent), do initial Quantity extraction with an ensemble of a Pointer Net <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref> and a CRF. They use a BERT-based classifier for Modifier tagging and a rule-based system for Units, and then use relationspecific taggers with the same architecgure as the Quantity-tagger for all other task components. <ref type="bibr" target="#b14">Gangwar et al. (2021)</ref> (Counts@IITK) similarly tag Quantities witha SciBERT sequence tagger and a CRF model and SciBERT for Modifiers, but use a Character based bidirectional LSTM for Unit tagging. They then encode the Quantity into SciBERT input when tagging MeasuredEntity and HasQuantity, and iteratively mark new spans in the input when tagging then next sub-task, using a rule-base for assembling the necessary relationships. Their performance on Quantity, Unit, and Modifier was near the top performing, but they struggled with MeasuredProperty and HasProperty. <ref type="bibr" target="#b34">Therien et al. (2021)</ref> (CLaC-BP) use SciBERT in a token-level multi-class classifier across all span classes. This is an interesting approach, given the opportunity for joint inference between the various types of spans. However, it penalizes them in that each token in their model can only be one class, while there are cases when a Quantity and Mea-suredEntity from one set may be part of, e.g. a Qualifier in another. Quantity spans are then fed to another SciBERT model for Modifier typing, and rule-based systems are used for Units and for the Relations between spans. <ref type="bibr" target="#b1">Avram et al. (2021)</ref> (UPB) use RoBERTa along with a CRF for Quantity extraction. They also tested SciBERT and BERT. They achieved their best results on their dev subset with SciBERT, but their best results on evaluation set came from RoBERTa. They use a BiLSTM to extract Units and classifier Modifiers, and then use a templated Question Answering system as a joint entity and relation extraction system for the remaining subtasks. Unlike LIORI, who did not use prefixes or suffixes in their question templates, UPB asks more natural language questions of the form "What is the measured property of the quantity ?" <ref type="bibr" target="#b22">Karia et al. (2021)</ref> (KGP) also use BioBERT after testing various BERT-based pre-trained models, but depart from many of the other submissions by using a binary classifier rather than BIO tags and CRF layers for Quantity sequence tagging. Modifiers and Units are handled using keywords and dictionary matching, while they use a multi-task BERT model for the remaining components, first finding MeasuredEntity based on the Quantity predictions, then fusing these results for the remaining spans and relations. They also continued refining their approach into the post-evaluation phase, and reported improving their score from an Overall F1 (Overlap) of 0.278 to 0.456. <ref type="bibr" target="#b27">Liu et al. (2021)</ref> (Stanford MLab) only tackle the Quantity, Unit, and Modifier subtasks. Notably, they report building their system for these components from inception to submission in under 48 hours. They use BERT-large for IOB sequence tagging for Quantities, use a similar IO sequence tagging scheme on Quantities to tag Units, and a multi-class classifier to classify Quantities to the appropriate Modifiers. Their system performs well on all subtasks they attempted, even scoring second place overall for Units. <ref type="bibr" target="#b25">Lathiff et al. (2021)</ref> (CLaC-np) diverge from other submissions in their approach. They preprocess their text using GATE and ANNIE, and use custom rules to further clean up tokenization. They treated Stanford Core Dependency Parse trees as graphs to extract subgraphs starting each path query from the CD tokens to identify MeasureEntities, MeasuredProperties and Qualifiers with the use of Graph CNN. They relied on the models from CLaC-BP to map from their tokens to annotation spans for each type in assembling their final submission.</p><p>Finally, not shown in Table <ref type="table" target="#tab_1">1</ref> is <ref type="bibr" target="#b36">Veyseh et al. (2021)</ref>. They formulated their own task based on the MeasEval data. Although they did not submit a solution during the evaluation period, they have submitted a system description paper describing a novel approach to relation extraction, which they have evaluated on MeasEval sub-task 5. Using our Gold Quantity, MeasuredEntity, MeasuredProperty, and Qualifier spans as input (without annotation sets), they compared their approach to two other baseline models. They encode contextual embeddings, positional embeddings, and entity types for each annotation span, and perform dependency path reasoning along with an "Information Bottleneck" regularization technique to complete their Relation Extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we present the design, the data, the evaluation the process, the results, and the systems for MeasEval at SemEval 2021. The shared task is challenging, partly due to the relatively small training data, and partly due to the inter-relationships between many different components of the task. Quantity and Unit identification, and to a lesser extent Modifier typing, appear to be the simplest parts of the task based on average performance, with one participating system building their endto-end pipeline for these components in under 48 hours. The contextual elements MeasuredEntity, MeasuredProperty, and Qualifier, and their relationships, are far more difficult, which is not surprising given that these are subject to more human annotator disagreement. The challenge of context is especially pronounced in the Qualifier span and Qualifies relationship.</p><p>Common components shown in Table <ref type="table" target="#tab_9">5</ref> include the BERT family of pre-trained neural language models, CRF models, BiLSTMs, and rule-based components. In general, the task does not appear to require whole new novel models and architectures, but rather pipelines and cascading ensembles stitching together various existing methods. There is still room for improvement on this task, and whether progress will come from novel models or creative applications of existing techniques remains to be seen. Work also remains to be done in applying the entities and relationships extracted for this task to the larger end goal of scientific knowledge graph construction and related downstream applications. Future work could be done to further analyze areas of error and disagreement in these annotations, and to investigate entity linking across Quantity, MeasuredEntity, and MeasuredProperty annotation spans to various measurement ontologies and to domain-specific entity and property ontologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotation Model. All relationships are optional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BRAT Example of a Quantity with related annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of average scores for each scoring component across top score for all participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of average scores for each subject area across top score for all participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>shows Krippendorff's Alpha values for each class.</figDesc><table><row><cell>Annotation Class</cell><cell>Krippendorff's Alpha</cell></row><row><cell>Quantity</cell><cell>0.943</cell></row><row><cell cols="2">MeasuredProperty 0.641</cell></row><row><cell>MeasuredEntity</cell><cell>0.546</cell></row><row><cell>Qualifier</cell><cell>0.334</cell></row><row><cell>Unit</cell><cell>0.866</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Krippendorf's Alpha scores for subset of data included in Inter-Annotator Agreement dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Top result for each team/user, ordered by Overall F1 along with micro-averages for each annotation span, for units, and for modifiers. Team Names marked with * have submitted system information for further analysis and discussion. Top, second, and third place scores per category represented by bold, underline, and italics respectively. line 1 use spaCy Named Entity Recognition (NER) models for each of the four classes independently. Unfortunately, some training examples need to be thrown away because spaCy's NER functionality does not support overlapping spans in the same model. Since there is frequently an overlap between MeasEval spans of different types, this necessitates training each annotation type separately, and stripping out edge cases where multiple annotations of the same type intersect.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Top result for each team/user for the Rela-</figDesc><table /><note>tion Extraction components of the score. Team Names marked with * have submitted system information for further analysis and discussion. Top, second, and third place scores per category represented by bold, underline, and italics respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Ablation analysis of duplicates and Overall</cell></row><row><cell>F1 (Overlap) score for each of the eight Teams with</cell></row><row><cell>System Papers.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Summary of techniques and architectures used in MeasEval System Description Submissions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://litterbase.awi.de/ 2 https://neuroelectro.org/ 3 http://zfin.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.qudt.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/harperco/MeasEval/ tree/main/annotationGuidelines 6 https://github.com/elsevierlabs/ OA-STM-Corpus</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/harperco/MeasEval/ tree/main/eval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/harperco/MeasEval/ blob/main/baselines/first-baseline.ipynb</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Darin McBeath and Pierre-Yves Vandenbussche for help with annotations and annotation rules. We also thank Elsevier's Discovery Lab team for their feedback on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ScienceIE -extracting keyphrases and relations from scientific publications</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Vikraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
				<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="546" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">UPB at SemEval-2021 Task 8 : Extracting Semantic Information on Measurements as Multi-Turn Question Answering</title>
		<author>
			<persName><forename type="first">Andrei-Marius</forename><surname>Avram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George-Eduard</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dumitru-Clementin Cercel</surname></persName>
		</author>
		<author>
			<persName><surname>Dascalu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A linked-data model for semantic sensor streams</title>
		<author>
			<persName><forename type="first">Payam</forename><surname>Barnaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber</title>
				<meeting>-2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SciB-ERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1371</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3613" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LITTERBASE: An Online Portal for Marine Litter and Microplastics and Their Implications for Marine Life</title>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tekman</surname></persName>
		</author>
		<author>
			<persName><surname>Gutow</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-812271-6.00104-6</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="106" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CONNER : A Cascade Count and Measurement Extraction Tool for Scientific Discourse</title>
		<author>
			<persName><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extraction and approximation of numerical attributes from the Web</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010 -48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
				<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIORI at SemEval-2021 Task 8: Ask Transformer for measurements</title>
		<author>
			<persName><forename type="first">Adis</forename><surname>Davletov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Gordeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emil</forename><surname>Davletov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining pattern matching with word embeddings for the extraction of experimental variables from scientific literature</title>
		<author>
			<persName><forename type="first">Helena</forename><forename type="middle">F</forename><surname>Deus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><forename type="middle">A</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darin</forename><surname>Mcbeath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4287" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using MathML to represent units of measurement for improved ontology alignment</title>
		<author>
			<persName><forename type="first">Chau</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="310" to="325" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The data revolution and economic analysis</title>
		<author>
			<persName><forename type="first">Liran</forename><surname>Einav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Levin</surname></persName>
		</author>
		<idno type="DOI">10.1086/674019</idno>
	</analytic>
	<monogr>
		<title level="j">Innovation Policy and the Economy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Proposal for Automatic Extraction Framework of Superconductors Related Information from Scientific Literature Proposal for Automatic Extraction Framework of Superconductors Related Information from Scientific Literature</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Foppiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Dieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="0" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic identification and normalisation of physical measurements in scientific literature</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Foppiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikiko</forename><surname>Tanifuji</surname></persName>
		</author>
		<idno type="DOI">10.1145/3342558.3345411</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Document Engineering</title>
				<meeting>the ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Counts @ IITK at SemEval-2021 Task 8 : SciBERT Based Entity And Semantic Relation Extraction For Scientific Data</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabhay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Sourav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributional vectors encode referential attributes</title>
		<author>
			<persName><forename type="first">Abhijeet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings -EMNLP 2015: Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Valx: A system for extracting and structuring numeric lab test comparison statements from text</title>
		<author>
			<persName><forename type="first">Tianyong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Weng</surname></persName>
		</author>
		<idno type="DOI">10.3414/ME15-01-0112</idno>
	</analytic>
	<monogr>
		<title level="j">Methods of information in medicine</title>
		<imprint>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Emilio Labra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Gayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Kirrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Neumaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anisa</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Rula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Schmelzeisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Sequeda</surname></persName>
		</author>
		<author>
			<persName><surname>Staab</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Antoine Zimmermann. 2021. Knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Measurement Context Ex-traction from Text: Discovering Opportunities and Gaps in Earth Science</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hundman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Maamann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">pages</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">International vocabulary of metrology. Basic and general concepts and associated terms</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Joint Committee for Guides in Metrology. 3rd ed edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plan to replicate 50 high-impact cancer papers shrinks to just 18</title>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aau9619</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ontological based interoperability and integration framework for heterogeneous weather systems</title>
		<author>
			<persName><forename type="first">Ramar</forename><surname>Kaladevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurunathan</forename><surname>Geetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanasamy</surname></persName>
		</author>
		<idno type="DOI">10.21311/001.39.1.19</idno>
	</analytic>
	<monogr>
		<title level="j">Revista Tecnica de la Facultad de Ingenieria Universidad del Zulia</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">KGP at SemEval-2021 Task 8 : Leveraging Multi-Staged Language Models for Extracting Measurements, their Attributes and Relations</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Karia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faraaz Rahman</forename><surname>Mallick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Review of relation extraction methods</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Konstantinova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-12580-0_2</idno>
	</analytic>
	<monogr>
		<title level="m">What is new out there? Communications in Computer and Information Science</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">436</biblScope>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Numerical Attributes in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcıa-Duran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Automated Knowledge Base Construction</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CLaC-np at SemEval-2021 Task 8 : Dependency DGCNN</title>
		<author>
			<persName><forename type="first">Nihatha</forename><surname>Lathiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Khloponin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stanford MLab at SemEval-2021 Task 8 : 48 Hours Is All You Need</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niveditha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Rozi</surname></persName>
		</author>
		<author>
			<persName><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach. arXiv</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ii</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How semantics can improve engineering processes: A case of units of measure and quantities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rijgersberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Top</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aei.2010.07.008</idno>
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ontology of units of measure and related concepts</title>
		<author>
			<persName><forename type="first">Hajo</forename><surname>Rijgersberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Van Assem</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-2012-0069</idno>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Zebrafish Information Network: the zebrafish model organism database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sprague</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkj086</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">90001</biblScope>
			<biblScope unit="page" from="D581" to="D585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Use cases and suitability metrics for unit ontologies</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirko</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Martin</forename><surname>Keil</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54627-8_4</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="10161" to="10201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CLaC-BP at SemEval-2021 Task 8 : SciBERT Plus Rules for MeasEval</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Therien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parsa</forename><surname>Bagherzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NeuroElectro: A window to the world&apos;s neuron electrophysiology data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shreejoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><forename type="middle">D</forename><surname>Savitskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">N</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><surname>Gerkin</surname></persName>
		</author>
		<idno type="DOI">10.3389/fninf.2014.00040</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DPR at SemEval-2021 Task 8: Dynamic Path Reasoning for Measurement Relation Extraction</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Amir Pouran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
		<title level="m">Deep contextualized entity representations with entityaware self-attention. arXiv</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
