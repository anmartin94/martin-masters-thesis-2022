<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 3: Community Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Walid</forename><surname>Magdy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abed</forename><forename type="middle">Alhakim</forename><surname>Freihat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Glass</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computing Research Institute</orgName>
								<orgName type="laboratory">ALT Research Group</orgName>
								<orgName type="institution">HBKU</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory Bilal Randeree Qatar Living</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 3: Community Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Advice and Help</term>
					<term>Beauty and Style</term>
					<term>Cars and driving</term>
					<term>Computers and Internet</term>
					<term>Doha Shopping</term>
					<term>Education</term>
					<term>Environment</term>
					<term>Family Life in Qatar</term>
					<term>Funnies</term>
					<term>Health and Fitness</term>
					<term>Investment and Finance</term>
					<term>Language</term>
					<term>Moving to Qatar</term>
					<term>Opportunities</term>
					<term>Pets and Animals</term>
					<term>Politics</term>
					<term>Qatar Living Lounge</term>
					<term>Qatari Culture</term>
					<term>Salary and Allowances</term>
					<term>Sightseeing and Tourist attractions</term>
					<term>Socialising</term>
					<term>Sports in Qatar</term>
					<term>Visas and Permits</term>
					<term>Welcome to Qatar</term>
					<term>Working in Qatar</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the SemEval-2016 Task 3 on Community Question Answering, which we offered in English and Arabic.</p><p>For English, we had three subtasks: Question-Comment Similarity (subtask A), Question-Question Similarity (B), and Question-External Comment Similarity (C). For Arabic, we had another subtask: Rerank the correct answers for a new question (D). Eighteen teams participated in the task, submitting a total of 95 runs (38 primary and 57 contrastive) for the four subtasks. A variety of approaches and features were used by the participating systems to address the different subtasks, which are summarized in this paper. The best systems achieved an official score (MAP) of 79.19, 76.70, 55.41, and 45.83 in subtasks A, B, C, and D, respectively. These scores are significantly better than those for the baselines that we provided. For subtask A, the best system improved over the 2015 winner by 3 points absolute in terms of Accuracy.</p><p>7 This is the rank of the thread in the original list of Google results, before the thread filtering; see above. 8  Here are some examples of Qatar Living categories:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building on the success of SemEval-2015 Task 3 "Answer Selection in Community Question Answering" 1 , we run an extension in 2016, which covers a full task on Community Question Answering (CQA) and which is, therefore, closer to the real application needs. All the information related to the task, data, participants, results and publications can be found on the SemEval-2016 Task 3 website. 2 1 http://alt.qcri.org/semeval2015/task3 2 http://alt.qcri.org/semeval2016/task3 CQA forums such as Stack Overflow 3 and Qatar Living 4 , are gaining popularity online. These forums are seldom moderated, quite open, and thus they typically have little restrictions, if any, on who can post and who can answer a question. On the positive side, this means that one can freely ask any question and can then expect some good, honest answers. On the negative side, it takes effort to go through all possible answers and to make sense of them. For example, it is not unusual for a question to have hundreds of answers, which makes it very time-consuming for the user to inspect and to winnow through them all. The present task could help to automate the process of finding good answers to new questions in a community-created discussion forum, e.g., by retrieving similar questions in the forum and by identifying the posts in the comment threads of those similar questions that answer the original question well.</p><p>In essence, the main CQA task can be defined as follows: "given (i) a new question and (ii) a large collection of question-comment threads created by a user community, rank the comments that are most useful for answering the new question".</p><p>The test question is new with respect to the collection, but it is expected to be related to one or several questions in the collection. The best answers can come from different question-comment threads. In the collection, the threads are independent of each other and the lists of comments are chronologically sorted and contain some meta information, e.g., date, user, topic, etc.</p><p>The comments in a particular thread are intended to answer the question initiating that thread, but since this is a resource created by a community of casual users, there is a lot of noise and irrelevant material, apart from informal language usage and lots of typos and grammatical mistakes. Interestingly, the questions in the collection can be semantically related to each other, although not explicitly.</p><p>Our intention was not to run just another regular Question Answering task. Similarly to the 2015 edition, we had three objectives: (i) to focus on semantic-based solutions beyond simple "bag-ofwords" representations and "word matching" techniques; (ii) to study the new natural language processing (NLP) phenomena arising in the community question answering scenario, e.g., relations between the comments in a thread, relations between different threads and question-to-question similarity; and (iii) to facilitate the participation of non IR/QA experts to our challenge. The third point was achieved by explicitly providing the set of potential answers-the search engine step was carried out by us-to be (re)ranked and by defining two optional subtasks apart from the main CQA task. Subtask A (Question-Comment Similarity): given a question from a question-comment thread, rank the comments according to their relevance (similarity) with respect to the question; Subtask B (Question-Question Similarity): given the new question, rerank all similar questions retrieved by a search engine, assuming that the answers to the similar questions should be answering the new question too.</p><p>Subtasks A and B should give participants enough tools to create a CQA system to solve the main task. Nonetheless, one can approach CQA without necessarily solving the two tasks above. Participants were free to use whatever approach they wanted, and the participation in the main task and/or the two subtasks was optional. A more precise definition of all subtasks can be found in Section 3.</p><p>Keeping the multilinguality from 2015, we provided data for two languages: English and Arabic. For English, we used real data from the communitycreated Qatar Living forum. The Arabic data was collected from medical forums, with a slightly different procedure. We only proposed the main ranking CQA task on this data, i.e., finding good answers for a given new question.</p><p>Finally, we provided training data for all languages and subtasks with human supervision. All examples were manually labeled by a community of annotators in a crowdsourcing platform. The datasets and the annotation procedure are described in Section 4, and some examples can be found in Figures <ref type="figure" target="#fig_0">3 and 4</ref>.</p><p>The rest of the paper is organized as follows: Section 2 introduces some related work. Section 3 gives a more detailed definition of the task. Section 4 describes the datasets and the process of their creation. Section 5 explains the evaluation measures. Section 6 presents the results for all subtasks and for all participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see <ref type="bibr" target="#b33">(Radlinski and Joachims, 2005;</ref><ref type="bibr" target="#b17">Jeon et al., 2005;</ref><ref type="bibr" target="#b37">Shen and Lapata, 2007;</ref><ref type="bibr" target="#b29">Moschitti et al., 2007;</ref><ref type="bibr" target="#b36">Severyn and Moschitti, 2015;</ref><ref type="bibr" target="#b30">Moschitti, 2008;</ref><ref type="bibr" target="#b39">Tymoshenko and Moschitti, 2015;</ref><ref type="bibr" target="#b38">Surdeanu et al., 2008)</ref>. In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, <ref type="bibr" target="#b42">Wang et al. (2007)</ref> proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; <ref type="bibr" target="#b15">Heilman and Smith (2010)</ref> used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; <ref type="bibr" target="#b40">Wang and Manning (2010)</ref> developed a probabilistic model to learn tree-edit operations on dependency parse trees; and <ref type="bibr" target="#b44">Yao et al. (2013)</ref> applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in <ref type="bibr" target="#b34">(Severyn and Moschitti, 2012;</ref><ref type="bibr" target="#b35">Severyn and Moschitti, 2013)</ref>. Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general.</p><p>Using information about the thread is another important direction. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to the usual local features that only look at the question-answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, whether the answer is first, whether the answer is last <ref type="bibr">(Hou et al., 2015)</ref>. Similarly, the third-best team, QCRI, used features that model a comment in the context of the entire comment thread, focusing on user interaction <ref type="bibr" target="#b32">(Nicosia et al., 2015)</ref>. Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments <ref type="bibr" target="#b46">(Zhou et al., 2015b)</ref>.</p><p>In a follow-up work, <ref type="bibr" target="#b45">Zhou et al. (2015a)</ref> included long-short term memory (LSTM) units in their convolutional neural network to learn the classification sequence for the thread. In parallel,  exploited the dependencies between the thread comments to tackle the same task. This was done by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields <ref type="bibr" target="#b20">(Lafferty et al., 2001)</ref>. This research direction was further extended by , who used the output structure at the thread level in order to make more consistent global decisions. For this purpose, they modeled the relations between pairs of comments at any distance in the thread, and they combined the predictions of local classifiers in a graph-cut and in an ILP frameworks.</p><p>Finally,  proposed two novel joint learning models that are on-line and integrate inference within the learning process. The first one jointly learns two node-and edge-level MaxEnt classifiers with stochastic gradient descent and integrates the inference step with loopy belief propagation. The second model is an instance of fully connected pairwise CRFs (FCCRF). The FCCRF model significantly outperforms all other approaches and yields the best results on the task (SemEval-2015 Task 3) to date. Crucial elements for its success are the global normalization and an Ising-like edge potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definition of the Subtasks</head><p>The challenge was structured as a set of four different and independent subtasks. Three of them (A, B and C) were offered for English, while the fourth one (D) was offered for Arabic. We describe them below in detail. In order to make the subtask definitions more clear, we also provide some high-level information about the datasets we used (they will be described in more detail later in Section 4).</p><p>The English data comes from the Qatar Living forum, which is organized as a set of seemingly independent question-comment threads. In short, for subtask A we annotated the comments in a questionthread as "Good", "PotentiallyUseful" or "Bad" with respect to the question that started the thread. Additionally, given original questions we retrieved related question-comment threads and we annotated the related questions as "PerfectMatch", "Relevant", or "Irrelevant" with respect to the original question (subtask B). We then annotated the comments in the threads of related questions as "Good", "Potential-lyUseful" or "Bad" with respect to the original question (subtask C).</p><p>For Arabic, the data was extracted from medical forums and has a different format. Given an original question, we retrieved pairs of the form (related question, answer to the related question). These pairs were annotated as "Direct" answer, "Relevant" and "Irrelevant" with respect to the original question.</p><p>English subtask A Question-Comment Similarity. Given a question Q and its first ten comments 5 in the question thread (c 1 , . . . , c 10 ), the goal is to rank these ten comments according to their relevance with respect to the question.</p><p>Note that this is a ranking task, not a classification task; we use mean average precision (MAP) as an official evaluation measure. This setting was adopted as it is closer to the application scenario than pure comment classification. For a perfect ranking, a system has to place all "Good" comments above the "PotentiallyUseful" and "Bad" comments; the latter two are not actually distinguished and are considered "Bad" in terms of evaluation.</p><p>Note also that subtask A this year is the same as subtask A at SemEval-2015 Task 3, but with slightly different annotation and evaluation measure.</p><p>English subtask B Question-Question Similarity. Given a new question Q (aka original question) and the set of the first ten related questions from the forum (Q 1 , . . . , Q 10 ) retrieved by a search engine, the goal is to rank the related questions according to their similarity with respect to the original question.</p><p>In this case, we consider the "PerfectMatch" and "Relevant" questions both as good (i.e., we do not distinguish between them and we will consider them both "Relevant"), and they should be ranked above the "Irrelevant" questions. As in subtask A, we use MAP as the official evaluation measure. To produce the ranking of related questions, participants have access to the corresponding related question-thread. <ref type="bibr">6</ref> Thus, being more precise, this subtask could have been named Question -Ques-tion+Thread Similarity.</p><p>English subtask C Question-External Comment Similarity. Given a new question Q (aka the original question), and the set of the first ten related questions (Q 1 , . . . , Q 10 ) from the forum retrieved by a search engine, each associated with its first ten comments appearing in its thread (c 1 1 , . . . , c 10 1 , . . . , c 1 10 , . . . , c 10 10 ), the goal is to rank the 100 comments {c j i } 10 i,j=1 according to their relevance with respect to the original question Q. This is the main English subtask. As in subtask A, we want the "Good" comments to be ranked above the "PotentiallyUseful" and "Bad" comments, which will be considered just bad in terms of evaluation. Although, the systems are supposed to work on 100 comments, we take an application-oriented view in the evaluation, assuming that users would like to have good comments concentrated in the first ten positions. We believe users care much less about what happens in lower positions (e.g., after the 10th) in the rank, as they typically do not ask for the next page of results in a search engine such as Google or Bing. This is reflected in our primary evaluation score, MAP, which we restrict to consider only the top ten results in subtask C.</p><p>Arabic subtask D Rank the correct answers for a new question. Given a new question Q (aka the original question), the set of the first 30 related questions retrieved by a search engine, each associated with one correct answer ((Q 1 , c 1 ) . . . , (Q 30 , c 30 )), the goal is to rank the 30 question-answer pairs according to their relevance with respect to the original question. We want the "Direct" and the "Relevant" answers to be ranked above the "Irrelevant" answers; the former two are considered "Relevant" in terms of evaluation. We evaluate the position of "Relevant" answers in the rank, therefore, this is again a ranking task.</p><p>Unlike the English subtasks, here we use 30 answers since the retrieval task is much more difficult, leading to low recall, and the number of correct answers is much lower. Again, systems were evaluated using MAP, restricted to the top-10 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>As we mentioned above, the task is offered for two languages, English and Arabic. Below we describe the data for each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English Dataset</head><p>We refer to the English data as the CQA-QL corpus; it is based on data from the Qatar Living forum.</p><p>The English data is organized with focus on the main task, which is subtask C, but it contains annotations for all three subtasks. It consists of a list of original questions, where for each original question there are ten related questions from Qatar Living, together with the first ten comments from their threads. The data is annotated with the relevance of each related question with respect to the original question (subtask B), as well as with the relevance of each comment with respect to the related (subtask A) and also with respect to the original question (subtask C).</p><p>To build the dataset, we first selected a set of questions to serve as original questions. In a real-world scenario those would be questions that were never asked before; however, here we used existing questions from Qatar Living. For the training and for the development datasets, we used questions from SemEval-2015 Task 3 , while we used new Qatar Living questions for testing.</p><p>From each original question, we generated a query, using the question's subject (after some word removal if the subject was too long). Then, we executed the query in Google, limiting the search to the Qatar Living forum, and we collected up to 200 resulting question-comment threads as related questions. Afterwards, we filtered out threads with less than ten comments as well as those for which the question was more than 2,000 characters long. Finally, we kept the top-10 surviving threads, keeping just the first 10 comments in each thread.</p><p>We formatted the results in XML with UTF-8 encoding, adding metadata for the related questions and for their comments; however, we did not provide any meta information about the original question, in order to emulate a scenario where it is a new question, never asked before in the forum. In order to have a valid XML, we had to do some cleansing and normalization of the data. We added an XML format definition at the beginning of the XML file and made sure it validates.</p><p>We provided a split of the data into three datasets: training, development, and testing. A dataset file is a sequence of original questions (OrgQuestion), where each question has a subject, a body (text), and a unique question identifier (ORGQ ID). Each such original question is followed by ten threads, where each thread has a related question (according to the search engine results) and its first ten comments.</p><p>Each related question (RelQuestion) has a subject and a body (text), as well as the following attributes:</p><p>• RELQ ID: question identifier;</p><p>• RELQ RANKING ORDER: the rank of the related question in the list of results returned by the search engine for the original question; 7</p><p>• RELQ CATEGORY: the question category, according to the Qatar Living taxonomy; 8</p><p>• RELQ DATE: date of posting;</p><p>• RELQ USERID: identifier of the user asking the question;</p><p>• RELQ USERNAME: name of the user asking the question;</p><p>• RELQ RELEVANCE2ORGQ: human assessement on the relevance this RelQuestion thread with respect to OrgQuestion. This label can take one of the following values:</p><p>-PerfectMatch: RelQuestion matches OrgQuestion (almost) perfectly; at test time, this label is to be merged with Relevant; -Relevant: RelQuestion covers some aspects of OrgQuestion; -Irrelevant: RelQuestion covers no aspects of OrgQuestion.</p><p>Each comment has a body text, 9 as well as the following attributes:</p><p>• RELC ID: comment identifier;</p><p>• RELC USERID: identifier of the user posting the comment;</p><p>• RELC USERNAME: name of the user posting the comment;</p><p>• RELC RELEVANCE2ORGQ: human assessment about whether the comment is Good, Bad, or Potentially Useful with respect to the original question, OrgQuestion. This label can take one of the following values:</p><p>-Good: at least one subquestion is directly answered by a portion of the comment; -PotentiallyUseful: no subquestion is directly answered, but the comment gives potentially useful information about one or more subquestions (at test time, this class will be merged with Bad); -Bad: no subquestion is answered and no useful information is provided (e.g., the answer is another question, a thanks, dialog with another user, a joke, irony, attack of other users, or is not in English, etc.).</p><p>Figure <ref type="figure">1</ref>: Screenshot for the first English annotation job, collecting labels for subtasks B and C.</p><p>Figure <ref type="figure">2</ref>: Screenshot of the second English annotation job, collecting labels for subtask A.</p><p>• RELC RELEVANCE2RELQ: human assessment about whether the comment is Good, Bad, or PotentiallyUseful (again, the latter two are merged under Bad at test time) with respect to the related question, RelQuestion.</p><p>We used the CrowdFlower 10 crowdsourcing platform to annotate the gold labels for the three subtasks, namely RELC RELEVANCE2RELQ for subtask A, RELQ RELEVANCE2ORGQ for subtask B, and RELC RELEVANCE2ORGQ for subtask C. We collected several annotations for each decision (there were at least three human annotators per example) and we resolved the discrepancies using the default mechanisms of CrowdFlower, which take into account the general quality of annotation for each annotator (based on the hidden tests).</p><p>Unlike SemEval-2015 Task 3 , where we excluded comments for which there was a lot of disagreement about the labels between the human annotators, this time we did not eliminate any comments (but we controlled the annotation quality with hidden tests), and thus we guarantee that for each question thread, we have the first ten comments without any comment being skipped.</p><p>To gather gold annotation labels, we created two annotation jobs on CrowdFlower, screenshots of which are shown in Figures <ref type="figure">1 and 2</ref>.</p><p>The first annotation job aims to collect labels for subtasks B and C. We show a screenshot in Figure <ref type="figure">1</ref>. An annotation example consists of an original question, a related question, and the first ten comments for that related question. We asked the annotators to judge the relevance of the thread with respect to the original question (RELQ RELEVANCE2ORGQ, for subtask B), as well as the relevance of each comment with respect to the original question (RELC RELEVANCE2ORGQ, for subtask C). Each example is judged by three annotators who must maintain 70% accuracy throughout the job, measured on a hidden set of 121 examples. <ref type="bibr">11</ref> The average inter-annotator agreement on the training, development, and testing datasets is 80%, 74%, and 87% for RELQ RELEVANCE2ORGQ, and 83%, 74%, and 88% for RELC RELEVANCE2ORGQ.</p><p>The second CrowdFlower job collects labels for subtask A; a screenshot is shown in Figure <ref type="figure">2</ref>. An annotation example consists of a question-comments thread, with ten comments, and we ask annotators to judge the relevance of each comment with respect to the thread question (RELC RELEVANCE2RELQ). Again, each example is judged by three annotators who must maintain 70% accuracy throughout the job, measured on a hidden set of 150 examples. The average inter-annotator agreement on the training, development, and testing datasets is 82%, 89%, and 79% for RELC RELEVANCE2RELQ.</p><p>A fully annotated example is shown in Figure <ref type="figure" target="#fig_0">3</ref>. Statistics about the datasets are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Note that the training data is split into two parts, where part2 is noisier than part1. For part2, a different annotation setup was used, 12 which confused the annotators, and they often provided annotation for RELC RELEVANCE2ORGQ while wrongly thinking that they were actually annotating RELC RELEVANCE2RELQ. Note that the development data was annotated with the same setup as training part2; however, we manually doublechecked and corrected it. Instead, the training part1 and testing datasets used the less confusing, and thus higher-quality annotation setup described above.</p><p>Note also that in addition to the above-described canonical XML format, we further released the data in an alternative uncleansed 13 multi-line format. We further released a simplified file format containing only the relevant information for subtask A, where duplicated related questions are removed. 14 Finally, we reformatted the training, development, and test data from SemEval-2015 Task 3 , to match the subtask A format for this year.  We released this reformatted SemEval-2015 Task 3, subtask A data as additional training data. We further released a large unannotated dataset from Qatar Living with 189,941 questions and 1,894,456 comments, which is useful for unsupervised learning or for training domain-specific word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Arabic Dataset</head><p>While at SemEval-2015  we used a dataset from the Fatwa website, this year we changed the domain to medical, which is largely ignored for Arabic. We will refer to the Arabic corpus as CQA-MD. We extracted data from three popular Arabic medical websites that allow visitors to post questions related to health and medical conditions, and to get answers by professional doctors. We collected 1,531 question-answer (QA) pairs from WebTeb, 15 69,582 pairs from Al-Tibbi, <ref type="bibr">16</ref> and 31,714 pairs from the medical corner of Islamweb. <ref type="bibr">17</ref> We used the 1,531 questions from WebTeb as our original questions, and we looked to find related QA pairs from the other two websites. We collected over 100,000 QA pairs in total from the other two websites, we indexed them in Solr, and we searched them trying to find answers to the WebTeb questions.</p><p>We used several different query/document formulations to perform 21 retrieval runs, and we merged the retrieved results, ranking them according to the reciprocal rank fusion algorithm <ref type="bibr" target="#b4">(Cormack et al., 2009)</ref>. Finally, we truncated the result list to the 30 top-ranked QA pairs, ending up with 45,164 QA pairs <ref type="bibr">18</ref> for the 1,531 original questions. Next, we used CrowdFlower to obtain judgments about the relevance of these QA pairs with respect to the original question using the following labels:</p><p>• "D" (Direct): The QA pair contains a direct answer to the original question such that if the user is searching for an answer to the original question, the proposed QA pair would be satisfactory and there would be no need to search any further.</p><p>• "R" (Related): The QA pair contains an answer to the original question that covers some of the aspects raised in the original question, but this is not sufficient to answer it directly. With this QA pair, it would be expected that the user will continue the search to find a direct answer or more information.</p><p>• "I" (Irrelevant): The QA pair contains an answer that is irrelevant to the original question. We controlled the quality of annotation using a hidden set of 50 test questions. We had three judgments per example, which we combined using the CrowdFlower mechanism. The average interannotator agreement was 81%.</p><p>Finally, we divided the data into training, development and testing datasets, based on confidence, where the examples in the test dataset were those with the highest annotation confidence. We further double-checked and manually corrected some of the annotations for the development and the testing datasets whenever necessary.</p><p>Figure <ref type="figure" target="#fig_1">4</ref> shows part of the XML file we generated. We can see that, unlike the English data, there are no threads here, just a set of question-answer pairs; moreover, we do not provide much meta data, but we give information about the confidence of annotation (for the training and development datasets only, but not for the test dataset).</p><p>Table <ref type="table" target="#tab_3">2</ref> shows some statistics about the dataset size and the distribution of the three classes in the CQA-MD corpus.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scoring</head><p>The official evaluation measure we used to rank the participating systems is Mean Average Precision (MAP) calculated for the ten comments a participating system has ranked highest. It is a wellestablished in Information Retrieval. We further report the results for two unofficial ranking measures, which we also calculate for the top-10 results only: Mean Reciprocal Rank (MRR) and Average Recall (AvgRec). Additionally, we report the results for four standard classification measures, which we calculate over the full list of results: Precision, Recall, F 1 (with respect to the Good/Relevant class) and Accuracy.</p><p>We released a specialized scorer that calculates and reports all above-mentioned seven scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Participants and Results</head><p>The list of all participating teams can be found in Table <ref type="table">7</ref>. The results for subtasks A, B, C, and D are shown in Tables <ref type="table" target="#tab_6">3, 4</ref>, 5, and 6, respectively. In all tables, the systems are ranked by the official MAP scores for their primary runs 19 (shown in the third column). The following columns show the scores based on the other six unofficial measures; the ranking with respect to these additional measures are marked with a subindex (for the primary runs).</p><p>Eighteen teams participated in the challenge presenting a variety of approaches and features to address the different subtasks. They submitted a total of 95 runs (38 primary and 57 contrastive), which are broken down by subtasks in the following way: The English subtasks A, B and C attracted 12, 11, and 10 systems and 29, 25 and 28 runs, respectively. The Arabic subtask D got 5 systems and 13 runs. The best MAP scores varied from 45.83 to 79.19, depending on the subtask. The best systems in each subtask were able to beat the baselines we provided by sizeable margins.</p><p>6.1 Subtask A, English (Question-Comment Similarity)</p><p>Table <ref type="table" target="#tab_6">3</ref> shows the results for subtask A, English, which attracted 12 teams, which submitted 29 runs: 12 primary and 17 contrastive. The last four rows of the table show the performance of four baselines. The first one is the chronological ranking, where the comments are ordered by their time of posting; we can see that all submissions outperform this baseline on all three ranking measures. The second baseline is a random baseline, which outperforms some systems in terms of F 1 , primarily because of having very high Recall. Baseline 3 classifies all comments as Good, and it outperforms four of the primary systems in terms of F 1 . Finally, baseline 4 classifies all comments as Bad; it outperforms one of the primary systems in terms of Accuracy. The winning team is that of KeLP <ref type="bibr" target="#b8">(Filice et al., 2016)</ref>, which achieved the highest MAP of 79.19, outperforming the second best by a margin; they are also first on AvgRec and MRR, and second on Accuracy. They learn semantic relations between questions and answers using kernels and previously-proposed features from . Their system is based on the KeLP machine learning platform , and thus the name of the team.</p><p>The second best system is that of ConvKN ( <ref type="bibr" target="#b3">Barrón-Cedeño et al., 2016)</ref> with MAP of 77.66; it is also first on Accuracy, second on F 1 , and third on AvgRec. The system combines convolutional tree kernels and convolutional neural networks, together with text similarity and thread-specific features. Their contrastive1 run achieved even better results: MAP of 78.71. The third best system is SemanticZ <ref type="bibr" target="#b24">(Mihaylov and Nakov, 2016b)</ref>   <ref type="bibr" target="#b27">(Mihaylova et al., 2016)</ref> with 77.16. The latter also has a contrastive run with MAP of 77.68, which would have ranked second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Subtask B, English (Question-Question</head><p>Similarity)</p><p>Table <ref type="table" target="#tab_7">4</ref> shows the results for subtask B, English, which attracted 11 teams and 25 runs: 11 primary and 14 contrastive. This turns out to be a hard task. For example, the IR baseline (i.e., ordering the related questions in the order provided by the search engine) outperforms 5 of the 11 systems in terms of MAP; it also outperforms several systems in terms of MRR and AvgRec. The random baseline outperforms one system in terms of F 1 and Accuracy, again due to high recall. The all-Good baseline outperforms two systems on F 1 , while the all-Bad baseline outperforms two systems on Accuracy. The winning team is that of UH-PRHLT (Franco-Salvador et al., 2016), which achieved MAP of 76.70 (just 2 MAP points over the IR baseline). They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet. Their contrastive2 run is even better, with MAP of 77.33.</p><p>The second best system is that of ConvKN ( <ref type="bibr" target="#b3">Barrón-Cedeño et al., 2016)</ref> with MAP of 76.02; they are also first on MRR, second on AvgRec and F 1 , and third on Accuracy.</p><p>The third best system is KeLP <ref type="bibr" target="#b8">(Filice et al., 2016</ref>) with MAP of 75.83; they are also first on AvgRec, F 1 , and Accuracy. They have a contrastive run with MAP of 76.28, which would have ranked second.</p><p>The fourth best, SLS <ref type="bibr" target="#b28">(Mohtarami et al., 2016</ref>) is very close, with MAP of 75.55; it is also first on MRR and Accuracy, and third on AvgRec. It uses a bag-of-vectors approach with various vector-and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Subtask C, English (Question-External</head><p>Comment Similarity)</p><p>The results for subtask C, English are shown in Table 5. This subtask attracted 10 teams, and 28 runs: 10 primary and 18 contrastive. Here the teams performed much better than they did for subtask B. The first three baselines were all outperformed by all participating systems. However, due to severe class imbalance, the all-Bad baseline outperformed 9 out of the 10 participating teams in terms of Accuracy.</p><p>The best system in this subtask is that of the SUper team <ref type="bibr" target="#b27">(Mihaylova et al., 2016)</ref>, which achieved MAP of 55.41; the system is also first on AvgRec and MRR. It used a rich set of features, grouped into three categories: question-specific features, answer-specific features, and question-answer similarity features. This includes more or less standard metadata, lexical, semantic, and user-related features, as well as some exotic ones such as features related to readability, credibility, as well as goodness polarity lexicons. <ref type="bibr">20</ref> It is important to note that this system did not try to solve subtask C directly, but rather just multipled their predicted score for subtask A by the reciprocal rank of the related question in the list of related questions (as returned by the search engine, and as readily provided by the organizers as an attribute in the XML file) for the original question. In fact, this is not an isolated case, but an approach taken by several participants in subtask C.</p><p>The second best system is that of KeLP <ref type="bibr" target="#b8">(Filice et al., 2016)</ref>, with MAP of 52.95; they are also first on F 1 , and second on AvgRec and MRR. KeLP also has a contrastive run with a MAP of 55.58, which would have made them first. This team really tried to solve the actual subtask C by means of stacking classifiers: they used their subtask A classifier to judge how good the answer is with respect to the original and with respect to the related question. Moreover, they used their subtask B classifier to judge the relatedness of the related question with respect to the original question. Finally, they used these three scores, together with some features based on them, to train a classifier that solves subtask C. <ref type="bibr">20</ref> These goodness polarity lexicons were at the core of another system, PMI-cool , which did not perform very well as it limited itself to lexicons and ignored other important features.</p><p>In fact, we anticipated solutions like this when we designed the task, i.e., that participants would solve subtasks A and B, and use them as auxiliary tasks to attack the main task, namely subtask C. Unfortunately, subtask B turned out to be too hard, and thus many participants decided to skip it and just to use the search engine's reciprocal rank.</p><p>The third best system is SemanticZ <ref type="bibr" target="#b24">(Mihaylov and Nakov, 2016b)</ref>, with MAP of 51.68. Similarly to SUper team, they simply multiply their predicted score for subtask A by the reciprocal rank of the related question in the list of related questions for the original question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Subtask D, Arabic (Reranking the correct answers for a new question)</head><p>Finally, the results for subtask D, Arabic are shown in Table <ref type="table">6</ref>. It attracted 5 teams, which submitted 13 runs: 5 primary and 8 contrastive. As the class imbalance here is even more severe than for subtask C, the all-Bad baseline outperforms all participating systems in terms of Accuracy. In contrast, the all-Good baseline only outperforms one system in terms of F 1 . Here the teams perform much better than for subtask B. The random baseline outperforms one system in terms of both MAP and AvgRec.</p><p>The clear winner here is SLS <ref type="bibr" target="#b28">(Mohtarami et al., 2016)</ref>, which is ranked first on all measures: MAP, AvgRec, MRR, F 1 , and Accuracy. Yet, their MAP of 45.83 is only slightly better than that of ConvKN, 45.50, which ranks second on MAP, AvgRec, MRR and F 1 , and third on Accuracy.</p><p>The third system is RDI <ref type="bibr" target="#b21">(Magooda et al., 2016)</ref> with MAP of 43.80, which is ranked third also on AvgRec and MRR. The system combines a TF.IDF module with a recurrent language model and information from Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Features and Techniques</head><p>The systems that participated in several subtasks typically re-used some features for all subtasks, whenever possible and suitable. Such features include the following: (i) similarity features between questions and comments from their threads or between original questions and related questions, e.g., cosine similarity applied to lexical, syntactic and semantic representations or distributed representa-tions, often derived using neural networks, (ii) content features, which are special signals that can clearly indicate a bad answer, e.g., when a comment contains "thanks", (iii) thread level/meta features, e.g., user ID, comment rank in the thread, and (iv) automatically generated features from syntactic structures using tree kernels.</p><p>Overall, most of the top positions are occupied by systems that used tree kernels, combined with similarity features. Regarding the machine learning approaches used, most systems chose SVM classifiers (often these were ranking versions such as SVM-Rank), or different kinds of neural networks. Below we look in more detail in the features and the used learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Feature Types</head><p>Participants preferred different kinds of features for different subtasks: Subtask A. Similarities between question subject vs. comment, question body vs. comment, and question subject+body vs. comment. Subtask B. Similarities between the original and the related question at different levels: subject vs. subject, body vs. body, and subject+body vs. sub-ject+body.</p><p>Subtask C. The same from above, plus the similarities of the original question subject, body, and full levels with the comments from the thread of the related question.</p><p>The similarity scores to be used as features were computed in various ways, e.g., the majority of teams used dot product calculated over word ngrams (n=1,2,3), character 3-grams, or with TF-IDF weighting. Or simply using word overlap, i.e., the number of common words between two texts, often normalized, e.g., by question/comment length. Or overlap in terms of nouns or named entities.</p><p>Several systems, e.g., UH-PRHLT, KeLP, SLS, SemanticZ, ECNU, used additional similarities based on distributed representations. For example, using the continuous skip-gram model of word2vec or Glove, trained on Google News, on the English Wikipedia, or on the unannotated Qatar Living dataset.</p><p>In particular, UH-PRHLT used word alignments and distributed representations to align the words of the question with the words of the comment.</p><p>On the alignment topic, it is worth mentioned that MTE-NN applied a model originally defined for machine translation evaluation <ref type="bibr" target="#b11">(Guzmán et al., 2015)</ref>, e.g., based on features computed with BLEU, TER, NIST, and Meteor <ref type="bibr" target="#b12">(Guzmán et al., 2016a)</ref>. Similarly, ECNU used Spearman, Pearson, and Kendall Ranking Coefficients as similarity scores for question similarity estimation, whereas ICL00 used word-toword translation probabilities, and UniMelb used convolutional neural networks (CNNs) fed with word embeddings and machine translation evaluation scores as input.</p><p>ConvKN used a CNN that also encodes relational links between the involved pieces of texts <ref type="bibr" target="#b36">(Severyn and Moschitti, 2015)</ref>. MTE-NN applied a simple neural network, ECNU and SLS used LSTM networks, and Overfitting applied Feedforward Neural Net Language Model (FNNLM).</p><p>It should be noted that ConvKN and KeLP used tree kernels with relational links <ref type="bibr" target="#b39">(Tymoshenko and Moschitti, 2015;</ref>, i.e., the questions are aligned with the comments (or with the other questions) by means of a special REL tag, directly annotated in the parse trees.</p><p>Regarding text structures, UH-PRHLT used Knowledge Graph Analysis, which consists in labeling, weighting, and expanding concepts in the text using a directed graph. They also used frames from FrameNet to generate semantic features.</p><p>Several teams, e.g., ConvKN, KeLP and SUper Team, used meta-features, such as the user ID. In particular, the SUper Team collected statistics about the users, e.g., the comments/questions they produced, time since their last activity, the number of good and bad comments in the training data, etc.</p><p>Other important features, which were used by most systems, are related to rank, e.g., rank of the comment in the question thread, or rank of the related question in the list of questions retrieved by the search engine for the original question. Some exotic features by the SUper Team modeled readability, credibility, sentiment analysis, trollness <ref type="bibr" target="#b25">(Mihaylov et al., 2015a;</ref><ref type="bibr" target="#b26">Mihaylov et al., 2015b;</ref><ref type="bibr" target="#b23">Mihaylov and Nakov, 2016a)</ref>, and goodness polarity, e.g., based on PMI lexicons as for PMI-Cool.</p><p>Regarding Arabic, QU-IR and SLS used word2vec, whereas RDI Team relied on language models. In particular, the winning SLS team used simple text-and vector-based features, where the text similarities are computed at the word-and the sentence-level. Most importantly, they computed two sets of features: one between the original and the related questions, and one between the original question and the related answer, which are then concatenated in one feature vector.</p><p>The ConvKN team combined some basic SLS features with tree kernels applied to syntactic trees, obtaining a result that is very close to that of the winning SLS team.</p><p>QU-IR used a standard Average Word Embedding and also a new method, Covariance Word Embedding, which computes a covariance matrix between each pair of dimensions of the embedding, thus considering vector components as random variables.</p><p>Finally, RDI used Arabic-Wikipedia to boost the weights of medical terms, which improved their ranking function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Learning Methods</head><p>The most popular machine learning approach was to use Support Vector Machines (SVM) on the features described in the previous section. SVMs were used in three different learning tasks: classification, regression, and ranking. Note that SVMs allow the use of complex convolutional kernels such as tree kernels, which were used by two systems (which in fact combined kernels with other features).</p><p>Neural networks were also widely used, e.g., in word2vec to train word embeddings. As previously mentioned, there were also systems using CNNs, LSTMs and FNNLM. Overfitting also used Random Forests.</p><p>Comparing tree kernels vs. neural networks: approaches based on the former were ranked first and second in Subtask A, second and third in Subtask B, and second in Subtasks C and D, while neural network-based systems did not win any subtask, but neural networks contributed to the best systems in all subtasks, e.g., with word2vec. Yet, post-competition improvements have shown that NN-based systems can perform on par with the best <ref type="bibr" target="#b12">(Guzmán et al., 2016a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have described SemEval-2016 Task 3 on Community Question Answering, which extended SemEval-2015 Task 3  with new subtasks (Question-Question similarity, Question-External Comment Similarity, and Reranking the correct answers for a new question), new evaluation metrics (based on ranking), new datasets, and new domains (biomedical for Arabic). The overall focus was on answering new questions that were not already answered in the target community forum.</p><p>The task attracted 18 teams, which submitted 95 runs; this is good growth compared to 2015, when 13 teams submitted 61 runs. The participants built on the lessons learned from the 2015 edition of the task, and further experimented with new features and learning frameworks. It was interesting to see that the top systems used both word embeddings trained using neural networks and syntactic kernels, which shows the importance of both distributed representations and linguistic analysis. It was also nice to see some new features being tried.</p><p>Apart from the new lessons learned from this year's edition, we believe that the task has another important contribution: the datasets we have created as part of the task (with over 7,000 questions and over 57,000 annotated comments), and which we have released for use to the research community, should be useful for follow up research beyond SemEval.</p><p>Finally, given the growth in the interest for the task, we plan a rerun at SemEval-2017 with data from a new domain.   Table <ref type="table">5</ref>: Subtask C, English (Question-External Comment Similarity): results for all submissions. The first column shows the rank of the primary runs with respect to the official MAP score. The second column contains the team's name and its submission type (primary vs. contrastive). The following columns show the results for the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column. Table <ref type="table">6</ref>: Subtask D, Arabic (Reranking the correct answers for a new question): results for all submissions. The first column shows the rank of the primary runs with respect to the official MAP score. The second column contains the team's name and its submission type (primary vs. contrastive). The following columns show the results for the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Annotated English question from the CQA-QL corpus. Shown are the first two comments only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Annotated question from the Arabic CQA-MD corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Main statistics about the English CQA-QL corpus.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main statistics about the CQA-MD corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>with MAP of 77.58. They use semantic similarity based on word embeddings and topics; they are second on AvgRec and MRR. Note also the cluster of systems of very close MAP: ConvKN (Barrón-Cedeño et al., 2016) with 77.66, SemanticZ (Mihaylov and Nakov, 2016b) with 77.58, ECNU (Wu and Lan, 2016) with 77.28, and SUper team</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>88.82 1 86.42 1 76.96 1 55.30 8 64.36 5 75.11 2 88.05 3 84.93 4 75.56 2 58.84 6 66.16 2 75.54 1 3 SemanticZ-primary 77.58 3 88.14 2 85.21 2 74.13 4 53.05 10 61.84 8 73.39 5 86.74 7 84.97 3 56.28 9 76.22 1 64.75 3 66.27 8 7 SLS-primary 76.33 7 87.30 6 82.99 7 60.36 8 67.72 3 63.83 6 68.81 7 82.67 9 80.26 8 73.18 5 19.71 12 31.06 12 64.43 9 83.36 8 77.38 10 62.48 7 62.53 5 62.50 7 69.51 6 10 PMI-cool-primary 68.79 10 79.94 10 80.00 9 47.81 12 70.58 2 57.00 9 56.73 12 11 79.38 11 76.97 11 55.64 10 46.80 11 50.84 11 63.21 10 12 75.41 12 70.58 12 50.28 11 53.50 9 51.84 10 59.60 11</figDesc><table><row><cell cols="3">MAP AvgRec 79.19 1 ConvKN-contrastive1 1 Kelp-primary 78.71 88.98</cell><cell>MRR 86.15</cell><cell>P 77.78</cell><cell>R 53.72</cell><cell>F 1 63.55</cell><cell>Acc 74.95</cell></row><row><cell>SUper team-contrastive1</cell><cell>77.68</cell><cell>88.06</cell><cell>84.76</cell><cell>75.59</cell><cell>55.00</cell><cell>63.68</cell><cell>74.50</cell></row><row><cell cols="2">2 ConvKN-primary 77.66 2 ConvKN-contrastive2 77.29</cell><cell>87.77</cell><cell>85.03</cell><cell>74.74</cell><cell>59.67</cell><cell>66.36</cell><cell>75.41</cell></row><row><cell>4 ECNU-primary SemanticZ-contrastive1</cell><cell cols="7">77.28 4 87.52 5 84.09 6 70.46 6 63.36 4 66.72 1 74.31 4 77.16 87.73 84.08 75.29 53.20 62.35 73.88</cell></row><row><cell>5 SUper team-primary MTE-NN-contrastive2</cell><cell cols="7">77.16 5 87.98 4 84.69 5 74.43 3 56.73 7 64.39 4 74.50 3 76.98 86.98 85.50 58.71 70.28 63.97 67.83</cell></row><row><cell>SUper team-contrastive2</cell><cell>76.97</cell><cell>87.89</cell><cell>84.58</cell><cell>74.31</cell><cell>56.36</cell><cell>64.10</cell><cell>74.34</cell></row><row><cell>MTE-NN-contrastive1</cell><cell>76.86</cell><cell>87.03</cell><cell>84.36</cell><cell>55.84</cell><cell>77.35</cell><cell>64.86</cell><cell>65.93</cell></row><row><cell>SLS-contrastive2</cell><cell>76.71</cell><cell>87.17</cell><cell>84.38</cell><cell>59.45</cell><cell>67.95</cell><cell>63.41</cell><cell>68.13</cell></row><row><cell>SLS-contrastive1</cell><cell>76.46</cell><cell>87.47</cell><cell>83.27</cell><cell>60.09</cell><cell>69.68</cell><cell>64.53</cell><cell>68.87</cell></row><row><cell cols="2">6 MTE-NN-primary 76.44 6 ECNU-contrastive2 75.71</cell><cell>86.14</cell><cell>82.53</cell><cell>63.60</cell><cell>66.67</cell><cell>65.10</cell><cell>70.95</cell></row><row><cell>SemanticZ-contrastive2</cell><cell>75.41</cell><cell>86.51</cell><cell>82.52</cell><cell>73.19</cell><cell>50.11</cell><cell>59.49</cell><cell>72.26</cell></row><row><cell>ICRC-HIT-contrastive1</cell><cell>73.34</cell><cell>84.81</cell><cell>79.65</cell><cell>63.43</cell><cell>69.30</cell><cell>66.24</cell><cell>71.28</cell></row><row><cell cols="2">8 ITNLP-AiKF-primary 71.52 8 ECNU-contrastive1 71.34</cell><cell>83.39</cell><cell>78.62</cell><cell>66.95</cell><cell>41.31</cell><cell>51.09</cell><cell>67.86</cell></row><row><cell cols="2">9 ICRC-HIT-primary 70.90 9 UH-PRHLT-contrastive1 67.57</cell><cell>79.50</cell><cell>77.08</cell><cell>54.10</cell><cell>50.11</cell><cell>52.03</cell><cell>62.45</cell></row><row><cell cols="2">11 UH-PRHLT-primary 67.42 UH-PRHLT-contrastive2 67.33</cell><cell>79.34</cell><cell>76.73</cell><cell>54.97</cell><cell>49.13</cell><cell>51.89</cell><cell>62.97</cell></row><row><cell cols="2">12 QAIIIT-primary 62.24 QAIIIT-contrastive2 61.93</cell><cell>75.22</cell><cell>69.95</cell><cell>49.48</cell><cell>49.96</cell><cell>49.72</cell><cell>58.93</cell></row><row><cell>QAIIIT-contrastive1</cell><cell>61.80</cell><cell>75.12</cell><cell>69.76</cell><cell>49.85</cell><cell>50.94</cell><cell>50.39</cell><cell>59.24</cell></row><row><cell>Baseline 1 (chronological) Baseline 2 (random)</cell><cell>59.53 52.80</cell><cell>72.60 66.52</cell><cell>67.83 58.71</cell><cell>-40.56</cell><cell>-74.57</cell><cell>-52.55</cell><cell>-45.26</cell></row><row><cell>Baseline 3 (all 'true')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>40.64</cell><cell>100.00</cell><cell>57.80</cell><cell>40.64</cell></row><row><cell>Baseline 4 (all 'false')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Subtask A, English (Question-Comment Similarity): results for all submissions. The first column shows the rank of the primary runs with respect to the official MAP score. The second column contains the team's name and its submission type (primary vs. contrastive). The following columns show the results for the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column. 89.33 5 83.02 4 33.29 11 100.00 1 49.95 9 33.29 11 89.07 6 81.48 7 100.00 1 18.03 11 30.55 11 72.71 9</figDesc><table><row><cell>Submission</cell><cell cols="2">MAP AvgRec</cell><cell>MRR</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>Acc</cell></row><row><cell>UH-PRHLT-contrastive2</cell><cell>77.33</cell><cell>90.84</cell><cell>83.93</cell><cell>63.57</cell><cell>70.39</cell><cell>66.80</cell><cell>76.71</cell></row><row><cell>1 UH-PRHLT-primary UH-PRHLT-contrastive1</cell><cell cols="3">76.70 1 90.31 4 83.02 4 76.56 90.22 83.02</cell><cell>63.53 7 62.74</cell><cell>69.53 3 70.82</cell><cell>66.39 3 66.53</cell><cell>76.57 4 76.29</cell></row><row><cell>Kelp-contrastive1</cell><cell>76.28</cell><cell>91.33</cell><cell>82.71</cell><cell>63.83</cell><cell>77.25</cell><cell>69.90</cell><cell>77.86</cell></row><row><cell>Kelp-contrastive2</cell><cell>76.27</cell><cell>91.44</cell><cell>84.10</cell><cell>64.06</cell><cell>77.25</cell><cell>70.04</cell><cell>78.00</cell></row><row><cell>SLS-contrastive1</cell><cell>76.17</cell><cell>90.55</cell><cell>85.48</cell><cell>74.39</cell><cell>52.36</cell><cell>61.46</cell><cell>78.14</cell></row><row><cell>SLS-contrastive2</cell><cell>76.09</cell><cell>90.14</cell><cell>84.21</cell><cell>77.21</cell><cell>45.06</cell><cell>56.91</cell><cell>77.29</cell></row><row><cell>2 ConvKN-primary 3 Kelp-primary ConvKN-contrastive1</cell><cell cols="3">76.02 2 90.70 2 84.64 1 75.83 3 91.02 1 82.71 6 75.57 89.64 83.57</cell><cell>68.58 3 66.79 4 63.77</cell><cell>66.52 6 75.97 2 72.53</cell><cell>67.54 2 71.08 1 67.87</cell><cell>78.71 3 79.43 1 77.14</cell></row><row><cell>4 SLS-primary SUper team-contrastive1</cell><cell cols="3">75.55 4 90.65 3 84.64 1 75.17 88.84 83.66</cell><cell>76.33 2 63.25</cell><cell>55.36 9 63.52</cell><cell>64.18 6 63.38</cell><cell>79.43 1 75.57</cell></row><row><cell cols="2">5 ICL00-primary 75.11 5 ICL00-contrastive1 74.89</cell><cell>89.08</cell><cell>82.71</cell><cell>33.29</cell><cell>100.00</cell><cell>49.95</cell><cell>33.29</cell></row><row><cell>6 SUper team-primary ICL00-contrastive2</cell><cell cols="3">74.82 6 88.54 7 83.66 3 74.05 89.11 82.79</cell><cell>63.64 6 33.29</cell><cell>57.08 8 100.00</cell><cell>60.18 7 49.95</cell><cell>74.86 7 33.29</cell></row><row><cell cols="2">7 ECNU-primary 73.92 7 ECNU-contrastive1 73.25</cell><cell>88.55</cell><cell>80.81</cell><cell>100.00</cell><cell>18.03</cell><cell>30.55</cell><cell>72.71</cell></row><row><cell>ECNU-contrastive2</cell><cell>71.62</cell><cell>86.55</cell><cell>80.88</cell><cell>54.61</cell><cell>71.24</cell><cell>61.82</cell><cell>70.71</cell></row><row><cell>8 ITNLP-AiKF-primary 9 UniMelb-primary 10 overfitting-primary QAIIIT-contrastive1</cell><cell cols="3">71.43 8 87.31 8 81.28 8 70.20 9 86.21 9 78.58 11 69.68 10 85.10 10 80.18 9 69.24 85.24 80.30</cell><cell>62.75 9 63.96 5 63.20 8 38.99</cell><cell>68.67 4 54.08 10 67.81 5 66.09</cell><cell>65.57 4 58.60 8 65.42 5 49.04</cell><cell>76.00 6 74.57 8 76.14 5 54.29</cell></row><row><cell>11 QAIIIT-primary QAIIIT-contrastive2</cell><cell cols="4">69.04 11 84.53 11 79.55 10 39.53 10 46.23 68.07 48.92 36.25</cell><cell>64.81 7 51.50</cell><cell>49.11 10 42.55</cell><cell>55.29 10 53.71</cell></row><row><cell>Baseline 1 (IR) Baseline 2 (random)</cell><cell>74.75 46.98</cell><cell>88.30 67.92</cell><cell>83.79 50.96</cell><cell>-32.58</cell><cell>-73.82</cell><cell>-45.20</cell><cell>-40.43</cell></row><row><cell>Baseline 3 (all 'true')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>33.29</cell><cell>100.00</cell><cell>49.95</cell><cell>33.29</cell></row><row><cell>Baseline 4 (all 'false')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Subtask B, English (Question-Question Similarity): results for all submissions. The first column shows the rank of the primary runs with respect to the official MAP score. The second column contains the team's name and its submission type (primary vs. contrastive). The following columns show the results for the primary, and then for other, unofficial evaluation measures. The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column. 51.07 7 53.89 6 9.34 10 100.00 1 17.09 8 9.34 10 6 SLS-primary 49.09 6 56.04 3 55.98 3 47.85 2</figDesc><table><row><cell>Submission</cell><cell cols="2">MAP AvgRec</cell><cell>MRR</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>Acc</cell></row><row><cell>Kelp-contrastive2</cell><cell>55.58</cell><cell>63.36</cell><cell>61.19</cell><cell>32.21</cell><cell>70.18</cell><cell>44.16</cell><cell>83.41</cell></row><row><cell>1 SUper team-primary SUper team-contrastive2</cell><cell cols="4">55.41 1 60.66 1 61.48 1 18.03 7 53.48 59.40 59.09 18.42</cell><cell>63.15 4 66.97</cell><cell>28.05 4 28.89</cell><cell>69.73 8 69.20</cell></row><row><cell>2 Kelp-primary Kelp-contrastive1</cell><cell cols="4">52.95 2 59.27 2 59.23 2 33.63 5 52.95 59.34 58.06 34.08</cell><cell>64.53 3 65.29</cell><cell>44.21 1 44.78</cell><cell>84.79 5 84.96</cell></row><row><cell>3 SemanticZ-primary SemanticZ-contrastive1</cell><cell cols="4">51.68 3 53.43 6 55.96 4 17.11 8 51.46 52.69 55.75 16.94</cell><cell>57.65 5 57.49</cell><cell>26.38 5 26.17</cell><cell>69.94 7 69.69</cell></row><row><cell>MTE-NN-contrastive2</cell><cell>49.49</cell><cell>55.78</cell><cell>51.80</cell><cell>15.68</cell><cell>73.09</cell><cell>25.82</cell><cell>60.76</cell></row><row><cell>4 MTE-NN-primary 5 ICL00-primary</cell><cell cols="5">76.15 2 49.19 5 13.61 8 49.38 4 55.44 4 51.56 7 15.26 9</cell><cell>25.43 6 21.19 7</cell><cell>58.27 9 90.54 2</cell></row><row><cell>SemanticZ-contrastive2</cell><cell>48.76</cell><cell>50.72</cell><cell>53.85</cell><cell>16.92</cell><cell>57.34</cell><cell>26.13</cell><cell>69.71</cell></row><row><cell>MTE-NN-contrastive1</cell><cell>48.52</cell><cell>54.71</cell><cell>50.51</cell><cell>15.13</cell><cell>76.61</cell><cell>25.27</cell><cell>57.67</cell></row><row><cell>7 ITNLP-AiKF-primary ECNU-contrastive1</cell><cell cols="4">48.49 7 55.16 5 55.21 5 30.05 6 48.49 53.17 53.47 68.75</cell><cell>50.92 6 8.41</cell><cell>37.80 2 14.99</cell><cell>84.34 6 91.09</cell></row><row><cell>SUper team-contrastive1</cell><cell>48.23</cell><cell>54.93</cell><cell>54.85</cell><cell>22.81</cell><cell>36.70</cell><cell>28.14</cell><cell>82.49</cell></row><row><cell>ECNU-contrastive2</cell><cell>47.24</cell><cell>53.21</cell><cell>51.89</cell><cell>70.27</cell><cell>7.95</cell><cell>14.29</cell><cell>91.09</cell></row><row><cell>ICL00-contrastive1</cell><cell>47.23</cell><cell>49.71</cell><cell>50.28</cell><cell>9.34</cell><cell>100.00</cell><cell>17.09</cell><cell>9.34</cell></row><row><cell>8 ConvKN-primary SLS-contrastive1</cell><cell cols="4">47.15 8 47.46 10 51.43 8 45.97 3 46.48 53.31 52.53 16.24</cell><cell>8.72 10 85.93</cell><cell>14.65 10 27.32</cell><cell>90.51 3 57.29</cell></row><row><cell>9 ECNU-primary SLS-contrastive2</cell><cell cols="4">46.47 9 50.92 8 51.41 9 66.29 1 46.39 52.83 51.17 16.18</cell><cell>9.02 9 85.63</cell><cell>15.88 9 27.22</cell><cell>91.07 1 57.23</cell></row><row><cell>UH-PRHLT-contrastive1</cell><cell>43.37</cell><cell>48.01</cell><cell>48.43</cell><cell>38.56</cell><cell>32.72</cell><cell>35.40</cell><cell>88.84</cell></row><row><cell>UH-PRHLT-contrastive2</cell><cell>43.32</cell><cell>47.97</cell><cell>48.45</cell><cell>38.21</cell><cell>32.72</cell><cell>35.26</cell><cell>88.77</cell></row><row><cell>ConvKN-contrastive1</cell><cell>43.31</cell><cell>44.19</cell><cell>48.89</cell><cell>30.00</cell><cell>3.21</cell><cell>5.80</cell><cell>90.26</cell></row><row><cell>10 UH-PRHLT-primary ICL00-contrastive2</cell><cell cols="4">43.20 10 47.96 9 47.79 10 37.65 4 41.32 44.56 43.55 9.34</cell><cell>34.25 7 100.00</cell><cell>35.87 3 17.09</cell><cell>88.56 4 9.34</cell></row><row><cell>ConvKN-contrastive2</cell><cell>41.12</cell><cell>38.89</cell><cell>44.17</cell><cell>33.55</cell><cell>32.11</cell><cell>32.81</cell><cell>87.71</cell></row><row><cell>Baseline 1 (IR+chronological) Baseline 2 (random)</cell><cell>40.36 15.01</cell><cell>45.97 11.44</cell><cell>45.83 15.19</cell><cell>-9.40</cell><cell>-75.69</cell><cell>-16.73</cell><cell>-29.59</cell></row><row><cell>Baseline 3 (all 'true')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.34</cell><cell>100.00</cell><cell>17.09</cell><cell>9.34</cell></row><row><cell>Baseline 4 (all 'false')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>47.45 3 49.21 3 19.24 5 100.00 1 32.27 4 19.24 5</figDesc><table><row><cell>Submission 1 SLS-primary 2 ConvKN-primary SLS-contrastive1</cell><cell cols="4">P 45.83 1 51.01 1 53.66 1 34.45 1 MAP AvgRec MRR 45.50 2 50.13 2 52.55 2 28.55 2 44.94 49.72 51.58 62.96</cell><cell>R 52.33 3 64.53 2 2.40</cell><cell>F 1 41.55 1 39.58 2 4.62</cell><cell>Acc 71.67 1 62.10 3 80.95</cell></row><row><cell cols="2">3 RDI team-primary 43.80 3 SLS-contrastive2 42.95</cell><cell>47.61</cell><cell>49.55</cell><cell>27.20</cell><cell>74.40</cell><cell>39.84</cell><cell>56.76</cell></row><row><cell>RDI team-contrastive1</cell><cell>42.18</cell><cell>47.03</cell><cell>47.93</cell><cell>19.24</cell><cell>100.00</cell><cell>32.27</cell><cell>19.24</cell></row><row><cell>ConvKN-contrastive2</cell><cell>39.98</cell><cell>43.68</cell><cell>46.41</cell><cell>26.26</cell><cell>68.39</cell><cell>37.95</cell><cell>57.00</cell></row><row><cell>QU-IR-contrastive2</cell><cell>39.07</cell><cell>42.72</cell><cell>44.14</cell><cell>24.90</cell><cell>44.08</cell><cell>31.82</cell><cell>63.66</cell></row><row><cell>RDI team-contrastive2</cell><cell>38.84</cell><cell>42.98</cell><cell>42.97</cell><cell>19.24</cell><cell>100.00</cell><cell>32.27</cell><cell>19.24</cell></row><row><cell>4 QU-IR-primary ConvKN-contrastive1</cell><cell cols="4">38.63 4 44.10 4 46.27 4 25.50 3 38.33 42.09 43.75 20.38</cell><cell>45.13 5 96.95</cell><cell>32.59 3 33.68</cell><cell>64.07 2 26.58</cell></row><row><cell>QU-IR-contrastive1</cell><cell>37.80</cell><cell>40.96</cell><cell>44.39</cell><cell>23.54</cell><cell>41.89</cell><cell>30.14</cell><cell>62.64</cell></row><row><cell>5 UPC USMBA-primary Baseline 1 (chronological) Baseline 2 (random)</cell><cell cols="4">29.09 5 30.04 5 34.04 5 20.14 4 28.71 30.93 -28.88 29.79 31.00 33.71 19.53</cell><cell>51.69 4 -20.66</cell><cell>28.99 5 -20.08</cell><cell>51.27 4 -68.35</cell></row><row><cell>Baseline 3 (all 'true')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.24</cell><cell>100.00</cell><cell>32.27</cell><cell>19.24</cell></row><row><cell>Baseline 4 (all 'false')</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.76</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://stackoverflow.com/ 4 http://www.qatarliving.com/forum</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We limit the number of comments we consider to the first ten only in order to spare some annotation efforts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that the search engine indexes entire Web pages, and thus, the search engine has compared the original question to the related questions together with their comment threads.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">As most of the time the comment's subject is just "RE: &lt;question subject&gt;", we decided to drop it from the dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://www.crowdflower.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The hidden tests for all subtasks were generated gradually. We started with a small number of initial tests, verified by two task coorganizers, and we gradually added more, choosing from those for which we had highest annotation agreement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Here are the annotation instructions we used for part2: http://alt.qcri.org/semeval2016/task3/data/uploads/annotation instructions for part2.pdf13  In fact, minimally cleansed, so that the XML file is valid.14  The same question can be retrieved as related for different original questions. These are not repetitions for subtasks B and C, but they are such for subtask A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">http://www.webteb.com/ 16 http://www.altibbi.com/ 17 http://consult.islamweb.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">We had less than 30 answers for some questions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">Participants could submit one primary run, to be used for the official ranking, and up to two contrastive runs, which are scored but have unofficial status.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar Foundation. It is part of the Interactive sYstems for Answer Search (Iyas) project, which is developed in collaboration with MIT-CSAIL.</p><p>We would like to thank the anonymous reviewers for their constructive comments, which have helped us improve the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PMI-cool at SemEval-2016 Task 3: Experiments with PMI and goodness polarity lexicons for community question answering</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Balchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasen</forename><surname>Kiprov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thread-level information for comment classification in community question answering</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
				<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL-IJCNLP &apos;15</title>
				<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="687" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ConvKN at SemEval-2016 Task 3: Answer and question selection for question answering on arabic and english fora</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Al</forename><surname>Fahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Obaidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kateryna</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName><surname>Uva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SI-GIR conference on Research and development in information retrieval, SIGIR &apos;09</title>
				<meeting>the 32nd international ACM SI-GIR conference on Research and development in information retrieval, SIGIR &apos;09<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ITNLP-AiKF at SemEval-2016 Task 3: Community question answering</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Chang E</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UPC-USMBA at SemEval-2016 Task 3: UPC-USMBA participation in SemEval 2016 task 3, subtask d: CQA for Arabic</title>
		<author>
			<persName><forename type="first">Yassine</forename><surname>El Adlouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imane</forename><surname>Lahbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Meknassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Said Ouatik El</forename><surname>Alaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>Sem; San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KeLP: a Kernel-based Learning Platform in java</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Machine Learning Open Source Software: Open Ecosystems</title>
				<meeting>the workshop on Machine Learning Open Source Software: Open Ecosystems<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KeLP at SemEval-2016 Task 3: Learning semantic relations between questions and answers</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Marc</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="UH-PRHLTatSemEval-2016" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining lexical and semantic-based features for community question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise neural machine translation evaluation</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
	<note>ACL-IJCNLP &apos;15</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine translation evaluation meets community question answering</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
				<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL &apos;16</title>
				<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MTE-NN at SemEval-2016 Task 3: Can machine translation evaluation help community question answering?</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT &apos;10</title>
				<meeting>the Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT &apos;10<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UniMelb at SemEval-2016 Task 3: Identifying similar questions by combining a CNN with string similarity measures</title>
		<author>
			<persName><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahar</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA. Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun Zhang; Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
	<note>Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval &apos;15</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding similar questions in large question and answer archives</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Jiwoon Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon Ho</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM &apos;05</title>
				<meeting>the 14th ACM International Conference on Information and Knowledge Management, CIKM &apos;05<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global thread-level inference for comment classification in community question answering</title>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;15</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;15<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="573" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint learning with global inference for comment classification in community question answering</title>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;16</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
				<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RDI at SemEval-2016 Task 3: RDI unsupervised framework for text ranking</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Magooda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Mahgoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazem</forename><surname>Raafat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eslam</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><forename type="middle">Al</forename><surname>Sallab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">QU-IR at SemEval-2016 Task 3: Learning to rank on Arabic community question answering forums with word embedding</title>
		<author>
			<persName><forename type="first">Rana</forename><surname>Malhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwan</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hunting for troll comments in news community forums</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL &apos;16<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SemanticZ at SemEval-2016 Task 3: Ranking relevant answers in community question answering using semantic similarity based on fine-tuned word embeddings</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding opinion manipulation trolls in news community forums</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning, CoNLL &apos;15</title>
				<meeting>the Nineteenth Conference on Computational Natural Language Learning, CoNLL &apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="310" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exposing paid opinion manipulation trolls</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP &apos;15</title>
				<meeting>the International Conference Recent Advances in Natural Language Processing, RANLP &apos;15<address><addrLine>Hissar</addrLine></address></meeting>
		<imprint>
			<publisher>Bulgaria</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering</title>
		<author>
			<persName><forename type="first">Tsvetomila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Gencheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Boyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Yovcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Momchil</forename><surname>Hardalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasen</forename><surname>Kiprov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Balchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ivelina Nikolova, and Galia Angelova</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SLS at SemEval-2016 Task 3: Neuralbased approaches for ranking in community question answering</title>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting syntactic and shallow semantic kernels for question answer classification</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="776" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernel methods, syntax and semantics for relational text categorization</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
				<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>Napa Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 3: Answer selection in community question answering</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Randeree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15</title>
				<meeting>the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">QCRI: Answer selection for community question answering -experiments for Arabic and English</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15</title>
				<meeting>the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Query chains: Learning to rank from implicit feedback</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD &apos;05</title>
				<meeting>the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD &apos;05<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structural relationships for large-scale learning of answer re-ranking</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12</title>
				<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="741" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL &apos;07</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL &apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to rank answers on large online QA collections</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT &apos;08</title>
				<meeting>the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT &apos;08<address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking</title>
		<author>
			<persName><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL &apos;16</title>
				<editor>
			<persName><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
			<persName><forename type="first">Daniele</forename><surname>Bonadiman</surname></persName>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</editor>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL &apos;16<address><addrLine>Melbourne, Australia; San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
	<note>Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Probabilistic tree-edit models with structured latent variables for textual entailment and question answering</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
				<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Overfitting at SemEval-2016 Task 3: Detecting semantically similar questions in community question answering forums with word embeddings</title>
		<author>
			<persName><forename type="first">Hujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? A quasisynchronous grammar for QA</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL &apos;07</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL &apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2016 Task 3: Exploring traditional method and deep learning method for question retrieval and answer ranking in community question answering</title>
		<author>
			<persName><forename type="first">Guoshun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</title>
				<meeting>the 10th International Workshop on Semantic Evaluation, SemEval &apos;16<address><addrLine>San Diego, CA; San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval &apos;16</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;13</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Answer sequence learning with neural networks for answer selection in community question answering</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ICRC-HIT: A deep learning based comment sequence labeling system for answer selection challenge</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15</title>
				<meeting>the 9th International Workshop on Semantic Evaluation, Sem-Eval &apos;15<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="210" to="214" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
