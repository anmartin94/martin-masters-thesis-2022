<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The CoNLL-2013 Shared Task on Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for English Language Communication</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for English Language Communication</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Hadiwinoto</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
							<email>joel.tetreault@nuance.com</email>
							<affiliation key="aff3">
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The CoNLL-2013 Shared Task on Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CoNLL-2013 shared task was devoted to grammatical error correction. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 <ref type="bibr">(CoNLL-2013)</ref>. In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay.</p><p>This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years <ref type="bibr" target="#b6">(Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b7">Dale et al., 2012)</ref>. In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.</p><p>The CoNLL-2013 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer. This overview paper contains a detailed description of the shared task, and is organized as follows. Section 2 provides the task definition. Section 3 describes the annotated training data provided and the blind test data. Section 4 describes the evaluation metric and the scorer. Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams. Section 6 presents the results of the shared task. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>The goal of the CoNLL-2013 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form.</p><p>Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors <ref type="bibr" target="#b10">(Han et al., 2006;</ref><ref type="bibr" target="#b9">Gamon, 2010;</ref><ref type="bibr" target="#b14">Rozovskaya and Roth, 2010;</ref><ref type="bibr" target="#b2">Dahlmeier and Ng, 2011b)</ref>, with relatively less work on correcting word choice errors <ref type="bibr" target="#b1">(Dahlmeier and Ng, 2011a)</ref>. Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors).</p><p>In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositions. Table <ref type="table">1</ref> shows examples of the five error types in our shared task.</p><p>Since there are five error types in our shared task compared to two in HOO 2012, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task relative to that of HOO 2012. To illustrate, consider the following sentence:</p><p>Although we have to admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded.</p><p>The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of <ref type="bibr" target="#b3">(Dahlmeier and Ng, 2012a)</ref>, for example, is designed to deal with multiple, interacting errors.</p><p>Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types are made. The other errors are to be left uncorrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>This section describes the training and test data released to each participating team in our shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data</head><p>The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English <ref type="bibr" target="#b5">(Dahlmeier et al., 2013)</ref>. As noted by <ref type="bibr" target="#b12">(Leacock et al., 2010)</ref>, the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The grammatical errors in these essays have been hand-corrected by professional English instructors at NUS. For each grammatical error instance, the start and end character offsets of the erroneous text span are marked, and the error type and the correction string are provided. Manual annotation is carried out using a graphical user interface specifically built for this purpose. The error annotations are saved as stand-off annotations, in SGML format.</p><p>To illustrate, consider the following sentence at the start of the first paragraph of an essay:</p><p>From past to the present, many important innovations have surfaced.</p><p>There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in Figure <ref type="figure" target="#fig_0">1</ref>. start par (end par) denotes the paragraph ID of the start (end) of the erroneous text span (paragraph ID starts from 0 by convention). start off (end off) denotes the character offset of the start (end) of the erroneous text span (again, character offset starts from 0 by convention). The error tag is ArtOrDet, and the correction string is the past. &lt;MISTAKE start par="0" start off="5" end par="0" end off="9"&gt; &lt;TYPE&gt;ArtOrDet&lt;/TYPE&gt; &lt;CORRECTION&gt;the past&lt;/CORRECTION&gt; &lt;/MISTAKE&gt; The NUCLE corpus was first used in <ref type="bibr" target="#b2">(Dahlmeier and Ng, 2011b)</ref>, and has been publicly available for research purposes since June 2011 1 . All instances of grammatical errors are annotated in NUCLE, and the errors are classified into 27 error types <ref type="bibr" target="#b5">(Dahlmeier et al., 2013)</ref>.</p><p>To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NU-CLE essays include sentence segmentation and word tokenization using the NLTK toolkit <ref type="bibr" target="#b0">(Bird et al., 2009)</ref>, and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser <ref type="bibr" target="#b11">(Klein and Manning, 2003;</ref><ref type="bibr" target="#b8">de Marneffe et al., 2006)</ref>. The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Revised version of NUCLE</head><p>NUCLE release version 2.3 was used in the CoNLL-2013 shared task. In this version, 17 essays were removed from the first release of NU-CLE since these essays were duplicates with multiple annotations.</p><p>In the original NUCLE corpus, there is not an explicit preposition error type. Instead, preposition errors are part of the Wcip (wrong collocation/idiom/preposition) and Rloc (local redundancy) error types. The Wcip error type combines errors concerning collocations, idioms, and prepositions together into one error type. The Rloc error type annotates extraneous words which are redundant and should be removed, and they include redundant articles, determiners, and prepositions.  The statistics of the NUCLE corpus (release 2.3 version) are shown in Table <ref type="table" target="#tab_2">2</ref>. The distribution of errors among the five error types is shown in Table <ref type="table">3</ref>. The newly added noun number error type in our shared task accounts for the second highest number of errors among the five error types. The five error types in our shared task constitute 35% of all grammatical errors in the training data, and 47% of all errors in the test data. These figures support our choice of these five error types to be the focus of our shared task, since they account for a large percentage of all grammatical errors in English learner essays.</p><p>While the NUCLE corpus is provided in our shared task, participating teams are free to not use NUCLE, or to use additional resources and tools in building their grammatical error correction systems, as long as these resources and tools are publicly available and not proprietary. For example, participating teams are free to use the Cambridge FCE corpus <ref type="bibr" target="#b17">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b13">Nicholls, 2003)</ref>  Error annotation on the test essays was carried out by a native speaker of English who is a lecturer at the NUS Centre for English Language Communication. The distribution of errors in the test essays among the five error types is shown in Table 3. The test essays were then preprocessed in the same manner as the NUCLE corpus. The preprocessed test essays were released to the participating teams.</p><p>Unlike the test data used in HOO 2012 which was proprietary and not available after the shared task, the test essays and their error annotations in the CoNLL-2013 shared task are freely available after the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Metric and Scorer</head><p>A grammatical error correction system is evaluated by how well its proposed corrections or edits match the gold-standard edits. An essay is first sentence-segmented and tokenized before evaluation is carried out on the essay. To illustrate, consider the following tokenized sentence S written by an English learner:</p><p>There is no a doubt, tracking system ID Prompt 1 Surveillance technology such as RFID (radio-frequency identification) should not be used to track people (e.g., human implants and RFID tags on people or products). Do you agree? Support your argument with concrete examples. 2 Population aging is a global phenomenon. Studies have shown that the current average life span is over 65. Projections of the United Nations indicate that the population aged 60 or over in developed and developing countries is increasing at 2% to 3% annually. Explain why rising life expectancies can be considered both a challenge and an achievement. has brought many benefits in this information age .</p><p>The set of gold-standard edits of a human annotator is g = {a doubt → doubt, system → systems, has → have}. Suppose the tokenized output sentence H of a grammatical error correction system given the above sentence is:</p><p>There is no doubt, tracking system has brought many benefits in this information age .</p><p>That is, the set of system edits is e = {a doubt → doubt}. The performance of the grammatical error correction system is measured by how well the two sets g and e match, in the form of recall R, precision P , and F 1 measure: R = 1/3, P = 1/1, F 1 = 2RP/(R + P ) = 1/2.</p><p>More generally, given a set of n sentences, where g i is the set of gold-standard edits for sentence i, and e i is the set of system edits for sentence i, recall, precision, and F 1 are defined as follows:</p><formula xml:id="formula_0">R = n i=1 |g i ∩ e i | n i=1 |g i | (1) P = n i=1 |g i ∩ e i | n i=1 |e i | (2) F 1 = 2 × R × P R + P (<label>3</label></formula><formula xml:id="formula_1">)</formula><p>where the intersection between g i and e i for sentence i is defined as</p><formula xml:id="formula_2">g i ∩ e i = {e ∈ e i |∃g ∈ g i , match(g, e)} (4)</formula><p>Evaluation by the HOO scorer <ref type="bibr" target="#b6">(Dale and Kilgarriff, 2011</ref>) is based on computing recall, precision, and F 1 measure as defined above.</p><p>Note that there are multiple ways to specify a set of gold-standard edits that denote the same corrections. For example, in the above learner-written sentence S, alternative but equivalent sets of goldstandard edits are {a → , system → systems, has → have}, {a → , system has → systems have}, etc. Given the same learner-written sentence S and the same system output sentence H shown above, one would expect a scorer to give the same R, P, F 1 scores regardless of which of the equivalent sets of gold-standard edits is specified by an annotator.</p><p>However, this is not the case with the HOO scorer. This is because the HOO scorer uses GNU wdiff 2 to extract the differences between the learner-written sentence S and the system output sentence H to form a set of system edits. Since in general there are multiple ways to specify a set of gold-standard edits that denote the same corrections, the set of system edits computed by the HOO scorer may not match the set of gold-standard edits specified, leading to erroneous scores. In the above example, the set of system edits computed by the HOO scorer for S and H is {a → }. Given that the set of gold-standard edits g is {a doubt → doubt, system → systems, has → have}, the scores computed by the HOO scorer are R = P = F 1 = 0, which are erroneous.</p><p>The MaxMatch (M 2 ) scorer 3 <ref type="bibr" target="#b4">(Dahlmeier and Ng, 2012b)</ref> was designed to overcome this limitation of the HOO scorer. The key idea is that the set of system edits automatically computed and used in scoring should be the set that maximally matches the set of gold-standard edits specified by the annotator. The M 2 scorer uses an efficient algorithm to search for such a set of system edits using an edit lattice. In the above example, given S, H, and g, the M 2 scorer is able to come up with the best matching set of system edits e = {a doubt → doubt}, thus giving the correct scores R = 1/3, P = 1/1, F 1 = 1/2. We use the M 2 scorer in the CoNLL-2013 shared task.</p><p>The original M 2 scorer implemented in <ref type="bibr" target="#b4">(Dahlmeier and Ng, 2012b)</ref> assumes that there is one set of gold-standard edits g i for each sentence i. However, it is often the case that multiple alternative corrections are acceptable for a sentence. As we allow participating teams to submit alternative sets of gold-standard edits for a sentence, we also extend the M 2 scorer to deal with multiple alternative sets of gold-standard edits.</p><p>Based on Equations 1 and 2, Equation 3 can be re-expressed as:</p><formula xml:id="formula_3">F 1 = 2 × n i=1 |g i ∩ e i | n i=1 (|g i | + |e i |)<label>(5)</label></formula><p>To deal with multiple alternative sets of goldstandard edits g i for a sentence i, the extended M 2 scorer chooses the g i that maximizes the cumulative F 1 score for sentences 1, . . . , i. Ties are broken based on the following criteria: first choose the g i that maximizes the numerator n i=1 |g i ∩ e i |, then choose the g i that minimizes the denominator n i=1 (|g i | + |e i |), finally choose the g i that appears first in the list of alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approaches</head><p>54 teams registered to participate in the shared task, out of which 17 teams submitted the output of their grammatical error correction systems by the deadline. These teams are listed in Table <ref type="table" target="#tab_6">5</ref>. Each team is assigned a 3 to 4-letter team ID. In the remainder of this paper, we will use the assigned team ID to refer to a participating team. Every team submitted a system description paper (the only exception is the SJT2 team).</p><p>Many different approaches are adopted by participating teams in the CoNLL-2013 shared task, and Table <ref type="table" target="#tab_8">6</ref> summarizes these approaches. A commonly used approach in the shared task and in grammatical error correction research in general is to build a classifier for each error type. For example, the classifier for noun number returns the classes {singular, plural}, the classifier for article returns the classes {a/an, the, }, etc. The classifier for an error type may be learned from training examples encoding the surrounding context of an error occurrence, or may be specified by deterministic hand-crafted rules, or may be built using a hybrid approach combining both machine learning and hand-crafted rules. These approaches are denoted by M, R, and H respectively in Table <ref type="table" target="#tab_8">6</ref>.</p><p>The machine translation approach (denoted by T in Table <ref type="table" target="#tab_8">6</ref>) to grammatical error correction treats the task as "translation" from bad English to good English. Both phrase-based translation and syntax-based translation approaches are used by teams in the CoNLL-2013 shared task. Another related approach is the language modeling approach (denoted by L in Table <ref type="table" target="#tab_8">6</ref>), in which the probability of a learner sentence is compared with the probability of a candidate corrected sentence, based on a language model built from a background corpus. The candidate correction is chosen if it results in a corrected sentence with a higher probability. In general, these approaches are not mutually exclusive. For example, the work of <ref type="bibr" target="#b3">(Dahlmeier and Ng, 2012a;</ref><ref type="bibr" target="#b18">Yoshimoto et al., 2013)</ref> includes elements of machine learningbased classification, machine translation, and language modeling approaches.</p><p>When different approaches are used to tackle different error types by a system, we break down the error types into different rows in Table <ref type="table" target="#tab_8">6</ref>, and specify the approach used for each group of error types. For instance, the HIT team uses a machine learning approach to deal with article/determiner, noun number, and preposition errors, and a rulebased approach to deal with subject-verb agreement and verb form errors. As such, the entry for HIT is sub-divided into two rows, to make it clear which particular error type is handled by which approach.</p><p>Table <ref type="table" target="#tab_8">6</ref> also shows the linguistic features used by the participating teams, which include lexical features (i.e., words, collocations, n-grams), partsof-speech (POS), constituency parses, dependency parses, and semantic features (including semantic role labels).</p><p>While all teams in the shared task use the NU-CLE corpus, they are also allowed to use additional external resources (both corpora and tools) so long as they are publicly available and not proprietary. The external resources used by the teams are also listed in Table <ref type="table" target="#tab_8">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>All submitted system output was evaluated using the M 2 scorer, based on the error annotations provided by our annotator. The recall (R), precision (P ), and F 1 measure of all teams are shown in Table 7. The performance of the teams varies greatly,   from barely half a per cent to 31.20% for the top team.</p><p>The nature of grammatical error correction is such that multiple, different corrections are often acceptable. In order to allow the participating teams to raise their disagreement with the original gold-standard annotations provided by the annotator, and not understate the performance of the teams, we allow the teams to submit their proposed alternative answers. This was also the practice adopted in HOO 2011 and HOO 2012. Specifically, after the teams submitted their system output and the error annotations on the test essays were released, we allowed the teams to propose alternative answers (gold-standard edits), to be submitted within four days after the initial error annotations were released. The same annotator who provided the error annotations on the test essays also judged the alternative answers proposed by the teams, to ensure consistency. In all, five teams (NTHU, STEL, TOR, UIUC, UMC) submitted alternative answers. The same submitted system output was then evaluated using the extended M 2 scorer, with the original annotations augmented with the alternative answers. Table <ref type="table" target="#tab_10">8</ref> shows the recall (R), precision (P ), and F 1 measure of all teams under this new evaluation setting.</p><p>The F 1 measure of every team improves when  The Approach column shows the approach adopted by each team, sometimes broken down according to the error type: H denotes a hybrid classifier approach, L denotes a language modeling-based approach, M denotes a machine learning-based classifier approach, R denotes a rule-based classifier (non-machine learning) approach, and T denotes a machine translation approach evaluated with alternative answers. Not surprisingly, the teams which submitted alternative answers tend to show the greatest improvements in their F 1 measure. Overall, the UIUC team <ref type="bibr" target="#b15">(Rozovskaya et al., 2013)</ref> achieves the best F 1 measure, with a clear lead over the other teams in the shared task, under both evaluation settings (without and with alternative answers).</p><p>For future research which uses the test data of the CoNLL-2013 shared task, we recommend that evaluation be carried out in the setting that does not use alternative answers, to ensure a fairer evaluation. This is because the scores of the teams which submitted alternative answers tend to be higher in a biased way when evaluated with alternative answers.  We are also interested in the analysis of scores of each of the five error types. To compute the recall of an error type, we need to know the error type of each gold-standard edit, which is provided by the annotator. To compute the precision of each error type, we need to know the error type of each system edit, which however is not available since the submitted system output only contains the corrected sentences with no indication of the error type of the system edits.</p><p>In order to determine the error type of system edits, we first perform POS tagging on the submitted system output using the Stanford parser <ref type="bibr" target="#b11">(Klein and Manning, 2003)</ref>. We also make use of the POS tags assigned in the preprocessed form of the test essays. We then assign an error type to a system edit based on the automatically determined POS tags, as follows:</p><p>• ArtOrDet: The system edit involves a change (insertion, deletion, or substitution) of words tagged as article/determiner, i.e., DT or PDT.</p><p>• Prep: The system edit involves a change of words tagged as preposition, i.e., IN or TO.</p><p>• Nn: The system edit involves a change of words such that a word in the source string is a singular noun (tagged as NN or NNP) and a word in the replacement string is a plural noun (tagged as NNS or NNPS), or vice versa. Since a word tagged as JJ (adjective) can serve as a noun, a system edit that involves a change of POS tags from JJ to one of {NN, NNP, NNS, NNPS} or vice versa also qualifies.</p><p>• Vform/SVA: The system edit involves a change of words tagged as one of the verb POS tags, i.e., VB, VBD, VBG, VBN, VBP, and VBZ.</p><p>The verb form and subject-verb agreement error types are grouped together into one category, since it is difficult to automatically distinguish the two in a reliable way.</p><p>The scores when distinguished by error type are shown in Tables <ref type="table" target="#tab_12">9 and 10</ref>. Based on the F 1 measure of each error type, the noun number error type gives the highest scores, and preposition errors remain the most challenging error type to correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The CoNLL-2013 shared task saw the participation of 17 teams worldwide to evaluate their grammatical error correction systems on a common test set, using a common evaluation metric and scorer. The five error types included in the shared task account for at least one-third to close to one-half of all errors in English learners' essays. The best system in the shared task achieves an F 1 score of 42%, when it is scored with multiple acceptable answers. There is still much room for improvement, both in the accuracy of grammatical error correction systems, and in the coverage of systems to deal with a more comprehensive set of error   If a team indicates that its system does not handle a particular error type, its entry for that error type is marked as "(not done)".</p><p>types. The evaluation data sets and scorer used in our shared task serve as a benchmark for future research on grammatical error correction 4 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example error annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of training and test data.In our shared task, in order to facilitate the detection and correction of article/determiner errors and preposition errors, we performed automatic mapping of error types in the original NUCLE corpus. The mapping relies on POS tags, constituent parse trees, and error annotations at the word token</figDesc><table /><note>level. Specifically, we map the error types Wcip and Rloc to Prep, Wci, ArtOrDet, and Rloc−. Prepositions in the error type Wcip or Rloc are mapped to a new error type Prep, and redundant articles or determiners in the error type Rloc are mapped to ArtOrDet. The remaining unaffected Wcip errors are assigned the new error type Wci and the remaining unaffected Rloc errors are assigned the new error type Rloc−. The code that performs automatic error type mapping was also provided to the participating teams.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(the training data provided in HOO 2012<ref type="bibr" target="#b7">(Dale et al., 2012)</ref>) as additional training data. Test Data 25 NUS students, who are non-native speakers of English, were recruited to write new essays to be used as blind test data in the shared task. Each student wrote two essays in response to the two prompts shown in Table4, one essay per prompt. Essays written using the first prompt are present in the NUCLE training data, while the second prompt is a new prompt not used previously. As a result, 50 test essays were collected. The statistics of the test essays are shown in Table2.</figDesc><table><row><cell cols="2">Error tag Training</cell><cell>%</cell><cell>Test</cell><cell>%</cell></row><row><cell></cell><cell>data</cell><cell></cell><cell>data</cell><cell></cell></row><row><cell></cell><cell>(NUCLE)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ArtOrDet</cell><cell>6,658</cell><cell>14.8</cell><cell>690</cell><cell>19.9</cell></row><row><cell>Prep</cell><cell>2,404</cell><cell>5.3</cell><cell>312</cell><cell>9.0</cell></row><row><cell>Nn</cell><cell>3,779</cell><cell>8.4</cell><cell>396</cell><cell>11.4</cell></row><row><cell>Vform</cell><cell>1,453</cell><cell>3.2</cell><cell>122</cell><cell>3.5</cell></row><row><cell>SVA</cell><cell>1,527</cell><cell>3.4</cell><cell>124</cell><cell>3.6</cell></row><row><cell>5 types</cell><cell>15,821</cell><cell cols="2">35.1 1,644</cell><cell>47.4</cell></row><row><cell>all types</cell><cell cols="4">45,106 100.0 3,470 100.0</cell></row><row><cell cols="5">Table 3: Error type distribution of the training and</cell></row><row><cell>test data.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The two prompts used for the test essays.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The list of 17 participating teams.</figDesc><table><row><cell cols="2">Rank Team</cell><cell>R</cell><cell>P</cell><cell>F 1</cell></row><row><cell>1</cell><cell>UIUC</cell><cell cols="3">23.49 46.45 31.20</cell></row><row><cell>2</cell><cell cols="4">NTHU 26.35 23.80 25.01</cell></row><row><cell>3</cell><cell>HIT</cell><cell cols="3">16.56 35.65 22.61</cell></row><row><cell>4</cell><cell cols="4">NARA 18.62 27.39 22.17</cell></row><row><cell>5</cell><cell>UMC</cell><cell cols="3">17.53 28.49 21.70</cell></row><row><cell>6</cell><cell>STEL</cell><cell cols="3">13.33 27.00 17.85</cell></row><row><cell>7</cell><cell>SJT1</cell><cell cols="3">10.96 40.18 17.22</cell></row><row><cell>8</cell><cell cols="4">CAMB 10.10 39.15 16.06</cell></row><row><cell>9</cell><cell>IITB</cell><cell cols="2">4.99 28.18</cell><cell>8.48</cell></row><row><cell>10</cell><cell>STAN</cell><cell cols="2">4.69 25.50</cell><cell>7.92</cell></row><row><cell>11</cell><cell>TOR</cell><cell cols="2">4.81 17.67</cell><cell>7.56</cell></row><row><cell>12</cell><cell>KOR</cell><cell cols="2">3.71 43.88</cell><cell>6.85</cell></row><row><cell>13</cell><cell>TILB</cell><cell>7.24</cell><cell>6.25</cell><cell>6.71</cell></row><row><cell>14</cell><cell>SZEG</cell><cell>3.16</cell><cell>5.52</cell><cell>4.02</cell></row><row><cell>15</cell><cell>UAB</cell><cell cols="2">1.22 12.42</cell><cell>2.22</cell></row><row><cell>16</cell><cell>SAAR</cell><cell cols="2">1.10 27.69</cell><cell>2.11</cell></row><row><cell>17</cell><cell>SJT2</cell><cell cols="2">0.24 13.33</cell><cell>0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Scores (in %) without alternative an-</cell></row><row><cell>swers.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Profile of the participating teams. The Error column shows the error type, where each letter denotes the error type beginning with that initial letter.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Scores (in %) with alternative answers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>NARA 20.43 34.06 25.54 12.54 29.10 17.53 16.41 48.87 24.57 24.80 14.81 18.54 NTHU 21.01 35.80 26.48 12.86 12.01 12.42 45.96 40.90 43.28 26.83 12.22 16.</figDesc><table><row><cell>Team ArtOrDet Prep Nn Vform/SVA R P F 1 R P F 1 R P F 1 R P F 1</cell><cell>CAMB 15.07 38.66 21.69 3.54 40.74 6.51 7.58 55.56 13.33 8.54 31.82 13.46</cell><cell>HIT 24.20 42.82 30.93 2.89 28.12 5.25 17.17 29.69 21.76 11.38 26.42 15.91</cell><cell>IITB 1.30 21.43 2.46 (not done) 9.85 28.68 14.66 13.82 30.09 18.94</cell><cell>KOR 4.78 53.23 8.78 0.32 4.76 0.60 6.82 49.09 11.97 (not done)</cell><cell>79</cell><cell>SAAR 0.72 62.50 1.43 (not done) (not done) 5.28 23.21 8.61</cell><cell>SJT1 16.81 47.15 24.79 1.29 12.50 2.33 13.64 42.19 20.61 2.44 14.63 4.18</cell><cell>SJT2 0.00 0.00 0.00 0.00 0.00 0.00 1.01 13.33 1.88 0.00 0.00 0.00</cell><cell>STAN 3.91 20.45 6.57 0.32 20.00 0.63 6.06 29.63 10.06 10.16 32.05 15.43</cell><cell>STEL 12.61 27.71 17.33 9.32 25.66 13.68 18.18 46.75 26.18 12.60 17.61 14.69</cell><cell>SZEG 1.16 1.70 1.38 (not done) 11.11 13.62 12.24 (not done)</cell><cell>TILB 4.49 4.49 4.49 10.61 5.07 6.86 7.07 21.21 10.61 10.98 9.57 10.23</cell><cell>TOR 8.55 25.54 12.81 2.25 5.38 3.17 1.77 31.82 3.35 2.44 12.24 4.07</cell><cell>UAB 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 8.13 12.42 9.83</cell><cell>UIUC 25.65 47.84 33.40 4.18 26.53 7.22 38.38 52.23 44.25 17.89 38.94 24.51</cell><cell>UMC 21.01 30.27 24.81 1.93 35.29 3.66 23.23 27.96 25.38 18.29 28.85 22.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Scores (in %) without alternative answers, distinguished by error type. If a team indicates that its system does not handle a particular error type, its entry for that error type is marked as "(not done)".NARA 25.79 43.34 32.34 17.60 34.56 23.33 19.15 57.04 28.68 34.62 19.10 24.62 NTHU 25.30 42.54 31.73 20.45 16.17 18.06 52.35 50.87 51.60 43.03 19.22 26.33.33 11.27 13.51 37.97 19.93 STEL 16.23 35.69 22.31 12.83 29.82 17.94 26.05 70.00 37.97 20.52 26.55 23.15</figDesc><table><row><cell>Team ArtOrDet Prep Nn Vform/SVA R P F 1 R P F 1 R P F 1 R P F 1</cell><cell>CAMB 19.62 49.81 28.15 5.04 50.00 9.15 9.50 69.09 16.70 16.52 54.41 25.34</cell><cell>HIT 27.41 47.44 34.74 4.58 37.50 8.16 19.95 35.37 25.51 17.90 38.32 24.40</cell><cell>IITB 1.79 28.57 3.37 (not done) 11.91 35.29 17.81 18.67 36.84 24.78</cell><cell>KOR 5.95 64.52 10.90 1.53 19.05 2.83 7.52 54.55 13.22 (not done)</cell><cell>57</cell><cell>SAAR 1.04 87.50 2.06 (not done) (not done) 8.60 33.93 13.72</cell><cell>SJT1 19.65 54.07 28.82 1.92 15.62 3.42 16.46 52.34 25.05 4.05 21.95 6.84</cell><cell>SJT2 0.00 0.00 0.00 0.00 0.00 0.00 1.26 16.67 2.35 0.00 0.00 0.00</cell><cell>STAN 4.91 24.81 8.20 0.38 20.00 0.75 6.78 SZEG 1.50 2.13 1.76 (not done) 12.87 15.95 14.25 (not done)</cell><cell>TILB 5.78 5.64 5.71 13.91 5.68 8.07 8.25 24.26 12.31 16.36 12.77 14.34</cell><cell>TOR 13.10 39.13 19.63 5.97 12.31 8.04 3.52 63.64 6.67 8.14 35.29 13.24</cell><cell>UAB 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12.61 17.39 14.62</cell><cell>UIUC 31.99 59.84 41.69 8.81 46.94 14.84 46.88 70.00 56.15 28.57 60.71 38.86</cell><cell>UMC 25.88 36.74 30.37 3.47 56.25 6.55 30.61 38.64 34.16 26.81 40.13 32.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Scores (in %) with alternative answers, distinguished by error type.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.comp.nus.edu.sg/∼nlp/corpora.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.gnu.org/s/wdiff/ 3 http://www.comp.nus.edu.sg/∼nlp/software.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correcting semantic collocation errors with L1-induced paraphrases</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grammatical error correction with alternating structure optimization</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="915" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A beamsearch decoder for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS Corpus of Learner English</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
				<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Helping Our Own: The HOO 2011 pilot shared task</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
				<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HOO 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</title>
				<meeting>the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Language Resources and Evaluation</title>
				<meeting>the Fifth Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using mostly native data to correct errors in learners&apos; writing: A meta-classifier approach</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting errors in English article usage by non-native speakers</title>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automated Grammatical Error Detection for Language Learners</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics 2003 Conference</title>
				<meeting>the Corpus Linguistics 2003 Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating confusion sets for context-sensitive error correction</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The University of Illinois system in the CoNLL-2013 shared task</title>
		<author>
			<persName><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
				<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="353" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">NAIST at 2013 CoNLL grammatical error correction shared task</title>
		<author>
			<persName><forename type="first">Ippei</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensuke</forename><surname>Mitsuzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
