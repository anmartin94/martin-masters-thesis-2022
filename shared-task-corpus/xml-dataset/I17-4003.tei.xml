<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anil</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT (BHU)</orgName>
								<address>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT (BHU)</orgName>
								<address>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mayank</forename><surname>Panchal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT (BHU)</orgName>
								<address>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anubhav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT (BHU)</orgName>
								<address>
									<settlement>Varanasi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unlike Entity Disambiguation in web search results, Opinion Disambiguation is a relatively unexplored topic. RevOpiD shared task at IJCNLP-2107 aimed to attract attention towards this research problem. In this paper, we summarize the first run of this task and introduce a new dataset that we have annotated for the purpose of evaluating Opinion Mining, Summarization and Disambiguation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the famous Asch Conformity experiment, individuals were first shown a line segment on a card. Next, they were shown another card with 3 line segments (with a significant difference in length) and were asked to decide which of the 3 matched the length of the previously shown line. The same task was then to be performed in the presence of a group of 8 people (where the remaining 7 were confederates/actors, all of whom were instructed beforehand to give the wrong answer). The error rate soared from a bare 1 percent in the case the subject was alone, to 36.8 percent when the people around expressed the wrong perception <ref type="bibr" target="#b0">(Asch and Guetzkow, 1951)</ref>. This goes to show how heavily can others' opinions influence our own. With the ever growing influence of sources of opinion today, the need to regulate them is also at an all time high. Documents in the form of social media posts, web blogs, biased or fake news articles, tweets and product reviews can be listed as the primary sources of opinionated information one encounters on a daily basis. Vidulich et. al <ref type="bibr" target="#b14">(Vidulich and Kaiman, 1961</ref>) also reported similar results in experiments with the sources of conformity. They found that dogmatists are influenced by the status of the source of information.</p><p>The domains of Search Result Ranking and Document Summarization then possess a great potential (and bear a great responsibility) in influencing popular opinion about a target entity. For example, if on searching for 'iPhone reviews', we see results (ranked by, say, PageRank) that coincidentally happen to be against the product, then one might form a perception of the general opinion around the world regarding the smartphone. This perception may or may not be in line with the original composition of the opinion worldwide. What, then, should be the basis of document ranking in Information Retrieval methods?</p><p>To delve deeper into addressing this problem, we chose to limit ourselves to a single type of documents: Product Reviews. The reason behind this choice is manifold: Product Reviews are concise, targeted, opinionated (though sometimes descriptive and sometimes objective), diverse (in terms of the category of product), readily available as datasets, and easily comprehended (which makes annotating such data relatively easier). Besides, finding a diverse subset of product review documents (in terms of opinions) provides a good application, which might be of commercial interest to e-commerce websites.</p><p>For a product with several reviews, it can get cumbersome for a user to browse through them all. According to an internet source, 90 percent of consumers form an opinion by reading at most 10 reviews, while 68 percent form their opinion after reading just 1-6 reviews. 1 It leads to a natural curiosity into the manner in which reviews are ordered. Order by date (most recent reviews first), order by upvotes (reviews voted 'helpful' the most are ranked first), group by words (show only those reviews which contain specific words, eg. 'battery'), group by stance (segregate reviews into positive and negative), group by stars (filter reviews which gave a certain number of stars to the product) are some of the techniques used in sorting and ranking of online customer reviews.</p><p>However, only the last two of these take into account the difference of opinions in reviews. And not even these take into account the overall opinion about the product. What we propose is a ranked list that aims to represent a gist of opinions of the whole set of reviews (for any given product). To this end, we will present a novel dataset that can be used as a benchmark for evaluating such a ranked list in Section 3. We will also summarize the details of RevOpiD 2017, the first run of Review Opinion Diversification shared task in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many researchers have undertaken the study of opinion diversity, but most exhibit limited scope owing to the absence of a standard dataset among the community. The Blog Track Opinion Finding Task (TREC 6-8) has a favourable corpus, and was initially meant to judge systems on their Reranking approach on web search results, based on opinion diversity.</p><p>The Aspect Based Sentiment Analysis task at SemEval 2014-2016 <ref type="bibr" target="#b11">(Pontiki et al., 2014)</ref> was an initiative towards the objective evaluation of sentiment expressed in product reviews. In a wide enough corpora of 39 datasets, ranging across 7 domains and 8 languages, the task was to identify target entity and pick the attribute commented upon (from a list of attributes already provided to annotators).</p><p>Our aim differs slightly in that we reward systems which ultimately produce an opinion diversified (and representative) ranking of a subset of the review corpora. The motivation for this statement bases itself on two targeted benefits:</p><p>1. Due to absence of an inventory of aspects or opinions for the participants to 'identify', the systems must mine new 'aspects' that vary enormously for different products. Thus, vague aspects in the form of topics modelled will be rewarded equivalently to another approach that, say, manages to match exact lexicons to the subtopics retrieved.</p><p>2. We avoid evaluating the opinions on the opinions mined since the number of opinions ex-pressed is a subjective choice made by annotators in the labelling process. For instance, if one annotator suggests having 'affordable' and 'worth the money' as two different opinions whereas a system assumes both to express the same opinion, it may still perform well on diversifying the ranked list. Hence our evaluation on the final ranked list prevents systems from over-fitting on the opinions mined.</p><p>Despite the limitations in previous opinion mining evaluations, a recurring and fundamental feature in most of these methodologies is the identification of nuggets (in summarization jargon) or subtopics (in indexing terminology) or attributes (in product reviews); and their subsequent application in having a fine-grained view at the relevance contained in a document. In our pursuit of a tested and suitable data collection, we observed the small-scale attempts at similar data annotation <ref type="bibr" target="#b8">(Marcheggiani et al., 2014)</ref>  <ref type="bibr" target="#b13">(Täckström and McDonald, 2011</ref>) (a few tens of reviews at most, for evaluation of their own opinion mining and discourse analysis systems respectively). The most well known among these is the 'Mining and Summarizing Customer Reviews' paper by Bing et. al. <ref type="bibr" target="#b6">(Hu and Liu, 2004)</ref>. The experimentation in this publication is based on a compilation of the first 100 reviews of 5 products from Amazon.com and cnet.com. Initially, there were 9 and then 3 more products were added in subsequent years <ref type="bibr" target="#b4">(Ding et al., 2008)</ref>  <ref type="bibr" target="#b7">(Liu et al., 2015)</ref>. These reviews were sentence-wise annotated with the following:</p><p>1. feature on which opinion is expressed, if any 2. orientation of opinion (+ or -) 3. opinion strength (on a scale of 1 to 3) An example of annotation by the human taggers (the authors of the paper themselves) for a Digital Camera is: "affordability[+3]while , there are flaws with the machine , the xtra gets five stars because of its affordability ."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>The dataset labelled by Bing et. al. is created through a fairly suitable and scalable annotation procedure, despite the inherent flaws associated with subjectivity of human annotation. A This sterling ring is not too wide, it has a nice touch with the CZ all the way around, making it easier to wear for my wife, because she doesn't worry about it spinning and cutting into the fingers to the side. The CZ stones are recessed a bit making it pretty smooth.</p><p>1.0/5.0 On the other hand, for evaluation of opinion diversity in reviews (or any document), labelling of each statement is less of a benefit and very time consuming.</p><p>2. The referred dataset contained 96 unique features for a total of 95 reviews (product: Digital Camera 2). Such an exhaustive labelling is again detrimental to the annotation efforts, and is of limited benefits. A reasoning behind this can be observed from the way commercial websites continue to sort their reviews.</p><p>TripAdvisor, for instance, uses a common set of just 6 attributes: 'Location', 'Service'...</p><p>Note that identification of opinions on a perproduct basis is a key point of the procedure described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Labelling Procedure</head><p>Having established our primary objectives behind the need for a opinion-labelled dataset, we now propose our opinion labelling procedure. Labelling process can be broken down into 2 steps. Note that this procedure is to be iterated for each product individually.</p><p>1. Make an opinion list, i.e., a set of popular opinions recurrently occurring in the reviews.</p><p>2. Make an opinion matrix. The opiniondocument matrix (or simply the opinion matrix) is a tabular output of the labelling process, with each row corresponding to a review and each column corresponding to an opinion from the opinion list of the product.</p><p>Due to the space constraints, we avoid full textual description and complete specification of guidelines for the dataset. We proceed to show a sample Opinion List (Table <ref type="table" target="#tab_3">2</ref>) and a sample Opinion-Document matrix (Table <ref type="table" target="#tab_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proportion</head><p>Our opinion annotated dataset is derived from a subset of Amazon SNAP online reviews dataset <ref type="bibr" target="#b9">(McAuley and Leskovec, 2013)</ref>. The original SNAP dataset contains more than 34 million reviews spanning over 2 million products. 85 products were chosen from among these, spanning 12 categories, and were labelled with opinions. The number of reviews per product is shown in Table <ref type="table" target="#tab_6">4</ref> and the number of opinions taken (as considered by annotators) for each product are shown in Table <ref type="table" target="#tab_7">5</ref>. The products in both these tables have been grouped by their category. Eg. Office category has 6 products which are included in our dataset.   </p><formula xml:id="formula_0">X X X Review2 X Review3 X X Review4 X X Review5 X X X Review6 X X Review7 X X Review8 X X Review9 X Review10 X X X Overall 5 4 5 4 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter Annotator Confidence</head><p>Our proposed evaluation framework relies heavily on the labelling procedure described above, which in turn has the major factor of human subjectivity. What one annotator deems as an opinion (as expressed in a certain number of reviews for a product) might not seem significant enough for another annotator. Thus inter annotator agreement studies are crucial for judging our dataset's reliability.</p><p>We conducted an experiment asking 5 of our an-notators to annotate a single product's review files (only the first 25 reviews). Since opinion lists are not marked 0s or 1s but contain natural language (opinions in the form of text), it is difficult to measure their agreement objectively. Instead, we checked the inter-annotator confidence on whether specific opinions occur in a given review or not.</p><p>For every pair of annotators A and B, whose inter annotator agreement is to be calculated, we manually select certain opinions from O1 (opinion list of A) which have more or less equivalent opinions in O2 (opinion list of B). Let this set be called O3. Thereafter, presence or absence of opinion o i in a review r i in opinion matrix M1 (matrix of A) is compared with that in M2 (matrix of B).</p><p>Cohen's Kappa inter-rater agreement <ref type="bibr" target="#b5">(Fleiss and Cohen, 1973)</ref> for different pairs of annotators is summarized in Table <ref type="table" target="#tab_9">6</ref>. For example annotators A1 and A2 show a Cohen's Kappa coefficient of 0.77 for the commonly occurring opinion "Realistic look". Some blank cells exist (for example, in A1-A3 and A2-A3 under 'Moderate') since not all opinions occur in the opinion lists of all annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RevOpiD-2017</head><p>RevOpiD-2017 is a part of the 8th International Joint Conference on Natural Language Processing (November 27 to December 1, 2017) at Taipei, Taiwan. The shared task consists of three independent subtasks. Participating systems are required to produce a top-k summarized ranking of reviews (one ranked list for each product for a given subtask) from amongst the given set of reviews. The redundancy of opinions expressed in the review corpus must be minimised, along with maximisation of a certain property. This property can be one of the following (one property corresponds to one subtask):</p><p>1. usefulness rating of the review (Subtask A) 2. representativeness of the overall corpus of reviews (Subtask B)</p><p>3. exhaustiveness of opinions expressed (Subtask C) Some Definitions:</p><p>1. Review: Review text and any other relevant metadata as may be considered necessary to be used by the participating system, from the given data.   3. Feature: A ratable aspect of the product.</p><p>4. Opinion: An ordered pair of an aspect and sentiment (for that aspect) in any review.</p><p>For the purpose of RevOpiD 2017, our derived dataset was split into three parts:</p><p>1. Training Data: was the same as the SNAP dataset, except it being a subset of the latter. Statistics of the training data has been shown in Table <ref type="table" target="#tab_10">7</ref>.</p><p>2. Development Data: contained annotated opinion matrices along with the text review files for 30 products. These matrices were used by an evaluation script to measure the performance of participating systems in Subtasks B and C (for representativeness and exhuastiveness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Test Data:</head><p>The test data contained the review text files alone (also devoid of usefulness scores) of 50 products. The opinion matrices were withheld by us to evaluate final scores based on this test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Subtask A (Usefulness Ranking)</head><p>Usefulness rating is a user-collected field in the provided training dataset. Given a corpus of reviews for a particular product, the goal is to rank the top-k of them, according to predicted usefulness rating, while simultaneously penalizing redundancy among the ranked list of reviews. An essential subsection of this task obviously includes predicting the usefulness rating for a particular review.</p><p>Kappa Inter Rater Agreement (on opinion matrix) scores for 3 of our annotators   Given a corpus of reviews for a particular product, the goal is to rank the top-k of them, so as to maximize representativeness of the ranked list. The ranking should summarize the perspectives expressed in the reviews given as input, incorporating a trade-off between diversity and novelty. An ideal representation would be one that covers the popular perspectives expressed in the cor-pus, in proportion to their expression in the corpus (for that product), e.g. if 90 reviews claim that the iPhone cost is low, and 10 reviews claim that it is high, the former perspective should have 90 percent visibility in the final ranking and the latter should have 10 percent (or may even be ignored owing to low popularity) in the final ranking. The ranking should be such that for every i in 1 &lt;= i &lt;= k, the top i reviews best represent the overall set of reviews for the product. That is, the #1 review should be the best single review to represent the overall opinion in the corpus; The combination of #1 and #2 reviews should be the best pair of reviews to represent the corpus, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Subtask C (Exhaustive Coverage</head><p>Ranking) Given a corpus of reviews for a particular product, the goal is to rank the top-k of them, so as to include the majority of popular perspectives in the corpus regarding the product, while simultaneously penalizing redundancy among the ranked list of reviews. This is similar to Subtask B, except that:</p><p>In Subtask B, the final ranking is judged on the basis of how well the ranked list represents the most popular opinions in the review corpus, in proportion. In Subtask C, the final ranking is judged on the basis of the exhaustive coverage of the opinions in the final ranking. That means, most of the significant (not necessarily all very popular) perspectives should be covered regardless of their proportions of popularity in the review corpus, e.g. if 90 reviews claim that the iPhone cost is low, and 10 reviews claim that it is high, both perspectives should be more or less equally reflected in the final ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>This being the first run of RevOpiD, we experimented with several measures of evaluation <ref type="bibr">(Singh et al.)</ref> and 8 of them were shortlisted to study their variations with the system submissions:</p><p>1. mth (More than half's): The fraction of reviews included (in submitted ranked list) with more than half votes in favour. In other words, if upvotes on a review be counted as the number of users who found it helpful, and downvotes be counted as the number of users who didn't find it helpful; then the mth count will be incremented by one if upvotes &gt; downvotes.  <ref type="bibr" target="#b2">(Dang and Croft, 2012)</ref>. A ranking S is said to be proportional to the corpus D, or a proportional representation of D, with respect to opinions/aspects T, if and only if the number of documents in S that is relevant to each of the aspects t i ∈ T is proportional to its overall popularity p i ∈ D.</p><p>5. α-DCG: A measure that rewards novel information (to be covered incrementally in each review) <ref type="bibr" target="#b1">(Clarke et al., 2008)</ref>.</p><p>6. Weighted Relevance: Discounted Cumulative Gain with the relevance of a review given by sum of weights of the opinions covered in the review (weight of an opinion = Number of reviews in which it appears in the whole opinion matrix / corpus size).</p><p>7. UnWeighted Relevance: A discounted sum of number of opinions present in the ranked list.</p><p>8. Recall: The fraction of opinions/columns covered by the ranking. An opinion is said to be covered if atleast a single 1 appears in that column in the ranked list submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Systems</head><p>There were 3 participating systems at RevOpiD-2017, namely JUNLP, CYUT and FAAD. Also included in our analysis is the official baseline (Subtasks B and C) 2 . The last row shows the scores obtained for a random submission script averaged over 5 runs. While CYUT and FAAD have attempted Subtask A alone, JUNLP has submitted runs for each of Subtasks A, B and C. 1. JUNLP <ref type="bibr">(Dey et al.)</ref>: Instead of posing this as a regression problem, they have modeled it as a classification task where the aim is to identify whether a review is useful or not. They've employed a bi-directional LSTM to represent each review which is used with a softmax layer to predict the usefulness score. First they choose the review with highest usefulness score, then they find its cosine similarity score with rest of the reviews. This is done in order to ensure diversity in the selection of top-k reviews.   <ref type="bibr">(Wu et al.)</ref>: This team (with prior work in helpfulness rating prediction of Chinese online reviews) implemented two models using linear regression with two different loss functions: least squares (CYUT 1) and cross entropy (CYUT 2). (i) Identify the features of the product that customers have expressed opinions on (called opinion features) and rank the features according to their frequencies that they appear in the reviews.</p><p>(ii) For each feature, identify how many customer reviews have positive, negative or neutral opinions. The specific reviews that express these opinions are attached to the feature.</p><p>(iii) Generate an opinion matrix based on these predicted occurrences of opinions and greedily select the best representative and exhaustive rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The results of RevOpiD-2017 have been summarized in Table <ref type="table" target="#tab_13">8</ref> for the chosen metrics already described above. Based on the system performances, the feature selection mechanism in CYUT's submission using Cross Entropy loss function proves the leader in Subtask A. JUNLP's submission (representative ranking) outperforms others marginally in Subtask B and Subtask C. Not a lot of improvement is reported over the baseline, therefore there exists a lot of scope for improvement in Subtasks B and C.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3. FAAD(Mishra et al.): Two supervised classifiers (Naıve Bayes and Logistic Regression) are fitted on top of several extracted features such as the number of nouns, number of verbs, and the number of sentiment words etc. from the provided development and training datasets. Three runs (FAAD 1,2,3) vary only in the weightage given to the two classifiers. 4. Baseline: A feature based opinion extraction based on the work of Bing et al. This task is done in three steps:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This ring is pretty, it can go good with another ring. It narrow and the stone size is small by it self. It would be a good thumb ring. Again nice ring that does not have alot of bling.This ring is amazing for the price. It doesn't turn my finger white, and the sizing is great. It's just a little bling that isn't too flashy. I wear it as a thumb ring. I think it's really pretty, and very sparkly.</figDesc><table><row><cell cols="2">Sterling Silver Cubic Zirconia Eternity Ring</cell></row><row><cell></cell><cell>Product Reviews</cell><cell>Rating</cell></row><row><cell>1. Alex</cell><cell>Date : 24/07/2016</cell></row><row><cell></cell><cell></cell><cell>3.0/5.0</cell></row><row><cell>2. Bran</cell><cell>Date : 20/07/2016</cell></row><row><cell cols="2">The ring was a gift and my daughter loved it!!! It is very sparkly and fit just right! I would</cell><cell>4.0/5.0</cell></row><row><cell>highly recommend this product.</cell><cell></cell></row><row><cell>3. Chau</cell><cell>Date : 14/07/2016</cell></row><row><cell></cell><cell></cell><cell>3.0/5.0</cell></row><row><cell>4. Dany</cell><cell>Date : 29/06/2016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A Sample Ranking of Product Reviews few drawbacks are yet to be addressed before we present our Opinion Labelling procedure:1. Bing et. al. aimed to mine features and opinions from review texts, and hence it is justifiable to practice sentence-wise labelling.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: Opinion List for "Sterling Silver Cubic</cell></row><row><cell cols="2">Zirconia Eternity Ring"</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Opinion Matrix</cell><cell></cell><cell></cell></row><row><cell>Realistic</cell><cell>Good</cell><cell>Many</cell><cell>Good</cell><cell>Sparkly</cell></row><row><cell>Look</cell><cell>deal</cell><cell>sizes</cell><cell>for</cell><cell>ring</cell></row><row><cell></cell><cell></cell><cell></cell><cell>gifts</cell><cell></cell></row><row><cell>Review1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Opinion Matrix for "Sterling Silver Cubic Zirconia Eternity Ring"</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Reviews per Product</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Opinions per Product</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baby</cell><cell>19</cell><cell>27</cell><cell>23</cell><cell>27</cell><cell>17</cell><cell>12</cell><cell>20</cell><cell>20</cell><cell>16</cell><cell>23</cell></row><row><cell>Automotive</cell><cell>22</cell><cell>23</cell><cell>23</cell><cell></cell><cell>23</cell><cell>13</cell><cell>17</cell><cell>18</cell><cell>23</cell><cell></cell></row><row><cell>Health</cell><cell>22</cell><cell>26</cell><cell>23</cell><cell>20</cell><cell>22</cell><cell>39</cell><cell>11</cell><cell>21</cell><cell>18</cell><cell></cell></row><row><cell>Grocery</cell><cell>21</cell><cell>26</cell><cell>14</cell><cell>21</cell><cell>27</cell><cell>11</cell><cell>17</cell><cell>22</cell><cell></cell><cell></cell></row><row><cell>PetSupplies</cell><cell>17</cell><cell>27</cell><cell>12</cell><cell>16</cell><cell>22</cell><cell>15</cell><cell>21</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Beauty</cell><cell>22</cell><cell>26</cell><cell>13</cell><cell>16</cell><cell>23</cell><cell>18</cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PatioLawn</cell><cell>22</cell><cell>26</cell><cell>17</cell><cell>30</cell><cell>13</cell><cell>19</cell><cell>26</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Office</cell><cell>19</cell><cell>27</cell><cell>18</cell><cell>15</cell><cell>11</cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ToolsHome</cell><cell>21</cell><cell>18</cell><cell>25</cell><cell>15</cell><cell>29</cell><cell>21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DigitalMusic</cell><cell>25</cell><cell>31</cell><cell>18</cell><cell>22</cell><cell>15</cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VideoGames</cell><cell>20</cell><cell>24</cell><cell>11</cell><cell>27</cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ToysGames</cell><cell>19</cell><cell>24</cell><cell>32</cell><cell>14</cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Opinions per Product 2. Corpus: All the reviews for a given product.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">: Inter Rater Agreement (on opinion matrix) Kappa scores for 3 of our annotators.</cell></row><row><cell cols="2">Product category: Automotive. Number of reviews: 25</cell><cell></cell><cell></cell></row><row><cell></cell><cell>.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Data Statistics</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Products</cell><cell>Reviews</cell><cell>Avg Reviews</cell></row><row><cell></cell><cell></cell><cell></cell><cell>per Products</cell></row><row><cell>Automotive</cell><cell>569</cell><cell>172106</cell><cell>302</cell></row><row><cell>Baby</cell><cell>1000</cell><cell>352231</cell><cell>352</cell></row><row><cell>Beauty</cell><cell>1000</cell><cell>316536</cell><cell>316</cell></row><row><cell>Digital music</cell><cell>468</cell><cell>145075</cell><cell>309</cell></row><row><cell>Grocery</cell><cell>800</cell><cell>293629</cell><cell>367</cell></row><row><cell>Health</cell><cell>1000</cell><cell>357669</cell><cell>357</cell></row><row><cell>Office</cell><cell>1000</cell><cell>327556</cell><cell>327</cell></row><row><cell>Patio lawn</cell><cell>859</cell><cell>263489</cell><cell>306</cell></row><row><cell>Pet supplies</cell><cell>1000</cell><cell>398658</cell><cell>398</cell></row><row><cell>Tools home</cell><cell>1000</cell><cell>320162</cell><cell>320</cell></row><row><cell>Toys games</cell><cell>1000</cell><cell>314634</cell><cell>314</cell></row><row><cell>Video games</cell><cell>1000</cell><cell>358235</cell><cell>358</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Data Statistics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>System and Baseline Scores 2. CYUT</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.brightlocal.com/learn/local-consumerreview-survey/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/shreyansh26/ RevOpiD/tree/master</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was assisted upon by several enthusiastic students as well as lab annotators: Shreyansh Singh (for developing the baseline), Shashwat Trivedi, Shivam Arora, Avijeet Diwaker, Divyanshu Gupta, Avi Chawla, Ayush Sharma, Tara Hemaliya, Vandana Singh, Neelam and Rajesh Kumar Mundotiya.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Effects of group pressure upon the modification and distortion of judgments. Groups, leadership, and men</title>
		<author>
			<persName><forename type="first">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asch</surname></persName>
		</author>
		<author>
			<persName><surname>Guetzkow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="222" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SI-GIR conference on Research and development in information retrieval</title>
				<meeting>the 31st annual international ACM SI-GIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diversity by proportionality: an election-based approach to search result diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Junlp: Ijcnlp-2017 revopid-a rank prediction model for review opinion diversification</title>
		<author>
			<persName><forename type="first">Monalisa</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP-2017 Shared Tasks</title>
				<meeting>the IJCNLP-2017 Shared Tasks</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 international conference on web search and data mining</title>
				<meeting>the 2008 international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. Educational and psychological measurement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Fleiss</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated rule selection for aspect extraction in opinion mining</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IJCAI</title>
				<meeting>eeding of the IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1291" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical multilabel conditional random fields for aspect-oriented opinion mining</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the European Conference on Information Retrieval</title>
				<meeting>eeding of the European Conference on Information Retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="273" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
				<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ijcnlp-2017 revopid shared task: A bidirectional-lstm approach for review opinion diversification</title>
		<author>
			<persName><forename type="first">Pruthwik</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prathyusha</forename><surname>Danda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silpa</forename><surname>Kanneganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Lanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP-2017 Shared Tasks</title>
				<meeting>the IJCNLP-2017 Shared Tasks</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Se-mEval</title>
				<meeting>Se-mEval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating opinion summarization in ranking</title>
		<author>
			<persName><forename type="first">Anil Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anubhav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Kumar Mundotiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 13th Asia Information Retrieval Societies Conference</title>
				<meeting>eeding of the 13th Asia Information Retrieval Societies Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discovering fine-grained sentiment with latent variable structured prediction models</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the European Conference on Information Retrieval</title>
				<meeting>eeding of the European Conference on Information Retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="368" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The effects of information source status and dogmatism upon conformity behavior</title>
		<author>
			<persName><forename type="first">N</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Vidulich</surname></persName>
		</author>
		<author>
			<persName><surname>Ivan P Kaiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">639</biblScope>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Pu</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">System report of cyut team at revopid-2017 shared task in ijcnlp-2017</title>
				<imprint/>
	</monogr>
	<note>Proceedings of the IJCNLP-2017 Shared Tasks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
