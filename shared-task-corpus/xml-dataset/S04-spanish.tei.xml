<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Senseval-3: The Spanish Lexical Sample Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Taulé¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>García¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Artigas¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Real</surname></persName>
							<email>fjreal@lsi.upc.es</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Ferrés</surname></persName>
							<email>dferres¤@lsi.upc.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Software Department</orgName>
								<orgName type="institution" key="instit1">TALP Research Center</orgName>
								<orgName type="institution" key="instit2">Universitat Politècnica de Catalunya £ lluism</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centre de Llenguatge i Computació Universitat de Barcelona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Senseval-3: The Spanish Lexical Sample Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe the Spanish Lexical Sample task. This task was initially devised for evaluating the role of unlabeled examples in supervised and semi-supervised learning systems for WSD and it was coordinated with five other lexical sample tasks (Basque, Catalan, English, Italian, and Rumanian) in order to share part of the target words.</p><p>Firstly, we describe the methodology followed to develop the linguistic resources necessary for the task: the MiniDir-2.1 lexicon and the MiniCors corpus. Secondly, we summarize the participant systems, the results obtained, and a comparative analysis. Participant systems include pure supervised, semi-supervised, and unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Spanish Lexicon: MiniDir-2.1</head><p>Due to the enormous effort needed for rigorously developing lexical resource and manually annotated corpora, we limited our work to the treatment of 46 words of three syntactic categories: 21 nouns, 7 adjectives, and 18 verbs. The selection was made trying to maintain the core words of the Senseval-2 Spanish task and sharing around 10 of the target words with Basque, Catalan, English, Italian, and Rumanian lexical tasks. Table <ref type="table" target="#tab_2">1</ref> shows the set of selected words.</p><p>We used the MiniDir-2.1 dictionary as the lexical resource for corpus tagging, which is a subset of the broader MiniDir 1 . MiniDir-2.1 was designed as a resource oriented to WSD tasks, i.e., with a granularity level low enough to avoid the overlapping of senses that commonly characterizes lexical sources. Regarding the words selected, the average number of senses per word is 5.33, corresponding to 4.52 senses for the nouns subgroup, 6.78 for verbs and 4 for adjectives (see table 1, right numbers in column '#senses').</p><p>The content of MiniDir-2.1 has been checked and refined in order to guarantee not only its consis-  tency and coverage but also the quality of the gold standard. Each sense in Minidir-2.1 is linked to the corresponding synset numbers in EuroWordNet <ref type="bibr">(Vossen, 1999)</ref> and contains syntagmatic information as collocates and examples extracted from corpora 2 . Regarding the dictionary entries, every sense is organized in nine lexical fields. See figure <ref type="figure" target="#fig_0">1</ref> for an example of one sense of the lexical entry conducir ('to drive').  After that, senses with less than 15 occurrences ( 3.5% of the examples) have been simply discarded from the datasets. See table 1, left numbers in column '#senses', for the final ambiguity rates. We know that this is a quite controversial decision that leads to a simplified setting. But we preferred to maintain the proportions of the senses naturally appearing in the EFE corpus rather than trying to artificially find examples of low frequency senses by mixing examples from many sources or by getting them with specific predefined patterns. Thus, systems trained on the MiniCors corpus are intended to discriminate between the typical word senses appearing in a news corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Spanish Corpus: MiniCors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Resources Provided to Participants</head><p>Participants were provided with the complete Minidir-2.1 dictionary, a training set with 2/3 of the labeled examples, a test set with 1/3 of the examples and a complementary big set of unlabeled examples, limited to 1,500 for each word (when available). Each example is provided with a non null list of category-labels marked according to two annotation schemes: ANPA and IPTC 3 .</p><p>Aiming at helping teams with few resources on the Spanish language, sentences in all datasets were tokenized, lemmatized and POS tagged, using the Spanish linguistic processors developed at TALP-CLiC 4 , and provided as complementary files. Table 1 contains information about the sizes of the datasets and the proportion of the most-frequent sense for each word (MFC). The baseline MFC classifier obtains a high accuracy of 67.72% due to the moderate number of senses considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Participant Systems</head><p>Seven teams took part on the Spanish Lexical Sample task, presenting a total of nine systems. We will refer to them as: Regarding the supervised learning approaches applied, we find Naive Bayes and Decision Lists (SWAT), Maximum Entropy (UA-SRT), Decision Trees (Duluth-SLSS), Support Vector Machines (IRST), AdaBoost (CSUSMCS), and a similarity method based on co-occurrences (UNED). Some systems used a voted combination of these basic learning algorithms to produce the final WSD system (SWAT, Duluth-SLSS). The two unsupervised algorithms apply only to nouns and target at obtaining high precision results (the annotations on adjectives and verbs come from a supervised MaxEnt system). UA-NSM method is called Specification Marks and uses the words that co-occur with the target word and their relation in the noun WordNet hierarchy. UA-NP bases the disambiguation on syntactic patterns and unsupervised corpus, relying on the "one sense per pattern" assumption.</p><p>All supervised teams used the POS and lemmatization provided by the organization, except Duluth-SLSS, which only used raw lexical information. A few systems used also the category labels provided with the examples. Apparently, none of them used the extra information in MiniDir (examples, collocations, synonyms, WordNet links, etc.), nor syntactic information. Thus, we think that there is room for substantial improvement in the feature set design. It is worth mentioning that the IRST system makes use of a kernel including semantic information within the SVM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and System Comparison</head><p>Table <ref type="table" target="#tab_4">2</ref> presents the global results of all participant systems, including the MFC baseline (most frequent sense classifier) and sorted by the combined F measure. The COMB row stands for a voted combination of the best systems (see the last part of the section). As it can be seen, IRST and UA-SRT are the best performing systems, with no significant differences between them 5 .</p><p>All supervised systems outperformed the MFC baseline, with a best overall improvement of 16.48 points (51.05% relative error reduction) 6 . Both unsupervised systems performed below MFC.</p><p>It is also observed that the POS and lemma information used by most supervised systems is relevant, since Duluth-SLSS (based solely on raw lexical information) performed significantly worse than the rest of supervised systems 7 .</p><p>5 Statistical significance was tested with a ¡ -test (0.95 confidence level) for the difference of two proportions.</p><p>6 These improvement figures are better than those observed in the Senseval-2 Spanish lexical sample task: 17 points but only 32.69% of error reduction.</p><p>7 With the exception of CSUSMCS, which according to ta-  Detailed results by groups of words are showed in table 3. Word groups include part-of-speech, intervals of the proportion of the most frequent sense (%MFS), intervals of the ratio number of examples per sense (ExS), and the words in the retraining set used by UA-SRT (those with a MFC accuracy lower than 70% in the training set). Each cell contains precision and recall. Bold face results correspond to the best system in terms of the F score. Last column, § -error, contains the best F improvement over the baseline: absolute difference and error reduction (%).</p><p>As in many other previous WSD works, verbs are the most difficult words (13.07 improvement and 46.7% error reduction), followed by adjectives (19.64, 52.1%), and nouns (20.78, 59.4%). The gain obtained by all methods on words with high MFC (more than 90%) is really low, indicating the difficulties of supervised ML algorithms at acquiring information about non-frequent senses). On the contrary, the gain obtained on the lowest MFC words is really good (44.3 points and 62.5% error reduction). This is a very good property of the Spanish dataset and the participant systems, which is not always observed in other empirical studies using other WSD corpora (e.g., in the Senseval-2 Spanish task values of 29.9 and 43.1% were observed). The two unsupervised systems failed at achieving a performance on nouns comparable to the baseline classifier. UA-NP has the best precision but at a cost of an extremely low recall (below 5%).</p><p>It is also observed that participant systems are quite different along word groups, being the best performances shared between IRST, UA-SRT, UMD, and UNED systems. Interestingly, IRST is the best system addressing the words with less examples per sense, suggesting that SVM is a good learning algorithm for training on small datasets, but loses this advantage for the words with more ble 3 shows a non-regular behavior with abnormal low results on some groups of words.  examples. These facts opens the avenue for further improvements on the Spanish dataset by combining the outputs of the best performing systems. As a first approach, we conducted some simple experiments on system combination by considering a voting scheme, in which each system votes and the majority sense is selected (ties are decided favoring the best method prediction). From all possible sets, the best combination includes the five systems with the best precision figures: UA-NP, IRST, UMD, UNED, and SWAT. The resulting F measure is 85.98, 1.78 points higher than the best single system (see table <ref type="table" target="#tab_4">2</ref>). This improvement comes mainly from the better F performance on nouns: from 83.89 to 87.28.</p><p>We also calculated the agreement rate and the Kappa statistic between each pair of systems. The agreement ratios ranged from 40.93% to 88.10%, and the Kappa values from 0.40 to 0.87. It is worth noting that the system relying on the simplest feature set (Duluth-SLSS) obtained the most similar output to the most frequent sense classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a Minidir-2.1 lexical entry</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>IRST, UA-NSM, UA-NP, UA-SRT, UMD, UNED, SWAT, Duluth-SLSS, and CSUSMCS. From them, seven are supervised and two unsupervised (UA-NSM, UA-NP). Only one of the participant systems uses a mixed learning strategy that allows to incorporate the knowledge from the unlabeled examples, namely UA-SRT. It is a Maximum Entropy-based system, which makes use of a re-training algorithm (inspired by Mitchell's cotraining) for iteratively relabeling unannotated examples with high precision and adding them to the training of the MaxEnt algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Information about Spanish datasets tagging handbook for the annotators. The interannotator complete agreement achieved was 90% for nouns, 83% for adjectives, and 83% for verbs. These are the best results obtained in a comparative study<ref type="bibr" target="#b1">(Taulé et al., 2004)</ref> with other dictionaries used for tagging the same corpus. The senses corresponding to multi-word expressions were eliminated since they are not considered in MiniDir-2.1.The initial goal was to obtain for each word at least 75 examples plus 15 examples per sense. For the words below these figures we performed a second round by labeling up to 200 examples more.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Overall results of all systems</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>IRST UA-SRT UMD UNED SWAT D-SLSS CSU... UA-NSM UA-NP MFC -error adjs (prec) 81.92 81.25 74.78 75.67 74.55 71.27 66.74 81.47 81.47 62.28 19.64 (rec) 81.92 81.25 74.78 75.67 74.55 71.43 66.74</figDesc><table><row><cell></cell><cell></cell><cell>81.47</cell><cell cols="2">81.47 62.28</cell><cell>52.1%</cell></row><row><cell>nouns</cell><cell>83.89 84.25 85.79 85.58 80.25 73.60 67.42</cell><cell>36.38</cell><cell cols="2">88.68 65.01</cell><cell>20.78</cell></row><row><cell></cell><cell>83.89 84.25 85.79 85.58 80.25 74.65 67.42</cell><cell>36.38</cell><cell>4.82</cell><cell>65.01</cell><cell>59.4%</cell></row><row><cell>verbs</cell><cell>85.09 84.43 80.81 79.14 79.81 75.80 68.56</cell><cell>84.76</cell><cell cols="2">84.76 72.02</cell><cell>13.07</cell></row><row><cell></cell><cell>85.09 84.43 80.81 79.14 79.81 76.31 68.56</cell><cell>84.76</cell><cell cols="2">84.76 72.02</cell><cell>46.7%</cell></row><row><cell>%MFS</cell><cell>97.17 97.17 96.69 96.69 96.69 96.69 97.17</cell><cell>83.31</cell><cell cols="2">96.90 96.69</cell><cell>0.48</cell></row><row><cell>(95,100)</cell><cell>97.17 97.17 96.69 96.69 96.69 96.69 97.17</cell><cell>83.31</cell><cell cols="2">64.09 96.69</cell><cell>14.5%</cell></row><row><cell>%MFS</cell><cell>92.77 92.54 91.38 91.84 91.38 91.61 65.50</cell><cell>90.68</cell><cell cols="2">92.56 91.38</cell><cell>1.39</cell></row><row><cell>(90,95)</cell><cell>92.77 92.54 91.38 91.84 91.38 91.61 65.50</cell><cell>90.68</cell><cell cols="2">78.32 91.38</cell><cell>16.1%</cell></row><row><cell>%MFS</cell><cell>89.04 90.11 86.36 90.37 86.10 85.71 83.16</cell><cell>63.10</cell><cell cols="2">88.89 84.76</cell><cell>5.61</cell></row><row><cell>(80,90)</cell><cell>89.04 90.11 86.36 90.37 86.10 86.63 83.16</cell><cell>63.10</cell><cell cols="2">57.75 84.76</cell><cell>36.8%</cell></row><row><cell>%MFS</cell><cell>83.82 88.51 80.91 85.11 78.64 75.08 75.89</cell><cell>59.06</cell><cell cols="2">85.59 73.62</cell><cell>14.89</cell></row><row><cell>(70,80)</cell><cell>83.82 88.51 80.91 85.11 78.64 75.08 75.89</cell><cell>59.06</cell><cell cols="2">46.12 73.62</cell><cell>56.4%</cell></row><row><cell>%MFS</cell><cell>79.73 78.59 80.31 80.88 69.98 66.22 57.93</cell><cell>48.37</cell><cell cols="2">80.25 64.44</cell><cell>16.4</cell></row><row><cell>(60,70)</cell><cell>79.73 78.59 80.31 80.88 69.98 66.35 57.93</cell><cell>48.37</cell><cell cols="2">24.86 64.44</cell><cell>46.2%</cell></row><row><cell>%MFS</cell><cell>81.06 76.11 80.38 77.82 76.96 67.91 60.34</cell><cell>44.54</cell><cell cols="2">70.04 54.27</cell><cell>26.8</cell></row><row><cell>(50,60)</cell><cell>81.06 76.11 80.38 77.82 76.96 68.26 60.34</cell><cell>44.54</cell><cell cols="2">28.33 54.27</cell><cell>58.6%</cell></row><row><cell>%MFS</cell><cell>78.75 75.33 72.07 66.57 71.03 61.18 56.02</cell><cell>57.80</cell><cell cols="2">78.31 45.17</cell><cell>33.6</cell></row><row><cell>(40,50)</cell><cell>78.75 75.33 72.07 66.57 71.03 61.81 56.02</cell><cell>57.80</cell><cell cols="2">48.29 45.17</cell><cell>61.3%</cell></row><row><cell>%MFS</cell><cell>68.35 73.39 71.43 64.71 62.75 49.35 37.54</cell><cell>49.30</cell><cell cols="2">65.92 29.13</cell><cell>44.3</cell></row><row><cell>(0,40)</cell><cell>68.35 73.39 71.43 64.71 62.75 52.94 37.54</cell><cell>49.30</cell><cell cols="2">33.05 29.13</cell><cell>62.5%</cell></row><row><cell>ExS</cell><cell>92.55 93.42 91.17 92.55 90.39 89.47 88.92</cell><cell>68.57</cell><cell cols="2">95.64 89.26</cell><cell>4.16</cell></row><row><cell>¡ 120</cell><cell>92.55 93.42 91.17 92.55 90.39 89.78 88.92</cell><cell>68.57</cell><cell cols="2">47.53 89.26</cell><cell>38.7%</cell></row><row><cell>ExS</cell><cell>86.70 88.32 86.04 88.41 83.38 80.44 64.77</cell><cell>68.00</cell><cell cols="2">91.61 75.21</cell><cell>13.2</cell></row><row><cell>(90,120)</cell><cell>86.70 88.32 86.04 88.41 83.38 80.44 64.77</cell><cell>68.00</cell><cell cols="2">54.99 75.21</cell><cell>53.2%</cell></row><row><cell>ExS</cell><cell>79.39 78.00 77.71 74.58 74.07 65.32 58.89</cell><cell>54.55</cell><cell cols="2">77.34 53.97</cell><cell>25.42</cell></row><row><cell>(60,90)</cell><cell>79.39 78.00 77.71 74.58 74.07 65.99 58.85</cell><cell>54.55</cell><cell cols="2">39.04 53.97</cell><cell>55.2%</cell></row><row><cell>ExS</cell><cell>74.92 72.31 70.68 66.12 64.17 56.04 53.42</cell><cell>55.54</cell><cell cols="2">70.42 45.11</cell><cell>29.81</cell></row><row><cell>(30,60)</cell><cell>74.92 72.31 70.68 66.12 64.17 58.14 53.42</cell><cell>55.54</cell><cell cols="2">51.95 45.11</cell><cell>54.3%</cell></row><row><cell>retrain</cell><cell>78.34 76.79 77.05 73.86 71.59 62.75 55.46</cell><cell>48.42</cell><cell cols="2">74.42 50.73</cell><cell>27.61</cell></row><row><cell></cell><cell>78.34 76.79 77.05 73.86 71.59 63.78 55.44</cell><cell>48.42</cell><cell cols="2">32.80 50.73</cell><cell>56.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of all participant systems on some selected subsets of words</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have used corpora from newspapers, El Periódico (3.5 million words), La Vanguardia (12.5 million words), and the Lexesp corpus<ref type="bibr" target="#b0">(Sebastián et al., 2000)</ref>, a balanced corpus of 5.5. million words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All the datasets of the Spanish Lexical Sample task and an extended version of this paper are available at: http://www.lsi.upc.es/¡ nlp/senseval-3/Spanish.html.4 http://www.lsi.upc.es/¡ nlp/freeling.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work has been supported by the Spanish research projects: XTRACT-2, BFF2002-04226-C03-03; FIT-150-500-2002-244; and HERMES, TIC2000-0335-C03-02. Francis Real holds a research grant by the Catalan Government (2002FI-00648). Authors want to thank the linguists of CLiC and UNED who collaborated in the annotation task.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lexesp, léxico informatizado del español</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebastián</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Carreiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Cuetos</forename><surname>Gómez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Edicions de la Universitat de Barcelona</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MiniCors and Cast3LB: Two Semantically Tagged Spanish Corpora</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Artigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Márquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th LREC</title>
				<meeting>the 4th LREC<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">EuroWordNet: A Multilingual Database with Lexical Semantic Networks</title>
		<editor>P. Vossen</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
