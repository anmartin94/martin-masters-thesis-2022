<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kata</forename><surname>Gábor</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Anne-Kathrin</forename><surname>Schumann</surname></persName>
							<email>schumann@gmx.de</email>
							<affiliation key="aff1">
								<orgName type="department">ProTechnology GmbH</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DFG SFB 991</orgName>
								<orgName type="institution">Heinrich-Heine University</orgName>
								<address>
									<settlement>Dsseldorf</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haïfa</forename><surname>Zargayouna</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at SemEval 2018. The challenge focuses on domain-specific semantic relations and includes three different subtasks. The subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. We expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. The task attracted a total of 32 participants, with 158 submissions across different scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the emerging trends of natural language technologies is their use for the humanities and sciences. Recent works in the semantic web <ref type="bibr" target="#b16">(Osborne and Motta, 2015;</ref><ref type="bibr" target="#b21">Wolfram, 2016)</ref> and natural language processing <ref type="bibr" target="#b20">(Tsai et al., 2013;</ref><ref type="bibr" target="#b14">Luan et al., 2017;</ref><ref type="bibr" target="#b1">Augenstein and Søgaard, 2017;</ref> aimed to improve the access to scientific literature, and in particular to respond to information needs that are currently beyond the capabilities of standard search engines. Such queries include finding all papers that address a problem in a specific way, or discovering the roots of a certain idea. This ambition involves the identification and classification of concepts, and the relations connecting them.</p><p>The purpose of the task is to automatically identify relevant domain-specific semantic relations in a corpus of scientific publications. In particular, we search for and classify relations that provide snippets of information such as "a (new) method is proposed for a task", or "a phenomenon is found in a certain context", or "results of different experiments are compared to each other". Identifying such semantic relations between domain-specific concepts allows us to detect research papers which deal with the same problem, or to track the evolution of results on a certain task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>SemEval 2010 Task 8 <ref type="bibr" target="#b10">(Hendrickx et al., 2010)</ref> proposed a discrete classification of word pairs into 9 semantic relations, however, this task was not tailored to the needs of scientific text analysis as neither relation types nor the vocabulary were domain-specific. SemEval 2012 Task 2 <ref type="bibr" target="#b11">(Jurgens et al., 2012)</ref> proposed a gradual notion of relational similarity: the task was to quantify the similarity between examples of relation instances. The data set was aimed at evaluating specific semantic representations for relational similarity, but does not fit our task: in this task, entity pairs were treated as static class instances; in particular, they were presented without any context. However, the relation types we deal with are contextual: e.g., a specific machine learning method is trained on a specific data set to perform an NLP task in the context of a given experiment reported by a paper. Finally, the most closely related to our task is SemEval 2017 Task 10 ( , which responds to the growing interest towards the semantic analysis of scientific corpora. This task focuses mostly on keyword extraction and categorization. The subtask concerned with relation classification proposes 3 categories of taxonomic relations <ref type="bibr">(synonym, hypernym, unrelated)</ref>. Our task goes a step further by proposing a more finegrained and, thus, more informative set of semantic relations (see Table <ref type="table" target="#tab_0">1</ref>). The relation types were selected and annotated based on a careful corpus study and are intended to represent the major re-lations that define the information content of the abstract of a scientific paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task description</head><p>The task consists in identifying and classifying instances of semantic relations between concepts in a set of 6 discrete categories. The relations are specific to the science domain and their instances can frequently be found in the abstract/introduction of scientific papers. The task is split into three subtasks. This is done to provide a framework for the systematic evaluation of the steps that are necessary for full information extraction from scientific text, i. e. relation extraction and relation classification. Two of the subtasks focus solely on the classification of relation instances into 6 relation categories. Another subtask includes both the extraction of relation instances and their classification. The data we provide is presented as complete abstracts of scientific papers. An abstract contains about 100 words on average. Entities are annotated in both the training and the test data. Furthermore, in the classification subtasks, the relation instances (entity pairs that belong to one of the relation classes) as well as the directionality of the relation (argu-ment1, argument2) are given in the training and test data. In the extraction subtask, relation instances are not provided in the test data. The training data for each subtask contains 350 annotated abstracts with the corresponding relation instances and their categories 1 . The test data consists of 150 abstracts 2 . Participants were allowed three submissions/subtask/team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relation classification scenario</head><p>Given a pair of entities in an abstract, the task consists in classifying the semantic relation between them. A pre-defined list of relations is given (see  <ref type="bibr">-1203.8, L08-1203.9</ref>). The information to be predicted is the relation class label: TOPIC(L08-1203.8, L08-1203.9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation extraction and classification scenario</head><p>Given an abstract with annotated entities, the subtask consists in:</p><p>• identifying instances of semantic relations between entities in the same sentence,</p><p>• assigning class labels, i.e. one of six predefined relation types (see  The training data we provide contains the same information as in the classification scenario, i.e. manually annotated entities, and labeled semantic relations holding between entities. The test data contains only abstracts with annotated entities: both the entity pairs and their relation type are to be predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Submissions are evaluated differently for the individual subtasks. A dedicated gold standard containing entity and relation annotations is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Metrics for the classification scenario (subtasks 1.1 and 1.2)</head><p>Submissions for scenario 1 are assessed by means of standard metrics:</p><p>• Class-wise evaluation: Precision, recall, and F1 (β = 1) for each relation type.</p><p>• Global evaluation:</p><p>-Macro-average of the F1 scores obtained for every relation type. -Micro-average of the F1 scores obtained for every relation type.</p><p>The official ranking of submissions is performed according to the macro-average F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Metrics for the extraction and classification scenario (Subtask 2)</head><p>Evaluation of submissions for scenario 2 is carried out in two steps:</p><p>• Evaluation of relation extraction: Extraction evaluation assesses the quality of identified relation instances. Relation labels and directionality are ignored in this step. Precision is calculated as the percentage of correctly connected entity pairs. Recall is calculated as the percentage of gold entity pairs found by the system. The official F1 score is calculated as the harmonic mean of precision and recall.</p><p>• Evaluation of relation classification: Classification evaluation considers only correctly identified relation instances as per step 1. For these instances, the same evaluation metrics are calculated as for task 1. The official score for this task is macro-average F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Preparation</head><p>The task is carried out on abstracts from published research papers in computational linguistics. Two existing high-quality corpora were used as starting points for data creation, namely ACL RD-TEC 2.0 (QasemiZadeh and Schumann, 2016) and ACL-RelAcS <ref type="bibr" target="#b7">(Gábor et al., 2016a)</ref>. Both resources are based on the ACL Anthology Reference Corpus <ref type="bibr" target="#b2">(Bird et al., 2008)</ref>. In ACL RD-TEC 2.0 entities were annotated manually, and it was used for the "clean" subtasks (subtasks 1.1 and 2). In ACL-RelAcS, entities were annotated fully automatically, and it was used for the "noisy" subtask (1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity annotation</head><p>Manual ("clean") entity annotations were carried out in accordance with the ACL RD-TEC annotation guidelines <ref type="bibr" target="#b19">(Schumann and QasemiZadeh, 2015)</ref>. Thus, for subtasks 1.1 and 2 (training data) termhood is defined by a combination of semantic, linguistic, and formal criteria. The formal criteria, for instance, aim at making the annotations maximally useful for real-world extraction scenarios by accounting for various contextual usage patterns of terminological units in scientific prose. Therefore, annotators are instructed to annotate maximal noun phrases, abbreviations, and their contextual variants, including variants with incorrect spelling. Still, entity annotation proves to be a non-trivial task even for human expert annotators: Qasemi-Zadeh and <ref type="bibr" target="#b18">Schumann (2016)</ref> show that agreement scores are satisfactory (e.g., κ &gt; 0.7) only after a thorough annotation training phase and the subsequent refinement of the annotation guidelines.</p><p>To extend the set of abstracts that were already available in ACL RD-TEC with double entity annotations, expert annotators were recruited from amongst the task organizers. Annotators were asked to read the ACL RD-TEC annotation guidelines. A training phase was carried out, during which each annotator carried out test annotations on unseen data. To facilitate annotations, abstracts were pre-annotated automatically using the automatic entity annotator of the ACL-RelAcS corpus (see below). Annotators were asked to correct the automatic annotations, in particular, to correct the boundary of the identified entity. Individual feedback was provided to novice annotators and annotation difficulties were clarified. Annotations were consistently monitored and potential causes for disagreement discussed and corrected.</p><p>The ACL RD-TEC already provided 171 doubleannotated and 129 single-annotated abstracts. While double-annotations could directly be passed over to manual relation annotation, more singlepass annotations had to be performed to create a fully double-annotated training set. The remaining 150 abstracts for the test set of subtask 1.1 were single-annotated. It should be noted that, due to their origin from ACL RD-TEC, abstracts for subtask 1.1 contain not only entity annotations, but also information about the the semantic class of the annotated entity. This information was not explicitly included in the provided data, but was accessible to participants through the original ACL RD-TEC corpus.</p><p>The "noisy" subtask (1.2) was carried out on data coming from the ACL-RelAcS corpus 1.0 <ref type="bibr" target="#b7">(Gábor et al., 2016a)</ref>. The corpus consists of 4.2 million words from the abstract and introduction sections of papers in the ACL Anthology Corpus, with an automatic annotation of entities. This automatic annotation is based on a gazetteer which, in turn, was created using a combination of terminology extraction tools and ontological resources. As a domain specific resource, the domain models and topic hierarchies in the NLP domain from Saffron Knowledge Extraction Framework 3 <ref type="bibr" target="#b3">(Bordea, 2013;</ref> were included. Terminology extraction was performed with TermSuite <ref type="bibr" target="#b6">(Daille et al., 2013)</ref> and the resulting list of terms was filtered by part of speech and looked up in BabelNet <ref type="bibr" target="#b15">(Navigli and Ponzetto, 2012)</ref>. The extracted terms that were found in BabelNet were added to the gazetteer and used for automatic annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation annotation</head><p>The work was divided as 1) defining the typology of semantic relations, 2) validation of the typology and of the annotation guidelines and 3) annotation. A data-driven approach was adopted to identify the relation types and define a typology <ref type="bibr" target="#b8">(Gábor et al., 2016b)</ref>. Domain experts studied the abstracts with entity annotation and were instructed to read the text and indicate the semantic relations that are explicit and relevant for the understanding of the abstract. They annotated entity pairs and the text span between them which explicitly indicates the relation.</p><p>Instances of explicit relations were thus discovered and manually annotated in a sample of 100 abstracts from ACL-RelAcS. A fine-grained typology of domain-specific relations was set up. The finegrained relation types (see Table <ref type="table" target="#tab_0">1</ref>) were defined very precisely and specifically, e.g. using strict constraints on which types of entities the relations take as argument. The manual annotation used this typology; the relations were then automatically converted to the 6 types used in the classification tasks.</p><p>Only explicit relations were annotated, between already annotated entities. Entity annotation itself is never modified or corrected manually during the relation annotation phase. On the textual level, a semantic relation is conceived as a text span link-ing two annotated instances of concepts within the same sentence. On the semantic level, relation types need to be specific enough to be easily distinguished from each other by a domain expert. Annotation was carried out by one of the organizers and two NLP student annotators who were subjected to a training of three weeks during which they annotated 100 abstracts under supervision. This training material was not included in the future dataset. Weekly feedback was given and difficult instances were discussed. If the annotation quality in the 100 abstracts was judged satisfactory, the annotator was allowed to carry on, and their subsequent annotations were included in the dataset (two out of three annotator candidates passed this phase).</p><p>Inter-annotator agreement was calculated using a double annotation on a sample of 150 abstracts from subtask 1.1 by two annotators. The overall class label agreement rate on these annotations was 90.8%. We also calculated the macro-averaged F1 score across classes, taking one of the annotators as "gold standard". The result was 0.91 (the performance of the best ranking system on this task is 0.81). When comparing agreement for individual relations, it turns out that the relation with the lowest agreement (F1=0.83) is PART WHOLE, followed by RESULT (F1=0.89).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline system</head><p>As a baseline, we created a simple memory-based k-nearest neighbor (k-nn) search <ref type="bibr" target="#b5">(Daelemans and van den Bosch, 2005)</ref> which relies on a small set of hand-crafted features.</p><p>Given a sentence s annotated with an ordered set of e 1 . . . e n entities appearing in it, we first pull out all tuples (e i , e j ), in which j − i ≤ 2. For each tuple (e i , e j ), we encode their co-occurrence context using a set of 5 vectors of low dimensionality (n = 100). These vectors encode information about (a) tokens that appear before e i in s (we use simple white-space tokenization), (b) tokens that appear between e i and e j , (c) tokens appearing after e j , as well as (d) two additional vectors that capture the context of e i and e j occurrences in the ACL Anthology Reference Corpus <ref type="bibr" target="#b2">(Bird et al., 2008)</ref>. To encode information about these contexttoken occurrences into low-dimensional vectors, we use positive-only random projections <ref type="bibr">(Qasemi-Zadeh and Kallmeyer, 2016)</ref>. Additionally, feature vectors in each of the above-mentioned categories are weighted using positive pointwise mutual information with respect to the collected co-occurrence information in vectors for each category for all the tuples in the training and test data (for each subtask). Finally, the weighted vectors are concatenated to form a 500 dimensional feature vector for each entity pair.</p><p>For each subtask, all the (e i , e j ) extracted from the sentences in the training set are added to the k-nn's training instance memory T : if (e i , e j ) is annotated with a relation, then the fetched label is assigned to it, otherwise it is marked as a negative example. Given the feature vector v for a tuple (e x , e y ) in the test set, the similarity between v and all the training instances t i ∈ T is computed using the Pearson's correlation to find the k most similar t i . Finally, we assign (e x , e y ) to the relation category l y using a majority voting.</p><p>Results obtained from this baseline system are listed in Tables <ref type="table" target="#tab_11">5, 7</ref>, 6, and 8 in the Appendix. We choose k = 5 based on the observed performances over the development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Summary of participating systems and results</head><p>The task attracted 32 participants altogether who took part in at least one subtask. ticipants exploited the possibility of aggregating training data from subtask 1.1 and subtask 1.2. Word embeddings were used as features by the majority of systems (13 systems). Some participants chose to calculate the embeddings on domainspecific corpora, such as ACL (4 systems) and arXiv (3 systems), sometimes in combination with pre-trained embeddings. Pre-trained embeddings alone were used by a minority of participants, with TakeLab highlighting some problems in dealing with out-of-vocabulary words. Apart from the corpora dedicated to training the embeddings, participants didn't use external resources, with the exception of one system which employed VerbNet and two systems that used WordNet synonyms and hypernyms.</p><p>Figure <ref type="figure">2</ref>: Popularity of methods chosen by participants (as number of systems that used the method, left) and average F1 score obtained for each method (right) in Subtask 2. Among the chosen features, positional embeddings were quite popular (5 systems), to account for the relative position of the left and right entities.</p><p>Only three participants recurred to syntactic features, in particular dependency trees, despite their apparent relevance for the task.</p><p>SpaCy 4 and CoreNLP 5 were the most popular tools to analyze and preprocess text, with a slight preference for the first one (4 participants vs. 2).</p><p>6 Analysis of Results 6.1 Which processing step is the most difficult?</p><p>From the overall task results provided in the Appendix (Tables <ref type="table" target="#tab_11">5 -8</ref>), it seems straightforward to conclude that the reliable identification of semantic relation instances is by far the most difficult step in the complete processing pipeline: Whereas systems reached an average F1 score of 47.28 in subtask 1.1 and 62.51 in subtask 1.2, performance scores drop rather sharply in scenario 2, namely to an average F1 of 30.8 for the extraction task and 20.34 for the extraction+classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Which relation types are the most difficult to classify?</head><p>We examined whether there were relation types that were more difficult for the systems to classify, and whether it is possible to relate this to the semantics of the relations. For instance, the class MODEL-FEATURE is broad because it encompasses relatively different sub-classes: models, parts of models (such as a representation, a tag used for a word), or attributes (frequency of a phenomenon). To analyze this, we calculated the average recall by relation type over a sample of submissions to subtask 1.1 (70 submissions) and 1.2 (42 submissions) and the characteristic prediction error types by relation, if any (Table <ref type="table" target="#tab_5">2</ref>). We also calculated the average F1 score by relation type of the five top scoring systems from different participants (Tables <ref type="table" target="#tab_7">3 and 4</ref>). Our analysis suggests that rather than the semantics of the relation types, it is their distribution in the data that poses difficulties. Class distribution is very imbalanced. Moreover, the distribution of classes in training and test data of subtask 1.1 and 1.2 is different. This difference is due to the nature of entities annotated automatically and those annotated manually. Because of the terminology extraction process and the resources that were used  for annotation, entities in subtask 1.2 are typically shorter terms with an intermediate level of specificity. On the other hand, entities in the clean scenario are more complex and more specific to the NLP domain. For instance, the TOPIC relation is more frequent in 1.2 than in 1.1 because entities like "paper" or "article" were annotated by the automated process, but not in the manual annotation.</p><p>Another aspect is that certain classes are lexically less varied than others and this might well affect the "difficulty" of the classification task. For instance, the TOPIC class has the lowest type-token ratio of all classes in subtask 1.2 6 . This does not seem surprising. Neither does it seem surprising that in subtask 1.2, TOPIC has gained the best average recall (2) and the highest F1 score among the top-5 systems (4). TOPIC is also much more frequent in subtask 1.2 than in subtask 1.1 and this effect is one likely cause for the difference in performance achieved over subtasks 1.1 and 1.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The effects of entity annotation</head><p>Entity annotation has a demonstrable effect on system performance. As stated earlier, annotation decisions have direct consequences for the distribution of certain types in the data and thus influence measurable system performance.  A maybe rather surprising result of this task is the difference in system performance for subtasks 1.1 and 1.2. While "clean" entities can, with some plausibility, be considered more useful for a potential human user of the extracted information, "noisy" entity annotations seem to be more machine-friendly. The difference in the distribution of the TOPIC relation between subtasks 1.1 and 1.2 has already been pointed out as one potential cause for this effect. Moreover, the complexity of clean entities in subtask 1.1 could also have contributed to the performance gap. Manually annotated entities, in most cases, are long noun phrases, whereas automatically annotated entities in subtask 1.2 are generally shorter, partial (and therefore less specific!) entity matches. This also means that more training examples are likely to be found for automatically annotated entities. Moreover, some instances of automatic annotations in subtask 1.2 included explicit verbal relation cues. These cues sometimes explicitly state the type of the semantic relation, but they were not annotated in subtask 1.1. Verbal cues (e. g. the well-known Hearst patterns <ref type="bibr" target="#b9">(Hearst, 1992)</ref>) have typically been used in earlier work on relation classification and, in fact, several teams participating in the task describe recurrent verbal elements between relation arguments.</p><p>The role of the specialized lexicon in relation extraction and classification is a topic that de-serves further exploration for the following reasons: Firstly, highly specialized, complex terminological units are the main units of knowledge representation in specialized domains. Secondly, task results clearly show that a careful handling of lexical information improves performance: many successful systems in the task used domain-specific training data. The only system that treated complete specialized entities as semantic units, UWNLP, ranked first in the relation extraction task. None of the systems participating in subtasks 1.1 or 2 used semantic class information available for annotated entities from ACL RD-TEC, although it may be hypothesized that this feature helps to generalize lexical instance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented the setup and results of SemEval 2018 Task 7: Semantic relation extraction and classification in scientific papers. The task is divided into three subtasks: classification on clean data, classification on noisy data, and a combined extraction and classification scenario. We also presented the dataset used for the challenge: a subset of abstracts of published papers in the ACL Anthology Reference Corpus, annotated for domain specific entities and semantic relations.</p><p>32 participants submitted to one or more subtasks. The most popular methods include Convolutional Neural Networks and Long Short Term Memory networks, with word embedding based features, often calculated on domain-specific corpora. Although it was allowed, only a minority of the participants used external knowledge resources. The results show that while good results can be obtained on the supervised multi-class classification of relation instances, the extraction of such instances remains very challenging. Moreover, the quality and type of entity annotation also plays an important role in determining relation extraction and classification results. Knowledge extraction from a special domain poses specific challenges, such as working with a smaller corpus, dealing with specialized vocabularies, and the scarcity of annotated data and available domain-specific resources. One of the important future directions is to explore domain adaptation techniques to address these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Competition Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank Participant</head><p>Macro-F1 Score     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 )</head><label>1</label><figDesc>, together with training examples for each relation.</figDesc><table><row><cell>• Subtask 1.1 : Relation classification on clean data. Entity occurrences are manually annotated in</cell></row><row><cell>both the training and the test data. In the train-</cell></row><row><cell>ing data, semantic relations are manually an-</cell></row><row><cell>notated between entities. In the test data, only</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>), to the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Semantic relation typology. The six major relation types result from a finer grained classification which was used in manual annotation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Relations: results and distribution.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>tems.</cell><cell>: Relations: Task 1.1 average results top 5 sys-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>tems.</cell><cell>: Relations: Task 1.2 average results top 5 sys-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Results for subtask 1.1.</figDesc><table><row><cell cols="2">Rank Participant</cell><cell>F1 Score</cell></row><row><cell>1</cell><cell>UWNLP</cell><cell>50.0</cell></row><row><cell>2</cell><cell>ETH-DS3Lab</cell><cell>48.8</cell></row><row><cell>3</cell><cell>SIRIUS-LTG-UiO</cell><cell>37.4</cell></row><row><cell>4</cell><cell>UC3M-NII</cell><cell>35.4</cell></row><row><cell>5</cell><cell>NTNU</cell><cell>33.9</cell></row><row><cell>6</cell><cell>Bf3R</cell><cell>33.4</cell></row><row><cell>7</cell><cell>UniMa</cell><cell>28.4</cell></row><row><cell cols="2">N/A Baseline</cell><cell>26.8</cell></row><row><cell>8</cell><cell>NEUROSENT-PDI</cell><cell>25.6</cell></row><row><cell>9</cell><cell>Texterra</cell><cell>15.6</cell></row><row><cell>10</cell><cell>xingwang</cell><cell>15.3</cell></row><row><cell>11</cell><cell>danish037</cell><cell>15.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Results for subtask 2: Extraction.</figDesc><table><row><cell cols="2">Rank Participant</cell><cell>Macro-F1 Score</cell></row><row><cell>1</cell><cell>ETH-DS3Lab</cell><cell>90.4</cell></row><row><cell>2</cell><cell>Talla</cell><cell>84.8</cell></row><row><cell>3</cell><cell>SIRIUS-LTG-UiO</cell><cell>83.2</cell></row><row><cell>4</cell><cell>MIT-MEDG</cell><cell>80.6</cell></row><row><cell>5</cell><cell>GU IRLAB</cell><cell>78.9</cell></row><row><cell>6</cell><cell>ClaiRE</cell><cell>78.4</cell></row><row><cell>7</cell><cell>TakeLab</cell><cell>75.7</cell></row><row><cell>8</cell><cell>OhioState</cell><cell>74.7</cell></row><row><cell>9</cell><cell>Texterra</cell><cell>74.4</cell></row><row><cell>10</cell><cell>IRCMS</cell><cell>71.1</cell></row><row><cell>11</cell><cell>LaSTUS/TALN</cell><cell>69.5</cell></row><row><cell>12</cell><cell>LIGHTREL</cell><cell>68.2</cell></row><row><cell>13</cell><cell>NTNU</cell><cell>66.0</cell></row><row><cell>14</cell><cell>LTRC</cell><cell>65.7</cell></row><row><cell cols="2">N/A Baseline</cell><cell>53.5</cell></row><row><cell>15</cell><cell>likewind 1234</cell><cell>45.8</cell></row><row><cell>16</cell><cell>BIT NLP</cell><cell>40.7</cell></row><row><cell>17</cell><cell>hccl</cell><cell>38.0</cell></row><row><cell>18</cell><cell>xingwang</cell><cell>26.7</cell></row><row><cell>19</cell><cell>NEUROSENT-PDI</cell><cell>21.8</cell></row><row><cell>20</cell><cell>UKP</cell><cell>15.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Results for subtask 1.2.</figDesc><table><row><cell cols="2">Rank Participant</cell><cell>Macro-F1 Score</cell></row><row><cell>1</cell><cell>ETH-DS3Lab</cell><cell>49.3</cell></row><row><cell>2</cell><cell>UWNLP</cell><cell>39.1</cell></row><row><cell>3</cell><cell>SIRIUS-LTG-UiO</cell><cell>33.6</cell></row><row><cell>4</cell><cell>Bf3R</cell><cell>20.3</cell></row><row><cell>5</cell><cell>UC3M-NII</cell><cell>18.5</cell></row><row><cell>6</cell><cell>NTNU</cell><cell>17.0</cell></row><row><cell cols="2">N/A Baseline</cell><cell>12.6</cell></row><row><cell>7</cell><cell>Texterra</cell><cell>9.6</cell></row><row><cell>8</cell><cell>xingwang</cell><cell>8.3</cell></row><row><cell>9</cell><cell>danish037</cell><cell>4.6</cell></row><row><cell>10</cell><cell>NEUROSENT-PDI</cell><cell>3.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Results for subtask 2: Extraction + Classification.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The training data for subtask 1.1 and subtask 2 were identical.2 After the end of the competition, the complete dataset was published at https://lipn.univ-paris13.fr/ gabor/semeval2018task7/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://saffron.insight-centre.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://spacy.io/ 5 https://stanfordnlp.github.io/ CoreNLP/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In this analysis, a tuple of two entities pertaining to a certain relation class was counted as a "type".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was generously supported by the program "Investissements d'Avenir" overseen by the French National Research Agency, ANR-10-LABX-0083 (Labex EFL). Behrang QasemiZadeh is funded by the Deutsche Forschungsgemeinschaft through the "Collaborative Research Centre 991 (CRC 991): The Structure of Representations in Language, Cognition, and Science".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ScienceIE -extracting keyphrases and relations from scientific publications</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Vikraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
				<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="546" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning of keyphrase boundary classification</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Fan</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Language Resources and Evaluation</title>
				<meeting>the 6th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Domain Adaptive Extraction of Topical Hierarchies for Expertise Mining</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Galway</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National University of Ireland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-independent term extraction through domain modelling</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Terminology and Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory-Based Language Processing</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antal</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Natural Language Processing</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TTC TermSuite : Une chaine de traitement pour la fouille terminologique multilingue</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Daille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Jacquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Monceaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Rocheteau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Traitement Automatique des Langues Naturelles Conference (TALN)</title>
				<meeting>the Traitement Automatique des Langues Naturelles Conference (TALN)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic annotation of the ACL Anthology Corpus 686 for the automatic analysis of scientific literature</title>
		<author>
			<persName><forename type="first">Kata</forename><surname>Gábor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hafa</forename><surname>Zargayouna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC)</title>
				<meeting>the International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
	<note>Isabelle Tellier, and Thierry Charnois</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A typology of semantic relations dedicated to scientific literature analysis</title>
		<author>
			<persName><forename type="first">Kata</forename><surname>Gábor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hafa</forename><surname>Zargayouna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Tellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAVE-SD Workshop at the 25th World Wide Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9792</biblScope>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;92</title>
				<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diarmuid</forename><surname>Saghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations</title>
				<meeting>the Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>SemEval-2010</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Holyoak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Task 2: Measuring degrees of relational similarity</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations</title>
				<meeting>the Workshop on Semantic Evaluations</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olena</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scientific information extraction with semisupervised neural tagging</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ba-belNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Klink-2: Integrating multiple web sources to generate semantic topic networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on The Semantic Web -ISWC 2015</title>
				<meeting>the 14th International Conference on The Semantic Web -ISWC 2015<address><addrLine>New York, NY, USA; New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9366</biblScope>
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random Positive-Only Projections: PPMI-enabled incremental semantic space construction</title>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Kallmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the Fifth Joint Conference on Lexical and Computational Semantics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ACL RD-TEC 2.0: A language resource for evaluating term extraction and entity recognition methods</title>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC)</title>
				<meeting>the International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The ACL RD-TEC Annotation Guideline: A Reference Dataset for the Evaluation of Automatic Term Recognition and Classification</title>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concept-based analysis of scientific literature</title>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management ACM</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1733" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bibliometrics, information retrieval and natural language processing: Natural synergies to support digital library research</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Wolfram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Bibliometricenhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL)</title>
				<meeting>the Joint Workshop on Bibliometricenhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
