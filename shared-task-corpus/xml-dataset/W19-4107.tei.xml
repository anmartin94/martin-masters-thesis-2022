<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSTC7 Task 1: Noetic End-to-End Response Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chulaka</forename><surname>Gunasekara</surname></persName>
							<email>chulaka.gunasekara@ibm.com</email>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
							<email>wlasecki@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Research</forename><surname>Center</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DSTC7 Task 1: Noetic End-to-End Response Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic dialogue systems have great potential as a new form of user interface between people and computers. Unfortunately, there are relatively few large resources of human-human dialogues <ref type="bibr" target="#b8">(Serban et al., 2018)</ref>, which are crucial for the development of robust statistical models. Evaluation also poses a challenge, as the output of an end-to-end dialogue system could be entirely reasonable, but not match the reference, either because it is a paraphrase, or it takes the conversation in a different, but still coherent, direction.</p><p>In this shared task, we introduced two new datasets and explored variations in task structure for research on goal-oriented dialogue. One of our datasets was carefully constructed with real people acting in a university student advising scenario. The other dataset was formed by applying a new disentanglement method <ref type="bibr" target="#b3">(Kummerfeld et al., 2019)</ref> to extract conversations from an IRC channel of technical help for the Ubuntu operating system. We structured the dialogue problem as next utterance selection, in which participants receive partial dialogues and must select the next utterance from a set of options. Going beyond prior work, we considered larger sets of options, and variations with either additional incorrect options, paraphrases of the correct option, or no correct option at all. These changes push the next utterance selection task towards real-world dialogue.</p><p>This task is not a continuation of prior DSTC tasks, but it is related to tasks 1 and 2 from DSTC6 <ref type="bibr" target="#b6">(Perez et al., 2017;</ref><ref type="bibr" target="#b1">Hori and Hori, 2017)</ref>. Like DSTC6 task 1, our task considers goal-oriented dialogue and next utterance selection, but our data is from human-human conversations, whereas theirs was simulated. Like DSTC6 task 2, we use online resources to build a large collection of dialogues, but their dialogues were shorter (2 -2.5 utterances per conversation) and came from a more diverse set of sources (1,242 twitter customer service accounts, and a range of films).</p><p>This paper provides an overview of (1) the task structure, (2) the datasets, (3) the evaluation metrics, and (4) system results. Twenty teams participated, with one clear winner, scoring the highest on all but one sub-task. The data and other resources associated with the task have been released 1 to enable future work on this topic and to make accurate comparisons possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>This task pushed the state-of-the-art in goaloriented dialogue systems in four directions deemed necessary for practical automated agents, using two new datasets. We sidestepped the challenge of evaluating generated utterances by formulating the problem as next utterance selection, as proposed by <ref type="bibr" target="#b5">Lowe et al. (2015)</ref>. At test time, participants were provided with partial conversations, each paired with a set of utterances that could be the next utterance in the conversation. Systems needed to rank these options, with the goal of placing the true utterance first. Prior work used sets of 2 or 10 utterances. We make the task harder by expanding the size of the sets, and considered several advanced variations: Subtask 1 100 candidates, including 1 correct option.</p><p>Subtask 2 120,000 candidates, including 1 correct option (Ubuntu data only).</p><p>Subtask 3 100 candidates, including 1-5 correct options that are paraphrases (Advising data only).</p><p>Subtask 4 100 candidates, including 0-1 correct options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 5</head><p>The same as subtask 1, but with access to external information.</p><p>These subtasks push the capabilities of systems. In particular, when the number of candidates is small (2-10) and diverse, it is possible that systems are learning to differentiate topics rather than learning dialogue. Our variations move towards a task that is more representative of the challenges involved in dialogue modeling.</p><p>As part of the challenge, we provided a baseline system that implemented the Dual-Encoder model from <ref type="bibr" target="#b5">Lowe et al. (2015)</ref>. This lowered the barrier to entry, encouraging broader participation in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We used two datasets containing goal-oriented dialogues between two participants, but from very different domains. This challenge introduced the two datasets, and we kept the test set answers secret until after the challenge. <ref type="bibr">2</ref> To construct the partial conversations we randomly split each conversation. Incorrect candidate utterances are selected by randomly sampling utterances from the dataset. For subtask 3 (paraphrases), the incorrect candidates are sampled with paraphrases as well. For subtask 4 (no correct option sometimes), twenty percent of examples were randomly sampled and the correct utterance was replaced with an additional incorrect one. 10:30 &lt;elmaya&gt; is there a way to setup grub to not press the esc button for the menu choices? 10:31 &lt;scaroo&gt; elmaya, edit /boot/grub/ menu.lst and comment the "hidemenu" line 10:32 &lt;scaroo&gt; elmaya, then run grub -install 10:32 &lt;scaroo&gt; grub-install 10:32 &lt;elmaya&gt; thanls scaroo 10:32 &lt;elmaya&gt; thanks Figure <ref type="figure">1</ref>: Example Ubuntu dialogue before our preprocessing.</p><p>Along with the datasets we provided additional sources of information. Participants were able to use the provided knowledge sources as is, or automatically transform them to appropriate representations (e.g. knowledge graphs, continuous embeddings, etc.) that were integrated with end-toend dialogue systems so as to increase response accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ubuntu</head><p>We constructed one dataset from the Ubuntu Internet Relay Chat (IRC) support channel, in which users help each other resolve technical problems related to the Ubuntu operating system. We consider only conversations in which one user asks a question and another helps them resolve their problem. We extracted conversations from the channel using the conversational disentanglement method described by <ref type="bibr" target="#b3">Kummerfeld et al. (2019)</ref>, trained with manually annotated data using Slate <ref type="bibr" target="#b3">(Kummerfeld, 2019)</ref>. <ref type="bibr">34</ref> This approach is not perfect, but we inspected one hundred dialogues and found seventy-five looked like reasonable conversations. See <ref type="bibr" target="#b3">Kummerfeld et al. (2019)</ref> for detailed analysis of the extraction process. We further applied several filters to increase the quality of the extracted dialogues: (1) the first message is not directed, (2) there are exactly two participants (a questioner and a helper), not counting the channel bot, (3) no more than 80% of the messages are by a single participant, and (4) there are at least three turns. This approach produced 135,000 conversations, and each was cut off at different points to create the necessary conversations for all the sub-Student Hi professor, I am looking for courses to take.  tasks. For this setting, manual pages were provided as a form of knowledge grounding. Figure <ref type="figure">1</ref> shows an example dialogue from the dataset. For the actual challenge we identify the users as 'speaker 1' (the person asking the question) and 'speaker 2' (the person answering), and removed usernames from the messages (such as 'elmaya' in the example). We also combined consecutive messages from a single user, and always cut conversations off so that the last speaker was the person asking the question. This meant systems were learning to behave like the helpers, which fits the goal of developing a dialogue system to provide help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Advising</head><p>Our second dataset is based on an entirely new collection of dialogues in which university students are being advised which classes to take. These were collected at the University of Michigan with IRB approval. Pairs of Michigan students playacted the roles of a student and an advisor. We provided a persona for the student, describing the classes they had taken already, what year of their degree they were in, and several types of class preferences (workloads, class sizes, topic areas, time of day, etc. had taken, what classes were available, and which were suggested (based on aggregate statistics from real student records). The data was collected over a year, with some data collected as part of courses in NLP and social computing, and some collected with paid participants.</p><p>In the shared task, we provide all of this information -student preferences, and course information -to participants. 815 conversations were collected, and then the data was expanded by collecting 82,094 paraphrases using the crowdsourcing approach described by . Of this data, 500 conversations were used for training, 100 for development, and 100 for testing. The remaining 115 conversations were used as a source of negative candidates in the candidate sets. For the test data, 500 conversations were constructed by cutting the conversations off at 5 points and using paraphrases to make 5 distinct conversations. The training data was provided in two forms. First, the 500 training conversations with a list of paraphrases for each utterance, which participants could use in any way. Second, 100,000 partial conversations generated by randomly selecting paraphrases for every message in each conversation and selecting a random cutoff point.</p><p>Two versions of the test data were provided to participants. The first had some overlap with the training set in terms of source dialogues, while the second did not. We include results on both in this paper for completeness, but encourage all future work to only consider the second test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison</head><p>Table <ref type="table" target="#tab_1">1</ref> provides statistics about the two raw datasets. The Ubuntu dataset is based on several orders of magnitude more conversations, but they are automatically extracted, which means there are errors (conversations that are missing utterances or contain utterances from other conversations). Both have similar length utterances, but these values are on the original Ubuntu dialogues, before we merge consecutive messages from the same user. The Advising dialogues contain more messages on average, but the Ubuntu dialogues cover a wider range of lengths (up to 118 messages). Interestingly, there is less diversity in tokens for Ubuntu, but more diversity in utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Twenty teams submitted entries for at least one subtask. <ref type="bibr">5</ref> Teams had 14 weeks to develop their systems with access to the training and validation data, plus the external resources we provided. Additional external resources were not permitted, with the exception of pre-trained embeddings that were publicly available prior to the release of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>Table <ref type="table">5</ref> presents a summary of approaches teams used. One clear trend was the use of the Enhanced LSTM model (ESIM, <ref type="bibr" target="#b0">Chen et al., 2017)</ref>, though each team modified it differently as they worked to improve performance on the task. Other approaches covered a wide range of neural model components: Convolutional Neural Networks, Memory Networks, the Transformer, Attention, and Recurrent Neural Network variants. Two teams used ELMo word representations <ref type="bibr" target="#b7">(Peters et al., 2018)</ref>, while three constructed ensembles. Several teams also incorporated more classical approaches, such as TF-IDF based ranking, as part of their system.</p><p>We provided a range of data sources in the task, with the goal of enabling innovation in training methods. Six teams used the external data, while four teams used the raw form of the Advising data. The rules did not state whether the validation data could be used as additional training data at test time, and so we asked each team what they used. As Table <ref type="table">5</ref> shows, only four teams trained their systems with the validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We considered a range of metrics when comparing models. Following <ref type="bibr" target="#b5">Lowe et al. (2015)</ref>, we use Recall@N, where we count how often the correct answer is within the top N specified by a system. In prior work, there were either 2 or 10 candidates (including the correct one), and N was set at 1, 2, or 5. Our sets are larger, with 100 candidates, and so we considered larger values of N: 1, 10, and 50. 10 and 50 were chosen to correspond to 1 and 5 in prior work (the expanded candidate set means they correspond to the same fraction of the space of options). We also considered a widely used metric from the ranking literature: Mean Reciprocal Rank (MRR). Finally, for subtask 3 we measured Mean Average Precision (MAP) since there are multiple correct utterances in the set.</p><p>To determine a single winner for each subtask, we used the mean of Recall@10 and MRR, as presented in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Table <ref type="table" target="#tab_3">2</ref> presents the overall scores for each team on each subtask, ordered by teams' average rank. Table <ref type="table">4</ref> presents the full set of results, including all metrics for all subtasks.</p><p>Overall Results Team 3 consistently scored highest, winning all but one subtask. Looking at individual metrics, they had the best score 75% of the time on Ubuntu and all of the time on the final Advising test set. The subtask they were beaten on was Ubuntu-2, in which the set of candidates was drastically expanded. Team 10 did best on that task, indicating that their extra filtering step provided a key advantage. They filtered the 120,000 sentence set down to 100 options using a TF-IDF based method, then applied their standard approach to that set. Subtasks 1. The first subtask drew the most interest, with every team participating in it for one of the datasets. Performance varied substantially, covering a wide range for both datasets, particularly on Ubuntu.</p><p>2. As expected, subtask 2 was more difficult than task 1, with consistently lower results. However, while the number of candidates was increased from 100 to 120,000, performance reached as high as half the level of task 1, which suggests systems could handle the large set effectively.</p><p>3. Also as expected, results on subtask 3 were slightly higher than on subtask 1. Comparing   MRR and MAP it is interesting to see that while the ranking of systems is the same, in some cases MAP was higher than MRR and in others it was lower.</p><p>4. For both datasets, results on subtask 4, where the correct answer was to choose no option 20% of the time, are generally similar. On average, no metric shifted by more than 0.016, and some went up while others went down. This suggests that teams were able to effectively handle the added challenge.</p><p>5. Finally, on subtask 5 we see some slight gains in performance, but mostly similar results, indicating that effectively using external resources remains a challenge. Metrics Finally, we can use Table <ref type="table">4</ref> to compare the metrics. In 39% of cases a team's ranking is identical across all metrics, and in 34% there is a difference of only one place. The maximum difference is 5, which occurred once, between team 6's results in the final Advising results shown in Table 3, where their Recall@1 result was 8th, their Recall@10 result was 11th and their Recall@50 result was 13th. Comparing MRR and Recall@N, the MRR rank is outside the range of ranks given by the recall measures 9% of the time (on Ubuntu and the final Advising evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advising Test Sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>This task provides the basis for a range of interesting new directions. We randomly selected negative options, but other strategies could raise the difficulty, for example by selecting very similar candidates according to a simple model. For evaluation, it would be interesting to explore human judgements, since by expanding the candidate sets we are introducing options that are potentially reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This task introduced two new datasets and three new variants of the next utterance selection task. Twenty teams attempted the challenge, with one clear winner. The datasets are being publicly released, along with a baseline approach, in order to facilitate further work on this task. This resource will support the development of novel dialogue systems, pushing research towards more realistic and challenging settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example Advising dialogue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Do you have any suggestions? Advisor What topic do you prefer, computer science or electrical engineering? Student I prefer electrical engineering. Advisor Based on your background, I would like to suggest you take one of the two courses:</figDesc><table><row><cell>EECS 550 Information Theory and EECS</cell></row><row><cell>551: Matrix Methods for Signal Process-</cell></row><row><cell>ing, Data Analysis and Machine Learning FA</cell></row><row><cell>2012</cell></row><row><cell>Student Can you describe a little bit about EECS 550?</cell></row><row><cell>Advisor This course contains a lot of concepts about</cell></row><row><cell>source, channel, rate of transformation of in-</cell></row><row><cell>formation, etc.</cell></row><row><cell>Student Sounds interesting. Do you know the class</cell></row><row><cell>size of this course?</cell></row><row><cell>Advisor This is a relatively small class and the average</cell></row><row><cell>size of it is around 12.</cell></row><row><cell>Student I would prefer class with larger class size.</cell></row><row><cell>What is EECS 551 about?</cell></row><row><cell>Advisor This course is about theory and application</cell></row><row><cell>of matrix methods to signal processing, data</cell></row><row><cell>analysis and machine learning</cell></row><row><cell>Student What is the course size of EECS 551?</cell></row><row><cell>Advisor It is around 71</cell></row><row><cell>Student I would take EECS 551. Thanks professor!</cell></row><row><cell>Advisor You are welcome!</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>). Advisors did not know the student's preferences, but did know what classes they Comparison of the diversity of the underlying datasets. Advising is smaller and has longer conversations, but less diversity in utterances. Tokens are based on splitting on whitespace.</figDesc><table><row><cell>Property</cell><cell cols="2">Advising Ubuntu</cell></row><row><cell>Dialogues</cell><cell cols="2">500 135,078</cell></row><row><cell>Utterances / Dialogue</cell><cell>18.6</cell><cell>10.0</cell></row><row><cell>Tokens / Utterance</cell><cell>9.6</cell><cell>9.9</cell></row><row><cell>Utterances / Unique utt.</cell><cell>4.4</cell><cell>1.1</cell></row><row><cell>Tokens / Unique tokens</cell><cell>10.5</cell><cell>22.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results, ordered by the average rank of each team across the subtasks they participated in. The top result in each column is in bold. For these results the metric is the average of MRR and Recall@10.</figDesc><table><row><cell></cell><cell></cell><cell>Recall @</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall @</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall @</cell><cell></cell><cell></cell></row><row><cell>Team</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>MRR</cell><cell>Team</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>MRR</cell><cell>Team</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>MRR</cell></row><row><cell>1</cell><cell cols="4">0.402 0.662 0.916 0.497</cell><cell>1</cell><cell cols="4">0.170 0.482 0.850 0.274</cell><cell>1</cell><cell cols="4">0.078 0.320 0.760 0.158</cell></row><row><cell>2</cell><cell cols="4">0.478 0.765 0.952 0.578</cell><cell>2</cell><cell cols="4">0.242 0.676 0.954 0.384</cell><cell>2</cell><cell cols="4">0.152 0.574 0.930 0.286</cell></row><row><cell>3</cell><cell cols="4">0.645 0.902 0.994 0.735</cell><cell>3</cell><cell cols="4">0.398 0.844 0.986 0.541</cell><cell>3</cell><cell cols="4">0.214 0.630 0.948 0.339</cell></row><row><cell>4</cell><cell cols="4">0.608 0.853 0.984 0.691</cell><cell>4</cell><cell cols="4">0.420 0.768 0.972 0.538</cell><cell>4</cell><cell cols="4">0.194 0.582 0.908 0.320</cell></row><row><cell>5</cell><cell cols="4">0.010 0.101 0.514 0.510</cell><cell>6</cell><cell cols="4">0.206 0.548 0.824 0.322</cell><cell>6</cell><cell cols="4">0.088 0.320 0.728 0.169</cell></row><row><cell>7</cell><cell cols="4">0.309 0.635 0.889 0.414</cell><cell>8</cell><cell cols="4">0.114 0.398 0.782 0.205</cell><cell>8</cell><cell cols="4">0.100 0.420 0.802 0.200</cell></row><row><cell>8</cell><cell cols="4">0.446 0.732 0.937 0.551</cell><cell>10</cell><cell cols="4">0.234 0.600 0.952 0.358</cell><cell>10</cell><cell cols="4">0.116 0.492 0.882 0.230</cell></row><row><cell>9</cell><cell cols="4">0.251 0.601 0.881 0.362</cell><cell>11</cell><cell cols="4">0.000 0.000 0.000 0.000</cell><cell>11</cell><cell cols="4">0.012 0.096 0.512 0.053</cell></row><row><cell>10</cell><cell cols="4">0.469 0.739 0.946 0.564</cell><cell>12</cell><cell cols="4">0.010 0.102 0.490 0.520</cell><cell>12</cell><cell cols="4">0.012 0.096 0.512 0.053</cell></row><row><cell>12</cell><cell cols="4">0.014 0.098 0.504 0.055</cell><cell>13</cell><cell cols="4">0.348 0.804 0.978 0.491</cell><cell>13</cell><cell cols="4">0.170 0.610 0.952 0.306</cell></row><row><cell>13</cell><cell cols="4">0.565 0.810 0.977 0.649</cell><cell>14</cell><cell cols="4">0.064 0.064 0.064 0.064</cell><cell>15</cell><cell cols="4">0.074 0.420 0.834 0.180</cell></row><row><cell>14</cell><cell cols="4">0.008 0.008 0.008 0.008</cell><cell>15</cell><cell cols="4">0.252 0.620 0.894 0.375</cell><cell>16</cell><cell cols="4">0.064 0.398 0.800 0.161</cell></row><row><cell>15</cell><cell cols="4">0.236 0.592 0.858 0.355</cell><cell>16</cell><cell cols="4">0.122 0.474 0.868 0.234</cell><cell>17</cell><cell cols="4">0.180 0.562 0.940 0.307</cell></row><row><cell>16</cell><cell cols="4">0.471 0.700 0.926 0.557</cell><cell>17</cell><cell cols="4">0.494 0.850 0.980 0.608</cell><cell>18</cell><cell cols="4">0.086 0.390 0.836 0.184</cell></row><row><cell>17</cell><cell cols="4">0.475 0.814 0.978 0.595</cell><cell>18</cell><cell cols="4">0.240 0.630 0.906 0.365</cell><cell>19</cell><cell cols="4">0.038 0.250 0.730 0.111</cell></row><row><cell>18</cell><cell cols="4">0.503 0.783 0.962 0.598</cell><cell>19</cell><cell cols="4">0.068 0.322 0.778 0.150</cell><cell>20</cell><cell cols="4">0.000 0.006 0.014 0.001</cell></row><row><cell>19</cell><cell cols="4">0.098 0.346 0.730 0.184</cell><cell>20</cell><cell cols="4">0.000 0.000 0.012 0.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell cols="4">0.001 0.003 0.012 0.200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Subtask 1 results. The left table is for Ubuntu, the middle table is for the initial Advising test set, and the right table is for the final Advising test set. The best results are bolded.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table4provides a comparison of the two versions of the Advising test set. The middle column of tables is for the first test set, which had overlap with the source dialogues from training (the actual utterances are different due to paraphrasing), while the right column is from entirely distinct dialogues. Removing overlap made the task considerably harder, though more realistic. In general, system rankings were not substantially impacted, with the exception of team 17, which did better on the original dataset. This may relate to their use of a memory network over the raw advising data, which may have led the model to match test dialogues with their corresponding training dialogues.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://ibm.github.io/dstc7-noesis/ public/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The entire datasets are now publicly available at https://ibm.github.io/dstc7-noesis/ public/datasets.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Previously,<ref type="bibr" target="#b5">Lowe et al. (2015)</ref> extracted conversations from the same IRC logs, but with a heuristic method.<ref type="bibr" target="#b3">Kummerfeld et al. (2019)</ref> showed that the heuristic was far less effective than a trained statistical model.4  The specific model used in DSTC 7 track 1 is from an earlier version of<ref type="bibr" target="#b3">Kummerfeld et al. (2019)</ref>, as described in the ArXiv preprint and released as the C++ version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that in the DSTC shared tasks participants remain anonymous, and so we refer to them using numbers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This material is based in part upon work supported by IBM under contract 4915012629. Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subtask 2 -Ubuntu Only Recall @ Team 1 10 50 MRR 2 0.016 0.041 0.068 0.024 3 0.067 0.185 0.266 0.106 10 0.196 0.361 0.429 0.253 16 0.000 0.000 0.005 0.000 18 0.000 0.000 0.000 0.000   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end conversation modeling track in DSTC6</title>
		<author>
			<persName><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dialog System Technology Challenges 6</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding task design trade-offs in crowdsourced paraphrase collection</title>
		<author>
			<persName><forename type="first">Youxuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slate: A superlightweight annotation tool for experts</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2019</title>
				<meeting>ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gouravajhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulaka</forename><surname>Peper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName><surname>Athreya</surname></persName>
		</author>
		<title level="m">Siva Sankalp Patel, Lazaros Polymenakos, and Walter S. Lasecki. 2019. A large-scale corpus for conversation disentanglement</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
				<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dialog system technology challenge 6 overview of track 1 -end-to-end goal-oriented dialog learning</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dialog System Technology Challenges 6</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of available corpora for building data-driven dialogue systems: The journal version</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.5087/dad.2018.101</idno>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
