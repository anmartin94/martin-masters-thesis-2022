<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Myroslava</forename><forename type="middle">O</forename><surname>Dzikovska</surname></persName>
							<email>m.dzikovska@ed.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
							<email>rodney.nielsen@unt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
							<email>claudialeacock@mheducation.com</email>
						</author>
						<author>
							<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
							<email>giampiccolo@celct.it</email>
						</author>
						<author>
							<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<email>peterc@vulcan.com</email>
						</author>
						<author>
							<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
						</author>
						<author>
							<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
							<email>hoa.dang@nist.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of North Texas Denton</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">CTB McGraw-Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Vulcan Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated scoring of essays <ref type="bibr" target="#b0">(Attali and Burstein, 2006;</ref><ref type="bibr">Shermis and Burstein, 2013)</ref>, error detection and correction <ref type="bibr" target="#b15">(Leacock et al., 2010)</ref>, and classification of texts by grade level <ref type="bibr" target="#b21">(Petersen and Ostendorf, 2009;</ref><ref type="bibr" target="#b24">Sheehan et al., 2010;</ref><ref type="bibr" target="#b18">Nelson et al., 2012)</ref>. In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions <ref type="bibr" target="#b14">(Leacock and Chodorow, 2003;</ref><ref type="bibr" target="#b23">Pulman and Sukkarieh, 2005;</ref><ref type="bibr" target="#b19">Nielsen et al., 2008a;</ref><ref type="bibr" target="#b17">Mohler et al., 2011)</ref> and in tutorial dialog systems <ref type="bibr" target="#b10">(Graesser et al., 1999;</ref><ref type="bibr" target="#b9">Glass, 2000;</ref><ref type="bibr" target="#b22">Pon-Barry et al., 2004;</ref><ref type="bibr" target="#b13">Jordan et al., 2006;</ref><ref type="bibr" target="#b28">VanLehn et al., 2007;</ref><ref type="bibr" target="#b6">Dzikovska et al., 2010)</ref> deeper semantic processing is likely to be appropriate.</p><p>Since the task of making and testing a full educational dialog system is daunting, <ref type="bibr" target="#b7">Dzikovska et al. (2012)</ref> identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you separate the salt from the water? REF. ANS.</p><p>The water was evaporated, leaving the salt. STUD. ANS. The water dried up and left the salt.</p><p>Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder? REF. ANS.</p><p>The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the brown mineral will have a scratch. STUD. ANS. The harder will leave a scratch on the other.</p><p>Figure <ref type="figure">1</ref>: Example questions and answers help a full dialog system to generate appropriate and effective feedback on errors. System designers typically create a repertoire of questions that the system can ask a student, together with reference answers (see Figure <ref type="figure">1</ref> for an example). For each student answer, the system needs to decide on the appropriate tutorial feedback, either confirming that the answer was correct, or providing additional help to indicate how the answer is flawed and help the student improve. This task requires semantic inference, for example, to detect when the student answers are explaining the same content but in different words, or when they are contradicting the reference answers.</p><p>Recognizing Textual Entailment (RTE) is a series of highly successful challenges used to evaluate tasks related to semantic inference, held annually since 2005. Initial challenges used examples from information retrieval, question answering, machine translation and information extraction tasks <ref type="bibr" target="#b5">(Dagan et al., 2006;</ref><ref type="bibr" target="#b8">Giampiccolo et al., 2008)</ref>. Later challenges started to explore the applicability and impact of RTE technology on specific application settings such as Summarization and Knowledge Base Population <ref type="bibr" target="#b2">(Bentivogli et al., 2009;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2010;</ref><ref type="bibr" target="#b4">Bentivogli et al., 2011)</ref>. The SRA Task offers a similar opportunity.</p><p>We therefore organized a joint challenge at SemEval-2013, aiming to bring together the educational NLP and the semantic inference communities. The goal of the challenge is to compare approaches for student answer assessment and to evaluate the methods typically used in RTE on data from educational applications.</p><p>We present the corpus used in the task (Section 2) and describe the Main task, including educational NLP and textual entailment perspectives and data set creation (Section 3). We discuss evaluation metrics and results in Section 4. Section 5 describes the Pilot task, including data set creation and evaluation results. Section 6 presents conclusions and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Student Response Analysis Corpus</head><p>We used the Student Response Analysis corpus (henceforth SRA corpus) <ref type="bibr" target="#b7">(Dzikovska et al., 2012)</ref> as the basis for our data set creation. The corpus contains manually labeled student responses to explanation and definition questions typically seen in practice exercises, tests, or tutorial dialogue.</p><p>Specifically, given a question, a known correct 'reference answer' and a 1-or 2-sentence 'student answer', each student answer in the corpus is labelled with one of the following judgments:</p><p>• 'Correct', if the student answer is a complete and correct paraphrase of the reference answer;</p><p>• 'Partially correct incomplete', if it is a partially correct answer containing some but not all information from the reference answer;</p><p>• 'Contradictory', if the student answer explicitly contradicts the reference answer;</p><p>• 'Irrelevant' if the student answer is talking about domain content but not providing the necessary information;</p><p>• 'Non domain' if the student utterance does not include domain content, e.g., "I don't know", "what the book says", "you are stupid".</p><p>The SRA corpus consists of two distinct subsets: BEETLE data, based on transcripts of students interacting with BEETLE II tutorial dialogue system <ref type="bibr" target="#b6">(Dzikovska et al., 2010)</ref>, and SCIENTSBANK data, based on the corpus of student answers to assessment questions collected by <ref type="bibr" target="#b20">Nielsen et al. (2008b)</ref>.</p><p>The BEETLE corpus consists of 56 questions in the basic electricity and electronics domain requiring 1-or 2-sentence answers, and approximately 3000 student answers to those questions. The SCI-ENTSBANK corpus contains approximately 10,000 answers to 197 assessment questions in 15 different science domains (after filtering, see Section 3.3) Student answers in the BEETLE corpus were manually labeled by trained human annotators using a scheme that straightforwardly mapped into SRA annotations. The annotations in the SCIENTSBANK corpus were converted into SRA labels from a substantially more fine-grained scheme by first automatically labeling them using a set of questionspecific heuristics and then manually revising them according to the class definitions <ref type="bibr" target="#b7">(Dzikovska et al., 2012)</ref>. We further filtered and transformed the corpus to produce training and test data sets as discussed in the next section.</p><p>3 Main Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Educational NLP perspective</head><p>The 5-way SRA task focuses on associating student answers with categorical labels that can be used in providing tutoring feedback. Most NLP research on short answer scoring reports agreement with a numeric score <ref type="bibr" target="#b14">(Leacock and Chodorow, 2003;</ref><ref type="bibr" target="#b23">Pulman and Sukkarieh, 2005;</ref><ref type="bibr" target="#b17">Mohler et al., 2011)</ref>, which is a potential contrast with our task. However, the majority of the NLP work makes use of underlying representations in terms of concepts, so the 5-way task is still likely to mesh well with the available technology. Research on tutorial dialog has emphasized generic methods that use latent semantic analysis or other machine learning methods to determine when text strings express similar concepts <ref type="bibr" target="#b11">(Hu et al., 2003;</ref><ref type="bibr" target="#b12">Jordan et al., 2004;</ref><ref type="bibr" target="#b28">VanLehn et al., 2007;</ref><ref type="bibr">Mc-Carthy et al., 2008)</ref>. Most of these methods, like the NLP methods, (with the notable exception of <ref type="bibr" target="#b19">(Nielsen et al., 2008a))</ref>, are however strongly dependent on domain expertise for the definitions of the concepts. In educational applications, there would be great value in a system that could operate more or less unchanged across a range of domains and question-types, requiring only a question text and a reference answer supplied by the instructional designers. Thus, the 5-way classification task at Se-mEval was set up to evaluate the feasibility of such answer assessment, either by adapting the existing educational NLP methods to the categorical labeling task or by employing the RTE approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RTE perspective and 2-and 3-way Tasks</head><p>According to the standard definition of Textual Entailment, given two text fragments called Text (T) and Hypothesis (H), it is said that T entails H if, typically, a human reading T would infer that H is most likely true <ref type="bibr" target="#b5">(Dagan et al., 2006)</ref>.</p><p>In a typical answer assessment scenario, we expect that a correct student answer would entail the reference answer, while an incorrect answer would not. However, students often skip details that are mentioned in the question or may be inferred from it, while reference answers often repeat or make explicit information that appears in or is implied from the question, as in Example 2 in Figure <ref type="figure">1</ref>. Hence, a more precise formulation of the task in this context considers the entailing text T as consisting of both the original question and the student answer, while H is the reference answer.</p><p>We carried out a feasibility study to check how well the entailment judgments in this formulation align with the annotated response assessment, by annotating a sample of the data used in the SRA task with entailment judgments. We found that some answers labeled as "correct" implied inferred or assumed pieces of information not present in the text. These reflected the teachers' assessment of student understanding but would not be considered entailed from the traditional RTE perspective. However, we observed that in most such cases, a substantial part of the hypothesis was still implied by the text. Moreover, answers assigned labels other than "correct" were always judged as "not entailed".</p><p>Overall, we concluded that the correlation between assessment judgments of the two types was sufficiently high to consider an RTE approach. The challenge for the textual entailment community was to address the answer assessment task at varying levels of granularity, using textual entailment techniques, and explore how well these techniques can help in this real-world educational setting.</p><p>In order to make the setup more similar to pre-vious RTE tasks, we introduced 3-way and 2-way versions of the task. The data for those tasks were obtained by automatically collapsing the 5-way labels. In the 3-way task, the systems were required to classify the student answer as either (i) correct; (ii) contradictory; or (iii) incorrect (combining the categories partially correct but incomplete, irrelevant and not in the domain from the 5-way classification).</p><p>In the two-way task, the systems were required to classify the student answer as either correct or incorrect (combining the categories contradictory and incorrect from the 3-way classification)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Preparation and Training Data</head><p>In preparation of the task four of the organizers examined all questions in the SRA corpus, and decided that to remove some of the questions to make the dataset more uniform.</p><p>We observed two main issues. First, a number of questions relied on external material, e.g., charts and graphs. In some cases, the information in the reference answer was sufficient to make a reasonable assessment of student answer correctness, but in other cases the information contained in the questions was deemed insufficient and the questions were removed.</p><p>Second, some questions in the SCIENTSBANK dataset could have multiple possible correct answers, e.g., a question asking for any example out of two or more unrelated possibilities. Such questions were also removed as they do not align well with the RTE perspective.</p><p>Finally, parts of the data were re-checked for reliability. In BEETLE data, a second manual annotation pass was carried out on a subset of questions to check for consistency. In SCIENTSBANK, we manually re-checked the test data. The automatic conversion from the original SCIENTSBANK annotations into SRA labels was not perfectly accurate <ref type="bibr" target="#b7">(Dzikovska et al., 2012)</ref>. We did not have the resources to check the entire data set. However, four of the organizers jointly hand-checked approximately 100 examples to establish consensus, and then one organizer hand-checked all of the test data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test Data</head><p>We followed the evaluation methodology of <ref type="bibr" target="#b19">Nielsen et al. (2008a)</ref> for creating the test data. Since our goal is to support systems that generalize across problems and domains (see Section 3.1), we created three distinct test sets:</p><p>1. Unseen answers (UA): a held-out set to assess system performance on the answers to questions contained in the training set (for which the system has seen example student answers).</p><p>It was created by setting aside a subset if randomly selected learner answers to each question included in the training data set.</p><p>2. Unseen questions (UQ): a test set to assess system performance on responses to previously unseen questions but which still fall within the application domains represented in the training data. It was created by holding back all student answers to a subset of randomly selected questions in each dataset.</p><p>3. Unseen domains (UD): a domain-independent test set of responses to topics not seen in the training data, available only in the SCIENTS-BANK dataset. It was created by setting aside the complete set of questions and answers from three science modules from the fifteen modules in the SCIENTSBANK data.</p><p>The final label distribution for train and test data is shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Task Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>The participants were invited to submit up to three runs in any combination of the tasks. Nine teams participated in the main task, most choosing to attempt all subtasks (5-way, 3-way and 2-way), with 1 team entering only the 5-way and 1 team entering only the 2-way task.</p><p>At least 6 (CNGL, CoMeT, CU, BIU, EHUALM, LIMSI) of the 9 systems used some form of syntactic processing, in most cases going beyond parts of speech to dependencies or constituency structure. CNGL emphasized this as an important aspect of the system. At least 5 (CoMeT, CU, EHUALM, ETS UKP) of the 9 systems used a system combination approach, with several components feeding into a final decision made by some form of stacked classifier. The majority of the systems used some kind  of measure of text-to-text similarity, whether the inspiration was LSA, MT measures such as BLEU or in-house methods. These methods were emphasized as especially important by Celi, ETS and SOFTCARDINALITY. These impressions are based on short summaries sent to us by the participants prior to the availability of the full system descriptions. Check the individual system papers for detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>For each evaluation data set (test set), we computed the per-class precision, recall and F 1 score. We also computed three main summary metrics: accuracy, macro-average F 1 and weighted average F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy is the overall percentage of correctly classified examples.</head><p>Macroaverage is the average value of each metric (precision, recall, F 1 ) across classes, without taking class size into account. It is defined as 1/N c c metric(c), where N c is the number of classes (2, 3, or 5 depending on the task). Note that in the 5-way SCIENTSBANK dataset the 'nondomain' class is severely underrepresented, with only 23 examples out of 4335 total (see Table <ref type="table" target="#tab_1">1</ref>). Therefore, we calculated macro-averaged P/R/F 1 over only 4 classes (i.e. excluding the 'non-domain' class) for SCIENTSBANK 5-way data.</p><p>Weighted Average (or simply weighted) is the average value for each metric weighted by class size, defined as 1/N c |c| * metric(c) where N is the total number of test items and |c| is the number of items labeled as c in gold-standard data. <ref type="bibr">1</ref> In general, macro-averaging favors systems that perform well across all classes regardless of class size. Accuracy and weighted average prefer systems that perform best on the largest number of examples, favoring higher performance on the most frequent classes. In practice, only a small number of the systems were ranked differently by the different metrics. We discuss this further in Section 4.7. Results for all metrics are available online, and this paper focuses on two metrics for brevity: weighted and macro-average F 1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The evaluation results for all metrics and all participant runs are provided online. <ref type="bibr">2</ref> The tables in this paper present the F 1 scores for the best system runs. Results are shown separately for each test set (TS), with the simple mean over the five TSs reported in the final column.</p><p>We used two baselines: the majority (most frequent) class baseline and a lexical overlap baseline described in detail in <ref type="bibr" target="#b7">(Dzikovska et al., 2012)</ref>. The performance of the baselines is presented jointly with system scores in the results tables.</p><p>For each participant, we report the single run with the best average TS performance, identified by the subscript in the run title, with the exception of ETS. With all other participants, there was almost always one run that performed best for a given metric on all the TSs. In the small number of cases where another run performed best on a given TS, we instead report that value and indicate its run with a subscript (these changes never resulted in meaningful changes in the performance rankings). ETS, on the other hand, subusing the multi-label metric are all equal and mathematically equivalent to accuracy.  mitted results for systems that were substantially different from one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs. <ref type="bibr">3</ref> In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test <ref type="bibr" target="#b29">(Yeh, 2000)</ref> with 10,000 iterations; p ≤ 0.05 was considered statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Five-way Task</head><p>The results for the five-way task are shown in Tables <ref type="table" target="#tab_3">2 and 3</ref>.</p><p>Comparison to baselines All of the systems performed substantially better than the majority class baseline ("correct" for both BEETLE and SCIENTS-BANK), on average exceeding it on the TS mean by 0.21 on the weighted F 1 and 0.24 on the macroaverage F 1 . Six systems outperformed the lexical baseline on the mean TS results for the weighted F 1 and five for the macro-average F 1 . Nearly all of the top results on a given TS (shown in bold in the tables) were statistically better than corresponding lexical baselines according to significance tests  The top performers on UA test sets were CoMeT 1 and ETS 2 , with the addition of UKP-BIU 1 on SUA. However, there was not a single best performer on UQ and UD sets. ETS 2 performed statistically better than all other systems on BEETLE UQ (BUQ), but it performed statistically worse than the lexical baseline on SCIENTSBANK UQ (SUQ), resulting in no overlap in the top performing systems on the two UQ test sets. SoftCardinality 1 performed statistically better than all other systems on SUD and was among the three or four top performers on SUQ, but was not a top performer on the other three TSs, generally not performing statistically better than the lexical baseline on the BEETLE TSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group performance</head><p>The two UA TSs had more systems that performed statistically better than the lexical baseline (generally six systems) than did the UQ TSs where on average only two systems performed statistically better than the lexical baseline. Over twice as many systems outperformed the lexical baseline on UD as on the UQ TSs. The top performing systems according to the macro-average F 1 were nearly identical to the top performing systems according to the weighted F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Three-way Task</head><p>The results for the three-way task are shown in Tables 4 and 5.</p><p>Comparison to baselines All of the systems performed substantially better than the majority baseline ("correct" for BEETLE and "incorrect" for SCI-ENTSBANK), on average exceeding it on the TS mean by 0.28 on the weighted F 1 and 0.31 on the macro-average F 1 . Five of the eight systems outperformed the lexical baseline on the mean TS results for the weighted F 1 and five on the macroaverage F 1 , and all top systems outperformed the lexical baseline with statistical significance.</p><p>Comparing UA and UQ/UD performance The top performers on both BUA and SUA were CoMeT 1 and ETS 2 . As for the 5-way task there was no single best performer for UQ and UD sets, and no overlap in top performing systems on BUQ and SUQ test sets, with ETS 2 being the top performer on BUQ, but statistically worse than the baseline on SUQ and SUD. On the weighted F 1 , SoftCardinality 1 performed statistically better than all other systems on SUD and was among the two statistically best systems on SUQ, but was not a top performer on BUQ or BUA/SUA TSs. On the macro-average F 1 , UKP-BIU 1 became one of the statistically best performers on all SCIENTSBANK TSs but, along with SoftCardinality 1 , never performed statistically better than the lexical baseline on the BEETLE TSs.</p><p>Group performance With the exception of SUA, only around two systems performed statistically better than the lexical baseline on each TS. The top performing systems were nearly the same according to the weighted F 1 and the macro-average F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Two-way Task</head><p>The results for the two-way task are shown in Table 6. Because the labels are roughly balanced in the two-way task, the results on the weighted and macro-average F 1 are very similar and the top performing systems are identical. Hence this section will focus only on the macro-average F 1 .</p><p>As in the previous tasks, all of the systems performed substantially better than the majority baseline ("incorrect" for all sets), on average exceeding it on the TS mean by 0.25 on the weighted F 1 and 0.30 on the macro-average F 1 . However, just four of   the nine systems in the two-way task outperformed the lexical baseline on the mean TS results. In fact, the average performance fell below the lexical baseline. The differences in the macro-average F 1 between the top results on a SCIENTSBANK TS and the corresponding lexical baselines were all statistically significant. Two of the top results on BUA were not statistically better than the lexical baseline, and all systems performed below the baseline on BUQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Discussion</head><p>All of the systems consistently outperformed the most frequent class baseline. Beating the lexical overlap baseline proved to be more challenging, being achieved by just over half of the results with about half of those being statistically significant improvements. This underscores the fact that there is still a considerable opportunity to improve student  The set of top performing systems on the weighted F 1 for a given TS were also always in the top on the macro-average F 1 , but a small number of additional systems joined the top performing set on the macro-average F 1 . Specifically, one, three, and two results joined the top set in the five-way, threeway, and two-way tasks, respectively. In principle, the metrics could differ substantially, because of the treatment of minority classes, but in practice they rarely did. Only one pair of participants swap adjacent TS mean rankings on the macro-average F 1 relative to the weighted F 1 on the two-way task. On the five-way task, two pairs swap rankings and another participant moved up two positions in the ranking, ending at the median value.</p><p>Most (28/34) rank changes were only one position and most (21/34) were in positions at or below the median ranking. In the five-way task, a pair of systems, UKP-BIU 1 and ETS 1 , had a meaningful performance rank swap on the macro-average F 1 relative to the weighted F 1 on the UD test set. Specifically, UKP-BIU 1 moved up four positions from rank 6, where it was not statistically better than the lexical baseline, to the second best performance.</p><p>Not surprisingly, performance on UA was substantially higher than on UQ and UD, since the UA is the only set which contains questions with example answers in training data. Performance on BUA was usually better than performance on SUA, most likely because BUA contains more similar questions and answers, focusing on a single science area, Elec-tricity and Magnetism, compared to 12 distinct science topics in SUA). In addition, the BEETLE study participants may have used simpler language, since they were aware that they were talking to a computer system instead of writing down answers for human teachers to assess as in SCIENTSBANK.</p><p>Performance on BUQ versus SUQ was much more varied, presumably since there was no direct training data for either TS. For the five-way task, the best performance on the weighted F 1 measure for BUQ is 0.09 below the best result for BUA and the analogous decrease from SUA to SUQ is 0.13, with an additional 0.02 drop on SUD. On the two-way task, the best weighted F 1 for BUQ drops 0.11 from the best BUA value, but the decrease from SUA to SUQ is just 0.03, with another 0.03 drop to SUD. While the drop in performance is fairly similar from BUA to BUQ on all tasks and either metric, the decrease from SUA to SUQ seems to potentially be dependent on the task, ranging from 0.13 on the fiveway task to 0.08 on the three-way task and 0.03 on the two-way task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Pilot Task on Partial Entailment</head><p>The SCIENTSBANK corpus was originally developed to assess student answers at a very fine-grained level and contains additional annotations that break down the answers into "facets", or low-level concepts and relationships connecting them (henceforth, SCIENTSBANK Extra). This annotation aims to support educational systems in recognizing when specific parts of a reference answer are expressed in the student answer, even if the reference answer is not entailed as a whole <ref type="bibr" target="#b20">(Nielsen et al., 2008b)</ref>. The task of recognizing such partial entailment relationships may also have various uses in applications such as summarization or question answering, but it has not been explored in previous RTE challenges.</p><p>Therefore, we proposed a pilot task on partial entailment, in which systems are required to recognize whether the semantic relation between specific parts of the Hypothesis is expressed by the Text, directly or by implication, even though entailment might not be recognized for the Hypothesis as a whole, based on the SCIENTSBANK facet annotation.</p><p>Each reference answer in SCIENTSBANK data is broken down into facets, where a facet is a triplet consisting of two key terms (both single words and multi-words, e.g. carbon dioxide, each other, burns out) and a relation linking them, as shown in Figure <ref type="figure">2</ref>. The student answers were then annotated with regards to each reference answer facet in order to indicate whether the facet was (i) expressed, either explicitly or by assumption or easy inference; (ii) contradicted; or (iii) left unaddressed. Considering the SCIENTSBANK reference answers as Hypotheses, the facets capture their atomic components, and facet annotations may correspond to the judgments on the sub-parts of the H which are entailed by T.</p><p>We carried out a feasibility study to explore this idea and to verify how well the facet annotations align with traditional entailment judgments. We focused on the reference answer facets labeled in the gold standard annotation as Expressed or Unaddressed. The working hypothesis was that Expressed labels assigned in SCIENTSBANK annotations corresponded to Entailed judgments in traditional textual entailment annotations, while Unaddressed labels corresponded to No-entailment judgments.</p><p>Similarly to the feasibility study reported in Section 3.2, we concluded that the correspondence between educational labels and entailment judgments was not perfect due to the difference in educational and textual entailment perspectives. Nevertheless, the two classes of assessment appeared to be sufficiently well correlated so as to offer a good testbed for partial entailment in a natural setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Definition</head><p>Given (i) a text T, made up of a Question and a Student Answer; (ii) a hypothesis H, i.e. the Reference Answer for that question and (iii) a facet, i.e. a pair of key terms in H, the task consists of determining whether T expresses, either directly or by implication, the same relationship between the facet words as in H. In other words, for each of H's facets the system assign one of the following judgments: Expressed, if the Student Answer expresses the same relationship between the meaning of the facet terms as in H; Unaddressed, if it does not.</p><p>Consider the example shown in Figure <ref type="figure">2</ref>. For facet 3, the system must decide whether the same relation between the two terms 'contains' and 'seeds' in H (the reference answer) is expressed, explicitly or implicitly, in T (the combination of question and student response). If the student answer is 'The part of a plant you are observing is a fruit if it has seeds.', the answer to the question is 'yes' and the correct judgment is 'Expressed'. But if the student says 'My rule is has to be sweet.', T does not express the same semantic relationship between 'contains' and 'seeds' exhibited in H, thus the correct judgment is 'Unaddressed'. Note that even though this is an exercise in textual entailment, student response assessment labels were used instead of traditional entailment judgments, due to the partial mismatch between the two assessment classes found in the feasibility study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset</head><p>We used a subset of the SCIENTSBANK Extra corpus <ref type="bibr" target="#b20">(Nielsen et al., 2008b)</ref> with the same problematic questions filtered out as the main task (see Section 3.3). We further filtered out all the student answer facets which were labeled other than 'Expressed' or 'Unaddressed' in the gold standard annotation; the facets in which the relationship between the two key terms, as classified in the manual annotation, proved to be problematic to define and judge, namely Topic, Agent, Root, Cause, Quantifier, Neg; and inter-propositional facets, i.e. facets that expressed relations between higher-level propositions. Finally, the facet relations were removed from the dataset, leaving the relationship between the two facet terms unspecified so as to allow a more fuzzy approach to the inference problem posed by the exercise.</p><p>We used the same training/test split as reported in Section 3.4. The training set created from the Training SCIENTSBANK Extra corpus contains 13,145 reference answer facets, 5,939 of which were labeled as 'Expressed' in the student answers and 7,206 as 'Unaddressed'. The Test set was created from the SCIENTSBANK Extra unseen data and is divided into the same subsets as the main task (Unseen Answers, Unseen Questions and Unseen Domains). It contains 16,263 facets total, with 5,945 instances labeled as 'Expressed', and 10,318 labeled as 'Unaddressed'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics and Baselines</head><p>The metrics used in the Pilot task were the same as in the Main task, i.e. Overall Accuracy, Macroaverage  .</p><p>and Weighted Average Precision, Recall and F 1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as 'Unaddressed'. Its performance is presented in Section 5.4 jointly with the system results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Participants and results</head><p>Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures <ref type="bibr" target="#b1">(Bär et al., 2012)</ref>, and (ii) recognizing textual entailment with BIUTEE <ref type="bibr" target="#b26">(Stern and Dagan, 2011)</ref>. The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with SCIENTSBANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table <ref type="table" target="#tab_12">7</ref> shows Weighted Average and Macro Average F 1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performance was observed on Run 2, with the highest results on the Unseen Domains test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>The Joint Student Response Analysis and 8th Recognizing Textual Entailment challenge has proven to be a useful, interdisciplinary task using a realistic dataset from the educational domain. In almost all cases the best systems significantly outperformed the lexical overlap baseline, sometimes by a large margin, showing that computational linguistics approaches can contribute to educational tasks. However, the lexical baseline was not trivial to beat, particularly in the 2-way task. These results are consistent with similar findings in previous RTE exercises. Moreover, there is still significant room for improvement in the absolute scores, reflecting the interesting challenges that both educational data and RTE tasks present to computational linguistics.</p><p>The educational setting places new stresses on semantic inference technology because the educational notion of 'Expressed' and the RTE notion of 'Entailed' are slightly different. This raises the educational question of whether RTE can work in this setting, and the RTE question of whether this setting is meaningful for evaluating RTE system performance. The experimental results suggests that the answer to both questions is 'yes', a significant finding for both educators and RTE technologists going forward.</p><p>The Pilot task, aimed at exploring notions of partial entailment, so far not explored in the series of RTE challenges, has proven to be an interesting, though challenging exercise. The novelty of the task, namely performing textual entailment not on a pair of full texts, but between a text and a hypothesis consisting of a pair of words, may have represented a more complex task than expected for some textual entailment engines. Despite this, the encouraging results obtained by the team which carried out the exercise has shown that this partial entailment task is worthy of further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.386 0.372 0.389 0.367 0.387 CNGL2 0.547 0.469 0.266 0.297 0.294 0.375 CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454 EHUALM2 0.566 0.4163 0.525 3 0.446 0.437 0.471 ETS1 0.552 0.547 0.535 0.487 0.447 0.</figDesc><table><row><cell cols="3">Dataset: BEETLE</cell><cell cols="3">SCIENTSBANK</cell></row><row><cell>Run</cell><cell>UA</cell><cell>UQ</cell><cell>UA</cell><cell>UQ</cell><cell>UD Mean</cell></row><row><cell>CELI1</cell><cell cols="5">0.423 0514</cell></row><row><cell>ETS2</cell><cell cols="5">0.705 0.614 0.625 0.356 0.434 0.547</cell></row><row><cell>LIMSIILES1</cell><cell cols="5">0.505 0.424 0.419 0.456 0.422 0.445</cell></row><row><cell cols="6">SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502</cell></row><row><cell>UKP-BIU1</cell><cell cols="5">0.448 0.269 0.590 0.3972 0.407 0.418</cell></row><row><cell>Median</cell><cell cols="5">0.552 0.445 0.535 0.397 0.422 0.454</cell></row><row><cell>Baselines:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lexical</cell><cell cols="5">0.483 0.463 0.435 0.402 0.396 0.436</cell></row><row><cell>Majority</cell><cell cols="5">0.229 0.248 0.260 0.239 0.249 0.245</cell></row></table><note>2 http://bit.ly/11a7QpP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Five-way task weighted-average F 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Five-way task macro-average F 1 (indicated by italics in the tables).</figDesc><table><row><cell>Comparing UA and UQ/UD performance The</cell></row><row><cell>BEETLE UA (BUA) and SCIENTSBANK UA (SUA)</cell></row><row><cell>test sets represent questions with example answers</cell></row><row><cell>in training data, while the UQ and UD test sets repre-</cell></row><row><cell>sent transfer performance to new questions and new</cell></row><row><cell>domains respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Three-way task weighted-average F 1</figDesc><table><row><cell cols="3">Dataset: BEETLE</cell><cell cols="3">SCIENTSBANK</cell></row><row><cell>Run</cell><cell>UA</cell><cell>UQ</cell><cell>UA</cell><cell>UQ</cell><cell>UD Mean</cell></row><row><cell>CELI1</cell><cell cols="5">0.494 0.441 0.373 0.412 0.415 0.427</cell></row><row><cell>CNGL2</cell><cell cols="5">0.567 0.450 0.330 0.308 0.311 0.393</cell></row><row><cell>CoMeT1</cell><cell cols="5">0.715 0.466 0.640 0.380 0.404 0.521</cell></row><row><cell>ETS1</cell><cell cols="5">0.592 0.521 0.477 0.459 0.439 0.498</cell></row><row><cell>ETS2</cell><cell cols="5">0.710 0.585 0.643 0.389 0.367 0.539</cell></row><row><cell>LIMSIILES1</cell><cell cols="5">0.563 0.431 0.404 0.409 0.429 0.447</cell></row><row><cell cols="6">SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509</cell></row><row><cell>UKP-BIU1</cell><cell cols="5">0.468 0.333 0.620 0.458 0.487 0.473</cell></row><row><cell>Median</cell><cell cols="5">0.580 0.446 0.516 0.411 0.422 0.485</cell></row><row><cell>Baselines:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lexical</cell><cell cols="5">0.552 0.477 0.405 0.390 0.416 0.448</cell></row><row><cell>Majority</cell><cell cols="5">0.191 0.197 0.201 0.194 0.197 0.196</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Three-way task macro-average F 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Two-way task macro-average F 1 response assessment systems.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>What is your "rule" for deciding if the part of a plant you are observing is a fruit? REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.FACET 1: Relation NMod of Term1 part Term2 plant FACET 2: Relation Theme Term1 contains Term2 part FACET 3: Relation Material Term1 contains Term2 seeds FACET 4: Relation Be Term1 fruit Term2 part Figure 2: Example of facet annotations supporting the partial entailment task</figDesc><table><row><cell>Run</cell><cell>UA</cell><cell>UQ</cell><cell>UD</cell><cell>UA</cell><cell>UQ</cell><cell>UD</cell></row><row><cell></cell><cell cols="3">Weighted Averaged</cell><cell></cell><cell>Macro Average</cell><cell></cell></row><row><cell>Run1</cell><cell>0.756</cell><cell>0.71</cell><cell>0.76</cell><cell cols="2">0.7370 0.686</cell><cell>0.755</cell></row><row><cell>Run 2</cell><cell>0.782</cell><cell>0.765</cell><cell>0.816</cell><cell>0.753</cell><cell>0.73</cell><cell>0.804</cell></row><row><cell>Run 3</cell><cell>0.744</cell><cell>0.733</cell><cell>0.77</cell><cell>0.719</cell><cell>0.7050</cell><cell>0.761</cell></row><row><cell cols="2">Baseline 0.54</cell><cell>0.547</cell><cell>0.478</cell><cell>0.402</cell><cell>0.404</cell><cell>0.384</cell></row></table><note>QUESTION:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Weighted-average and macro-average F 1 scores</cell></row><row><cell>(UA: Unseen Answers; UQ: Unseen Questions; UD Un-</cell></row><row><cell>seen Domains)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This metric is called microaverage in<ref type="bibr" target="#b7">(Dzikovska et al., 2012)</ref>. However, microaverage is used to define a different metric in tasks where more than one label can be associated with each data item(Tsoumakas et al., 2010). therefore, we use weighted average to match the terminology used by the Weka toolkit. The micro-average precision, recall and F1 computed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In a small number of cases, ETS's third run performed marginally better, see full results online.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research reported here was supported by the US ONR award N000141010085 and by the Institute of Education Sciences, U.S. Department of Education, through Grant R305A120808 to the University of North Texas. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. The RTErelated activities were partially supported by the Pascal-2 Network of Excellence, ICT-216886-NOE. We would also like to acknowledge the contribution of Alessandro Marchetti and Giovanni Moretti from CELCT to the organization of the challenge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater v.2. The Journal of Technology</title>
		<author>
			<persName><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ukp: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fifth PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernando</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Analysis Conference</title>
				<meeting>Text Analysis Conference</meeting>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thesixth PAS-CAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notebook papers and results</title>
				<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Text Analysis Conference</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The seventh PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notebook papers and results</title>
				<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Text Analysis Conference</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3944</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beetle II: a system for tutoring and computational linguistics experimentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Myroslava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwendolyn</forename><surname>Steinhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaine</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">B</forename><surname>Farrow</surname></persName>
		</author>
		<author>
			<persName><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2010 System Demonstrations</title>
				<meeting>of ACL 2010 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards effective tutorial feedback for explanation questions: A dataset and baselines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Myroslava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 2012 Conference of NAACL: Human Language Technologies</title>
				<meeting>of 2012 Conference of NAACL: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The fourth PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Analysis Conference</title>
				<meeting>Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Processing language input in the CIRCSIM-Tutor intelligent tutoring system</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<idno>FS-00-01</idno>
	</analytic>
	<monogr>
		<title level="m">Papers from the 2000 AAAI Fall Symposium, Available as AAAI technical report</title>
				<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autotutor: A simulation of a human tutor</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wiemer-Hastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wiemer-Hastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kreuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="35" to="51" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Andrew Olney, Phanni Penumatsa, and Art Graesser</title>
		<author>
			<persName><forename type="first">Xiangen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Louwerse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Artificial intelligence (IJCAI&apos;03)</title>
				<meeting>the 18th International Joint Conference on Artificial intelligence (IJCAI&apos;03)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1489" to="1491" />
		</imprint>
	</monogr>
	<note>A revised algorithm for latent semantic analysis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining competing language understanding approaches in an intelligent tutoring system</title>
		<author>
			<persName><forename type="first">Pamela</forename><forename type="middle">W</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Makatchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Van-Lehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intelligent Tutoring Systems Conference</title>
				<meeting>of Intelligent Tutoring Systems Conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="346" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A natural language tutorial dialogue system for physics</title>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Makatchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umarani</forename><surname>Pappuswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Vanlehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Albacete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 19th Intl. FLAIRS conference</title>
				<meeting>of 19th Intl. FLAIRS conference</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">C-rater: Automated scoring of short-answer questions</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and the Humanities</title>
				<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="389" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessing forward-, reverse-, and average-entailment indices on natural language input from the intelligent tutoring system, iSTART</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 21st Intl. FLAIRS conference</title>
				<meeting>of 21st Intl. FLAIRS conference</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="165" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to grade short answer questions using semantic similarity measures and dependency graph alignments</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="752" to="762" />
		</imprint>
	</monogr>
	<note>Portland. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measures of text difficulty: Testing their predictive value for grade levels and student performance</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Perfetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><surname>Liben</surname></persName>
		</author>
		<ptr target="http://www.ccsso.org/Documents/2012/Measures%20ofText%20Difficulty_fina%l.2012.pdf" />
	</analytic>
	<monogr>
		<title level="m">Student Achievement Partners</title>
				<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to assess low-level conceptual understanding</title>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 21st Intl. FLAIRS Conference</title>
				<meeting>of 21st Intl. FLAIRS Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="427" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotating students&apos; understanding of science concepts</title>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Language Resources and Evaluation Conference</title>
				<meeting>the Sixth International Language Resources and Evaluation Conference<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A machine learning approach to reading level assessment</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="106" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Advantages of spoken language interaction in dialogue-based intelligent tutoring systems</title>
		<author>
			<persName><forename type="first">Heather</forename><surname>Pon-Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brady</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">Owen</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ITS-2004 Conference</title>
				<meeting>of ITS-2004 Conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic short answer marking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><forename type="middle">Z</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName><surname>Sukkarieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Building Educational Applications Using NLP</title>
				<meeting>the Second Workshop on Building Educational Applications Using NLP<address><addrLine>Ann Arbor, Michigan, June</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generating automated text complexity classifications that are aligned with targeted text complexity standards</title>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Kostin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Futagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
		<idno>RR-10- 28</idno>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Educational Testing Service</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Handbook on Automated Essay Evaluation: Current Applications and New Directions</title>
		<editor>Mark D. Shermis and Jill Burstein</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A confidence model for syntactically-motivated entailment proofs</title>
		<author>
			<persName><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
				<meeting><address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
	<note>RANLP 2011</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
	</analytic>
	<monogr>
		<title level="m">Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas</title>
				<editor>
			<persName><forename type="first">Oded</forename><surname>Maimon</surname></persName>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</editor>
		<meeting><address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
	<note>Data Mining and Knowledge Discovery Handbook</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Vanlehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLaTE Workshop on Speech and Language Technology in Education</title>
				<meeting>of SLaTE Workshop on Speech and Language Technology in Education<address><addrLine>Farmington, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Computational linguistics (COLING 2000)</title>
				<meeting>the 18th International Conference on Computational linguistics (COLING 2000)<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
