<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peter</forename><surname>Potash</surname></persName>
							<email>ppotash@cs.uml.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
							<email>aromanov@cs.uml.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead. The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the 'Hashtag Wars' segment of the TV show @midnight. The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are. 7 teams participated in subtask A, and 5 teams participated in subtask B. The best accuracy in subtask A was 0.675. The best (lowest) rank edit distance for subtask B was 0.872.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most work on humor detection approaches the problem as binary classification: humor or not humor. While this is a reasonable initial step, in practice humor is continuous, so we believe it is interesting to evaluate different degrees of humor, particularly as it relates to a given person's sense of humor. To further such research, we propose a dataset based on humorous responses submitted to a Comedy Central TV show, allowing for computational approaches to comparative humor ranking.</p><p>Debuting in Fall 2013, the Comedy Central show @midnight 1 is a late-night "game-show" that presents a modern outlook on current events by focusing on content from social media. The show's contestants (generally professional comedians or actors) are awarded points based on how funny their answers are. The segment of the show that best illustrates this attitude is the Hashtag Wars (HW). Every episode the show's host proposes a topic in the form of a hashtag, and the show's contestants must provide tweets that would have this hashtag. Viewers are encouraged to tweet their own responses. From the viewers' tweets, we are able to apply labels that determine how relatively humorous the show finds a given tweet.</p><p>Because of the contest's format, it provides an adequate method for addressing the selection bias <ref type="bibr" target="#b6">(Heckman, 1979)</ref> often present in machine learning techniques <ref type="bibr" target="#b19">(Zadrozny, 2004)</ref>. Since each tweet is intended for the same hashtag, each tweet is effectively drawn from the same sample distribution. Consequently, tweets are seen not as humor/nonhumor, but rather varying degrees of wit and cleverness. Moreover, given the subjective nature of humor, labels in the dataset are only "gold" with respect to the show's sense of humor. This concept becomes more grounded when considering the use of supervised systems for the dataset.</p><p>The idea of the dataset is to learn to characterize the sense of humor represented in this show. Given a set of hashtags, the goal is to predict which tweets the show will find funnier within each hashtag. The degree of humor in a given tweet is determined by the labels provided by the show. We propose two subtasks to evaluate systems on the dataset. The first subtask is pairwise comparison: given two tweets, select the funnier tweet, and the pairs will be derived from the labels assigned by the show to individual tweets. The second subtask is to rank the the tweets based on the comparative labels provided by the show. This is a semiranking task because most labels are applied to more than one tweet. Seen as a classification task, the labels are comparative, because there is a notion of distance. We introduce a new edit distance-inspired metric for this subtask.</p><p>A number of different computational approaches to humor have been proposed within the last decade <ref type="bibr" target="#b18">(Yang et al., 2015;</ref><ref type="bibr" target="#b10">Mihalcea and Strapparava, 2005;</ref><ref type="bibr" target="#b20">Zhang and Liu, 2014;</ref><ref type="bibr" target="#b13">Radev et al., 2015;</ref><ref type="bibr" target="#b14">Raz, 2012;</ref><ref type="bibr" target="#b15">Reyes et al., 2013;</ref><ref type="bibr" target="#b0">Barbieri and Saggion, 2014;</ref><ref type="bibr" target="#b16">Shahaf et al., 2015;</ref><ref type="bibr" target="#b12">Purandare and Litman, 2006;</ref><ref type="bibr" target="#b7">Kiddon and Brun, 2011)</ref>. In particular, <ref type="bibr" target="#b20">Zhang and Liu (2014)</ref>; <ref type="bibr" target="#b14">Raz (2012)</ref>; <ref type="bibr" target="#b15">Reyes et al. (2013)</ref>; <ref type="bibr" target="#b0">Barbieri and Saggion (2014)</ref> focus on recognizing humor in Twitter. However, the majority of this work focuses on distinguishing humor from non-humor.</p><p>This representation has two shortcomings: (1) it ignores the continuous nature of humor, and (2) it does not take into account the subjectivity in humor perception. Regarding the first issue, we believe that shifting away from the binary approach to humor detection as done in the present task is a good pathway towards advancing this work. Regarding the second issue, consider a humour annotation task done by <ref type="bibr" target="#b16">Shahaf et al. (2015)</ref>, in which the annotators looked at pairs of captions from the New Yorker Caption Content 2 , <ref type="bibr" target="#b16">Shahaf et al. (2015)</ref> report that "Only 35% of the unique pairs that were ranked by at least five people achieved 80% agreement..." In contrast, the goal of the present task is to not to identify humour that is universal, but rather, to capture the specific sense of humour represented in the show.</p><p>2 Related Work <ref type="bibr" target="#b10">Mihalcea and Strapparava (2005)</ref> developed a humor dataset of puns and humorous one-liners intended for supervised learning. In order to generate negative examples for their experimental design, the authors used news titles from Reuters and the British National Corpus, as well as proverbs. Recently, <ref type="bibr" target="#b18">Yang et al. (2015)</ref> used the same dataset for experimental purposes, taking text from AP News, New York Times, Yahoo! Answers, and proverbs as their negative examples. To further reduce the bias of their negative examples, the authors selected negative examples with a vocabulary that is in the dictionary created from the positive examples. Also, the authors forced the negative examples to have a similar text length compared to the positive examples. <ref type="bibr" target="#b20">Zhang and Liu (2014)</ref> constructed a dataset for recognizing humor in Twitter in two parts. First, the authors use the Twitter API with targeted user mentions and hashtags to produce a set of 1,500 humorous tweets. After manual inspections, 1,267 of the original 1,500 tweets were found to be humorous, of which 1,000 were randomly sampled as positive examples in the final dataset. Second, the authors collect negative examples by extracting 1,500 tweets from the Twitter Streaming API, manually checking for the presence of humor. Next, the authors combine these tweets with tweets from part one that were found to actually not contain humor. The authors argue this last step will partly assuage the selection bias of the negative examples.</p><p>In <ref type="bibr" target="#b15">Reyes et al. (2013)</ref> the authors create a model to detect ironic tweets. To construct their dataset they collect tweets with the following hashtags: irony, humor, politics, and education. Therefore, a tweet is considered ironic solely because of the presence of the appropriate hashtag. <ref type="bibr" target="#b0">Barbieri and Saggion (2014)</ref> also use this dataset for their work.</p><p>Finally, recently researchers have developed a dataset similar to our HW dataset based on the New Yorker Caption Contest (NYCC) <ref type="bibr" target="#b13">(Radev et al., 2015;</ref><ref type="bibr" target="#b16">Shahaf et al., 2015)</ref>. Whereas for the HW segment, viewers submit a tweet in response to a hashtag, for the NYCC readers submit humorous captions in response to a cartoon. It is important to note this key distinction between the two datasets, because we believe that the presence of the hashtag allows for further innovative NLP methodologies aside from solely analyzing the tweets themselves. In <ref type="bibr" target="#b13">Radev et al. (2015)</ref>, the authors developed more than 15 unsupervised methods for ranking submissions for the NYCC. The methods can be categorized into broader categories such as originality and content-based.</p><p>Alternatively, <ref type="bibr" target="#b16">Shahaf et al.(2015)</ref> approach the NYCC dataset with supervised models, evaluating on a pairwise comparison task, upon which we base our evaluation methodology. The features to represent a given caption fall in the general areas of Unusual Language, Sentiment, and Taking Expert Advice. For a single data point (which represents two captions), the authors concatenate the features of each individual caption, as well as encoding the difference between each caption's vector. The authors' best-performing system records a 69% accuracy on the pairwise evaluation task. Note that for this evaluation task, random baseline is 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">#HashtagWars Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collection</head><p>The following section describes our data collection process. First, when a new episode airs (which generally happens four nights a week), a new hashtag will be given. We wait until the following morning to use the public Twitter search API 3 to collect tweets that have been posted with the new hashtag. Generally, this returns 100-200 tweets. We wait until the following day to allow for as many tweets as possible to be submitted. The day of the ensuing episode (i.e. on a Monday for a hashtag that came out for a Thursday episode), @midnight creates a Tumblr post 4 that announces the top-10 tweets from the previous episode's hashtag (the tweets are listed as embedded images, as is often done for sharing public tweets on websites). If they're not already present, we add the tweets from the top-10 to our existing list of tweets for the hashtag. We also perform automated filtering to remove redundant tweets. Specifically, we see that the text of tweets (aside from hashtags and user mentions) are not the same. The need for this results from the fact that some viewers submit identical tweets.</p><p>Using both the @midnight official Tumblr account, as well as the show's official website where the winning tweet is posted, we annotate each tweet with labels 0, 1 and 2. Label 2 designates the winning tweet. Thus, the label 2 only occurs once for each hashtag. Label 1 indicates that the tweet was selected as a top-10 tweet (but not the winning tweet) and label 0 is assigned for all other tweets. It is important to note that every time we collect a tweet, we must also collect its tweet ID. While this was initially done to comply with Twitter's terms of use 5 , which disallows the public distribution of users' tweets, The presence of tweet IDs allows us to easily handle the evaluation process when referencing tweets (see Section 4). The need to determine the tweet IDs for tweets that weren't found in the initial query (i.e. tweets added from the top 10) makes the data collection process slightly laborious, since the top-10 list doesn't contain the tweet ID. In fact, it doesn't even contain the text itself since it's actually an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">A Semi-Automated System for Data</head><p>Collection Because the data collection process is continuously repeated and requires a non-trivial amount of human labor, we have built a helper system that can partially automate the process of data collection. This system is organized as a website with a convenient user interface.</p><p>On the start page the user enters the id of the Tumblr post with the tweets in the top 10. Next, we invoke Tesseract 6 , an OCR command-line utility, to recognize the textual content of the tweet images. Using the recognized content, the system forms a webpage on which the user can simultaneously see the text of the tweets as well as the original images. On this page, the user can query the Twitter API to search by text, or click the button "Open twitter search" to open the Twitter Search page if the API returns zero results. We note that the process is not fully automated because a given text query can we return redundant results, and we primarily check to make sure we add the tweet that came from the appropriate user. With the help of this system, the process of collecting the top-10 tweets (along with their tweet IDs) takes roughly 2 minutes. Lastly, we note that the process for annotating the winning tweet (which is already included in the top-10 posted in the Tumblr list) is currently manual, because it requires going to the @midnight website. This is another aspect of the data collection system that could potentially be automated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>Data collection occurred for roughly eight months, producing a total of 12,734 tweets for 112 hashtags. The resulting dataset is what we used for the task.</p><p>The distribution of the number of tweets per hashtag is represented in Figure <ref type="figure">1</ref>. For 71% of hashtags, we have at least 90 tweets. The files of the individual hashtags are formatted so that the individual hashtag tokens are easily recoverable. Specifically, tokens are separated by the ' ' character. For example, the hashtag FastFoodBooks has the file name "fast food books.tsv". Figure <ref type="figure">2</ref> represents an example of the tweets collected for the hashtag FastFoodBooks. Ob-Figure <ref type="figure">1</ref>: Distribution of the numbers of tweets per hashtag serve that this hashtag requires external knowledge about fast food and books in order to understand the humor. Furthermore, this hashtag illustrates how prevalent puns are in the dataset, especially related to certain target hashtags. In contrast, the hashtag IfIWerePresident (see Figure <ref type="figure">3</ref>) does not require external knowledge and the tweets are understandable without awareness of any specific concepts.</p><p>For the purpose of our task, we released 5 files/660 tweets as the trial data, 101 files/11,325 tweets (separate from the trial data) as the training data, and 6 files/749 tweets as the evaluation data. The 6 evaluation files were chosen based on the following logic: first, we examined the results of our own systems on individual hashtags using leave-one-out evaluation <ref type="bibr" target="#b11">(Potash et al., 2016)</ref>. We looked for a mixture of hashtags that had high, average, and low performance. Secondly, we wanted a mixture of hashtags that promote different types of humor, such as puns that use external knowledge (for example the hashtag FastFoodBooks in Figure <ref type="figure">3</ref>.2), or hashtags that seek to express more general humor (for example the hashtag IfIWerePresident in Figure <ref type="figure">3</ref>.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Subtasks</head><p>In this task, the results are evaluated in two subtasks. Subtask A requires the participants to generate the correct pairwise comparisons of tweets to determine which tweet is funnier according to the TV show @midnight. Subtask B asks for the correct ranking of tweets in terms of how funny they are (again, according to @midnight).</p><p>As I Lay Dying of congestive heart failure @midnight #FastFoodBooks Harry Potter and the Order of the Big Mac #FastFoodBooks @midnight The Girl With The Jared Tattoo #FastFood-Books @midnight A Room With a Drive-thru @midnight #Fast-FoodBooks Figure <ref type="figure">2</ref>: An example of the items in the dataset for the hashtag FastFoodBooks that requires external knowledge in order to understand the humor. Furthermore, the tweets for this hashtag are puns connecting book titles and fast food-related language #IfIWerePresident my Cabinet would just be cats. @midnight Historically, I'd oversleep and eventually get fired. @midnight #IfIWerePresident #IfIWerePresident I'd pardon Dad so we could be together again... @midnight #IfIWerePresident my estranged children would finally know where I was @midnight Figure <ref type="figure">3</ref>: An example of the items in the dataset for the hashtag IfIWerePresident that does not require external knowledge in order to understand the humor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subtask A: Pairwise Comparison</head><p>For the first subtask, we follow the approach taken by <ref type="bibr" target="#b16">Shahaf et al. (2015)</ref> and make predictions on pairs of tweets with the goal of determining which tweet is funnier. Using the tweets for each hashtag, we construct pairs of tweets in which one tweet is judged by the show to be funnier than the other. The pairs used for evaluation are constructed as follows:</p><p>(1) The tweets that are the top-10 funniest tweets are paired with the tweets not in the top-10.</p><p>(2) The winning tweet is paired with the other tweets in the top-10.</p><p>If we have n tweets for a given hashtag, (1) will produce 10(n − 10) pairs, and (2) will produce 9 pairs, giving us 10n − 91 data points for a single hashtag. Constructing the pairs for evaluation in this way ensures that one of the tweets in each pair has been judged to be funnier than the other. We follow Shahaf et al. and use the label 1 to denote that the first tweet is funnier, and 0 to denote that the second tweet is funnier. However, this labeling is counter-intuitive to zero-indexing, and could be changed to avoid confusion in labeling (see Section 5).</p><p>Since we only provide teams with files containing tweet ID, tweet text, and tweet label (gold label: 0, 1, or 2), it is up to the teams to form the appropriate pairs with the correct labels. In order to produce balanced training data, we recommend that the ordering of tweets in a pair be determined by a coin-flip. At evaluation time, we provide the teams with hashtag files with tweet id and tweet text. We then ask the teams to provide predictions for every possible tweet combination. Our evaluation script then chooses only the tweet pairs where two different labels are present. The pairs can be listed in either ordering of the tweets because the scorer accounts for the two possible orderings for each pair. We decided against the idea of providing the appropriate pairs themselves for evaluation because it is very easy to use frequencies of tweet IDs in the pairs to determine overall tweet label.</p><p>The evaluation measure for subtask A is the micro average of accuracy across the individual evaluation hashtags. For a given hashtag, the accuracy is the number of correctly predicted pairs divided by the total number of pairs. Therefore, random guessing will produce 50% accuracy on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subtask B: Ranking</head><p>The second subtask asks teams to use the same input data for training and evaluation as subtask A. However, whereas subtask A creates pairs of tweets based on the labeling, subtask B asks teams to predict the labels directly. For this dataset, the number of tweets per class is known. Moreover, since the labels describe a partial ordering, predicting the labels is akin to providing a ranking of tweets in order of how funny they are. Therefore, for subtask B, we ask the teams to provide prediction files where the tweets are ranking by how funny they are. From the provided ranking we infer the labeling: the first tweet is labeled 2, the next nine labeled 1, and the rest labeled 0.</p><p>The metric for evaluating subtask B is inspired by a notion of edit distance, because standard clas-sification metrics do not take into account class' comparative rankings. Treating labels as buckets, the metric determines, for a predicted label, how many 'moves' are needed to place it in the correct bucket. For example, if the correct label is 1 and the predicted label is 0, the edit distance is 1. Similarly, if the correct label is 0 and the predicted label is 2, the edit distance is 2. For a given hashtag file, the maximum edit distance for all tweets is 22. As a result, the edit distance for a given hashtag file is the total number of moves for all tweets divided by 22. This gives a normalized metric between 0 and 1 where a lower value is better. For the final distance metric, we micro-average across all evaluation files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Three teams participated only in subtask A, one team participated only in subtask B, and four teams participated in both subtasks. The official results for participating teams are shown in Tables <ref type="table" target="#tab_1">1 and 2</ref> for subtasks A and B, respectively. Note that due to space constraints we use short versions of hashtag names in the tables. Namely, "Christmas" corresponds to the hashtag RuinAChristmasMovie, "Shakespeare" corresponds to ModernShakespeare, "Bad Job" to Bad-JobIn5Words, "Break Up" to BreakUpIn5Words, "Broadway" to BroadwayACeleb, and "Cereal" to CerealSongs.</p><p>We report the results broken down by hashtag, as well as the overall micro-average. This table records results that were submitted to the Co-daLab competition pages 7 . TakeLab <ref type="bibr" target="#b8">(Kukovačec et al., 2017)</ref> submitted predictions with the labels flipped, which causes each run to appear in the table twice. The corrected files are not given an official ranking. After the release of the labeled evaluation data, many teams reported improved results. We have accrued these new results and combined them with the official submission rankings to produce Tables <ref type="table" target="#tab_4">3 and 4</ref>. The goal of these tables is to report the most up-to-date results on the evaluation set. Moreover, all results that do not have an official ranking in these tables are results that are reported individually by the teams in their system papers (except for TakeLab's results) after the gold evaluation labels were released.   The official results for the subtask B broken down by hashtag. Bold indicates the best run for the given hashtag. "Christmas" corresponds to the hashtag RuinAChristmasMovie, "Shakespeare" corresponds to ModernShakespeare, "Bad Job" to BadJobIn5Words, "Break Up" to BreakUpIn5Words, "Broadway" to BroadwayACeleb, and "Cereal" to CerealSongs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task Analysis</head><p>The last row of Table <ref type="table" target="#tab_1">1</ref> shows the average accuracy of each hashtag across all systems (the official results of the TakeLab systems are not included in this average since we also include in the average the unofficial, corrected results).</p><p>The two easiest hashtags are ones that require less external knowledge compared to the other four. These four hashtags specifically riff on a particular Christmas movie, Shakespeare quote, celebrity/Broadway play, or cereal/song. Consequently, one single system did best in three out of four of these hashtags (TakeLab). It is not coincidence, since this system made extensive use of external knowledge bases. Furthermore, the three hashtags where it did best required knowledge of specific entities, whereas the knowledge required in the hashtag ModernShakespeare is the actual lines from Shakespeare plays.</p><p>As we mentioned in Section 3.2, the evaluation hashtags were chosen partly because of our own system performance on the hashtags <ref type="bibr" target="#b11">(Potash et al., 2016)</ref>. One of the most difficult hashtags from our initial experiments was the hashtag CerealSongs, which was the hashtag systems performed the worse on in this task. We believe this is because the humor in this hashtag is based on two sources of external knowledge: cereals and songs. Correspondingly, the hashtag with the second worse performance also requires two sources of external knowledge: Broadway plays and celebrities (this hashtag was originally chosen as a representative of the hashtags our systems recorded average performance). The hashtag BadJobIn5Words was one that had high performance by our own systems, and that continued in this task. This hashtag had the second highest accuracy, and would have had the highest if the Duluth team <ref type="bibr" target="#b17">(Yan and Pedersen, 2017)</ref> did not have such remarkable success on the highest accuracy hashtag, BreakUpIn5Words.</p><p>The poor performance for the hashtags Cere-alSongs and BroadwayACeleb is also interesting   to note since they were chosen because the hashtag names had strong similarity to hashtags in the training data. For example, 12 hashtags in the training data had the word 'Song'. Likewise, five hashtags had the word 'Celeb', and there was one more hashtag with the word 'Broadway'. Alternatively, The two hashtags with the best performance followed the 'X in X words' format, for which there were 16 such hashtags in the training data.</p><p>Regarding the hashtag BadJobIn5Words, there are six hashtags in the training data beginning with the word 'Bad'.</p><p>Our current task analysis has focused on subtask A. The primary reason for this is that the performance on subtask B was relatively poor. To put the results in perspective, we created random guesses for subtask B, and these random guesses recorded an average distance of 0.880. From the results, only one team was able to beat this score. We can see that two of the three highest performing teams in subtask A did not participate in subtask B, and the other team that did participate approached subtask B as a secondary task (see Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">System Analysis</head><p>For the teams that participated in both subtasks, they used the output of a single system to predict for both subtasks. Two teams, SVNIT <ref type="bibr" target="#b9">(Mahajan and Zaveri, 2017)</ref> and QUB <ref type="bibr" target="#b5">(Han and Toner, 2017)</ref> , initially predicted the labels of each tweet based on the output of a supervised classifier, and then used these labels to both rank the tweets and make pairwise predictions for the subtasks. Duluth took a similar approach, but used the output of a language model to rank the tweets, as opposed to labels provided by a classifier. Conversely, TakeLab sought to solve subtask A first, then used the frequencies of a tweet being chosen as funnier in a pair to provide a single, ordered metric to make predictions for subtask B. The team that only participated in subtask B, #WarTeam, also used the output of a supervised classifier to label the tweets, which in turn provided the ranking. One of interesting results from having the two subtasks (which are effectively two different ways of evaluating the same overall task) is to see how it distinguishes the unified approaches to solving both subtasks. We can see that, in fact, the top team is not con-sistent between the two subtasks. It is not a surprise to see that the best performing team (out of the four that participated in both subtasks) in subtask A was TakeLab, who focused primarily on this task. Conversely, TakeLab finished second in subtask B to Duluth, who focused on creating an ordered metric for ranking via language models.</p><p>In terms of overall system approach, we can analyze how heavily systems rely on featureengineering, verse using learned representations from neural networks. Three of the top four systems for subtask A leveraged neural network architectures. Two of these systems used only pretrained word representations as external knowledge for the neural network systems. This is in opposition to other systems that relied on the output of separate tools, or looking up terms in corpora. Some teams, such as HumorHawk 8 (Donahue et al., 2017) and #WarTeam, used a combination of these two types of systems, and notably, the system that was ranked first in Subtask A (HumorHawk) was an ensemble system that utilized prediction from both feature-based and neural networks-based models.</p><p>As for the feature-based systems, one trend we observed is that many teams tried to capture the incongruity aspect of humor <ref type="bibr" target="#b3">(Cattle and Ma, 2017)</ref> , often present in the dataset. The approaches used by teams varied from n-gram language models, word association, to semantic relatedness features. In addition, the TakeLab team used cultural reference features, such as movie and song references, and Google Trends features for named entities. During the performed analysis, the team found these features most useful for the model.</p><p>Considering neural network-based systems, LSTMs were used the most, which is expected given the sequential nature of text data. Plain LSTM models alone, using pretrained word embeddings, achieved competitive results, and DataStories <ref type="bibr" target="#b2">(Baziotis et al., 2017)</ref> ranked third using a siamese bidirectional LSTM model with attention.</p><p>One key difference between the dataset used in this task and the datasets based on the NYCC <ref type="bibr" target="#b13">(Radev et al., 2015;</ref><ref type="bibr" target="#b16">Shahaf et al., 2015)</ref> is the presence of the hashtag. Some teams used additional hashtag-based features in their systems. <ref type="bibr">8</ref> Two of the organizers were members of this team. They were not involved in the data selection process. They had no knowledge of which files were selected for evaluation, nor how these files were chosen.</p><p>For example, humor patterns, defined by the hashtag, were one of the most important features for the TakeLab team. Other teams used semantic distances between the hashtag and tweets as features.</p><p>Table <ref type="table" target="#tab_1">1</ref> also includes the standard deviation of system scores across the hashtags. Looking at the numbers there appears to be little in the way of a pattern regarding the standard deviation numbers. When correlated with system accuracy, the results is 0.11, which supports the idea that consistency across the hashtags has no relation to overall system performance. Even between the two purest neural network-based systems, DataStories and HumorHawk run 1, the standard deviations vary greatly: 0.134 (DataStories) and 0.049 (Hu-morHawk run 1). In fact, 0.049 was the lowest standard deviation across all systems. Duluth recorded the highest standard deviation across the datasets, primarily due to the fact that it had the single highest accuracy on any hashtag (0.913 for the hashtag BreakUpIn5Words), as well as the lowest single hashtag score for any system with an overall accuracy greater than 0.600 (0.485 for the hashtag RuinAChristmasMovie). One possibility for this high standard deviation is that this is the only unsupervised system. However, the other run submitted by Duluth (whose primary difference is that its language model was trained on a dataset of tweets as opposed to news articles) has a both a significantly lower accuracy and standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented the results of the SemEval 2017 shared task: #HashtagWars: Learning a Sense of Humor. It was the first year this task was presented, attracting 8 teams and 19 systems across two substasks. The top performing systems achieved 0.675 accuracy in subtask A and 0.872 score on subtask B, advancing the difficult task of humor understanding. Interestingly, the topranked system used an ensemble of both featurebased and neural network-based systems, suggesting that despite the overwhelming success of neural networks in the past few years, human intuition is still important for systems that seek to automatically understand humor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The official results for the subtask A broken down by hashtag. Bold indicates the best run for the given hashtag. "Christmas" corresponds to the hashtag RuinAChristmasMovie, "Shakespeare" corresponds to ModernShakespeare, "Bad Job" to BadJobIn5Words, "Break Up" to BreakUpIn5Words, "Broadway" to BroadwayACeleb, and "Cereal" to CerealSongs.</figDesc><table><row><cell>Rank</cell><cell>Team</cell><cell>Run</cell><cell>Christmas</cell><cell>Shakespeare</cell><cell>Bad Job</cell><cell>Hashatag Break Up</cell><cell>Broadway</cell><cell>Cereal</cell><cell>Average</cell></row><row><cell>1</cell><cell>Duluth</cell><cell>2</cell><cell>0.818</cell><cell>0.909</cell><cell>1.000</cell><cell>0.636</cell><cell>1.000</cell><cell>0.909</cell><cell>0.872 (±0.137)</cell></row><row><cell>2</cell><cell>TakeLab</cell><cell>1</cell><cell>0.909</cell><cell>0.909</cell><cell>1.000</cell><cell>0.818</cell><cell>1.000</cell><cell>0.818</cell><cell>0.908 (±0.081)</cell></row><row><cell>3</cell><cell>QUB</cell><cell>1</cell><cell>0.818</cell><cell>0.909</cell><cell>0.818</cell><cell>1.000</cell><cell>1.000</cell><cell>0.909</cell><cell>0.924 (±0.081)</cell></row><row><cell>3</cell><cell>QUB</cell><cell>2</cell><cell>0.818</cell><cell>0.909</cell><cell>0.818</cell><cell>1.000</cell><cell>1.000</cell><cell>0.909</cell><cell>0.924 (±0.081)</cell></row><row><cell>5</cell><cell>SVNIT</cell><cell>2</cell><cell>0.818</cell><cell>1.000</cell><cell>0.909</cell><cell>1.000</cell><cell>1.000</cell><cell>0.818</cell><cell>0.938 (±0.089)</cell></row><row><cell>6</cell><cell>TakeLab</cell><cell>2</cell><cell>0.818</cell><cell>1.000</cell><cell>1.000</cell><cell>0.909</cell><cell>1.000</cell><cell>0.909</cell><cell>0.944 (±0.074)</cell></row><row><cell>7</cell><cell>SVNIT</cell><cell>1</cell><cell>1.000</cell><cell>0.818</cell><cell>1.000</cell><cell>0.909</cell><cell>1.000</cell><cell>1.000</cell><cell>0.949 (±0.076)</cell></row><row><cell>8</cell><cell>Duluth</cell><cell>1</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>0.909</cell><cell>0.909</cell><cell>0.967 (±0.047)</cell></row><row><cell>9</cell><cell cols="2">#WarTeam 1</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000 (±0.000)</cell></row><row><cell></cell><cell>Average</cell><cell></cell><cell cols="7">0.889 (±0.088) 0.939 (±0.064) 0.949 (±0.08) 0.919 (±0.124) 0.990 (±0.030) 0.909 (±0.064) 0.936 (±0.036)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Unofficial results for the subtask A on the released evaluation set reported by the participating teams</figDesc><table><row><cell>Official Ranking</cell><cell>Team</cell><cell>Score</cell><cell>Notes</cell></row><row><cell></cell><cell>Duluth</cell><cell cols="2">0.853 Bigram language model (news dataset)</cell></row><row><cell>1</cell><cell>Duluth</cell><cell cols="2">0.872 Trigram language model (news dataset)</cell></row><row><cell>2</cell><cell>TakeLab</cell><cell cols="2">0.908 Gradient boosting classifier with a rich set of features, including cultural references</cell></row><row><cell>3</cell><cell>QUB</cell><cell cols="2">0.924 A set of imblanaced classifiers with n-gram features</cell></row><row><cell>3</cell><cell>QUB</cell><cell cols="2">0.924 A set of imblanaced classifiers with n-gram features</cell></row><row><cell>5</cell><cell>SVNIT</cell><cell cols="2">0.938 Multilayer perceptron with incongruity, ambiguity, and stylistic features</cell></row><row><cell>6</cell><cell>TakeLab</cell><cell cols="2">0.944 Gradient boosting classifier with a rich set of features, including cultural references</cell></row><row><cell>7</cell><cell>SVNIT</cell><cell cols="2">0.949 A Naive Bayes classifier with incongruity, ambiguity, and stylistic features</cell></row><row><cell>8</cell><cell>Duluth</cell><cell cols="2">0.967 Trigram language model (tweets dataset)</cell></row><row><cell>9</cell><cell cols="3">#WarTeam 1.000 A word-based voting algorithm of a Naive Bayes and neural network word scorers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Unofficial results for the subtask B on the released evaluation set reported by the participating teams</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cc.com/shows/-midnight</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://contest.newyorker.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://dev.twitter.com/rest/public/ search 4 http://atmidnightcc.tumblr.com/ 5 https://dev.twitter.com/overview/ terms</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/tesseract-ocr/ tesseract</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://competitions.codalab.org/ competitions/15682, https://competitions. codalab.org/competitions/15689</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic detection of irony and humour in twitter</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Creativity</title>
				<meeting>the International Conference on Computational Creativity</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Datastories at semeval-2017 task 6: Siamese lstm with attention for humorous text comparison</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Pelekis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Doulkeridis</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2065" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Srhr at semeval-2017 task 6: Word associations for humour recognition</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cattle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="400" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humorhawk at semeval-2017 task 6: Mixing meaning and sound for humor recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="98" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Qub at semeval-2017 task 6: Cascaded imbalanced classification for humor analysis in twitter</title>
		<author>
			<persName><forename type="first">Xiwu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Toner</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="383" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the econometric society</title>
		<imprint>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">That&apos;s what she said: double entendre identification</title>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Takelab at semeval-2017 task 6: #rank-inghumorin4pages</title>
		<author>
			<persName><forename type="first">Marin</forename><surname>Kukovačec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Malenica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Mršić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domagoj</forename><surname>Anto-Niošajatović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janšnajder</forename><surname>Alagić</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2066" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="395" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Svnit @ semeval 2017 task-6: Learning a sense of humor using supervised approach</title>
		<author>
			<persName><forename type="first">Rutal</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukesh</forename><surname>Zaveri</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2069" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making computers laugh: Investigations in automatic humor recognition</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03216</idno>
		<title level="m">#hashtagwars: Learning a sense of humor</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Humor: Prosody analysis and automatic recognition for f* r* i* e* n* d* s*</title>
		<author>
			<persName><forename type="first">Amruta</forename><surname>Purandare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
				<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aikaterini</forename><surname>Iliakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Chanfreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>De Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08126</idno>
		<title level="m">Humor in collective discourse: Unsupervised funniness detection in the new yorker cartoon caption contest</title>
				<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic humor classification on twitter</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</title>
				<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multidimensional approach for detecting irony in twitter</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inside jokes: Identifying humorous cartoon captions</title>
		<author>
			<persName><forename type="first">Dafna</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mankoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1065" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Duluth at semeval-2017 task 6: Language models in humor detection</title>
		<author>
			<persName><forename type="first">Xinru</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Humor recognition and humor anchor extraction pages</title>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
				<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing humor on twitter</title>
		<author>
			<persName><forename type="first">Renxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naishi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
