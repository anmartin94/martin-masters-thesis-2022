<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval 2017 Task 10: ScienceIE -Extracting Keyphrases and Relations from Scientific Publications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London (UCL)</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mrinal</forename><surname>Das</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London (UCL)</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikraman</forename><surname>Lakshmi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval 2017 Task 10: ScienceIE -Extracting Keyphrases and Relations from Scientific Publications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Empirical research requires gaining and maintaining an understanding of the body of work in specific area. For example, typical questions researchers face are which papers describe which tasks and processes, use which materials and how those relate to one another. While there are review papers for some areas, such information is generally difficult to obtain without reading a large number of publications.</p><p>Current efforts to address this gap are search engines such as Google Scholar, 1 Scopus 2 or Semantic Scholar, 3 which mainly focus on navigating author and citations graphs.</p><p>The task tackled here is mention-level identification and classification of keyphrases, e.g. classification and relation extraction. However, keyphrases are much more challenging to identify than e.g. person names, since they vary significantly between domains, lack clear signifiers and contexts and can consist of many tokens. For this purpose, a double-annotated corpus of 500 publications with mention-level annotations was produced, consisting of scientific articles of the Computer Science, Material Sciences and Physics domains.</p><p>Extracting keyphrases and relations between them is of great interest to scientific publishers as it helps to recommend articles to readers, highlight missing citations to authors, identify potential reviewers for submissions, and analyse research trends over time. Note that organising keyphrases in terms of synonym and hypernym relations is particularly useful for search scenarios, e.g. a reader may search for articles on information extraction, and through hypernym prediction would also receive articles on named entity recognition or relation extraction.</p><p>We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges.</p><p>Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction <ref type="bibr" target="#b17">(Kate and Mooney, 2010;</ref><ref type="bibr" target="#b36">Singh et al., 2013;</ref><ref type="bibr" target="#b3">Augenstein et al., 2015;</ref><ref type="bibr" target="#b10">Goyal and Dyer, 2016)</ref>.</p><p>Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction <ref type="bibr" target="#b20">(Kim et al., 2010b;</ref><ref type="bibr" target="#b13">Hasan and Ng, 2014;</ref><ref type="bibr" target="#b39">Sterckx et al., 2016;</ref><ref type="bibr" target="#b2">Augenstein and Søgaard, 2017)</ref>, semantic relation extraction <ref type="bibr" target="#b12">Gupta and Manning, 2011;</ref><ref type="bibr">Marsi and Oztürk, 2015)</ref>, topic classification of scientific articles (Ó <ref type="bibr" target="#b29">Séaghdha and Teufel, 2014)</ref>, citation context extraction <ref type="bibr" target="#b42">(Teufel, 2006;</ref><ref type="bibr" target="#b16">Kaplan et al., 2009)</ref>, extracting author and citation graphs <ref type="bibr" target="#b30">(Peng and McCallum, 2006;</ref><ref type="bibr" target="#b6">Chaimongkol et al., 2014;</ref><ref type="bibr" target="#b35">Sim et al., 2015)</ref> or a combination of those <ref type="bibr" target="#b33">(Radev and Abu-Jbara, 2012;</ref><ref type="bibr" target="#b9">Gollapalli and Li, 2015;</ref><ref type="bibr" target="#b11">Guo et al., 2015)</ref>.</p><p>The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet. <ref type="bibr">4</ref> Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task organisers' collaboration with Elsevier. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The task is divided into three subtasks:</p><p>A) Mention-level keyphrase identification B) Mention-level keyphrase classification.</p><p>Keyphrase types are PROCESS (including methods, equipment), TASK and MATE-RIAL (including corpora, physical materials)</p><p>C) Mention-level semantic relation extraction between keyphrases with the same keyphrase types. Relation types used are HYPONYM-OF and SYNONYM-OF.</p><p>We will refer to the above subtasks as Subtask A, Subtask B, and Subtask C respectively. A shortened (artificial) example of a data instance for the Computer Science area is displayed in Example 1, examples for Material Science and Physics are included in the appendix. The first part is the plain text paragraph (with keyphrases in italics for better readability), followed by stand-off keyphrase annotations based on character offsets, followed relation annotations.</p><p>Example 1. Text: Information extraction is the process of extracting structured data from unstructured text, which is relevant for several end-to-end tasks, including question answering. This paper addresses the tasks of named entity recognition (NER), a subtask of information extraction, using conditional random fields (CRF). Our method is evaluated on the ConLL-2003 NER corpus. The training data part of the corpus consists of 350 documents, 50 for development and 100 for testing. This is similar to the pilot task described in Section 5, for which 144 articles were used for training, 40 for development and for 100 testing.</p><p>We present statistics about the dataset in Table 1. Notably, the dataset contains many long keyphrases. 22% of all keyphrases in the training set consist of words of 5 or more tokens. This contributes to making the task of keyphrase identification very challenging. However, 93% of those keyphrases are noun phrases 7 , which is valuable information for simple heuristics to identify keyphrase candidates. Lastly, 31% of keyphrases contained in the training dataset only appear in it once, systems will have do generalise to unseen keyphrases well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation Process</head><p>Mention-level annotation is very time-consuming, and only a handful of semantic relations such as hypernymy and synonymy can be found in each publication. We therefore only annotate paragraphs of publications likely to contain relations.</p><p>We originally intended to identify suitable documents by automatically extracting a knowledge graph of relations from a large scientific dataset using Hearst-style patterns <ref type="bibr" target="#b14">(Hearst, 1991;</ref><ref type="bibr" target="#b37">Snow et al., 2005)</ref>, then using those to find potential relations in a distinct set of documents, similar to the distant supervision <ref type="bibr" target="#b28">(Mintz et al., 2009;</ref><ref type="bibr" target="#b37">Snow et al., 2005)</ref> heuristic. Documents containing a high number of such potential relations would then be selected. However, this requires automatically learning to identify keyphrases between which those potential relations hold, and requires relations to appear several times in a dataset for such a knowledge graph to be useful.</p><p>In the end, this strategy was not feasible due to the difficulty of learning to detect keyphrases automatically and only a small overlap between relations in different documents. Instead, keyphrasedense paragraphs were detected automatically using a coarse unsupervised approach <ref type="bibr" target="#b27">(Mikolov et al., 2013)</ref> and those likely to contain relations were selected manually for annotation.</p><p>For annotation, undergraduate student volunteers studying Computer Science, Material Science or Physics were recruited using UCL's stu-dent newsletter, which reaches all of its students. Students were shown example annotations and the annotation guidelines, and if they were still interested in participating in the annotation exercise, afterwards asked to select beforehand how many documents they wanted to annotate. Approximately 50% of students were still interested, having seen annotated documents and read annotation guidelines. They were then given two weeks to annotate documents with the BRAT tool <ref type="bibr" target="#b38">(Stenetorp et al., 2012)</ref>, which was hosted on an Amazon EC2 instance as a web service. Students were compensated for annotations per document. Annotation time was estimated as approximately 12 minutes per document and annotator, on which basis they were paid roughly 10 GBP per hour. They were only compensated upon completion of all annotations, i.e. compensation was conditioned on annotating all documents. The annotation cost was covered by Elsevier. To develop annotation guidelines, a small pilot annotation exercise on 20 documents was performed with one annotator after which annotation guidelines were refined. <ref type="bibr">8</ref> We originally intended for student annotators to triple annotate documents and apply majority voting on the annotations, but due to difficulties with recruiting high-quality annotators we instead opted to double-annotate documents, where the second annotator was an expert annotator. Where annotations disagreed, we opted for the expert's annotation. Pairwise inter-annotator agreement between the student annotator and the expert annotator measured with Cohen's kappa is shown in Table 2. The * indicates annotation quality decreased over time, ending with the annotator not completing annotating all documents. To account for this, documents for which no annotations are given are excluded from computing inter-annotator agreement. Out of the annotators completing the annotation exercise, Cohen's kappa ranges between 0.45 and 0.85, with half of them having a substantial agreement of 0.6 or higher. For future iterations of this task, we recommend to invest significant efforts into recruiting high-quality annotators, perhaps with more pre-annotation quality screening.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>SemEval 2017 Task 10 offers three different evaluation scenarios:</p><p>1) Only plain text is given (Subtasks A, B, C).</p><p>2) Plain text with manually annotated keyphrase boundaries are given (Subtasks B, C).</p><p>3) Plain text with manually annotated keyphrases and their types are given (Subtask C).</p><p>We refer to the above scenarios as Scenario 1, Scenario 2, and Scenario 3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metrics</head><p>Keyphrase identification (Subtask A) has traditionally been evaluated by calculating the exact matches with the gold standard. There is existing work for capturing semantically similar keyphrases <ref type="bibr" target="#b46">(Zesch and Gurevych, 2009;</ref><ref type="bibr" target="#b19">Kim et al., 2010a)</ref>, however since these are captured using relations, similar to the pilot task on keyphrase extraction (Section 5) we evaluate keyphrases, keyphrase types and relations with exact match criteria. The output of systems is matched exactly against the gold standard. The traditionally used metrics of precision, recall and F1-score are computed and the micro-average of those metrics across publications of the three genres are calculated. These metrics are also calculated for Subtasks B and C. In addition, for Subtasks B and C, participants are given the option of using text manually annotated with keyphrase mentions and types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Pilot Task</head><p>A pilot task on keyphrase extraction from scientific documents was run by other organisers at Se-mEval 2010 <ref type="bibr" target="#b20">(Kim et al., 2010b)</ref>. The task was to extract a list of keyphrases representing key topics from scientific documents, i.e. similar to the first part of our proposed Subtask A, only on type-level. Participants were allowed to submit up to 3 runs and were required to submit a list of 15 keyphrases for each document, ranked by the probability of being reader-assigned phrases. and added to author-provided keyphrases required by the journals they were published in. Guidelines were for the keyphrases to exactly appear anywhere in the text of the paper, in reality 15% of annotator-provided keyphrases did not, as well as 19% of author-provided keyphrases. The number of author-specified keywords was 4 on average, whereas annotators identified 12 on average.</p><p>Returned phrases are considered correct if they are exact matches of either the annotator-or authorassigned keyphrases, allowing for minor syntactic variations (A of B → B A ; A's B → A B). Precision, recall and F1 is calculated for the top 5, top 10 and all keywords. 19 systems were submitted to the task, the best one achieving an F1 of 27.5% on the combined author-assigned and annotatorassigned keywords.</p><p>Lessons learned from the task were that performance varies depending on how many keywords are to be extracted, the task organisers recommend against fixing a threshold for a number of keyphrases to extract lead. They further recommend a more semantically-motivated task, taking into account synonyms of keyphrases instead of requiring exact matches. Both of those recommendations will be taken into account for future task design. To fulfill the latter, we will ask annotator to assign types to the identified keywords (process, task, material) and identify semantic relations between them (hypernym, synonym).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Existing Resources</head><p>As part of the FUSE project with IARPA, we created a small annotated corpus of 100 noun phrases generated from the titles and abstracts derived from the Web Of Science corpora 9 of the domains Physics, Computer Science, Chemistry and Computer Science. These corpora cannot be distributed publicly and were made available by the IARPA funding agency. Annotation was performed by 3 annotators using 14 fine-grained types, including PROCESS.</p><p>We measured inter-annotator agreement among the three annotators for the 14 categories using Fleiss' Kappa. The k value was found to be 0.28 which implies that there was fair agreement between them, however distinguishing between the fine-grained types added significantly to the annotation time. Therefore we only use three main types for the SemEval 2017 Task 10.</p><p>There are some existing keyphrase extraction corpora, however, they are not similar enough to the proposed task to justify reuse. Below is a description of existing corpora.</p><p>The SemEval 2010 Keyphrase Extraction corpus (Kim et al., 2010b) 10 consists of a handful of document-level keyphrases per article. In contrast to the task proposed, the keyphrases are annotated on type-level and not further classified as process, task or material and semantic relations are not annotated. Further, the domains considered are different and mostly sub-domains of Computer Science.</p><p>The corpus released by  11 contains sentence-level fine-grained semantic annotations for 230 publication abstracts in Japanese and 400 in English. In contrast to what we propose, the annotations are more fine-grained and annotations are only available for abstracts.</p><p>Gupta and Manning (2011) studied keyphrase extraction from ACL Anthology articles, applying a pattern-based bootstrapping approach based on 15 016 documents and assigning the types FO-CUS, TECHNIQUE and DOMAIN. Performance was evaluated on 30 manually annotated documents. Although the latter corpus is related to what we propose, manual annotation is only available for a small number of documents and only for the Natural Language Processing domain.</p><p>The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>We frame the task as a sequence-to-sequence prediction task. We preprocess the files by splitting documents into sentences and tokenising them with nltk, then aligning span annotations from .ann files to tokens. Each sentence is regarded as one sequence. We then split the task into the three subtasks, keyphrase boundary identification, keyphrase classification and relation classification and add three output layers. We predict the following types, for the three subtasks respectively: Subtask A: t A = O, B, I for tokens being outside, at the beginning, or inside a keyphrase Subtask B: t B = O, M, P, T for tokens being outside a keyphrase, or being part of a material, process or task Subtask C: t C = O, S, H for Synonym-of and Hyponym-of relations. For Subtask A and B, we predict one output label per input token. For Subtask C we predict a vector for each token, that encodes what the relationship between that token and every other token in the sequence is for the first token in each keyphrase. After predictions for tokens are obtained, these are converted back to spans and relations between them in a postprocessing step.</p><p>We report results for two simple models: one to estimate the upper bound, that converts .ann files into instances, as described above, then converts them back into .ann files. Next, to estimate a lower bound, a random baseline, that for each token assigns a random label for each of the subtasks.</p><p>The upper bound span-token-span round-trip conversion performance, an F1 of 0.84, shows that we already lose a significant amount of performance due to sentence splitting and tokenisation alone. The random baseline further shows hard especially the keyphrase boundary identification task is and as a result the overall task, since the subtasks depend on one another. For Subtask A, a random baseline achieves an F1 of 0.03. The overall tasks gets easier if keyphrase boundaries are given, resulting in F1 of 0.23 for keyphrase classification, and if keyphrase types are given, an F1 of 0.04 are achieved with the random baseline for Subtask C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary of Participating Systems</head><p>In this section, we summarise the outcome of the competition. For more details please refer to the respective system description papers and the task website https://scienceie. github.io/.</p><p>We had three subtasks, described in Sec 2, which were grouped together in three evaluation scenarios, described in Sec 4. The competition was hosted in CodaLab 12 in two phases: (i) de-velopment phase and (ii) testing phase. Fifty four teams participated in the development phase, and out of them twenty six teams participated in the final competition. One of the major success of the competition is due to such wide participation and application of various different techniques starting from neural networks, supervised classification with careful feature engineering to simple rule based methods. We present a summary of approaches used by task participants below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation Scenario 1</head><p>In this scenario teams need to solve all three subtasks A, B, and C; where no annotation information was given. Some teams participated only in Subtask A, or B; but the overall micro F1 performance across subtasks is considered for the ranking of the teams. Seventeen teams participated in this scenario. The F1 scores range from 0.04 to 0.43. Complete results are given in Table <ref type="table" target="#tab_6">3</ref>.</p><p>Various different types of methods have been applied by different teams with various levels of supervision. The best three teams TTI COIN, TIAL UW, and s2 end2end have used recurrent neural network (RNN) based approaches to obtain F1 scores of 0.38, 0.42 and 0.43 respectively. However, TIAL UW, and s2 end2end, by using a conditional random fields (CRF) layer on top of RNNs achieve a higher F1 in Subtask A compared to TTI COIN.</p><p>The fourth team PKU ICL with an F1 of 0.37 found classification models based on random forest and support vector machines (SVM) useful with carefully engineered feature such as TF-IDF over a very large external corpus, IDF weighted word-embeddings etc, along with an existing taxonomy. SciX on the other hand used noun phrase chunking and trained an SVM classifier on provided training data to classify phrases, and used a CRF to predict labels of the phrases. CRF based methods with parts-of-speech (POS) tagging and orthographic features such as presence of symbols and capitalisation have been tried by several teams (NTNU, SZTE-NLP, WING-NUS) and they leading to a reasonable performance (F1: 0.23, 0.26, and 0.27, respectively).</p><p>Noun phrase extraction with length constraint by HCC-NLP, and using a global list of keyphrases by NITK IT PG are found not to perform satisfactorily (F1: 0.16 and 0.14 respectively). The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>competitions/15898</head><p>Teams Overall A B C s2 end2end <ref type="bibr" target="#b1">(Ammar et al., 2017)</ref> 0.43 0.55 0.44 0.28 TIAL UW 0.42 0.56 0.44 TTI COIN <ref type="bibr" target="#b43">(Tsujimura et al., 2017)</ref> 0.38 0.5 0.39 0.21 PKU ICL <ref type="bibr" target="#b44">(Wang and Li, 2017)</ref> 0.37 0.51 0.38 0.19 NTNU-1  0.33 0.47 0.34 0.2 WING-NUS <ref type="bibr" target="#b31">(Prasad and Kan, 2017)</ref> 0.27 0.46 0.33 0.04 Know-Center <ref type="bibr" target="#b18">(Kern et al., 2017)</ref> 0.27 0.39 0.28 SZTE-NLP <ref type="bibr" target="#b5">(Berend, 2017)</ref> 0.26 0.35 0.28 NTNU <ref type="bibr" target="#b22">(Lee et al., 2017b)</ref> 0.23 0.3 0.24 0.08 LABDA    <ref type="bibr" target="#b8">(Flores et al., 2017)</ref> 0.04 0.08 0.04 upper bound 0.84 0.85 0.85 0.77 random 0.00 0.03 0.01 0.00 former is surprising, as keyphrases are with an overwhelming majority noun phrases, the latter not as much, many keyphrases only appear once in the dataset (see Table <ref type="table" target="#tab_2">1</ref>). GMBUAP further tried using empirical rules obtained by observing the training data for Subtask A, and a Naive Bayes classifier trained on provided training data for Subtask B. Such simple methods on their own prove not to be accurate enough. Attempts of such give us additional insight about the hardness of the problem and applicability of simple methods to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation Scenario 2</head><p>In this scenario teams needed to solve sub-tasks B, and C. Partial annotation was provided to the teams, that is, solution to the Subtask A. Four teams participated in this scenario with F1 cores ranging from 0.43 to 0.64. Please refer to  which are called etc in the text between two keyphrases. Interestingly, the RNN based approach of s2 end2end in Scenario 1 performs better than MayoNLP without using partial annotation of Subtask A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluation Scenario 3</head><p>In this scenario, teams need to solve only Subtask C. Partial annotations were provided to the teams for Subtask B and C. Five teams participated in this scenario, and F1 scores ranged from 0.1 to 0.64. Please refer to Table <ref type="table" target="#tab_8">5</ref> for complete result. Neural network (NN) based models are found to perform better than other methods in this scenario. The best method by MIT uses a convolutional NN (CNN). The other method uses two phases of NN and found to be reasonably effective (F1: 0.54).</p><p>On the other hand, application of supervised classification with five different classifiers (SVM, decision tree, random forest, multinomial naive <ref type="bibr">15</ref> After the end of the evaluation period, team TTI COIN rel discovered a bug in preprocessing, leading to low results. Their overall result after having corrected for that error is a Macro F1 of 0.48.</p><p>Bayes and k-nearest neighbour) using three different feature selection techniques (chi square, decision tree, and recursive feature elimination) found close accuracy (F1: 0.5) with the top performing ones.</p><p>LaBDA also use a CNN based method. However, the rule based post-processing and argument ordering strategy applied by MIT seemed to give additional advantage as also observed by them.</p><p>However most of the teams in this scenario outperform, all teams from other scenarios (who did not have access to partial information for Subtask B, and C) in relation prediction. This also asserts the significance of accuracy on Subtask A, and B in order to perform accurately on Subtask C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we present the setup and discuss participating systems of SemEval 2017 Task 10 on identifying and classifying keyphrases and relations between them from scientific articles, to which 26 systems were submitted. Successful systems vary in their approaches. Most of them use RNNs, often in combination with CRFs as well as CNNs, however the system performing best for evaluation scenario 1 uses an SVM with a well-engineered lexical feature set. Identifying keyphrases is the most challenging subtask, since the dataset contains many long and infrequent keyphrases, and systems relying on remembering them do not perform well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Characteristics of SemEval 2017 Task 10 dataset, statistics of training sets</figDesc><table><row><cell cols="2">Student Annotator IAA</cell></row><row><cell>1</cell><cell>0.85</cell></row><row><cell>2</cell><cell>0.66</cell></row><row><cell>3</cell><cell>0.63</cell></row><row><cell>4</cell><cell>0.60</cell></row><row><cell>5</cell><cell>0.50</cell></row><row><cell>6</cell><cell>0.48</cell></row><row><cell>7</cell><cell>0.47</cell></row><row><cell>8</cell><cell>0.45</cell></row><row><cell>9*</cell><cell>0.25</cell></row><row><cell>10*</cell><cell>0.22</cell></row><row><cell>11*</cell><cell>0.20</cell></row><row><cell>12*</cell><cell>0.15</cell></row><row><cell>13*</cell><cell>0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Inter-annotator agreement between the student annotator and the expert annotator, measured with Cohen's Kappa</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>F1 scores of teams participating in Scenario 1 and baseline models for Overall, Subtask A, Subtask B, and Subtask C. Ranking of the teams is based on overall performance measured in Micro F1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>F1 scores of teams participating in Scenario 2 and baseline models for Overall, Subtask B, and Subtask C. Ranking of the teams is based on overall performance measured in Micro F1. Teams participating in Scenario 2 received partial annotation with respect to Subtask A.</figDesc><table><row><cell>Teams</cell><cell></cell><cell></cell><cell>Overall B</cell><cell>C</cell></row><row><cell cols="2">MayoNLP (Liu et al., 2017)</cell><cell></cell><cell>0.64 0.67 0.23</cell></row><row><cell cols="4">UKP/EELECTION (Eger et al., 2017) 13 0.63 0.66</cell></row><row><cell cols="3">LABDA (Segura-Bedmar et al., 2017)</cell><cell>0.48 0.51</cell></row><row><cell cols="2">BUAP (Alemán et al., 2017)</cell><cell></cell><cell>0.43 0.45</cell></row><row><cell>upper bound</cell><cell></cell><cell></cell><cell>0.84 0.85 0.77</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell>0.15 0.23 0.01</cell></row><row><cell>Table 4: Teams</cell><cell>Overall</cell><cell></cell></row><row><cell>MIT (Lee et al., 2017a)</cell><cell>0.64</cell><cell></cell></row><row><cell>s2 rel (Ammar et al., 2017)</cell><cell>0.54</cell><cell></cell></row><row><cell>NTNU-2 (Barik and Marsi, 2017)</cell><cell>0.5</cell><cell></cell></row><row><cell>LaBDA (Suárez-Paniagua et al., 2017)</cell><cell>0.38</cell><cell></cell></row><row><cell>TTI COIN rel (Tsujimura et al., 2017) 15</cell><cell>0.1</cell><cell></cell></row><row><cell>upper bound</cell><cell>0.84</cell><cell></cell></row><row><cell>random</cell><cell>0.04</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">rankings are consistent in each category. BUAP</cell></row><row><cell></cell><cell></cell><cell cols="2">with the worst F1 score for Subtask B (0.45),</cell></row><row><cell></cell><cell></cell><cell cols="2">is still better than the best team in Scenario 1</cell></row><row><cell></cell><cell></cell><cell cols="2">s2 end2end for Subtask B (0.44). Partial annota-</cell></row><row><cell></cell><cell></cell><cell cols="2">tion or accuracy for Subtask A proves to be crit-</cell></row><row><cell></cell><cell></cell><cell cols="2">ical, reinforcing again that identifying keyphrase</cell></row><row><cell></cell><cell></cell><cell cols="2">boundaries is the most difficult part of the shared</cell></row><row><cell></cell><cell></cell><cell>task.</cell></row><row><cell></cell><cell></cell><cell cols="2">Unlike the Scenario 1, in this case the top</cell></row><row><cell></cell><cell></cell><cell cols="2">two teams used classifiers with lexical features</cell></row><row><cell></cell><cell></cell><cell cols="2">(F1: 0.64) as well as neural networks (F1: 0.63).</cell></row><row><cell></cell><cell></cell><cell cols="2">The first team MayoNLP used SVM with rich</cell></row><row><cell></cell><cell></cell><cell cols="2">feature sets like n-grams, lexical features, or-</cell></row><row><cell></cell><cell></cell><cell cols="2">thographic features, whereas the second team</cell></row><row><cell></cell><cell></cell><cell cols="2">UKP/EELECTION used used three different neu-</cell></row><row><cell></cell><cell></cell><cell cols="2">ral network approaches and subsequently com-</cell></row><row><cell></cell><cell></cell><cell cols="2">bined them via majority voting. Both these meth-</cell></row><row><cell>for complete result.</cell><cell>Table 4</cell><cell cols="2">ods perform quite similarly. However, a CRF based approach and an SVM with simpler fea-ture sets attempted by the two teams LABDA and</cell></row><row><cell cols="2">Except MayoNLP, other three teams partici-</cell><cell cols="2">BUAP are found to be less effective in this sce-</cell></row><row><cell cols="2">pated only in Subtask B. Although ranking is done</cell><cell>nario.</cell></row><row><cell cols="2">based on overall performance, but in this scenario</cell><cell cols="2">MayoNLP applied a simple rule based method</cell></row><row><cell cols="2">13 After the end of the evaluation period, team</cell><cell cols="2">for synonym-of relation extraction, and Hearst</cell></row><row><cell cols="2">UKP/EELECTION discovered those results were based on training on the development set. For training on the training set, their results are: 0.69 F1 overall and 0.72 F1 for Subtask B only</cell><cell cols="2">patterns for hyponym-of relation detection. The rules for synonym-of detection is based on pres-ence of phrases such as in terms of, equivalently,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: F1 scores of teams participating in Sce-</cell></row><row><cell>nario 3 and baseline models. Teams participating</cell></row><row><cell>in Scenario 3 received partial annotation with re-</cell></row><row><cell>spect to Subtask A, and Subtask B. Ranking of the</cell></row><row><cell>teams is based on overall performance measured</cell></row><row><cell>in Micro F1.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://scholar.google.co.uk/ 2 http://www.scopus.com/ 3 https://www.semanticscholar.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://wordnet.princeton.edu/ 5 https://www.elsevier.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.sciencedirect.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Parts of speech are determined automatically, using the nltk POS tagger</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Annotation guidelines were available to task participants, they can be found here: https://scienceie. github.io/resources.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">http://thomsonreuters. com/en/products-services/ scholarly-scientific-research/ scholarly-search-and-discovery/ web-of-science.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/snkim/ AutomaticKeyphraseExtraction 11 https://github.com/mynlp/ranis</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://competitions.codalab.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Elsevier for supporting this shared task. Special thanks go to Ronald Daniel Jr. for his feedback on the task setup and Pontus Stenetorp for his advice on brat and shared task organisation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Darnes</forename><surname>Yuridiana Alemán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josefa</forename><surname>Vilariño</surname></persName>
		</author>
		<author>
			<persName><surname>Somodevilla</surname></persName>
		</author>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">BUAP at SemEval-2017 Task 10</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The AI2 system at SemEval-2017 Task 10 (ScienceIE): semisupervised end-to-end entity and relation extraction</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Task Learning of Keyphrase Boundary Classification</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting Relations between Non-Standard Entities using Distant Supervision and Imitation Learning</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NTNU-2@ScienceIE: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents</title>
		<author>
			<persName><forename type="first">Biswanath</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SZTE-NLP at SemEval-2017 Task 10: A High Precision Sequence Model for Keyphrase Extraction Utilizing Sparse Coding for Feature Generation</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Corpus for Coreference Resolution on Scientific Papers</title>
		<author>
			<persName><forename type="first">Panot</forename><surname>Chaimongkol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EELEC-TION at SemEval-2017 Task 10: Ensemble of nEural Learners for kEyphrase ClassificaTION</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik-Lân Do</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Kiaeeha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GMBUAP at SemEval-2017 Task 10: A First Approach Based Empirical Rules and n-grams for the Extraction and Classification of Keyphrases</title>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mireya</forename><surname>Tovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">A</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EMNLP versus ACL: Analyzing NLP research over time</title>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Sujatha Das Gollapalli</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Posterior regularization for Joint Modelling of Multiple Structured Prediction Tasks with Soft Constraints</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised Declarative Knowledge Induction for Constraint-Based Learning of Information Structure in Scientific Documents</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="131" to="143" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analyzing the Dynamics of Research by Extracting Key Aspects of Scientific Papers</title>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJC-NLP</title>
				<meeting>IJC-NLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic Keyphrase Extraction: A Survey of the State of the Art</title>
		<author>
			<persName><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Noun Homograph Disambiguation Using Local Context in Large Text Corpora. Using Corpora pages</title>
		<author>
			<persName><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIPN at SemEval-2017 Task 10: Selecting Candidate Phrases with Part-of-Speech Tag Sequences from the training data</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">David</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Citation Contexts for Research Paper Summarization: A Coreference-chain based Approach</title>
		<author>
			<persName><forename type="first">Dain</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takenobu</forename><surname>Tokunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLPIR4DL workshop at ACL-IJCNLP</title>
				<meeting>NLPIR4DL workshop at ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint Entity and Relation Extraction using Card-Pyramid Parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
				<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Know-Center at SemEval-2017 Task 10: CODE annotator</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Rexha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating n-gram based evaluation metrics for automatic keyphrase extraction</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
				<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 5 : Automatic Keyphrase Extraction from Scientific Articles</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olena</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
				<meeting>the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The NTNU System at SemEval-2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications Using Multiple Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Kuei-Ching</forename><surname>Lung-Hao Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuen-Hsien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Shen Feichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfang</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MayoNLP at SemEval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word Embedding Distance Pattern for Keyphrase Classification in Scientific Publications</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extraction and generalisation of variables from scientific publications</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<author>
			<persName><surname>Pinaröztürk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NTNU-1@ScienceIE at SemEval-2017 Task 10: Identifying and Labelling Keyphrases with Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Kumar Sikdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswanath</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rune</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><surname>Saetre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
				<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of rhetorical structure with untopic models</title>
		<author>
			<persName><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
				<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Information extraction from research papers using conditional random fields</title>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="963" to="979" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">WING-NUS at SemEval-2017 Task 10: Keyphrase Extraction and Classification as Joint Sequence Labeling</title>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The ACL RD-TEC 2.0: A Language Resource for Evaluating Term Extraction and Entity Recognition Methods</title>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rediscovering ACL Discoveries Through the Lens of ACL Anthology Network Citing Sentences</title>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Abu-Jbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries</title>
				<meeting>the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LABDA at SemEval-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the BANNER tool and the UMLS Semantic Network</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristóbal</forename><surname>Colón-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Se-mEval</title>
				<meeting>Se-mEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Utility Model of Authors in the Scientific Community</title>
		<author>
			<persName><forename type="first">Yanchuan</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint Inference of Entities, Relations, and Coreference</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction (AKBC)</title>
				<meeting>the 2013 Workshop on Automated Knowledge Base Construction (AKBC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BRAT: a Web-based Tool for NLP-Assisted Text Annotation</title>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Topić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL System Demonstrations</title>
				<meeting>EACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised Keyphrase Extraction as Positive Unlabeled Learning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LaBDA Team at SemEval-2017 Task C: Relation Classification between keyphrases via Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Víctor</forename><surname>Suárez-Paniagua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Annotation of Computer Science Papers for Semantic Relation Extraction</title>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yo</forename><surname>Shidahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
			<persName><forename type="first">Hrafn</forename><surname>Loftsson</surname></persName>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
			<persName><forename type="first">Asuncin</forename><surname>Moreno</surname></persName>
			<persName><forename type="first">Jan</forename><surname>Odijk</surname></persName>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Argumentative Zoning for Improved Citation Indexing</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Attitude and Affect in Text</title>
				<editor>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
			<persName><forename type="first">Yan</forename><surname>Qu</surname></persName>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="159" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TTI-COIN at SemEval-2017 Task 10: Investigating Embeddings for End-to-End Relation Extraction from Scientific Papers</title>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Tsujimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Se-mEval</title>
				<meeting>Se-mEval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">PKU ICL at</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Keyphrase Extraction with Model Ensemble and External Knowledge</title>
		<idno>SemEval-2017 Task 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Approximate Matching for Evaluating Keyphrase Extraction</title>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
				<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
