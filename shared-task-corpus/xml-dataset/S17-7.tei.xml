<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2017 Task 7: Detection and Interpretation of English Puns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA/UKP-DIPF</orgName>
								<orgName type="institution">Technische Universität Darmstadt https</orgName>
								<address>
									<addrLine>//www.ukp.tu-darmstadt.de</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ontological Semantic Technology Lab Texas A&amp;M University-Commerce</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Hempelmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ontological Semantic Technology Lab Texas A&amp;M University-Commerce</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA/UKP-DIPF</orgName>
								<orgName type="institution">Technische Universität Darmstadt https</orgName>
								<address>
									<addrLine>//www.ukp.tu-darmstadt.de</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2017 Task 7: Detection and Interpretation of English Puns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect. Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-percontext assumption. This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set. Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word sense disambiguation (WSD), the task of identifying a word's meaning in context, has long been recognized as an important task in computational linguistics, and has been the focus of a considerable number of Senseval/SemEval evaluation tasks. Traditional approaches to WSD rest on the assumption that there is a single, unambiguous communicative intention underlying each word in the document. However, there exists a class of language constructs known as puns, in which lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the speaker or writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings. Though puns are a recurrent and expected feature in many discourse types, they have attracted relatively little attention in the fields of computational linguistics and natural language processing in general, or WSD in particular. In this document, we describe a shared task for evaluating computational approaches to the detection and semantic interpretation of puns.</p><p>A pun is a form of wordplay in which one sign (e.g., a word or phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect <ref type="bibr" target="#b0">(Aarons, 2017;</ref><ref type="bibr" target="#b12">Hempelmann and Miller, 2017)</ref>. For example, the first of the following two punning jokes exploits the sound similarity between the surface sign "propane" and the latent target "profane", while the second exploits contrasting meanings of the word "interest":</p><p>(1) When the church bought gas for their annual barbecue, proceeds went from the sacred to the propane.</p><p>(2) I used to be a banker but I lost interest.</p><p>Puns where the two meanings share the same pronunciation are known as homophonic or perfect, while those relying on similar-but not identicalsounding signs are known as heterophonic or imperfect. Where the signs are considered as written rather than spoken sequences, a similar distinction can be made between homographic and heterographic puns.</p><p>Conscious or tacit linguistic knowledgeparticularly of lexical semantics and phonology-is an essential prerequisite for the production and interpretation of puns. This has long made them an attractive subject of study in theoretical linguistics, and has led to a small but growing body of research into puns in computational linguistics. Most computational treatments of puns to date have focused on generative algorithms <ref type="bibr">Ritchie, 1994, 1997;</ref><ref type="bibr" target="#b38">Ritchie, 2005;</ref><ref type="bibr" target="#b13">Hong and Ong, 2009;</ref><ref type="bibr" target="#b45">Waller et al., 2009;</ref><ref type="bibr" target="#b17">Kawahara, 2010)</ref> or modelling their phonological properties <ref type="bibr">(Hempelmann, 2003a,b)</ref>. However, several studies have explored the detection and interpretation of puns <ref type="bibr" target="#b47">(Yokogawa, 2002;</ref><ref type="bibr" target="#b42">Taylor and Mazlack, 2004;</ref><ref type="bibr" target="#b27">Miller and Gurevych, 2015;</ref><ref type="bibr" target="#b16">Kao et al., 2015;</ref><ref type="bibr" target="#b28">Miller and Turković, 2016;</ref><ref type="bibr" target="#b26">Miller, 2016)</ref>; the most recent of these focus squarely on computational semantics. In this paper, we present the first organized public evaluation for the computational processing of puns.</p><p>We believe computational interpretation of puns to be an important research question with a number of real-world applications. For example:</p><p>• It has often been argued that humour can enhance human-computer interaction (HCI) <ref type="bibr" target="#b11">(Hempelmann, 2008)</ref>, and at least one study <ref type="bibr" target="#b29">(Morkes et al., 1999)</ref> has already shown that incorporating canned humour into a user interface can increase user satisfaction without adversely affecting user efficiency. An interactive system that is able to recognize and produce contextually appropriate responses to users' puns could further enhance the HCI experience.</p><p>• Recognizing humorous ambiguity is also important in machine translation, particularly for sitcoms and other comedic works, which feature puns and other forms of wordplay as a recurrent and expected feature <ref type="bibr" target="#b40">(Schröter, 2005)</ref>. Puns can be extremely difficult for non-native speakers to detect, let alone translate. Future automatic translation aids could scan source texts, flagging potential puns for special attention, and perhaps even proposing ambiguity-preserving translations that best match the original pun's double meaning.</p><p>• Wordplay is a perennial topic of scholarship in literary criticism and analysis, with entire books (e.g., <ref type="bibr">Wurth, 1895;</ref><ref type="bibr" target="#b39">Rubinstein, 1984;</ref><ref type="bibr" target="#b18">Keller, 2009)</ref> having been dedicated to cataloguing the puns of certain authors. Computerassisted detection and classification of puns could help digital humanists in producing similar surveys of other oeuvres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data sets</head><p>The pun processing tasks at SemEval-2017 used two manually annotated data sets, both of which we are freely releasing to the research community.1 Our first data set, containing English homographic puns, is based on the one described by <ref type="bibr" target="#b26">Miller and</ref><ref type="bibr" target="#b28">Turković (2016) and</ref><ref type="bibr" target="#b26">Miller (2016)</ref>.2 It contains punning and non-punning jokes, aphorisms, and other short, self-contained contexts sourced from professional humorists and online collections. For the purposes of deciding which contexts contain a pun, we used a somewhat weaker definition of homography: the lexical units corresponding to a pun's two distinct meanings must be spelled exactly the same way, with the exception that inflections and particles (e.g., the prepositions or dummy object pronouns in phrasal verbs such as "duke it out") may be disregarded. The contexts have the following characteristics:</p><p>• Each context contains a maximum of one pun.</p><p>• Each pun (and its latent target) contains exactly one content word (i.e., a noun, verb, adjective, or adverb) and zero or more non-content words (e.g., prepositions or articles). Here "word" is defined as a sequence of letters delimited by space or punctuation. This means that puns and targets do not include hyphenated words, and they do not consist of multi-word expressions containing more than one content word, such as "get off the ground" or "state of the art". Puns and targets may be multi-word expressions containing only one content word-this includes phrasal verbs such as "take off" or "put up with".</p><p>• Each pun (and its target) has a lexical entry in WordNet 3.1. However, the sense of the pun or the target may or may not exist in WordNet 3.1.</p><p>The homographic data set contains 2250 contexts, of which 1607 (71%) contain a pun. Sense annotation was carried out by three trained human judges, two of whom independently applied sense keys from WordNet 3.1. Each pun word was annotated with two sets of sense keys, one for each meaning of the pun. As in previous Senseval/SemEval word sense annotation tasks, annotators were permitted to select more than one sense key per meaning, or to indicate that the meaning was not listed in 1https://www.ukp.tu-darmstadt.de/data/senselabelling-resources/sense-annotated-englishpuns/ 2The only significant difference is that we removed several hundred of the contexts not containing puns and added them to our new heterographic data set.  <ref type="bibr" target="#b19">Krippendorff's (1980)</ref> α and a variation of the MASI set comparison metric <ref type="bibr" target="#b32">(Passonneau, 2006;</ref><ref type="bibr" target="#b26">Miller, 2016)</ref>, was 0.777. Disagreements were resolved automatically by taking the intersection of the corresponding sense sets; for contexts where this was not possible, the third judge manually adjudicated the disagreements. Of the 1607 puns, 1298 (81%) have both meanings in WordNet.</p><p>The second data set is similar to the first, except that the puns are heterographic rather than homographic. It was constructed in a similar manner, including the use of two annotators and an adjudicator. However, as heterographic puns have an extra level of complexity (it being sometimes necessary to discuss or explain an obscure joke before one "gets it"), the annotators were given an opportunity to resolve their disagreements themselves before passing the remainder on to the adjudicator. Pre-adjudication agreement for the sense annotations was α = 0.838. The final data set contains 1780 contexts, of which 1271 (71%) contain a pun. Of the puns, 1098 (86%) have both meanings in WordNet.</p><p>As described in the following section, the two data sets are used in three subtasks-pun detection, pun location, and pun interpretation. The pun detection subtask uses the full data sets, while the other two subtasks use subsets of the full data sets. Table <ref type="table">1</ref> presents some statistics on the size of each subtask's data set in terms of the number of contexts and word tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task definition</head><p>Participating systems competed in any or all of the following three subtasks, evaluated consecutively. Within each subtask, participants had the choice of running their system on either or both data sets. Subtask 1: Pun detection. For this subtask, participants were given an entire raw data set. For each context in the data set, the system had to decide whether or not it contains a pun. For example, take the following two contexts:</p><p>(2) I used to be a banker but I lost interest.</p><p>(3) What if there were no hypothetical questions?</p><p>For (2), the system should have returned "pun", whereas for (3) the system should have returned "non-pun". Systems had to classify all contexts in the data set. Scores were calculated using the standard precision, recall, accuracy, and F-score measures as used in classification <ref type="bibr">(Manning et al., 2008, §8.</ref>3):</p><formula xml:id="formula_0">P = TP TP + FP R = TP TP + FN A = TP + TN TP + TN + FP + FN F 1 = 2PR P + R</formula><p>where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. Subtask 2: Pun location. For this subtask, the contexts not containing puns were removed from the data sets. For any or all of the contexts, systems had to make a single guess as to which word is the pun. For example, given context (2) above, the system should have indicated that the tenth word, "interest", is the pun.</p><p>Scores were calculated using the standard coverage, precision, recall, and F-score measures as used in word sense disambiguation <ref type="bibr" target="#b31">(Palmer et al., 2007)</ref>:</p><formula xml:id="formula_1">C = # of guesses # of contexts P = # of correct guesses # of guesses R = # of correct guesses # of contexts F 1 = 2PR P + R .</formula><p>Note that, according to the above definitions, it is always the case that P ≥ R, and F 1 = P = R whenever P = R.</p><p>Subtask 3: Pun interpretation. For this subtask, the pun word in each context is marked, and contexts where the pun's two meanings are not found in WordNet are removed from the data sets. For any or all of the contexts, systems had to annotate the two meanings of the given pun by reference to WordNet sense keys. For example, given context (2), the system should have returned the WordNet sense keys interest%1:09:00:: (glossed as "a sense of concern with and curiosity about someone or something") and interest%1:21:00:: ("a fixed charge for borrowing money; usually a percentage of the amount borrowed").</p><p>As with the pun location subtask, scores were calculated using the coverage, precision, recall, and F-score measures from word sense disambiguation. A guess is considered to be "correct" if one of its sense lists is a non-empty subset of one of the sense lists from the gold standard, and the other of its sense lists is a non-empty subset of the other sense list from the gold standard. That is, the order of the two sense lists is not significant, nor is the order of the sense keys within each list. If the gold standard sense lists contain multiple senses, then it is sufficient for the system to correctly guess only one sense from each list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines</head><p>For each subtask, we provide results for various baselines:</p><p>Pun detection. The only baseline we use for this subtask is a random classifier. It makes no assumption about the underlying class distribution, labelling each context as "pun" or "non-pun" with equal probability. On average, its recall and accuracy will therefore be 0.5, and its precision equal to the proportion of contexts containing puns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pun location.</head><p>For this subtask we present the results of three naïve baselines. The first simply selects one of the context words at random. The second baseline always selects the last word of the context as a pun. It is informed by empirical studies of large joke corpora, which have found that punchlines tend to occur in a terminal position <ref type="bibr" target="#b1">(Attardo, 1994)</ref>. The third baseline is a slightly more sophisticated pun location baseline inspired by <ref type="bibr" target="#b23">Mihalcea et al. (2010)</ref>. In that study, genuine joke punchlines were selected among several non-humorous alternatives by finding the candidate whose words have the highest mean polysemy. We adapt this technique by selecting as the pun the word with the highest polysemy (counting together senses from all parts of speech). In the case of a tie, we choose the most polysemous word nearest to the end of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pun interpretation.</head><p>Following the practice in traditional word sense disambiguation, we present the results of the random and most frequent sense baselines, as adapted to pun annotation.</p><p>The random baseline attempts to lemmatize the pun word, looks it up in WordNet, and selects two of its senses at random, one for each meaning of the pun. It scores</p><formula xml:id="formula_2">P = R = 1 n n i=1 G i 1 • G i 2 S i 2 ,</formula><p>where n is the number of contexts, G i j is the number of gold-standard sense keys in the jth meaning of the pun word in context i, and S i is the number of sense keys WordNet contains for the pun word in context i. We compute the random baseline only for the homographic data set. (It would in principle be adaptable to the heterographic data set, though the large number of potential target words means the scores would be negligible.)</p><p>The most frequent sense (MFS) baseline is a supervised baseline in that it depends on a manually sense-annotated background corpus. As its name suggests, it involves always selecting from the candidates that sense that has the highest frequency in the corpus. For the homographic data set, our MFS implementation attempts to lemmatize the pun word (if necessary, building a list of candidate lemmas) and then selects the two most frequent senses of these lemmas according to WordNet's built-in sense frequency counts.3 For the heterographic data set, only the first sense is selected from the list of candidate lemmas. A second list is constructed by finding all other lemmas in WordNet with the minimum <ref type="bibr" target="#b21">Levenshtein (1966)</ref> distance to the lemmas in the first list. The most frequent sense of the lemmas in the second list is selected as the second meaning of the pun.</p><p>In addition to the two naïve baselines, we also provide scores for the homographic pun interpretation system described by <ref type="bibr" target="#b27">Miller and Gurevych (2015)</ref>. This system works by running each pun through a variation of the <ref type="bibr" target="#b20">Lesk (1986)</ref> algorithm that scores each candidate sense according to the lexical overlap with the pun's context. The two top-scoring senses are then selected; in case of ties, the system attempts to select senses which are not closely related to each other, and at least one of whose parts of speech matches the one applied to the pun by a POS tagger.</p><p>The baseline pun interpretation scores presented in this paper differ slightly from those given in <ref type="bibr" target="#b27">Miller and Gurevych (2015)</ref> and <ref type="bibr" target="#b26">Miller (2016)</ref>. This is because the scoring program used in those studies compared sense keys on the basis of their underlying WordNet synsets, whereas in this shared task the sense keys are compared directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participating systems</head><p>Our shared task saw participation from ten systems: BuzzSaw (Oele and Evang, 2017). BuzzSaw assumes that each meaning of the pun will exhibit high semantic similarity with one and only one part of the context. The system's approach to homographic pun interpretation is to compute the semantic similarity between the two halves of every possible contiguous, binary partitioning of the context, retaining the partitioning with the lowest similarity between the two parts. A Lesk-like WSD algorithm based on word and sense embeddings is then used to disambiguate the pun word separately with respect to each part of the context.</p><p>The pun interpretation system is also used for homographic pun location. First, the interpretation system is run once for each polysemous word in the context. The word whose two disambiguated senses have maximum cosine distance between their sense embeddings is selected as the pun word.</p><p>Duluth <ref type="bibr" target="#b33">(Pedersen, 2017)</ref>. For pun detection, the Duluth system assumes that all-words WSD systems will have difficulties in consistently assigning sense labels to contexts containing puns. The system therefore disambiguates each context with four slightly different configurations of the same WSD algorithm. If more than two sense labels differ across runs, the context is assumed to contain a pun. For pun location, the system selects the word whose sense label changed across runs; if multiple words changed senses, then the system selects the one closest to the end of the context.</p><p>Homographic pun interpretation is carried out by running various configurations of a WSD algorithm on the pun word and selecting the two most frequently returned senses. For heterographic puns, the system attempts to recover the target form either by generating a list of WordNet lemmas with minimal edit distance to the pun word, or by querying the Datamuse API for words with similar spellings, pronunciations, and meanings. WSD algorithms are then run separately on the pun and the set of target candidates, with the best matching pun and target senses retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECNU (Xiu et al., 2017</head><p>). ECNU uses a supervised approach to pun detection. The authors collected a training set of 60 homographic and 60 heterographic puns, plus 60 proverbs and famous sayings, from various Web sources. The data is then used to train a classifier, using features derived from Word-Net and word2vec embeddings. The ECNU pun locator is knowledge-based, determining each context word's likelihood of being the pun on the basis of the distance between its sense vectors, or between its senses and the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELiRF-UPV (Hurtado et al., 2017</head><p>). This system's approach to homographic pun location rests on two hypotheses: that the pun will be semantically very similar to one of the non-adjacent words in the sentence, and that the pun will be located near the end of the sentence. The system therefore calculates the similarity between every pair of non-adjacent words in the context using word2vec, retaining the pair with the highest similarity. The word in the pair that is closer to the end of the context is selected as the pun.</p><p>To interpret homographic puns, ELiRF-UPV first finds the two context words whose word embeddings are closest to that of the pun.</p><p>Then, for each context word, the system builds a bag-of-words representation for each of its candidate senses, and for each of the pun word's candidate senses, using information from WordNet. The lexical overlap between every pair of pun and context senses is calculated, and the pun sense with the highest overlap is selected as one of the meanings of the pun.</p><p>Fermi (Indurthi and Oota, 2017). Fermi takes a supervised approach to the detection of homographic puns. Unlike ECNU, the authors did not construct their own data set of puns, but rather split the shared task data set into separate training and test sets, the first of which they manually annotated. A bi-directional RNN then learns a classification model, using distributed word embeddings as input features.</p><p>Fermi's approach to pun location is a knowledge-based approach similar to that of ELiRF-UPV. For every pair of words in the context, a similarity score is calculated on the basis of the maximum pairwise similarity of their WordNet synsets. In the highest-scoring pair, the word closest to the end of the context is selected as the pun. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PunFields (Mikhalkova and Karyakin, 2017).</head><p>PunFields uses separate methods for pun detection, location, and interpretation; central to all of them is the notion of semantic fields. The system's approach to pun detection is a supervised one, with features being vectors tabulating the number of words in the context that appear in each of the 34 sections of Roget's Thesaurus. For pun location, PunFields uses a weakly supervised approach that scores candidates on the basis of their presence in Roget's sections, their position within the context, and their part of speech.</p><p>For pun interpretation, the system partitions the context on the basis of semantic fields, and then selects as the first sense of the pun the one whose WordNet gloss has the greatest number of words in common with the first partition. For homographic puns, the second sense selected is the one with the highest frequency count in WordNet (or the next-highest frequency count, in case the first selected sense already has the highest frequency). For heterographic puns, a list of candidate target words is produced using <ref type="bibr">Damerau-Levenshtein (1964)</ref> distance. Among their corresponding Word-Net senses, the system selects the one whose definition has the highest lexical overlap with the second partition.</p><p>UWaterloo <ref type="bibr" target="#b44">(Vechtomova, 2017)</ref>. UWaterloo is a rule-based pun locator that scores candidate words according to eleven simple heuristics. These heuristics involve the position of the word within the context or relative to certain punctuation or function words, the word's inverse document frequency in a large reference corpus, normalized pointwise mutual information (PMI) with other words in the context, and whether the word exists in a reference set of homophones and similar-sounding words.</p><p>Only words in the second half of the context are scored; in the event of a tie, the system chooses the word closer to the end of the context.</p><p>UWAV <ref type="bibr" target="#b43">(Vadehra, 2017)</ref>. UWAV participated in the pun detection and location subtasks. The detection component is another supervised system, taking the votes of three classifiers (support vector machine, naïve Bayes, and logistic regression) trained on lexical-semantic and word embedding features of a manually annotated data set.</p><p>For pun location, UWAV splits the context in half and checks whether any word in the second half is in some predefined lists of homonyms, homophones, and antonyms. If so, one of those words is selected as the pun. Otherwise, word2vec similarity is calculated between every pair of words in the context. In the highestscoring word pair, the word closest to the end of the context is selected.</p><p>One further team submitted answers after the official evaluation period was over: N-Hance <ref type="bibr" target="#b41">(Sevgili et al., 2017)</ref>. The N-Hance system assumes every pun has a particularly strong association with exactly one other word in the context. To detect and locate puns, then, it calculates the PMI between every pair of words in the context. If the PMI of the highest-scoring pair exceeds a certain threshold relative to the other pairs' PMI scores, then the context is assumed to contain a pun, with the pun being the word in the pair closest to the end of the context. Otherwise, the context is assumed to have no pun.</p><p>For homographic pun interpretation, the first sense is selected by finding the maximum overlap between the candidate sense definitions and the pun's context. N-Hance then finds the word in the context that has the highest PMI score with the pun. The system selects as the second sense of the pun that sense whose synonyms have the greatest word2vec cosine similarity with the paired word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and analysis</head><p>Tables <ref type="table" target="#tab_2">2 through 4</ref> show the results for each of the three subtasks and two data sets. Results for the participating systems are shown in the upper section of each table; the lower section shows the baselines and the N-Hance system entered out of competition. Pun detection results for ECNU and Fermi are also in the non-competition section, since their training data, by accident or design, included some contexts from the test data. To calculate the pun detection scores for these two systems, we first removed the overlapping contexts from the test set.4 The PunFields pun locator is also marked 4Two further supervised pun detection systems, UWAV and Punfields, were found to have inadvertently used training contexts that also appear in the test data. In these two cases, however, the authors removed the overlapping contexts from as it makes use of POS frequency counts of the homographic data set that were published in <ref type="bibr" target="#b27">Miller and Gurevych (2015)</ref>.</p><p>For each metric, the result of the best-performing participating system is shown in boldface. Where a baseline or non-competition entry matched or outperformed the best participating system, its result is also shown in boldface. Generally only the bestscoring run submitted by each system is shown;5 we have made an exception for Duluth's Datamuse-and edit distance-based pun interpretation variations ("DM" and "ED", respectively), neither of which outperformed the other on all metrics. Subtask 1: Pun detection. No one system emerged as the clear winner for this subtask, making it hard to draw conclusions on what approaches work best. Among the participating systems for the homographic data set, Punfields achieved the highest precision (0.7993), JU_CSE_NLP the highest recall (0.9079), and Duluth the highest accuracy and F-score (0.7364 and 0.8254, respectively). N-Hance equalled or outperformed the participating systems on recall, accuracy, and F-score. For the heterographic data set, Idiom Savant had the highest precision, accuracy, and F-score (0.8704, 0.7837, and 0.8439, respectively), while JU_CSE_NLP achieved the best recall (0.9402). N-Hance performed about as well as Idiom Savant in terms of F-Score (0.8440). For both data sets, all systems outperformed the random baseline.</p><p>Subtask 2: Pun location. The last word baseline (F 1 = 0.4704 and 0.5704 for homographic and heterographic puns, respectively) turned out to be surprisingly hard to beat for this subtask. For the homographic data set, this baseline was exceeded only by Idiom Savant (F 1 = 0.6631) and UWaterloo (F 1 = 0.6523). For the heterographic puns, it was bested only by Idiom Savant (F 1 = 0.6845), UWaterloo (F 1 = 0.7964), and N-Hance (F 1 = 0.6553).</p><p>Idiom Savant was not the only system to measure semantic relatedness via word2vec, though it was the only one to do so with n-grams from a large background corpus. It was also the only system to directly (albeit simplistically) measure phonetic their training data, retrained their systems, and submitted new results, which we report here.</p><p>5Participants were permitted to submit the results of up to two runs for each subtask and data set. The intention was to allow participants the opportunity to fix problems in the formatting of their output files, or to try minor variations of the same system.    distance using a pronunciation dictionary, and the only system that flagged puns of a certain genre for special processing. These features, alone or in combination, may have contributed to the system's success. UWaterloo and N-Hance were the only systems making use of pointwise mutual information, to which their success might be credited. Evidently the notion of a unique "trigger" word in the context that activates the pun is an important one to model. UWaterloo also shares with Idiom Savant the use of hand-crafted rules based on real-world knowledge of punning jokes. Subtask 3: Pun interpretation. As in the pun detection subtask, no one approach worked best here, at least for the homographic data set. Only two systems (BuzzSaw and Duluth) were able to beat the most frequent sense baseline. The <ref type="bibr" target="#b27">Miller and Gurevych (2015)</ref> system remains the bestperforming pun interpreter in terms of precision (0.1975) and F-score (0.1603), though BuzzSaw was able to exceed it in terms of recall (0.1525). Both BuzzSaw and <ref type="bibr" target="#b27">Miller and Gurevych (2015)</ref> apply Lesk-like algorithms to "disambiguate" the pun word. However, lexical overlap approaches are also used by most of the lower-performing systems. For heterographic pun interpretation, Idiom Savant achieved the highest scores (P = 0.0842, R = 0.0710, F 1 = 0.0771), though its recall is not much higher than the most frequent sense baseline (0.0701).</p><p>It seems that for probabilistic approaches like those submitted, classifying texts as puns and, to a lesser degree, pinpointing the punning lexical material are easier than actual semantic tasks like our Subtask 3. This may be because probabilistic approaches cannot, in principle, see past the arbitrariness of the linguistic sign, instead relying on context to reflect meaning. We assume that producing a full semantic analysis in terms of a knowledge-based system, akin to those proposed in <ref type="bibr" target="#b3">Bar-Hillel's (1960)</ref> famous evaluation of fully automatic high-quality translation, might be necessary, because only these approaches can get beyond observed shared features to natural language meaning. Such knowledge-based approaches to meaning in humour, based on relevant semantic humour theories <ref type="bibr" target="#b35">(Raskin, 1985;</ref><ref type="bibr" target="#b2">Attardo and Raskin, 1991)</ref>, have been in development since <ref type="bibr" target="#b37">Raskin et al. (2009)</ref> and one recent (albeit non-scalable) approach, <ref type="bibr" target="#b16">Kao et al. (2015)</ref>, has already shown very interesting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding remarks</head><p>In this paper we have introduced SemEval-2017 Task 7, the first shared task for the computational processing of puns. We have described the rules for three subtasks-pun detection, pun location, and pun interpretation-and described the manually annotated data sets used for their evaluation. Both data sets are now freely available for use by the research community. We have also described the approaches and presented the results of ten participating teams, as well as several baseline algorithms and a further system entered out of competition.</p><p>We observe most systems performed well on the pun detection task, with F-scores in the range of 0.5587 to 0.8440. However, only a few systems beat a simple baseline on pun location. Pun interpretation remains an extremely challenging problem, with most systems failing to exceed the baselines, and with sense assignment accuracy much lower than what is seen with traditional word sense disambiguation. Interestingly, though there exists a considerable body of research in linguistics on phonological models of punning <ref type="bibr" target="#b12">(Hempelmann and Miller, 2017)</ref> and on semantic theories of humour <ref type="bibr">(Raskin, 2008)</ref>, little to none of this work appeared to inform the participating systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>words / context pun type subtask contexts words min mean max</head><label></label><figDesc></figDesc><table><row><cell>homographic detection</cell><cell>2 250 24 499</cell><cell>2</cell><cell>10.9</cell><cell>44</cell></row><row><cell>homographic location</cell><cell>1 607 18 998</cell><cell>3</cell><cell>11.8</cell><cell>44</cell></row><row><cell>homographic interpretation</cell><cell>1 298 15 510</cell><cell>3</cell><cell>11.9</cell><cell>44</cell></row><row><cell>heterographic detection</cell><cell>1 780 19 461</cell><cell>2</cell><cell>10.9</cell><cell>69</cell></row><row><cell>heterographic location</cell><cell>1 271 15 145</cell><cell>3</cell><cell>11.9</cell><cell>69</cell></row><row><cell>heterographic interpretation</cell><cell>1 098 13 258</cell><cell>3</cell><cell>12.1</cell><cell>69</cell></row><row><cell cols="2">Table 1: Data set statistics</cell><cell></cell><cell></cell><cell></cell></row><row><cell>WordNet. Interannotator agreement, as measured</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>by</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">: Pun detection results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">homographic</cell><cell></cell><cell></cell><cell cols="2">heterographic</cell><cell></cell></row><row><cell>system</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell>BuzzSaw</cell><cell cols="4">1.0000 0.2775 0.2775 0.2775</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Duluth</cell><cell cols="8">1.0000 0.4400 0.4400 0.4400 1.0000 0.5311 0.5311 0.5311</cell></row><row><cell>ECNU</cell><cell cols="8">1.0000 0.3373 0.3373 0.3373 1.0000 0.5681 0.5681 0.5681</cell></row><row><cell>ELiRF-UPV</cell><cell cols="4">1.0000 0.4462 0.4462 0.4462</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fermi</cell><cell cols="4">1.0000 0.5215 0.5215 0.5215</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Idiom Savant</cell><cell cols="8">0.9988 0.6636 0.6627 0.6631 1.0000 0.6845 0.6845 0.6845</cell></row><row><cell cols="9">JU_CSE_NLP 1.0000 0.3348 0.3348 0.3348 1.0000 0.3792 0.3792 0.3792</cell></row><row><cell>PunFields  ‡</cell><cell cols="8">1.0000 0.3279 0.3279 0.3279 1.0000 0.3501 0.3501 0.3501</cell></row><row><cell>UWaterloo</cell><cell cols="8">0.9994 0.6526 0.6521 0.6523 0.9976 0.7973 0.7954 0.7964</cell></row><row><cell>UWAV</cell><cell cols="8">1.0000 0.3410 0.3410 0.3410 1.0000 0.4280 0.4280 0.4280</cell></row><row><cell>random</cell><cell cols="8">1.0000 0.0846 0.0846 0.0846 1.0000 0.0839 0.0839 0.0839</cell></row><row><cell>last word</cell><cell cols="8">1.0000 0.4704 0.4704 0.4704 1.0000 0.5704 0.5704 0.5704</cell></row><row><cell cols="9">max. polysemy 1.0000 0.1798 0.1798 0.1798 1.0000 0.0110 0.0110 0.0110</cell></row><row><cell>N-Hance</cell><cell cols="8">0.9956 0.4269 0.4250 0.4259 0.9882 0.6592 0.6515 0.6553</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Pun location results</figDesc><table><row><cell></cell><cell></cell><cell cols="2">homographic</cell><cell></cell><cell></cell><cell cols="2">heterographic</cell><cell></cell></row><row><cell>system</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>C</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell>BuzzSaw</cell><cell cols="4">0.9761 0.1563 0.1525 0.1544</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Duluth (DM)</cell><cell cols="8">0.8606 0.1683 0.1448 0.1557 0.9791 0.0009 0.0009 0.0009</cell></row><row><cell>Duluth (ED)</cell><cell cols="8">0.9992 0.1480 0.1479 0.1480 0.9262 0.0315 0.0291 0.0303</cell></row><row><cell>ELiRF-UPV</cell><cell cols="4">0.9646 0.1014 0.0978 0.0996</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Idiom Savant</cell><cell cols="8">0.9900 0.0778 0.0770 0.0774 0.8434 0.0842 0.0710 0.0771</cell></row><row><cell>PunFields</cell><cell cols="8">0.8760 0.0484 0.0424 0.0452 0.9709 0.0169 0.0164 0.0166</cell></row><row><cell>random</cell><cell cols="4">1.0000 0.0931 0.0931 0.0931</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MFS</cell><cell cols="8">1.0000 0.1348 0.1348 0.1348 0.9800 0.0716 0.0701 0.0708</cell></row><row><cell cols="5">Miller &amp; Gurevych 0.6826 0.1975 0.1348 0.1603</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>N-Hance</cell><cell cols="4">0.9831 0.0204 0.0200 0.0202</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Pun interpretation results</figDesc><table /><note>* Evaluated on 2237 of the 2250 homographic contexts, and 1778 of the 1780 heterographic contexts.† Evaluated on 675 of the 2250 homographic contexts. ‡ Uses POS frequency counts from the homographic test set.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3These counts come from the SemCor<ref type="bibr" target="#b25">(Miller et al., 1993)</ref> corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Institute for Educational Research (DIPF). The authors thank Edwin Simpson for helping build the heterographic data set.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Routledge Handbook of Language and Humor</title>
		<author>
			<persName><forename type="first">Debra</forename><surname>Aarons</surname></persName>
		</author>
		<editor>Salvatore Attardo</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="80" to="94" />
			<pubPlace>Routledge, New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Puns and tacit linguistic knowledge. Routledge Handbooks in Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Linguistic Theories of Humor</title>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Attardo</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110219029</idno>
		<ptr target="https://doi.org/10.1515/9783110219029" />
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Mouton de Gruyter</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Script theory revis(it)ed: Joke similarity and joke representation model</title>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Attardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Raskin</surname></persName>
		</author>
		<idno type="DOI">10.1515/humr.1991.4.3-4.293</idno>
		<ptr target="https://doi.org/10.1515/humr.1991.4.3-4.293" />
	</analytic>
	<monogr>
		<title level="j">Humor: International Journal of Humor Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="293" to="348" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The present status of automatic translation of languages</title>
		<author>
			<persName><forename type="first">Yehoshua</forename><surname>Bar-Hillel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computers</title>
				<editor>
			<persName><forename type="first">Franz</forename><forename type="middle">L</forename><surname>Alt</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1960" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An implemented model of punning riddles</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth National Conference on Artificial Intelligence: AAAI-94</title>
				<meeting>the Twelfth National Conference on Artificial Intelligence: AAAI-94</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="633" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational rules for generating punning riddles</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<idno type="DOI">10.1515/humr.1997.10.1.25</idno>
		<ptr target="https://doi.org/10.1515/humr.1997.10.1.25" />
	</analytic>
	<monogr>
		<title level="j">Humor: International Journal of Humor Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="76" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<idno type="DOI">10.1145/363958.363994</idno>
		<ptr target="https://doi.org/10.1145/363958.363994" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Samuel Doogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyang</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Veale</surname></persName>
		</author>
		<idno>SemEval-2017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Idiom Savant at</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task 7: Detection and interpretation of English puns</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Paronomasic Puns: Target Recoverability Towards Automatic Generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><surname>Hempelmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>West Lafayette, IN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Purdue University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YPS -The Ynperfect Pun Selector for computational humor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><surname>Hempelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI 2003 Workshop on Humor Modeling in the Interface</title>
				<meeting>the CHI 2003 Workshop on Humor Modeling in the Interface</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computational humor: Beyond the pun? In Victor Raskin, editor, The Primer of Humor Research</title>
		<author>
			<persName><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><surname>Hempelmann</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110198492.333</idno>
		<ptr target="https://doi.org/10.1515/9783110198492.333" />
		<imprint>
			<date type="published" when="2008" />
			<publisher>Mouton de Gruyter</publisher>
			<biblScope unit="page" from="333" to="360" />
			<pubPlace>Berlin, number 8 in Humor Research</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Routledge Handbook of Language and Humor, Routledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<editor>Salvatore Attardo</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="108" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Puns: Taxonomy and phonology. Routledge Handbooks in Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically extracting word relationships as templates for pun generation</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethel</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Approaches to Linguistic Creativity: Proceedings of the Workshop</title>
				<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ELiRF-UPV at SemEval-2017 Task 7: Pun detection and interpretation</title>
		<author>
			<persName><forename type="first">-</forename><forename type="middle">F</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Encarna</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascual Andrés Carrasco</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José Ángel</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fermi at SemEval-2017 Task 7: Detection and interpretation of homographic puns in English language</title>
		<author>
			<persName><forename type="first">Vijayasaradhi</forename><surname>Indurthi</surname></persName>
		</author>
		<author>
			<persName><surname>Subba Reddy Oota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="456" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A computational model of linguistic humor in puns</title>
		<author>
			<persName><forename type="first">Justine</forename><forename type="middle">T</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12269</idno>
		<ptr target="https://doi.org/10.1111/cogs.12269" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1270" to="1285" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Shigeto</forename><surname>Kawahara</surname></persName>
		</author>
		<ptr target="http://user.keio.ac.jp/~kawahara/pdf/punbook.pdf" />
		<title level="m">Papers on Japanese imperfect puns. Online collection of previously published journal and conference articles</title>
				<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Development of Shakespeare&apos;s Rhetoric: A Study of Nine Plays. Number 136 in Swiss Studies in English</title>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">Daniel</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Narr, Tübingen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Content Analysis: An Introduction to Its Methodology. Number 5 in The Sage CommText Series</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Sage Publications</publisher>
			<pubPlace>Beverly Hills, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
		<idno type="DOI">10.1145/318723.318728</idno>
		<ptr target="https://doi.org/10.1145/318723.318728" />
	</analytic>
	<monogr>
		<title level="m">SIGDOC &apos;86: Proceedings of the 5th Annual International Conference on Systems Documentation</title>
				<editor>
			<persName><forename type="first">Virginia</forename><surname>De-Buys</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computational models for incongruity detection in humour</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-12116-6_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-12116-6_30" />
	</analytic>
	<monogr>
		<title level="m">Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing: 11th International Conference, CIC-Ling</title>
				<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
	<note>Theoretical Computer Science and General Issues</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PunFields at SemEval-2017 Task 7: Employing Roget&apos;s Thesaurus in automatic pun recognition and interpretation</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mikhalkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Karyakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075671.1075742</idno>
		<ptr target="https://doi.org/10.3115/1075671.1075742" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey</title>
				<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adjusting Sense Representations for Word Sense Disambiguation and Automatic Pun Interpretation</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Technische Universität Darmstadt</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dr.-Ing. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic disambiguation of English puns</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing: Proceedings of the Conference</title>
				<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards the automatic detection and identification of English puns</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Turković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Humour Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effects of humor in task-oriented human-computer interaction and computer-mediated communication: A direct test of SRCT theory</title>
		<author>
			<persName><forename type="first">John</forename><surname>Morkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hadyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Kernal</surname></persName>
		</author>
		<author>
			<persName><surname>Nass</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15327051HCI1404_2</idno>
		<ptr target="https://doi.org/10.1207/S15327051HCI1404_2" />
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="435" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BuzzSaw at SemEval-2017 Task 7: Global vs. local context for interpreting and locating homographic English puns with sense embeddings</title>
		<author>
			<persName><forename type="first">Dieke</forename><surname>Oele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="443" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation of wsd systems</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Word Sense Disambiguation: Algorithms and Applications</title>
				<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
			<persName><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="75" to="106" />
		</imprint>
	</monogr>
	<note>number 33 in Text, Speech, and Language Technology</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Edition of the International Conference on Language Resources and Evaluation</title>
				<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="831" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Duluth at SemEval-2017 Task 7: Puns upon a midnight dreary, lexical semantics for the weak and weary</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">JU_CSE_NLP at SemEval-2017 Task 7: Employing rules to detect and interpret English puns</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Pramanick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="431" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Number 24 in Studies in Linguistics and Philosophy</title>
		<author>
			<persName><surname>Victor Raskin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-009-6472-3</idno>
		<ptr target="https://doi.org/10.1007/978-94-009-6472-3" />
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Semantic Mechanisms of Humor</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Primer of Humor Research. Number 8 in Humor Research</title>
		<idno type="DOI">10.1515/9783110198492</idno>
		<ptr target="https://doi.org/10.1515/9783110198492" />
		<editor>Victor Raskin</editor>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Mouton de Gruyter</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How to understand and assess a theory: The evolution of SSTH into the GTVH and now into the OSTH</title>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Victor Raskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">M</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1515/JLT.2009.016</idno>
		<ptr target="https://doi.org/10.1515/JLT.2009.016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Literary Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="312" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Computational mechanisms for pun generation</title>
		<author>
			<persName><forename type="first">Graeme</forename><forename type="middle">D</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Workshop on Natural Language Generation (ENLG-05)</title>
				<editor>
			<persName><forename type="first">Graham</forename><surname>Wilcock</surname></persName>
			<persName><forename type="first">Kristiina</forename><surname>Jokinen</surname></persName>
			<persName><forename type="first">Chris</forename><surname>Mellish</surname></persName>
			<persName><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</editor>
		<meeting>the 10th European Workshop on Natural Language Generation (ENLG-05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Dictionary of Shakespeare&apos;s Sexual Puns and Their Significance. Macmillan</title>
		<author>
			<persName><forename type="first">Frankie</forename><surname>Rubinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Shun the Pun, Rescue the Rhyme? The Dubbing and Subtitling of Languageplay in Film</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Schröter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Karlstad University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">N-Hance at SemEval-2017 Task 7: A computational approach using word association for puns</title>
		<author>
			<persName><forename type="first">Özge</forename><surname>Sevgili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Ghotbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selma</forename><surname>Tekir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="435" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Computationally recognizing wordplay in jokes</title>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">J</forename><surname>Mazlack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth Annual Conference of the Cognitive Science Society</title>
				<editor>
			<persName><forename type="first">Kenneth</forename><surname>Forbus</surname></persName>
			<persName><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
			<persName><forename type="first">Terry</forename><surname>Regier</surname></persName>
		</editor>
		<meeting>the Twenty-sixth Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1315" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">UWAV at SemEval-2017 Task 7: Automated feature-based system for locating puns</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Vadehra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="448" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">UWaterloo at SemEval-2017 Task 7: Locating the pun using syntactic characteristics and corpus-based metrics</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="420" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluating the STANDUP pun generating software with children with cerebral palsy</title>
		<author>
			<persName><forename type="first">Annalu</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>O'mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<idno type="DOI">10.1145/1497302.1497306</idno>
		<ptr target="https://doi.org/10.1145/1497302.1497306" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2017 Task 7: Using supervised and unsupervised methods to detect and locate English puns</title>
		<author>
			<persName><forename type="first">Yuhuan</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
				<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Japanese pun analyzer using articulation similarities</title>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yokogawa</surname></persName>
		</author>
		<idno type="DOI">10.1109/FUZZ.2002.1006660</idno>
		<ptr target="https://doi.org/10.1109/FUZZ.2002.1006660" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 IEEE International Conference on Fuzzy Systems: FUZZ 2002</title>
				<meeting>the 2002 IEEE International Conference on Fuzzy Systems: FUZZ 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1114" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
