<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitar</forename><surname>Dimitrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sofia University &quot;St. Kliment Ohridski&quot;</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bishr</forename><forename type="middle">Bin</forename><surname>Ali</surname></persName>
							<email>bishrkc@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
							<email>sshaar@hbku.edu.qa</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
							<email>fialam@hbku.edu.qa</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
							<email>fsilvestri@diag.uniroma1.it</email>
							<affiliation key="aff3">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamed</forename><surname>Firooz</surname></persName>
							<email>mhfirooz@fb.com</email>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<email>pnakov@hbku.edu.qa</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
						</author>
						<author>
							<persName><forename type="first">San</forename><surname>Martino</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Padova</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image. It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Internet and social media have amplified the impact of disinformation campaigns. Traditionally a monopoly of states and large organizations, now such campaigns have become within the reach of even small organisations and individuals <ref type="bibr" target="#b13">(Da San Martino et al., 2020b)</ref>.</p><p>Such propaganda campaigns are often carried out using posts spread on social media, with the aim to reach very large audience. While the rhetorical and the psychological devices that constitute the basic building blocks of persuasive messages have been thoroughly studied <ref type="bibr" target="#b46">(Miller, 1939;</ref><ref type="bibr" target="#b65">Weston, 2008;</ref><ref type="bibr" target="#b63">Torok, 2015)</ref>, only few isolated efforts have been made to devise automatic systems to detect them .</p><p>WARNING: This paper contains meme examples and wording that might be offensive to some readers. Thus, in 2020, we proposed SemEval-2020 task 11 on Detection of Persuasion Techniques in News Articles, with the aim to help bridge this gap <ref type="bibr" target="#b12">(Da San Martino et al., 2020a)</ref>. The task focused on text only. Yet, some of the most influential posts in social media use memes, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, <ref type="bibr">1</ref> where visual cues are being used, along with text, as a persuasive vehicle to spread disinformation <ref type="bibr" target="#b59">(Shu et al., 2017)</ref>. During the 2016 US Presidential campaign, malicious users in social media (bots, cyborgs, trolls) used such memes to provoke emotional responses <ref type="bibr" target="#b24">(Guo et al., 2020)</ref>.</p><p>In 2021, we introduced a new SemEval shared task, for which we prepared a multimodal corpus of memes annotated with an extended set of techniques, compared to SemEval-2020 task 11. This time, we annotated both the text of the memes, highlighting the spans in which each technique has been used, as well as the techniques appearing in the visual content of the memes.</p><p>Based on our annotations, we offered the following three subtasks: Subtask 1 (ST1) Given the textual content of a meme, identify which techniques (out of 20 possible ones) are used in it. This is a multilabel classification problem.</p><p>Subtask 2 (ST2) Given the textual content of a meme, identify which techniques (out of 20 possible ones) are used in it together with the span(s) of text covered by each technique. This is a multilabel sequence tagging task.</p><p>Subtask 3 (ST3) Given a meme, identify which techniques (out of 22 possible ones) are used in the meme, considering both the text and the image. This is a multilabel classification problem.</p><p>A total of 71 teams registered for the task, 22 of them made an official submission on the test set and 15 of the participating teams submitted a system description paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Propaganda Detection Previous work on propaganda detection has focused on analyzing textual content <ref type="bibr" target="#b10">(Barrón-Cedeno et al., 2019;</ref><ref type="bibr" target="#b53">Rashkin et al., 2017)</ref>. See  for a recent survey on computational propaganda detection. <ref type="bibr" target="#b53">Rashkin et al. (2017)</ref> developed the TSHP-17 corpus, which had document-level annotations with four classes: trusted, satire, hoax, and propaganda. Note that TSHP-17 was labeled using distant supervision, i.e., all articles from a given news outlet were assigned the label of that news outlet. The news articles were collected from the English Gigaword corpus (which covers reliable news sources), as well as from seven unreliable news sources, including two propagandistic ones. They trained a model using word n-grams, and reported that it performed well only on articles from sources that the system was trained on, and that the performance degraded quite substantially when evaluated on articles from unseen news sources. <ref type="bibr" target="#b10">Barrón-Cedeno et al. (2019)</ref> developed a corpus QProp with two labels (propaganda vs. non-propaganda), and experimented with two corpora: TSHP-17 and QProp . They binarized the labels of TSHP-17 as follows: propaganda vs. the other three categories.</p><p>They performed massive experiments, investigated writing style and readability level, and trained models using logistic regression and SVMs. Their findings confirmed that using distant supervision, in conjunction with rich representations, might encourage the model to predict the source of the article, rather than to discriminate propaganda from non-propaganda. The study by <ref type="bibr" target="#b27">Habernal et al. (2017</ref> also proposed a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring, and irrelevant authority, which directly relate to propaganda techniques.</p><p>A more fine-grained propaganda analysis was done by Da San , who developed a corpus of news articles annotated with the spans of use of 18 propaganda techniques, from an invetory they put together. They targeted two tasks: (i) binary classification -given a sentence, predict whether any of the techniques was used in it; and (ii) multi-label multi-class classification and span detection task -given a raw text, identify both the specific text fragments where a propaganda technique is being used as well as the type of technique. They further proposed a multigranular gated deep neural network that captures signals from the sentence-level task to improve the performance of the fragment-level classifier and vice versa. Subsequently, an automatic system, Prta, was developed and made publicly available <ref type="bibr" target="#b14">(Da San Martino et al., 2020c)</ref>, which performs fine-grained propaganda analysis of text using these 18 fine-grained propaganda techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Content</head><p>Another line of related research is on analyzing multimodal content, e.g., for predicting misleading information , for detecting deception <ref type="bibr" target="#b22">(Glenski et al., 2019)</ref>, emotions and propaganda <ref type="bibr" target="#b0">(Abd Kadir et al., 2016)</ref>, hateful memes <ref type="bibr">(Kiela et al., 2020)</ref>, and propaganda in images <ref type="bibr" target="#b55">(Seo, 2014)</ref>.  developed a corpus of 500K Twitter posts consisting of images and labeled with six classes: disinformation, propaganda, hoaxes, conspiracies, clickbait, and satire. <ref type="bibr" target="#b22">Glenski et al. (2019)</ref> explored multilingual multimodal content for deception detection. Multimodal hateful memes were the target of the Hateful Memes Challenge, which was addressed by fine-tuning state-of-art methods such as ViLBERT <ref type="bibr" target="#b40">(Lu et al., 2019)</ref>, Multimodal Bitransformers <ref type="bibr" target="#b34">(Kiela et al., 2019)</ref>, and VisualBERT <ref type="bibr" target="#b37">(Li et al., 2019)</ref> to classify hateful vs. not-hateful memes <ref type="bibr">(Kiela et al., 2020)</ref>.</p><p>Related Shared Tasks The present shared task is closely related to SemEval-2020 task 11 on Detection of Persuasion Techniques in News Articles <ref type="bibr" target="#b12">(Da San Martino et al., 2020a)</ref>, which focused on news articles, and asked (i) to detect the spans where propaganda techniques are used, as well as (ii) to predict which propaganda technique (from an inventory of 14 techniques) is used in a given text span. Another closely related shared task is the NLP4IF-2019 task on Fine-Grained Propaganda Detection, which asked to detect the spans of use in news articles of each of 18 propaganda techniques . While these tasks focused on the text of news articles, here we target memes and multimodality, and we further use an extended inventory of 22 propaganda techniques.</p><p>Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification <ref type="bibr" target="#b61">(Thorne et al., 2018)</ref>, the SemEval 2017 and 2019 tasks on predicting the veracity of rumors in Twitter <ref type="bibr" target="#b16">(Derczynski et al., 2017;</ref><ref type="bibr" target="#b23">Gorrell et al., 2019)</ref>, the SemEval-2019 task on Fact-Checking in Community Question Answering Forums <ref type="bibr" target="#b44">(Mihaylova et al., 2019)</ref>, the NLP4IF-2021 shared task on Fighting the COVID-19 Infodemic <ref type="bibr">(Shaar et al., 2021)</ref>. We should also mention the CLEF 2018-2021 CheckThat! lab <ref type="bibr">Elsayed et al., 2019a,b;</ref>, which featured tasks on automatic identification  and verification <ref type="bibr" target="#b30">Hasanain et al., 2019</ref><ref type="bibr">Nakov et al., 2021)</ref> of claims in political debates and social media. While these tasks focused on factuality, check-worthiness, and stance detection, here we target propaganda; moreover, we focus on memes and on multimodality rather than on analyzing the text of tweets, political debates, or community question answering forums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Persuasion Techniques</head><p>Scholars have proposed a number of inventories of persuasion techniques of various sizes <ref type="bibr" target="#b46">(Miller, 1939;</ref><ref type="bibr" target="#b63">Torok, 2015;</ref><ref type="bibr" target="#b1">Abd Kadir and Sauffiyan, 2014)</ref>. Here, we use an inventory of 22 techniques, borrowing from the lists of techniques described in , <ref type="bibr" target="#b58">(Shah, 2005)</ref> and <ref type="bibr" target="#b1">(Abd Kadir and Sauffiyan, 2014)</ref>. Among these 22 techniques, the first 20 are applicable to both text and images, while the last two, Appeal to (Strong) Emotions and Transfer, are reserved for images.</p><p>Below, we provide a definition for each of these 22 techniques; more detailed instructions of the annotation process and examples are provided in Appendix A.</p><p>1. Loaded Language: Using specific words and phrases with strong emotional implications (either positive or negative) to influence an audience.</p><p>2. Name Calling or Labeling: Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable, or loves, praises.</p><p>3. Doubt: Questioning the credibility of someone or something.</p><p>4. Exaggeration or Minimisation: Either representing something in an excessive manner, e.g., making things larger, better, worse ("the best of the best", "quality guaranteed"), or making something seem less important or smaller than it really is, e.g., saying that an insult was just a joke.</p><p>5. Appeal to Fear or Prejudices: Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases, the support is built based on preconceived judgments.</p><p>6. Slogans: A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Whataboutism:</head><p>A technique that attempts to discredit an opponent's position by charging them with hypocrisy without directly disproving their argument.</p><p>8. Flag-Waving: Playing on strong national feeling (or positive feelings toward any group, e.g., based on race, gender, political preference) to justify or promote an action or idea.</p><p>9. Misrepresentation of Someone's Position (Straw Man): When an opponent's proposition is substituted with a similar one, which is then refuted in place of the original proposition.</p><p>10. Causal Oversimplification: Assuming a single cause or reason, when there are actually multiple causes for an issue. It includes transferring blame to one person or group of people without investigating the actual complexities of the issue.</p><p>11. Appeal to Authority: Stating that a claim is true because a valid authority or expert on the issue said so, without any other supporting evidence offered. We consider the special case in which the reference is not an authority or an expert as part of this technique, although it is referred to as Testimonial in the literature.</p><p>12. Thought-Terminating Cliché: Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract the attention away from other lines of thought.</p><p>13. Black-and-White Fallacy or Dictatorship: Presenting two alternative options as the only possibilities, when in fact more possibilities exist. As an extreme case, tell the audience exactly what actions to take, eliminating any other possible choices (Dictatorship).</p><p>14. Reductio ad Hitlerum: Persuading an audience to disapprove of an action or an idea by suggesting that the idea is popular with groups that are hated or in contempt by the target audience. It can refer to any person or concept with a negative connotation.</p><p>15. Repetition: Repeating the same message over and over again, so that the audience will eventually accept it.</p><p>16. Obfuscation, Intentional Vagueness, Confusion: Using words that are deliberately not clear, so that the audience can have their own interpretations.</p><p>17. Presenting Irrelevant Data (Red Herring):</p><p>Introducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made.</p><p>18. Bandwagon Attempting to persuade the target audience to join in and take the course of action because "everyone else is taking the same action."</p><p>19. Smears: A smear is an effort to damage or call into question someone's reputation, by propounding negative propaganda. It can be applied to individuals or groups.</p><p>20. Glittering Generalities (Virtue): These are words or symbols in the value system of the target audience that produce a positive image when attached to a person or an issue.</p><p>21. Appeal to (Strong) Emotions: Using images with strong positive/negative emotional implications to influence an audience.</p><p>22. Transfer: Also known as Association, this is a technique that evokes an emotional response by projecting positive or negative qualities (praise or blame) of a person, entity, object, or value onto another one in order to make the latter more acceptable or to discredit it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>The annotation process is explained in detail in Appendix A, and in this section, we give a just brief summary. We collected English memes from our personal Facebook accounts over several months in 2020 by following 26 public Facebook groups, which focus on politics, vaccines, COVID-19, and gender equality. We considered a meme to be a "photograph style image with a short text on top of it", and we removed examples that did not fit this definition, e.g., cartoon-style memes, memes whose textual content was strongly dominant or non-existent, memes with a single-color background image, etc. Then, we annotated the memes using our 22 persuasion techniques. For each meme, we first annotated its textual content, and then the entire meme. We performed each of these two annotations in two phases: in the first phase, the annotators independently annotated the memes; afterwards, all annotators met together with a consolidator to discuss and to select the final gold label(s).</p><p>The final annotated dataset consists of 950 memes: 687 memes for training, 63 for development, and 200 for testing. While the maximum number of sentences in a meme is 13, the average number of sentences per meme is just 1.68, as most memes contain very little text.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the number of instances of each technique for each of the tasks. Note that Transfer and Appeal to (Strong) Emotions are not applicable to text, i.e., to Subtasks 1 and 2. For Subtasks 1 and 3, each technique can be present at most once per example, while in Subtask 2, a technique could appear multiple times in the same example. This explains the sizeable differences in the number of instances for some persuasion techniques between Subtasks 1 and 2: some techniques are over-used in memes, with the aim of making the message more persuasive, and thus they contribute higher counts to Subtask 2.  Note that the number of instances for Subtasks 1 and 3 differs, and in some cases by quite a bit, e.g., for Smears, Doubt, and Appeal to Fear/Prejudice. This shows that many techniques cannot be found in the text, and require the visual content, which motivates the need for multimodal approaches for Subtask 3. Note also that different techniques have different span lengths, e.g., Loaded Language and Name Calling are about 2-3 words long, e.g., violence, mass shooter, and coward. However, for techniques such as Whataboutism, the average span length is 22 words.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows statistics about the distribution of the number of persuasion techniques per meme. Note the difference for memes without persuasion techniques between Figures <ref type="figure" target="#fig_1">2a and 2c</ref>: we can see that the number of memes without any persuasion technique drastically drops for Subtask 3. This is because the visual modality introduces additional context that was not available during the text-only annotation, which further supports the need for multimodal analysis. The visual modality also has an impact on memes that already had persuasion techniques in the text-only phase.</p><p>We observe that the number of memes with only one persuasion technique in Subtask 3 is considerably lower compared to Subtask 1, while the number of memes with three or more persuasion techniques has greatly increased for Subtask 3. 5 Evaluation Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measures</head><p>Subtasks 1 and 3 To measure the performance of the systems, for Subtasks 1 and 3, we use Micro and Macro F 1 , as these are multi-class multi-label tasks, where the labels are imbalanced. The official measure for the task is Micro F 1 .</p><p>Subtask 2 For Subtask 2, the evaluation requires matching the text spans. Hence, we use an evaluation function that gives credit to partial matches between gold and predicted spans.</p><p>Let document d be represented as a sequence of characters. The i-th propagandistic text fragment is then represented as a sequence of contiguous characters t ⊆ d. A document includes a set of (possibly overlapping) fragments T . Similarly, a learning algorithm produces a set S with fragments s ⊆ d, predicted on d. A labeling function l(x) ∈ {1, . . . , 20} associates t ∈ T , s ∈ S with one of the techniques. An example of (gold) annotation is shown in Figure <ref type="figure" target="#fig_2">3</ref>, where an annotation t 1 marks the span stupid and petty with the technique Loaded Language. We define the following function to handle partial overlaps of fragments with the same labels:</p><formula xml:id="formula_0">C(s, t, h) = |(s ∩ t)| h δ (l(s), l(t)) ,<label>(1)</label></formula><p>where h is a normalizing factor and δ(a, b) = 1 if a = b, and 0, otherwise. For example, still with reference to Figure <ref type="figure" target="#fig_2">3</ref></p><formula xml:id="formula_1">, C(t 1 , s 1 , |t 1 |) = 6 16 and C(t 1 , s 2 , |t 1 |) = 0.</formula><p>Given Eq. (1), we now define variants of precision and recall that can account for the imbalance in the corpus:</p><formula xml:id="formula_2">P (S, T ) = 1 |S| s ∈ S, t ∈ T C(s, t, |s|),<label>(2)</label></formula><formula xml:id="formula_3">R(S, T ) = 1 |T | s ∈ S, t ∈ T C(s, t, |t|),<label>(3)</label></formula><p>We define (2) to be zero if |S| = 0, and Eq. (3) to be zero if |T | = 0. Following <ref type="bibr" target="#b50">Potthast et al. (2010)</ref>, in (2) and ( <ref type="formula" target="#formula_3">3</ref>) we penalize systems predicting too many or too few instances by dividing by |S| and |T |, respectively. Finally, we combine Eqs. (2) and ( <ref type="formula" target="#formula_3">3</ref>) into an F 1 -measure, the harmonic mean of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task Organization</head><p>We ran the shared task in two phases:</p><p>Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. A live leaderboard was made available to keep track of all submissions.</p><p>Test Phase In the second phase, the test set was released and the participants were given just a few days to submit their final predictions.</p><p>In the Development Phase, the participants could make an unlimited number of submissions, and see the outcome in their private space. The best score for each team, regardless of the submission time, was also shown in a public leaderboard. As a result, not only could the participants observe the impact of various modifications in their systems, but they could also compare against the results by other participating teams. In the Test Phase, the participants could again submit multiple runs, but they would not get any feedback on their performance. Only the latest submission of each team was considered as official and was used for the final team ranking. The final leaderboard on the test set was made public after the end of the shared task.</p><p>In the Development Phase, a total of 15, 10 and 13 teams made at least one submission for ST1, ST2 and ST3, respectively. In the Test Phase the number of teams who made official submissions was 16, 8, and 15 for ST1, ST2, ST3, respectively.</p><p>After the competition was over, we left the submission system open for the development set, and we plan to reopen it on the test set as well.  <ref type="bibr" target="#b67">(Zhu et al., 2021)</ref> 13 <ref type="bibr">(Hossain et al., 2021)</ref> 15 <ref type="bibr" target="#b26">(Gupta and Sharma, 2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Participants and Results</head><p>Below, we give a general description of the systems that participated in the three subtasks and their results, with focus on those ranked among the top-3. Appendix C gives a description of every system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Subtask 1 (Unimodal: Text)</head><p>Table <ref type="table" target="#tab_3">2</ref> gives an overview of the systems that took part in Subtask 1. We can see that transformers were quite popular, and among them, most commonly used was RoBERTa, followed by BERT. Some participants used learning models such as LSTM, CNN, and CRF in their final systems, while internally, Naïve Bayes and Random Forest were also tried. In terms of representation, embeddings clearly dominated. Moreover, techniques such as ensembles, data augmentation, and post-processing were also used in some systems.</p><p>The evaluation results are shown in Table <ref type="table" target="#tab_5">3</ref>, which also includes two baselines: (i) random, and (ii) majority class. The latter always predicts Loaded Language, as it is the most frequent technique for Subtask 1 (see Table <ref type="table" target="#tab_1">1</ref>).</p><p>The  The final prediction for MinD averages the probabilities for these models, and further uses postprocessing rules, e.g., each bigram appearing more than three times is flagged as a Repetition.</p><p>Team Alpha <ref type="bibr" target="#b20">(Feng et al., 2021)</ref> was ranked second. However, they used features from images, which was not allowed (images were only allowed for Subtask 3).</p><p>Team Volta  was third. They used a combination of transformers with the <ref type="bibr">[CLS]</ref> token as an input to a two-layer feed-forward network. They further used example weighting to address class imbalance.</p><p>We should also mention team LeCun, which used additional corpora such as the PTC corpus (Da San Martino et al., 2020a), and augmented the training data using synonyms, random insertion/deletion, random swapping, and backtranslation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Subtask 2 (Unimodal: Text)</head><p>The approaches for this task varied from modeling it as a question answering (QA) task to performing multi-task learning. Table <ref type="table" target="#tab_7">4</ref> presents a high-level summary. We can see that BERT dominated, while RoBERTa was much less popular. We further see a couple of systems using data augmentation. Unfortunately, there are too few systems with system description papers for this subtask, and thus it is hard to do a very deep analysis.  Table <ref type="table">5</ref> shows the evaluation results. We report our random baseline, which is based on the random selection of spans with random lengths and a random assignment of labels. The best model by team Volta  used various transformer models, such as BERT and RoBERTa, to predict token classes by considering the output of each token embedding. Then, they assigned classes for a given word as the union of the classes predicted for the subwords that make that word (to account for BPEs).</p><p>Team HOMADOS (Kaczyński and Przybyła, 2021) was second, and they used a multi-task learning (MTL) and additional datasets such as the PTC corpus from SemEval-2020 task 11 (Da San Martino et al., 2020a), and a fake news corpus (Przybyla, 2020). They used BERT, followed by several output layers that perform auxiliary tasks of propaganda detection and credibility assessment in two distinct scenarios: sequential and parallel MTL. Their final submission used the latter.</p><p>Team TeamFPAI <ref type="bibr" target="#b66">(Xiaolong et al., 2021)</ref> formulated the task as a question answering problem using machine reading comprehension, thus improving over the ensemble-based approach of <ref type="bibr" target="#b38">Liu et al. (2018)</ref>. They further explored data augmentation and loss design techniques, in order to alleviate the problem of data sparseness and data imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Subtask 3 (Multimodal: Memes)</head><p>Table <ref type="table" target="#tab_10">6</ref> presents an overview of the approaches used by the systems that participated in Subtask 3. This is a very rich and very interesting table. We can see that transformers were quite popular for text representation, with BERT dominating, but RoBERTa being quite popular as well. For the visual modality, the most common representations were variants of ResNet, but VGG16 and CNNs were also used. We further see a variety of representations and fusion methods, which is to be expected given the multi-modal nature of this subtask. 11 <ref type="bibr" target="#b67">(Zhu et al., 2021)</ref> 13 <ref type="bibr" target="#b51">(Pritzkau, 2021)</ref> 15 <ref type="bibr" target="#b60">(Singh and Lefever, 2021)</ref>  Table <ref type="table" target="#tab_12">7</ref> shows the performance on the test set for the participating systems for Subtask 3. The two baselines shown in the table are similar to those for Subtask 1, namely a random baseline and a majority class baseline. However, this time the most frequent class baseline always predicts Smears (for Subtask 1, it was Loaded Language), as this is the most frequent technique for Subtask 3 (as can be seen in Table <ref type="table" target="#tab_1">1</ref>).</p><p>Team Alpha <ref type="bibr" target="#b20">(Feng et al., 2021)</ref> pre-trained a transformer using text with visual features. They extracted grid features using ResNet50, and salient region features using BUTD. They further used these grid features to capture the high-level semantic information in the images. Moreover, they used salient region features to describe objects and to caption the event present in the memes. Finally, they built an ensemble of fine-tuned De-BERTA+ResNet, DeBERTA+BUTD, and ERNIE-VIL systems.</p><p>Team MinD <ref type="bibr" target="#b62">(Tian et al., 2021</ref>) combined a system for Subtask 1 with (i) ResNet-34, a face recognition system, (ii) OCR-based positional embeddings for text boxes, and (iii) Faster R-CNN to extract region-based image features. They used late fusion to combine the textual and the visual representations. Other multimodal fusion strategies they tried were concatenation of the representation and mapping using a multi-layer perceptron.</p><p>Team 1213Li <ref type="bibr">(Peiguang et al., 2021)</ref> used RoBERTa and ResNet-50 as feature extractors for texts and images, respectively, and adopted a label embedding layer with a multi-modal attention mechanism to measure the similarity between labels with multi-modal information, and fused features for label prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. It was a successful task: a total of 71 teams registered to participate, 22 teams eventually made an official submission on the test set, and 15 teams also submitted a task description paper.</p><p>In future work, we plan to increase the data size and to add more propaganda techniques. We further plan to cover several different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research part of the Tanbih mega-project, <ref type="bibr">3</ref> which is developed at the Qatar Computing Research Institute, HBKU, and aims to limit the impact of "fake news," propaganda, and media bias by making users aware of what they are reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics and Broader Impact</head><p>User Privacy Our dataset only includes memes and it contains no user information.</p><p>Biases Any biases in the dataset are unintentional, and we do not intend to do harm to any group or individual. Note that annotating propaganda techniques can be subjective, and thus it is inevitable that there would be biases in our goldlabeled data or in the label distribution. We address these concerns by collecting examples from a variety of users and groups, and also by following a well-defined schema, which has clear definitions and on which we achieved high inter-annotator agreement.</p><p>Moreover, we had a diverse annotation team, which included six members, both female and male, all fluent in English, with qualifications ranging from undergrad to MSc and PhD degrees, including experienced NLP researchers, and covering multiple nationalities. This helped to ensure the quality. No incentives were provided to the annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misuse Potential</head><p>We ask researchers to be aware that our dataset can be maliciously used to unfairly moderate memes based on biases that may or may not be related to demographics and other information within the text. Intervention with human moderation would be required in order to ensure this does not occur.</p><p>To collect the data for the dataset, we used Facebook, as it has many public groups with a large number of users, who intentionally or unintentionally share a large number of memes. We used our own private Facebook accounts to crawl the public posts from users and groups. To make sure the resulting feed had a sufficient number of memes, we initially selected some public groups focusing on topics such as politics, vaccines, COVID-19, and gender equality. Then, using the links between groups, we expanded our initial group pool to a total of 26 public groups. We went through each group, and we collected memes from old posts, dating up to three months before the newest post in the group. Out of the 26 groups, 23 were about politics, US and Canadian: left, right, centered, antigovernment, and gun control. The other 3 groups were on general topics such as health, COVID-19, pro-vaccines, anti-vaccines, and gender equality. Even though the number of political groups was larger (i.e., 23), the other 3 general groups had a higher number of users and a substantial amount of memes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Annotation Process</head><p>We annotated the memes using the 22 persuasion techniques from Section 3 in a multi-label setup. Our annotation focused (i) on the text only, using 20 techniques, and (ii) on the entire meme (text + image), using all 22 techniques.</p><p>We could not annotate the visual modality as an independent task because memes have the text as part of the image. Moreover, in many cases, the message in the meme requires both modalities. For example, in Figure <ref type="figure" target="#fig_1">28</ref>, the image by itself does not contain any persuasion technique, but together with the text, we can see Smears and Reductio at Hitlerum.</p><p>The annotation team included six members, both female and male, all fluent in English, with qualifications ranging from undergrad to MSc and PhD degrees, including experienced NLP researchers, and covering multiple nationalities. This helped to ensure the quality of the annotation, and our focus was really on having very high-quality annotation. No incentives were given to the annotators.</p><p>We used PyBossa 4 as an annotation platform, as it provides the functionality to create a custom annotation interface that we found to be a good fit for our needs in each phase of the annotation process. Figure <ref type="figure" target="#fig_3">4</ref> shows examples of the annotation interface for the five different phases of annotation, which we describe in detail below.</p><p>Phase 1: Filtering and Text Editing The first phase of the annotation process is about selecting the memes for our task, followed by extracting and editing the textual contents of each meme. After we collected the memes, we observed that we needed to remove some of them as they did not fit our definition: "photograph style image with a short text on top of it." Thus, we asked the annotators to exclude images with the characteristics listed below. During this phase, we filtered out a total of 111 memes.</p><p>• Images with diagrams/graphs/tables (see Fig-</p><p>ure <ref type="figure" target="#fig_5">5a</ref>).</p><p>• Cartoons. (see Figure <ref type="figure" target="#fig_5">5b</ref>)</p><p>• Memes for which no multi-modal analysis is possible: e.g., only text, only image, etc. (see Figure <ref type="figure" target="#fig_5">5c</ref>)</p><p>Next, we used the Google Vision API 5 to extract the text from the memes. As the resulting text sometimes contains errors, manual checking was needed to correct it. Thus, we defined several text editing rules, and we asked the annotators to apply them on the memes that passed the filtering rules above.</p><p>1. When the meme is a screenshot of a social network account, e.g., WhatsApp, the user name and login can be removed as well as all "Like", "Comment', "Share".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Remove the text related to logos that are not part of the main text.</p><p>3. Remove all text related to figures and tables.</p><p>4. Remove all text that is partially hidden by an image, so that the sentence is almost impossible to read.</p><p>5. Remove all text that is not from the meme, but on banners carried on by demonstrators, street advertisements, etc. 6. Remove the author of the meme if it is signed.</p><p>7. If the text is in columns, first put all text from the first column, then all text from the next column, etc.</p><p>8. Rearrange the text, so that there is one sentence per line, whenever possible.</p><p>9. If there are separate blocks of text in different locations of the image, separate them by a blank line. However, if it is evident that the text blocks are part of a single sentence, keep them together.</p><p>Phase 2: Text Annotation The annotations for phase 2 are targeted at Subtasks 1 and 2. Given the list of propaganda techniques for text only annotation, as discussed in Section A.4 (i.e., techniques 1-20), and the textual content of the target meme, the annotators were asked to identify which techniques appear in the text, and also to annotate the span of each instance of a technique use. In this phase, there were three annotators per example.</p><p>Phase 3: Text Consolidation Phase 3 is the consolidation step for the annotations from phase 2.</p><p>The three annotators met with the rest of the team, who acted as consolidators, and discussed each annotation, so that a consensus could be reached.  We made sure to consider different interpretations and to anotate techniques corresponding to the most likely one. While this phase was devoted to checking the annotations from phase 2, when a novel instance of a technique was found, it could be added; conversely, an instance of a technique with perfect agreement from phase 2 could also be dropped. Phase 3 was essential for ensuring quality, and it served as an additional training opportunity for the entire team, which was very useful.</p><p>Phase 4: Multimodal Annotation In this phase, the goal is to identify which of the 22 techniques, discussed in Section A.4, appear in the meme: in the text and in the visual content. Note that some of the techniques occurring in the text might be identified only in this phase because the image provides the necessary context. Thus, we presented the meme with the consolidated propaganda labels from phase 3. We intentionally provided the consolidated text labels to the annotators in order to ensure that they focus their attention on identifying propaganda techniques that require both modalities rather than repeating what was already labeled in the earlier phases. In this phase, there were three annotators per example.</p><p>Phase 5: Multimodal Consolidation. In phase 5, we consolidated the annotations from phase 4 in a discussion of the entire team of six annotators (just as we did for phase 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Annotation Agreement</head><p>We assessed the quality for the individual annotators from phases 2 and 4 (i.e., when combining the annotations for the meme's text and for the entire meme) to the final consolidated labels at phase 5. Since our annotation is multi-label, we computed Krippendorff's α <ref type="bibr" target="#b4">(Artstein and Poesio, 2008)</ref>. The results are shown in Table <ref type="table" target="#tab_14">8</ref>, and the numbers indicate moderate to substantial agreement <ref type="bibr" target="#b36">(Landis and Koch, 1977)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agreement Pair</head><p>Krippendorff's α  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Propaganda Techniques: Definitions</head><p>Below, we present the definitions of our 22 propaganda techniques, together with examples: both textual, and memes. Note that, for copyright reasons, we show our own recreated versions of actual memes from our dataset, where, for each meme, we indicate the image(s) we used and the corresponding license terms (as hyperlinks in the image caption).</p><p>1. Loaded Language: Using specific words and phrases with strong emotional implications (i.e., either positive or negative) to influence an audience. An example meme is shown in Figure <ref type="figure" target="#fig_6">6</ref>, which contains four instances of this persuasion technique in its text: killed thousands of innocents, retaliate, kill, and warmonger. 2. Name Calling or Labeling: Labeling the object of the propaganda as either something the target audience fears, hates, finds undesirable, or loves, praises.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows three instances of this technique: the two biggest threats to America, the worst senate leader ever, and the most corrupt President ever. Figure <ref type="figure" target="#fig_6">6</ref> also contains an instance: warmonger. An example is shown in Figure <ref type="figure" target="#fig_8">8</ref>, where the entire text in the meme represents a span for this technique, while the image is just for illustration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Exaggeration or Minimisation:</head><p>Representing something in an excessive manner, making it larger, better, worse (e.g., the best of the best); or making it seem less important or smaller than it really is (e.g., saying that an insult was just a joke).</p><p>An example is shown in Figure <ref type="figure" target="#fig_9">9</ref>, where the entire meme conveys an exaggeration. Moreover, all three Name Calling instances in Figure <ref type="figure" target="#fig_7">7</ref> are also examples of Exaggeration. 5. Appeal to Fear/Prejudice: Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases, the support is built based on preconceived judgments.</p><p>An example is shown in Figure <ref type="figure" target="#fig_0">10</ref>, where both the text and the image instill fear. Image ; License 6. Slogans: A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.</p><p>An example is shown in Figure <ref type="figure" target="#fig_0">11</ref>, which contains a slogan in its textual content: "Vaccines. It isn't always about you." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Whataboutism:</head><p>A technique that attempts to discredit an opponent's position by charging them with hypocrisy without directly disproving their argument.</p><p>An example meme is shown in Figure <ref type="figure" target="#fig_0">12</ref>, where the entire text represents a span for this technique, while the image is just for illustration. 8. Flag-Waving: Playing on strong national feeling (or to any group such as race, gender, political preference) to justify or promote an action or idea.</p><p>An example is shown in Figure <ref type="figure" target="#fig_0">13</ref>, with the technique expressed in the text and the image. An example meme is shown in Figure <ref type="figure" target="#fig_0">14</ref>, which contains an instance of this technique in its text: here, the entire text in the meme represents a span for this technique, while the image is irrelevant for that technique (however, it is relevant for other techniques such as Smears). An example meme is shown in Figure <ref type="figure" target="#fig_0">15</ref>, which contains an instance of this technique in its text: "You can't get rich in politics unless you are a crook." This statement says that if somebody got rich in politics, the only reason for this happening should be that this person is a crook, while in reality there are typically multiple causes. The image is irrelevant for that technique (however, it is relevant for other techniques such as Smears). 11. Appeal to Authority: Stating that a claim is true simply because a valid authority or expert on the issue said it was true, without any other supporting evidence offered. We consider the special case in which the reference is not an authority or an expert in this technique, although it is referred to as Testimonial in literature.</p><p>An example meme is shown in Figure <ref type="figure" target="#fig_0">16</ref>, which contains a quote by the 3rd President of the United States. 12. Thought-Terminating Cliché: Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought.</p><p>Figure <ref type="figure" target="#fig_0">17</ref> shows a meme with an instance of this technique in its text: "PERIOD."  14. Reductio ad Hitlerum: Persuading an audience to disapprove an action or idea by suggesting that the idea is popular with groups hated or in contempt by the target audience. It can refer to any person or concept with a negative connotation.</p><p>Figure <ref type="figure" target="#fig_0">19</ref> shows a meme trying to discredit the idea of being anti-union by saying that so is Donald Trump, who in turn is shown in bad light.  16. Obfuscation, Intentional Vagueness, Confusion: Using words that are deliberately unclear, so that the audience may have their own interpretations.</p><p>Figure <ref type="figure" target="#fig_0">21</ref>, shows an example, where the entire quote by Joe Biden is a span of this technique, as it is unclear what exactly is meant here. Introducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made.</p><p>An example meme is shown in Figure <ref type="figure" target="#fig_1">22</ref>, which contains an instance of this technique in its text. We can see that there is no real connection between the two sentences. Here, the entire text represents a span for this technique, while the image is for reinforcement. 18. Bandwagon: Attempting to persuade the target audience to join in and take the course of action because "everyone else is taking the same action." Figure <ref type="figure" target="#fig_1">23</ref> shows an example that covers the entire text; the image less relevant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.">Smears:</head><p>A smear is an effort to damage or to call into question someone's reputation, by propounding negative propaganda. It can be applied to individuals or groups.</p><p>An example meme is shown in Figure <ref type="figure" target="#fig_1">24</ref>, where the combination of the image and the text conveys the idea that Biden is unpopular. 20. Glittering Generalities: These are words or symbols in the value system of the target audience that produce a positive image when attached to a person or issue. Peace, hope, happiness, security, wise leadership, freedom, "The Truth", etc. are virtue words. Virtue can be also expressed in images, where a person or an object is depicted positively.</p><p>Figure <ref type="figure" target="#fig_1">25</ref> shows an example of the use of this technique, in the right half of the meme. The technique covers the entire text span starting from "2 &amp; 1/2 years . . ." until "GDP up 3.2% . . ." It is also expressed in the image, which depicts Donald Trump in a positive way. The text-image combination further strengthens the technique. 21. Appeal to (Strong) Emotions: Using images with strong positive/negative emotional implications to influence an audience. We reserve this technique to the images content only.</p><p>An example is shown in Figure <ref type="figure" target="#fig_1">26</ref>, which invokes strong emotions in the audience. 22. Transfer: Also known as Association, this is a technique of projecting positive or negative qualities (praise or blame) of a person, entity, object, or value onto another one to make the second one more acceptable or to discredit it. It evokes an emotional response, which stimulates the target to identify with recognized authorities. Often highly visual, this technique often utilizes symbols (for example, the swastikas used in Nazi Germany, originally a symbol for health and prosperity) superimposed over other visual images.</p><p>Figure <ref type="figure" target="#fig_1">27</ref> shows an example, where the Transfer technique makes use of a communist symbol (namely, hammer and sickle) on top of the pictures of two targeted politicians, with the aim of depicting them in a negative way. The technique is further reinforced by the use of the red color (which is also a symbol of Communism), and by the two instances of Name Calling ("Moscow Mitch" and "Moscow's bitch"), which make a connection to Moscow (which in turn was the capital of the former Communist block). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Participating Systems</head><p>Below, we give a brief description of the participating systems, listed in alphabetical order, with reference to the corresponding task description paper. The numbers in square brackets refer to the official ranking of the target system on the individual subtasks.</p><p>1213Li <ref type="bibr">(Peiguang et al., 2021)</ref>[ST3: 3rd] used RoBERTa and ResNet-50 as feature extractors for texts and images. They used a label embedding layer with a multi-modal attention mechanism to measure the similarity between labels with the multi-modal information and fused features for label prediction.</p><p>AIMH <ref type="bibr">(Messina et al., 2021)</ref> [ST1: 5th, ST3: 4th] used transformer-based models and proposed visual-textual transformers to mainly address subtask 3 (ST3). For the visual part, they used ResNet50, and for the textual part, they used BERT. The same network used the multi-label classification on text (ST1) by using only the textual part of the network.</p><p>Alpha <ref type="bibr" target="#b20">(Feng et al., 2021)</ref> [ST1:2nd, ST3:1st] team pre-trained a transformer using text with visual features. They extract grid features, using ResNet50, and salient region features, using BUTD. They used grid features to capture the high-level semantic information found in the images. Additionally, they used salient region features to describe objects and to caption the event present in the memes. For ST1, they combined the text and the text representation of the visual features, and trained De-BERTa. For ST3, they built an ensemble of finetuned DeBERTA+ResNet, DeBERTA+BUTD, and ERNIE-VIL.</p><p>HOMADOS <ref type="bibr" target="#b33">(Kaczyński and Przybyła, 2021)</ref> [ST2: 2nd] used a multi-task learning (MTL) approach with additional datasets such as the PTC corpus from <ref type="bibr">SemEval-2020</ref><ref type="bibr" target="#b12">(Da San Martino et al., 2020a</ref>, and a fake news corpus <ref type="bibr" target="#b52">(Przybyla, 2020)</ref>. The model was trained using BERT followed by several output layers, which solve auxiliary tasks of propaganda detection and credibility assessment in two distinct scenarios: sequential and parallel MTL, effectively accelerating the training process. The final submission used a parallel MTL approach on the propaganda detection of SemEval-2020, which ranked second.</p><p>TeamFPAI <ref type="bibr" target="#b66">(Xiaolong et al., 2021</ref>) (ST2: 3rd) formulated the task as a question answering one in a machine reading comprehension (MRC) framework, which achieved better results compared to an ensemble-based approach <ref type="bibr" target="#b38">(Liu et al., 2018)</ref>. Moreover, data augmentation and loss design techniques were also explored to alleviate the problem of data sparseness and imbalance. Their system was ranked 3rd in the final evaluation phase.</p><p>CSECUDSG <ref type="bibr">(Hossain et al., 2021</ref>) (ST1: 13th, ST2: 6th, ST3: 6th) participated in all three subtasks. For ST1, they used a majority vote late fusion on top of logistic regression, decision tree, and fine-tuned DistilBERT models. For ST2, they reformulated the task as one of multi-label classification, where a pre-trained BERT model was used to design binary classifiers for each technique in a multilabel classification setting. For ST3, they used a majority voting late fusion on top of fine-tuned Dis-tilBERT, ResNet50, and a predicted label from an early fusion model. The early fusion model consisted of features from (i) multi-kernel CNN on top of the LSTM model with word embeddings including (ii) word2vec <ref type="bibr" target="#b45">(Mikolov et al., 2013)</ref>, (iii) word embeddings fine-tuned FastBERT <ref type="bibr" target="#b39">(Liu et al., 2020)</ref>, (iv) RoBERTa, (v) sentence embeddings from Fast-BERT, (vi) image features from YouTube-8M (Abu- <ref type="bibr" target="#b2">El-Haija et al., 2016)</ref>, and (vii) multimodal features from VisualBERT <ref type="bibr" target="#b37">(Li et al., 2019)</ref>.</p><p>LeCun <ref type="bibr" target="#b17">(Dia et al., 2021)</ref> [ST1: 6th] trained five models and combined them in an ensemble. Initially, they pre-processed text using stemming. Later, they trained DebERTA and RoBERTa models with augmented data using synonym replacement, random insertion, random swap, random deletion and back-translation. They first trained the five models separately, and then they fine-tuned the ensemble on the official non-augmented data.</p><p>LIIR <ref type="bibr" target="#b21">(Ghadery et al., 2021)</ref>[ST3: 8th] used data augmentation through back-translation and CLIP to obtain image and text representations, which were then fed to a chained classifier that uses the correlations between the output techniques.</p><p>LT3-UGent <ref type="bibr" target="#b60">(Singh and Lefever, 2021)</ref> [ST3: 14th] participated in subtask 3 only. They used Multimodal Compact Bilinear Pooling to combine representations from ResNet-51 and BERT. They further fine-tuned on the PTC corpus (Da San <ref type="bibr" target="#b12">Martino et al., 2020a)</ref>.</p><p>MinD <ref type="bibr" target="#b62">(Tian et al., 2021)</ref> [ST1: 1st, ST3: 2nd] used five pre-trained models for ST1: BERT, RoBERTa, XLNet, DeBERTa, and ALBERT. They first fine-tuned them on the PTC corpus <ref type="bibr" target="#b12">(Da San Martino et al., 2020a)</ref>, and then on the training data. For the final prediction, they averaged the probabilities of the models. They also used a post-processing rule: a bigram that appeared more than three times was flagged as a Repetition. The system for ST1 was also used for ST3, combined with (i) ResNet-34, a face recognition system, (ii) OCR-based positional embeddings for text boxes in the image, and (iii) Faster R-CNN to extract region-based image features. They combined the textual and the visual representations by averaging their probabilities. Other multimodal fusion strategies included concatenation of the representation and mapping them to the space using a multilayer perceptron.</p><p>NLP-IITR <ref type="bibr" target="#b26">(Gupta and Sharma, 2021)</ref> [ST1: 15th] used an ensemble that included included fine-tuned RoBERTa, BERT, and three additional models. They further used pre-processing. To tackle data scarceness for some rare labels, they used data augmentation using back-translation.</p><p>NLyticsFKIE <ref type="bibr" target="#b51">(Pritzkau, 2021)</ref> [ST1: 9th, ST3: 13th] used RoBERTa as a text encoder in ST1 and ST3. For ST1, they used RoBERTa's output to build a classifier to predict each label separately. For ST3, they still used RoBERTa to encode the text and a VGG-16 layer to encode the image. They used multiple copies of a cross-modality encoder that outputs an encoding of the image features with respect to the text features, and vice versa. The concatenation of the two cross-encoders' outputs was then passed through a residual layer followed by layer normalization.</p><p>Volta  [ST1: 3rd, ST2: 1st, ST3: 5th] used a combination of transformers for all subtasks. For ST1, they used RoBERTa's [CLS] token, which they fed to a feed-forward neural network, and example weighting to take care of class imbalance. For ST2, they predicted token classes by considering the output of each token embedding as obtained by RoBERTa. To account for subwords' class, they merged each subword belonging to the same token and assigned the union of the subwords' labels. For ST3, they separately encoded the textual features (extracted using RoBERTa) and the multi-modal features (extracted using UNITER, VisualBERT, and LXMERT). This layer's input was a sequence of textual subwords and visual tokens extracted by keeping the top 36 regions of interest as returned by Faster R-CNN. A concatenation of the two different [CLS] tokens was then fed into an MLP, and weighted labels were used with a cross-entropy loss.</p><p>WVOQ <ref type="bibr" target="#b54">(Roele, 2021)</ref> [ST2: 5th] used a novel approach to ST2 consisting of adopting an encoderdecoder strategy. The encoder encodes the passage, while the decoder generates a marked version of the input, where the markup outlines the various spans along with the classes they belong to. In this way, the system performed simultaneous span detection and classification. The encoder-decoder used a specialization of BART.</p><p>YNU-HPCC <ref type="bibr" target="#b67">(Zhu et al., 2021)</ref> [ST1: 12th, ST2: 7th, ST3: 11th] For ST1, they used a CNN on top of ALBERT and fine-tuned the model for multilabel classification. For ST2, each propaganda technique was considered as an independent task, and features were extracted from the pre-trained BERT model. Subsequently, the problem was addressed as a multi-task sequence labeling one, and the results for each task were combined. For ST3, a multi-modal network was used, where embeddings from textual and visual networks were concatenated, which was followed by a fully connected layer. For the text, the same approach was used for ST1, and for the image, ResNet and VGGNet were used for image feature extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A meme with a civil war threat during the President Trump's impeachment trial. Two persuasion techniques are used: (i) Appeal to Fear in the image, and (ii) Exaggeration in the text. Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of the number of persuasion techniques per meme. Subfigure (b) reports the number of instances of persuasion techniques for a meme. Note that a meme could have multiple instances of the same technique for this subtask. Subfigures (a) and (c) show the number of distinct persuasion techniques in a meme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of gold annotation (top) and the predictions of a supervised model (bottom) in a document represented as a sequence of characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of the annotation interface for different phases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Example of a meme with a graph Source(s): Image ; License (b) Example of a cartoon meme; Source(s): Image ; License . (c) Example of a meme with only text modality; License .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of memes we filtered out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example for Loaded Language; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example for Name Calling; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example for Doubt; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Example for Exaggeration; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Example for Appeal to Fear; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Example for Slogan; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Example for Whataboutism; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Example for Flag-Waving; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Example for Misrepresentation of Someone's Position (Straw Man); Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Example for Causal Oversimplification; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Example for Appeal to Authority; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Example for Thought-Terminating Cliché; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Example for Black-and-White Fallacy; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Example for Reduction ad Hitlerum; Source(s): Image , License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Example for Repetition; Source(s): Image 1, Image 2, Image 3, Image 4; License 1, License 2, License 3, License 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Example for Obfuscation, Intentional vagueness, Confusion; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Example for Presenting Irrelevant Data (Red Herring); Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Example for Bandwagon; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Example for Smears; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Example for Glittering Generalities; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Example for Appeal to (Strong) Emotions; Source(s): Image ; License</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Example for Transfer; Source(s): Image 1, Image 2; License 1, License 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics about the persuasion techniques. For each technique, we show the average length of its spans (in number of words) and the number of its instances as annotated in the text only vs. in the entire meme.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The upto-date leaderboards can be found on the website of the competition.2   </figDesc><table><row><cell>Rank. Team</cell><cell cols="6">Transformers</cell><cell></cell><cell cols="4">Models</cell><cell></cell><cell cols="6">Repres. Misc</cell></row><row><cell cols="2">BERT</cell><cell>RoBERTa</cell><cell>XLNet</cell><cell>ALBERT</cell><cell>DistilBERT</cell><cell>DeBERTa</cell><cell>LSTM</cell><cell>CNN</cell><cell>SVM</cell><cell>Naive Bayes</cell><cell>Random Forest</cell><cell>CRF</cell><cell>Embeddings</cell><cell>Char n-grams</cell><cell>PoS</cell><cell>Ensemble</cell><cell>Data augmentation</cell><cell>Postprocessing</cell></row><row><cell>1. MinD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. Alpha</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. Volta</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5. AIMH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6. LeCun</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7. WVOQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9. NLyticsFKIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12. YNU-HPCC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13. CSECUDSG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15. NLP-IITR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Tian et al., 2021)</cell><cell></cell><cell></cell><cell cols="7">6 (Dia et al., 2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2 (Feng et al., 2021)</cell><cell></cell><cell cols="6">7 (Roele, 2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3 (Gupta et al., 2021)</cell><cell></cell><cell cols="7">9 (Pritzkau, 2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">5 (Messina et al., 2021)</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>ST1: Overview of the approaches used by the participating systems. =part of the official submission; =considered in internal experiments; Repres. stand for Representations. References to system description papers are shown below the table.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>best system MinD (Tian et al., 2021) used five transformers: BERT, RoBERTa, XLNet, De-BERTa, and ALBERT. It was fine-tuned on the PTC corpus (Da San Martino et al., 2020a) and then on the training data for Subtask 1.</figDesc><table><row><cell>Rank Team</cell><cell cols="2">F1-Micro F1-Macro</cell></row><row><cell>1 MinD</cell><cell>.593</cell><cell>.290 2</cell></row><row><cell>2 Alpha</cell><cell>.572</cell><cell>.262 5</cell></row><row><cell>3 Volta</cell><cell>.570</cell><cell>.266 3</cell></row><row><cell>4 mmm</cell><cell>.548</cell><cell>.303 1</cell></row><row><cell>5 AIMH</cell><cell>.539</cell><cell>.245 6</cell></row><row><cell>6 LeCun</cell><cell>.512</cell><cell>.227 8</cell></row><row><cell>7 WVOQ</cell><cell>.511</cell><cell>.227 8</cell></row><row><cell>8 TeamUNCC</cell><cell>.510</cell><cell>.236 7</cell></row><row><cell>9 NLyticsFKIE</cell><cell>.498</cell><cell>.140 13</cell></row><row><cell>10 TeiAS</cell><cell>.497</cell><cell>.214 10</cell></row><row><cell>11 DAJUST</cell><cell>.497</cell><cell>.187 11</cell></row><row><cell>12 YNUHPCC</cell><cell>.493</cell><cell>.263 4</cell></row><row><cell>13 CSECUDSG</cell><cell>.489</cell><cell>.185 12</cell></row><row><cell>14 TeamFPAI</cell><cell>.406</cell><cell>.115 15</cell></row><row><cell>15 NLPIITR</cell><cell>.379</cell><cell>.126 14</cell></row><row><cell>Majority baseline</cell><cell>.374</cell><cell>.033</cell></row><row><cell>16 TriHeadAttention</cell><cell>.184</cell><cell>.024 18</cell></row><row><cell>Random baseline</cell><cell>.064</cell><cell>.044</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for Subtask 1. The systems are ordered by the official score: F1-micro.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>ST2: Overview of the approaches used by the participating systems. =part of the official submission; =considered in internal experiments; Trans. is for Transformers; Repres. is for Representations. References to system description papers are shown below the table.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>ST3: Overview of the approaches used by the participating systems. =part of the official submission; =considered in internal experiments. References to system description papers are shown below the table.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Results for Subtask 3. The systems are ordered by the official score: F1-micro.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Inter-annotator agreement in terms of Krippendorff's α between each of the annotators and the consolidated annotation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In order to avoid potential copyright issues, all memes we show in this paper are our own recreation of existing memes, using images with clear copyright.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://propaganda.math.unipd.it/semeval2021task6/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://tanbih.qcri.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://pybossa.com 5 http://cloud.google.com/vision</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Data Collection and Annotation</head><p>A.1 Data Collection B Subtasks: Definition, Data Format, and Data Examples</p><p>Below, we describe the three subtasks and the general data format for each of them. We further show an example of an annotated example for each subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Subtask 1</head><p>This is a multi-label classification problem, defined as follows:</p><p>Subtask 1 (ST1) Given only the "textual content" of a meme, identify which of the 20 techniques are used in it.</p><p>The data for ST1 comes as a JSON object in the following format: { id -&gt; example identifier, labels -&gt; list of persuasion techniques, text -&gt; text of the meme }</p><p>Here is an example: { "id": "125", "labels": [ "Loaded Language", "Name calling/Labeling" ], "text": "I HATE TRUMP\n\n MOST TERRORIST DO" } B.2 Subtask 2 ST2 is a more complex version of ST1, as it asks not only for the techniques but also for the exact spans of use each technique. This subtask is a combination of the two subtasks in SemEval-2020 task 11. It is a multi-label sequence tagging problem, defined as follows:</p><p>Subtask 2 (ST2) Given only the "textual content" of a meme, identify which of the 20 techniques are used in it together with the span(s) of text covered by each technique.</p><p>The data for ST2 comes as a JSON object with the following format: Here is an example: { "id": "125", "text": "I HATE TRUMP\n\n MOST TERRORIST DO" "labels": [ { "start": 2, "end": 6, "technique": "Loaded Language", "text_fragment": "HATE" }, { "start": 19, "end": 28, "technique": "Name calling/ Labeling", "text_fragment": "TERRORIST" } ] }</p><p>Note that the labels to be predicted for ST2 are the same ones as for ST1, but this time the spans are to be predicted as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Subtask 3</head><p>ST3 is a multi-modal version of ST1, where the image is also provided. It is a multi-label classification problem, defined as follows:</p><p>Subtask 3 (ST3) Given a meme, identify which of the 22 techniques are used both in the textual and in the visual content of the meme.</p><p>The data for ST3 comes as a JSON object with the following format: { id -&gt; example identifier, labels -&gt; list of persuasion techniques, image -&gt; name of the image file, text -&gt; text of the meme }</p><p>Here is an example: Here, the image, which is shown in Figure <ref type="figure">28</ref>), gives rise to two additional persuasion techniques compared to ST1: Reductio ad Hitlerum and Smears. These techniques are not clearly present in the text alone. Indeed, the image is needed for us to see that there is Smears, as this can be only seen when we understand that this is a dialog with a negative propaganda targeting one of the participants (Ilhan Omar). Similarly, we need the image for Reductio ad Hitlerum: the image shows us that Ilhan Omar is depicted as a bad person (she is targeted by the Name Calling "terrorist", and she is also the target of the Smears), and thus the message being conveyed is that any choice that such a bad person does has to be a bad choice, i.e., hating Trump is a bad thing to do as this is something terrorists do. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion and techniques of propaganda in YouTube videos</title>
		<author>
			<persName><forename type="first">Anitawati</forename><surname>Shamsiah Abd Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lokman</surname></persName>
		</author>
		<author>
			<persName><surname>Tsuchiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Science and Technology</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A content analysis of propaganda in Harakah newspaper</title>
		<author>
			<persName><forename type="first">Abd</forename><surname>Shamsiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><surname>Sauffiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Media and Information Warfare</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="116" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A large-scale video classification benchmark</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Before name-calling: Dynamics and triggers of ad hominem fallacies in web argumentation</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT&apos;18</title>
				<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT&apos;18<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="386" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Survey article: Inter-coder agreement for computational linguistics</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims, task 1: Check-worthiness</title>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wajdi</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spas</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2018 Working Notes</title>
				<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2019 CheckThat! lab on automatic identification and verification of claims. Task 1: Check-worthiness</title>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni Da San</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2019</title>
				<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims, task 2: Factuality</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wajdi</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spas</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2018 Working Notes</title>
				<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of CheckThat! 2020 -automatic identification and verification of claims in social media</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bayan</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zien Sheikh</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction, CLEF &apos;2020</title>
				<meeting>the 11th International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction, CLEF &apos;2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reem Suwaileh, and Fatima Haouari. 2020. Check-That! at CLEF 2020: Enabling the automatic identification and verification of claims in social media</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Hasanain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Information Retrieval, ECIR &apos;19</title>
				<meeting>the European Conference on Information Retrieval, ECIR &apos;19<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="499" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proppy: Organizing the news based on their propagandistic content</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israa</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1849" to="1864" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Findings of the NLP4IF-2019 shared task on fine-grained propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF &apos;19</title>
				<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF &apos;19<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SemEval-2020 task 11: Detection of propaganda techniques in news articles</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation, SemEval &apos;20</title>
				<meeting>the Fourteenth Workshop on Semantic Evaluation, SemEval &apos;20<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1377" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on computational propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Cresci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">Di</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI-PRICAI &apos;20</title>
				<meeting>the International Joint Conference on Artificial Intelligence, IJCAI-PRICAI &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4826" to="4832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prta: A system to support the analysis of propaganda techniques in the news</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="287" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of propaganda in news articles</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP &apos;19</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP &apos;19<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5636" to="5646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval &apos;17</title>
				<meeting>the 11th International Workshop on Semantic Evaluation, SemEval &apos;17<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
	<note>Geraldine Wong Sak Hoi, and Arkaitz Zubiaga</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LeCun at SemEval-2021 Task 6: Detecting persuasion techniques in text using ensembled pretrained transformers and data augmentation</title>
		<author>
			<persName><forename type="first">Abujaber</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qarqaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Malak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CheckThat! at CLEF 2019: Automatic identification and verification of claims</title>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval, ECIR &apos;19</title>
				<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2019 CheckThat!: Automatic identification and verification of claims</title>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Atanasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="301" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alpha at SemEval-2021 Tasks 6: Transformer based propaganda classification</title>
		<author>
			<persName><forename type="first">Zhida</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LIIR at SemEval 2021 Task 6: Detection of persuasion techniques in texts and images using CLIP features</title>
		<author>
			<persName><forename type="first">Erfan</forename><surname>Ghadery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Sileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual multimodal digital deception detection and disinformation spread across social platforms</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Glenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Volkova</surname></persName>
		</author>
		<idno>abs/1909.05838</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<title level="m">SemEval-2019 task 7: RumourEval, determining rumour veracity and support for rumours</title>
				<meeting><address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
	<note>Proceedings of the 13th International Workshop on Semantic Evaluation, Sem-Eval &apos;19</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The future of false information detection on social media: New perspectives and trends</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volta at SemEval-2021 Task 6: Towards detecting persuasive texts and images using textual and multimodal ensemble</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NLPIITR at SemEval-2021 Task 6: detection of persuasion techniques in texts and images</title>
		<author>
			<persName><forename type="first">Vansh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raksha</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Argotario: Computational argumentation meets serious games</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffael</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Klamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;17</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;17<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adapting serious game for fallacious argumentation to German: Pitfalls, insights, and best practices</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC&apos;18</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC&apos;18<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview of CheckThat! 2020 Arabic: Automatic identification and verification of claims in social media</title>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zien</forename><surname>Sheikh Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bayan</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum, CLEF &apos;2020</title>
				<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims. Task 2: Evidence and Factuality</title>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2019</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Tamer Elsayed, Alberto Barrón-Cedeño, and Preslav Nakov</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<ptr target="CEUR-WS.org" />
		<imprint>
			<pubPlace>Lugano, Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Radiathun Tasnia, and Abu Nowshed Chy. 2021. CSE-CUDSG at SemEval-2021 Task 6: Orchestrating multimodal neural architectures for identifying persuasion techniques in texts and images</title>
		<author>
			<persName><forename type="first">Tashin</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannatun</forename><surname>Naim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fareen</forename><surname>Tasneem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HOMA-DOS at SemEval-2021 Task 6: Multi-task learning for propaganda detection</title>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Kaczyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Przybyła</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation</title>
				<meeting>the International Workshop on Semantic Evaluation<address><addrLine>Sem-Eval &apos;21, Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised multimodal bitransformers for classifying images and text</title>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrat</forename><surname>Bhooshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS 2019 Workshop on Visually Grounded Interaction and Language, ViGIL@NeurIPS &apos;19</title>
				<meeting>the NeurIPS 2019 Workshop on Visually Grounded Interaction and Language, ViGIL@NeurIPS &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Hamed Firooz, and Davide Testuggine</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes</title>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Firooz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems, NeurIPS &apos;20</title>
				<meeting>the Annual Conference on Neural Information Processing Systems, NeurIPS &apos;20</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: A simple and performant baseline for vision and language</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multi-answer multitask framework for real-world machine reading comprehension</title>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;18</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;18<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FastBERT: a selfdistilling BERT with adaptive inference time</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL &apos;20<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6035" to="6044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems, NeurIPS &apos;19</title>
				<meeting>the Conference on Neural Information Processing Systems, NeurIPS &apos;19<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on computational propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Cresci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">Di</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI-PRICAI &apos;20</title>
				<meeting>the International Joint Conference on Artificial Intelligence, IJCAI-PRICAI &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4826" to="4832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<idno>2021. AIMH at SemEval-2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Task 6: multimodal classification using an ensemble of transformer models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation</title>
				<meeting>the International Workshop on Semantic Evaluation<address><addrLine>Sem-Eval &apos;21, Bangkok, Thailand</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 8: Fact checking in community question answering forums</title>
		<author>
			<persName><forename type="first">Tsvetomila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation, SemEval &apos;19</title>
				<meeting>the 13th International Workshop on Semantic Evaluation, SemEval &apos;19<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="860" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Learning Representations</title>
				<meeting>the 1st International Conference on Learning Representations<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Workshop Track, ICLR &apos;13</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Techniques of Propaganda. From &quot;How to Detect and Analyze Propaganda,&quot; an address given at Town Hall. The Center for learning</title>
		<author>
			<persName><forename type="first">Clyde</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wajdi</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spas</forename><surname>Kyuchukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni Da San</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of CLEF, CLEF &apos;18</title>
				<meeting>the International Conference of CLEF, CLEF &apos;18<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2021. The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously factchecked claims, and fake news</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubén</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">Maria</forename><surname>Gautam Kishore Shahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval, ECIR &apos;21</title>
				<imprint>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">2021. 1213Li at SemEval-2021 Task 6: detection of propaganda with multi-modal attention and pre-trained models</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Peiguang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Xian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING&apos;10</title>
				<meeting>the 23rd International Conference on Computational Linguistics, COLING&apos;10<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">NLyticsFKIE at SemEval-2021 Task 6: Detection of persuasion techniques in texts and images</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Pritzkau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Capturing the style of fake news</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Przybyla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="490" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Truth of varying shades: Analyzing language in fake news and political fact-checking</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Yea</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;17</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;17<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2931" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">WVOQ at SemEval-2021 Task 6: BART for span detection and classification</title>
		<author>
			<persName><forename type="first">Cees</forename><surname>Roele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visual propaganda in the age of social media: An empirical analysis of Twitter images during the 2012 Israeli-Hamas conflict</title>
		<author>
			<persName><forename type="first">Hyunjin</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1080/15551393.2014.955501</idno>
	</analytic>
	<monogr>
		<title level="j">Visual Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">2021. Findings of the NLP4IF-2021 shared task on fighting the COVID-19 infodemic</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wajdi</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF@NAACL&apos; 21</title>
				<meeting>the Fourth Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF@NAACL&apos; 21</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Overview of CheckThat! 2020 English: Automatic identification and verification of claims in social media</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2020-Conference and Labs of the Evaluation Forum, CLEF &apos;2020</title>
				<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">War, propaganda and the media</title>
		<author>
			<persName><forename type="first">Anup</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fake news detection on social media: A data mining perspective</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">LT3 at SemEval-2021 Task 6: Using multi-modal compact bilinear pooling to combine visual and textual understanding in memes</title>
		<author>
			<persName><forename type="first">Pranaydeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;18</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;18<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MinD at SemEval-2021 Task 6: Propaganda detection using transfer learning and multimodal fusion</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Symbiotic radicalisation strategies: Propaganda tools and neuro linguistic programming</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Torok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australian Security and Intelligence Conference, ASIC &apos;15</title>
				<meeting>the Australian Security and Intelligence Conference, ASIC &apos;15<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Explaining multimodal deceptive news prediction models</title>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellyn</forename><surname>Ayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><forename type="middle">L</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuanyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web and Social Media, ICWSM &apos;19</title>
				<meeting>the International Conference on Web and Social Media, ICWSM &apos;19<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="659" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A rulebook for arguments</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Hackett Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">TeamFPAI at SemEval-2020 Task 6: BERT-MRC for propaganda techniques detection</title>
		<author>
			<persName><forename type="first">Hou</forename><surname>Xiaolong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Junsong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Lianxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Zhihao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Jianping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, SemEval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, SemEval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">YNU-HPCC at SemEval-2021 Task 6: Combining AL-BERT and Text-CNN for persuasion detection in texts and images</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation, Sem-Eval &apos;21</title>
				<meeting>the International Workshop on Semantic Evaluation, Sem-Eval &apos;21<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
