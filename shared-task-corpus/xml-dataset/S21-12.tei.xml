<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2021 Task 12: Learning with Disagreements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommaso</forename><surname>Fornaciari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Università Bocconi</orgName>
								<address>
									<settlement>Milano</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anca</forename><surname>Dumitrache</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Austrian Research Institute for Artificial Intelligence 5 University of Essex 6 IT</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Edwin</forename><surname>Simpson</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
							<email>m.poesio@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Heĳn</surname></persName>
						</author>
						<title level="a" type="main">SemEval-2021 Task 12: Learning with Disagreements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on Learning with Disagreements (L --D ) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The assumption that natural language ( ) expressions have a single and clearly identifiable interpretation in a given context, or that images have a preferred labels, still underlies most work in and computer vision. However, there is now plenty of evidence that this assumption is just a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread.</p><p>In , that annotator/coder disagreement can be genuine-i.e., resulting from debatable, difficult, or linguistic ambiguity-has long been known for anaphora and coreference <ref type="bibr" target="#b29">(Poesio and Artstein, 2005;</ref><ref type="bibr" target="#b47">Versley, 2008;</ref><ref type="bibr" target="#b34">Recasens et al., 2011)</ref>. But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging <ref type="bibr" target="#b27">(Plank et al., 2014b)</ref>, to more See also the analysis of disagreements in OntoNotes and word senses in <ref type="bibr" target="#b32">Pradhan et al. (2012)</ref>, <ref type="bibr" target="#b20">Passonneau et al. (2012), and</ref><ref type="bibr" target="#b16">Martínez Alonso et al. (2016)</ref>. complex ones like semantic role assignment <ref type="bibr" target="#b3">(Dumitrache et al., 2019)</ref>, to subjective tasks such as sentiment analysis <ref type="bibr" target="#b12">(Kenyon-Dean et al., 2018)</ref>, and to the inferences that can be drawn from sentences <ref type="bibr" target="#b22">(Pavlick and Kwiatkowski, 2019)</ref>.</p><p>In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization <ref type="bibr" target="#b36">(Rodrigues and Pereira, 2018)</ref>-in fact, possibly even more than for . In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to the same items. The problem of disagreement among coders, including experts, on the classification of noisy image data has arisen in many applications. This includes classification of astronomical images <ref type="bibr" target="#b44">(Smyth et al., 1994)</ref>, medical image classification <ref type="bibr" target="#b33">(Raykar et al., 2010)</ref>, and numerous others <ref type="bibr" target="#b39">(Sharmanska et al., 2016;</ref><ref type="bibr" target="#b36">Rodrigues and Pereira, 2018;</ref><ref type="bibr" target="#b4">Firman et al., 2018)</ref>.</p><p>Many researchers have concluded that rather than attempting to eliminate disagreements from annotated corpora, we should preserve them-indeed, some researchers have argued that corpora should aim to collect all distinct interpretations of an expression <ref type="bibr" target="#b44">(Smyth et al., 1994;</ref><ref type="bibr" target="#b29">Poesio and Artstein, 2005;</ref><ref type="bibr" target="#b0">Aroyo and Welty, 2015;</ref><ref type="bibr" target="#b39">Sharmanska et al., 2016;</ref><ref type="bibr" target="#b25">Plank, 2016;</ref><ref type="bibr" target="#b12">Kenyon-Dean et al., 2018;</ref><ref type="bibr" target="#b4">Firman et al., 2018;</ref><ref type="bibr" target="#b22">Pavlick and Kwiatkowski, 2019)</ref>. <ref type="bibr" target="#b29">Poesio and Artstein (2005)</ref> and <ref type="bibr" target="#b35">Recasens et al. (2012)</ref> suggest that the best way to create resources capturing disagreements is by preserving implicit ambiguity-i.e., having multiple annotators label the items, and then keeping all these annotations, not just an aggregated 'gold standard'. A number of corpora with these characteristics now exist <ref type="bibr" target="#b21">(Passonneau and Carpenter, 2014;</ref><ref type="bibr" target="#b26">Plank et al., 2014a;</ref><ref type="bibr" target="#b3">Dumitrache et al., 2019;</ref><ref type="bibr" target="#b31">Poesio et al., 2019;</ref><ref type="bibr" target="#b36">Rodrigues and Pereira, 2018;</ref><ref type="bibr" target="#b23">Peterson et al., 2019)</ref> Much recent research has explored the question of whether corpora of this type, besides being more accurate characterizations of the linguistic reality of language interpretation and image categorization, are also better resources for training and computer vision models, and if so, what is the best way for exploiting disagreements in modeling. Beigman <ref type="bibr" target="#b1">Klebanov and Beigman (2009)</ref> used information about disagreements to exclude items on which judgements are unclear ('hard' items). In the CrowdTruth project <ref type="bibr" target="#b0">(Aroyo and Welty, 2015;</ref><ref type="bibr" target="#b3">Dumitrache et al., 2019)</ref> information about disagreement is used to weigh the items used for training. <ref type="bibr" target="#b26">Plank et al. (2014a)</ref> proposed to use the information about disagreement to supplement the gold label during training. Finally, methods were proposed for training directly from the data with disagreements, without first obtaining an aggregated label <ref type="bibr" target="#b40">(Sheng et al., 2008;</ref><ref type="bibr" target="#b36">Rodrigues and Pereira, 2018;</ref><ref type="bibr" target="#b23">Peterson et al., 2019;</ref><ref type="bibr" target="#b45">Uma et al., 2020)</ref>. Only limited comparisons of these methods have been carried out <ref type="bibr" target="#b11">(Jamison and Gurevych, 2015)</ref>, and the sparse research landscape remains fragmented; in particular, methods applied in</p><p>have not yet been tested in , and vice versa. The objective of SemEval-2021 Task 12, Learning with Disagreements (L --D ), was to provide a unified testing framework for learning from disagreements in and using datasets containing information about disagreements for interpreting language and classifying images. The expectation being that unifying research on disagreement from different fields may lead to novel insights and impact widely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task organization</head><p>In order to provide a thorough benchmark for methods for learning from disagreements, we identified five well-known datasets for very different and tasks, all characterized by providing a multiplicity of labels for each instance, by having a size sufficient to train state-of-the-art models, and by evincing different characteristics in terms of the crowd annotators and data collection procedure. We found or developed near-state-of-the-art models for the tasks represented by these datasets. Both 'hard' and 'soft' evaluation metrics were employed <ref type="bibr">(Uma et al., n.d.)</ref>.</p><p>The shared task was set up on the CodaLab Competitions platform, which enables training and uniform evaluation on these datasets, such https://www.microsoft.com/en-us/research/project/ codalab/ that the crowd learning adaptations of the base models proposed by participants to the task would be directly comparable.</p><p>In this section, we briefly introduce the five datasets included in the benchmark and our evaluation criteria. We also elaborate on the setup of the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>There are by now quite a few datasets preserving disagreements, and covering many levels of language interpretation; remarkably, none of these has ever been used for a shared task like the one we are proposing, and the majority of them have never been used for a shared task at all. Our shared task has aimed at leveraging this diversity. The datasets included are outlined in this section and their characteristics are summarized in Table <ref type="table" target="#tab_2">1</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows the observed agreement of each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The Gimpel et al. corpus</head><p>One widely used resource for developing disagreement-aware models is the dataset of Twitter posts annotated with tags collected by <ref type="bibr" target="#b6">Gimpel et al. (2011)</ref>. <ref type="bibr" target="#b27">Plank et al. (2014b)</ref> mapped the Gimpel tags to the universal tag set <ref type="bibr" target="#b24">(Petrov et al., 2012)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The corpus</head><p>The Phrase Detectives corpus <ref type="bibr" target="#b31">(Poesio et al., 2019)</ref> is a crowdsourced coreference corpus collected  with the Phrase Detectives gamified online platform <ref type="bibr" target="#b30">(Poesio et al., 2013)</ref>. We use , a simplified version of the corpus containing only binary information status labels: Discourse New (the entity referred to has never been mentioned before) and Discourse Old (it has been mentioned). consists of 542 documents, for a total of 408K tokens and over 96K markables. These documents were annotated by game players who produced an average of 11.87 annotations per markable.</p><p>Forty-five of the documents (5.2K markables), collectively called gold , additionally contain expert-adjudicated gold labels. This subset of was designated as the test set. The training and development datasets consist of 473 documents (and 86.9K markables) and 24 documents (4.2K markables) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">The Humour dataset</head><p>The comprehension and appreciation of humour is known to vary across individuals <ref type="bibr" target="#b37">(Ruch, 2008)</ref>, making disagreement over the perceived funniness of jokes an appealing subject of study. For our training data, we used the corpus of <ref type="bibr" target="#b42">Simpson et al. (2019)</ref>, which consists of 4,030 short texts (3398 jokes, mostly based on puns, and 632 non-jokes such as proverbs and aphorisms). 28,210 unique pairings of these texts were presented to five crowdsourcers each, who indicated which text in the pair (if either) they found to be funnier. The goal is to learn a model that can predict binary pairwise labels that can predict which of two short texts is funnier.</p><p>The 4,030 text instances were split into 60% (2,418 texts, 9,916 unique pairs) for the training set and 20% (806 texts, 1,086 unique pairs) for the development set. Since this dataset has already been published, we constructed a new test dataset along similar lines: 1,000 short texts (all punning jokes) were paired in 7,000 different ways, and each of these 7,000 pairs was then presented to five crowd workers for a preference judgement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">The</head><p>-10 corpus <ref type="bibr" target="#b13">Krizhevsky's (2009)</ref> -10 dataset consists of 60K tiny images from the web, carefully labelled and expert-adjudicated to produce a single gold label for each image in one of 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. <ref type="bibr" target="#b23">Peterson et al. (2019)</ref> collected crowd annotations for 10K images from this dataset (the designated test portion) using Amazon Mechanical Turk, creating the -10 dataset which we use for this shared task.</p><p>We randomly selected 7K, 1K, and 2K images for training, development and testing respectively. We kept as much data as we could for training without jeopardizing the evaluation process, as the base model was found to be sensitive to data size. As with the original dataset, each subset we created contains an equal number of images per category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation metrics</head><p>While recent research questions the assumption that a single 'hard' label (a gold label) exists for every employed, paid in line with the federal minimum wage.</p><p>http://labelme.csail.mit.edu/Release3.0 https://github.com/jcpeterson/cifar-10h item in a dataset, the models proposed for learning from multiple interpretations are still largely evaluated under this assumption, using 'hard' measures like accuracy or class-weighted F 1 <ref type="bibr" target="#b40">(Sheng et al., 2008;</ref><ref type="bibr" target="#b26">Plank et al., 2014a;</ref><ref type="bibr" target="#b17">Martínez Alonso et al., 2015;</ref><ref type="bibr" target="#b39">Sharmanska et al., 2016;</ref><ref type="bibr" target="#b36">Rodrigues and Pereira, 2018)</ref>. For reference and comparison reasons, we also evaluate the models produced for this shared task using F 1 . However, a way of evaluating models as to their ability to capture disagreement is needed, especially for datasets with substantial extent of disagreement. The simplest 'soft' metric of this type is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to the full distribution produced by annotators, using, for example, cross-entropy. This approach was adopted in, inter alia, <ref type="bibr" target="#b23">(Peterson et al., 2019;</ref><ref type="bibr" target="#b45">Uma et al., 2020)</ref>. <ref type="bibr" target="#b23">Peterson et al. (2019)</ref> tested this approach on image classification tasks, generating the soft label by transforming the item annotation distribution using standard normalization. In this shared task we also use standard normalization to produce soft labels for the humour dataset. <ref type="bibr" target="#b45">Uma et al. (2020)</ref> show that the choice of soft label encoding function depends on the characteristics of the dataset. For and -, they show that a softmax function over the annotator distribution is preferable over standard normalization. On the other end, for</p><p>, training a soft-loss model using the posterior probability produced by <ref type="bibr">Hovy et al.'s (2013)</ref> probabilistic aggregation model as a soft label produces predictions that a most accurate with respect to the gold.</p><p>Therefore, in this shared task we used different soft label encoders to generate soft labels from annotator distributions for the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task setup</head><p>CodaLab was the designated site for hosting SemEval-2021 competitions. L --D was run in two main phases: Practice phase. In the practice phase, the goal was to train models for each task to learn from crowd annotations, given (1) the training data (consisting of raw and preprocessed input data and crowd annotations), (2) the development data with no labels, and (3) the base models (discussed in Section 3). While participants were encouraged to start with the Our competition can be found at https://competitions. codalab.org/competitions/25748. base models and extend them, we did not make this mandatory. Participants could test the performance of their models on the development set by making predictions on the given development input data and then uploading their submissions to CodaLab for preliminary testing. We permitted up to 999 submissions in this phase. The 'leader board' was made public to allow participants not only to see how their models performed, but also to compare the performance of their model to those submitted by other participants.</p><p>Evaluation phase. The evaluation phase was the official testing phase of the competition. In this phase, we released test data (without labels) but we also released the gold labels and crowd annotations for the development set to facilitate quick offline testing and refining of models and model selection. The number of submissions for this phase was limited to ten submissions per participant to prevent the participants from fine-tuning their models on the test data. The allowed number of submissions was later increased to 999 to more encourage submission attempts. The leader board was also kept public in this phase. Each participant could see the best model of each of the tasks using each of the evaluation metrics.</p><p>Post-campaign evaluation. As our aim was to make this benchmark available beyond the competition to researchers developing disagreement-aware models, we included a third, post-evaluation phase to allow lifetime access to the data. Researchers participating in this phase will be able to access the same data as in the evaluation phase and test their models on the test data for the various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Base models and baselines</head><p>In order to encourage the participants to focus on the development of methods for learning from disagreement, as opposed to achieving higher performance by developing better models, we provided 'base' models for each of the tasks represented by the aforementioned corpora. In this section, we briefly discuss the baseline models for each task that we provided. In Section 5, we report the results using these base models and two crowd learning approaches: majority voting and the soft loss method <ref type="bibr" target="#b23">(Peterson et al., 2019;</ref><ref type="bibr" target="#b45">Uma et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The</head><p>tagging model. The tagger is a bi-  with additional use of attention over the input word and character embeddings, as used in <ref type="bibr" target="#b45">Uma et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The</head><p>classification model. The model for this task was developed by comparing architectures from two models: a state-of-the-art coreference model and a state-of-the-art classification model. We combined the mention representation component of <ref type="bibr">Lee et al.'s (2018)</ref> coreference resolution system with the mention sorting and non-syntactic feature extraction components of the classification model proposed by <ref type="bibr" target="#b8">Hou (2016)</ref> to create a novel classification model that outperforms Hou (2016) on the corpus. The training parameters were set following <ref type="bibr" target="#b14">Lee et al. (2018)</ref>.</p><p>The humour preference learning model. We use as base model for this task Gaussian process preference learning (</p><p>) with stochastic variational inference, as described and implemented by <ref type="bibr" target="#b43">Simpson and Gurevych (2020)</ref>. As an input vector to , we first take the mean word embedding of a text, using 300-dimensional word2vec embeddings trained on the Google News corpus <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref>. Then, we compute the frequency of each unigram in the text in a 2017 Wikipedia dump, and each bigram in the text in a Google Books Ngram dataset. Finally, we concatenate the mean unigram and bigram frequencies with the mean word embedding vector to obtain the input vector representation for each short text. The model is trained on pairwise labels from the training set to obtain a ranking function that can be used to score test instances or output pairwise label probabilities. As a Bayesian model, it takes into account sparsity and noise in the crowdsourced training labels, and moderates its confidence accordingly. Hence, it is a strong baseline for accounting for disagreement among annotators. This same approach set the previous state of the art on the humour dataset <ref type="bibr" target="#b42">(Simpson et al., 2019)</ref>.</p><p>The LabelMe image classification model. For this task, we replicated the model from <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref>. The images were encoded using pretrained layers of the -16 deep neural network <ref type="bibr" target="#b41">(Simonyan et al., 2013)</ref>. This encoding is passed into a feed-forward neural network layer This model was developed for fine-grained information status classification on the corpus <ref type="bibr" target="#b15">(Markert et al., 2012;</ref><ref type="bibr" target="#b9">Hou et al., 2013)</ref>.</p><p>with a e activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax activation to produce the model's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The</head><p>-10 image classification model. The trained model provided for this task is the ResNet-34A model <ref type="bibr" target="#b7">(He et al., 2016)</ref>, a deep residual framework which is one of the best performing systems for the -10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participating systems</head><p>Unfortunately, we observed a dramatic difference in the number of participants that signed up to the competition (over 100 groups), the number of groups that participated in the trial phase, and the number of groups that submitted a run for official evaluation.</p><p>Only one group, , submitted in the evaluation phase <ref type="bibr" target="#b19">(Osei-Brefo et al., 2021)</ref>. However, they did submit models for each of the tasks, and did adopt a learning from disagreements approach.</p><p>tagging. For tagging, developed a novel tagging model by fine-tuning the language model <ref type="bibr" target="#b2">(Devlin et al., 2019</ref> where the '[ ]' token was added for classification and the '[ ]' token separated the tweet from the token under consideration. To learn the class for the token, the learned classification token was passed through a single feed-forward neural network layer with softmax activation. The output of this layer represented the probabilities of the token belonging to each of the 12 classes.</p><p>To extend this model for crowd learning, added an adaptation of the crowd layer from <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref>. Rather than compute a single loss from the crowd layer as <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref> do, compute a joint loss from both the crowd layer and the base model (without the crowd layer bottleneck). https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10 Two participating groups cited an inability to come up with a novel crowd learning paradigm as the reason they did not submit for official evaluation. Humour preference learning. For humour preference learning, the participant submitted predictions using the base model without modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LabelMe image classification ( -).</head><p>For this task, adapted the <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref> crowd layer to the base model.</p><p>-10 image classification ( -10 ). For -10 , the crowd labels were aggregated into hard labels using majority voting. However, combined Zagoruyko and Komodakis's (2016) WideResNet model, which has been shown to outperform <ref type="bibr">He et al.'s (2016)</ref> ResNet with the novel Sharpness-Aware Minimization (</p><p>) optimization technique, proposed by <ref type="bibr" target="#b5">Foret et al. (2020)</ref>, that has been shown to efficiently improve model generalization, especially on noisy, singly labelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>Table <ref type="table">2</ref> contains the results of various models discussed in Sections 3 and 4 on this shared task when evaluated based on the hard metric (i.e., the class-weighted F 1 with respect to the gold labels) and the soft metric (the cross-entropy between the soft labels for each task-see Section 2.2-and the model prediction for that task). The best results for each task are highlighted in bold.</p><p>concentrated their effort on the -10 dataset, on which they did achieve good results and outperformed the baseline (see below). In the other datasets, their official results at the end of the evaluation phase were less competitive.</p><p>With the and datasets, the model proposed by , adding a crowd layer on top of , achieved substantially worse results than training from a label aggregated using majority voting or training using a soft-loss function, both according to the hard evaluation metric (F 1 ) and the soft metric ( ). The ranking between soft-loss method, aggregation, and crowd layer with is consistent with that obtained by <ref type="bibr">Uma et al. (n.d.)</ref>, but the results obtained by are much worse for reasons that will require further investigation. <ref type="bibr">(With , Uma et al. (n.d.)</ref> obtain comparable results with soft-loss functions and with the crowd layer.) More generally, the results show that although the hard label (the majority voting aggregate of the annotator distribution) and the soft label (a probability distribution encoding of the annotator distribution) were drawn from the same annotator distribution with this dataset, given the same base model, training by targeting the soft label (base model + soft loss) outperforms training using majority voting aggregates (base model + majority voting) regardless of which evaluation metric is used to compare the models.</p><p>For the humour preference learning task, again, the base model outperforms 's submission on both metrics, but in this case the difference in performance between and is much less substantial with the hard metric, although it remains large according to the soft metric. This large difference may be due to a technical issue that requires further investigation, since 's submission was also supposed to have been produced by the same base system. A possible reason for poor crossentropy error is the use of discrete labels, which are heavily penalized for overconfidence by crossentropy error. On this soft metric, the Bayesian probabilistic approach of may have advantages over approaches with poorer calibration, which remains to be explored in future work. The approach therefore remains the state of the art with this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For</head><p>-, again, soft-loss training achieved better hard and soft scores than both aggregation training with majority voting labels and the extension of the base model using a crowd layer adapted from <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref>. The finding that the group's adaption of the Rodrigues and Pereira (2018) crowd layer yielded lower F 1 than training using majority voting is unexpected, given that in <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref>; <ref type="bibr" target="#b45">Uma et al. (2020)</ref> and <ref type="bibr">Uma et al. (n.d.)</ref>, the crowd layer, particularly the -variant, was shown to be a competitive approach to learning from crowds and always outperforms majority voting. However, 's crowd layer does achieve better soft evaluation (cross-entropy) scores than majority voting.</p><p>There is one dataset, however, on which outperformed the two baselines:</p><p>-   <ref type="table">2</ref>: Results on the benchmarks and participant submissions on all the tasks using F 1 (higher is better) and cross-entropy (lower is better) <ref type="bibr">Foret et al.'s (2020)</ref> optimization technique. The results show that WideResNet outperforms ResNet with this task both according to the hard metric and the soft metric. Interestingly, this is the one dataset in which the Deep Learning from Crowds approach of <ref type="bibr" target="#b36">Rodrigues and Pereira (2018)</ref> works best according to <ref type="bibr">Uma et al. (n.d.)</ref>, outperforming both soft-loss training and majority voting training. It would thus be interesting to understand if the performance of 's model could be further increased by adopting one of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This shared task presented the first unified testing framework for learning with disagreements. The datasets include sequence labelling, three classification tasks, and preference learning, hence provide a testbed for a wide range of challenges when learning from multiple annotators. We proposed to evaluate not just the 'hard' performance against a gold standard, but also the ability to predict the distribution of different interpretations of the data-that is, the alternative labellings provided by different annotators. The results show the benefit of soft loss functions that account for the distribution of labels in the training data. However, modelling alternative As a postscript, we should note that after the end of the official competition we did carry out an investigation of the reasons for the poor performance of 's models on the tasks other than -10 . Some points emerging from the discussion are presented in the participants' paper for the shared task. interpretations of data remains an under-researched topic in and computer vision. To encourage future work on learning with disagreements, the shared task and datasets will remain available for evaluating new methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Observed Agreement for each dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of dataset characteristics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>5K, and 2.5K images for training, development, and testing respectively, careful to keep the label proportions in each subset close to the proportions in the 10K dataset.</figDesc><table><row><cell>2.1.4 The LabelMe corpus</cell></row><row><cell>Much research on learning from disagreements</cell></row><row><cell>was motivated by computer vision datasets, so we</cell></row><row><cell>intended to include some of these, too. Possibly</cell></row><row><cell>the most widely used such corpus is the LabelMe</cell></row><row><cell>dataset (Russell et al., 2008). It classifies outdoor</cell></row><row><cell>images according to 8 categories: highway, inside</cell></row><row><cell>city, tall building, street, forest, coast, mountain</cell></row><row><cell>or open country. Using Amazon Mechanical Turk,</cell></row><row><cell>Rodrigues and Pereira (2018) collected an average</cell></row><row><cell>of 2.5 annotations per image from 59 annotators for</cell></row><row><cell>10K images in this dataset.</cell></row><row><cell>We randomly selected 5K, 2.</cell></row><row><cell>https://github.com/dali-ambiguity</cell></row><row><cell>-based workers from Amazon Mechanical Turk were</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This proved unnecessary as the inherent difficulty of the shared task was enough of a deterrent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Truth is a lie: Crowd truth and the seven myths of human annotation</title>
		<author>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v36i1.2564</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From annotator agreement to noise models</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName><surname>Beigman</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2009.35.4.35402</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="503" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A crowdsourced frame disambiguation corpus with ambiguity</title>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dumitrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2164" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DiverseNet: When one right answer is not enough</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><surname>Brostow</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00587</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5598" to="5607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization. CoRR, abs</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for Twitter: Annotation, features, and experiments</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incremental fine-grained information status classification using attention-based LSTMs</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1880" to="1890" />
		</imprint>
	</monogr>
	<note>The COL-ING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global inference for bridging anaphora resolution</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="907" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning whom to trust with MACE</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise or additional information? Leveraging crowdsource annotation item agreement for natural language tasks</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentiment analysis: It&apos;s complicated!</title>
		<author>
			<persName><forename type="first">Kian</forename><surname>Kenyon-Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eisha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Georges-Filteau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Glasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barleen</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhanderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Belfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirmal</forename><surname>Kanagasabai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Sarrazingendron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Ruths</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1886" to="1895" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective classification for fine-grained information status</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supersense tagging with interannotator disagreement</title>
		<author>
			<persName><forename type="first">Alonso</forename><surname>Héctor Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-1706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Linguistic Annotation Workshop</title>
				<meeting>the 10th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to parse with IAA-weighted loss</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Skjaerholt</surname></persName>
		</author>
		<author>
			<persName><surname>Søgaard</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1357" to="1361" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
				<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UOR at SemEval-2021 Task 12: On crowd annotations; learning with disagreements to optimise crowd truth</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Osei-Brefo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanet</forename><surname>Markchom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhi</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansaf</forename><surname>Salleb-Aouissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Ide</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-012-9188-x</idno>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The benefits of a model of annotation</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00185</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="311" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inherent disagreements in human textual inferences</title>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00293</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human uncertainty makes classification more robust</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruairidh</forename><forename type="middle">M</forename><surname>Battleday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00971</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the 2019 IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9616" to="9625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
				<meeting>the 8th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What to do about non-standard (or non-canonical) language in NLP</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Natural Language Processing</title>
				<meeting>the 13th Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning part-of-speech taggers with inter-annotator agreement loss</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/E14-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
				<meeting>the 14th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistically debatable or just plain wrong?</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2083</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="507" to="511" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2067</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky</title>
				<meeting>the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Phrase detectives: Utilizing collective intelligence for internet-scale language resource creation</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ducceschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2448116.2448119</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Interactive Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A crowdsourced corpus of multiple judgments and disagreement on anaphoric interpretation</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Paun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1778" to="1789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes, Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes, Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identity, non-identity, and near-identity: Addressing the complexity of coreference</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.lingua.2011.02.004</idno>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1152" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Annotating near-identity from coreference disagreements</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
				<meeting>the 8th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning from crowds</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1611" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Psychology of humor</title>
		<author>
			<persName><forename type="first">Willibald</forename><surname>Ruch</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110198492.17</idno>
	</analytic>
	<monogr>
		<title level="m">The Primer of Humor Research, number 8 in Humor Research</title>
				<editor>
			<persName><surname>Victor Raskin</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Mouton de Gruyter</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LabelMe: A database and Web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-007-0090-8</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ambiguity helps: Classification with disagreements in crowdsourced annotations</title>
		<author>
			<persName><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.241</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2194" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Get another label? Improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foster</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><surname>Ipeirotis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1401890.1401965</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predicting humorousness and metaphor novelty with Gaussian process preference learning</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik-Lân Do</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5716" to="5728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable Bayesian preference learning for crowds</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-019-05867-2</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="689" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inferring ground truth from subjective labelling of venus images</title>
		<author>
			<persName><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usama</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Neural Information Processing Systems</title>
				<meeting>the 7th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A case for soft-loss functions</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Paun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th AAAI Conference on Human Computation and Crowdsourcing</title>
				<meeting>the 8th AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Paun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">and Massimo Poesio. n.d. Learning from disagreements</title>
		<imprint/>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vagueness and referential ambiguity in a large-scale annotated corpus</title>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11168-008-9059-1</idno>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
