<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mokanarangan</forename><surname>Thayaparan</surname></persName>
							<email>mokanarangan.thayaparan@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Valentino</surname></persName>
							<email>marco.valentino@manchester.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
							<email>pajansen@email.arizona.edu</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
							<email>dustalov@yandex-team.ru</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Crowdsourcing Research Group Yandex</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness -finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-hop inference is the task of aggregating more than one fact to perform an inference. In the context of natural language processing, multi-hop inference is typically evaluated using auxiliary tasks such as question answering, where multiple sentences from external corpora need to be retrieved and composed Figure <ref type="figure">1</ref>: The motivating example provided to participants. Given a question and correct answer (top), the explanation regeneration task requires participating models to find sets of facts that, taken together, provide a detailed chain-of-reasoning for the answer <ref type="bibr">(bottom)</ref>. This 2021 instantiation of the shared task focuses on the subtask of collecting the most relevant facts for building explanations.</p><p>to form reasoning chains that support the correct answer (see Figure <ref type="figure">1</ref>). As such, multi-hop inference represents a crucial step towards explainability in complex question answering, as the set of supporting facts can be interpreted as an explanation for the underlying inference process <ref type="bibr" target="#b30">(Thayaparan et al., 2020)</ref>.</p><p>Constructing long inference chains can be extremely challenging for existing models, which generally exhibit a large drop in performance when composing explanations and inference chains requiring more than 2 inference steps <ref type="bibr" target="#b10">(Fried et al., 2015;</ref><ref type="bibr" target="#b16">Jansen et al., , 2018</ref><ref type="bibr" target="#b20">Khashabi et al., 2019;</ref><ref type="bibr" target="#b38">Yadav et al., 2020)</ref>. To this end, this Shared Task on Multi-hop Inference for Explanation Regeneration <ref type="bibr" target="#b12">(Jansen and</ref><ref type="bibr">Ustalov, 2019, 2020)</ref> has focused on expanding the capacity of models to compose long inference chains, where participants are asked to develop systems capable of reconstructing detailed explanations for science exam questions drawn from the WorldTree explanation corpus <ref type="bibr" target="#b37">(Xie et al., 2020;</ref><ref type="bibr" target="#b16">Jansen et al., 2018)</ref>, which range in compositional complexity from 1 to 16 facts (with the average explanation including 6 facts).</p><p>Large explanations are typically evaluated on two dimensions: relevance and completeness. Relevance refers to whether each fact in an explanation is relevant, topical, and required to complete the chain of inference that moves from question to correct answer. Conversely, completeness evaluates whether the entire set of facts in the explanation, together, composes a complete chain of inference from question to answer, without significant gaps. In practice, both of these are challenging to evaluate automatically <ref type="bibr" target="#b1">(Buckley and Voorhees, 2004;</ref><ref type="bibr" target="#b33">Voorhees, 2002)</ref>, given that multi-hop datasets typically include a single example of a complete explanation, in large part due to the time and expense associated with generating such annotation. Underscoring this difficulty, post-competition manual analyses on participating systems in the previous two iterations of this shared task showed that models may be performing up to 20% better at retrieving correct facts to build their explanation from, highlighting this significant methodological challenge.</p><p>This 2021 instantiation of the Shared Task on Explanation Regeneration focuses on the theme of determining relevance in large multi-hop explanations. To this end, participants were given access to a large pre-release dataset of approximately 250k explanatory relevancy ratings that augment the 2020 shared task data <ref type="bibr" target="#b15">(Jansen and Ustalov, 2020)</ref>, and were tasked with ranking the facts most critical to assembling large explanations for a given question highest. Similarly to the previous instances of our competition, the shared task has been organized on the CodaLab platform. <ref type="bibr">1</ref> We released train and development datasets along with the baseline solution in advance to allow one to get to know the task specifics. <ref type="bibr">2</ref> We ran the practice 1 https://competitions.codalab.org/ competitions/23615 2 https://github.com/cognitiveailab/ tg2021task phase from February 15 till March 9, 2021. Then we released the test dataset without answers and ran the official evaluation phase from March 10 till March 24, 2021. After that we established postcompetition phase to enable long-term evaluation of the methods beyond our shared task. Participating systems substantially increased task performance compared to a supplied baseline system by 32%, while achieving moderate overall absolute task performance -highlighting both the success of this shared task, as well as the continued challenge of determining relevancy in large multi-hop inference problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Drift. Multi-hop question answering systems suffer from the tendency of composing out-of-context inference chains as the number of required hops (aggregated facts) increases. This phenomenon, known as semantic drift, has been observed in a number of works <ref type="bibr" target="#b10">(Fried et al., 2015;</ref><ref type="bibr" target="#b12">Jansen, 2017)</ref>, which have empirically demonstrated that multi-hop inference models exhibit a substantial drop in performance when aggregating more than 2 facts or paragraphs. Semantic drift has been observed across a variety of representations and traversal methods, including word and dependency level <ref type="bibr" target="#b26">(Pan et al., 2017;</ref><ref type="bibr" target="#b10">Fried et al., 2015)</ref>, sentence level , and paragraph level <ref type="bibr" target="#b6">(Clark and Gardner, 2018)</ref>. <ref type="bibr" target="#b20">Khashabi et al. (2019)</ref> have demonstrated that ongoing efforts on "very long" multi-hop reasoning are unlikely to succeed without the adoption of a richer underlying representation that allows for reasoning with fewer hops.</p><p>Many-hop multi-hop training data. There is a recent explosion of explanation-centred datasets for multi-hop question answering <ref type="bibr" target="#b18">(Jhamtani and Clark, 2020;</ref><ref type="bibr" target="#b37">Xie et al., 2020;</ref><ref type="bibr" target="#b16">Jansen et al., 2018;</ref><ref type="bibr" target="#b39">Yang et al., 2018;</ref><ref type="bibr" target="#b30">Thayaparan et al., 2020;</ref><ref type="bibr" target="#b35">Wiegreffe and Marasović, 2021)</ref>. However, most of these datasets require the aggregation of only two sentences or paragraphs, making it hard to evaluate the robustness of the models in terms of semantic drift. On the other hand, the WorldTree corpus <ref type="bibr" target="#b37">(Xie et al., 2020;</ref><ref type="bibr" target="#b16">Jansen et al., 2018)</ref> used in this shared task is explicitly designed to test multi-hop inference models on the reconstruction of long inference chains requiring the aggregation of an average of 6 facts, and as many as 16 facts.</p><p>Question: Which of the following best explains why the Sun appears to move across the sky every day?</p><p>Answer: Earth rotates on its axis.</p><p>Explanatory Relevance Ratings # Fact (Table <ref type="table">Row</ref>) Relevance 1</p><p>The Earth rotating on its axis causes the Sun to appear to move across the sky during the day 6 2</p><p>If a human is on a rotating planet then other celestial bodies will appear to move from that human's perspective due to the rotation of that planet 6 3 The Earth rotates on its tilted axis 6 4</p><p>Diurnal motion is when objects in the sky appear to move due to Earth's rotation on its axis 6 5</p><p>Apparent motion is when an object appears to move relative to another object's perspective / another object 's position 5 6</p><p>Earth rotating on its axis occurs once per day 4 7</p><p>Rotation is a kind of motion 4 8</p><p>A rotation is a kind of movement 4 9</p><p>The Sun sets in the west 2 10 The Sun is a kind of star 2 11 Earth is a kind of planet 2 12 Earth's angle of tilt causes the length of day and night to vary 0 13 The Earth being tilted on its rotating axis causes seasons 0 14 Revolving is a kind of motion 0 15 The Earth revolving around the Sun causes stars to appear in different areas in the sky at different times of year 0 Table <ref type="table">1</ref>: An example of the relevance ratings used in the 2021 shared task. (top) The question and correct answer.</p><p>(bottom) Facts from the corpus, and their associated relevance rating, sorted from most-relevant to least-relevant.</p><p>While the dataset provides manual relevancy ratings for the top 30 rows, only 15 are shown here for space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation regeneration approaches on</head><p>WorldTree. A number of approaches have been proposed for the explanation regeneration task on WorldTree, including those from previous iterations of this shared task. These approaches adopt a set of diverse techniques ranging from graph-based learning , to Transformer-based language models <ref type="bibr" target="#b2">(Cartuyvels et al., 2020;</ref><ref type="bibr" target="#b8">Das et al., 2019;</ref><ref type="bibr" target="#b28">Pawate et al., 2020;</ref><ref type="bibr" target="#b4">Chia et al., 2019)</ref>, Integer Linear Programming <ref type="bibr" target="#b11">(Gupta and Srinivasaraghavan, 2020)</ref>, and sparse retrieval models <ref type="bibr" target="#b32">(Valentino et al., 2021;</ref><ref type="bibr" target="#b4">Chia et al., 2019)</ref>.</p><p>The current state-of-the-art on the explanation regeneration task is represented by a model that employs a combination of language models and Graph Neural Networks (GNN) , with the bulk of performance contributed from the language model. Strong performance is also achieved by transformer models adapted to rank inference chains <ref type="bibr" target="#b8">(Das et al., 2019)</ref> or operating in an iterative and recursive fashion <ref type="bibr" target="#b2">(Cartuyvels et al., 2020)</ref>. In contrast with neural-based models, recent works <ref type="bibr" target="#b32">(Valentino et al., 2021)</ref> have shown that the explanatory patterns emerging in the WorldTree corpus can be leveraged to improve sparse retrieval models and provide a viable way to alleviate semantic drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Following the previous editions of the shared task, we frame explanation generation as a ranking problem. Specifically, for a given science question, a model is supplied both the question and correct answer text, and must then selectively rank all the atomic scientific and world knowledge facts in the knowledge base such that those that were labelled as most relevant to building an explanation by a human annotator are ranked the highest. Additional details on the ranking problem are described in the 2019 shared task summary paper <ref type="bibr" target="#b14">(Jansen and Ustalov, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training and Evaluation Dataset</head><p>Questions and Explanations: The 2021 shared task adopts the same set of questions and knowledge base included in the 2020 shared task <ref type="bibr" target="#b15">(Jansen and Ustalov, 2020)</ref>, with additional relevance annotation described below. The questions and explanations are drawn from the WorldTree V2 explanation corpus <ref type="bibr" target="#b37">(Xie et al., 2020)</ref>, a set of detailed multi-fact explanations to standardized elementary and middle-school science exam questions drawn from the Aristo Reasoning Challenge (ARC) corpus   Relevancy Ratings: The WorldTree V2 dataset used in previous iterations of the shared task includes a single complete explanation per question, supplied as a list of binary classifications that describe which facts are included in the gold explanation for a given question. This 2021 edition of the shared task augments these original WorldTree explanations with a pre-release dataset 3 of approximately 250,000 manual relevancy ratings. Specifically, for each question in the corpus, a set of 30 facts determined to be the most likely facts relevant to building an explanation were manually assigned relevancy ratings by annotators. Ratings are on a 7-point scale (0-6), where facts rated as a 6 are the most critical to building an explanation, while facts rated as 0 are unrelated to the question. An example of these relevance ratings is shown in Table <ref type="table">1</ref>.</p><p>Evaluation Metrics: Historically, performance on the explanation regeneration task was evaluated using Mean Average Precision (MAP) , using the binary ratings (gold or not gold) associated with each fact for a given explanation. To leverage the new graded annotation schema, here we switch to evaluate system performance using Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" target="#b17">(Järvelin and Kekäläinen, 2002;</ref><ref type="bibr" target="#b34">Wang et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System Descriptions and Performance</head><p>The 2021 shared task received 4 submissions, with 3 teams choosing to submit system description papers. The performance of the submitted systems are shown in Table <ref type="table" target="#tab_1">2</ref>. Overall, we observe that all participating teams substantially improved upon the NDCG score achieved by the baseline model, with increases of up to 30%. In this section, we summarize the key features of the approaches proposed by the teams.</p><p>Baseline (tf.idf). We adopt a term frequencyinverse document frequency (tf.idf) baseline (see, e.g. <ref type="bibr">Manning et al., 2008, Ch. 6</ref>). Specifically, given a question and its correct answer, the baseline calculates the cosine similarity between a query vector (representing the question and correct answer) and document vectors (representing a given fact) for each fact in the knowledge base. The model then adopts the tf.idf weighting scheme to rank each fact in the knowledge base for a given question-answer pair. This baseline achieves a NDCG score of 0.501 on the test set.</p><p>DeepBlueAI. The model presented by <ref type="bibr" target="#b27">Pan et al. (2021)</ref> represents the top-performing system in this edition of the shared task with a NDCG score of 0.820 -representing a substantial 32% improvement when compared to the tf.idf baseline. The model employs a two step retrieval strategy. In the first step, a pre-trained language model is finetuned to retrieve the top-K (K &gt; 100) relevant facts for each question and answer pair. Subsequently, the same architecture is adopted to build a re-ranking model to refine the list of the top-K candidate facts. The authors propose the use of a triplet loss for the fine-tuning of the model. Specifically, the triplet loss minimizes the distance between an anchor and a positive example, while maximizing the distance between the same anchor and a negative example. The team treats question and correct answer as the anchor, while the facts annotated with high ratings are adopted as positive examples. Different experiments are conducted with three negative sampling strategies for retrieval and re-ranking. The best results are obtained when sampling negative examples from the same tables of highly relevant facts. The authors find that the best performance is obtained when averaging the results from RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> and ERNIE 2.0  with different random seeds.</p><p>RedDragonAI. The system developed by Kalyan et al. (2021) combines iterative information retrieval with an ensemble of language models, achieving a NDCG score of 0.771. The first step of the proposed approach is to retrieve a limited number of facts to be subsequently re-ranked by language models. The first step is a modification of the approach proposed by Chia et al. ( <ref type="formula">2020</ref>), where the model iteratively selects the closest n   facts to the question using BM25 vectors and then update the query vector via a max operation. The iterative retrieval step is performed until a list of K = 200 facts is selected from the knowledge base. Subsequently, the top K explanation facts are re-ranked using language models. The best model consists of an ensemble of BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and SciBERT <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref>. These models are fine-tuned to predict the target explanatory relevance ratings using the following input: Question + Answer [SEP] Explanation. Specifically, the authors frame the problem as a regression via mean squared error loss. The ensemble is achieved by linearly combining the scores of the models. The authors reported two negative results obtained using a two-stage approach and different negative sampling techniques. In the two-stage approach, the facts were firstly categorized using binary scores to discriminate between relevant and irrelevant sentences, and then re-ranked predicting the target explanatory relevance rating. Regarding the negative sampling strategy, the authors noticed that highest percentage of errors occurring at inference time was due to irrelevant facts that are lexically close to highly relevant explanation sentences. They attempted to alleviate this problem by randomly sampling facts from the knowledge base and retrieving close negative examples during training. Neither of these two methods resulted in significant improvements.</p><p>Google-BERT. <ref type="bibr" target="#b36">Xiang et al. (2021)</ref> propose a framework composed of three main steps. In the first step, the model adopts a simple tf.idf model with cosine similarity to retrieve the top-K relevant explanation sentences (K = 50) for each question and correct answer pair. In the second step, the authors employ an autoregressive model which selects the most relevant facts in a iterative manner. Specifically, the authors propose the adoption of a BERT-based model <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> that selects the facts at iteration n given the facts retrieved in the previous step. The model uses up to 4 iterations. Finally, the authors employ a re-ranking module to re-score the retrieved candidate explanations computing the relevance between each fact and the question-answer pairs. The re-ranking model is implemented using a BERT model for binary classification. The ablation study shows that the first two steps allow achieving a performance of 0.679 NDCG, that is improved up to 0.700 NDCG using the re-ranking model. Moreover, the experiments show that the best performance is achieved when the re-ranking model is adopted to re-score the top K = 30 facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Detailed Analysis</head><p>In order to better understand the behavior and contribution of the proposed systems, we perform a detailed analysis by grouping the explanatory facts in the supporting knowledge base in different categories. Specifically, we adopt categories that cover various aspects of the multi-hop inference process, ranging from different kinds of knowledge to different degrees of explanatory relevance and lexical overlap, to analyse the performance of each model beyond the overall explanation regeneration score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance by Table Knowledge Types</head><p>Similarly to the previous editions of the shared task <ref type="bibr" target="#b12">(Jansen and</ref><ref type="bibr">Ustalov, 2019, 2020)</ref>, we present the results achieved by the systems considering    <ref type="table">6</ref>: Percentage of lexical overlap and respective NDCG scores for each model. In this experiment, we measure the performance of the systems considering only those facts that have a percentage of overlap ≤ a given threshold T .</p><p>The percentage of overlap is computed by dividing the number of shared terms between question-answer pair and a fact by the total number of unique terms. To evaluate the systems in the most challenging setting, we gradually decrease the value of T down to 0.</p><p>different knowledge types in the knowledge base. The explanatory facts in the WorldTree corpus are stored in semi-structured tables that are broadly divided into three main categories:</p><p>• Retrieval: Facts that generally encode knowledge about taxonomic relations or properties.</p><p>• Inference-Supporting: Facts that include knowledge about actions, affordances, uses of materials or devices, sources of things, requirements, or affect relationships.</p><p>• Complex Inference: Facts that encode knowledge of causality, processes, changes, coupled relationships, and if/then relationships.</p><p>We break down the NDCG performance of each model across these knowledge types and report the results in Table <ref type="table" target="#tab_3">3</ref>.</p><p>In line with previous editions of the shared task, we observe that the performance of the models tends to be higher for the retrieval type, while de-creasing for inference-supporting and complex inference facts. This can be explained by the fact that retrieval knowledge is generally specific to the concepts in the questions and therefore easier to rank, while inference-supporting and complex facts typically include more abstract scientific knowledge requiring multi-hop inference. These results are consistent across all the models except from Google-BERT, which exhibits the best performance on the inference-supporting type and more stable results in general. We attribute this outcome to the autoregressive component adopted by the system, which may facilitate the ranking of more challenging explanatory facts. With respect to the general performance of the models, we observe that DeepBlueAI consistently outperforms other approaches across all knowledge categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance by Relevance Ratings</head><p>As described in Section 4, the dataset for the 2021 shared task includes relevance ratings that range from 0 (not relevant) to 6 (highly relevant). To better understand the quality of the facts retrieved by each model, we calculated the NDCG score of each model broken down by relevance ratings. The results of this analysis are reported in Table <ref type="table" target="#tab_4">4</ref>.</p><p>Similar to the results obtained on different knowledge types, we observe that DeepBlueAI consistently outperforms other approaches across all relevance rating bins. In contrast to other models, DeepBlueAI exhibits increasing performance for higher relevance ratings, confirming that the model is particularly suited for retrieving highly relevant facts (i.e., facts with relevance ratings &gt; 4). We conjecture that these results are due to the particular training configuration adopted by the system, which employs a triplet loss to encourage the retrieval of highly relevant facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Precision@k</head><p>We compute the Precision@k to complement the results obtained via the NDCG metric. In contrast to NDCG which weights facts based on relevancy ratings, here for this evaluation we consider all the facts with a rating greater than 0 as gold. The results of the analysis are reported in Table <ref type="table" target="#tab_5">5</ref>. The results show that DeepBlueAI substantially outperforms other models for values of k ≤ 10. As k becomes large, other models overtake it's performance, though the difference between models becomes small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Performance by Lexical Overlap</head><p>One of the crucial issues regarding the evaluation of multi-hop inference models is the possibility to achieve strong overall performance without using real compositional methods <ref type="bibr" target="#b25">(Min et al., 2019;</ref><ref type="bibr" target="#b3">Chen and Durrett, 2019;</ref><ref type="bibr" target="#b31">Trivedi et al., 2020)</ref>. Therefore, in order to evaluate multi-hop inference more explicitly, we break down the performance of each model with respect to the difficulty of accessing specific facts in an explanation via direct lexical overlap. This comes from the assumption that facts sharing many terms with question or answer are relatively easier to find and rank highly.</p><p>Table <ref type="table">6</ref> reports the performance of the systems by considering a difference percentage L of lexical overlaps between question-answer pairs and facts computed as follows:</p><formula xml:id="formula_0">L = |t(Q||A) ∩ t(F i )| |t(Q||A) ∪ t(F i )| × 100</formula><p>In the equation above, t(Q||A) represents the set of unique terms (without stop-words) in question and correct answer, while t(F i ) is the set of unique terms in a given fact F i . The percentage of overlaps is then derived by dividing the number of shared terms between a question-answer pair and a fact by the number of their unique terms. Therefore, a value of L equal to 50%, for example, means that 50% of the unique terms in a question-answer pair and a fact are shared.</p><p>Given a question and a value L computed for each fact annotated with relevance ratings, we measure the performance of the systems considering only those facts that have a percentage of overlaps ≤ a given threshold T . To evaluate the systems in the most challenging setting, we gradually decrease the value of T down to 0.</p><p>Overall, we observe that DeepBlueAI consistently outperforms all the other models across all the considered categories. Interestingly, we observe that Google-BERT performs better than Red-DragonAI when considering facts that have zero lexical overlaps with question or answer, confirming the importance of performing specific analysis for the evaluation of multi-hop inference.</p><p>Despite the substantial improvement on the baseline obtained by the competing models, we still observe a significant drop in performance with low degrees of lexical overlaps. This drop indicates that the proposed models still struggle to retrieve abstract explanatory facts requiring multi-hop inference, leaving wide space for future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The 2021 edition of the Shared Task on Multi-Hop Inference for Explanation Regeneration was a success, with 4 participating teams each substantially improving performance over the baseline model. The best performing team, DeepBlueAI, produced a system that improves absolute performance by 32%, up to 0.820 NDCG, bringing overall state-ofthe-art performance at this relevancy ranking aspect of multi-hop inference to a moderate level. We hope that future systems for many-hop multi-hop inference that aim to build large detailed explanations for question answering will be able to leverage these results to build strong relevancy retrieval subcomponents to augment their compositional inference algorithms.</p><p>Award #1815948, "Explainable Natural Language Inference"). This edition of the shared task would not have been possible without the hard work of a number of relevance annotators, and their generous offer to anonymously use their data while their work is under review. A special thanks to André Freitas for the helpful discussions. Additionally, we would like to thank the Computational Shared Facility of the University of Manchester for providing the infrastructure to run our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Precison@k DeepBlueAI RedDragonAI Google-BERT Baseline (tf.idf)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Overall task performance systems participat-</cell></row><row><cell>ing in the 2021 Shared Task on Multi-Hop Inference</cell></row><row><cell>for Explanation Regeneration. Performance is mea-</cell></row><row><cell>sured using Normalized Discounted Cumulative Gain</cell></row><row><cell>(NDCG).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table type DeepBlueAI</head><label>type</label><figDesc>RedDragonAI Google-BERT Baseline (tf.idf)</figDesc><table><row><cell>Retrieval</cell><cell>0.775</cell><cell>0.736</cell><cell>0.671</cell><cell>0.477</cell></row><row><cell>Inference-supporting</cell><cell>0.716</cell><cell>0.712</cell><cell>0.683</cell><cell>0.433</cell></row><row><cell>Complex inference</cell><cell>0.738</cell><cell>0.688</cell><cell>0.664</cell><cell>0.406</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance (NDCG) of the systems when considering different types of knowledge.</figDesc><table><row><cell cols="5">Relevance (&gt;) DeepBlueAI RedDragonAI Google-BERT Baseline (tf.idf)</cell></row><row><cell>0</cell><cell>0.820</cell><cell>0.771</cell><cell>0.700</cell><cell>0.501</cell></row><row><cell>2</cell><cell>0.818</cell><cell>0.764</cell><cell>0.686</cell><cell>0.489</cell></row><row><cell>4</cell><cell>0.831</cell><cell>0.692</cell><cell>0.601</cell><cell>0.416</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance (NDCG) when restricted to examining facts with a given minimum relevance rating.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Precision@k for each model across varying values of k.</figDesc><table><row><cell cols="5">Overlaps (≤ T ) DeepBlueAI RedDragonAI Google-BERT Baseline (tf.idf)</cell></row><row><cell>100.0%</cell><cell>0.820</cell><cell>0.771</cell><cell>0.700</cell><cell>0.501</cell></row><row><cell>90.0%</cell><cell>0.820</cell><cell>0.771</cell><cell>0.700</cell><cell>0.501</cell></row><row><cell>80.0%</cell><cell>0.820</cell><cell>0.771</cell><cell>0.699</cell><cell>0.501</cell></row><row><cell>70.0%</cell><cell>0.818</cell><cell>0.769</cell><cell>0.698</cell><cell>0.497</cell></row><row><cell>60.0%</cell><cell>0.816</cell><cell>0.766</cell><cell>0.695</cell><cell>0.493</cell></row><row><cell>50.0%</cell><cell>0.813</cell><cell>0.763</cell><cell>0.691</cell><cell>0.487</cell></row><row><cell>40.0%</cell><cell>0.804</cell><cell>0.754</cell><cell>0.679</cell><cell>0.471</cell></row><row><cell>30.0%</cell><cell>0.791</cell><cell>0.738</cell><cell>0.661</cell><cell>0.443</cell></row><row><cell>20.0%</cell><cell>0.751</cell><cell>0.704</cell><cell>0.628</cell><cell>0.382</cell></row><row><cell>10.0%</cell><cell>0.653</cell><cell>0.603</cell><cell>0.559</cell><cell>0.261</cell></row><row><cell>0.0%</cell><cell>0.467</cell><cell>0.358</cell><cell>0.425</cell><cell>0.134</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We thank the authors of this dataset for allowing it to be used anonymously for this shared task, while it is under consideration for publication.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Peter Jansen's work on the shared task was supported by National Science Foundation (NSF</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SciB-ERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrieval Evaluation with Incomplete Information</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno type="DOI">10.1145/1008992.1009000</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;04</title>
				<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;04<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>. Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoregressive Reasoning over Chains of Facts with Transformers</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Cartuyvels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Spinks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.610</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6916" to="6930" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding Dataset Design Choices for Multi-hop Reasoning</title>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4026" to="4032" />
		</imprint>
	</monogr>
	<note>NAACL-HLT 2019</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Witteveen</surname></persName>
		</author>
		<author>
			<persName><surname>Andrews</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5311</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</title>
				<meeting>the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Red Dragon AI at TextGraphs 2020 Shared Task : LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Witteveen</surname></persName>
		</author>
		<author>
			<persName><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs). Association for Computational Linguistics</title>
				<meeting>the Graph-based Methods for Natural Language Processing (TextGraphs). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and Effective Multi-Paragraph Reading Comprehension</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="845" to="855" />
		</imprint>
	</monogr>
	<note>ACL 2018. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chains-of-Reasoning at TextGraphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5313</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</title>
				<meeting>the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>NAACL-HLT 2019</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Higher-order Lexical Semantic Models for Non-factoid Answer Reranking</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustave</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00133</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explanation Regeneration via Multi-Hop ILP Inference over Knowledge Base</title>
		<author>
			<persName><forename type="first">Aayushee</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopalakrishnan</forename><surname>Srinivasaraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</title>
				<meeting>the Graph-based Methods for Natural Language Processing (TextGraphs)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Study of Automatically Acquiring Explanatory Inference Patterns from Corpora of Explanations: Lessons from Elementary Science Exams</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Workshop on Automated Knowledge Base Construction (AKBC)</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framing QA as Building and Ranking Intersentence Answer Justifications</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00287</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="449" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</title>
		<title level="s">Hong Kong. Association for Computational Linguistics</title>
		<meeting>the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TextGraphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</title>
				<meeting>the Graph-based Methods for Natural Language Processing (TextGraphs)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Marmorstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2732" to="2740" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cumulated Gain-Based Evaluation of IR Techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TextGraphs-15 Shared Task System Description : Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Sureshkumar Vivek Kalyan</surname></persName>
		</author>
		<author>
			<persName><surname>Witteveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-15: Graph-based Methods for Natural Language Processing</title>
				<meeting>TextGraphs-15: Graph-based Methods for Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Erfan Sadeqi Azer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">QASC: A Dataset for Question Answering via Sentence Composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6319</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8082" to="8090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PGL at TextGraphs 2020 Shared Task: Explanation Regeneration using Language and Graph Learning Methods</title>
		<author>
			<persName><forename type="first">Weibin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyue</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</title>
				<meeting>the Graph-based Methods for Natural Language Processing (TextGraphs)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="98" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositional Questions Do Not Necessitate Multi-hop Reasoning</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4249" to="4257" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepBlueAI at TextGraphs 2021 Shared Task: Treating Multi-Hop Inference Explanation Regeneration as A Ranking Problem</title>
		<author>
			<persName><forename type="first">Chunguang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-15: Graph-based Methods for Natural Language Processing. Association for Computational Linguistics</title>
				<meeting>TextGraphs-15: Graph-based Methods for Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ChiSquareX at TextGraphs 2020 Shared Task: Leveraging Pretrained Language Models for Explanation Regeneration</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Girish Pawate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Chandak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs)</title>
				<meeting>the Graph-based Methods for Natural Language Processing (TextGraphs)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6428</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Survey on Explainability in Machine Reading Comprehension</title>
		<author>
			<persName><forename type="first">Mokanarangan</forename><surname>Thayaparan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.712</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8846" to="8863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unification-based Reconstruction of Multi-hop Explanations for Science Questions</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mokanarangan</forename><surname>Thayaparan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="200" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Philosophy of Information Retrieval Evaluation</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45691-0_34</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluation of Cross-Language Information Retrieval Systems</title>
				<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of NDCG Type Ranking Measures</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Learning Theory</title>
				<meeting>the 26th Annual Conference on Learning Theory<address><addrLine>Princeton, NJ, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="54" />
		</imprint>
	</monogr>
	<note>30 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Teach Me to Explain: A Review of Datasets for Explainable NLP</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Three-step Method for Multi-Hop Inference Explanation Regeneration</title>
		<author>
			<persName><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wandi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-15: Graph-based Methods for Natural Language Processing</title>
				<meeting>TextGraphs-15: Graph-based Methods for Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">WorldTree V2: A Corpus of Science-Domain Structured Explanations and Inference Patterns supporting Multi-Hop Inference</title>
		<author>
			<persName><forename type="first">Zhengnan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaycie</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Marmorstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Language Resources and Evaluation</title>
				<meeting>the 12th Conference on Language Resources and Evaluation<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5456" to="5473" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.414</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4514" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
