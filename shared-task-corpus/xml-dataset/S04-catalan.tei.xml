<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Senseval-3: The Catalan Lexical Sample Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Taulé¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>García¡</surname></persName>
						</author>
						<author>
							<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Real</surname></persName>
							<email>fjreal@lsi.upc.es</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Ferrés</surname></persName>
							<email>dferres¤@lsi.upc.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Software Department</orgName>
								<orgName type="institution" key="instit1">TALP Research Center</orgName>
								<orgName type="institution" key="instit2">Universitat Politècnica de Catalunya £ lluism</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centre de Llenguatge i Computació Universitat de Barcelona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Senseval-3: The Catalan Lexical Sample Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe the Catalan Lexical Sample task. This task was initially devised for evaluating the role of unlabeled examples in supervised and semi-supervised learning systems for WSD and it is the counterpart of the Spanish Lexical Sample task. It was coordinated also with other lexical sample tasks (Basque, English, Italian, Rumanian, and Spanish) in order to share part of the target words.</p><p>Firstly, we describe the methodology followed for developing the specific linguistic resources necessary for the task: the MiniDir-Cat lexicon and the MiniCors-Cat corpus. Secondly, we briefly describe the seven participant systems, the results obtained, and a comparative evaluation between them. All participant teams applied only pure supervised learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Catalan Lexicon: MiniDir-Cat</head><p>Catalan language participates for the first time in the Senseval evaluation exercise. Due to the time constraints we had to reduce the initial expectations on providing annotated corpora for up to 45 words to the final 27 word set treated. We preferred to reduce the number of words, while maintaining the quality in the dictionary development, corpus annotation process, and number of examples per word. These words belong to three syntactic categories: 10 nouns, 5 adjectives, and 12 verbs. The selection was made by choosing a subset of the Spanish lexical sample task and trying to share around 10 of the target words with Basque, English, Italian, and Rumanian lexical sample tasks. See table 1 for a complete list of the words.</p><p>We used the MiniDir-Cat dictionary as the lexical resource for corpus tagging, which is a dictionary being developed by the CLiC research group 1 . MiniDir-Cat was conceived specifically as a resource oriented to WSD tasks: we have emphasized low granularity in order to avoid the overlapping of senses usually present in many lexical sources.  Regarding the polysemy of the selected words, the average number of senses per word is 5.37, corresponding to 4.30 senses for the nouns subset, 6.83 for verbs and 4 for adjectives (see table 1, right numbers in column '#senses').</p><p>The content of MiniDir-2.1 has been checked and refined in order to guarantee not only its consistency and coverage but also the quality of the gold standard. Each sense in Minidir-2.1 is linked to the corresponding synset numbers in the semantic net EuroWordNet <ref type="bibr">(Vossen, 1999)</ref>  (only in the case of adjectives), COLLOCATIONS, and SYNSETS. See figure <ref type="figure" target="#fig_0">1</ref> for an example of one sense of the lexical entry banda (noun 'gang').  For every word, a total of 300 examples have been manually tagged by two independent expert human annotators, though some of them had to be discarded due to errors in the automatic POS tagging and multiword filtering. In the cases of disagreement a third lexicographer defined the definitive sense tags. All the annotation process has been assisted by a graphical Perl-Tk interface specifically designed for this task (in the framework of the Meaning European research project), and a tagging handbook for the annotators <ref type="bibr" target="#b0">(Artigas et al., 2003)</ref>. The inter-annotator agreement achieved was very high: 96.5% for nouns, 88.7% for adjectives, 92.1% for verbs, 93.16% overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Catalan Corpus: MiniCors-Cat</head><p>The initial goal was to obtain, for each word, at least 75 examples plus 15 examples per sense. However, after the labeling of the 300 examples, senses with less than 15 occurrences were simply discarded from the Catalan datasets. See table 1, left numbers in column '#senses', for the final ambiguity rates. We know that this is a quite controversial decision that leads to a simplified setting. But we preferred to maintain the proportions of the senses naturally appearing in the ACN corpus rather than trying to artificially find examples of low frequency senses by mixing examples from many sources or by getting them with specific predefined patterns. Thus, systems trained on the MiniCors-Cat corpus are only intended to discriminate between the most important word senses appearing in a general news corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Resources Provided to Participants</head><p>Participants were provided with the complete Minidir-Cat dictionary, a training set with 2/3 of the labeled examples, a test set with 1/3 of the examples and a complementary large set of all the available unlabeled examples in the ACN corpus (with a maximum of 2,472 extra examples for the adjective popular). Each example is provided with a non null list of category-labels marked according to the newspaper section labels (politics, sports, international, etc.) 3 . Aiming at helping teams with few resources on the Catalan language, all corpora were tokenized, lemmatized and POS tagged, using the Catalan linguistic processors developed at TALP-CLiC 4 , and provided to participants.</p><p>Table <ref type="table" target="#tab_3">1</ref> contains information about the sizes of the datasets and the proportion of the most-frequent sense for each word (MFC). This baseline classifier obtains a high accuracy of 66.36% due to the small number of senses considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Participant Systems</head><p>Five teams took part on the Catalan Lexical Sample task, presenting a total of seven systems. We will refer to them as: IRST, SWAT-AB, SWAT-CP, SWAT-CA, UNED, UMD, and Duluth-CLSS. All of them are purely supervised machine learning approaches, so, unfortunately, none of them incorporates the knowledge from the unlabeled examples. Most of these systems participated also in the Spanish lexical sample task, with almost identical configurations.</p><p>Regarding the supervised learning approaches applied, we find AdaBoost, Naive Bayes, vectorbased cosine similarity, and Decision Lists (SWAT systems), Decision Trees (Duluth-CLSS), Support Vector Machines (IRST), and a similarity method based on co-occurrences (UNED). Some systems used a combination of these basic learning algorithms to produce the final WSD system. For instance, Duluth-CLSS applies a bagging-based ensemble of Decision Trees. SWAT-CP performs a majority voting of Decision Lists, the cosine-based vector model and the Bayesian classifier. SWAT-CA combines, again by majority voting, the previous three classifiers with the AdaBoost based SWAT-AB system. The Duluth-CLSS system is a replica of the one presented at the Senseval-2 English lexical sample task.</p><p>All teams used the POS and lemmatization provided by the organization, except Duluth-CLSS, which only used raw lexical information. A few systems used also the category labels provided with the examples. Apparently, none of them used the extra information in MiniDir (examples, collocations, synonyms, WordNet links, etc.), nor syntactic information. Thus, we think that there is room for substantial improvement in the feature set design. It is worth mentioning that the IRST system makes use of a kernel within the SVM framework, including semantic information. See IRST system description paper for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and System Comparison</head><p>Table <ref type="table" target="#tab_5">2</ref> presents the global results of all participant systems, including the MFC baseline (most frequent sense classifier), sorted by the combined F measure. The COMB row stands for a voted combination of the best systems (see last part of the section for a description). As in the Spanish lexical sample task the IRST system is the best performing one. In this case it achieves a substantial improvement with respect to the second system (SWAT-AB) <ref type="bibr">5</ref> .</p><p>All systems obtained better results than the baseline MFC classifier, with a best overall improvement of 18.87 points (56.09% relative error reduction) 6 . For the multiple systems presented by SWAT, the combination of learning algorithms in the SWAT-CP and SWAT-CA did not help improving the accuracy of the basic AdaBoost-based system SWAT-AB. It is also observed that the POS and Lemma information used by most systems is relevant, since the system relying only on raw lexical information <ref type="bibr">5</ref> The difference is statistically significant using a ¡ -test for the difference of two proportions with a confidence level of 0.90. If we raise the confidence level to 0.95 we lose significance by a short margin:  Detailed results by groups of words are showed in table 3. Word groups include part-of-speech, intervals of the proportion of the most frequent sense (%MFS), and intervals of the ratio: number of examples per sense (ExS). Each cell contains precision and recall. Bold face results correspond to the best system in terms of the F score. Last column, -error, contains the best F improvement over the baseline: absolute difference and error reduction(%).</p><p>As in many other previous WSD works, verbs are significantly more difficult (16.67 improvement and 49.3% error reduction) than nouns (23.46, 65.6%). The improvements obtained by all methods on words with high MFC (more than 90%) is generally low. This is not really surprising, since statisticallybased supervised ML algorithms have difficulties at acquiring information about non-frequent senses. Notice, however, the remarkable 44.9% error reduction obtained by SWAT-AB, the best system on this subset. On the contrary, the gain obtained on the lowest MFC words is really good (34.2 points and 55.3% error reduction). This is a good property of the Catalan dataset and the participant systems, which is not always observed in other empirical studies using other WSD corpora. It is worth noting that even better results were observed in the Spanish lexical sample task.</p><p>Systems are quite different along word groups: IRST is globally the best but not on the words with highest (between 80% and 100%) an lowest (less than 50%) MFC, in which SWAT-AB is better. UNED and UMD are also very competitive on nouns but overall results are penalized by the lower performance on adjectives (specially UNED) and verbs (specially UMD). Interestingly, IRST is the best system addressing the words with few examples per sense, suggesting that SVM is a good algorithm for training on small datasets, but loses this advantage for the words with more examples.</p><p>All, these facts, open the avenue for further im- Table <ref type="table">3</ref>: Results of all participant systems on some selected subsets of words provements on the Catalan dataset by combining the outputs of the best performing systems, or by performing a selection of the best at word level. As a first approach, we conducted some simple experiments on system combination by considering a voting scheme, in which each system votes and the majority sense is selected (ties are decided favoring the best method prediction). From all possible sets, the best combination of systems turned out to be: IRST, SWAT-AB, and UNED. The resulting F measure is 86.86, 1.63 points higher than the best single system (see table <ref type="table" target="#tab_5">2</ref>). This improvement comes mainly from the better F performance on noun and verb categories: from 87.63 to 90.11 and from 82.63 to 85.47, respectively.</p><p>Finally, see the agreement rates and the Kappa statistic between each pair of systems in table 4. Due to space restrictions we have indexed the systems by numbers: 1=MFC, 2=UMD, 3=IRST, 4=UNED, 5=D-CLSS, 6=SWAT-AB, 7=SWAT-CP, and 8=SWAT-CA. The upper diagonal contains the agreement ratios varying from 70.13% to 96.01%, and the lower diagonal contains the corresponding Kappa values, ranging from 0.67 and 0.95. It is worth noting that the system relying on the simplest feature set (Duluth-CLSS) obtains the most similar output to the most frequent sense classifier, and that the combination-based systems SWAT-CP and SWAT-CA generate almost the same output.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a MiniDir-Cat entry</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Information on the Catalan datasets per sentence). The context considered for each example includes the paragraph in which the target word occurs, plus the previous and the following paragraphs. All the examples have been extracted from the corpus of the ACN Catalan news agency, which includes about 110,588 news (January 2000-December 2003). This corpus has been tagged with POS. Following MiniDir-2.1, those examples containing the current word in a multiword expression have been discarded.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Duluth-CLSS) performed significantly worse than the rest (confidence level 0.95).</figDesc><table><row><cell>System</cell><cell>prec.</cell><cell>recall</cell><cell>cover. F</cell></row><row><cell>IRST</cell><cell cols="2">85.82% 84.64%</cell><cell>98.6% 85.23</cell></row><row><cell>SWAT-AB</cell><cell cols="2">83.39% 82.47%</cell><cell>98.9% 82.93</cell></row><row><cell>UNED</cell><cell cols="3">81.85% 81.85% 100.0% 81.85</cell></row><row><cell>UMD</cell><cell cols="2">81.46% 80.34%</cell><cell>98.6% 80.89</cell></row><row><cell>SWAT-CP</cell><cell cols="3">79.67% 79.67% 100.0% 79.67</cell></row><row><cell>SWAT-CA</cell><cell cols="3">79.58% 79.58% 100.0% 79.58</cell></row><row><cell cols="4">Duluth-CLSS 75.37% 76.48% 100.0% 75.92</cell></row><row><cell>MFC</cell><cell cols="3">66.36% 66.36% 100.0% 66.36</cell></row><row><cell>COMB</cell><cell cols="3">86.86% 86.86% 100.0% 86.86</cell></row><row><cell>¡ £ ¢ ¥ ¤  § ¦¨ 6 These improvement figures are better than those observed © ¤ . ¦¨</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in the Senseval-2 Spanish lexical sample task: 17 points and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>32.69% of error reduction.</cell><cell></cell><cell></cell><cell></cell></row></table><note>(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Overall results of all systems</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Agreement and Kappa values between each pair of systems</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have used a 3.5 million subset of the newspaper El Periódico in the Catalan version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All the datasets of the Catalan Lexical Sample task and an extended version of this paper are available at: http://www.lsi.upc.es/ nlp/senseval-3/Catalan.html.4 http://www.lsi.upc.es/ nlp/freeling.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work has been supported by the research projects: XTRACT-2, BFF2002-04226-C03-03; FIT-150-500-2002-244; HERMES, TIC2000-0335-C03-02; and MEANING, IST-2001-34460. Francis   Real holds a predoctoral grant by the Catalan Government (2002FI-00648).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Manual de anotación semántica</title>
		<author>
			<persName><forename type="first">N</forename><surname>Artigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<idno>XTRACT-03/03</idno>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>CLiC, UB</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">EuroWordNet: A Multilingual Database with Lexical Semantic Networks</title>
		<editor>P. Vossen</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
