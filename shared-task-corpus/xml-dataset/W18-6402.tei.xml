<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Findings of the Third Shared Task on Multimodal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
							<email>loic.barrault@univ-lemans.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">University of Le</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">University of Le</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Centre for Language Evolution</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Findings of the Third Shared Task on Multimodal Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/W18-64029</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the results from the third shared task on multimodal machine translation. In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech. The image can be used in addition to (or instead of) the source sentence. This year the task was extended with a third target language (Czech) and a new test set. In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech. Seven teams submitted 45 different systems to the two variants of the task. Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Shared Task on Multimodal Machine Translation tackles the problem of generating a description of an image in a target language using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description.</p><p>Initial results in this area showed the potential for visual context to improve translation quality <ref type="bibr" target="#b10">(Elliott et al., 2015;</ref><ref type="bibr" target="#b20">Hitschler et al., 2016)</ref>. This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 .</p><p>This year we challenged participants to target the task of multimodal translation, with two variants:</p><p>• Task 1: Multimodal translation takes an image with a source language description that is then translated into a target language. The training data consists of source-target parallel sentences and their corresponding images.</p><p>• Task 1b: Multisource multimodal translation takes an image with a description in three source languages that is then translated into a target language. The training data consists of source-target parallel data and their corresponding images, but where the source sentences are presented in three different languages, all parallel.</p><p>Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset ) is now 5-way aligned, with images described in English, which are translated into German, French and Czech. 1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be used (simultaneously) and Czech is the only target language.</p><p>We introduce two new evaluation sets that extend the existing Multi30K dataset: a set of 1071 English sentences and their corresponding images and translations for Task 1, and 1,000 translations for the 2017 test set into Czech for Task 1b.</p><p>Another new feature of this year's shared task is the introduction of a new evaluation metric: Lexical Translation Accuracy (LTA), which measures the accuracy of a system at translating correctly a subset of ambiguous source language words.</p><p>Participants could submit both constrained (shared task data only) and unconstrained (any data) systems for both tasks, with a limit of two systems per task variant and language pair per team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>The Multi30K dataset  is the primary resource for the shared task. It contains 31K images originally described in English <ref type="bibr" target="#b35">(Young et al., 2014)</ref> with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions.</p><p>Over the two last years, we have extended the Multi30K dataset with 2,071 new images and two additional languages for the translation task: French and Czech. Table <ref type="table" target="#tab_1">1</ref> presents an overview of the new evaluation datasets. Figure <ref type="figure">1</ref> shows an example of an image with an aligned English-German-French-Czech description.</p><p>This year we also released a new version of the evaluation datasets featuring a subset of sentences that contain ambiguous source language words, which may have different senses in the target language. We expect that these ambiguous words could benefit from additional visual context.</p><p>In addition to releasing the parallel text, we also distributed two types of visual features extracted from a pre-trained ResNet-50 object recognition model <ref type="bibr" target="#b18">(He et al., 2016)</ref> for all of the images, namely the 'res4 relu' convolutional features (which preserve the spatial location of a feature in the original image) and averaged pooled features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi30K Czech Translations</head><p>This year the Multi30K dataset was extended with translations of the image descriptions into Czech. The translations were produced by 15 workers (university and high school students and teachers, all with a good command of English) at the cost of EUR 3,500. The translators used the same platform that was used to collect the French translations for the Multi30K dataset. The Czech translators had access to the source segment in English and the image only (no automatic translation into Czech was presented). The translated segments were automatically checked for mismatching punctuation, spelling errors (using aspell), inadequately short and long sentences, and non-standard charac-En: A boy dives into a pool near a water slide. De: Ein Junge taucht in der Nähe einer Wasserrutsche in ein Schwimmbecken. Fr: Un garçon plonge dans une piscine près d'un toboggan. Cs: Chlapec skáče do bazénu poblíž skluzavky.</p><p>Figure <ref type="figure">1</ref>: Example of an image with a source description in English, together with its German, French and Czech translations.</p><p>ters. The segments containing errors were manually checked and fixed if needed. In total, 5,255 translated segments (16%) were corrected. After the manual correction, 1% of the segments were sampled and manually annotated for translation quality. This annotation task was performed by three annotators (and every segment was annotated by two different people to measure annotation agreement). We found that 94% of the segments did not contain any spelling errors, 96% of the segments fully preserved the meaning, and 75% of translations were annotated as fluent Czech. The remaining 25% contained some stylistic problems (usually inappropriate lexical choice and/or word order adopted from the English source segment). However, the annotation agreement for stylistic problems was substantially lower compared to other categories due to the subjectivity of deciding on the best style for a translation.    ), as described above. The new evaluation data for Task 1b consists of Czech translations, which we collected following the procedure described above. Table <ref type="table" target="#tab_2">2</ref> shows the distribution of images across the groups and tasks. We initially downloaded 2,000 images per Flickr group, which were then manually filtered by three of the authors. The filtering was done to remove (near) duplicate images, clearly watermarked images, and images with dubious content. This process resulted in a total of 2,071 images, 1,000 were used for Task 1 and 1,071 for Task 1b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset for LTA</head><p>In this year's task we also evaluate systems using Lexical Translation Accuracy (LTA) . LTA measures how accurately a system translates a subset of ambiguous words found in the Multi30K corpus. To measure this accuracy, we extract a subset of triplets form the Multi30K dataset in the form (i, aw, clt) where i is the index representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings.</p><p>We prepared the evaluation dataset following the procedure described in , with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS <ref type="bibr" target="#b30">(Riedl and Biemann, 2016)</ref> (for German only) and lemmatised 3 . Second, we perform automatic word alignment using fast align <ref type="bibr" target="#b8">(Dyer et al., 2013)</ref> to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulting in a cleaned dictionary containing {aw : lt} pairs, where aw is an ambiguous English word, and lt is the set of lexical translations of aw in the corpus. For English-Czech, we were unable to perform this 'human filtering' step, and so we use the unfiltered, noisy dictionary. Table <ref type="table" target="#tab_4">3</ref> shows summary statistics about number of ambiguous words and the total number of their instances in the training and validation sets.</p><p>Given a dictionary, we identify instances i in the test sets 4 which contain an ambiguous word aw from the dictionary, resulting in triplets of the form <ref type="bibr">(i, aw, lt</ref>  annotators (students of translation studies) to select, from the set of lexical translations lt, only those translations, denoted as clt, which conform to the source context i -both image and its English description. For example, in the test instance shown in Figure <ref type="figure">2</ref>, hat is an ambiguous word aw and {kappe, mütze, hüten, kopf, kopfbedeckung, kopfbedeckungen, hut, helm, hüte, helmen, mützen} is the set of its lexical translations lt. The human annotator looked at both the image and its description and then selected the following subset {kappe, mütze, mützen} as the correct lexical translations clt that conform to the context of the test instance in Figure <ref type="figure">2</ref>. We also asked annotators to expand the clt set with other synonyms outside the lt set that satisfy the context if they can. The number of ambiguous words and instances for each language pair in the resulting dataset for the test instances is given in Table <ref type="table" target="#tab_6">4</ref>. For English-Czech, while the first human filtering step (dictionary filtering) was not performed, the second human filtering step (test set filtering) was done. We note that this cleaning done by the Czech-English annotators was very selective, most likely due to the noisier nature of the initial annotations from the unfiltered dictionary. Given a human filtered dictionary, the LTA evaluation is straight forward: for each MT system submission, we check if any word in clt is found in the translation of the submission's i th instance.The preprocessing steps may result in mismatches due to sub-optimal handling of morphological variants, but we do not expect this to be a rare event because the dictionaries, gold standard text, and system submissions are pre-processed using the same tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants</head><p>This year we attracted submissions from seven groups. Table <ref type="table">5</ref> presents an overview of the groups En: a cute boy with his hat looking out of a window. De: ein süß jung mit mütze blicken aus einem fenster. aw: hat lt: {kappe, mütze, hüten, kopf, kopfbedeckung, kopfbedeckungen, hut, helm, hüte, helmen, mützen} clt: {kappe, mütze, mützen}  and their submission identifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFRL-OHIO-STATE (Task 1)</head><p>The AFRL-OHIO-STATE team builds on their previous year Visual Machine Translation (VMT) submission by combining it with text-only translation models. Two types of models were submitted: AFRL-OHIO-STATE 1 2IMPROVE U is a system combination of the VMT system and an instantiation of a Marian NMT model <ref type="bibr" target="#b21">(Junczys-Dowmunt et al., 2018)</ref>, and AFRL-OHIO-STATE 1 4COMBO U is a systems combination of the VMT system along with instantiations of Marian, OpenNMT, and Moses <ref type="bibr" target="#b22">(Koehn et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUNI (Task 1)</head><p>The CUNI submissions use two architectures based on the self-attentive Transformer model <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. For German and Czech, a language model is used to extract pseudo-in-ID Participating team AFRL-OHIOSTATE Air Force Research Laboratory &amp; Ohio State University <ref type="bibr" target="#b17">(Gwinnup et al., 2018)</ref> CUNI Univerzita Karlova v Praze <ref type="bibr" target="#b19">(Helcl et al., 2018)</ref> LIUMCVC Laboratoire d'Informatique de l'Université du Maine &amp; Universitat Autonoma de Barcelona Computer Vision Center <ref type="bibr" target="#b2">(Caglayan et al., 2018)</ref> MeMAD Aalto University, Helsinki University &amp; EURECOM <ref type="bibr" target="#b16">(Grönroos et al., 2018)</ref> OSU-BAIDU Oregon State University &amp; Baidu Research <ref type="bibr" target="#b37">(Zheng et al., 2018)</ref> SHEF University of Sheffield  UMONS Université de Mons <ref type="bibr" target="#b6">(Delbrouck and Dupont, 2018)</ref> Table <ref type="table">5</ref>: Participants in the WMT18 multimodal machine translation shared task.</p><p>domain data from all available parallel corpora and mix it with the original Multi30k data and the EU Bookshop corpus. At inference time, both submitted models use only the text input. The first model was trained using the parallel data only. The second model is a reimplementation of the Imagination model <ref type="bibr" target="#b12">(Elliott and Kádár, 2017)</ref> adapted to the Transformer architecture. During training, the model uses the encoder states to predict the image representation. This allows using additional English-only captions from the MSCOCO dataset <ref type="bibr" target="#b25">(Lin et al., 2014)</ref>.</p><p>LIUMCVC (Task 1) LIUMCVC proposes a refined version of their multimodal attention model <ref type="bibr" target="#b3">(Caglayan et al., 2016)</ref>, where source-side information from the textual encoder (i.e. last hidden state of the bidirectional gated recurrent units (GRU)) is now used to filter the convolutional feature maps before the actual decoder-side multimodal attention is computed. The authors also experiment with the impact of L 2 normalisation and input image size for convolutional feature extraction process and found that multimodal attention without L 2 normalisation performs significantly worse than baseline NMT.</p><p>MeMAD (Task 1)</p><p>The MeMAD team adapts the Transformer neural machine translation architecture to a multimodal setting. They use global image features extracted from Detectron <ref type="bibr" target="#b14">(Girshick et al., 2018)</ref>, a pre-trained object detection and localisation neural network, and two additional training corpora: MS-COCO <ref type="bibr" target="#b25">(Lin et al., 2014)</ref> (an English multimodal dataset, which they extend with synthetic multilingual data) and OpenSubtitles <ref type="bibr" target="#b26">(Lison and Tiedemann, 2016)</ref> (a multilingual, text-only dataset). Their experiments show that the effect of the visual features in the system is small; the largest differences in quality amongst the systems tested is attributed to the quality of the underlying text-only neural MT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OSU-BAIDU (Tasks 1 and 1b)</head><p>For Task 1, the OREGONSTATE system ensembles models including some neural machine translation models which only consider text information and multimodal machine translation models which also consider image information. Both types of models use global attention mechanism to align source to target words. For the multimodal model, 1024 dimensional vectors are extracted as image information from a ResNet-101 convolutional neural network and these are used to initialize the decoder. The models are trained using scheduled sampling  and reinforcement learning <ref type="bibr" target="#b29">(Rennie et al., 2017)</ref> to further improve performance.</p><p>For Task 1b, for each language in the multisource inputs, single-source models are trained using the same architecture as in Task 1. The resulting models are ensembled with different combinations. The final submissions only ensemble models trained from English-to-Czech pair, which outperforms other combinations on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHEF (Tasks 1 and 1b)</head><p>For Task 1, SHEF adopts a two-step pipeline approach. In the first (base) step -submitted as a baseline system -they use an ensemble of standard attentive text-only neural machine translation models built using the NMTPY toolkit <ref type="bibr" target="#b4">(Caglayan et al., 2017)</ref> to produce 10-best high quality trans-lation candidates. In the second (re-ranking) step, the 10-best candidates are re-ranked using word sense disambiguation (WSD) approaches: (i) most frequency sense (MFS), (ii) lexical translation (LT) and, (iii) multimodal lexical translation (MLT). Models (i) and (ii) are baselines, whilst MLT is a novel multimodal cross-lingual WSD model. The main idea is to have the cross-lingual WSD model select the translation candidate which correctly disambiguates ambiguous words in the source sentence and the intuition is that the image could help in the disambiguation process. The re-ranking cross-lingual WSD models are based on neural sequence learning models for WSD <ref type="bibr" target="#b28">(Raganato et al., 2017;</ref><ref type="bibr" target="#b36">Yuan et al., 2016)</ref> trained on the Multimodal Lexical Translation Dataset . More specifically, they train LSTMs as taggers to disambiguate/translate every word in the source sentence.</p><p>For Task 1b, the SHEF team explores three approaches. The first approach takes the concatenation of the 10-best translation candidates of German-Czech, French-Czech and English-Czech neural MT systems and then re-ranks them using the same multimodal cross-lingual WSD model as in Task 1. The second approach explores consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appears in the different n-bests. The highest ranked hypothesis with the majority votes is selected. The third approach uses data augmentation: extra source (Czech) data is generated by building systems that translate from German into English and French into English. An English-Czech neural machine translation system is then built and the 10-best list is generated. For re-ranking, classifiers are trained to predict binary scores derived from Meteor for each hypothesis in the 10-best list using word embeddings and image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UMONS (Task 1)</head><p>The UMONS submission uses as baseline a conditional GRU decoder. The architecture is enhanced with another GRU that receives as input the global visual features provided by the task (i.e. 2048-dimensional ResNet pool5 features) as well as the hidden state of the second GRU. Each GRU disposes of 256 computational units. All non-linear transformations in the decoder (apart from the textual attention module) use gated hyperbolic tangent activations. Both visual and textual representation are separately projected onto a vocabulary-sized space. At every timestep, the decoder ends up with two modality-dependent probability distributions over the target tokens, eventually merged with an element-wise addition.</p><p>Baseline (Tasks 1 and 1b) The baseline system for both tasks is a text-only neural machine translation system built with the NMTPY <ref type="bibr" target="#b4">(Caglayan et al., 2017)</ref> following a standard attentive approach <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> with a conditional GRU decoder. The baseline was trained using the Adam optimizer, with a learning rate of 5e −5 and a batch size of 64. The input embedding dimensionality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic Metric Results</head><p>The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts. <ref type="bibr">5</ref> The evaluation was performed using <ref type="bibr">MultEval (Clark et al., 2011)</ref> with the primary metric of Meteor 1.5 <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref>. We also report the results using BLEU <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref> and TER <ref type="bibr" target="#b31">(Snover et al., 2006)</ref> metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: English → German</head><p>Table <ref type="table" target="#tab_8">6</ref> shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is substantially better than other systems, although it uses unconstrained data. The MeMAD team did not submit a constrained or monomodal submission, so we cannot conclude whether this improvement comes from the use of multimodal data or from the additional parallel data. However, as mentioned in Section 3, the authors themselves state that the gains mainly come from the additional parallel text data in the monomodal system. The vast majority of systems beat the strong text-only Baseline by a considerable margin. For other teams submitting monomodal and multimodal versions of their systems (e.g. CUNI and LIUMCVC), there does not seem to be a marked difference in automatic metric scores.</p><p>We can also observe that the ambiguous word evaluations (LTA) does not lead to the same system ranking as the automatic metrics. While this could stem mainly from the fact that the LTA evaluation is only performed on a small subset of the test cases, we consider that these two automatic evaluations are complementary. General translation quality is measured with the standard metrics (BLEU, METEOR and TER), while the LTA evaluations captures the ability of the system to model complex words which, in many cases, could require the use of the image input to disambiguate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 1: English → French</head><p>Table <ref type="table" target="#tab_9">7</ref> shows the results for the Test 2018 dataset with French as target language. Once again, the MeMAD 1 FLICKR FR MeMAD-OpenNMTmmod U system performs significantly better than the other systems. <ref type="bibr">6</ref> For teams submitting monomodal and multimodal versions of their systems (e.g. CUNI and LIUMCVC), there does not seem to be a marked difference in automatic metric scores. Another interesting observation is that in this case the clearly superior performance of the MeMAD 1 FLICKR FR MeMAD-OpenNMTmmod U system also shows in the LTA evaluation.</p><p>All submissions significantly outperformed the English→French baseline system. For this language pair, the evaluation metrics are in better agreement about the ranking of the submissions, however, the LTA metric is once again less correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task 1: English → Czech</head><p>The Czech language is a new addition to the 2018 evaluation campaign. Table <ref type="table" target="#tab_10">8</ref> shows the results for the Test 2018 dataset with Czech as target language. A smaller number of teams have submitted systems for this language pair. This is a more complex language pair as demonstrated by the lower automatic scores obtained by the systems. The best results are obtained by the CUNI 1 FLICKR CS NeuralMonkeyImagination U system, under the unconstrained conditions.</p><p>The constrained systems all perform similarly to each other, and all except CUNI 1 FLICKR CS NeuralMonkeyTextual U are significantly better than the baseline system. Interestingly, for the OSU-BD submissions, LTA seems to disagree significantly with the other metrics. More analysis is necessary to understand why this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task 1b: Multisource English, German, French → Czech</head><p>Multisource multimodal translation is a new task this year. This task invites participants to use multiple source language inputs, as well as the image, in order to generate Czech translations. Only a few systems have been submitted compared to the other tasks. The results for the Test 2018 dataset are presented in Table <ref type="table" target="#tab_11">9</ref>. We observe that all teams outperformed the text-only baseline, even though in some cases the difference is not significant. No teams used unconstrained data in their submissions. Again, the LTA results do not follow those of the automatic metrics, particularly for the two top submissions: LTA scores differ by a large margin, while all other metric scores are the same or very similar. This could however result from the very small number of samples available for LTA evaluation for this task: only 52 test instances. Differences in the translation of a very few number of instances can therefore result in considerably differences in LTA scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human Judgment Results</head><p>In addition to the automatic metrics evaluation, we conducted human evaluation to assess the translation quality of the submissions. This evaluation was undertaken for the Task 1 German, French and Czech outputs as well as for the Task 1b Czech outputs for the Test 2018 dataset. This section describes how we collected the human assessments and computed the results. We are grateful to all of the assessors for their contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>The system outputs indicated as the primary submission were manually evaluated by bilingual Direct Assessment (DA) <ref type="bibr" target="#b15">(Graham et al., 2015)</ref> using      the Appraise platform <ref type="bibr" target="#b13">(Federmann, 2012)</ref>. The annotators (mostly researchers) were asked to evaluate the semantic relatedness between the source sentence in English and the target sentence in German, French or Czech. For the Multisource Task (1b), only the English source is presented. For the evaluation task, the image was shown along with the source sentence and the candidate translation.</p><formula xml:id="formula_0">EN → DE BLEU ↑ Meteor ↑ TER ↓ LTA ↑ •MeMAD 1 FLICKR DE MeMAD-OpenNMT</formula><p>Evaluators were ask to rely on the image when necessary to obtain a better understanding of the source sentence (e.g. in cases where the text was ambiguous). Note that the reference sentence is not displayed during the evaluation to avoid influencing the assessment. Instead, as a control experiment to estimate the quality of the reference sentences (and test the quality of the annotations), we included the references as hypotheses for human evaluation. Figure <ref type="figure" target="#fig_0">3</ref> shows an example of the direct assessment interface used in the evaluation. The score of each translation candidate ranges from 0 (the meaning of the source is not preserved in the target language sentence) to 100 (the meaning of the source is "perfectly" preserved). The overall score of a given system (z) corresponds to the mean standardised score of its translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>For Task 1 English-German translation, we collected 3,422 DAs, resulting in a minimum of 300 and a maximum of 324 direct assessments per system submission, respectively. We collected 2,938 DAs for the English-French translations. This results in a minimum of 280 and a maximum of 307 direct assessments per system submission, respectively. We collected 8,096 DAs for the Task 1 English-Czech translation, representing a minimum of 1,330 and a maximum of 1,370 direct assessments per system submission. For Task 1b English,German,French→Czech translation, we collected 6,827 direct assessments. The least evaluated system received 1,345 assessments, while the most evaluated system received 1,386 direct assessments. Tables <ref type="table" target="#tab_1">10, 11</ref>, 12 and 13 show the results of the human evaluation for the English to German, English to French and English to Czech Multimodal Translation task (Test 2018 dataset) as well as the Multisource Translation task. The systems are ordered by standardised mean DA scores and clustered according to the Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are con- System performance on the English→German Test 2018 dataset as measured by human evaluation against Meteor scores. sidered tied. The supplementary Wilcoxon signedrank scores can be found in Tables 14, 15 and 16 in Appendix A.</p><p>The comparison between automatic and human evaluation are presented in Figures <ref type="figure">4, 5</ref>, 6 and 7. We can observe that METEOR scores are well correlated with the human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>As mentioned in Section 5, we included the reference sentences in the DA evaluation as if they were candidate translations generated by a system. The first observation is that for all language pairs and all tasks, the references (see gold * in Tables <ref type="table" target="#tab_1">10,  11</ref>, 12 and 13) are significantly better than all automatic systems with average raw scores above 90%. This does not only validates the references but also the DA evaluation process.</p><p>For the first time in the MMT evaluation campaign series, using additional (unconstrained) data resulted in some significant improvement both in terms of automatic score and human evaluation. The biggest improvements come from the unconstrained MeMAD system (for the English-German and English-French), which achieves large improvements in Meteor score compared to the second best system. This is also the case in terms of human evaluation. For English-German, for example, the average raw DA score (87.2, see second column of      System performance on the English,German,French→Czech Test 2018 dataset as measured by human evaluation against Meteor scores. ture (as opposed to recurrent neural networks) combined with global image feature that are different from the ResNet features made available by the task organisers. However, according to the authors it seems that most of the improvements come from the additional parallel data.</p><p>Many teams proposed a combination of several systems. This is the case for AFRL-OHIO-STATE, LIUMCVC, OSU-BAIDU and SHEF teams. LI-UMCVC also submitted a non-ensembled version of each system. Their conclusion is that ensembling multiple systems benefit monomodal and multimodal systems.</p><p>Lexical Translation Accuracy LTA was a new evaluation for this campaign. Unlike other automatic metrics, LTA only evaluates a specific aspect of translation quality, namely lexical disambiguation. One of the motivations for multimodality in machine translation is that the visual features could help to disambiguate ambiguous words <ref type="bibr" target="#b10">(Elliott et al., 2015;</ref><ref type="bibr" target="#b20">Hitschler et al., 2016)</ref>. Our aims in introducing the LTA metric was to directly evaluate the disambiguation performance of participating systems.</p><p>The LTA columns in Tables <ref type="table" target="#tab_8">6, 7</ref>, 8, and 9 show some interesting trends. First, for teams submitting text-only and multimodal variants of models, the multimodal versions seem to perform better at LTA compared to their text-only counterparts (e.g. CUNI's systems). This trend is not visible using the Meteor, BLEU, or TER metrics. Second, the SHEF systems that were built precisely to perform cross-lingual LTA-style WSD perform well on this metric but they are not always the best-performing system on this metric.</p><p>Multisource multimodal translation Only two teams participated in this task. The automatic results are presented in Table <ref type="table" target="#tab_11">9</ref>, the human evaluation results are presented in Table <ref type="table" target="#tab_1">13</ref> and the comparison between automatic and human evaluation results are shown in Figure <ref type="figure">6</ref>. Although many direct assessments have been collected for this task, it was not possible to separate the systems into different clusters. We can see that there is still a large margin between the performance of the systems and the human gold reference, but this was also the case for the English-Czech language pair in Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented the results of the third shared task on multimodal translation. The shared task attracted submissions from seven groups, who submitted a total of 45 systems across the two proposed tasks. The Multimodal Translation task attracted the majority of the submissions, with fewer groups attempting multisource multimodal translation.</p><p>The main findings of the shared task are:</p><p>(i) Additional data can greatly improve the results as demonstrated by the winning unconstrained systems.</p><p>(ii) Almost all systems achieved better results compared to the baseline text-only translation system. Various text and visual integration schemes have been proposed, leading to only slight changes in the automatic and human evaluation results.</p><p>(iii) Automatic metrics and human evaluation provided similar results. However, it is difficult to evaluate the impact of the multimodality. In the future, submission of monomodal equivalent of the systems will be encouraged in order to better emphasize the effect of using the visual inputs.</p><p>We are considering to change the data in favor of a more ambiguous task where all modalities should be used in order to generate the output. A possibility would be to re-use the list of ambiguous words extracted for LTA computation and select the image/sentence pairs containing one or more of those words.  English → German Wilcoxon signed-rank test at p-level p ≤ 0.05. '-' means that the value is higher than 0.05.  English → French Wilcoxon signed-rank test at p-level p ≤ 0.05. '-' means that the value is higher than 0.05.    English,French,German → Czech Wilcoxon signed-rank test at p-level p ≤ 0.05. '-' means that the value is higher than 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English→Czech</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of the human direct assessment evaluation interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5:System performance on the English→French Test 2018 dataset as measured by human evaluation against Meteor scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-10 1.2e-30 2.1e-28 2.0e-30 8.1e-28 1.8e-29 5.9e-31 2.5e-36 3.7e-37 4.2e-38 MeMAD MeMAD-OpenNMT-mmod U --4.4e-14 1.5e-12 2.7e-14 1.0e-12 7.0e-14 5.5e-15 1.2e-19 3.2e-21 3.5e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-02 1.3e-09 1.8e-10 1.5e-10 5.1e-12 6.2e-13 4.4e-11 6.6e-14 3.3e-20 MeMAD MeMAD-OpenNMT-mmod U --3.0e-05 6.6e-06 3.0e-06 3.3e-07 8.1e-08 9.0e-07 1.4e-08 3.9e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of the Multi30K training, development and 2018 test datasets. The figures correspond to tuples with an image and parallel sentences in four languages: English, German, French and Czech.</figDesc><table><row><cell>Group</cell><cell cols="2">Task 1 Task 1b</cell></row><row><cell>Strangers!</cell><cell>154</cell><cell>150</cell></row><row><cell>Wild Child</cell><cell>83</cell><cell>83</cell></row><row><cell>Dogs in Action</cell><cell>92</cell><cell>78</cell></row><row><cell>Action Photography</cell><cell>259</cell><cell>238</cell></row><row><cell>Flickr Social Club</cell><cell>263</cell><cell>241</cell></row><row><cell>Everything Outdoor</cell><cell>214</cell><cell>206</cell></row><row><cell>Outdoor Activities</cell><cell>6</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Distribution of images in the Test 2018 dataset by Flickr group. sampled additional images from two thematically related groups (Everything Outdoor and Flickr Social Club) because Outdoor Activities only returned 10 new CC-licensed images and Flickr-Social no longer exists. The translations were collected using the same procedure as before for each of the languages: professional translations for German and internally crowdsourced translations for French and Czech (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Statistics of the ambiguous words extracted</cell></row><row><cell>from the training and validation sets after human</cell></row><row><cell>filtering (dictionary filtering). For EN-CS, the num-</cell></row><row><cell>bers are larger because we could not perform the</cell></row><row><cell>dictionary filtering step.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 2: A test instance with ambiguous word aw and lexical translation options lt. Human annotator corrects/selects those options clt which conform to the source sentence En and corresponding image.</figDesc><table><row><cell cols="3">Language Pair Ambiguous Words Test instances</cell></row><row><cell>EN-DE</cell><cell>38</cell><cell>358</cell></row><row><cell>EN-FR</cell><cell>70</cell><cell>438</cell></row><row><cell>EN-CS</cell><cell>29</cell><cell>140</cell></row><row><cell>EN-CS(1B)</cell><cell>28</cell><cell>52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Statistics of dataset used for the LTA evaluation after human filtering.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Official automatic results for the MMT18 Task 1 on the English → German Test 2018 dataset (ordered by Meteor). Grey background indicate use of resources that fall outside the constraints provided for the shared task. (P) indicate a primary system designated for human evaluation.</figDesc><table><row><cell>EN → FR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>EN → CS</cell></row></table><note>Official automatic results for the MMT18 Task 1 on the English → French Test 2018 dataset (ordered by Meteor). Grey background indicate use of resources that fall outside the constraints provided for the shared task. (P) indicate a primary system designated for human evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Official automatic results for the MMT18 Task 1 on the English → Czech Test 2018 dataset (ordered by Meteor). Grey background indicate use of resources that fall outside the constraints provided for the shared task. (P) indicate a primary system designated for human evaluation. Submissions marked with * are not significantly different from the Baseline.</figDesc><table><row><cell>OSU-BD 1b CS RLMIX C</cell><cell>26.4</cell><cell>28.2</cell><cell>52.7</cell><cell>55.77</cell></row><row><cell>OSU-BD 1b CS RLNMT C (P)</cell><cell>26.4</cell><cell>28.0</cell><cell>52.1</cell><cell>61.54</cell></row><row><cell>SHEF 1b CS CON C</cell><cell>24.7</cell><cell>27.6</cell><cell>52.1</cell><cell>61.54</cell></row><row><cell>*SHEF 1b CS MLTC C (P)</cell><cell>24.5</cell><cell>27.5</cell><cell>52.5</cell><cell>61.54</cell></row><row><cell>SHEF1 1b CS ARNN C (P)</cell><cell>25.2</cell><cell>27.5</cell><cell>53.9</cell><cell>51.92</cell></row><row><cell>*SHEF1 1b CS ARF C</cell><cell>24.1</cell><cell>27.1</cell><cell>54.6</cell><cell>51.92</cell></row><row><cell>Baseline</cell><cell>23.6</cell><cell>26.8</cell><cell>54.1</cell><cell>53.85</cell></row></table><note>EN,DE,FR → CS BLEU ↑ Meteor ↑ TER ↓ LTA ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Official automatic results for the MMT18 Task 1b on the English,German,French → Czech Test 2018 dataset (ordered by Meteor). Submissions marked with * are not significantly different from the Baseline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>) is only 4.5% away from</cell></row><row><cell>the result of the reference evaluation (91.7). The</cell></row><row><cell>MeMAD team use a transformer NMT architec-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="3">: Results of the human evaluation of the WMT18 English-German Multimodal Translation task</cell></row><row><cell cols="3">(Test 2018 dataset). Systems are ordered by standardized mean DA scores (z) and clustered according</cell></row><row><cell cols="3">to Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied, although systems within a cluster may be statistically significantly different from each other (see Table 14). Systems</cell></row><row><cell cols="3">using unconstrained data are identified with a gray background.</cell></row><row><cell cols="3">English→French # Ave % Ave z System</cell></row><row><cell>1 90.3</cell><cell cols="2">0.487 gold FR 1</cell></row><row><cell>2 86.8</cell><cell cols="2">0.349 MeMAD MeMAD-OpenNMT-mmod U</cell></row><row><cell>3 78.5</cell><cell cols="2">0.047 CUNI NeuralMonkeyImagination U</cell></row><row><cell>77.3</cell><cell cols="2">-0.005 UMONS DeepGru C</cell></row><row><cell>74.9</cell><cell>-0.05</cell><cell>LIUMCVC NMTEnsemble C</cell></row><row><cell>74.9</cell><cell cols="2">-0.075 SHEF1 1 FR MFS C</cell></row><row><cell>74.5</cell><cell cols="2">-0.088 SHEF 1 FR MLT C</cell></row><row><cell>73.0</cell><cell>-0.11</cell><cell>LIUMCVC MNMTEnsemble C</cell></row><row><cell>74.4</cell><cell>-0.12</cell><cell>OSU-BD RLNMT C</cell></row><row><cell>66.0</cell><cell cols="2">-0.376 baseline FR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: Results of the human evaluation of the WMT18 English-French Multimodal Translation task</cell></row><row><cell>(Test 2018 dataset). Systems are ordered by standardized mean DA score (z) and clustered according to</cell></row><row><cell>Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied, although systems within a cluster may be statistically significantly different from each other (see Table 15). Systems</cell></row><row><cell>using unconstrained data are identified with a gray background.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell cols="2">: Results of the human evaluation of the WMT18 English-Czech Multimodal Translation task</cell></row><row><cell cols="2">(Test 2018 dataset). Systems are ordered by standardized mean DA score (z) and clustered according to</cell></row><row><cell cols="2">Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied, although systems within a cluster may be statistically significantly different from each other (see Table 16). Systems</cell></row><row><cell cols="2">using unconstrained data are identified with a gray background.</cell></row><row><cell cols="2">English,French,German→Czech # Ave % Ave z System</cell></row><row><cell>93.6</cell><cell>0.803 gold CS 1b</cell></row><row><cell>63.3</cell><cell>-0.149 SHEF 1b CS MLTC C</cell></row><row><cell>61.8</cell><cell>-0.178 SHEF1 1b CS ARNN C</cell></row><row><cell>62.1</cell><cell>-0.206 OSU-BD 1b CS RLNMT C</cell></row><row><cell>59.4</cell><cell>-0.284 baseline CS task1b</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell>: Results of the human evaluation of the WMT18 English,French,German-Czech Multisource</cell></row><row><cell>Multimodal Translation task (Test 2018 dataset). Systems are ordered by standardized mean DA score (z)</cell></row><row><cell>and clustered according to Wilcoxon signed-rank test at p-level p ≤ 0.05. Systems within a cluster are considered tied, although systems within a cluster may be statistically significantly different from each</cell></row><row><cell>other (see Table 17).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 :</head><label>16</label><figDesc>English → Czech Wilcoxon signed-rank test at p-level p ≤ 0.05. '-' means that the value is higher than 0.05.</figDesc><table><row><cell></cell><cell>baseline CS .encs.task1b</cell></row><row><cell></cell><cell>OSU-BD 1b CS RLNMT C</cell></row><row><cell>English,French,German→Czech</cell><cell>gold CS 1b SHEF 1b CS MLTC C SHEF1 1b CS ARNN C</cell></row><row><cell></cell><cell>gold CS task1b</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 :</head><label>17</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The current version of the dataset can be found here: https://github.com/multi30k/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Strangers!, Wild Child, Dogs in Action, Action Photography, and Outdoor Activities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For English, German and French, we use the tool from http://staffwww.dcs.shef.ac.uk/people/ A.Aker/activityNLPProjects.html. For Czech, we pre-processed the data using MorphoDiTa<ref type="bibr" target="#b33">(Straková et al., 2014)</ref> from http://ufal.mff.cuni.cz/ morphodita4  The test data and the submissions undergo the same preprocessing steps as the training and the validation sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We note that their original submission had tokenisation issues, which were fixed by the task organisers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the CHIST-ERA M2CR project (French National Research Agency No. ANR-15-CHR2-0006-01 -Loïc Barrault and Fethi Bougares), by the MultiMT project (EU H2020 ERC Starting Grant No. 678017 -Lucia Specia and Chiraag Lala), and by an Amazon Research Award (Desmond Elliott). We thank the Charles University team for collecting and describing the Czech data. The Czech data collection was supported by the Czech Science Foundation, grant number P103/12/G084.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Significance tests</head><p>Tables 14, 15 and 16 show the Wilcoxon signedrank test used to create the clustering of the systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIUM-CVC submissions for WMT18 multimodal translation task</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shared Task Papers, Brussels, Belgium. Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the Third Conference on Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multimodal attention for neural machine translation</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno>abs/1609.03976</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName><surname>Walid</surname></persName>
		</author>
		<author>
			<persName><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName><surname>Fethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><surname>Barrault</surname></persName>
		</author>
		<idno>1706.00457</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Umons submission for wmt18 multimodal translation task</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
				<meeting>the Third Conference on Machine Translation<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2014 Workshop on Statistical Machine Translation</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)</title>
				<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-language image description with neural sequence models</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>abs/1510.04709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Vision and Language</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagination improves Multimodal Translation</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><surname>Kádár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
				<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Appraise: An open-source toolkit for manual evaluation of machine translation output</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can machine translation systems be evaluated by the crowd alone</title>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The MeMAD submission to the WMT18 multimodal translation task</title>
		<author>
			<persName><forename type="first">Stig-Arne</forename><surname>Grönroos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mats</forename><surname>Sjöberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raúl</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
				<meeting>the Third Conference on Machine Translation<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The afrl-ohio state wmt18 multimodal system: Combining visual with traditional</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Gwinnup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Sandvick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duselis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation, Brussels, Belgium. Association for Computational Linguistics</title>
				<meeting>the Third Conference on Machine Translation, Brussels, Belgium. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CUNI system for the WMT18 multimodal translation tasks</title>
		<author>
			<persName><forename type="first">Jindrřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dušan</forename><surname>Variš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
				<meeting>the Third Conference on Machine Translation<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal Pivots for Image Caption Translation</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
				<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Annual meeting of Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sheffield submissions for wmt18 multimodal translation shared task</title>
		<author>
			<persName><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
				<meeting>the Third Conference on Machine Translation<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal Lexical Translation</title>
		<author>
			<persName><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Opensub-titles2016: Extracting large parallel corpora from movie and tv subtitles</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation</title>
				<meeting>the 10th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual Meeting on Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1156" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised compound splitting with distributional semantics rivals supervised methods</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
				<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Machine Translation</title>
				<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised word sense disambiguation with neural models</title>
		<author>
			<persName><forename type="first">Dayu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Altendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07012</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble sequence level training for multimodal mt: Osu-baidu wmt18 multimodal machine translation system report</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation, Brussels, Belgium. Association for Computational Linguistics</title>
				<meeting>the Third Conference on Machine Translation, Brussels, Belgium. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
