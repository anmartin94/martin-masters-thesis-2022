<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2018 Task 1: Affect in Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
							<email>saif.mohammad@nrc-cnrc.gc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felipe</forename><surname>Bravo-Marquez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Waikato</orgName>
								<address>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
							<email>msalameh@qatar.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
							<email>svetlana.kiritchenko@nrc-cnrc.gc.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">National Research Council</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2018 Task 1: Affect in Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction</p><p>Emotions are central to language and thought. They are familiar and commonplace, yet they are complex and nuanced. Humans are known to perceive hundreds of different emotions. According to the basic emotion model (aka the categorical model) <ref type="bibr" target="#b6">(Ekman, 1992;</ref><ref type="bibr" target="#b33">Plutchik, 1980;</ref><ref type="bibr" target="#b31">Parrot, 2001;</ref><ref type="bibr" target="#b9">Frijda, 1988)</ref>, some emotions, such as joy, sadness, and fear, are more basic than others-physiologically, cognitively, and in terms of the mechanisms to express these emotions. Each of these emotions can be felt or expressed in varying intensities. For example, our utterances can convey that we are very angry, slightly sad, absolutely elated, etc. Here, intensity refers to the degree or amount of an emotion such as anger or sadness. <ref type="bibr">1</ref> As per the valence-arousal-dominance (VAD) model <ref type="bibr" target="#b36">(Russell, 1980</ref><ref type="bibr" target="#b37">(Russell, , 2003</ref>, emotions are points in a three-dimensional space of valence (positivenessnegativeness), arousal (active-passive), and dominance (dominant-submissive). We use the term affect to refer to various emotion-related categories such as joy, fear, valence, and arousal.</p><p>Natural language applications in commerce, public health, disaster management, and public policy can benefit from knowing the affectual states of people-both the categories and the intensities of the emotions they feel. We thus present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks where automatic systems have to infer the affectual state of a person from their tweet. <ref type="bibr">2</ref> We will refer to the author of a tweet as the tweeter. Some of the tasks are on the intensities of four basic emotions common to many proposals of basic emotions: anger, fear, joy, and sadness. Some of the tasks are on valence or sentiment intensity. Finally, we include an emotion classification task over eleven emotions commonly expressed in tweets. <ref type="bibr">3</ref> For each task, we provide separate training, development, and test datasets for English, Arabic, and Spanish tweets. The tasks are as follows:</p><p>1. Emotion Intensity Regression (EI-reg): Given a tweet and an emotion E, determine the intensity of E that best represents the mental state of the tweeter-a real-valued score between 0 (least E) and 1 (most E); 2. Emotion Intensity Ordinal Classification (EIoc): Given a tweet and an emotion E, classify the tweet into one of four ordinal classes of intensity of E that best represents the mental state of the tweeter; 3. Valence (Sentiment) Regression (V-reg): Given a tweet, determine the intensity of sentiment or valence (V) that best represents the mental state of the tweeter-a real-valued score between 0 (most negative) and 1 (most positive); 4. Valence Ordinal Classification (V-oc): Given a tweet, classify it into one of seven ordinal classes, corresponding to various levels of positive and negative sentiment intensity, that best represents the mental state of the tweeter;</p><p>5. Emotion Classification (E-c): Given a tweet, classify it as 'neutral or no emotion' or as one, or more, of eleven given emotions that best represent the mental state of the tweeter.</p><p>Here, E refers to emotion, EI refers to emotion intensity, V refers to valence, reg refers to regression, oc refers to ordinal classification, c refers to classification.</p><p>For each language, we create a large single textual dataset, subsets of which are annotated for many emotion (or affect) dimensions (from both the basic emotion model and the VAD model). For each emotion dimension, we annotate the data not just for coarse classes (such as anger or no anger) but also for fine-grained real-valued scores indicating the intensity of emotion. We use Best-Worst Scaling (BWS), a comparative annotation method, to address the limitations of traditional rating scale methods such as inter-and intraannotator inconsistency. We show that the finegrained intensity scores thus obtained are reliable (repeat annotations lead to similar scores). In total, about 700,000 annotations were obtained from about 22,000 English, Arabic, and Spanish tweets.</p><p>Seventy-five teams (about 200 team members) participated in the shared task, making this the largest SemEval shared task to date. In total, 319 submissions were made to the 15 task-language pairs. Each team was allowed only one official submission for each task-language pair. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze system predictions for consistent bias towards a particular race or gender using a corpus specifically compiled for that purpose. We find that a majority of systems consistently assign higher scores to sentences involving one race or gender. We also find that the bias may change depending on the specific affective dimension being predicted. All of the tweet data (labeled and unlabeled), annotation questionnaires, evaluation scripts, and the bias evaluation corpus are made freely available on the task website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Building on Past Work</head><p>There is a large body of prior work on sentiment and emotion classification . There is also growing work on related tasks such as stance detection  and argumentation mining <ref type="bibr" target="#b39">(Wojatzki et al., 2018;</ref><ref type="bibr" target="#b30">Palau and Moens, 2009)</ref>. However, there is little work on detecting the intensity of affect in text. <ref type="bibr" target="#b24">Mohammad and Bravo-Marquez (2017)</ref> created the first datasets of tweets annotated for anger, fear, joy, and sadness intensities. Given a focus emotion, each tweet was annotated for intensity of the emotion felt by the speaker using a technique called Best-Worst Scaling (BWS) <ref type="bibr" target="#b19">(Louviere, 1991;</ref><ref type="bibr">Mohammad, 2016, 2017)</ref>.</p><p>BWS is an annotation scheme that addresses the limitations of traditional rating scale methods, such as inter-and intra-annotator inconsistency, by employing comparative annotations. Note that at its simplest, comparative annotations involve giving people pairs of items and asking which item is greater in terms of the property of interest. However, such a method requires annotations for N 2 items, which can be prohibitively large.</p><p>In BWS, annotators are given n items (an ntuple, where n &gt; 1 and commonly n = 4). They are asked which item is the best (highest in terms of the property of interest) and which is the worst (lowest in terms of the property of interest). When working on 4-tuples, best-worst annotations are particularly efficient because each best and worst annotation will reveal the order of five of the six item pairs. For example, for a 4-tuple with items A, B, C, and D, if A is the best, and D is the worst, then A &gt; B, A &gt; C, A &gt; D, B &gt; D, and C &gt; D. Real-valued scores of association between the items and the property of interest can be determined using simple arithmetic on the number of times an item was chosen best and number of times it was chosen worst (as described in Section 3.4.2) <ref type="bibr" target="#b29">(Orme, 2009;</ref><ref type="bibr" target="#b8">Flynn and Marley, 2014)</ref>.</p><p>It has been empirically shown that annotations for 2N 4-tuples is sufficient for obtaining reliable scores (where N is the number of items) <ref type="bibr" target="#b19">(Louviere, 1991;</ref>.  showed through empirical experiments that BWS produces more reliable and more discriminating scores than those obtained using rating scales. (See <ref type="bibr">Mohammad, 2016, 2017)</ref> for further details on BWS.) <ref type="bibr" target="#b24">Mohammad and Bravo-Marquez (2017)</ref> collected and annotated 7,100 English tweets posted in 2016. We will refer to the tweets alone as Tweets-2016, and the tweets and annotations together as the Emotion Intensity Dataset (or, EmoInt Dataset). This dataset was used in the 2017 WASSA Shared Task on Emotion Intensity. <ref type="bibr">4</ref> We build on that earlier work by first compiling a new set of English, Arabic, and Spanish tweets posted in 2017 and annotating the new tweets for emotion intensity in a similar manner. We will refer to this new set of tweets as Tweets-2017. Similar to the work by <ref type="bibr" target="#b24">Mohammad and Bravo-Marquez (2017)</ref>, we create four subsets annotated for intensity of fear, joy, sadness, and anger, respectively. However, unlike the earlier work, here a common dataset of tweets is annotated for all three negative emotions: fear, anger, and sadness. This allows one to study the relationship between the three basic negative emotions.</p><p>We also annotate tweets sampled from each of the four basic emotion subsets (of both <ref type="bibr">Tweets-2016 and</ref><ref type="bibr">Tweets-2017)</ref> for degree of valence. Annotations for arousal, dominance, and other basic emotions such as surprise and anticipation are left for future work.</p><p>In addition to knowing a fine-grained score indicating degree of intensity, it is also useful to qualitatively ground the information on whether the intensity is high, medium, low, etc. Thus, we manually identify ranges in intensity scores that correspond to these coarse classes. For each of the four emotions E, the 0 to 1 range is partitioned into the classes: no E can be inferred, low E can be inferred, moderate E can be inferred, and high E can be inferred. This data can be used for developing systems that predict the ordinal class of emotion intensity (EI ordinal classification, or EI-oc, systems). We partition the 0 to 1 interval of valence into: very negative, moderately negative, slightly negative, neutral or mixed, slightly positive, moderately positive, and very positive mental state of the tweeter can be inferred. This data can be used to develop systems that predict the ordinal class of valence (valence ordinal classification, or V-oc, systems). <ref type="bibr">5</ref> Annotated  Finally, the full Tweets-2016 and Tweets-2017 datasets are annotated for the presence of eleven emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, and trust. This data can be used for developing multilabel emotion classification, or E-c, systems. Table 1 shows the two stages in which the annotations for English tweets were done. The Arabic and Spanish tweets were all only from 2017. Together, we will refer to the joint set of tweets from Tweets-2016 and Tweets-2017 along with all the emotion-related annotations described above as the SemEval-2018 Affect in Tweets Dataset (or AIT Dataset for short).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Affect in Tweets Dataset</head><p>We now present how we created the Affect in Tweets Dataset. We present only the key details here; a detailed description of the English datasets and the analysis of various affect dimensions is available in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compiling English Tweets</head><p>We first compiled tweets to be included in the four EI-reg datasets corresponding to anger, fear, joy, and sadness. The EI-oc datasets include the same tweets as in EI-reg, that is, the Anger EI-oc dataset has the same tweets as in the Anger EI-reg dataset, the Fear EI-oc dataset has the same tweets as in the Fear EI-reg dataset, and so on. However, the labels for EI-oc tweets are ordinal classes instead of realvalued intensity scores. The V-reg dataset includes a subset of tweets from each of the four EI-reg emotion datasets. The V-oc dataset has the same tweets as in the V-reg dataset. The E-c dataset includes all the tweets from the four EI-reg datasets. The total number of instances in the E-c, EI-reg, EI-oc, V-reg, and V-oc datasets is shown in the last column of Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Basic Emotion Tweets</head><p>To create a dataset of tweets rich in a particular emotion, we used the following methodology.</p><p>For each emotion X, we selected 50 to 100 terms that were associated with that emotion at different intensity levels. For example, for the anger dataset, we used the terms: angry, mad, frustrated, annoyed, peeved, irritated, miffed, fury, antagonism, and so on. We will refer to these terms as the query terms. The query terms we selected included emotion words listed in the Roget's Thesaurus, nearest neighbors of these emotion words in a word-embeddings space, as well as commonly used emoji and emoticons. The full list of the query terms is available on the task website.</p><p>We polled the Twitter API, over the span of two months (June and July, 2017), for tweets that included the query terms. We randomly selected 1,400 tweets from the joy set for annotation of intensity of joy. For the three negative emotions, we first randomly selected 200 tweets each from their corresponding tweet collections. These 600 tweets were annotated for all three negative emotions so that we could study the relationships between fear and anger, between anger and sadness, and between sadness and fear. For each of the negative emotions, we also chose 800 additional tweets, from their corresponding tweet sets, that were annotated only for the corresponding emotion. Thus, the number of tweets annotated for each of the negative emotions was also 1,400 (the 600 included in all three negative emotions + 800 unique to the focus emotion). For each emotion, 100 tweets that had an emotion-word hashtag, emoticon, or emoji query term at the end (trailing query term) were randomly chosen. We removed the trailing query terms from these tweets. As a result, the dataset also included some tweets with no clear emotion-indicative terms.</p><p>Thus, the EI-reg dataset included 1,400 new tweets for each of the four emotions. These were annotated for intensity of emotion. Note that the EmoInt dataset already included 1,500 to 2,300 tweets per emotion annotated for intensity. Those tweets were not re-annotated. The EmoInt EI-reg tweets as well as the new EI-reg tweets were both annotated for ordinal classes of emotion (EI-oc) as described in Section 3.4.3</p><p>The new EI-reg tweets formed the EI-reg development (dev) and test sets in the AIT task; the number of instances in each is shown in the third and fourth columns of Table <ref type="table" target="#tab_5">3</ref>. The EmoInt tweets formed the training set. 6</p><p>6 Manual examination of the new EI-reg tweets later re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Valence Tweets</head><p>The valence dataset included tweets from the new EI-reg set and the EmoInt set. The new EI-reg tweets included were all 600 tweets common to the three negative emotion tweet sets and 600 randomly chosen joy tweets. The EmoInt tweets included were 600 randomly chosen joy tweets and 200 each, randomly chosen tweets, for anger, fear, and sadness. To study valence in sarcastic tweets, we also included 200 tweets that had hashtags #sarcastic, #sarcasm, #irony, or #ironic (tweets that are likely to be sarcastic). Thus the V-reg set included 2,600 tweets in total. The V-oc set is comprised of the same tweets as in the V-reg set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Multi-Label Emotion Tweets</head><p>We selected all of the 2016 and 2017 tweets in the four EI-reg datasets to form the E-c dataset, which is annotated for presence/absence of 11 emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compiling Arabic Tweets</head><p>We compiled the Arabic tweets in a similar manner to the English dataset. We obtained the the Arabic query terms as follows:</p><p>• We translated the English query terms for the four emotions to Arabic using Google Translate.</p><p>• All words associated with the four emotions in the NRC Emotion Lexicon were translated into Arabic. (We discarded incorrect translations.)</p><p>• We trained word embeddings on a tweet corpus collected using dialectal function words as queries. We used nearest neighbors of the emotion query terms in the word-embedding space as additional query terms.</p><p>• We included the same emoji used in English for anger, fear, joy and sadness. However, most of the fear emoji were not included, as they were rarely associated with fear in Arabic tweets.</p><p>In total, we used 550 Arabic query terms and emoji to poll the Twitter API to collect around 17 million tweets between March and July 2017.</p><p>For each of the four emotions, we randomly selected 1,400 tweets to form the EI-reg datasets. The same tweets were used for building the EIoc datasets. The sets of tweets for the negative emotions included 800 tweets unique to the focus emotion and 600 tweets common to the three negative emotions.</p><p>vealed that it included some near-duplicate tweets. We kept only one copy of such pairs. Thus the dev. and test set numbers add up to a little lower than 1,400.</p><p>The V-reg dataset was formed by including about 900 tweets from the three negative emotions (including the 600 tweets common to the three negative emotion datasets), and about 900 tweets for joy. The same tweets were used to form the Voc dataset. The multi-label emotion classification dataset was created by taking all the tweets in the EI-reg datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compiling Spanish Tweets</head><p>The Spanish query terms were obtained as follows:</p><p>• The English query terms were translated into Spanish using Google Translate. The translations were manually examined by a Spanish native speaker, and incorrect translations were discarded.</p><p>• The resulting set was expanded using synonyms taken from a Spanish lexicographic resource, Wordreference 7 .</p><p>• We made sure that both masculine and feminine forms of the nouns and adjectives were included.</p><p>• We included the same emoji used in English for anger, sadness, and joy. The emoji for fear where not included, as tweets containing those emoji were rarely associated with fear.</p><p>We collected about 1.2 million tweets between July and September 2017. We annotated close to 2,000 tweets for each emotion. The sets of tweets for the negative emotions included ∼1,500 tweets unique to the focus emotion and ∼500 tweets common to the two remaining negative emotions. The same tweets were used for building the Spanish EI-oc dataset. The V-reg dataset was formed by including about 1,100 tweets from the three negative emotions (including the 750 tweets common to the three negative emotion datasets), about 1,100 tweets for joy, and 268 tweets with sarcastic hashtags (#sarcasmo, #ironia). The same tweets were used to build the V-oc dataset. The multi-label emotion classification dataset was created by taking all the tweets in the EI-reg and V-reg datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotating Tweets</head><p>We describe below how we annotated the English tweets. The same procedure was used for Arabic and Spanish annotations.</p><p>We annotated all of our data by crowdsourcing. The tweets and annotation questionnaires were uploaded on the crowdsourcing platform, Figure Eight (earlier called CrowdFlower). <ref type="bibr">8</ref> All the annotation tasks described in this paper were approved by the National Research Council Canada's Institutional Review Board.</p><p>About 5% of the tweets in each task were annotated internally beforehand (by the authors of this paper). These tweets are referred to as gold tweets. The gold tweets were interspersed with other tweets. If a crowd-worker got a gold tweet question wrong, they were immediately notified of the error. If the worker's accuracy on the gold tweet questions fell below 70%, they were refused further annotation, and all of their annotations were discarded. This served as a mechanism to avoid malicious annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Multi-Label Emotion Annotation</head><p>We presented one tweet at a time to the annotators and asked which of the following options best described the emotional state of the tweeter:</p><p>-anger (also includes annoyance, rage)</p><p>-anticipation (also includes interest, vigilance)</p><p>-disgust (also includes disinterest, dislike, loathing)</p><p>-fear (also includes apprehension, anxiety, terror)</p><p>-joy (also includes serenity, ecstasy)</p><p>-love (also includes affection)</p><p>-optimism (also includes hopefulness, confidence)</p><p>-pessimism (also includes cynicism, no confidence) -sadness (also includes pensiveness, grief)</p><p>-surprise (also includes distraction, amazement) -trust (also includes acceptance, liking, admiration) -neutral or no emotion Example tweets were provided in advance with examples of suitable responses.</p><p>On the Figure Eight task settings, we specified that we needed annotations from seven people for each tweet. However, because of the way the gold tweets were set up, they were annotated by more than seven people. The median number of annotations was still seven. In total, 303 people annotated between 10 and 4,670 tweets each. A total of 174,356 responses were obtained. Annotation Aggregation: One of the criticisms for several natural language annotation projects has been that they keep only the instances with high agreement, and discard instances that obtain low agreements. The high agreement instances anger antic. disg.  tend to be simple instantiations of the classes of interest, and are easier to model by automatic systems. However, when deployed in the real world, natural language systems have to recognize and process more complex and subtle instantiations of a natural language phenomenon. Thus, discarding all but the high agreement instances does not facilitate the development of systems that are able to handle the difficult instances appropriately. Therefore, we chose a somewhat generous aggregation criterion: if more than 25% of the responses (two out of seven people) indicated that a certain emotion applies, then that label was chosen. We will refer to this aggregation as Ag2. If no emotion got at least 40% of the responses (three out of seven people) and more than 50% of the responses indicated that the tweet was neutral, then the tweet was marked as neutral. In the vast majority of the cases, a tweet was labeled either as neutral or with one or more of the eleven emotion labels. 107 English tweets, 14 Arabic tweets, and 88 Spanish tweets did not receive sufficient votes to be labeled a particular emotion or to be labeled neutral. These very-low-agreement tweets were set aside. We will refer to the remaining dataset as E-c (Ag2), or simply E-c, data. Class Distribution: Table <ref type="table" target="#tab_3">2</ref> shows the percentage of tweets that were labeled with a given emotion using Ag2 aggregation. The numbers in these rows sum up to more than 100% because a tweet may be labeled with more than one emotion. Observe that joy, anger, disgust, sadness, and optimism get a high number of the votes. Trust and surprise are two of the lowest voted emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Annotating Intensity with BWS</head><p>We followed the procedure described by  to obtain bestworst scaling (BWS) annotations.</p><p>Every 4-tuple was annotated by four independent annotators. The questionnaires were developed through internal discussions and pilot annotations. They are available on the SemEval-2018 AIT Task webpage.</p><p>Between 118 and 220 people residing in the United States annotated the 4-tuples for each of the four emotions and valence. In total, around 27K responses for each of the four emotions and around 50K responses for valence were obtained. 9 Annotation Aggregation: The intensity scores were calculated from the BWS responses using a simple counting procedure <ref type="bibr" target="#b29">(Orme, 2009;</ref><ref type="bibr" target="#b8">Flynn and Marley, 2014)</ref>: For each item, the score is the proportion of times the item was chosen as having the most intensity minus the percentage of times the item was chosen as having the least intensity. <ref type="bibr">10</ref> We linearly transformed the scores to lie in the 0 (lowest intensity) to 1 (highest intensity) range. Distribution of Scores: Figure <ref type="figure" target="#fig_0">1</ref> shows the histogram of the V-reg tweets. The tweets are grouped into bins of scores 0-0.05, 0.05-0.1, and so on until 0.95-1. The colors for the bins correspond to their ordinal classes as determined from the manual annotation described in the next subsection. The histograms for the four emotions are shown in Figure <ref type="figure">5</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Identifying Ordinal Classes</head><p>For each of the EI-reg emotions, the authors of this paper independently examined the ordered list of tweets to identify suitable boundaries that partitioned the 0-1 range into four ordinal classes: no emotion, low emotion, moderate emotion, and high emotion. Similarly the V-reg tweets were examined and the 0-1 range was partitioned into seven classes: very negative, moderately negative, slightly negative, neutral or mixed, slightly positive, moderately positive, and very positive mental state can be inferred. 11 Annotation Aggregation: The authors discussed their individual annotations to obtain consensus on the class intervals. The V-oc and EI-oc datasets were thus labeled. Class Distribution: The legend of Figure <ref type="figure" target="#fig_0">1</ref> shows the intervals of V-reg scores that make up the seven V-oc classes. The intervals of EI-reg scores that make up each of the four EI-oc classes are shown in Figure <ref type="figure">5</ref> in the Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Annotating Arabic and Spanish Tweets</head><p>The annotations for Arabic and Spanish tweets followed the same process as the one described for English above. We manually translated the English questionnaire into Arabic and Spanish.</p><p>On Figure Eight, we used similar settings as for English. For Arabic, we set the country of annotators to fourteen Arab countries available in Crowdflower as well as the United States of America. 12 For Spanish, we set the country of annotators to USA, Mexico, and Spain.</p><p>Annotation aggregation was done the same way for Arabic and Spanish, as for English. Table <ref type="table" target="#tab_3">2</ref> shows the distributions for different emotions in the E-c annotations for Arabic and Spanish (in addition to English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training, Development, and Test Sets</head><p>Table <ref type="table" target="#tab_1">14</ref> in Appendix summarizes key details of the current set of annotations done for the SemEval-2018 Affect in Tweets (AIT) Dataset. AIT was partitioned into training, development, and test sets for machine learning experiments as described in Table <ref type="table" target="#tab_5">3</ref>. All of the English tweets that came from Tweets-2016 were part of the training sets. All of the English tweets that came from Tweets-2017 were split into development and test sets. <ref type="bibr">13</ref> The Arabic and Spanish tweets are all from 2017 and were split into train, dev, and test sets.</p><p>12 Algeria, Bahrain, Egypt, Jordan, Kuwait, Morocco, Oman, Palestine, Qatar, Saudi Arabia, Tunisia, UAE, Yemen.</p><p>13 This split of Tweets-2017 was first done such that 20% of the tweets formed the dev. set and 80% formed the test setindependently for EI-reg, EI-oc, V-reg, V-oc, and E-c. Then we moved some tweets from the test sets to the dev. sets such that a tweet in any dev. set does not occur in any test set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Agreement and Reliability of Annotations</head><p>It is challenging to obtain consistent annotations for affect due to a number of reasons, including: the subtle ways in which people can express affect, fuzzy boundaries of affect categories, and differences in human experience that impact how they perceive emotion in text. In the subsections below we analyze the AIT dataset to determine the extent of agreement and the reliability of the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">E-c Annotations</head><p>Table <ref type="table" target="#tab_7">4</ref> shows the inter-rater agreement and Fleiss' κ for the multi-label emotion annotations. The inter-rater agreement (IRA) is calculated as the percentage of times each pair of annotators agree.</p><p>For the sake of comparison, we also show the scores obtained by randomly choosing whether a particular emotion applies or not. Observe that the scores obtained through the actual annotations are markedly higher than the scores obtained by random guessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EI-reg and V-reg Annotations</head><p>For real-valued score annotations, a commonly used measure of quality is reproducibility of the end result-if repeated independent manual annotations from multiple respondents result in similar   intensity rankings (and scores), then one can be confident that the scores capture the true emotion intensities. To assess this reproducibility, we calculate average split-half reliability (SHR), a commonly used approach to determine consistency <ref type="bibr" target="#b17">(Kuder and Richardson, 1937;</ref><ref type="bibr" target="#b4">Cronbach, 1946;</ref><ref type="bibr" target="#b24">Mohammad and Bravo-Marquez, 2017)</ref>. The intuition behind SHR is as follows. All annotations for an item (in our case, tuples) are randomly split into two halves. Two sets of scores are produced independently from the two halves. Then the correlation between the two sets of scores is calculated. The process is repeated 100 times, and the correlations are averaged. If the annotations are of good quality, then the average correlation between the two halves will be high.</p><p>Table <ref type="table" target="#tab_8">5</ref> shows the split-half reliabilities for the AIT data. Observe that correlations lie between 0.82 and 0.92, indicating a high degree of reproducibility. 14</p><p>14 Past work has found the SHR for sentiment intensity annotations for words, with 6 to 8 annotations per tuple to be 0.95 to 0.98 <ref type="bibr" target="#b23">(Mohammad, 2018b;</ref>. In contrast, here SHR is calculated from whole sentences, which is a more complex annotation task and thus the SHR is expected to be lower than 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation for Automatic Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">For EI-reg, EI-oc, V-reg, and V-oc</head><p>The official competition metric for EI-reg, EI-oc, V-reg, and V-oc was the Pearson Correlation Coefficient with the Gold ratings/labels. For EI-reg and EI-oc, the correlation scores across all four emotions were averaged (macro-average) to determine the bottom-line competition metric. Apart from the official competition metric described above, some additional metrics were also calculated for each submission. These were intended to provide a different perspective on the results. The secondary metric used for the regression tasks was:</p><p>• Pearson correlation for a subset of the test set that includes only those tweets with intensity score greater or equal to 0.5.</p><p>The secondary metrics used for the ordinal classification tasks were:</p><p>• Pearson correlation for a subset of the test set that includes only those tweets with intensity classes low X, moderate X, or high X (where X is an emotion). We will refer to this set of tweets as the some-emotion subset.</p><p>• Weighted quadratic kappa on the full test set.</p><p>• Weighted quadratic kappa on the some-emotion subset of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">For E-c</head><p>The official competition metric used for E-c was multi-label accuracy (or Jaccard index). Since this is a multi-label classification task, each tweet can have one or more gold emotion labels, and one or more predicted emotion labels. Multi-label accuracy is defined as the size of the intersection of the predicted and gold label sets divided by the size of their union. This measure is calculated for each tweet t, and then is averaged over all tweets T in the dataset:</p><formula xml:id="formula_0">Accuracy = 1 |T | t∈T G t ∩ P t G t ∪ P t</formula><p>where G t is the set of the gold labels for tweet t, P t is the set of the predicted labels for tweet t, and T is the set of tweets. Apart from the official competition metric (multi-label accuracy), we also calculated micro-averaged F-score and macro-averaged F-score. 15  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Systems</head><p>Seventy-five teams (about 200 team members) participated in the shared task, submitting to one or more of the five subtasks. The numbers of teams submitting predictions for each tasklanguage pair are shown in Table <ref type="table" target="#tab_10">6</ref>. The English tasks were the most popular (33 to 48 teams for each task); however, the Arabic and Spanish tasks also got a fair amount of participation (about 13 teams for each task). Emotion intensity regression attracted the most teams.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows how frequently various machine learning algorithms were used in the five tasks. Observe that SVM/SVR, LSTMs and Bi-LSTMs were some of the most widely used algorithms. Understandably, regression algorithms such as Linear Regression were more common in the regression tasks than in the classification tasks.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows how frequently various features were used. Observe that word embeddings, affect lexicon features, and word n-grams were some of the most widely used features. Many teams also used sentence embeddings and affect-specific word embeddings. A number of teams also made use of distant supervision corpora (usually tweets with emoticons or hashtagged emotion words). Several teams made use of the AIT2018 Distant Supervision Corpus-a corpus of about 100M tweets containing emotion query words-that we provided. A small number of teams used training data from one task to supplement the training data for another task. (See row 'AIT-2018 train-dev (other task)'.)</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows how frequently features from various affect lexicons were used. Observe that several of the NRC emotion and sentiment lexicons as well as AFINN and Bing Liu Lexicon were widely used <ref type="bibr" target="#b27">(Mohammad and Turney, 2013;</ref><ref type="bibr" target="#b23">Mohammad, 2018b;</ref><ref type="bibr" target="#b15">Kiritchenko et al., 2014;</ref><ref type="bibr" target="#b28">Nielsen, 2011;</ref><ref type="bibr" target="#b11">Hu and Liu, 2004)</ref>. Several teams used the AffectiveTweets package to obtain lexicon features <ref type="bibr" target="#b24">(Mohammad and Bravo-Marquez, 2017</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results and Discussion</head><p>Tables <ref type="table" target="#tab_1">7 through 11</ref> show the results obtained by the top three teams on EI-reg, EI-oc, V-reg, V-oc, and E-c, respectively. The tables also show: (a) the results obtained by the median rank team for each task-language pair, (b) the results obtained by a baseline SVM system using just word unigrams as features, and (c) the results obtained by a system that randomly guesses the prediction-the random baseline. <ref type="bibr">17</ref> Observe that the top teams obtained markedly higher results than the SVM unigrams baselines.</p><p>Most of the top-performing teams relied on both deep neural network representations of tweets (sentence embeddings) as well as features derived from existing sentiment and emotion lexicons. Since many of the teams used similar models when participating in different tasks, we present further details of the systems grouped by the language for which they submitted predictions.   High-Ranking English Systems: The best performing system for regression (EI-reg, V-reg) and ordinal classification (EI-oc,V-oc) sub-tasks in English was SeerNet. The team proposed a unified architecture for regression and ordinal classification based on the fusion of heterogeneous features and the ensemble of multiple predictive models.</p><p>The following models or resources were used for feature extraction:</p><p>• DeepMoji <ref type="bibr" target="#b7">(Felbo et al., 2017)</ref>: a neural network for predicting emoji for tweets trained from a very large distant supervision corpus. The last two layers of the network were used as features.</p><p>• Skip thoughts: an unsupervised neural network for encoding sentences <ref type="bibr" target="#b16">(Kiros et al., 2015)</ref>.</p><p>• Sentiment neurons <ref type="bibr" target="#b34">(Radford et al., 2017)</ref>: a byte-level recurrent language model for learning sentence representations.</p><p>• Features derived from affective lexicons.</p><p>These feature vectors were used for training XG Boost and Random Forest models (using both regression and classification variants), which were later stacked using ordinal logistic regression and ridge regression models for the corresponding ordinal classification and regression tasks.</p><p>Other teams also relied on both deep neural network representations of tweets and lexicon features to learn a model with either a traditional machine learning algorithm, such as SVM/SVR (PlusEmo2Vec, TCS Research) and Logistic Regression (PlusEmo2Vec), or a deep neural network (NTUA-SLP, psyML). The sentence embeddings were obtained by training a neural network on the provided training data, a distant supervision corpus (e.g., AIT2018 Distant Supervision Corpus that has tweets with emotion-related query terms), sentiment-labeled tweet corpora (e.g., Semeval-2017 Task4A dataset on sentiment analysis in Twitter), or by using pre-trained models.     <ref type="bibr" target="#b0">Badaro et al., 2014)</ref>. Some teams (e.g., EMA) used Arabic translations of the NRC Emotion Lexicon <ref type="bibr" target="#b27">(Mohammad and Turney, 2013)</ref>. Pre-trained Arabic word embeddings (AraVec) generated from a large set of tweets were also used as additional input features by EMA and UNCC. AffecThor collected 4.4 million Arabic tweets to train their own word embeddings. Traditional machine learning algorithms (Random Forest, SVR and Ridge regression) used by EMA obtained results rivaling those obtained by deep learning approaches.</p><p>High-Ranking Spanish Systems: Convolutional neural networks and recurrent neural networks with gated units such as LSTM and GRU were employed by the winning Spanish teams (AffecThor, Amobee, ELIRF-UPV, UG18). Word embeddings trained from Spanish tweets, such as the ones provided by <ref type="bibr" target="#b35">Rothe et al. (2016)</ref>, were used as the basis for training deep learning models. They were also employed as features for more traditional learning schemes such as SVMs (UG18). Spanish Affective Lexicons such as the Spanish Emotion Lexicon (SEL) <ref type="bibr" target="#b38">(Sidorov et al., 2012)</ref> and <ref type="bibr">ML-SentiCon (Cruz et al., 2014)</ref> were also used to build the feature space (UWB, SINAI). Translation was used in two different ways: 1) automatic translation of English affective lexicons into Spanish (SINAI), and 2): training set augmentation via automatic translation of English tweets (Amobee, UG18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Summary</head><p>In the standard deep learning or representation learning approach, data representations (tweets in our case) are jointly trained for the task at hand via neural networks with convolution or recurrent layers <ref type="bibr" target="#b18">(LeCun et al., 2015)</ref>. The claim is that this can lead to more robust representations than relying on manually-engineered features. In contrast, here, most of the top-performing systems employed manually-engineered representations for tweets. These representations combine trained representations, models trained on distant supervision corpora, and unsupervised word and sentence embeddings, with manually-engineered features, such as features derived from affect lexicons. This shows that despite being rather powerful, representation learning can benefit from working in tandem with task-specific features. For emotion intensity tasks, lexicons such as the Affect Intensity Lexicon <ref type="bibr" target="#b23">(Mohammad, 2018b</ref>) that provide intensity scores are particularly helpful. Similarly, tasks on valence, arousal, and dominance can benefit from lexicons such as ANEW <ref type="bibr" target="#b2">(Bradley and Lang, 1999)</ref> and the newly created NRC Valence-Arousal-Dominance Lexicon <ref type="bibr" target="#b22">(Mohammad, 2018a)</ref>, which has entries for about 20,000 English terms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Examining Gender and Race Bias in Sentiment Analysis Systems</head><p>Automatic systems can benefit society by promoting equity, diversity, and fairness. Nonetheless, as machine learning systems become more human-like in their predictions, they are inadvertently accentuating and perpetuating inappropriate human biases. Examples include, loan eligibility and crime recidivism prediction systems that negatively assess people belonging to a certain pin/zip code (which may disproportionately impact people of a certain race) <ref type="bibr" target="#b3">(Chouldechova, 2017)</ref>, and resumé sorting systems that believe that men are more qualified to be programmers than women <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>. Similarly, sentiment and emotion analysis systems can also perpetuate and accentuate inappropriate human biases, e.g., systems that consider utterances from one race or gender to be less positive simply because of their race or gender, or customer support systems that prioritize a call from an angry male user over a call from the equally angry female user. Discrimination-aware data mining focuses on measuring discrimination in data <ref type="bibr" target="#b40">(Zliobaite, 2015;</ref><ref type="bibr" target="#b32">Pedreshi et al., 2008;</ref><ref type="bibr" target="#b10">Hajian and Domingo-Ferrer, 2013)</ref>. In that spirit, we carried out an analysis of the systems' outputs for biases towards certain races and genders. In particular, we wanted to test a hypothesis that a system should equally rate the intensity of the emotion expressed by two sentences that differ only in the gender/race of a person mentioned. Note that here the term system refers to the combination of a machine learning architecture trained on a labeled dataset, and possibly using additional language resources. The bias can originate from any or several of these parts.</p><p>We used Equity Evaluation Corpus (EEC), a recently created dataset of 8,640 English sentences carefully chosen to tease out gender and race biases . We used the EEC as a supplementary test set in the EI-reg and V-reg English tasks. Specifically, we compare emotion and sentiment intensity scores that the systems predict on pairs of sentences in the EEC that differ only in one word corresponding to race or gender (e.g., 'This man made me feel angry' vs. 'This woman made me feel angry'). Complete details on how the EEC was created, its constituent sentences, and the analysis of automatic systems for race and gender bias is available in ; we summarize the key results below.</p><p>Despite the work we describe here and that proposed by others, it should be noted that mechanisms to detect bias can often be circumvented. Nonetheless, as developers of sentiment analysis systems, and NLP systems more broadly, we cannot absolve ourselves of the ethical implications of the systems we build. Thus, the Equity Evaluation Corpus is not meant to be a catch-all for all inappropriate biases, but rather just one of the several ways by which we can examine the fairness of sentiment analysis systems. The EEC corpus is freely available so that both developers and users can use it, and build on it. 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Methodology</head><p>The race and gender bias evaluation was carried out on the EI-reg and V-reg predictions of 219 automatic systems (by 50 teams) on the EEC sentences. The EEC sentences were created from simple templates such as '&lt;noun phrase&gt; feels devastated', where &lt;noun phrase&gt; is replaced with one of the following:</p><p>• common African American (AA) female and male first names, • common European American (EA) female and male first names, • noun phrases referring to females and males, such as 'my daughter' and 'my son'.</p><p>Notably, one can derive pairs of sentences from the EEC such that they differ only in one phrase cor-18 http://saifmohammad.com/WebPages/Biases-SA.html responding to gender or race (e.g., 'My daughter feels devastated' and 'My son feels devastated').</p><p>For the full lists of names, noun phrases, and sentence templates see . In total, 1,584 pairs of scores were compared for gender and 144 pairs of scores were compared for race.</p><p>For each submission, we performed the paired two sample t-test to determine whether the mean difference between the two sets of scores (across the two races and across the two genders) is significant. We set the significance level to 0.05. However, since we performed 438 assessments (219 submissions evaluated for biases in both gender and race), we applied Bonferroni correction. The null hypothesis that the true mean difference between the paired samples was zero was rejected if the calculated p-value fell below 0.05/438.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Gender Bias Results</head><p>Individual submission results were communicated to the participants. Here, we present the summary results across all the teams. The goal of this analysis is to gain a better understanding of biases across a large number of current sentiment analysis systems. Thus, we partition the submissions into three groups according to the bias they show:</p><p>• F = M: submissions that showed no statistically significant difference in intensity scores predicted for corresponding female and male noun phrase sentences,</p><p>• F↑-M↓: submissions that consistently gave higher scores for sentences with female noun phrases than for corresponding sentences with male noun phrases,</p><p>• F↓-M↑: submissions that consistently gave lower scores for sentences with female noun phrases than for corresponding sentences with male noun phrases,</p><p>Table <ref type="table" target="#tab_1">12</ref> shows the number of submissions in each group. If all the systems are unbiased, then the number of submissions for the group F = M would be the maximum, and the number of submissions in all other groups would be zero.</p><p>Observe that on the four emotion intensity prediction tasks, only about 12 of the 46 submissions (about 25% of the submissions) showed no statistically significant score difference. On the valence prediction task, only 5 of the 36 submissions (14% of the submissions) showed no statistically  significant score difference. Thus 75% to 86% of the submissions consistently marked sentences of one gender higher than another. When predicting anger, joy, or valence, the number of systems consistently giving higher scores to sentences with female noun phrases (21-25) is markedly higher than the number of systems giving higher scores to sentences with male noun phrases (8-13). (Recall that higher valence means more positive sentiment.)</p><p>In contrast, on the fear task, most submissions tended to assign higher scores to sentences with male noun phrases (23) as compared to the number of systems giving higher scores to sentences with female noun phrases (12). When predicting sadness, the number of submissions that mostly assigned higher scores to sentences with female noun phrases ( <ref type="formula">18</ref>) is close to the number of submissions that mostly assigned higher scores to sentences with male noun phrases (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Race Bias Results</head><p>We did a similar analysis as for gender, for race. For each submission on each task, we calculated the difference between the average predicted score on the set of sentences with African American (AA) names and the average predicted score on the set of sentences with European American (EA) names. Then, we aggregated the results over all such sentence pairs in the EEC.</p><p>Table <ref type="table" target="#tab_1">13</ref> shows the results. The table has the same form and structure as the gender result table. Observe that the number of submissions with no statistically significant score difference for sentences pertaining to the two races is about 5-11 (about 11% to 24%) for the four emotions and 3 (about 8%) for valence. These numbers are even lower than what was found for gender.</p><p>The majority of the systems assigned higher scores to sentences with African American names on the tasks of anger, fear, and sadness intensity prediction. On the joy and valence tasks, most submissions tended to assign higher scores to sen-  tences with European American names. We found the score differences across genders and across races to be somewhat small (&lt; 0.03 in magnitude, which is 3% of the 0 to 1 score range). However, what impact a consistent bias, even with a magnitude &lt; 3%, might have in downstream applications merits further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary</head><p>We organized the SemEval-2018 Task 1: Affect in Tweets, which included five subtasks on inferring the affectual state of a person from their tweet. For each task, we provided training, development, and test datasets for English, Arabic, and Spanish tweets. This involved creating a new Affect in Tweets dataset of more than 22,000 tweets such that subsets are annotated for a number of emotion dimensions. For each emotion dimension, we annotated the data not just for coarse classes (such as anger or no anger) but also for fine-grained real-valued scores indicating the intensity of emotion. We used Best-Worst Scaling to obtain finegrained real-valued intensity scores and showed that the annotations are reliable (split-half reliability scores &gt; 0.8).</p><p>Seventy-five teams made 319 submissions to the fifteen task-language pairs. Most of the topperforming teams relied on both deep neural network representations of tweets (sentence embeddings) as well as features derived from existing sentiment and emotion lexicons. Apart from the usual evaluations for the quality of predictions, we also examined 219 EI-reg and V-reg English submissions for bias towards particular races and genders using the Equity Evaluation Corpus. We found that a majority of the systems consistently provided slightly higher scores for one race or gender. All of the data is made freely available. <ref type="bibr">19</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Valence score (V-reg), class (V-oc) distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Machine learning algorithms used by teams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Features and resources used by teams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Lexicons used by teams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The annotations of English Tweets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>fear joy love optim. pessi. sadn. surp. trust neutral</figDesc><table><row><cell>English Arabic Spanish</cell><cell>36.1 39.4 32.2</cell><cell>13.9 9.6 11.7</cell><cell>36.6 16.8 39.3 12.3 19.6 17.8 26.9 25.2 14.7 10.5 30.5 7.9</cell><cell>31.3 24.5 10.2</cell><cell>11.6 22.8 16.7</cell><cell>29.4 37.4 23.0</cell><cell>5.2 2.2 4.6</cell><cell>5.0 5.3 4.6</cell><cell>2.7 0.6 4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Percentage of tweets that were labeled with a given emotion (after aggregation of votes).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The number of tweets in the SemEval-2018 Affect in Tweets Dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Annotator agreement for the Multi-label Emotion Classification (E-c) Datasets.</cell></row><row><cell cols="4">Language Affect Dimension Spearman Pearson English Emotion Intensity anger 0.89 0.90 fear 0.84 0.85 joy 0.90 0.91 sadness 0.82 0.83 Valence 0.92 0.92</cell></row><row><cell>Arabic</cell><cell>Emotion Intensity anger fear joy sadness Valence</cell><cell>0.88 0.85 0.88 0.86 0.94</cell><cell>0.89 0.87 0.89 0.87 0.94</cell></row><row><cell>Spanish</cell><cell>Emotion Intensity anger fear joy sadness Valence</cell><cell>0.88 0.85 0.89 0.86 0.89</cell><cell>0.88 0.86 0.89 0.86 0.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Split-half reliabilities in the AIT Dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Number of teams in each task-language pair.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>). 16 16 https://affectivetweets.cms.waikato.ac.nz/</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Task 1 emotion intensity regression (EI-reg): Results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Task 2 emotion intensity ordinal classification (EI-oc): Results.</figDesc><table><row><cell>Rank Team Name English 1 SeerNet 2 TCS Research 3 PlusEmo2Vec 18 Median Team 31 SVM-Unigrams 35 Random Baseline Arabic 1 EiTAKA 2 AffecThor 3 EMA 6 Median Team 9 SVM-Unigrams 13 Random Baseline Spanish 1 AffecThor 2 Amobee 3 ELiRF-UPV 6 Median Team 9 SVM-Unigrams 13 Random Baseline</cell><cell>r (all) r (0.5-1) 87.3 69.7 86.1 68.0 86.0 69.1 78.4 59.1 58.5 44.9 3.1 1.2 82.8 57.8 81.6 59.7 80.4 57.6 72.0 36.2 57.1 42.3 -5.2 2.2 79.5 65.9 77.0 64.2 74.2 57.1 60.9 50.9 57.4 51.5 -2.3 2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Task 3 valence regression (V-reg): Results.</figDesc><table><row><cell>High-Ranking Arabic Systems: Top teams trained their systems using deep learning tech-</cell></row><row><cell>niques, such as CNN, LSTM and Bi-LSTM (Af-</cell></row><row><cell>fecThor, EiTAKA, UNCC). Traditional machine</cell></row><row><cell>learning approaches, such as Logistic Regression,</cell></row><row><cell>Ridge Regression, Random Forest and SVC/SVM,</cell></row><row><cell>were also employed (EMA, INGEOTEC, PARTNA,</cell></row><row><cell>Tw-StAR). Many teams relied on Arabic pre-</cell></row><row><cell>processing and normalization techniques in an</cell></row><row><cell>attempt to decrease the sparsity due to mor-</cell></row><row><cell>phological complexity in the Arabic language.</cell></row><row><cell>EMA applied stemming and lemmatization us-</cell></row><row><cell>ing MADAMIRA (a morphological analysis and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Task 4 valence ord. classifn. (V-oc): Results.</cell></row><row><cell>disambiguation tool for Arabic), while TwStar</cell></row><row><cell>and PARTNA used stemmer designed for handling</cell></row><row><cell>tweets. In addition, top systems applied addi-</cell></row><row><cell>tional pre-processing, such as dropping punctua-</cell></row><row><cell>tions, mentions, stop words, and hashtag symbols.</cell></row><row><cell>Many teams (e.g., AffecThor, EiTAKA and</cell></row><row><cell>EMA) utilized Arabic sentiment lexicons</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: Task 5 emotion classification (E-c): Results.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>Analysis of gender bias: The number of submissions in each of the three bias groups.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc>Analysis of race bias: The number of submissions in each of the three bias groups.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Intensity is different from arousal, which refers to the extent to which an emotion is calming or exciting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://competitions.codalab.org/competitions/17751 3 Determined through pilot annotations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://saifmohammad.com/WebPages/EmoInt2017.html 5 Note that valence ordinal classification is the traditional sentiment analysis task most commonly explored in NLP literature. The classes may vary from just three (positive, negative, and neutral) to five, seven, or nine finer classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://www.wordreference.com/sinonimos/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.figure-eight.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Gold tweets were annotated more than four times.10  Code for generating tuples from items as well as for generating scores from BWS annotations: http://saifmohammad.com/WebPages/BestWorst.html11  Valence is a bi-polar scale; hence, more classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Formulae are provided on the task webpage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">The results for each of the 75 participating teams are shown on the task website and also in the supplementary material file. (Not shown here due to space constraints.)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">https://competitions.codalab.org/competitions/17751</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Table <ref type="table">14</ref> shows the summary details of the annotations done for the SemEval-2018 Affect in Tweets dataset. Figure <ref type="figure">5</ref> shows the histograms of the EI-reg tweets in the anger, joy, sadness, and fear datasets. The tweets are grouped into bins of scores 0-0.05, 0.05-0.1, and so on until 0.95-1. The colors for the bins correspond to their ordinal classes: no emotion, low emotion, moderate emotion, and high emotion. The ordinal classes were determined from the EI-oc manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material:</head><p>The supplementary pdf associated with this paper includes longer versions of tables included in this paper. Tables 1 to 15 in the supplementary pdf show result tables that include the scores of each of the 319 systems participating in the tasks. Table <ref type="table">16</ref> in the supplementary pdf shows the annotator agreement for each of the twelve classes, for each of the three languages, in the Multi-label Emotion Classification (E-c) Dataset. We observe that the Fleiss' κ scores are markedly higher for the frequently occurring four basic emotions (joy, sadness, fear, and anger), and lower for the less frequent emotions. (Frequencies for the emotions are shown in Table <ref type="table">2</ref>.) Also, agreement is low for the neutral class. This is not surprising because the boundary between neutral (or no emotion) and slight emotion is fuzzy. This means that often at least one or two annotators indicate that the person is feeling some joy or some sadness, even if most others indicate that the person is not feeling any emotion.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large scale arabic sentiment lexicon for arabic opinion mining</title>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Badaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hazem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wassim</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><surname>El-Hajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing</title>
				<meeting>the EMNLP 2014 Workshop on Arabic Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
				<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Affective norms for English words (ANEW): Instruction manual and affective ratings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>The Center for Research in Psychophysiology, University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A case study of the splithalf reliability coefficient</title>
		<author>
			<persName><surname>Lj Cronbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ml-senticon: Un lexicón multilingüe de polaridades semánticas a nivel de lemas</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fermín</surname></persName>
		</author>
		<author>
			<persName><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Troyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F Javier</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
				<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1615" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Best-worst scaling: theory and methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A J</forename><surname>Marley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Choice Modelling</title>
				<editor>
			<persName><forename type="first">Stephane</forename><surname>Hess</surname></persName>
			<persName><forename type="first">Andrew</forename><surname>Daly</surname></persName>
		</editor>
		<imprint>
			<publisher>Edward Elgar Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="178" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The laws of emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nico</surname></persName>
		</author>
		<author>
			<persName><surname>Frijda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">349</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A methodology for direct and indirect discrimination prevention in data mining</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hajian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1445" to="1459" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Capturing reliable fine-grained sentiment associations by crowdsourcing and best-worst scaling</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
				<meeting>The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>The Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Vancouver</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM)</title>
				<meeting>the 7th Joint Conference on Lexical and Computational Semantics (*SEM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The theory of the estimation of test reliability</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Kuder</surname></persName>
		</author>
		<author>
			<persName><surname>Marion W Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Best-worst scaling: A model for the largest difference judgments</title>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentiment lexicons for arabic social media</title>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
				<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sentiment analysis: Detecting valence, emotions, and other affectual states from text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Emotion Measurement</title>
		<editor>Herb Meiselman</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>The Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word affect intensities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)</title>
				<meeting>the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WASSA-2017 shared task on emotion intensity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Bravo-Marquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</title>
				<meeting>the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding emotions: A dataset of tweets to study interactions between affect categories</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)</title>
				<meeting>the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stance and sentiment in tweets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parinaz</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Section of the ACM Transactions on Internet Technology on Argumentation in Social Media</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowdsourcing a word-emotion association lexicon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="465" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new ANEW: Evaluation of a word list for sentiment analysis in microblogs</title>
		<author>
			<persName><forename type="first">Finnårup</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESWC Workshop on &apos;Making Sense of Microposts&apos;: Big things come in small packages</title>
				<meeting>the ESWC Workshop on &apos;Making Sense of Microposts&apos;: Big things come in small packages<address><addrLine>Heraklion, Crete</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Maxdiff analysis: Simple counting, individual-level logit, and HB</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Orme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Sawtooth Software, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Argumentation mining: the detection, classification and structure of arguments in text</title>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Mochales Palau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on artificial intelligence and law</title>
				<meeting>the 12th international conference on artificial intelligence and law</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Emotions in Social Psychology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Parrot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="560" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A general psychoevolutionary theory of emotion</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion: Theory, research, and experience</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3" to="33" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ultradense word embeddings by orthogonal transformation</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Core affect and the psychological construction of emotion</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Empirical study of machine learning based approach for opinion mining in tweets</title>
		<author>
			<persName><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabino</forename><surname>Miranda-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Viveros-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noé</forename><surname>Castro-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Velásquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismael</forename><surname>Díaz-Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Suárez-Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Treviño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican international conference on Artificial intelligence</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantifying qualitative data for understanding controversial issues</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)</title>
				<meeting>the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A survey on measuring indirect discrimination in machine learning</title>
		<author>
			<persName><forename type="first">Indre</forename><surname>Zliobaite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00148</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
