<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">George Washington University Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of the Basque Country Donostia</orgName>
								<orgName type="institution" key="instit2">Basque Country d University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of the Basque Country Donostia</orgName>
								<orgName type="institution" key="instit2">Basque Country d University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
						</author>
						<title level="a" type="main">SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012)(2013)(2014)(2015)(2016)(2017).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other. The STS task is motivated by the observation that accurately modeling the meaning similarity of sentences is a foundational language understanding problem relevant to numerous applications including: machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. STS enables the evaluation of techniques from a diverse set of domains against a shared interpretable performance criteria. Semantic inference tasks related to STS include textual entailment <ref type="bibr" target="#b11">(Bentivogli et al., 2016;</ref><ref type="bibr" target="#b17">Bowman et al., 2015;</ref><ref type="bibr" target="#b22">Dagan et al., 2010)</ref>, semantic relatedness <ref type="bibr" target="#b11">(Bentivogli et al., 2016)</ref> and paraphrase detection <ref type="bibr" target="#b87">(Xu et al., 2015;</ref><ref type="bibr" target="#b31">Ganitkevitch et al., 2013;</ref><ref type="bibr" target="#b25">Dolan et al., 2004)</ref>. STS differs from both textual entailment and paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., "night" and "day" are highly related but not particularly similar).</p><p>To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016</ref>. During this time, diverse similarity methods and data sets 1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntactic similarity <ref type="bibr" target="#b9">(Bär et al., 2012;</ref><ref type="bibr">Šarić et al., 2012a;</ref><ref type="bibr" target="#b42">Jimenez et al., 2012a)</ref>. During subsequent evaluations, strong new similarity signals emerged, such as <ref type="bibr" target="#b78">Sultan et al. (2015)</ref>'s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems . The best performance tends to be obtained by ensembling feature engineered and deep learning models <ref type="bibr" target="#b69">(Rychalska et al., 2016)</ref>.</p><p>Significant research effort has focused on STS over English sentence pairs. 2 English STS is a 1 i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet <ref type="bibr" target="#b59">(Miller, 1995;</ref><ref type="bibr" target="#b28">Fellbaum, 1998)</ref>, FrameNet <ref type="bibr" target="#b8">(Baker et al., 1998)</ref>, OntoNotes <ref type="bibr" target="#b39">(Hovy et al., 2006)</ref>, web discussion fora, plagiarism, MT post-editing and Q&amp;A data sets. Data sets are summarized on: http://ixa2.si.ehu.es/stswiki. <ref type="bibr">2</ref> The 2012 and 2013 STS tasks were English only. The 2014 and 2015 task included a Spanish track and 2016 had a well-studied problem, with state-of-the-art systems often achieving 70 to 80% correlation with human judgment. To promote progress in other languages, the 2017 task emphasizes performance on Arabic and Spanish as well as cross-lingual pairings of English with material in Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions.</p><p>STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., <ref type="bibr" target="#b6">Arora et al. (2017)</ref>; <ref type="bibr" target="#b21">Conneau et al. (2017)</ref>; <ref type="bibr" target="#b61">Mu et al. (2017)</ref>; <ref type="bibr" target="#b64">Pagliardini et al. (2017)</ref>; <ref type="bibr" target="#b85">Wieting and Gimpel (2017)</ref>; ; <ref type="bibr" target="#b37">Hill et al. (2016)</ref>; <ref type="bibr" target="#b45">Kenter et al. (2016)</ref>; Lau and Baldwin (2016); <ref type="bibr">Wieting et al. (2016b,a)</ref>; ; <ref type="bibr" target="#b67">Pham et al. (2015)</ref>). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks <ref type="bibr">(2012)</ref><ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref><ref type="bibr">(2016)</ref><ref type="bibr">(2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Overview</head><p>STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table <ref type="table" target="#tab_0">1</ref> guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale is designed to be accessible by reasonable human judges without any formal expertise in linguistics. Using reasonable human interpretations of natural language semantics was popularized by the related textual entailment task <ref type="bibr" target="#b22">(Dagan et al., 2010)</ref>. The resulting annotations reflect both pragmatic and world knowledge and are more interpretable and useful within downstream systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Data</head><p>The Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b17">(Bowman et al., 2015)</ref> is the primary evaluation data source with the exception that one of the pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>The two sentences are completely equivalent, as they mean the same thing. The bird is bathing in the sink. Birdie is washing itself in the water basin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>The two sentences are mostly equivalent, but some unimportant details differ. Two boys on a couch are playing video games. Two boys are playing a video game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The two sentences are roughly equivalent, but some important information differs/missing. John said he is considered a witness but not a suspect. "He is not a suspect anymore." John said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>The two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0</head><p>The two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud.  <ref type="bibr" target="#b4">Agirre et al. (2013)</ref>. cross-lingual tracks explores data from the WMT 2014 quality estimation task <ref type="bibr" target="#b15">(Bojar et al., 2014)</ref>. <ref type="bibr">3</ref> Sentences pairs in SNLI derive from Flickr30k image captions <ref type="bibr" target="#b88">(Young et al., 2014)</ref> and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness <ref type="bibr" target="#b54">(Marelli et al., 2014)</ref>, we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores.</p><p>Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) <ref type="bibr">(Snover et al., 2006). 4</ref> Participants are not allowed to use the gold quality estimation annotations to inform STS scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tracks</head><p>Table <ref type="table" target="#tab_2">2</ref> summarizes the evaluation data by track.</p><p>The six tracks span four languages: Arabic, English, Spanish and Turkish. Track 4 has subtracks with 4a drawing from SNLI and 4b pulling from WMT's quality estimation task. Track 6 is a surprise language track with no annotated training data and the identity of the language pair first announced when the evaluation data was released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Preparation</head><p>This section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Arabic, Spanish and Turkish Translation</head><p>Sentences from SNLI are human translated into Arabic, Spanish and Turkish. Sentences are translated independently from their pairs. Arabic translation is provided by CMU-Qatar by native Arabic speakers with strong English skills. Translators are given an English sentence and its Arabic machine translation 5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Embedding Space Pair Selection</head><p>We construct our own pairings of the SNLI sentences to deter gold entailment labels being used to inform STS scores. The word embedding similarity selection heuristic from STS 2016 <ref type="bibr" target="#b2">(Agirre et al., 2016)</ref> is used to find interesting pairs. Sentence embeddings are computed as the sum of in-dividual word embeddings, v(s) = w∈s v(w). 7 Sentences with likely meaning overlap are identified using cosine similarity, Eq. (1).</p><formula xml:id="formula_0">sim v (s 1 , s 2 ) = v(s 1 )v(s 2 ) v(s 1 ) 2 v(s 2 ) 2</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Annotation</head><p>Annotation of pairs with STS labels is performed using Crowdsourcing, with the exception of Track 4b that uses a single expert annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Crowdsourced Annotations</head><p>Crowdsourced annotation is performed on Amazon Mechanical Turk. <ref type="bibr">8</ref> Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to <ref type="bibr" target="#b2">Agirre et al. (2016)</ref>. Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master 9 qualification is required to perform the task. Gold scores average the five individual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expert Annotation</head><p>English-Spanish WMT quality estimation pairs for Track 4b are annotated for STS by a University of Sheffield graduate student who is a native speaker of Spanish and fluent in English. This track differs significantly in label distribution and the complexity of the annotation task. Sentences in a pair are translations of each other and tend to be more semantically similar. Interpreting the potentially subtle meaning differences introduced by MT errors is challenging. To accurately assess STS performance on MT quality estimation data, no attempt is made to balance the data by similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training Data</head><p>The following summarizes the training data: Table 3 English;   English, Spanish and English-Spanish training data pulls from prior STS evaluations. Arabic and Arabic-English training data is produced by translating a subset of the English training data and transferring the similarity scores. For the MT quality estimation data in track 4b, Spanish sentences are translations of their English counterparts, differing substantially from existing Spanish-English STS data. We release one thousand new Spanish-English STS pairs sourced from the 2013 WMT translation task and produced by a phrase-based Moses SMT system <ref type="bibr" target="#b16">(Bojar et al., 2013)</ref>. The data is expert annotated and has a similar label distribution to the track 4b test data with 17% of the pairs scoring an STS score of less than 3, 23% scoring 3, 7% achieving a score of 4 and 53% scoring 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training vs. Evaluation Data Analysis</head><p>Evaluation data from SNLI tend to have sentences that are slightly shorter than those from prior years of the STS shared task, while the track 4b MT qual-   Similarity scores for our pairings of the SNLI sentences are slightly lower than recent shared task years and much lower than early years. The change is attributed to differences in data selection and filtering. The average 2017 similarity score is 2.2 overall and 2.3 on the track 7 English data. Prior English data has the following average similarity scores: 2016 2.4; 2015 2.4; 2014 2.8; 2013 3.0; 2012 3.5. Translation quality estimation data from track 4b has an average similarity score of 4.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">System Evaluation</head><p>This section reports participant evaluation results for the SemEval-2017 STS shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Participation</head><p>The task saw strong participation with 31 teams producing 84 submissions. 17 teams provided 44 systems that participated in all tracks. Table <ref type="table">9</ref> summarizes participation by track. Traces of the focus on English are seen in 12 teams participating just in track 5, English. Two teams participated exclusively in tracks 4a and 4b, English-Spanish. One team took part solely in track 1, Arabic.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">CodaLab</head><p>As directed by the SemEval workshop organizers, the CodaLab research platform hosts the task. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Baseline</head><p>The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence. <ref type="bibr">12</ref> For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation. <ref type="bibr">13</ref> The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23 rd overall out the 44 system submissions that participated in all tracks.</p><p>11 https://competitions.codalab.org/ competitions/16051</p><p>12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers.</p><p>13 http://translate.google.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Rankings</head><p>Participant performance is provided in Table <ref type="table" target="#tab_0">10</ref>. ECNU is best overall (avg r: 0.7316) and achieves the highest participant evaluation score on: track 2, Arabic-English (r: 0.7493); track 3, Spanish (r: 0.8559); and track 6, Turkish-English (r: 0.7706).</p><p>BIT attains the best performance on track 1, Arabic (r: 0.7543). CompiLIG places first on track 4a, SNLI Spanish-English (r: 0.8302). SEF@UHH exhibits the best correlation on the difficult track 4b WMT quality estimation pairs (r: 0.3407). RTV has the best system for the track 5 English data (r: 0.8547), followed closely by DT Team (r: 0.8536).</p><p>Especially challenging tracks with SNLI data are: track 1, Arabic; track 2, Arabic-English; and track 6, English-Turkish. Spanish-English performance is much higher on track 4a's SNLI data than track 4b's MT quality estimation data. This highlights the difficulty and importance of making fine grained distinctions for certain downstream applications. Assessing STS methods for quality estimation may benefit from using alternatives to Pearson correlation for evaluation. <ref type="bibr">14</ref> Results tend to decrease on cross-lingual tracks. The baseline drops &gt; 10% relative on Arabic-English and Spanish-English (SNLI) vs. monolingual Arabic and Spanish. Many participant systems show smaller decreases. ECNU's top ranking entry performs slightly better on Arabic-English than Arabic, with a slight drop from Spanish to Spanish-English (SNLI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Methods</head><p>Participating teams explore techniques ranging from state-of-the-art deep learning models to elaborate feature engineered systems. Prediction signals include surface similarity scores such as edit distance and matching n-grams, scores derived from word alignments across pairs, assessment by MT evaluation metrics, estimates of conceptual similarity as well as the similarity between word and sentence level embeddings. For cross-lingual and non-English tracks, MT was widely used to convert the two sentences being compared into the same language. <ref type="bibr">15</ref> Select methods are highlighted below.   83.02• 15.50 compiLIG  76.84 14.64 compiLIG  79.10 14.94 DT TEAM <ref type="bibr" target="#b52">(Maharjan et al., 2017)</ref> 85.36 DT TEAM <ref type="bibr" target="#b52">(Maharjan et al., 2017)</ref> 83.60 DT TEAM <ref type="bibr" target="#b52">(Maharjan et al., 2017)</ref> 83.29 FCICU <ref type="bibr" target="#b32">(Hassan et al., 2017)</ref> 82.17 ITNLPAiKF <ref type="bibr" target="#b51">(Liu et al., 2017)</ref> 82.31 ITNLPAiKF <ref type="bibr" target="#b51">(Liu et al., 2017)</ref> 82.31 ITNLPAiKF <ref type="bibr" target="#b51">(Liu et al., 2017)</ref> 81.59 L2F/INESC-ID <ref type="bibr" target="#b30">(Fialho et al., 2017</ref>   <ref type="table" target="#tab_0">10</ref>: STS 2017 rankings ordered by average correlation across tracks 1-5. Performance is reported by convention as Pearson's r × 100. For tracks 1-6, the top ranking result is marked with a • symbol and results in bold have no statistically significant difference with the best result on a track, p &gt; 0.05 Williams' t-test <ref type="bibr" target="#b23">(Diedenhofen and Musch, 2015)</ref>.</p><p>ECNU <ref type="bibr" target="#b79">(Tian et al., 2017)</ref> The best overall system is from ENCU and ensembles well performing a feature engineered models with deep learning methods. Three feature engineered models use Random Forest (RF), Gradient Boosting (GB) and XGBoost (XGB) regression methods with features based on: n-gram overlap; edit distance; longest common prefix/suffix/substring; tree kernels <ref type="bibr" target="#b60">(Moschitti, 2006)</ref>; word alignments <ref type="bibr" target="#b78">(Sultan et al., 2015)</ref>; summarization and MT evaluation metrics (BLEU, GTM-3, NIST, WER, ME-TEOR, ROUGE); and kernel similarity of bagsof-words, bags-of-dependencies and pooled wordembeddings. ECNU's deep learning models are differentiated by their approach to sentence embeddings using either: averaged word embeddings, projected word embeddings, a deep averaging network (DAN) <ref type="bibr" target="#b41">(Iyyer et al., 2015)</ref> or LSTM (Hochreiter and Schmidhuber, 1997). Each network feeds the element-wise multiplication, subtraction and concatenation of paired sentence embeddings to additional layers to predict similarity scores. The ensemble averages scores from the four deep learning and three feature engineered models. <ref type="bibr">16</ref> BIT  Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclusively. Another ensembles IC with <ref type="bibr" target="#b78">Sultan et al. (2015)</ref>'s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best.</p><p>HCTI <ref type="bibr" target="#b74">(Shao, 2017)</ref> Third place overall is obtained by HCTI with a model similar to a convolutional Deep Structured Semantic Model (CDSSM) <ref type="bibr" target="#b20">(Chen et al., 2015;</ref><ref type="bibr" target="#b40">Huang et al., 2013)</ref>. Sentence embeddings are generated with twin convolutional neural networks (CNNs). The embeddings are then compared using cosine similarity and element wise difference with the resulting values fed to additional layers to predict similarity labels. The archiand MITRE. HCTI submitted a separate run using ar, es and en trained models that underperformed using their en model with MT for ar and es. CompiLIG's model is cross-lingual but includes a word alignment feature that depends on MT. SEF@UHH built ar, es, and en models and use bi-directional MT for cross-lingual pairs. LIM-LIG and DT Team only participate in monolingual tracks. <ref type="bibr">16</ref> The two remaining ECNU runs only use either RF or GB and exclude the deep learning models.</p><p>tecture is abstractly similar to ECNU's deep learning models. UMDeep <ref type="bibr" target="#b10">(Barrow and Peskov, 2017)</ref> took a similar approach using LSTMs rather than CNNs for the sentence embeddings.</p><p>MITRE <ref type="bibr" target="#b36">(Henderson et al., 2017)</ref> Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components include: alignment similarity; TakeLab STS <ref type="bibr">(Šarić et al., 2012b)</ref>; string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment <ref type="bibr" target="#b19">(Chen et al., 2016)</ref>.</p><p>FCICU <ref type="bibr" target="#b32">(Hassan et al., 2017)</ref> Fifth place overall is FCICU that computes a sense-base alignment using BabelNet <ref type="bibr" target="#b63">(Navigli and Ponzetto, 2010)</ref>. Babel-Net synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of <ref type="bibr" target="#b78">Sultan et al. (2015)</ref>'s method. Both runs average the Babelnet based scores with soft-cardinality <ref type="bibr" target="#b43">(Jimenez et al., 2012b)</ref>.</p><p>CompiLIG  The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using features including: cross-lingual conceptual similarity using DBNary <ref type="bibr" target="#b73">(Serasset, 2015)</ref>, cross-language Multi-Vec word embeddings <ref type="bibr" target="#b12">(Berard et al., 2016)</ref>, and <ref type="bibr" target="#b18">Brychcin and Svoboda (2016)</ref>  gineering combined with the following deep learning models: DSSM <ref type="bibr" target="#b40">(Huang et al., 2013)</ref>, CDSSM <ref type="bibr" target="#b75">(Shen et al., 2014)</ref> and skip-thoughts <ref type="bibr" target="#b46">(Kiros et al., 2015)</ref>. Engineered features include: unigram overlap, summed word alignments scores, fraction of unaligned words, difference in word counts by type (all, adj, adverbs, nouns, verbs), and min to max ratios of words by type. Select features have a multiplicative penalty for unaligned words.</p><p>SEF@UHH (Duma and Menzel, 2017) First place on the challenging Spanish-English MT pairs (Track 4b) is SEF@UHH. Unsupervised similarity scores are computed from paragraph vectors (Le and Mikolov, 2014) using cosine, negation of Bray-Curtis dissimilarity and vector correlation. MT converts cross-lingual pairs, L 1 -L 2 , into two monolingual pairs, L 1 -L 1 and L 2 -L 2 , with averaging used to combine the monolingual similarity scores. Bray-Curtis performs well overall, while cosine does best on the Spanish-English MT pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Figure <ref type="figure" target="#fig_0">1</ref> plots model similarity scores against human STS labels for the top 5 systems from tracks 5 (English), 1 (Arabic) and 4b (English-Spanish MT). While many systems return scores on the same scale as the gold labels, 0-5, others return scores from approximately 0 and 1. Lines on the graphs illustrate perfect performance for both a 0-5 and a 0-1 scale. Mapping the 0 to 1 scores to range from 0-5, 20 approximately 80% of the scores from top performing English systems are within 1.0 pt of the gold label. Errors for Arabic are more broadly distributed, particularly for model scores between 1 and 4. The English-Spanish MT plots the weak relationship between the predicted and gold scores.</p><p>Table <ref type="table" target="#tab_0">12</ref> provides examples of difficult sentence pairs for participant systems and illustrates common sources of error for even well-ranking systems including: (i) word sense disambiguation "making" and "preparing" are very similar in the context of "food", while "picture" and "movie" are not similar when picture is followed by "day"; (ii) attribute importance "outside" vs. "deserted" are smaller details when contrasting "The man is in a deserted field" with "The man is outside in the field"; (iii) compositional meaning "A man is carrying a canoe with a dog" has the same content words as "A dog is carrying a man in a canoe" but carries a different meaning; (iv) negation systems score ". . . with goggles and a swimming cap" as nearly equivalent to ". . . without goggles or a swimming cap". Inflated similarity scores for examples like "There is a young girl" vs. "There is a young boy with the woman" demonstrate (v) semantic blending, whereby appending "with a woman" to "boy" brings its representation closer to that of "girl".</p><p>For multilingual and cross-lingual pairs, these issues are magnified by translation errors for systems that use MT followed by the application of a monolingual similarity model. For track 4b Spanish-English MT pairs, some of the poor performance can in part be attributed to many systems using MT to re-translate the output of another MT system, obscuring errors in the original translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Contrasting Cross-lingual STS with MT Quality Estimation</head><p>Since MT quality estimation pairs are translations of the same sentence, they are expected to be minimally on the same topic and have an STS score ≥ 1. <ref type="bibr">21</ref> The actual distribution of STS scores is such that only 13% of the test instances score below 3, 22% of the instances score 3, 12% score 4 and 53% score 5. The high STS scores indicate that MT systems are surprisingly good at preserving meaning. However, even for a human, interpreting changes caused by translations errors can be difficult due both to disfluencies and subtle errors with important changes in meaning.</p><p>The Pearson correlation between the gold MT quality scores and the gold STS scores is 0.41, which shows that translation quality measures and STS are only moderately correlated. Differences are in part explained by translation quality scores penalizing all mismatches between the source segment and its translation, whereas STS focuses on differences in meaning. However, the difficult interpretation work required for STS annotation may Pairs Human DT Team ECNU BIT FCICU ITNLP-AiKF There is a cook preparing food.</p><p>5.0 4.1 4.1 3.7 3.9 4.5 A cook is making food. The man is in a deserted field.</p><p>4.0 3.0 3.1 3.6 3.1 2.8 The man is outside in the field. A girl in water without goggles or a swimming cap.</p><p>3.0 4.8 4.6 4.0 4.7 0.1 A girl in water, with goggles and swimming cap. A man is carrying a canoe with a dog.</p><p>1.8 3.2 4.7 4.9 5.0 4.6 A dog is carrying a man in a canoe. There is a young girl.</p><p>1.0 2.6 3.3 3.9 1.9 3.1 There is a young boy with the woman. The kids are at the theater watching a movie. 0.2 1.0 2.3 2.0 0.8 1.7 it is picture day for the boys  increase the risk of inconsistent and subjective labels. The annotations for MT quality estimation are produced as by-product of post-editing. Humans fix MT output and the edit distance between the output and its post-edited correction provides the quality score. This post-editing based procedure is known to produce relatively consistent estimates across annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">STS Benchmark</head><p>The STS Benchmark is a careful selection of the English data sets used in SemEval and *SEM STS shared tasks between 2012 and 2017. Tables <ref type="table" target="#tab_0">11  and 13</ref> provide details on the composition of the benchmark. The data is partitioned into training, development and test sets. <ref type="bibr">22</ref> The development set can be used to design new models and tune hyperparameters. The test set should be used sparingly and only after a model design and hyperparameters have been locked against further changes. Using the STS Benchmark enables comparable assessments across different research efforts and improved tracking of the state-of-the-art. Table <ref type="table" target="#tab_0">14</ref> shows the STS Benchmark results for some of the best systems from Track 5 (EN-EN) <ref type="bibr">23</ref> and compares their performance to competitive baselines from the literature. All baselines were run by the organizers using canonical pre-trained models made available by the originator of each method, <ref type="bibr">24</ref> with the exception of PV-DBOW that <ref type="bibr">22</ref> Similar to the STS shared task, while the training set is provided as a convenience, researchers are encourage to incorporate other supervised and unsupervised data as long as no supervised annotations of the test partitions are used.</p><p>23 Each participant submitted the run which did best in the development set of the STS Benchmark, which happened to be the same as their best run in Track 5 in all cases.  embeddings from glove.6B.zip; Word2vec: https://code.google.com/archive/ p/word2vec/, Google News trained embeddings from GoogleNews-vectors-negative300.bin.gz.</p><p>25 sent2vec: results shown here tokenized by tweetTokenize.py constrasting dev experiments used wikiTokenize.py, both distributed with sent2vec. LexVec: numbers were converted into words, all punctuation was removed, and text is lowercased; FastText: Since, to our knowledge, the tokenizer and preprocessing used for the pre-trained FastText embeddings is not publicly described. We use the following heuristics to preprocess and tokenize sentences for Fast-Text: numbers are converted into words, text is lowercased, and finally prefixed, suffixed and infixed punctuation is recursively removed from each token that does not match an entry in the model's lexicon; Paragram: Joshua (Matt <ref type="bibr" target="#b55">Post, 2015)</ref> pipeline to pre-process and tokenized English text; C-PHRASE, GloVe, PV-DBOW &amp; SIF: PTB tokenization provided by Stanford CoreNLP  with postprocessing based on dev OOVs; Word2vec: Similar to Fast-inference hyperparameters are used unless noted otherwise. The averaged word embedding baselines compute a sentence embedding by averaging word embeddings and then using cosine to compute pairwise sentence similarity scores.</p><p>While state-of-the-art baselines for obtaining sentence embeddings perform reasonably well on the benchmark data, improved performance is obtained by top 2017 STS shared task systems. There is still substantial room for further improvement. To follow the current state-of-the-art, visit the leaderboard on the STS wiki. <ref type="bibr">26</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have presented the results of the 2017 STS shared task. This year's shared task differed substantially from previous iterations of STS in that the primary emphasis of the task shifted from English to multilingual and cross-lingual STS involving four different languages: Arabic, Spanish, English and Turkish. Even with this substantial change relative to prior evaluations, the shared task obtained strong participation. 31 teams produced 84 system submissions with 17 teams producing a total of 44 system submissions that processed pairs in all of the STS 2017 languages. For languages that were part of prior STS evaluations Text, to our knownledge, the preprocessing for the pre-trained Word2vec embeddings is not publicly described. We use the following heuristics for the Word2vec experiment: All numbers longer than a single digit are converted into a '#' (e.g., 24 → ##) then prefixed, suffixed and infixed punctuation is recursively removed from each token that does not match an entry in the model's lexicon.</p><p>26 http://ixa2.si.ehu.es/stswiki/index. php/STSbenchmark (e.g., English and Spanish), state-of-the-art systems are able to achieve strong correlations with human judgment. However, we obtain weaker correlations from participating systems for Arabic, Arabic-English and Turkish-English. This suggests further research is necessary in order to develop robust models that can both be readily applied to new languages and perform well even when less supervised training data is available. To provide a standard benchmark for English STS, we present the STS Benchmark, a careful selection of the English data sets from previous STS tasks <ref type="bibr">(2012)</ref><ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref><ref type="bibr">(2016)</ref><ref type="bibr">(2017)</ref>. To assist in interpreting the results from new models, a number of competitive baselines and select participant systems are evaluated on the benchmark data. Ongoing improvements to the current state-of-the-art is available from an online leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model vs. human similarity scores for top systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Similarity scores with explanations and English examples from</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>STS 2017 evaluation data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Year</cell><cell>Data set</cell><cell>Pairs Source</cell></row><row><cell></cell><cell>MSRpar</cell><cell>1500 newswire</cell></row><row><cell></cell><cell>MSRvid</cell><cell>1500 videos</cell></row><row><cell></cell><cell>OnWN</cell><cell>750 glosses</cell></row><row><cell></cell><cell>SMTnews</cell><cell>750 WMT eval.</cell></row><row><cell></cell><cell>SMTeuroparl</cell><cell>750 WMT eval.</cell></row><row><cell></cell><cell>HDL</cell><cell>750 newswire</cell></row><row><cell></cell><cell>FNWN</cell><cell>189 glosses</cell></row><row><cell></cell><cell>OnWN</cell><cell>561 glosses</cell></row><row><cell></cell><cell>SMT</cell><cell>750 MT eval.</cell></row><row><cell></cell><cell>HDL</cell><cell>750 newswire headlines</cell></row><row><cell></cell><cell>OnWN</cell><cell>750 glosses</cell></row><row><cell></cell><cell>Deft-forum</cell><cell>450 forum posts</cell></row><row><cell></cell><cell>Deft-news</cell><cell>300 news summary</cell></row><row><cell></cell><cell>Images</cell><cell>750 image descriptions</cell></row><row><cell></cell><cell>Tweet-news</cell><cell>750 tweet-news pairs</cell></row><row><cell></cell><cell>HDL</cell><cell>750 newswire headlines</cell></row><row><cell></cell><cell>Images</cell><cell>750 image descriptions</cell></row><row><cell></cell><cell>Ans.-student</cell><cell>750 student answers</cell></row><row><cell></cell><cell>Ans.-forum</cell><cell>375 Q&amp;A forum answers</cell></row><row><cell></cell><cell>Belief</cell><cell>375 committed belief</cell></row><row><cell></cell><cell>HDL</cell><cell>249 newswire headlines</cell></row><row><cell></cell><cell>Plagiarism</cell><cell>230 short-answer plag.</cell></row><row><cell></cell><cell>post-editing</cell><cell>244 MT postedits</cell></row><row><cell></cell><cell>Ans.-Ans.</cell><cell>254 Q&amp;A forum answers</cell></row><row><cell></cell><cell>Quest.-Quest.</cell><cell>209 Q&amp;A forum questions</cell></row><row><cell></cell><cell>Trial</cell><cell>Mixed STS 2016</cell></row></table><note>Spanish; 10  Table 5 Spanish-English; Table 6 Arabic; and Table 7 Arabic-English. Arabic-English parallel data is supplied by translating English training data, Table 8.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>English training data.</figDesc><table><row><cell>Year</cell><cell cols="2">Data set Pairs Source</cell></row><row><cell cols="2">2014 Trial</cell><cell>56</cell></row><row><cell cols="2">2014 Wiki</cell><cell>324 Spanish Wikipedia</cell></row><row><cell cols="2">2014 News</cell><cell>480 Newswire</cell></row><row><cell cols="2">2015 Wiki</cell><cell>251 Spanish Wikipedia</cell></row><row><cell cols="2">2015 News</cell><cell>500 Sewswire</cell></row><row><cell cols="2">2017 Trial</cell><cell>23 Mixed STS 2016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Spanish training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Spanish-English training data.</figDesc><table><row><cell>Year</cell><cell>Data set</cell><cell>Pairs Source</cell></row><row><cell cols="2">2017 Trial</cell><cell>23 Mixed STS 2016</cell></row><row><cell cols="2">2017 MSRpar</cell><cell>510 newswire</cell></row><row><cell cols="2">2017 MSRvid</cell><cell>368 videos</cell></row><row><cell cols="2">2017 SMTeuroparl</cell><cell>203 WMT eval.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Arabic training data.</figDesc><table /><note>ity estimation data has sentences that are much longer. The track 5 English data has an average sentence length of 8.7 words, while the English sentences from track 4b have an average length of 19.4. The English training data has the following average lengths: 2012 10.8 words; 2013 8.8 words (excludes restricted SMT data); 2014 9.1 words; 2015 11.5 words; 2016 13.8 words.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Arabic-English training data.    </figDesc><table><row><cell>Year</cell><cell>Data set</cell><cell>Pairs Source</cell></row><row><cell></cell><cell>MSRpar</cell><cell>1039 newswire</cell></row><row><cell></cell><cell>MSRvid</cell><cell>749 videos</cell></row><row><cell cols="2">2017 SMTeuroparl</cell><cell>422 WMT eval.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Arabic-English parallel data.</figDesc><table><row><cell cols="2">6.2 Evaluation Metric</cell><cell></cell></row><row><cell cols="3">Systems are evaluated on each track by their Pear-</cell></row><row><cell cols="3">son correlation with gold labels. The overall rank-</cell></row><row><cell cols="3">ing averages the correlations across tracks 1-5 with</cell></row><row><cell cols="3">tracks 4a and 4b individually contributing.</cell></row><row><cell>Track</cell><cell>Language(s)</cell><cell>Participants</cell></row><row><cell>1</cell><cell>Arabic</cell><cell>49</cell></row><row><cell>2</cell><cell>Arabic-English</cell><cell>45</cell></row><row><cell>3</cell><cell>Spanish</cell><cell>48</cell></row><row><cell>4a</cell><cell>Spanish-English</cell><cell>53</cell></row><row><cell>4b</cell><cell>Spanish-English MT</cell><cell>53</cell></row><row><cell>5</cell><cell>English</cell><cell>77</cell></row><row><cell>6</cell><cell>Turkish-English</cell><cell>48</cell></row><row><cell cols="2">Primary All except Turkish</cell><cell>44</cell></row><row><cell cols="3">Table 9: Participation by shared task track.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>LIM-LIG (Nagoudi et al., 2017)  Using only weighted word embeddings, LIM-LIG took second place on Arabic.17  Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems.</figDesc><table><row><cell>Genre</cell><cell>Train</cell><cell>Dev</cell><cell>Test Total</cell></row><row><cell>news</cell><cell>3299</cell><cell>500</cell><cell>500 4299</cell></row><row><cell>caption</cell><cell></cell><cell>625</cell><cell>525 3250</cell></row><row><cell>forum</cell><cell>450</cell><cell>375</cell><cell>254</cell></row><row><cell>total</cell><cell cols="3">5749 1500 8628</cell></row><row><cell></cell><cell></cell><cell></cell><cell>'s improvements to</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sultan et al. (2015)'s method.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DT Team (Maharjan et al., 2017) Second place</cell></row><row><cell></cell><cell></cell><cell></cell><cell>on English (track 5) 18 is DT Team using feature en-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: STS Benchmark annotated examples</cell></row><row><cell>by genres (rows) and by train, dev. test splits</cell></row><row><cell>(columns).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Difficult English sentence pairs (Track 5) and scores assigned by top performing systems.19    </figDesc><table><row><cell>Genre</cell><cell>File</cell><cell cols="4">Yr. Train Dev Test</cell></row><row><cell>news</cell><cell>MSRpar</cell><cell>12</cell><cell>1000</cell><cell>250</cell><cell>250</cell></row><row><cell>news</cell><cell>headlines</cell><cell>13/6</cell><cell>1999</cell><cell>250</cell><cell>250</cell></row><row><cell>news</cell><cell>deft-news</cell><cell>14</cell><cell>300</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">captions MSRvid</cell><cell>12</cell><cell>1000</cell><cell>250</cell><cell>250</cell></row><row><cell cols="2">captions images</cell><cell>14/5</cell><cell>1000</cell><cell>250</cell><cell>250</cell></row><row><cell cols="2">captions track5.en-en</cell><cell>17</cell><cell>0</cell><cell>125</cell><cell>125</cell></row><row><cell>forum</cell><cell>deft-forum</cell><cell>14</cell><cell>450</cell><cell>0</cell><cell>0</cell></row><row><cell>forum</cell><cell>ans-forums</cell><cell>15</cell><cell>0</cell><cell>375</cell><cell>0</cell></row><row><cell>forum</cell><cell>ans-ans</cell><cell>16</cell><cell>0</cell><cell>0</cell><cell>254</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell>: STS Benchmark detailed break-down by</cell></row><row><cell>files and years.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Wikipedia trained word frequencies enwiki vocab min200.txt, https://github.com/alexandres/lexvec embeddings from lexvec.commoncrawl.300d.W+C.pos.vectors, first 15 principle components removed, α = 0.001, dev</figDesc><table><row><cell></cell><cell cols="2">STS 2017 Participants on STS Benchmark</cell></row><row><cell>Name</cell><cell>Description</cell><cell></cell><cell>Dev Test</cell></row><row><cell>ECNU</cell><cell>Ensemble (Tian et al., 2017)</cell><cell></cell><cell>84.7 81.0</cell></row><row><cell>BIT</cell><cell>WordNet+Embeddings (Wu et al., 2017)</cell><cell></cell><cell>82.9 80.9</cell></row><row><cell>DT TEAM</cell><cell>Ensemble (Maharjan et al., 2017)</cell><cell></cell><cell>83.0 79.2</cell></row><row><cell>HCTI</cell><cell>CNN (Shao, 2017)</cell><cell></cell><cell>83.4 78.4</cell></row><row><cell cols="2">SEF@UHH Doc2Vec (Duma and Menzel, 2017)</cell><cell></cell><cell>61.6 59.2</cell></row><row><cell></cell><cell>Sentence Level Baselines</cell><cell></cell></row><row><cell>sent2vec</cell><cell cols="3">Sentence spanning CBOW with words &amp; bigrams (Pagliardini et al., 2017)</cell><cell>78.7 75.5</cell></row><row><cell>SIF</cell><cell cols="3">Word embedding weighting &amp; principle component removal (Arora et al., 2017)</cell><cell>80.1 72.0</cell></row><row><cell>InferSent</cell><cell cols="3">Sentence embedding from bi-directional LSTM trained on SNLI (Conneau et al., 2017) 80.1 75.8</cell></row><row><cell cols="3">C-PHRASE Prediction of syntactic constituent context words (Pham et al., 2015)</cell><cell>74.3 63.9</cell></row><row><cell>PV-DBOW</cell><cell cols="3">Paragraph vectors, Doc2Vec DBOW (Le and Mikolov, 2014; Lau and Baldwin, 2016)</cell><cell>72.2 64.9</cell></row><row><cell></cell><cell cols="2">Averaged Word Embedding Baselines</cell></row><row><cell>LexVec</cell><cell cols="2">Weighted matrix factorization of PPMI (Salle et al., 2016a,b)</cell><cell>68.9 55.8</cell></row><row><cell>FastText</cell><cell cols="2">Skip-gram with sub-word character n-grams (Joulin et al., 2016)</cell><cell>65.3 53.6</cell></row><row><cell>Paragram</cell><cell cols="3">Paraphrase Database (PPDB) fit word embeddings (Wieting et al., 2015)</cell><cell>63.0 50.1</cell></row><row><cell>GloVe</cell><cell cols="2">Word co-occurrence count fit embeddings (Pennington et al., 2014)</cell><cell>52.4 40.6</cell></row><row><cell>Word2vec</cell><cell cols="3">Skip-gram prediction of words in a context window (Mikolov et al., 2013a,b)</cell><cell>70.0 56.5</cell></row><row><cell></cell><cell cols="2">24 sent2vec:</cell><cell>https://github.com/epfml/</cell></row><row><cell></cell><cell cols="3">sent2vec, trained model sent2vec twitter unigrams;</cell></row><row><cell></cell><cell>SIF:</cell><cell cols="2">https://github.com/epfml/sent2vec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>STS Benchmark. Pearson's r × 100 results for select participants and baseline models. Wikipedia trained embeddings from wiki.en.vec; Paragram: http://ttic.uchicago.edu/˜wieting/, embeddings trained on PPDB and tuned to WS353 from Paragram-WS353; GloVe: https://nlp.stanford. edu/projects/glove/, Wikipedia and Gigaword trained 300 dim.</figDesc><table><row><cell cols="3">uses the model from Lau and Baldwin (2016)</cell></row><row><cell cols="3">and InferSent which was reported independently.</cell></row><row><cell cols="3">When multiple pre-trained models are available for</cell></row><row><cell cols="3">a method, we report results for the one with the</cell></row><row><cell cols="3">best dev set performance. For each method, input</cell></row><row><cell cols="3">sentences are preprocessed to closely match the</cell></row><row><cell cols="3">tokenization of the pre-trained models. 25 Default</cell></row><row><cell cols="3">experiments varied α, principle components removed and</cell></row><row><cell cols="3">whether GloVe, LexVec, or Word2Vec word embeddings</cell></row><row><cell cols="3">were used; C-PHRASE: http://clic.cimec.unitn.</cell></row><row><cell cols="2">it/composes/cphrase-vectors.html;</cell><cell>PV-</cell></row><row><cell>DBOW:</cell><cell cols="2">https://github.com/jhlau/doc2vec,</cell></row><row><cell cols="3">A P -N E W S trained apnews dbow.tgz; LexVec: https:</cell></row><row><cell cols="3">//github.com/alexandres/lexvec, embedddings</cell></row><row><cell cols="2">lexvec.commoncrawl.300d.W.pos.vectors.gz;</cell><cell>FastText:</cell></row><row><cell cols="3">https://github.com/facebookresearch/</cell></row><row><cell cols="3">fastText/blob/master/pretrained-vectors.</cell></row><row><cell>md,</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Previous years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs.4  HTER is the minimal number of edits required for correction of a translation divided by its length after correction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Produced by the Google Translate API. 6 http://www.sdl.com/languagecloud/ managed-translation/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use 50-dimensional GloVe word embeddings<ref type="bibr" target="#b66">(Pennington et al., 2014)</ref> trained on a combination of Gigaword 5<ref type="bibr" target="#b65">(Parker et al., 2011)</ref> and English Wikipedia available at http://nlp.stanford.edu/projects/glove/.8 https://www.mturk.com/ 9 A designation that statistically identifies workers who perform high quality work across a diverse set of tasks.10 Spanish data from 2015 and 2014 uses a 5 point scale that collapses STS labels 4 and 3, removing the distinction between unimportant and important details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">e.g.,<ref type="bibr" target="#b68">Reimers et al. (2016)</ref> report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus.15  Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">The approach is similar to SIF<ref type="bibr" target="#b6">(Arora et al., 2017)</ref> but without removal of the common principle component18  RTV took first place on track 5, English, but submitted no system description paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">ECNU, BIT and LIM-LIG are scaled to the range 0-5. 20 snew = 5 × s−min(s) max(s)−min(s) is used to rescale scores.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">The evaluation data for track 4b does in fact have STS scores that are ≥ 1 for all pairs. In the 1,000 sentence training set for this track, one sentence that received a score of zero.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Alexis Conneau for the evaluation of InferSent on the STS Benchmark. This material is based in part upon work supported by QNRF-NPRP 6 -1020-1-199 OPTDIAC that funded Arabic translation, and by a grant from the Spanish MINECO (projects TUNER TIN2015-65308-C5-1-R and MUSTER PCIN-2015-226 cofunded by EU FEDER) that funded STS label annotation and by the QT21 EU project (H2020 No. 645452) that funded STS labels and data preparation for machine translation pairs. Iñigo Lopez-Gazpio is supported by the Spanish MECD. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of QNRF-NPRP, Spanish MINECO, QT21 EU, or the Spanish MECD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S15-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SemEval-2016</title>
				<meeting>the SemEval-2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-1051" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012/SemEval</title>
				<meeting>*SEM 2012/SemEval</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic Textual Similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S13-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2013</title>
				<meeting>*SEM 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UdL at SemEval-2017 Task 1: Semantic textual similarity estimation of english sentence pairs using regression model over pairwise features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucie</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamel</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zighed</forename><surname>Abdelkader</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIPN-IIMAS at SemEval-2017 Task 1: Subword embeddings, attention recurrent neural networks and cross word alignment for semantic textual similarity</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan Vladimir Meza</forename><surname>Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet Project</title>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P98/P98-1013.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING &apos;98</title>
				<meeting>COLING &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ukp: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-1059" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012/SemEval</title>
				<meeting>*SEM 2012/SemEval</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UMDeep at SemEval-2017 Task 1: End-to-end shared weight LSTM model for semantic textual similarity</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Peskov</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SICK through the SemEval glasses. lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-015-9332-5</idno>
		<ptr target="https://doi.org/10.1007/s10579-015-9332-5" />
	</analytic>
	<monogr>
		<title level="j">Lang Resour Eval</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="124" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MultiVec: a multilingual and multilevel representation learning toolkit for NLP</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2016/pdf/666Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2016</title>
				<meeting>LREC 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RTM at SemEval-2017 Task 1: Referential translation machines for predicting semantic similarity</title>
		<author>
			<persName><forename type="first">Ergun</forename><surname>Biçici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ResSim at SemEval-2017 Task 1: Multilingual word representations for semantic textual similarity</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robertöstling</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W14/W14-3302.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT 2014</title>
				<meeting>WMT 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Findings of the 2013 Workshop on Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT 2013</title>
				<meeting>WMT 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D15/D15-1075.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
				<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UWB at SemEval-2016 Task 1: Semantic textual similarity using lexical, syntactic, and semantic information</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Brychcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Svoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree LSTM for natural language inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="CoRRabs/1609.06038.http://arxiv.org/abs/1609.06038" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning bidirectional intent embeddings by convolutional deep structured semantic models for spoken language understanding</title>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/learning-bidirectional-intent-embeddings-by-convolutional-deep-structured-semantic-models-for-spoken-language-understanding/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS-SLU</title>
				<meeting>NIPS-SLU</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="CoRRabs/1705.02364.http://arxiv.org/abs/1705.02364" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Loïc Barrault, and Antoine Bordes</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rational, evaluation and approaches</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324909990234</idno>
		<ptr target="https://doi.org/10.1017/S1351324909990234" />
	</analytic>
	<monogr>
		<title level="j">J. Nat. Language Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="105" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">cocor: A comprehensive solution for the statistical comparison of correlations</title>
		<author>
			<persName><forename type="first">Birk</forename><surname>Diedenhofen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Musch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1371/journal.pone.0121945</idno>
		<ptr target="http://dx.doi.org/10.1371/journal.pone.0121945" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C/C04/C04-1051.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 04</title>
				<meeting>COLING 04</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SEF@UHH at SemEval-2017 Task 1: Unsupervised knowledge-free semantic textual similarity via paragraph vector</title>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Mirela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Duma</surname></persName>
		</author>
		<author>
			<persName><surname>Menzel</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lump at SemEval-2017 Task 1: Towards an interlingua semantic similarity</title>
		<author>
			<persName><forename type="first">Cristina</forename><forename type="middle">España</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CompiLIG at SemEval-2017 Task 1: Cross-language plagiarism detection methods for semantic textual similarity</title>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Agnès</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">L2f/inesc-id at semeval-2017 tasks 1 and 2: Lexical and semantic features in word and textual similarity</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Fialho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><forename type="middle">Patinho</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
				<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FCICU at SemEval-2017 Task 1: Sense-based language independent semantic textual similarity approach</title>
		<author>
			<persName><forename type="first">Basma</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reem</forename><surname>Bahgat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Farag</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
				<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UMD-TTIC-UW at SemEval-2016 Task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.anthology.aclweb.org/S/S16/S16-1170.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MITRE at SemEval-2017 Task 1: Simple semantic similarity</title>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Merkhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Strickhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
				<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N/N06/N06-2015.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT</title>
				<meeting>NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
				<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Soft cardinality: A parameterized similarity function for text comparison</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-1061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012/Se-mEval 2012</title>
				<meeting>*SEM 2012/Se-mEval 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Soft Cardinality: A parameterized similarity function for text comparison</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/S/S12/S12-1061.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012/Se-mEval 2012</title>
				<meeting>*SEM 2012/Se-mEval 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Siamese cbow: Optimizing word embeddings for sentence representations</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="CoRRabs/1506.06726.http://arxiv.org/abs/1506.06726" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Skip-thought vectors</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">STS-UHH at SemEval-2017 Task 1: Scoring semantic textual similarity using supervised and unsupervised ensemble</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Kohail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><forename type="middle">Rekaby</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An empirical evaluation of doc2vec with practical insights into document embedding generation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Jey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-1609.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Representation Learning for NLP</title>
				<meeting>ACL Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1405.4053</idno>
		<ptr target="http://arxiv.org/abs/1405.4053" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PurdueNLP at SemEval-2017 Task 1: Predicting semantic textual similarity with paraphrase and event embeddings</title>
		<author>
			<persName><forename type="first">I-Ta</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahak</forename><surname>Goindani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><forename type="middle">Marie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Leonor</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ITNLP-AiKF at SemEval-2017 Task 1: Rich features based svr for semantic textual similarity computing</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dt team at semeval-2017 task 1: Semantic similarity using alignments, sentence-level embeddings and gaussian mixture model output</title>
		<author>
			<persName><forename type="first">Nabin</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Banjade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipesh</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lasang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Tamang</surname></persName>
		</author>
		<author>
			<persName><surname>Rus</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014 Demonstrations</title>
				<meeting>ACL 2014 Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2014/pdf/363Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 14</title>
				<meeting>LREC 14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joshua 6: A phrase-based and hierarchical statistical machine translation</title>
		<author>
			<persName><forename type="first">Yuan Cao Gaurav Kumar Matt</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://ufal.mff.cuni.cz/pbml/104/art-post-cao-kumar.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">516</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">QLUT at SemEval-2017 Task 1: Semantic textual similarity based on word embeddings</title>
		<author>
			<persName><forename type="first">Fanqing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuteng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuwang</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2013</title>
				<meeting>NIPS 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1007/1187184232</idno>
		<ptr target="http://dx.doi.org/10.1007/1187184232" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML&apos;06</title>
				<meeting>ECML&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Representing sentences as low-rank subspaces</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
		<idno>abs/1704.05358</idno>
		<ptr target="http://arxiv.org/abs/1704.05358" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">LIM-LIG at SemEval-2017 Task1: Enhancing the semantic similarity for arabic sentences with vectors weighting</title>
		<author>
			<persName><forename type="first">Billah</forename><surname>El Moatez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémy</forename><surname>Nagoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName><surname>Schwab</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">BabelNet: Building a very large multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1703.02507.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Gigaword Fifth Edition LDC2011T07. Linguistic Data Consortium</title>
				<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model</title>
		<author>
			<persName><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1094" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
				<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Pakulska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystyna</forename><surname>Chodorowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Andruszkiewicz</surname></persName>
		</author>
		<title level="m">Samsung Poland NLP Team at SemEval-2016</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Task 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensemble methods to measure semantic similarity</title>
		<ptr target="http://www.aclweb.org/anthology/S16-1091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2016</title>
				<meeting>SemEval-2016</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Enhancing the lexvec distributed word representation model using positional contexts and external memory</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<ptr target="CoRRabs/1606.01283.http://arxiv.org/abs/1606.01283" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Matrix factorization using window sampling and negative sampling for improved word representations</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P16-2068" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DBnary: Wiktionary as a lemonbased multilingual lexical resource in RDF</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Serasset</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-140147</idno>
		<ptr target="https://doi.org/10.3233/SW-140147" />
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal (special issue on Multilingual Linked Open Data)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="355" to="361" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">HCTI at SemEval-2017 Task 1: Use convolutional neural network to evaluate semantic textual similarity</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shao</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM &apos;14</title>
				<meeting>CIKM &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<ptr target="http://mt-archive.info/AMTA-2006-Snover.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
				<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">OPI-JSA at SemEval-2017 Task 1: Application of ensemble learning for computing semantic textual similarity</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Martynaśpiewak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sobecki</surname></persName>
		</author>
		<author>
			<persName><surname>Karaś</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">DLS@CU: Sentence similarity from word alignment and semantic vector composition</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Md Arafat Sultan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Sumner</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/S/S15/S15-2027.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2017 Task 1: Leverage kernelbased traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Takelab: Systems for measuring semantic text similarity</title>
		<author>
			<persName><forename type="first">Goran</forename><surname>Franešarić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaň</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojana Dalbelo</forename><surname>Snajder</surname></persName>
		</author>
		<author>
			<persName><surname>Bašić</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012/SemEval</title>
				<meeting>*SEM 2012/SemEval</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">TakeLab: Systems for measuring semantic text similarity</title>
		<author>
			<persName><forename type="first">Goran</forename><surname>Franešarić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janšnajder</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojana Dalbelo</forename><surname>Bašić</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S12-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q/Q15/Q15-1025.pdf" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Revisiting recurrent networks for paraphrastic sentence embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1705.00364</idno>
		<ptr target="http://arxiv.org/abs/1705.00364" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">BIT at SemEval-2017 Task 1: Using semantic information space to evaluate semantic textual similarity</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Su</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT)</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S15-2001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q14-1006" />
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Neobility at SemEval-2017 Task 1: An attention-based sentence similarity model</title>
		<author>
			<persName><forename type="first">Wenli</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S17-2023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings SemEval-2017</title>
				<meeting>SemEval-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
