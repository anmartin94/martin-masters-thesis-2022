<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2015 Task 15: A Corpus Pattern Analysis Dictionary-Entry-Building Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vít</forename><surname>Baisa</surname></persName>
							<email>xbaisa@fi.muni.cz</email>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Bradbury</surname></persName>
							<email>j.bradbury3@wlv.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
							<email>cinkova@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName><forename type="first">Ismaïl</forename><surname>El Maarouf</surname></persName>
							<email>i.el-maarouf@wlv.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Octavian</forename><surname>Popescu</surname></persName>
							<email>o.popescu@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wolverhampton</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Wolverhampton</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2015 Task 15: A Corpus Pattern Analysis Dictionary-Entry-Building Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis. CPA is a corpus-driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used. Task 15 draws on the Pattern Dictionary of English Verbs (www.pdev.org.uk), for the targeted lexical entries, and on the British National Corpus for the input text.</p><p>Dictionary entry building is split into three subtasks which all start from the same concordance sample: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically. Subtask 1 attracted 3 teams, though none could beat the baseline (rule-based system). Subtask 2 attracted 2 teams, one of which beat the baseline (majority-class classifier). Subtask 3 did not attract any participant.</p><p>The task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences, and which is freely accessible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is a central vision of NLP to represent the meanings of texts in a formalised way, amenable to automated reasoning. Since its birth, SEMEVAL (or SENSEVAL as it was then; <ref type="bibr" target="#b20">(Kilgarriff and Palmer, 2000)</ref>) has been part of the programme of enriching NLP analyses of text so they get ever closer to a 'meaning representation'. In relation to lexical information, this meant finding a lexical resource which</p><p>• identified the different meanings of words in a way that made high-quality disambiguation possible, • represented those meanings in ways that were useful for the next steps of building meaning representations.</p><p>Most lexical resources explored to date have had only limited success, on either front. The most obvious candidates-published dictionaries and WordNets-look like they might support the first task, but are very limited in what they offer to the second.</p><p>FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as <ref type="bibr" target="#b13">(Gildea and Jurafsky, 2002)</ref> looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction <ref type="bibr" target="#b2">(Baker et al., 2007)</ref> and in 2010, one on Linking Events and Their Participants <ref type="bibr" target="#b29">(Ruppenhofer et al., 2010)</ref>.</p><p>While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target.</p><p>• It is organised around frames, rather than words, so inevitably its priority is to give a co-herent account of the different verb senses in a frame, rather than the different senses of an individual verb. This will tend to make it less good for supporting disambiguation.</p><p>• Frames are not 'data-driven': they are the work of a theorist (Fillmore) doing his best to make sense of the data for a set of verbs. The prospects of data-driven frame discovery are, correspondingly, slim.</p><p>• While FrameNet has worked hard at being systematic in its use of corpus data, FrameNetters looked only for examples showing the verb being used in the relevant sense. From the point of view of a process that could possibly be automated, this is problematic.</p><p>An approach which bears many similarities to FrameNet, but which starts from the verb rather than the frame, and is more thoroughgoing in its empiricism, is Hanks's Corpus Pattern Analysis <ref type="bibr" target="#b14">(Hanks and Pustejovsky, 2005;</ref><ref type="bibr" target="#b15">Hanks, 2012;</ref><ref type="bibr" target="#b16">Hanks, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus Pattern Analysis</head><p>Corpus Pattern Analysis (CPA) is a new technique of language analysis, which produces the main patterns of use of words in text. Figure <ref type="figure">1</ref> is a sample lexical entry from the main output of CPA, the Pattern Dictionary of English Verbs 1 (PDEV). This tells us that, for the verb abolish, three patterns were found. For each pattern it tells us the percentage of the data that it accounted for, its grammatical structure and the semantic type (drawn from a shallow ontology of 225 semantic types 2 ) of each of the arguments in this structure. For instance, pattern 1 means: i) that the subject is preferably a word referring to <ref type="bibr">[[Human]</ref>] or [[Institution]] (semantic alternation), and ii) that the object is preferably</p><formula xml:id="formula_0">[[Action]], [[Rule]] or [[Privilege]].</formula><p>It also tells us the implicature (which is similar to a "definition" in a traditional dictionary) of a sentence exemplifying the pattern: that is, if we have a sentence of the pattern</p><formula xml:id="formula_1">[[Institution | Hu- man]] abolish [[Action=Punishment | Rule | Privilege]], then we know that [[Institution | Human]] formally declares that [[Action=Punishment | Rule | Privi- lege]</formula><p>] is no longer legal or operative. Abolish has only one sense. For many verbs, there will be multiple senses, each with one or more pattern.</p><p>There are currently full CPA entries for more than 1,000 verbs with a total of over 4,000 patterns. For each verb a random sample of (by default) 250 corpus instances was examined, used to build the lexical entry, and tagged with the senses and patterns they represented. For commoner verbs, more corpus lines were examined. The corpus instances were drawn from the written part of the British National Corpus 3 (BNC).</p><p>PDEV has been studied from different NLP perspectives, all mainly involved with Word Sense Disambiguation and semantic analysis <ref type="bibr" target="#b7">(Cinková et al., 2012a;</ref><ref type="bibr" target="#b17">Holub et al., 2012;</ref><ref type="bibr" target="#b11">El Maarouf and Baisa, 2013;</ref><ref type="bibr" target="#b19">Kawahara et al., ;</ref><ref type="bibr" target="#b26">Popescu, 2013;</ref><ref type="bibr" target="#b27">Pustejovsky et al., 2004;</ref>. For example, <ref type="bibr" target="#b26">(Popescu, 2013)</ref> described experiments in modeling finite state automata on a set of 721 verbs taken from PDEV. The author reports an accuracy of over 70% in pattern disambiguation. <ref type="bibr" target="#b17">(Holub et al., 2012)</ref> trained several statistical classifiers on a modified subset of 30 PDEV entries <ref type="bibr" target="#b9">(Cinková et al., 2012c</ref>) using morpho-syntactic as well as semantic features, and obtained over 80% accuracy. On a smaller set of 20 high frequency verbs <ref type="bibr" target="#b11">(El Maarouf and Baisa, 2013)</ref> reached a similar 0.81 overall F1 score with a supervised SVM classifier based on dependency parsing and named entity recognition features.</p><p>The goal of Task 15 at SemEval 2015 are i) to explore in more depth the mechanics of corpus-based semantic analysis and ii) to provide a high-quality standard dataset as well as baselines for the advancement of semantic processing. Given the complexity and wealth of PDEV, a major issue was to select relevant subtasks and subsets. The task was eventually split into three essential steps in building a CPA lexical entry, that systems could tackle separately:</p><p>1. CPA parsing: all sentences in the dataset to be syntactically and semantically parsed. 2. CPA clustering: all sentences in the dataset to be grouped according to their similarities.  3. CPA lexicography: all verb patterns found in the dataset to be described in terms of their syntactic and semantic properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>In order to encourage participants to design systems which could successfully tackle all three subtasks, all tasks were to be evaluated on the same set of verbs. As opposed to previous experiments on PDEV, it was decided that the set of verbs from the test dataset would be different from the set of verbs given in the training set. This was meant to avoid limiting tasks to supervised approaches and to encourage innovative approaches, maybe using patterns learnt in an unsupervised manner from very large corpora and other resources. This also implied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subtask 1: CPA Parsing</head><p>The CPA parsing subtask focuses on the detection and classification (syntactic and semantic) of the arguments of the verb. The subtask is similar to Semantic Role Labelling <ref type="bibr" target="#b5">(Carreras and Marquez, 2004</ref>) that arguments will be identified in the dependency parsing paradigm <ref type="bibr" target="#b4">(Buchholz and Marsi, 2006)</ref>, using head words instead of phrases.</p><p>The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology.</p><p>In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to <ref type="bibr">[[Rule]</ref>]. The expected output is represented in XML format in Example (2).</p><p>(1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replaced it with inheritance tax.</p><p>(2) In 1981 the Conservative &lt;entity syn='subj' sem='Institution'&gt; government &lt;/entity&gt; &lt;entity syn='v' sem='-'&gt; abolished &lt;/entity&gt; capital transfer &lt;entity synt='obj' sem='Rule'&gt; tax &lt;/entity&gt; capital transfer tax and replaced it with inheritance tax</p><p>The only dependency relations shown are those involving the node verb. Thus, for example, the dependency relation between Conservative and government is not shown. Also only the relations in Table <ref type="table" target="#tab_1">1</ref> are shown. The relation between abolished and replaced is not shown as it is not one of the targeted dependency relations. The input text consisted of individual sentences one word per line with both ID and FORM fields, and in which only the target verb token was pre-tagged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subtask 2: CPA Clustering</head><p>The CPA clustering subtask is similar to a Word Sense Discrimination task in which systems have to  predict which pattern a verb instance belongs to.</p><p>With respect to abolish (Figure <ref type="figure">1</ref>), it would involve identifying all sentences containing the verb abolish which belonged to the same pattern (one of the patterns in Figure <ref type="figure">1</ref>) and tagging them with the same number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subtask 3: CPA Automatic Lexicography</head><p>The CPA automatic lexicography subtask aims to evaluate how systems can approach the design of a lexicographical entry within CPA's framework.</p><p>The input was, as for the other tasks, plain text with node verb identified. The output format was a variant of that shown in Figure <ref type="figure">1</ref>, simplified to a form which would be more tractable by systems while still being a relevant representation from the lexicographical perspective.</p><p>Specifically, contextual roles were discarded and semantic alternations were decomposed into semantic strings 4 so that pattern 1 in Figure <ref type="figure">1</ref> would give rise to six strings (with V for the verb, here abolish):</p><formula xml:id="formula_2">[[Human]] V [[Action]] [[Human]] V [[Rule]] [[Human]] V [[Privilege]] [[Institution]] V [[Action]] [[Institution]] V [[Rule]] [[Institution]] V [[Privilege]]</formula><p>This transformation from the PDEV format as in Figure <ref type="figure">1</ref> was done automatically and checked manually. These strings are different to (and generally more numerous than) the patterns evaluated in subtask 2. The goal of this subtask was to generalize sentence examples for each verb and create a list of possible semantic strings. This subtask was autonomous with respect to other subtasks in that participants did not have to return the set of sen-4 Task Data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Microcheck and Wingspread Datasets</head><p>All subtasks (except the first) include two setups and their associated datasets: the number of patterns for each verb is disclosed in the first dataset but not in the second. This setup was created to see whether it would influence the results.</p><p>The two datasets were also created in the hope that system development would start on the first small and carefully crafted dataset (Microcheck) and only then be tested on a larger and more varied subset of verbs (Wingspread) 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation Process</head><p>Both Microcheck and Wingspread start from data extracted from PDEV and the manually patterntagged BNC. We took only verbs declared as complete and started by the same lexicographer, so that each verb had been checked twice: once by the lexicographer who compiled the entry and once by the editor-in-chief. Some tagging errors may have slipped in but the tagging is generally of high quality <ref type="bibr" target="#b7">(Cinková et al., 2012a;</ref><ref type="bibr" target="#b8">Cinková et al., 2012b)</ref>. Additional checks have been performed on Microcheck, since this was the dataset chosen for subtask 1, for which data had to be created. This section describes the annotation process.</p><p>PDEV contains only one kind of link between a given pattern and a given corpus instance: each verb token found in the sample is tagged with a pattern identifier, and the pattern then specifies syntactic  roles and their semantic types. The job in subtask 1 annotation consists of tagging the arguments of each token in the sample, both syntactically and semantically (see Table <ref type="table" target="#tab_1">1</ref> for tagsets of each layer). The syntactic information was the same as for subtask 3 except that category names were shortened and pairs of categories were merged in two places. <ref type="bibr">6</ref> The annotation was carried out by 4 annotators, with 3 for the training data and 3 for test data, and 2 annotators annotating both training and test data, one of them being an expert PDEV annotator. Annotators could ask for feedback on the task at any moment, and any doubts were cleared by the expert annotator. Each pair of annotators annotated one share of the dataset, and their annotation was double-checked by the expert annotator. The agreement was not very high (e.g. Annotator 2, see Table 2) in some cases so the double-check by the expert annotator was crucial. Table <ref type="table" target="#tab_3">2</ref> reports the agreement in terms of F-score and Cohen's Kappa <ref type="bibr" target="#b10">(Cohen, 1960)</ref> between each annotator and the expert annotator. 7</p><p>6 See http://alt.qcri.org/semeval2015/ task15/index.php?id=appendices <ref type="bibr">7</ref> The expert did not start from scratch, but from other anno-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistics on the Data</head><p>Strict rules were implemented to develop a highquality and consistent dataset:</p><p>1. PDEV patterns discriminate exploited 8 uses of a pattern using a different tag; these were left aside for the CPA task. 2. For the test set, when patterns contained at least one semantic type or grammatical category which was not covered in the training set, they were discarded. 3. Only patterns which contained more than 3 examples were kept in the final dataset.</p><p>Applying these filters led to the Microcheck dataset, containing 28 verbs (train: 21; test: 7), 378 patterns (train: 306; test: 72) with 4,529 annotated sentences (train: 3,249; test: 1,280) and to the Wingspread dataset set containing 93 verbs (train: tators' work. Since his target was the conformity of the tagging with guidelines as well as with CPA's principles, we maintain that the expert would have produced a very similar output had he not started from the product of other annotators, who themselves used the output of a system to speed up their work.</p><p>73; test: 20), 856 patterns (train: 652; test: 204), and 12,440 annotated sentences (train: 10,017; test: 2,423). More detailed figures for the test datasets are provided in Tables <ref type="table" target="#tab_5">3 and 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Metrics</head><p>The final score for all subtasks is the average of Fscores over all verbs (Eq. 1). What varies across subtasks is the way Precision and Recall are defined.</p><formula xml:id="formula_3">F1 verb = 2 × Precision verb × Recall verb Precision verb + Recall verb Score Task = n verb i=1 F1 verb i n verb (1)</formula><p>Subtask 1. Equation 2 illustrates that Precision and Recall are computed on all tags, both syntactic and semantic. To count as correct, tags had to be set on the same token as in the gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision = Correct tags Retrieved tags</head><p>Recall = Correct tags Reference tags</p><p>(2)</p><p>Subtask 2. Clustering is known to be difficult to evaluate. Subtask 2 used the B-cubed definition of Precision and Recall, first used for coreference <ref type="bibr" target="#b1">(Bagga and Baldwin, 1999)</ref> and later extended to cluster evaluation <ref type="bibr" target="#b0">(Amigó et al., 2009)</ref>. Both measures are averages of the precision and recall over all instances. To calculate the precision of each instance we count all correct pairs associated with this instance and divide by the number of actual pairs in the candidate cluster that the instance belongs to. Recall is computed by interchanging Gold and Candidate clusterings (Eq. 3).</p><formula xml:id="formula_4">Precision i = Pairs i in Candidate found in Gold Pairs i in Candidate Recall i = Pairs i in Gold found in Candidate Pairs i in Gold (3)</formula><p>Subtask 3. This task was evaluated as a slot-filling exercise <ref type="bibr" target="#b22">(Makhoul et al., 1999)</ref>, so the scores were computed by taking into account the kinds of errors that systems make over the 9 slots: errors of Insertion, Substitution, Deletion. Equation 4 formulates how Precision and Recall are computed.</p><formula xml:id="formula_5">Precision = Correct Correct + Subst + Ins Recall = Correct Correct + Subst + Del (4)</formula><p>In order not to penalize systems, the best match was computed for each Candidate pattern, and one candidate pattern could match more than one Gold pattern. When a given slot was filled both in the Gold data and the Candidate data, this counted as a "match". When not, it was a Deletion. If a slot was filled in the run but not in the gold, it was counted as an Insertion. When a match (aligned slots) was also a semantic type match, it was Correct (1 point).</p><p>When not, it was a Substitution; the CPA ontology was used to allow for partial matches, allowing hypernyms and hyponyms. For that particular task, the maximum number of Candidate patterns was limited to 150% with respect to the number in the Gold set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The evaluation was split into 2 phases (one week for each): a feedback phase and a validation phase. The reason for this was to allow for the detection of unforeseen issues in the output of participants' systems so as to prepare for any major problem. However, this was not put to use by participants since only one team submitted their output in the first phase which also happened to be their final submission.</p><p>5 teams 9 participated in the task, but none participated in more than one subtask. Subtask 1 attracted 3 teams and subtask 2 attracted 2, while subtask 3 did not receive any submissions. Systems were allowed 3 runs on each subtask and each dataset, and were asked to indicate which would be the official one. The following subsections report in brief on the main features of their systems (for more details see relevant papers in SemEval proceedings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Subtask 1</head><p>All systems for this subtask used syntactic dependencies and named entities as features. Since the  subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP <ref type="bibr">(Feng et al., 2015)</ref> used the Stanford CoreNLP package 10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical classifier.</p><p>CMILLS <ref type="bibr" target="#b23">(Mills and Levow, 2015)</ref> used three models to solve the task: one for argument detection, and the other two for each layer. Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics. CMILLS also reported the use of an external resource, the enTentTen12 <ref type="bibr" target="#b18">(Jakubíček et al., 2013)</ref> corpus available in Sketch Engine <ref type="bibr" target="#b21">(Kilgarriff et al., 2014)</ref>.</p><p>FANTASY approached the subtask in a supervised setting to predict first the syntactic tags, and then the semantic tags. The team used features from the MST parser 11 , as well as Stanford CoreNLP for NE, Wordnet 12 , they also applied word embedding representations to predict the output of each layer.</p><p>The baseline system was a rule-based system taking as input the output of the BLLIP parser <ref type="bibr" target="#b6">(Charniak and Johnson, 2005)</ref>, and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subtask 2</head><p>As opposed to subtask 1, systems in subtask 2 used very few semantic and syntactic resources.</p><p>BOB90 used a supervised approach to tackle the clustering problem. The main features used were preposition analyses.</p><p>DULUTH <ref type="bibr" target="#b24">(Pedersen, 2015)</ref> used an unsupervised approach and focused on lexical similarity (both first and second order representations) based on unigrams and bigrams (see SenseClusters 13 ). The number of clusters was predicted on the basis of the best value for the clustering criterion function. The team also performed some corpus pre-processing, like conversion to lower case and conversion of all numeric values to a string.</p><p>The baseline system clusters everything together, so its score depends on the distribution of patterns: the more a pattern covers all instances of the data (majority class), the higher the baseline score.</p><p>6 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Subtask 1</head><p>As previously noted, subtask 1 provided only one dataset, Microcheck. The results on the test set are described in Table <ref type="table" target="#tab_7">5</ref>: FANTASY is the best system with 0.589 average F1 score, but does not beat the baseline (0.624).</p><p>It is worth noting that, on the same set of verbs, BLCUNLP and FANTASY are almost on a par, but since the former did not submit one verb file, the score gap is more significant. FANTASY is a more precise system while BLCUNLP has higher recall.</p><p>To get a better picture of the results, Table <ref type="table">6</ref> provides the F-scores for the ten most frequent categories in the test set. We can see that FANTASY has the best semantic model since it gets the highest scores on most semantic categories (except for State Of Affairs) and systematically beats the baseline, which assigns a word the most frequent semantic category in the training set. The baseline and BCUNLP however get higher scores on most syntactic relations except on obj, where the difference is low. The gap is much more significant on advprep and acomp, which suggests that FANTASY does not properly handle prepositional complements correctly (and/or causal complements). This could be due to the choice of parser or to model parameters. Overall, it seems that progress can still be made, since systems can benefit from one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Subtask 2</head><p>Subtask 2 was evaluated on both datasets. BOB90 only submitted one run while DULUTH submitted three. The results are displayed on Table <ref type="table" target="#tab_9">7</ref>. For this task, only BOB90 beat the baseline with a higher amplitude on Microcheck (+0.153) than on Wingspread (+0.071). This high score welcomes a more detailed evaluation of the system, since it would seem that, as also found for subtask 1, prepositions play a substantial role in CPA patterns and semantic similarity.</p><p>It can also be observed that overall results are better on Wingspread. This seems to be mainly due to the higher number of verbs with a large majority class in Wingspread (see Table <ref type="table">3</ref>), since the baseline system scores 0.72 on Wingspread, and 0.588 on Microcheck. This shows that when the distribution of patterns is highly skewed, the evaluation of systems is difficult, and tends to underrate potentially useful systems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper introduces a new SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis. Dictionary entry building is split into three subtasks: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically.</p><p>Drawing from the Pattern Dictionary of English Verbs, we have produced a high-quality resource for the advancement of semantic processing: it contains 121 verbs connected to a corpus of 17,000 sentences. This resource will be made freely accessible from the task website for more in depth future research. Task 15 has attracted 5 participants, 3 on subtask 1 and 2 on subtask 2. Subtask 1 proved to be more difficult for participants than expected, since no system beat the baseline. We however show that the submissions possess interesting features that should be put to use in future experiments on the dataset. Subtask 2's baseline was beaten by one of the participants on a large margin, despite the fact that the baseline is very competitive.</p><p>It seems that splitting the task into 3 subtasks has had the benefit of attracting different approaches (supervised and unsupervised) towards the common target of the task, which is to build a dictionary entry. Lexicography is such a complex task that it needs major efforts from the NLP community to support it. We hope that this task will stimulate more research and the development of new approaches to the automatic creation of lexical resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Syntactic tagset used for subtask 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Inter-annotator figures where annotators are compared to the expert (annotator 4) who reviewed all the annotations (Microcheck Task 1).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Statistics on the Microcheck test dataset; abbreviations as for previous table.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Official scores for subtask 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Official scores for subtask 2.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://pdev.org.uk 2 http://pdev.org.uk/#onto</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.natcorp.ox.ac.uk/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See<ref type="bibr" target="#b3">(Bradbury and El Maarouf, 2013)</ref>. tences which matched their candidate patterns, patterns were evaluated independently.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The datasets as well as the systems' outputs will soon be made publicly available on the task website.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">An exploitation corresponds to an anomalous use of a pattern, as in a figurative use.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Unfortunately, teams BOB90 and FANTASY did not submit articles, so it is difficult to analyze their results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://nlp.stanford.edu/software/ corenlp.shtml 11 http://www.seas.upenn.edu/˜strctlrn/ MSTParser/MSTParser.html 12 http://wordnet.princeton.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">http://senseclusters.sourceforge.net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are very grateful for feedback on the task from Ken Litkowski as well as from participants who greatly contributed to the overall quality of the task. We would also like to thank SemEval's organizers for their support. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints</title>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-document event coreference: Annotations, experiments, and observations</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Coreference and its Applications</title>
				<meeting>the Workshop on Coreference and its Applications</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2007 Task 19: Frame Semantic Structure Extraction</title>
		<author>
			<persName><forename type="first">Collin</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
				<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical classification of verbs based on Semantic Types: the case of the &apos;poison&apos; verbs</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismaïl</forename><forename type="middle">El</forename><surname>Maarouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of JSSP2013</title>
				<meeting>JSSP2013<address><addrLine>Trento,Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
				<meeting>CoNLL<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis</forename><surname>Marquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
				<meeting>CoNLL<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coarse-tofine n-best parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
				<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Managing Uncertainty in Semantic Tagging</title>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Kríž</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="840" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing semantic granularity for NLP -report on a lexicographic experiment</title>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Kríž</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th EURALEX International Congress</title>
				<meeting>the 15th EURALEX International Congress<address><addrLine>Oslo, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A database of semantic clusters of verb usages</title>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Rambousek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Smejkalová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)</title>
				<meeting>the 8th International Conference on Language Resources and Evaluation (LREC 2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3176" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic classification of semantic patterns from the Pattern Dictionary of English Verbs</title>
		<author>
			<persName><forename type="first">Ismaïl</forename><surname>El Maarouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vít</forename><surname>Baisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of JSSP2013</title>
				<meeting>JSSP2013<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="95" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disambiguating Verbs by Collocation: Corpus Lexicography meets Natural Language Processing</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Ismaïl El Maarouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vít</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baisa</surname></persName>
		</author>
		<author>
			<persName><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval<address><addrLine>Reykjavik, Iceland; Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1001" to="1006" />
		</imprint>
	</monogr>
	<note>Proceedings of LREC</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic Labeling of Semantic Roles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Pattern Dictionary for Natural Language Processing</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Revue Française de linguistique applique</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How people use words to make meanings: Semantic types meet valencies</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Input, Process and Product: Developments in Teaching and Language Corpora</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Boulton</surname></persName>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</editor>
		<meeting><address><addrLine>Brno</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lexical Analysis: Norms and Exploitations</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tailored Feature Extraction for Lexical Disambiguation of English Verbs Based on Corpus Pattern Analysis</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Kríž</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Bick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
				<meeting>the 24th International Conference on Computational Linguistics<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1195" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vojtěch Kovář, Pavel Rychlý, and Vít Suchomel</title>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Jakubíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Corpus Linguistics</title>
				<meeting>the International Conference on Corpus Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>The TenTen Corpus Family</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inducing Examplebased Semantic Frames from a Massive Amount of Verb Uses</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the Special Issue on SENSEVAL</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pavel Rychlý, and Vít Suchomel</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vít</forename><surname>Baisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Bušta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Jakubíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtěch</forename><surname>Kovář</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sketch Engine: ten years on. Lexicography</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance Measures For Information Extraction</title>
		<author>
			<persName><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Kubala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Broadcast News Workshop</title>
				<meeting>DARPA Broadcast News Workshop</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CMILLS: Adapting SRL Features to Dependency Parsing</title>
		<author>
			<persName><forename type="first">Chad</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Duluth: Word Sense Discrimination in the Service of Lexicography</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
				<meeting>SemEval<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mapping CPA onto OntoNotes Senses</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="882" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning corpus patterns using finite state automata</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics</title>
				<meeting>the 10th International Conference on Computational Semantics<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="191" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated Induction of Sense in Context</title>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constructing a corpus-based ontology using model bias</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FLAIRS</title>
				<meeting>FLAIRS<address><addrLine>Melbourne, FL</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 10: Linking Events and Their Participants in Discourse</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Workshop on Semantic Evaluation</title>
				<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
