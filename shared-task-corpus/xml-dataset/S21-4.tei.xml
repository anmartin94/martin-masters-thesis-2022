<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Yang</surname></persName>
							<email>xiaoyu.yang@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Ping</forename><surname>Ruan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<addrLine>4 iFlytek Research</addrLine>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<addrLine>4 iFlytek Research</addrLine>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<email>quanliu@ustc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<addrLine>4 iFlytek Research</addrLine>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
							<email>siwei@iflytek.com.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<email>xiaodan.zhu@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces the SemEval-2021 shared task 4: Reading Comprehension of Abstract Meaning (ReCAM). This shared task is designed to help evaluate the ability of machines in representing and understanding abstract concepts. Given a passage and the corresponding question, a participating system is expected to choose the correct answer from five candidates of abstract concepts in a cloze-style machine reading comprehension setup.</p><p>Based on two typical definitions of abstractness, i.e., the imperceptibility and nonspecificity, our task provides three subtasks to evaluate the participating models. Specifically, Subtask 1 aims to evaluate how well a system can model concepts that cannot be directly perceived in the physical world. Subtask 2 focuses on models' ability in comprehending nonspecific concepts located high in a hypernym hierarchy given the context of a passage. Subtask 3 aims to provide some insights into models' generalizability over the two types of abstractness. During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29 submissions to Subtask 3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans use words with abstract meaning in their daily life. In the past, research efforts have been exerted to better understand and model abstract meaning <ref type="bibr" target="#b40">(Turney et al., 2011;</ref><ref type="bibr" target="#b38">Theijssen et al., 2011;</ref><ref type="bibr" target="#b2">Changizi, 2008;</ref><ref type="bibr" target="#b37">Spreen and Schulz, 1966)</ref>. Modelling abstract meaning is closely related to many other NLP tasks such as reading comprehension, metaphor modelling, sentiment analysis, summarization, and word sense disambiguation.</p><p>In the past decade, significant advancement has been seen in developing computational models for semantics, based on deep neural networks. In this shared task, we aim to help assess the capability of the state-of-the-art deep learning models on representing and modelling abstract concepts in a specific reading comprehension setup.</p><p>We introduce SemEval-2021 Task 4, Reading Comprehension of Abstract Meaning (ReCAM). Specifically, we design this shared task by following the machine reading comprehension framework <ref type="bibr" target="#b12">(Hermann et al., 2015;</ref><ref type="bibr" target="#b27">Onishi et al., 2016;</ref><ref type="bibr" target="#b13">Hill et al., 2016)</ref>, in which computers are given a passage D i as well as a human summary S i to comprehend. If a model can digest the passage as humans do, we expect it to predict the abstract word used in the summary, if the abstract word is masked. Unlike the previous work that requires computers to predict concrete concepts, e.g., named entities, in our task we ask models to fill in abstract words removed from human summaries. During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 submissions to Subtask 2. The participating teams additionally made 29 submissions to Subtask 3. In this paper, we induce the shared task and provide a summary for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>We organize our shared task based on two typical definitions of abstractness, named as imperceptibility and nonspecificity in this paper, implemented in Subtask 1 and Subtask 2, respectively. Subtask 3 further evaluates models' generalizability over the two definitions of abstractness.</p><p>Passage ... Observers have even named it after him, "Abenomics". It is based on three key pillars of monetary policy to ensure long-term sustainable growth in the world's third-largest economy, with fiscal stimulus and structural reforms. In this weekend's upper house elections, .... Question Abenomics: The @placeholder and the risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer</head><p>(A) chance (B) prospective (C) government (D) objective (E) threat Table <ref type="table">1</ref>: An example for Subtask 1. The correct answer to the question is objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subtask 1: ReCAM-Imperceptibility</head><p>In one definition <ref type="bibr" target="#b40">(Turney et al., 2011;</ref><ref type="bibr" target="#b38">Theijssen et al., 2011;</ref><ref type="bibr" target="#b37">Spreen and Schulz, 1966)</ref>, concrete words refer to things, events, and properties that humans can directly perceive with their senses, e.g., trees and flowers. In contrast, abstract words refer to "ideas and concepts that are distant from immediate perception", e.g., objective, culture, and economy. In Subtask 1, we perform reading comprehension on imperceptible abstract concepts, named as ReCAM-ImPerceptibility. Table <ref type="table">1</ref> shows an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subtask 2: ReCAM-NonSpecificity</head><p>The second typical definition of abstractness is based on nonspecific concepts <ref type="bibr" target="#b38">(Theijssen et al., 2011;</ref><ref type="bibr" target="#b37">Spreen and Schulz, 1966)</ref>. Compared to specific concepts such as groundhog and whale, words such as vertebrate are regarded as more abstract.</p><p>Our Subtask 2, named as ReCAM-NonSpecificity, is designed based on this viewpoint. We will discuss how the datasets are constructed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Subtask 3: ReCAM-Cross</head><p>In this subtask, participants are asked to submit their predictions on the test data of Subtask 2, using models trained on the training data of Subtask 1, and vice versa. This subtask aims to demonstrate models' generalizability between modelling the two typical definitions of abstractness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Construction</head><p>We develop our multi-choice machine reading comprehension datasets based on the XSum summarization dataset <ref type="bibr" target="#b25">(Narayan et al., 2018)</ref>. We first locate words with abstract meaning using our abstractness scorers. Then we perform data filtering to select our target words to construct our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The XSum Data</head><p>By collecting online articles from the British Broadcasting Corporation (BBC), <ref type="bibr" target="#b25">Narayan et al. (2018)</ref> developed a large-scale text summarization dataset, XSum, in which each article has a single sentence summary. We developed our ReCAM dataset based on XSum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding Imperceptible Concepts</head><p>Abstractness Scorer for Imperceptibility Following Turney et al. ( <ref type="formula">2011</ref>), we use the MRC Psycholinguistic Database <ref type="bibr">(Coltheart, 1981)</ref>, which includes 4,295 words rated with a degree of abstractness by human subjects, to train our abstractness scorer for imperceptibility. The rating of the words in the MRC Psycholinguistic Database ranges from 158 (highly abstract) to 670 (highly concrete). We linearly scale the rating to the range of 0 (highly abstract) to 1 (highly concrete). The neural regression model accepts fixed Glove embedding <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> as input and predicts the abstractness rating score between 0 and 1. Our regression model is a three-layer network that consists of two nonlinear hidden layers with the ReLU activation and a sigmoid output layer. The mean square error (MSE) is used as the training loss.</p><p>To test the regression model's performance, we randomly split the MRC Psycholinguistic Database into train and test set with the size of 2,148 and 1,877, respectively. Table <ref type="table" target="#tab_1">2</ref> shows the final performance of the neural regression model on the MRC database. We use the Pearson correlation between ratings predicted by models and original ratings from MRC as the evaluation metric. We can see that the regression model achieves high correlation coefficients (the higher, the better), i.e., 0.934 and 0.835, on the training and test set. The correlations are significant (p-values are smaller than 10 −5 ), reflecting the quality of our models in finding abstract words. Note that Turney et al. (2011) report a correlation score of 0.81 on their MRC test set.</p><p>Their training-test split is unavailable, so we run cross-validation here in our experiment. The scorer can then be used to assign an imperceptibility score to a word that is not in the MRC Psycholinguistic Database.</p><p>Using the abstractness scorer described above, we assign an abstractness value to each word in summaries and select words with a value lower than 0.35 as the candidates for our target words (words that will be removed from the summaries  to construct questions). We only consider content words as potential target words, i.e., nouns, verbs, adjectives, and adverbs. For this purpose, we use part-of-speech tagging model (?) implemented in Stanza <ref type="bibr" target="#b31">(Qi et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finding Nonspecific Concepts</head><p>Nonspecificity Scorer Following the work of Changizi ( <ref type="formula">2008</ref>), we assign a nonspecificity score to a word token based on the hypernym hierarchy of WordNet <ref type="bibr" target="#b23">(Miller, 1998)</ref>. Specifically, the root of the hierarchy is at level 0 and regarded as the most abstract. The abstractness of a node in the hierarchy is measured by the maximal length of its path to the root. The hypernym level in WordNet is between 0 and 17. For each word token in summaries, we use Adapted Lesk Algorithm <ref type="bibr" target="#b0">(Banerjee and Pedersen, 2002)</ref> to label the sense since the WordNet hypernym hierarchy works at the sense level. Since a summary sentence may be short, we concatenate each summary sentence with the corresponding passage for word sense disambiguation. Built on this, each token, which is labelled with a sense, receives an abstractness score based on the WordNet hierarchy.</p><p>Using the nonspecificity scorer, we assign an nonspecificity value to each word in summaries and select words with a value smaller than six as the candidate target words. The targets words will be nouns and verbs since the hypernym hierarchy in WordNet (?) consists of these two POS types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Filtering</head><p>We aim to avoid developing simple questions. For example, if a target word also appears in the passage, it is likely that a model can easily find the answer without the need to understand the passage in depth.</p><p>Filtering by Lemmas We lemmatized passages and summaries. If a lemma appears both in a summary and the corresponding passage, the lexemes of the lemma will not be considered as target words. Note that a strict filter may exclude some good candidates for target words but helps avoid introducing many simple questions.</p><p>Filtering by Synonyms and Antonyms For a word in a summary, if a synonym or antonym of the word appears in the corresponding passage, we will not consider this word to be our target word. We use WordNet (?) to derive synonyms and antonyms. Instead of using word sense disambiguation (WSD), for a word w i in a summary, we use all senses of this word and add all synonyms and antonyms into a pool. Only if none of the words in the pool appear in the passage, we consider w i as a candidate target word. Otherwise, we will not use w i to construct a question for this passage-summary pair.</p><p>Filtering by Similarity We further filter words by similarity. For each candidate target word in a summary and each word in the passage, we calculate similarity and use that to perform further filtering.</p><p>We use 300-dimension GloVe word embedding trained on 840 billion tokens <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref>. We calculate the cosine similarity between a candidate target word and a passage word. For contextual embedding, we embed each sentence in a passage as well as the summary into a contextaware representation matrix using the BERT-large uncased language model. Then, we calculate the similarity between each passage token and question token with the cosine similarity. If the similarity is higher than 0.85, we will not consider the involved summary words as candidate target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Constructing Multiple Choices</head><p>We train machine reading comprehension models using the data built so far to generate four choices for each question. Together with the ground-truth (the target word identified above and removed from the human summary), we have five choices/options for each question. In our work, we propose to use three models, Gated-Attention Reader <ref type="bibr" target="#b12">(Hermann et al., 2015)</ref>, Attentive Model and Attention Model with Word Gloss to generate the candidate options. Please find details of the models in Appendix B and Appendix C as well as the training details in Appendix D.</p><p>We adopt the idea of k-fold cross validation to train the above mentioned three models to generate candidate answer words. Specifically, we split the data into 4 folds. Each time, we train the baseline models on 3 folds of data and use the trained MRR R@1 R@5 R@10 GAReader 0.245 0.175 0.314 0.378 AttReader 0.235 0.167 0.300 0.363 +gloss 0.179 0.123 0.227 0.276  models to predict candidate words on the remaining 1-fold data. With 4-fold iteration, we obtain predication of each model on the entire data. The performance of the three baseline models are listed in Table <ref type="table" target="#tab_2">3</ref> for Subtask 1 and Table <ref type="table" target="#tab_3">4</ref> for Subtask 2, using several typical retrieval-based evaluation metrics.</p><p>For each target word that has been removed from the corresponding summary sentence (again, a question is a summary sentence containing a removed target word), we collect top-10 words predicted by each of the three models. In this way, we can collect a candidate word pool of 30 predicted word tokens for each removed target word. To avoid including multiple correct choices for each question, we adopt synonym and context similarity filtering methods described in Section 3.4. Specifically we first calculate similarity between the ground-truth target word and each word type in the pool. We exclude a word type from the multiple choices if its similarity to the ground-truth is higher than 0.85. In addition, we also exclude synonyms of the ground-truth target word. For the remaining word tokens in the pool, we select four most frequent word types (a word type may have multiple tokens in the pool). Together with the ground-truth word, we obtain five choices for each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Further Quality Control</head><p>We further make the following efforts to remove noise in the dataset and improve the datasets' qual-ity. We observe that up to now, there are mainly two kinds of noise in our dataset: 1) some target words cannot be inferred solely based on the corresponding passage; 2) more than one of the multiple choices are correct answers.</p><p>The first issue is mainly related to the property of the XSum dataset, in which the first sentence of a passage is used as the summary. The second type of problems are often caused by our automatic generation method. Although we have applied strict rules in Section 3.4 to handle this, among a small portion of the resulting data, multiple potentially correct answers still exist in candidate answers.</p><p>To further ensure the quality of our dataset, we invite workers in Amazon Mechanical Turk to perform further data selection. Each annotator needs to follow the procedure of Appendix A to answer the question and annotate relevant information, with which further data selection is applied. To ensure quality, we only include workers from Englishspeaking countries and only if their previous HITs' approval rates are above 90%. To see more details about this process, please refer to Appendix E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">ReCAM Data Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Systems and Results</head><p>Our shared task received 23 submissions to Subtask 1, 28 submissions to Subtask 2, and 29 submissions to Subtask 3. We use accuracy as the evaluation metric for the three subtasks.</p><p>In general, most participating teams use pretrained language models in their systems such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b19">(Lan et al., 2020)</ref>, DistilBERT <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019)</ref>, ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>, DeBERTa <ref type="bibr" target="#b10">(He et al., 2020)</ref>, XL-Net <ref type="bibr" target="#b45">(Yang et al., 2019)</ref>, T5 <ref type="bibr" target="#b32">(Raffel et al., 2020)</ref>. Data augmentation, external knowledge resources, and/or transfer learning are additionally used by many teams to further enhance their model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subtask 1: ReCAM-Imperceptibility</head><p>Table <ref type="table" target="#tab_7">6</ref> shows all the official submissions and most of them outperform the baseline model. The baseline used for Subtask 1 is the Gated-Attention (GA) Reader <ref type="bibr" target="#b8">(Dhingra et al., 2017)</ref>. The GA Reader uses a multi-layer iterated architecture with a gated attention mechanism to derive better query-aware passage representation. The motivation behind using GA Reader is to have a simple comparison between our task and the CNN/Daily Mail reading comprehension dataset since GA Reader achieves reasonably good performance on the CNN/Daily Mail reading comprehension dataset.</p><p>Note that the last column of the table lists the accuracy (Acc. Cross) for models trained on the Subtask 2 training data and tested on the Subtask 1 testset. We will discuss those results later in Section 4.3.</p><p>The best result in Subtask 1 was achieved by team SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref> with an accuracy of 0.951.</p><p>The system was built on a pre-trained ELECTRA discriminator and it further applied upper attention and auto-denoising mechanism to process long sequences. The second-placed system, PINGAN omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref>, adopted an ensemble of ELECTRA-based models with task-adaptive pre-training and a mutlihead attention based multiple-choice classifier. ECNU-ICA-1 <ref type="bibr">(Liu et al., 2021)</ref> ranked third in this subtask with a knowledge-enhanced Graph Attention Network and a semantic space transformation strategy.</p><p>Most teams in Subtask 1 utilize pre-trained language models (PLM), like BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b19">(Lan et al., 2020)</ref>, Dis-tilBERT <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019)</ref>, ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>, De-BERTa <ref type="bibr" target="#b10">(He et al., 2020)</ref>, XLNet <ref type="bibr" target="#b45">(Yang et al., 2019)</ref>, T5 <ref type="bibr" target="#b32">(Raffel et al., 2020)</ref>. SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref>   tives of these pre-trained models.</p><p>Most participating systems performed intermediate task pre-training <ref type="bibr">(Pruksachatkun et al., 2020)</ref> for their language models. For example, CNN/Daily Mail dataset was selected by ZJUKLAB <ref type="bibr" target="#b43">(Xie et al., 2021a)</ref> to further pretrain their language models. The CNN/Daily Mail dataset and Newsroom dataset boost model performance on both Subtask 1 and Subtask 2. Data augmentation methods are also popular among participants. ZJUKLAB <ref type="bibr" target="#b43">(Xie et al., 2021a)</ref> performed negative data augmentation with a language model to leverage misleading words. IIE-NLP-Eyas (Xie et al., 2021b) adopted template-based input reconstruction methods to augment their dataset and further fine-tuned their language models based on the dataset.</p><p>Most teams also used an ensemble of multiple pre-trained language models to further enhance model performance. SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref> applied Wrong Answer Ensemble <ref type="bibr" target="#b16">(Kim and Fung, 2020)</ref> by training the model to learn the correct and wrong answer separately and ensembled them to obtain the final predictions. Stochastic Weight Averaging <ref type="bibr" target="#b14">(Izmailov et al., 2018)</ref> was also performed across multiple checkpoints in the same run to achieve better generalization.</p><p>In addition, some interesting approaches were additionally used to tackle the task from different perspectives.</p><p>PINGAN omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref> turned the original multi-choice task into a masked-sentence classification task by adding each option to the placeholder. Noise detection methods and auto denoising methods were further proposed by adding a noise-tolerant loss. ZJUKLAB <ref type="bibr" target="#b43">(Xie et al., 2021a)</ref> used label smoothing to encourage the activations of the penultimate layer. ECNU-ICA-1 <ref type="bibr">(Liu et al., 2021)</ref> utilized a semantic space transformation strategy to convert ordinary semantic representations into abstract representations for classification.</p><p>Many teams used external knowledge resources to further improve model performance. Word-Net <ref type="bibr" target="#b9">(Fellbaum, 1998)</ref> was widely used to provide candidate word definitions. ECNU-ICA-1 <ref type="bibr">(Liu et al., 2021</ref>) also used ConceptNet5 <ref type="bibr" target="#b36">(Speer et al., 2016)</ref> and Graph Neural Network in their systems. To alleviate the noise induced by incorporating structured knowledge through unimportant edges, they propose a noise reduction strategy. owlmx used the MRC Psycholinguistic Database to obtain a measurement of imperceptibility abstractness.</p><p>Different pre-processing techniques were proposed in multiple systems. ZJUKLAB <ref type="bibr" target="#b43">(Xie et al., 2021a</ref>) used a sliding window to limit input length in training. PINGAN Omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref> used the cycle noisy label detection algorithm to make models more robust.</p><p>Much interesting analysis regarding the failure cases and data distribution was discussed in several system description papers. XRJL <ref type="bibr" target="#b15">(Jiang et al., 2021)</ref> found that for a few questions, common  sense knowledge was further needed to help find the answer. They also pointed out that there were still a few questions in which multiple candidate choices may serve as appropriate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subtask 2: ReCAM-Nonspecificity</head><p>In Subtask 2, we received 28 submissions. Table <ref type="table" target="#tab_9">7</ref> shows the official leaderboard. The best result in Subtask 2 was achieved by team PINGAN omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref> with an accuracy of 0.953, using a model similar to the team's model in Subtask 1. The second-placed team SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021</ref>) also adopted the same model it used in Subtask 1 with a data augmentation method based on the hypernym hierarchy in WordNet.</p><p>In general, the participating teams in Subtask 2 used pre-trained language models and neural networks similar to those they used in Subtask 1. The main differences lie in how the participants performed data augmentation and leveraged external knowledge. For example, in addition to SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref>, the IRG team <ref type="bibr">(Sharma et al., 2021</ref>) also performed data augmentation using hypernyms from WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subtask 3: Cross-task Performance</head><p>In this section, we explore models' performance across the two types of definitions of abstractness. Specifically, in this subtask, participants train their models on the training set of one subtask and test on the testset of the other subtask. We received 29 submissions in total from the participants.</p><p>Cross-task performance: Subtask 2-to-1 testing. We asked participants to test their models trained on the Subtask 2 training data on the Subtask 1 test data. The results are shown in the last column of Table <ref type="table" target="#tab_7">6</ref>.</p><p>The results we received show that the performance of all systems drops substantially. For some systems ranking among top 10, the accuracy can decrease by 5 points (IIE-NLP-Eyas <ref type="bibr" target="#b44">(Xie et al., 2021b)</ref> and XRJL <ref type="bibr" target="#b15">(Jiang et al., 2021)</ref>), or even more (14 points for nxc). Some systems show good generalization ability in this Subtask 2-to-1 scenario; the performance of PINGAN-Omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref> is only 1.3 point less, which may be due to the the data augmentation and task adaptive training used in the model. Cross-task Performance: Subtask 1-to-2 Testing. Participants are asked to test their Subtask 1 systems on the Subtask 2 testset. Details of the results can be seen in the last column of Table <ref type="table" target="#tab_9">7</ref>. All systems' performances drop. For example, among the top-10 systems, the accuracy decreases by 5 points (IIE-NLP-Eyas <ref type="bibr" target="#b44">(Xie et al., 2021b</ref>)) or 7 points (tt123).</p><p>However, ECNU-ICA-1 <ref type="bibr">(Liu et al., 2021)</ref> shows a very good generalization ability in Subtask 1-to-2 testing. PINGAN-Omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref>, SRC-B-roc <ref type="bibr" target="#b46">(Zhang et al., 2021)</ref> and XRJL <ref type="bibr" target="#b15">(Jiang et al., 2021)</ref>'s systems are rather consistent in this Subtask 1-to-2 cross testing. Some algorithms they used may explain the models' good generalization ability. ECNU-ICA-1's algorithm of using knowledge-enhanced Graph Attention Network can provide external knowledge to the model. The Wrong Answer Ensemble algorithm <ref type="bibr" target="#b16">(Kim and Fung, 2020)</ref> used in PINGAN-Omini-Sinitic <ref type="bibr" target="#b15">(Wang et al., 2021)</ref> is a relatively simple but an effective way of improving model performance and generalization ability. Also, the Stochastic Weight Averaging algorithm across multiple checkpoints is effective for better generalization. XRJL <ref type="bibr" target="#b15">(Jiang et al., 2021)</ref> retrieves the definitions of candidate answers from WordNet and feeds them to the model as extra inputs. We also think data augmentation methods contribute to the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There have been tasks being proposed to evaluate machines' ability on reading comprehension, which either require models to find an entity or text span from the source document as the answer <ref type="bibr" target="#b12">(Hermann et al., 2015;</ref><ref type="bibr" target="#b13">Hill et al., 2016;</ref><ref type="bibr" target="#b27">Onishi et al., 2016;</ref><ref type="bibr" target="#b33">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b39">Trischler et al., 2017)</ref>, or further generate an answer <ref type="bibr" target="#b26">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b11">He et al., 2018;</ref><ref type="bibr" target="#b18">Kočiskỳ et al., 2018)</ref>. The cloze-style MRC tasks <ref type="bibr" target="#b12">(Hermann et al., 2015;</ref><ref type="bibr" target="#b27">Onishi et al., 2016;</ref><ref type="bibr" target="#b13">Hill et al., 2016)</ref> are most similar to ours, in which the missing words in the cloze questions are entities appearing in source documents. Unlike previous work, ReCAM questions specifically focus on abstract words unseen in the corresponding source documents.</p><p>In general, multi-choice questions have been widely used as a tool for language examination to test both humans and machines. In this paper, we follow the multiple-choice framework for our proposed ReCAM task to evaluate computers' ability in comprehending abstract concepts, in which computers are asked to predict the missing abstract words in human-written summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>This shared task aims to study the ability of machines in representing and understanding abstract concepts, based on two definitions of abstractness, the imperceptibility and nonspecificity, in a specific machine reading comprehension setup. We provide three subtasks to evaluate models' ability in comprehending the two types of abstract meaning as well as their generalizability. In Subtask 1, the top system achieves an accuracy of 0.951, and in Subtask 2, an accuracy of 0.953, suggesting the current systems perform well in the specific setup of our share task. In Subtask 3, we found that in general the models' performances dropped in both Subtask 2-to-1 and Subtask 1-to-2 testing. However, some models generalize well, benefiting from technologies such as data augmentation and task adaptive training. We hope the shared task can help shed some light on modelling abstract concepts and help design more challenging tasks in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Gated-Attention Reader</head><p>The Gated-Attention (GA) Reader <ref type="bibr" target="#b8">(Dhingra et al., 2017)</ref>, the state-of-art model on CNN/Daily Mail reading comprehension dataset <ref type="bibr" target="#b12">(Hermann et al., 2015)</ref>, is adapted here in our experiments. The GA Reader uses a multi-layer iterated architecture with a gated attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader, to derive better query-aware passage representation. To apply GA Reader to our ARC task, we input the news passage p as the document and the processed summary s as the query to GA Reader.</p><p>Specifically, for an input passage p = [p 1 , p 2 , ..., p lp ] with l p words and its corresponding summary s = [s 1 , s 2 , ..., s ls ] with l s words, we first derive their corresponding word embedding sequence P = [p 1 , p 2 , ..., p lp ] and S = [s 1 , s 2 , ..., s ls ] respectively. Then the GA Reader accepts the P and S as inputs and return the hidden states</p><formula xml:id="formula_0">H p = [h p 1 , h p 2 , ..., h p lp ] and H s = [h s 1 , h s 2 , .</formula><p>.., h s ls ] as the sequential representation for passage p and summary s respectively. As for the final prediction process, we do not adopt the operations in <ref type="bibr" target="#b8">Dhingra et al. (2017)</ref> because in ARC the answer words are unseen in the corresponding passage, however, GA Reader in <ref type="bibr" target="#b8">Dhingra et al. (2017)</ref> tries to select a entity word in the passage as the final prediction since their target answer word appears in the passage. So we redesign the part of prediction.</p><p>First, the corresponding representation of "@placeholder" in H s , denoted as h s q (q is the position index of @placeholder in summary s), is used as the final vector representation for summary s. For the final vector representation p for passage p, a bilinear attention between h s q and H p is used for its derivation:</p><formula xml:id="formula_1">e i = h s q T W att h p i , ∀i ∈ [1, ..., l p ] (1) p = lp i=1 exp e i lp j=1 exp e j h p i ,<label>(2)</label></formula><p>We set a token embedding a e t for each candidate abstractive word a t (t ∈ [1, ..., n c ], n c is the size of candidate set). We first concatenate the h s p and p, then use the bilinear product and softmax to predict the probability distribution over all n c candidate abstractive words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Attentive Model</head><p>The word gloss, which defines a word sense meaning, has been mainly used in word sense disambiguation (WSD) task and its variants <ref type="bibr" target="#b20">(Lesk, 1986;</ref><ref type="bibr" target="#b24">Moro et al., 2014)</ref>. Since the goal of ARC is to predict a word that can summarize corresponding information from the source passage, which is an abstracting process, it may be helpful when the gloss, i.e., interpretation of candidate abstractive words, are provided.</p><p>We design an attentive model with word gloss (AMWG) as Figure <ref type="figure">1</ref> shows. Specifically, all the encoders are 1-layer bi-directional recurrent neural networks (RNNs) with Gated Recurrent Units (GRU) <ref type="bibr">(Cho et al.)</ref> ] to the WordGloss Encoder.</p><p>Similar to Section B, the corresponding representation of "@placeholder", i.e., h s q , is used as the final vector representation for summary s. And an bilinear attention f p att (•) is applied to h s q and H p as follows:</p><formula xml:id="formula_2">e i = h s q T W p att h p i , ∀i ∈ [1, ..., l p ]<label>(5)</label></formula><formula xml:id="formula_3">α i = exp e i lp j=1 exp e j , ∀i ∈ [1, ..., l p ]<label>(6)</label></formula><p>Then p is derived as the vector representation for passage p by the weighed sum of H p , which is further concatenated with the h s q to form the final summarization vector v:</p><formula xml:id="formula_4">p = lp i=1 α i h p i ,<label>(7)</label></formula><formula xml:id="formula_5">v = concat(p, h s q ),<label>(8)</label></formula><p>Another attention f g att (•) is applied to v and H gt , </p><p>The following weighted sum of H gt , i.e, a g t , is derive as the final vector representation for the gloss of candidate word a t :</p><formula xml:id="formula_7">a g t = lg t j=1 β j h gt j (21)</formula><p>We also set a token embedding a e t for each candidate word a t (t ∈ [1, ..., n c ], n c is the size of candidate set), which is further concatenated with a g t to build the final representation a t for candidate word a t . For the final prediction, we input the summarization vector v and candidate representation vector a t to f pred (•) and apply the softmax to derive the probability distribution over all n c candidate abstractive words, a t = concat(a g t , a e t ),</p><formula xml:id="formula_8">r t = v T W pred a t , ∀t ∈ [1, ..., n c ],<label>(22)</label></formula><p>o t = sof tmax t (r t ), ∀t ∈ [1, ..., n c ] (24) in which o t gives the probability of predicting the candidate word a t as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training Details</head><p>We train all models using the non-negative loglikelihood as the objective function. The gloss of candidate words are derived from WordNet using the NLTK tools <ref type="bibr" target="#b1">(Bird and Loper, 2004)</ref>. Specifically, we first lemmatize the candidate word and use the lemmatized word as the query word for the searching in WordNet. To cope with the semantic ambiguity of words, we just concatenate the gloss of the first sense in each retrieved POS for the query word with corresponding POS tag as the deliminator. Models in our experiments are trained with the following hyperparameter settings: All word embeddings and token embeddings a e t have 300 dimensions and are initialized with Glove <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref>. The passage p and summary s share one set of word embeddings, which are fixed during training. The glosses {g t } for candidate words {a t } keep its own word embeddings.</p><p>The hidden state vectors of all bi-directional GRU-RNNs in all models have 150 dimensions. The number of attention hops in GA Reader is set to 3. The batch size is set to 32. The method of Adam (Kingma and Ba, 2015) is adopted for optimization with initial learning rate 1e − 03. A dropout with rate 0.3 is applied to the input layers for all GRU-RNN encoders and the final summarization vector v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Annotation Selection</head><p>To ensure most of our annotation is valid, we select annotations satisfying the following criteria: a) the average accuracy is higher than 40%; b) both text spans should not be empty; c) if the difficulty level is rated as easy, then this data sample should be answered correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>e,</head><label></label><figDesc>j = tanh(W g att v + b) T h gt j , ∀j ∈ [1, ..., l gt ] ∀j ∈ [1, ..., l gt ],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fitting performance of neural regression model on the MRC database.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Three baseline models are used to generate candidate multiple choices for Subtask 1. The table shows their performance on the XSum dataset, evalu-</figDesc><table><row><cell cols="4">ated with MRR(Craswell, 2009), Recall@1, Recall@5,</cell></row><row><cell cols="2">and Recall@10.</cell><cell></cell></row><row><cell></cell><cell>MRR R@1</cell><cell>R@5</cell><cell>R@10</cell></row><row><cell cols="4">GAReader 0.343 0.268 0.422 0.484</cell></row><row><cell cols="4">AttReader 0.348 0.273 0.424 0.490</cell></row><row><cell>+gloss</cell><cell cols="3">0.228 0.166 0.286 0.345</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Three baseline models are used to generate candidate multiple choices for Subtask 2. The table shows their performance on the XSum dataset, evaluated with MRR, Recall@1, Recall@5, and Recall@10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>lists the size of our ReCAM datasets, i.e., numbers of questions. For example, in total Subtask 2 has 6,186 questions, which are split into training/development/test subsets.</figDesc><table><row><cell cols="4">Dataset Subtask 1 Subtask 2 Total</cell></row><row><cell>Train</cell><cell>3,227</cell><cell>3,318</cell><cell>6,545</cell></row><row><cell>Dev</cell><cell>837</cell><cell>851</cell><cell>1,688</cell></row><row><cell>Test</cell><cell>2,025</cell><cell>2,017</cell><cell>4,042</cell></row><row><cell>Total</cell><cell>6,089</cell><cell>6,186</cell><cell>12,275</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Size of the ReCAM Dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>conducted an ablation study regarding the performance discrepancy of different transformersbased pre-training models. They tested BERT, AL-BERT, and ELECTRA by directly fine-tuning the pre-trained LMs on the ReCAM data. ELECTRA outperforms BERT and ALBERT by large margins, which may be due to the different learning objec-</figDesc><table><row><cell cols="2">Rank Team</cell><cell cols="2">Acc Acc. Cross</cell></row><row><cell>-</cell><cell>GA Reader</cell><cell>25.1</cell><cell>-</cell></row><row><cell>1</cell><cell>SRC-B-roc</cell><cell cols="2">95.1 91.8 (↓ 3.3)</cell></row><row><cell>2</cell><cell>PINGAN-</cell><cell cols="2">93.0 91.7 (↓ 1.3)</cell></row><row><cell></cell><cell>Omini-Sinitic</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>ECNU-ICA-1</cell><cell cols="2">90.5 88.6(↓ 1.9)</cell></row><row><cell>4</cell><cell>tt123</cell><cell cols="2">90.0 86.2(↓ 3.8)</cell></row><row><cell>5</cell><cell>cxn</cell><cell>88.7</cell><cell>-</cell></row><row><cell>6</cell><cell>nxc</cell><cell cols="2">88.6 74.2(↓ 14.4)</cell></row><row><cell>7</cell><cell>ZJUKLAB</cell><cell>87.9</cell><cell>-</cell></row><row><cell>8</cell><cell>IIE-NLP-Eyas</cell><cell cols="2">87.5 82.1(↓ 5.4)</cell></row><row><cell>9</cell><cell>hzxx1997</cell><cell>86.7</cell><cell>-</cell></row><row><cell>10</cell><cell>XRJL</cell><cell cols="2">86.7 81.8(↓ 4.9)</cell></row><row><cell>11</cell><cell>noobs</cell><cell cols="2">86.2 78.6(↓ 7.6)</cell></row><row><cell>12</cell><cell>godrevl</cell><cell>83.1</cell><cell>-</cell></row><row><cell>13</cell><cell cols="3">ReCAM@IITK 82.1 80.7(↓ 1.4)</cell></row><row><cell>14</cell><cell>DeepBlueAI</cell><cell cols="2">81.8 76.3(↓ 5.5)</cell></row><row><cell>15</cell><cell>LRG</cell><cell cols="2">75.3 61.8(↓ 13.5)</cell></row><row><cell>16</cell><cell>xuliang</cell><cell>74.7</cell><cell>-</cell></row><row><cell>17</cell><cell cols="2">Llf1206571288 72.8</cell><cell>-</cell></row><row><cell>18</cell><cell>Qing</cell><cell>71.4</cell><cell>-</cell></row><row><cell>19</cell><cell>NEUer</cell><cell cols="2">56.6 51.8(↓ 4.8)</cell></row><row><cell>20</cell><cell>CCLAB</cell><cell cols="2">46.3 35.2(↓ 11.1)</cell></row><row><cell>21</cell><cell>UoR</cell><cell cols="2">42.0 39.4(↓ 2.6)</cell></row><row><cell>22</cell><cell>munia</cell><cell>19.3</cell><cell>-</cell></row><row><cell>23</cell><cell cols="2">BaoShanCollege 19.0</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Official results of Subtask 1 and Subtask 3. Acc is the accuracy of the models trained on the Subtask 1 training data and tested on the Subtask 1 testset. Acc. cross is the accuracy of models trained on the Subtask 2 training data and tested on the Subtask 1 testset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Official results of Subtask 2 and Subtask 3.</figDesc><table><row><cell>Acc is the accuracy (%) of the models trained on the</cell></row><row><cell>Subtask 2 training data and tested on the Subtask 2 test-</cell></row><row><cell>set. Acc. Cross is the accuracy(%) of models trained on</cell></row><row><cell>the Subtask 1 training data and tested on the Subtask 2</cell></row><row><cell>testset.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>r t = [h s q ; p] T W p a e t , ∀t ∈ [1, ..., n c ],(3)o t = sof tmax t (r t ), ∀t ∈ [1, ..., n c ](4)in which o t represents the probability of predicting the candidate abstractive word a t as the final answer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>. For an input news passage p = [p 1 , p 2 , ..., p lp ] with l p words, we can derive its hidden states H p = [h p 1 , h p 2 , ..., h p lp ] by sending its word embedding sequence P = [p 1 , p 2 , ..., p lp ] to the Passage Encoder. Similarly, we can derive hidden states H s = [h s 1 , h s 2 , ..., h s ls ] for summary s by inputting its word embedding sequence S = [s 1 , s 2 , ..., s ls ] into the Summary Encoder and hidden states H gt = [h gt 1 , h gt 2 , ..., h gt lg t</figDesc><table><row><cell>] for gloss</cell></row><row><cell>g t of the candidate word a t by sending its word</cell></row><row><cell>embedding sequence G t = [g t 1 , g t 2 , ..., g t lg t</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Annotation Script</head><p>[ , ,..., ]  g g g</p><p>Figure <ref type="figure">1</ref>: The model architecture of the attentive model with word gloss (AMWG) implemented in this paper. denotes the concatenation of input vectors. All the encoders are 1-layer bi-directional GRU-RNNs, denotes the weighted sum of vectors.</p><p>Another attention f g att (•) is applied to v and H gt ,</p><p>The following weighted sum of H gt , i.e, a g t , is derive as the final vector representation for the gloss of candidate word a t :</p><p>We also set a token embedding a e t for each candidate word a t (t ∈ [1, ..., n c ], n c is the size of candidate set), which is further concatenated with a g t to build the final representation a t for candidate word a t . For the final prediction, we input the summarization vector v and candidate representation vector a t to f pred (•) and apply the softmax to derive the probability distribution over all n c candidate abstractive words,</p><p>in which o t gives the probability of predicting the candidate word a t as the final answer. The word gloss, which defines a word sense meaning, has been mainly used in word sense disambiguation (WSD) task and its variants <ref type="bibr" target="#b20">(Lesk, 1986;</ref><ref type="bibr" target="#b24">Moro et al., 2014)</ref>. Since the goal of ARC is to predict a word that can summarize corresponding information from the source passage, which is an abstracting process, it may be helpful when the gloss, i.e., interpretation of candidate abstractive words, are provided.</p><p>We design an attentive model with word gloss (AMWG) as Figure <ref type="figure">1</ref> shows. Specifically, all the encoders are 1-layer bi-directional recurrent neural networks (RNNs) with Gated Recurrent Units (GRU) <ref type="bibr">(Cho et al.)</ref>. For an input news passage p = [p 1 , p 2 , ..., p lp ] with l p words, we can derive its hidden states ] to the WordGloss Encoder.</p><p>Similar to Section B, the corresponding representation of "@placeholder", i.e., h s q , is used as the final vector representation for summary s. And an bilinear attention f p att (•) is applied to h s q and H p as follows:</p><p>Then p is derived as the vector representation for passage p by the weighed sum of H p , which is further concatenated with the h s q to form the final summarization vector v:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An adapted lesk algorithm for word sense disambiguation using wordnet</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45715-1_11</idno>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing, Third International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002-02-17" />
			<biblScope unit="volume">2276</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics</title>
				<meeting>the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Economically organized hierarchies in wordnet and the oxford english dictionary</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Changizi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogsys.2008.02.001</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Syst. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="214" to="228" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Fethi Bougares Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The MRC psycholinguistic database</title>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mean reciprocal rank</title>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Database Systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1703" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DeBERTa: Decodingenhanced BERT with disentangled attention. CoRR, abs</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dureader: a chinese machine reading comprehension dataset from real-world applications</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Reading for Question Answering@ACL</title>
				<meeting>the Workshop on Machine Reading for Question Answering@ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-19" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
				<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</title>
				<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018-08-06" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">XRJL-HKUST at SemEval-2021 Task 4: Wordnet-enhanced dual multi-head co-attention for reading comprehension of abstract meaning</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to classify the wrong answers for multiple choice question answering (student abstract)</title>
		<author>
			<persName><forename type="first">Hyeondey</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13843" to="13844" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The narrativeqa reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gáabor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th annual international conference on Systems documentation</title>
				<meeting>the 5th annual international conference on Systems documentation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Xin Lin, and Liang He. 2021. ECNU ICA 1 SemEval-2021 Task 4: Leveraging knowledge-enhanced graph attention networks for reading comprehension of abstract meaning</title>
		<author>
			<persName><forename type="first">Pingsheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint/>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
				<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09" />
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2230" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<editor>Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intermediate-task transfer learning with pretrained language models: When and why does it work?</title>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.467</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5231" to="5247" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stanza: A Python natural language processing toolkit for many human languages</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yash Bhartia, and Tirtharaj Dash. 2021. LRG at SemEval-2021 Task 4: Improving reading comprehension with abstract words using augmentation, linguistic features and voting</title>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint/>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parameters of abstraction, meaningfulness, and pronunciability for 329 nouns</title>
		<author>
			<persName><forename type="first">Otfried</forename><surname>Spreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolph</forename><forename type="middle">W</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-5371(66)80061-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the difficulty of making concreteness concrete</title>
		<author>
			<persName><surname>Dl Theijssen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhj</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName><surname>Oostdijk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-2623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017</title>
				<meeting>the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghong</forename><surname>Hao</surname></persName>
		</author>
		<title level="m">Shaojun Wang, and Jing Xiao. 2021. PINGAN Omini-Sinitic at SemEval-2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reading comprehension of abstract meaning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ZJUKLAB at SemEval-2021 Task 4: Negative augmentation with language model for reading comprehension of abstract meaning</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing plm for recam with special tokens, re-ranking, siamese encoders and back translation</title>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
	<note>Vancouver</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TA-MAMC at SemEval-2021 Task4: Task-adaptive pretraining and multi-head attention for abstract meaning reading comprehension</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimeng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
				<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
