<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">Roth</forename><surname>Talita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Stuttgart Institute for Natural Language Processing</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Rani</forename><surname>Anthonio</surname></persName>
							<email>anthonta@ims.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stuttgart Institute for Natural Language Processing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best scoring system achieved an accuracy of 68%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of this shared task is to evaluate the ability of NLP systems to detect whether a sentence from an instructional text requires clarification. Such clarifications can be critical to ensure that instructions are clear enough to be followed and the desired goal can be reached. We set up this task as a binary classification task, in which systems have to predict whether a given sentence in context requires clarification. Our data is based on texts for which revision histories exist, making it possible to identify (a) sentences that received edits which made the sentence more precise, and (b) sentences that remained unchanged over multiple text revisions.</p><p>The task of predicting revision requirements in instructional texts was originally proposed by <ref type="bibr" target="#b2">Bhat et al. (2020)</ref>, who attempted to predict whether a given sentence will be edited according to an article's revision history. The shared task follows this setup, with two critical differences: First, we apply a set of rules to identify a subset of edits that provide clarifying information. This makes it possible to focus mainly on those edits that are related to implicit and underspecified language, excluding grammar corrections and other edit types. Since the need for such edits may depend on discourse context, a second difference is that we provide context for each sentence to be classified (see Table <ref type="table">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Store Asparagus</head><p>Keep the asparagus refrigerated for five to seven. [Cooked asparagus is best within a few days.] [Transfer the asparagus to a container.] Label the container with the date.</p><p>Table <ref type="table">1</ref>: Examples of a sentence that requires clarification according to the revision history () and a sentence that remained unedited over many article-level revisions (). Annotators and systems were provided with additional context, here shortened in brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Data</head><p>In our task, sentences from instructional texts are provided in their original context and systems need to predict whether the sentence requires clarification. We define a clarification as a type of revision in which information is added or further specified.</p><p>Systems participating in the shared task are required to distinguish between sentences that require clarification and sentences that do not. For simplicity, we assume all sentences that remained unchanged over multiple article-level revisions (until the final available version) to not require clarification. Based on this assumption, we create a class-balanced data set for our task by selecting for each sentence that requires clarification exactly one sentence that does not require clarification.</p><p>In the following, we provide details on the collection procedure and an annotation-based verification thereof as well as statistics of the final data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>We extract instances of clarifications from a resource of revision edits called wikiHowToImprove . Specifically, we used a state-of-the-art a constituency parser <ref type="bibr" target="#b3">(Mrini et al., 2020)</ref> to preprocess all revisions from wikiHow-  ToImprove and applied a set of rule-based filters to identify specific types of edits (see Table <ref type="table" target="#tab_1">2</ref>).</p><p>Sentences that require clarification identified this way are likely to share specific syntactic properties. Accordingly, it might be easy for a computational model to distinguish them from sentences that do not require clarification. We counteract this potential issue by relying on syntactic similarity to pair each sentence that requires clarification with a sentence that does not. Following <ref type="bibr" target="#b2">Bhat et al. (2020)</ref>, we specifically select sentences that are part of the final version of an article (according to wikiHowToImprove) and that remained unchanged over the past 75% of revisions on the article level. For the syntactic similarity measure, we calculate the inverse of the relative edit distance in terms of part-of-speech tags between two sentences.</p><p>Data and data format. We divide the collected data into training, development and test sets, following the splits by article of wikiHowToImprove. For all parts of the data, we provide the article name and the full paragraph in addition to the sentence to be classified. For the sentences that require clarification in the training set, we additionally provide the type of revision and the revised sentence.</p><p>Out-of-domain data. We collect a small set of data from other sources, following the procedure outlined above, to create a possibility of testing how well models would generalize beyond the type of instructions provided in wikiHow articles. For this purpose, we create a corpus of board game manuals that consists of modern games for which multiple print-runs and editions of manuals exist. <ref type="bibr">1</ref> We apply the same preprocessing and filtering criteria to this corpus as described above. In order to increase the size of this data, we allow edits that go beyond the exact match of a syntactic pattern (e.g. we include</p><p>The price. . . → This unit price. . . , which contains a small change in addition to the added modifier).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation and Statistics</head><p>Previous work has found that revisions do not always improve a sentence . Based on this insight, we decided to collect human judgements on all edited sentences that would be included as requiring revision in our development, test, and out-of-domain data. We used Amazon Mechanical Turk to collect 5 judgements per edit and only kept sentences that require clarification if a majority of annotators judged the revised version as being better than the original version.</p><p>Statistics. Our rule-based extraction approach yielded a total of 24,553 sentences that received clarification edits. We discarded 1,599 of these sentences as part of the annotation process. In these cases, annotators found the edits to be unhelpful or they had disagreements about the need for clarification. Finally, we paired the remaining 22,954 sentences with sentences that received no clarification. Statistics for the training, development, test and out-of-domain sentences as well as for the full data set are provided in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participants and Results</head><p>Two teams registered for the shared task and submitted predictions of their systems: Wiriyathammabhum (2021) and <ref type="bibr" target="#b4">Ruby et al. (2021)</ref>.</p><p>Wiriyathammabhum approached the task as a text classification problem and experimented with different training regimes of transformer-based models <ref type="bibr" target="#b5">(Vaswani et al., 2017</ref>  predictions against the expected performance of a random baseline and against a simple logistic regression classifier that makes use of uni-grams, bigrams and sentence length as features. The results, summarized in Table <ref type="table">4</ref>, show that the participating systems perform substantially better than both baselines on the test set. 2 Compared to this high performance (66.4-68.8%), results on the out-ofdomain data are considerably low (59.1%) and they do not exceed the accuracy of the logistic regression classifier (61.4%). We next discuss potential reasons for this and highlight other observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The results of the participating teams and the logistic regression baseline provide some insights regarding the task posed and the data sets provided.</p><p>Task. The results suggest that it is generally possible to predict whether a sentence requires clarification and models can pick up reliable patterns for most types of revision. In fact, the per-type results shown in Table <ref type="table" target="#tab_5">5</ref> indicate that the best participating system is able to identify over 90% of cases that require one of the following two types of clarifications: replacements of pronouns and replacements of occurrences of 'do' as a main verb. These two types may seem like easy targets because pronouns and relevant word forms can be found simply by matching strings. However, the results of the logistic regression model show that a simple word-based classification is insufficient. Not all occurrences of pronouns and 'do' require clarification (cf. Table <ref type="table" target="#tab_1">2</ref>).</p><p>On the other end, we find that required insertions of modifiers, quantifiers and modal verbs are hard to predict. In fact, the systems only identify up to 56% of such cases, which is only slightly better than the performance of a random baseline (50%). One reason could be that commonsense knowledge plays an important role in such clarifications.</p><p>Data. It is worth noting that the distribution of different revision types is not balanced and the overall results are skewed accordingly. In almost half of the test sentences that require clarification, the edit involved the insertion of an adverbial or adjectival modifier (49%, 840 out of 1,707). Predicting the need for such edits is particularly difficult because they often add only subtle and context-specific information. Replacements of pronouns form the second most-frequent clarification type in our data (23%, 398/1707). Both participating systems were able to identify over 92% of sentences that require such a replacement. The remaining cases are distributed as follows: insertions of optional verb complements (15%, 262/1707), insertions of quantifiers and modal verbs (10%, 166/1707) and replacements of 'do' as a main verb (2%, 41/1707).</p><p>One potential reason for the differences in results between the test data and the out-of-domain data is that revision types are distributed differently as well. In fact, the edits of sentences that require clarification in the out-of-domain data almost always involve the insertion of an adverbial/adjectival modifier or an optional complement (82%, 18/22).</p><p>Insights from Participants. In addition to our observations, the system descriptions also report a number of interesting findings. For instance, Ruby et al. found that pronouns requiring replacement are often denoting a generic referent or a type of individual, rather than a specific entity. Based on this observation, they perform several experiments in which they first identify pronouns that should potentially be revised and then they combine representations of the identified pronouns with a sentencelevel system to generate predictions.</p><p>A more technically motivated approach is taken by Wiriyathammabhum, who build on the observation that the distribution of sentence labels (re-quiring revision or not) is generally unbalanced and that revised versions of sentences that required clarification may be viewed as instances of sentences that do not require further clarification.</p><p>Both participants discuss interesting approaches to the shared task and show interim results on the training/development sets. For details, we refer the interested reader to the system description papers <ref type="bibr" target="#b6">(Wiriyathammabhum, 2021;</ref><ref type="bibr" target="#b4">Ruby et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Two teams participated in our shared task on predicting the need for clarifications, with the top performing system achieving an accuracy of 68.4%. Perhaps unsurprisingly, the main takeaway from both systems is that transformer-based models pose a strong baseline for future work. Linguistic insights. An analysis of the different types of needed clarifications showed that certain revision requirements are more difficult to predict than others. For example, we found edits that introduce potentially subtle and context-specific shades of meaning much more difficult to predict than cases where generic pronouns are resolved. Nonetheless, we find that the best system is able to predict the need for clarification across all types with an accuracy higher than expected by chance. We take this as a promising result and as motivation for future work on this task.</p><p>Open questions. A number of unanswered questions remain: for example, we have not investigated what is a realistic upper bound for the discussed task. We did find that annotators are generally able to identify which of two versions of a sentence is revised/better and they generally achieve high agreement. However, it still remains unclear under which conditions a revision is seen as mandatory. It also remains unclear to what extent the selected revision types actually reflect general clarification needs in a representative way.</p><p>In a preliminary study, we originally assumed that revisions of board game manuals could provide us with useful information about when clarifications are necessary. However, we found the application of syntactic rules for finding such revisions to be of limited use. Our annotation further showed that people also have difficulty distinguishing old game instructions from revised ones. It is quite likely that some texts are simply too specific for annotators (and computational models) as they require too much specialized knowledge.</p><p>Lessons learned. From our results, we draw the following conclusions for future tasks: a focus on instructions on everyday situations as described in wikiHow is generally desirable to enable a distinction between clarification needs due to implicit and underspecified language on the one hand and clarification needs due to lack of familiarity or specialized knowledge on the other hand. To better understand different needs for clarification, it will also be necessary to consider additional types of revisions in the future. Lastly, more context should be considered, both on the methods side as well as with regard to the data itself, in order to be able to better identify subtle clarification requirements.</p><p>We are already implementing some of these lessons in a follow-up task that will take place as part of SemEval-2022. In that task, the focus will be on sentences that require clarification and systems will need to predict which of multiple possible changes represent plausible clarifications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Board games in this set include Android: Netrunner, Brass: Lancashire, Champions of Midgard, Descent: Journeys into the Dark (2nd Ed.), Feast for Odin, Food Chain Magnate, Gloomhaven, Istanbul, Le Havre, Root, Teotihuacan: City of Gods, T.I.M.E. Stories, Unfair and War of the Ring (2nd Ed.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The change in temperature takes care of the rest.) You should do this as soon as you are finished.</figDesc><table><row><cell>Edit type</cell><cell>Description</cell><cell>Example</cell></row><row><cell>Modifiers</cell><cell>Insertion of an adver-bial/adjectival modifier</cell><cell cols="2">Try watching one game to see if you like it. (→ Try watching one game alone to see if you like it.)</cell></row><row><cell></cell><cell></cell><cell>Learn about some teams.</cell><cell>Article: Enjoy Football</cell></row><row><cell></cell><cell></cell><cell cols="2">Do not be ashamed of it with your parents.</cell></row><row><cell>Pronouns</cell><cell>Replacement of a pro-noun with a noun phrase</cell><cell cols="2">(→ Do not be ashamed of your choice with your parents.) Stay true to what you want.</cell></row><row><cell></cell><cell></cell><cell cols="2">Article: Explain Cross Dressing to Parents</cell></row><row><cell></cell><cell></cell><cell cols="2">Press and hold to take a photo.</cell></row><row><cell>Complements</cell><cell>Insertion of an optional verb complement</cell><cell cols="2">(→ Press and hold the button to take a photo.) Keep on pressing to extend the Snap to up to 30s.</cell></row><row><cell></cell><cell></cell><cell cols="2">Article: Set Up Snapchat Spectacles</cell></row><row><cell></cell><cell></cell><cell cols="2">Dry the shoe off with the hand towel.</cell></row><row><cell>Quantifier/</cell><cell>Insertion of a quantifier</cell><cell cols="2">(→ Dry each shoe off with the hand towel.)</cell></row><row><cell>Modals</cell><cell>or modal verb</cell><cell>Avoid using too much water.</cell></row><row><cell></cell><cell></cell><cell cols="2">Article: Make Your Sneakers Look New Again</cell></row><row><cell></cell><cell></cell><cell cols="2">The change in temperature does the rest.</cell></row><row><cell>Verbs</cell><cell>with another main verb Replacement of 'do'</cell><cell>(→</cell></row></table><note>Article: Cut a Glass Bottle</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Revision types and example sentences that require clarification from our training set (). Additionally shown are clarified versions (→ . . . ) and sentences that remain unrevised until the final version of an article ().</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics on sentence and word counts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Test accuracy (%) by edit type.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that due to a software bug during the evaluation phase, we allowed team Ruby et al. to submit an updated set of predictions after their official submission.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research presented in this paper was funded by the DFG Emmy Noether program (RO 4848/2-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2020. wikiHowToImprove: A resource and analyses on edits in instructional texts</title>
		<author>
			<persName><forename type="first">Talita</forename><surname>Anthonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irshad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
				<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="5721" to="5729" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What can we learn from noun substitutions in revision histories?</title>
		<author>
			<persName><forename type="first">Talita</forename><surname>Anthonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1359" to="1370" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards modeling revision requirements in wiki-How instructions</title>
		<author>
			<persName><forename type="first">Irshad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talita</forename><surname>Anthonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.675</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8407" to="8414" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking self-attention: Towards interpretability in neural parsing</title>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Mrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ndapa</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Nakashole</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.65</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A mention-based system for revision requirements detection</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Ruby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Understanding Implicit and Underspecified Language</title>
				<meeting>the First Workshop on Understanding Implicit and Underspecified Language</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TTCB System description to a shared task on implicit and underspecified language 2021</title>
		<author>
			<persName><forename type="first">Peratham</forename><surname>Wiriyathammabhum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Understanding Implicit and Underspecified Language</title>
				<meeting>the First Workshop on Understanding Implicit and Underspecified Language</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
