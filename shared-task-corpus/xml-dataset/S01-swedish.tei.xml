<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SENSEV AL-2: The Swedish Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Kokkinakis</surname></persName>
							<email>dimitrios.kokkinakis@svenska.gu.se</email>
						</author>
						<author>
							<persName><forename type="first">Jerker</forename><surname>Jarborg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yvonne</forename><surname>Cederholm</surname></persName>
							<email>yvonne.cederholm@svenska.gu.se</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Goteborg University</orgName>
								<address>
									<settlement>Sprakdata</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Gateborg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Goteborg University</orgName>
								<address>
									<addrLine>SE-405 30</addrLine>
									<postBox>Box 200</postBox>
									<settlement>Goteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Jerker.Jaerborg@ svenska.gu.se</orgName>
								<address>
									<addrLine>SE-405 30, SE-405 30</addrLine>
									<postBox>Box 200, Box 200</postBox>
									<settlement>Goteborg, Goteborg</settlement>
									<country>Sweden, Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SENSEV AL-2: The Swedish Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the organisation and results of the SENSEVAL-2 exercise for Swedish. We present some of the experiences we gained by participating as developers and organisers in the exercise. We particularly focus on the choice of the lexical and corpus material, the annotation process, the scoring scheme, the motivations for choosing the lexical-sample branch of the exercise, the participating systems and the official results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Word sense ambiguity is a potential source for errors in human language technology applications, such as Machine Translation, and it is considered as the great open problem at the lexical level of Natural Language Processing (NLP). There are, however, several computer programs for automatically determining which sense of a word is being used in a given context, according to a variety of semantic, or defining dictionaries as demonstrated in the SENSEV AL-l exercise; <ref type="bibr" target="#b3">(Kilgarriff and Palmer, 2000)</ref>. The purpose of SENSEV AL is to be able to say which programs and methods perform better, which worse, which words, or varieties of language, present particular problems to which programs; when modifications improve performance of systems, and how much and what combinations of modifications are optimal. Specifically for Swedish, we would also like to investigate to what extent sense disambiguation can be accomplished and the potential resources available for the task. We would thus be creating a framework that can be shared both within the 45 exercise and for future evaluation exercises of similar kind, national and international.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Choice of Task Three tasks were identified for SENSEVAL-2, namely: the lexical-sample, the all-words and the 'in a system' tasks. In the lexical sample task, first, we sample the lexicon, then we find instances in context of the sample words and the evaluation . is carried out on the sampled instances. In the all-word task a system will be evaluated on its disambiguation performance on every word in the test collection. Finally, in the third type of task, a. word sense disambiguation (WSD) system is evaluated on how well it improves the performance of a NL system (MT, IR etc). The reasons we chose the lexical-sample task for Swedish are summarised below:</p><p>1. Cost-effectiveness of annotation: it is easier and quicker for the human annotators to sense-tag multiple occurrences of one word at a time, particularly when robust interactive means are utilized (Section 3); 2. The lexical-sample reduces the work of preparing training data since only a subset of the sense inventory is used; 3. More systems can/could (eventually) participate; 4. The all-words task requires access to a full dictionary, which is problematic from the copyright point of view, since industrial partners were also allowed to participate; and, as <ref type="bibr" target="#b3">Kilgarriff and Palmer (2000)</ref> noted: 5. Provided that the sample is well chosen, the lexical sample strategy would be more informative about the current strengths and failings of sense disambiguation research than the all-words task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Development Process</head><p>In this section we will give a concise description of how the whole exercise (for Swedish) was set up, putting more emphasis on some of the main ingredients of the work, i.e. sampling, resources, annotation and scoring.</p><p>A number of likely participants were invited to express their interest and participate in the Swedish SENSEVAL <ref type="bibr">(summer, 2000)</ref>. A plan for selecting the evaluation material was agreed in Sprakdata, and human annotators were set on the task of generating the training and testing material. The material was released to the participants at the end of April 2001 and during the second week of June, 2001 the results were returned for scoring. The Swedish SENSEV AL material was divided into three parts and released in stages: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dictionary and Corpus</head><p>At least three lexical resources were candidates for the Swedish lexicon-sample task. These were the Swedish versions of the WordNet (http://www.ling.lu.se/projects/Swordnet) and the Swedish SIMPLE (http://spraakdata.gu.se/simple/), as well as the Gothenburg Lexical Data Base/semantic Database (GLDB/SDB) (http://spraakdata.gu.se/lb/gldb.html). We chose the GLDB/SDB. The creation of a Swedish version of WordNet, a resource that is extensively used for the semantic annotation of texts in other languages, is under development and had (up to that point) limited coverage, while the SIMPLE lexicon, although available, has limited coverage (in principle it could be used and it is linked to the GLDB/SDB). However, a draWback of the Swedish SIMPLE is that very fine-grained subsenses are not adequately described (or not described at all) in the material. GLDB/SDB is a generic defining dictionary of 65,000 lemmas available and developed at our department and became the final choice for the lexical inventory. (see <ref type="bibr" target="#b0">Allen, 1999</ref><ref type="bibr" target="#b0">Allen, [1981</ref> for a description of the model utilized in the dictionary).</p><p>For the textual material we chose the Stockholm-Umea Corpus (SUC), <ref type="bibr" target="#b1">Ejerhed et al. (1992)</ref>. The particular corpus was chosen for three main reasons. It is available to the research community; it is considered the "standard reference" corpus for contemporary writte~ Swedish; and, third, it is the corpus utilised in the SemTag project (next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sampling</head><p>There is no standard method for sampling the lexical data. However, certain features were considered. These were: frequency, polysemy, part-of-speech and distribution of senses. Words were chosen based not so much on intuition, but rather on their frequency and polysemy. Still, it was hard to find a balance between these two features since high frequency words tend to be monosemous in a corpus, while highly polysemous words tend to have few senses in a corpus. In the case that a word was frequent and polysemous we tried to provide more data (context), than for words that were less frequent. Part-of-speech information was consulted for the decision of choosing more nouns in the sample (highest portion in the GLDB/SDB), than verbs (less than nouns, but more than adjectives in the GLDB/SDB) and adjectives (which are fewer than nouns and verbs in GLDB/SDB). We chose a sample of words where the amount of senses was evenly distributed, i.e. lemmas (dictionary entries) with 2-7 lexemes (senses) and 1-23 cycles (subsenses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SemTag</head><p>Creating a sense-annotated reference corpus is a laborious task. Therefore, we developed the majority of the test and reference material within an ongoing project highly relevant for our mission, namely SemTag (Lexikalisk betydelse och anviindningsbetydelse -"Lexical Sense and Sense in Context", financed by the Swedish Council for Research in the Humanities and Social Sciences (HSFR)); see <ref type="bibr" target="#b2">Jarborg (1999)</ref>. In brief, the purpose of the project is to create a large sample of sense-annotated corpus (several hundreds of thousands of words), which can be used among other things for:</p><p>â€¢ measuring the performance of automatic methods for WSD; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Corpus/Sense Inventory</head><p>Table <ref type="table">1</ref> shows information on the sense inventory, the amount of corpus instances (training/testing) and the distribution of senses and sub-senses (Lexemes/Cycles) in the material for the twenty nouns (N), fifteen verbs (V) and the five adjectives (A). The total amount of training and testing corpus instances was: 8716/1525. The average polysemy in the sample is 3,517,6 for lexemes and cycles respectively. ag.gif for a screenshot of this tool). Due to our limited financial resources only two professional lexicographers and a trained Phd student were involved in the tagging process, which was preferred to (untrained) students doing the annotation. High replicability between the human annotators was observed (&gt;95%). The uncertain cases were not used in the training or testing material, while the provided dictionary descriptions for the 40 lemmas were revised (extended and/or modified) prior to their release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scoring</head><p>Prior to SENSEV AL, evaluating WSD performance was based solely on the exact match criterion, which is not consider a "fair" metric, and has a lot of drawbacks (e.g. it does not account for the semantic distance between senses when assigning penalties for incorrect labels, and it does not offer a mechanism to offer partial credit; cf <ref type="bibr" target="#b4">Resnik &amp; Yarowsky (2000)</ref>) Instead, in SENSEVAL-2 three scoring policies are adopted:</p><p>1. Fine-grained: answers must match exactly 2. Coarse-grained: answers are mapped to coarse-grained senses and compared to the gold standard tags, also mapped to coarsegrained ones (sense map is required; see below) 3. Mixed-grained: if a sense subsumption hierarchy is available, then the mixed-grained scoring gives some credit to choosing a more coarse-grained sense than the gold standard tag, but not full credit (also using a sense map; see below).</p><p>A "sense map" containing a complete list of all sense-ids involved in the evaluation was provided in order to perform the two last types of scoring policies. Each line in the sense map included sense subsumption information and contained a list of the subsumer senses and branching factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants and Results</head><p>Five groups showed interest in participating in the Swedish task (eight systems in total). Table <ref type="table">2</ref> provides information for the participating systems, while their average performance is given in Table <ref type="table" target="#tab_3">3</ref>, the score in parenthesis concerns: Verbs/Noun/ Adjectives. All systems returned answers for all instances, thus precision equals recall, all used supervised methods and all systems scored lower on the adjectives and higher on the nouns.  The process of WSD is a complex, controversial matter, but relevant for a number of NLP applications. Our contribution to the exercise will eventually sharpen the focus of WSD in Sweden; the material developed in SENSEVAL-2 can be used as benchmark for other researchers that need to measure their system's WSD performance against a concrete reference point (although the dictionary is limited). We think that WSD opens up exciting opportunities for linguistic analysis, contributing with very important information for the assignment of lexical semantic knowledge to polysemous and homonymous content words. The existence of sense ambiguity (polysemy and homonymy) is one of the major problems affecting the usefulness of basic corpus exploration tools. In this respect, we regard WSD as a very important process when it is seen in the context of a wider and deeper NLP system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>freezing and showing the data formatting conventions (lexicon &amp; corpus); Training data: the finalised sense inventory and portion of the 'gold standard'; Evaluation data: the rest of the 'gold standard', untagged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>POS Instances i Cycles</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>hora/1 mrua/1 skjuta/1 spela/1 vanta/1 viixa/1 oka/1 oppna/1</cell><cell>i v v v v v v v v</cell><cell>523/92 96/16 79/14 267/47 248/43 203/36 436177 147/25</cell><cell>5114 217 6115 6/23 3/15 2/9 2/2 4/16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bred/1</cell><cell>A</cell><cell>103/18</cell><cell>311</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>klar/1</cell><cell>A</cell><cell>307/54</cell><cell>4/11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>naturlig/1</cell><cell>A</cell><cell>139/24</cell><cell>4/5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>stark/1</cell><cell>A</cell><cell>352/62</cell><cell>5111</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>oppen</cell><cell>A</cell><cell>189/33</cell><cell>7/21</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Table 1. Data for the Swedish Lexical Sample</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>Annotation</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">The annotation was carried out interactively</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">using a concordance-based interface (developed</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">in SemTag) and which interacts with the corpus</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>the</cell><cell>dictionary;</cell><cell>(see</cell></row><row><cell></cell><cell></cell><cell>I</cell><cell></cell></row><row><cell>barn/1</cell><cell>N</cell><cell>656/115 I</cell><cell>316</cell></row><row><cell>betydelse/1</cell><cell>N</cell><cell>295/52</cell><cell>2/1</cell></row><row><cell>farg/1</cell><cell>N</cell><cell>110/19</cell><cell>4/ll</cell></row><row><cell>konst/1</cell><cell>N</cell><cell>77/13</cell><cell>316</cell></row><row><cell>kraft/1</cell><cell>N</cell><cell>152127</cell><cell>4/11</cell></row><row><cell>kyrka/1</cell><cell>N</cell><cell>154/27</cell><cell>2/3</cell></row><row><cell>kiinsla/1</cell><cell>N</cell><cell>142/25</cell><cell>2/4</cell></row><row><cell>ledning/1</cell><cell>N</cell><cell>9l/16</cell><cell>4/1</cell></row><row><cell>makt/1</cell><cell>N</cell><cell>128/22</cell><cell>3/4</cell></row><row><cell>massa/1</cell><cell>N</cell><cell>93116</cell><cell>613</cell></row><row><cell>mening/1</cell><cell>N</cell><cell>168/29</cell><cell>4/1</cell></row><row><cell>natur/1</cell><cell>N</cell><cell>90/16</cell><cell>3/4</cell></row><row><cell>program/1</cell><cell>N</cell><cell>139/24</cell><cell>4110</cell></row><row><cell>rad/1</cell><cell>N</cell><cell>145/25</cell><cell>4/3</cell></row><row><cell>rum/1</cell><cell>N</cell><cell>223/39</cell><cell>317</cell></row><row><cell>scen/1</cell><cell>N</cell><cell>101117</cell><cell>417</cell></row><row><cell>tillfalle/1</cell><cell>N</cell><cell>ll7/20</cell><cell>2/4</cell></row><row><cell>uppgift/1</cell><cell>N</cell><cell>174/30</cell><cell>2/3</cell></row><row><cell>vatten/1</cell><cell>N</cell><cell>285/50</cell><cell>2/3</cell></row><row><cell>amne/1 betyda/1 flytta/1 fylla/2 roija/1 forklara/1 galla/1 handla/1</cell><cell>N v v v v v v v</cell><cell>198/34 198/35 188/33 96117 ~45/61 169/30 843/148 250/44</cell><cell>4/4 4/4 2/4 4111 5/19 2/9 4/6 415</cell></row><row><cell></cell><cell></cell><cell></cell><cell>47</cell></row></table><note>! Corpus ! Lexemesl ! Word I http:/ I svens ka. gu.se/ -svedk!S ENS EV AUi mages/semt</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Results. Overall Precision followed by precision for (Verb/Noun/Adective) instances 48 Conclusion</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the Swedish Council for Research in the Humanities and Social Sciences (HSFR) for providing financial support for the coordination of the task.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modersmalet i Faderneslandet. Ett urval uppsatser under fyrtio ar av Sture Allen</title>
		<author>
			<persName><forename type="first">Allen</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Lernma-Lexeme Model of the Swedish Lexical Database. Empirical Semantics</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Rieger</surname></persName>
			<persName><forename type="first">)</forename><surname>Bochum</surname></persName>
		</editor>
		<imprint>
			<publisher>Allen S</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
	<note>Reprinted in</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Linguistic Annotation of the Stockholm-Umea Corpus project</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ejerhed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kallgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wennstedt</surname></persName>
		</author>
		<author>
			<persName><surname>Astri</surname></persName>
		</author>
		<idno>No. 33</idno>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Umea</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Research Reports from the Department of Swedish, Spnikdata, GU-ISS-99-6</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jarborg</surname></persName>
		</author>
		<ptr target="http://svenska.gu.se/-svedk/resrapp/konfront.pdf." />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Lexikon i konfrontation. In Swedish</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to the Special Issue on SENSEV AL</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer and the Humanities</title>
		<imprint>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000" />
			<publisher>Kluwer Acad. Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing Systems and Distinguishing Senses: New Evaluation Methods for Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="133" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
