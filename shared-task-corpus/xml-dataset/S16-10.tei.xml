<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
							<email>nschneid@inf.ed.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
							<email>anders@johannsen.com</email>
						</author>
						<author>
							<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
							<email>marine@cs.umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Dept</orgName>
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<settlement>Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This task combines the labeling of multiword expressions and supersenses (coarse-grained classes) in an explicit, yet broad-coverage paradigm for lexical semantics. Nine systems participated; the best scored 57.7% F 1 in a multi-domain evaluation setting, indicating that the task remains largely unresolved. An error analysis reveals that a large number of instances in the data set are either hard cases, which no systems get right, or easy cases, which all systems correctly solve.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical analysis tasks, e.g., part-of-speech tagging, are rather successful applications of natural language processing (NLP). They are comprehensive, i.e., they operate under the assumption that all grammatically-relevant parts of a sentence will be analyzed: We do not expect a POS tagger to only know a subset of the tags in the language. Most POS tags accommodate unseen words and adapt readily to new text genres. Together, these factors indicate a representation which achieves broad coverage.</p><p>Explicit analysis of lexical semantics, by contrast, has been more difficult to scale to broad coverage owing to limited comprehensiveness and extensibility. The dominant paradigm of fine-grained word sense disambiguation, WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, is difficult to annotate in corpora, results in considerable data sparseness, and does not readily generalize to out-of-vocabulary words. While the main corpus with WordNet senses, SemCor <ref type="bibr" target="#b22">(Miller et al., 1993)</ref>, does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech. This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora.</p><p>To address this limitation, in the DiMSUM 2016 shared task, 1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following <ref type="bibr" target="#b43">Schneider and Smith, 2015)</ref>, on multiple nontraditional genres of text. By moving away from fine-grained sense inventories and lexicalized, language-specific 2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis. We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres.</p><p>The integrated lexical semantic representation ( §2, §3) has been annotated in an extensive benchmark data set comprising several nontraditional domains ( §4). Objective, controlled evaluation procedures ( §5) facilitate a comparison of the 9 systems submitted as part of the official task ( §6). While the systems range in performance, all are below 60% in our composite evaluation, suggesting that further work is needed to make progress on this difficult task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Multiword expressions. Most contemporary approaches to English syntactic and semantic analysis treat space-separated words as the basic units of structure. However, this fails to reflect the basic units of meaning for sentences with noncompositional or idiosyncratic expressions, such as:</p><p>(1) The staff leaves a lot to be desired .</p><p>(2) I googled restaurants in the area and Fuji Sushi came up and reviews were great so I made a carry out order of : L 17 .</p><p>In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as "prepackaged" in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see <ref type="bibr" target="#b2">Baldwin and Kim (2010)</ref> for a review. <ref type="bibr" target="#b42">Schneider et al. (2014b)</ref> introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems <ref type="bibr" target="#b39">(Schneider et al., 2014a)</ref>. Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: <ref type="bibr" target="#b8">Constant and Sigogne, 2011;</ref><ref type="bibr" target="#b11">Green et al., 2012;</ref><ref type="bibr" target="#b32">Ramisch et al., 2012;</ref><ref type="bibr" target="#b51">Vincze et al., 2013)</ref>, though the corpus and identification system of <ref type="bibr" target="#b50">Vincze et al. (2011)</ref> targets several kinds of MWEs.</p><p>Importantly, the MWEs in <ref type="bibr">Schneider et al.'s (2014b)</ref> corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supersenses.</head><p>As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not suffer from these problems, as it uses a much smaller number of coarsegrained classes. However, these classes only apply to a subset of the nouns in a sentence and exclude verbs and adjectives. They therefore provide far from complete coverage in a corpus.</p><p>Noun and verb supersenses <ref type="bibr" target="#b7">(Ciaramita and Altun, 2006</ref>) offer a middle ground in granularity: they generalize named entity classes to cover all nouns (with 26 classes), but also cover verbs (15 classes)see table 1-and provide a human-interpretable high-level clustering. WordNet supersenses for adjectives and adverbs nominally exist, but are based on morphosyntactic rather than semantic properties. There is, however, recent work on developing supersense taxonomies for English adjectives and prepositions <ref type="bibr" target="#b48">(Tsvetkov et al., 2014;</ref>.</p><p>The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data-including out-of-vocabulary words in English or other languages <ref type="bibr" target="#b41">(Schneider et al., 2012;</ref><ref type="bibr" target="#b18">Johannsen et al., 2014)</ref>. Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish. <ref type="bibr">3</ref> Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection <ref type="bibr" target="#b52">(Ye and Baldwin, 2007;</ref><ref type="bibr" target="#b12">Heilman, 2011;</ref><ref type="bibr" target="#b15">Hovy et al., 2013;</ref><ref type="bibr" target="#b47">Tsvetkov et al., 2013)</ref>.</p><p>Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V:CONTACT and N:FOOD, whereas the idiomatic interpretation, 'divulge a secret', is represented as an MWE holistically tagged as V:COMMUNICATION. <ref type="bibr" target="#b43">Schneider and Smith (2015)</ref> develop this idea at Figure <ref type="figure">1</ref>: Illustration of the target representation. MWE positional markers are shown above the sentence and noun and verb supersenses below the sentence. Links illustrate the behavior of the MWE tags. The supersense labeling must respect the MWEs; thus, V.COGNITION applies to a four-word unit-to, be, and desired must not receive separate supersenses from leaves.</p><p>length, and provide a web reviews data set with the integrated annotation. Here, we expand the paradigm to additional domains and compare the performance of several systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation</head><p>The analysis for each sentence is represented as a sequence of paired MWE and supersense tags. Figure <ref type="figure">1</ref> illustrates the MWE part above the sentence and the supersense part below the sentence.</p><p>The MWE portion is a BIO-style (Ramshaw and <ref type="bibr" target="#b33">Marcus, 1995)</ref> positional marker. Of the schemes discussed by <ref type="bibr" target="#b39">Schneider et al. (2014a)</ref>, we adopt the 6-tag scheme, which uses case to allow gaps in an MWE (lowercase tag variants mark tokens within a gap). The positions are thus O, o, B, b, I, i. Systems are expected to ensure that the full tag sequence for a sentence is valid: global validity can be enforced with first-order constraints to prohibit invalid bigrams such as O I and b I (see <ref type="bibr">Schneider et al., 2014a for details)</ref>.</p><p>Because strong MWEs receive a supersense as a unit (if at all), I and i are never accompanied by a supersense label. O or o indicates that the token is not part of any MWE, but many such tokens do bear a noun or verb supersense.</p><p>This task uses a CoNLL-style main file format consisting of one line per token, each line having 9 tab-delimited columns. Scripts to convert to and from the .sst format, which displays one sentence per line and contains annotations in a JSON data structure, are provided as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>The task built upon two existing data sets of social web text, which were harmonized to form the training data. Four new samples from three domains were newly annotated to form the test set. The train and test sets are summarized in tables 2 and 3 and are publicly available on the web. <ref type="bibr">4</ref> The domains covered are online customer reviews, tweets, and TED talks. This section describes, for each domain, how its component data sets were sampled, preprocessed, and annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation Process</head><p>We compiled data sets from various sources, with varying degrees of existing pre-annotation. Unless already provided, we added Universal POS tags as defined by the Universal Dependencies project <ref type="bibr" target="#b24">(Nivre et al., 2015)</ref>, and baseline supersenses (heuristically using the most frequent WordNet sense, and in some cases grouping sequences of proper nouns as MWEs). The pre-annotated supersenses were then manually corrected by a trained annotator, who simultaneously annotated the sentence for comprehensive MWEs.</p><p>The annotator (a linguist) was trained by the first author of this paper using <ref type="bibr" target="#b43">Schneider and Smith's (2015)</ref> web interface and annotation guidelines. Prior to starting on the data sets for this task, the annotator devoted approximately 8 hours to training practice on a separate data set which already had a gold standard. Periodic feedback was given on initial annotations as the annotator grew accustomed to the conventions. The annotator spent approximately 50 hours on DiMSUM data (not including the initial training phase), which amounts to roughly 90 seconds per sentence.</p><p>In order to estimate inter-annotator agreement (IAA), the first author independently annotated a sample of Ritter tweets ( §4.3) in 6 groups of 11 sentences, spaced out across the main annotator's annotation batches. IAA estimates for these sets ranged from 60% to 75% F 1 for MWEs, and 67%-80% accuracy for supersenses (on tokens which had supersenses in both annotations). Resources did not allow for more systematic double annotation and IAA estimation throughout the data.</p><p>The test set newly annotated for this task comprises exactly 1,000 sentences and exactly 16,500 words. 3,120 word tokens (19%) differ from the preannotation with respect to gold MWE boundaries Domain Source corpus UPOS (UD 1.2-style) Docs Sents Words w s #lemmas Train REVIEWS STREUSLE 2.1 <ref type="bibr" target="#b43">(Schneider and Smith, 2015)</ref> Conv. from PTB parses 723 3,812 55,579 14.6 5,052 TWEETS Lowlands (tweets w/ URLs) <ref type="bibr" target="#b18">(Johannsen et al., 2014)</ref> Conv. from Petrov-style N/A 200 3,062 15.3 1,201 TWEETS Ritter <ref type="bibr" target="#b35">(Ritter et al., 2011;</ref><ref type="bibr" target="#b18">Johannsen et al., 2014)</ref> Conv.   Conv. from Petrov-style N.A. 340 6,357 18.7 1,365 TWEETS Tweebank <ref type="bibr">(Kong et al., 2014)</ref> Conv. from TweetNLP POS in FUDG parses N/A 500 6,627 13.3 1,786 TED NAIST-NTT (⊂ IWSLT train) <ref type="bibr" target="#b6">(Cettolo et al., 2012;</ref><ref type="bibr" target="#b23">Neubig et al., 2014)</ref> Conv. from PTB parses 10 100 2,187 21.9 630 TED IWSLT test <ref type="bibr" target="#b6">(Cettolo et al., 2012)</ref>    <ref type="bibr" target="#b27">(Petrov et al., 2011)</ref>, PTB <ref type="bibr" target="#b20">(Marcus et al., 1993)</ref>, or TweetNLP <ref type="bibr" target="#b25">(Owoputi et al., 2013</ref>) POS tagsets do not. Disambiguation was done manually or via a gold parse, where available. We also modified the tokenization of the Tweebank data, to be consistent with UPOS conventions for English (e.g., separating clitics).</p><p>Only some portions of the data group sentences into documents: N/A = not applicable; N.A. = not available.  and/or supersenses. <ref type="bibr">5</ref> In addition, portions of the training data were reannotated for improved quality and consistency with the STREUSLE annotations, as explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">REVIEWS</head><p>Training. The REVIEWS part of the training data consists of the STREUSLE corpus <ref type="bibr" target="#b42">(Schneider et al., 2014b;</ref><ref type="bibr" target="#b43">Schneider and Smith, 2015)</ref>, <ref type="bibr">6</ref> comprising comprehensive multiword expression and supersense annotations on a 55,000-token portion of the English Web Treebank (EWTB; <ref type="bibr" target="#b3">Bies et al., 2012)</ref> made up of 723 online user reviews for services (such as restaurants and beauticians).</p><p>STREUSLE annotation was done by linguists, who took pains to establish conventions and resolve disagreements. Each sentence was annotated independently by at least 2 annotators; disagreements were resolved by negotiation.</p><p>The task release is based on version 2.1 of STREUSLE, with weak MWEs removed and Penn Treebank-style POS tags replaced with Universal POS tags. 7</p><p>Test. The test portion comprises 340 sentences (6,357 tokens) from the online review site Trustpilot, a subset of the data used in  (the website as a general resource was described in ). The reviews were chosen to obtain a demographic balance (by age, gender, and location), and contained gold POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TWEETS</head><p>Training. <ref type="bibr" target="#b18">Johannsen et al. (2014)</ref> recently annotated two samples of 987 Twitter messages (18,000 words) with supersenses: (a) the POS+NERannotated data set of <ref type="bibr" target="#b35">Ritter et al. (2011)</ref>, and (b) <ref type="bibr">Plank et al.'s (2014)</ref> sample of 200 tweets. <ref type="bibr">8</ref> Annotators were shown pre-annotations from a heuristic supersense chunking/tagging system (based on <ref type="bibr">5</ref> On the surface, this might be taken to mean that the accuracy of the heuristic baseline used for pre-annotation is 81%. However, because the annotator saw the pre-annotation, we expect that this agreement rate is higher than if the gold standard had been produced from scratch. the most frequent sense of each word) and asked to correct the boundaries and supersense labels. Though there was no explicit MWE annotation phase, many of the multiword chunks tagged with a noun or verb supersense would be considered MWEs.</p><p>We fully reannotated both data sets to match the conventions of the REVIEWS data from the STREUSLE corpus. The annotator examined every sentence and corrected any MWE or supersense decisions deemed to be inconsistent with the guidelines.</p><p>Test. Our test set consists of 500 tweets (6,627 tokens) taken from the Tweebank corpus (Kong et al., 2014), 9 which already contained some goldstandard MWEs. We converted the POS tags from gold ARK TweetNLP POS + FUDG dependencies to UPOS and had an annotator supply supersenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TED TALKS</head><p>Test. To test the broad-coverage aspect of the submitted systems, the test set contained a "surprise" domain. We opted to sample transcribed sentences from TED talks. Because individual TED talks tend to heavily repeat vocabulary, we took the first 10 sentences from each of 16 documents in order to achieve a lexically diverse sample. Specifically, we chose (a) 100 sentences (2,187 tokens) from the 10 talks in the NAIST-NTT Ted Talk Treebank 10 (Neubig et al., 2014) (which in turn is a subset of the IWSLT training data); and (b) 60 sentences (1,329 tokens) from the IWSLT test data <ref type="bibr" target="#b6">(Cettolo et al., 2012)</ref>. <ref type="bibr">11</ref> The latter 6 documents were chosen to maximize language pair diversity. <ref type="bibr">12</ref> We induced parts of speech by conversion from the gold PTB trees for the NAIST-NTT data, and 9 http://www.cs.cmu.edu/~ark/TweetNLP/ 10 http://ahclab.naist.jp/resource/tedtreebank/ 11 https://wit3.fbk.eu/ 12 These 6 talks are known to have been translated from English into (at least) the following languages: {ar, de, es, fa, he, hi, it, ko, nl, th, vi, zh}. Additionally, we note that 4 of the documents have Czech (cs) translations, while the other 2 have French (fr) translations. <ref type="bibr" target="#b23">Neubig et al. (2014)</ref> report that all the 10 documents in the NAIST-NTT Treebank have been translated from English into the following 18 languages: {ar, bg, de, el, es, fr, he, it, ja, ko, nl, pl, pt-BR, ro, ru, tr, zh-CN, zh-TW}. Many additional languages are represented for subsets of the documents. for the remaining data, by automatic tagging with an averaged structured perceptron model (Rungsted 13 ) trained on the English Universal Dependencies v1.2 treebank <ref type="bibr" target="#b24">(Nivre et al., 2015</ref>). 14</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparing Domains</head><p>A natural question to ask about lexical semantic annotations is whether we observe strong differences between domains. For example, which kinds of multiword expressions and which kinds of supersenses occur more often in some domains than in others?</p><p>In this section, we report our observations but do not make any strong claims about their generality, for the following reasons: the samples are not necessarily representative of their domains overall, and, in fact, may have been sampled in a biased way (e.g., the Lowlands sample was limited to tweets containing a URL, and as a result, most of these tweets are headlines and advertisements). Furthermore, the annotation procedures differed by subcorpus, likely biasing the results.</p><p>MWEs. gent than subcorpora in different domains. Lowlands stands out as having a large share of proper noun MWEs-presumably due to the headlineoriented nature of the sample. STREUSLE has the smallest proportion of nominal MWEs, perhaps owing to the way it was annotated: initial rounds of STREUSLE annotation targeted MWEs only, with noun and verb supersenses added only later; whereas in the other data sets, MWE and supersense annotation were performed jointly, so annotator attention may have been focused on nominal and verbal expressions rather than other MWEs.</p><p>Supersenses. In the spirit of <ref type="bibr" target="#b41">Schneider et al. (2012)</ref>, we performed an analysis to see which supersenses were more characteristic of some domains than others. Figure <ref type="figure">3</ref> plots the relative frequency (out of all supersense-labeled units) of each supersense in each of the three domains. We use the REVIEWS domain as base frequency: relative to that, the x-axis is the supersense's occurrence rate in the TWEETS domain, and the y-axis represents the rate for the TED talks.</p><p>These plots show some clear outliers: among nouns (left plot), N.GROUP and N.FOOD are overrepresented in REVIEWS relative to the other domains-unsurprising because restaurants and other businesses are prominent in this subcorpus. On the other hand, N.PERSON is underrepresented in REVIEWS. N.TIME and N.COMMUNICATION are more popular in the TWEETS domain than the others. Among verbs (right plot), V.STATIVE is underrepresented, apparently due to the relative rarity of the copula (which often can be safely omitted in headlines and other telegraphic messages without obscuring the meaning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Submission conditions. We invited submissions in multiple data conditions. The open condition encouraged participants to make wide use of any and all available resources, including for distant or direct supervision. A closed condition encouraged controlled comparisons of algorithms by limiting their training to specific resources distributed for the task. Lastly, we allowed for a semi-supervised closed condition, in which use of a specific large unla- Figure <ref type="figure">3</ref>: Supersense rate differences by domains, compared to reviews data set. Circle area proportional to the supersense's total frequency across all domains. Noun supersenses on the left, verb supersenses on the right. Each domain's rate is microaveraged across its subcorpora; thus, larger subcorpora weigh more heavily than smaller subcorpora in the same domain.</p><p>beled corpus-the Yelp Academic Dataset 15 -was permitted. Teams were permitted to submit no more than one run per condition. Only one team submitted a system in the semi-supervised closed condition.</p><p>All conditions had access to: 1) the annotated data we provided; 2) Brown clusterings <ref type="bibr" target="#b5">(Brown et al., 1992)</ref> computed from large corpora of tweets and web reviews; <ref type="bibr">16</ref> and 3) the English WordNet lexicon. The input at test time included POS tags.</p><p>No sentence-level metadata was provided in the input at test time: test set sentence IDs were obscured to hide the source domain, and the order of sentences was randomized to remove document structure. The training data, however, marked the domain from which the sentence was drawn (REVIEWS or TWEETS); systems were free to make use of this information, so long as it was not required as part of the input at test time.</p><p>Scoring. We provided an evaluation script to allow participants to check the format of system output and to compute all official scores.</p><p>The MWE measure looks at precision, recall, and F 1 of the identified MWEs. Tokens not involved in a 15 https://www.yelp.com/academic _ dataset predicted or gold MWE do not factor into this measure. To award partial credit for partial overlap between a predicted MWE and a gold MWE, these scores are computed based on links between consecutive tokens in an expression <ref type="bibr" target="#b39">(Schneider et al., 2014a)</ref>. The tokens must appear in order but do not need to be adjacent. The precision is the proportion of predicted links whose words both belong to the same expression in the gold standard. Recall is the same as precision, but swapping the predicted and gold annotations. 17 Figure <ref type="figure" target="#fig_2">4</ref> defines this measure in detail and illustrates the calculations for an example.</p><p>To isolate the supersense classification performance, we compute precision, recall, and F 1 of the supersense-labeled word tokens. The numerator of both precision and recall is the number of tokens labeled with the correct supersense. (This interacts slightly with MWE identification, however, as supersenses are only marked on the first token of MWEs. We do not mark supersenses on all words of the MWE to avoid giving MWEs a disproportionate influence on the supersense score.)</p><p>Finally, combined precision, recall, and F 1 aggregate the MWE and supersense subscores. The combined precision ratio is computed from the MWE MWE Precision: The proportion of predicted links whose words both belong to the same expression in the gold standard. MWE Recall: Same as precision, but swapping the predicted and gold annotations. MWE precision of the bottom annotation relative to the top one is 2 5. (Note that a link between words w 1 and w 2 is "matched" if, in the other annotation, there is a path between w 1 and w 2 .) The MWE recall value is 3 4. Supersense precision and recall are both 1 2. Combined precision/recall scores add the respective subscores' numerators and denominators: thus, combined precision is 2+1 5+2 = 3 7, and combined recall is 3+1 4+2 = 2 3. Combined F 1 is their harmonic mean, i.e. 12 23. and supersense precision ratios by adding their numerators and denominators, and likewise for combined recall (see the example in figure <ref type="figure" target="#fig_2">4</ref>).</p><p>Within each domain, scores are computed as microaverages. The official tri-domain scores reported here are domain macroaverages: per-domain measures are aggregated with the three domains weighted equally. The main score, tri-domain combined F 1 , is the arithmetic mean of the three perdomain combined F 1 scores. (Some system papers report domain microaverages, which give less influence to the TED domain because it is the smallest of the domains in the test set.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Entries and Results</head><p>Six teams 18 participated in the task, submitting a total of nine unique system entries prior to the deadline. We give an overview of these systems and analyze their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synopsis of approaches</head><p>From the UFRGS&amp;LIF team <ref type="bibr" target="#b9">(Cordeiro et al., 2016)</ref>, S106 detects MWEs by heuristic patternmatching against sequences in the training data, and predicts the most frequent supersense observed for each type in the training data.</p><p>From the UTU team (Björne and Salakoski, 2016), S211, S254, and S255 match word sequences against a variety of resources and then choose a <ref type="bibr">18</ref> None of the teams included any DiMSUM organizers.</p><p>supersense with an ensemble of classifiers. The method performs reasonably well for supersenses, but is weak at detecting MWEs.</p><p>The UW-CSE team <ref type="bibr" target="#b13">(Hosseini et al., 2016</ref>) experimented with a sequence CRF as well as a double-chained CRF, with separate chains for MWE tags and supersenses, and some parameters shared between them. The closed-condition and open-condition feature sets were drawn from AMALGrAM <ref type="bibr" target="#b43">(Schneider and Smith, 2015)</ref>. Of the official submissions, S248 used a single-chain CRF and S249 a double-chained CRF. A full comparison demonstrates that the double-chained CRF performs best on the combined measure in both the closed and open conditions.</p><p>From the ICL-HD team <ref type="bibr" target="#b19">(Kirilin et al., 2016)</ref>, S214 uses the AMALGrAM sequence tagger <ref type="bibr" target="#b43">(Schneider and Smith, 2015)</ref> with an augmented feature set that leverages word embeddings and a knowledge base. The word embedding features, the knowledge base-derived features, and their union all improve over the condition with no new features, with respect to both MWE performance and supersense performance. The best results for the combined measure are obtained with the word embedding features (but not the knowledge base features). The word embeddings are shown to be somewhat complementary to AMALGrAM's Brown cluster features: ablating either reduces performance.</p><p>From the WHUNlp team <ref type="bibr" target="#b45">(Tang et al., 2016)</ref>  MWEs, and a maximum entropy classifier then predicts a supersense independently for each lexical expression. Each of these models has a small number of feature templates recording words and POS tags.</p><p>From the VectorWeavers team <ref type="bibr" target="#b37">(Scherbakov et al., 2016)</ref>, S227 relies on neural network classifiers to detect MWE boundaries and label supersenses, using features based on word embeddings and syntactic parses. Results show that syntax helps identify MWE boundaries accurately, and that simple incremental composition functions can help construct useful MWE representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Overall results</head><p>The main results appear in table 4. The first column of table <ref type="table" target="#tab_11">4</ref> gives the ranking of the systems. Several systems may share a rank if they do not produce significantly different predictions, as detailed below. The score is the combined supersense and MWE measure, macroaveraged over the three test set domains as described above. The final column indicates the resource condition: systems entered in the open condition (all resources allowed) are designated "++"; "+" indicates the more restricted semi-supervised closed condition, while the remaining systems are in the closed condition (most restrictive). Details of the resource conditions and scoring appear in §5.</p><p>Ranking and significance. The overall best scoring system, with a combined measure of 57.77%, is S214. The competition, however, is close: S249 scored 57.71%, and S248 obtained a combined score of 57.10%. To check whether the predictions of the systems are significantly different from each other,  we ran McNemar's test, a paired test that operates directly on the predicted system output. A consequence of this is that we do not directly test whether the computed scores are significantly different from each other, only whether the predictions are.</p><p>According to McNemar's test, the predictions of the highest-ranking and the next-highest-ranking system are not significantly different at p &lt; .05. The third highest ranking system performs significantly worse than the top system, but is not significantly different from the second-place system. We therefore decided to rank all three systems together. In general, adjacent entries in the sorted scoring table are ranked together if the difference between them is not statistically significant according to the test. Drilling down. When scores in the 3 domains are compared for each system, there is surprisingly little difference overall. We expected that the TED domain would be most difficult because it is not represented in the training data, but the scores in table <ref type="table" target="#tab_13">5</ref> give no clear indication that this is the case. Perhaps systems escaped domain bias because the training data included two highly divergent genres; or perhaps other aspects of the data sets (e.g., topic) matter more for this task than differences in genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Easy and hard decisions</head><p>Overall, the results clearly show that the joint supersense and MWE tagging task is not yet resolved. Given the wide range of participating systems and previous work, it is reasonable to assume that the task itself is not easy. On the other hand, it is not  uniformly hard. In fact, some decisions are relatively easy, in the sense that most or all systems get them right; whereas others are hard, in that none or very few systems produce the correct answer. Figure <ref type="figure" target="#fig_3">5</ref> explores this for the supersense-tagging subtask. The tallest bars are near the left and right sides of the graph, representing the hard and easy instances, respectively. Hard instances account for about 25% of instances where the gold data has a supersense, which also puts an upper bound on any system combination. Even an oracle system allowed to choose the best prediction for each instance from among all the systems would still not push the accuracy above 75%.</p><p>The distribution of easy and hard instances varies a lot between labels, though. As shown for supersenses in figure <ref type="figure" target="#fig_5">6</ref>, individual labels range from the fairly easy (e.g. V.STATIVE and V.COMMUNICATION) to the more difficult (e.g. N.ATTRIBUTE and V.CONTACT). The most common supersense, V.STATIVE, is easy because it has few distinct lexical forms (the ten most common lemmas make up more than 77% of the instances). Examples of V.STATIVE lemmas include be, have, use, and get. Supersenses may be difficult for more than one reason. For instance, V.CONTACT-e.g. deliver, receive, and take-has more distinct forms than V.STATIVE and also a more complex mapping between lemmas and supersenses. In contrast, person names, job titles, etc. that should be tagged as N.PERSON are rarely ambiguous with respect to supersense. The main challenge in that case is that the category is open-ended and not in general evident from syntactic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">System correlation</head><p>Finally, we examine whether the submitted approaches capture different aspects of the task. I.e., could we produce a better system by combining the individual systems? We cannot estimate this from the results tables, since, combinatorially, there are many ways to obtain a given score. However, we can estimate it from the prediction overlap between systems. The N × N labeled matrix in figure <ref type="figure" target="#fig_6">7</ref> shows how the N systems relate to each other. Each cell compares the predictions of two systems a and b in the joint supersense and MWE task. The value of a cell T a,b is the number of correct predictions made by a that were not correctly predicted by b. This is an asymmetric measure of predictive similarity. A single low number indicates one out of two things: either the systems are similar, or a is better than b. When the sum T a,b + T b,a is small, the two systems make similar predictions.</p><p>Clustering the systems in figure <ref type="figure" target="#fig_6">7</ref> (shown on the left side of the plot) results in groups that correspond to the ranking in table 4. Inside the cluster of systems ranked at 1, the asymmetric predictive advantage ranges between 267 and 469. Lower-ranked systems all have a smaller predictive advantage with respect to the top-ranked systems. The best combination system would thus likely be between two of the rank-1 systems. However, the gains are small, and overall the systems seem to extract the same knowledge, or subsets of the same knowledge, out of the training data. Each cell compares the predictions of two systems i and j with respect to a gold standard. The value in the i, j-th cell is the number of predictions that i got right but j did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This task featured a broad-coverage lexical semantic analysis task that combines MWE identification and supersense tagging. The semantic tagset strikes a balance between the extremely difficult fine-grained distinctions in classical WSD, and the restrictiveness of the NER task. To guard against domain bias, we provided training data from two different genres, namely online reviews and tweets, as well as a test-only data set with TED talk transcripts. The training and test data sets are publicly available at https://github.com/dimsum16/dimsum-data.</p><p>The best scoring systems obtained 57.7% F 1 on a composite measure over the two subtasks of MWE and supersense tagging, averaged over the three test domains. This level of performance suggests that the task is not yet resolved. Furthermore, our error analysis suggests that the submitted systems arrived at similar generalizations from the training data. Substantially improving performance would thus seem to require novel approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>16 I.e., TweetNLP clusters (http://www.cs.cmu.edu/~ark/ TweetNLP/) and the Yelp Academic Dataset clusters used in AMALGrAM (http://www.cs.cmu.edu/~ark/LexSem/).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: A REVIEWS sentence with MWE and supersense analyses: gold above and hypothetical prediction below. MWE precision of the bottom annotation relative to the top one is 2 5. (Note that a link between words w 1 and w 2 is "matched" if, in the other annotation, there is a path between w 1 and w 2 .) The MWE recall value is 3 4. Supersense precision and recall are both 1 2. Combined precision/recall scores add the respective subscores' numerators and denominators: thus, combined precision is 2+1 5+2 = 3 7, and combined recall is 3+1 4+2 = 2 3. Combined F 1 is their harmonic mean, i.e. 12 23.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of systems predicting the correct supersense (for tokens where there is a gold supersense).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Easy and hard supersense decisions. Shown in blue in the left side of the plot is the proportion of instances of the given supersense type where at most one system gave the wrong answer. On the right side in red is the corresponding figure where at most one system gave the right answer. Supersenses are sorted by corpus frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: System clusters. Each cell compares the predictions of two systems i and j with respect to a gold standard. The value in the i, j-th cell is the number of predictions that i got right but j did not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The 41 noun and verb supersenses in WordNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Source datasets and preprocessing to obtain 17-tag Universal POS tags (UPOS) version 1.2. Most sources already contained some form of POS tags, which we automatically converted to UPOS. We added missing necessary distinctions-e.g., UD-style UPOS distinguishes auxiliaries from main verbs, butPetrov-style  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Annotated datasets: status of lexical semantic annotations (retained, revised, or newly annotated for this task) per subcorpus; word token and MWE instance counts; number and proportion (out of all MWEs) that are gappy; proportion of tokens that belong to an MWE; number of units labeled with a noun supersense, and proportion that are MWEs; likewise for verb supersenses.Additional statistics relatively consistent across domains: MWEs per word: mean/median .055 (lowest: TED, .044; highest: STREUSLE, .090). Supersenses per word: mean/median 0.3. Just 8 MWEs contain more than one gap (all in STREUSLE or Ritter).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>http://www.ark.cs.cmu.edu/LexSem/ 7 The PTB-to-UPOS conversion script is available at: http://tiny.cc/ptb2upos 8 The supersense-annotated tweets are available at https:// github.com/coastalcph/supersense-data-twitter.</figDesc><table /><note>6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Counts of MWE occurrences, grouped by the POS of the first word in the MWE. Blue bars represent POSes that tend to start nominal MWEs; red bars roughly capture verbal MWEs.</figDesc><table><row><cell>Tweets: TED:</cell><cell>Tweebank NAIST-NTT</cell><cell>95 14</cell><cell>63 26</cell><cell>36 12</cell><cell>17 1</cell><cell>4 2</cell><cell>5 0</cell><cell>91 31</cell><cell>16 2</cell><cell>10 0</cell><cell>17 5</cell><cell>0 0</cell><cell>1 0</cell><cell>0 0</cell><cell>4 0</cell><cell>1 0</cell><cell>2 0</cell></row><row><cell>TED:</cell><cell>IWSLT test</cell><cell>3</cell><cell>19</cell><cell>12</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>13</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MWE instances by first word's POS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Reviews: STREUSLE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PROPN NOUN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Reviews: Trustpilot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ADJ DET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PRON</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Tweets: Lowlands</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NUM VERB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Tweets: Ritter</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ADV AUX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ADP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Tweets: Tweebank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SCONJ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TED: NAIST-NTT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CONJ INTJ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TED: IWSLT test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PUNCT X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0%</cell><cell></cell><cell>25%</cell><cell></cell><cell>50%</cell><cell></cell><cell>75%</cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 2 summarizes MWEs in the seven subcorpora with respect to syntactic status. Colors represent the POS tag of the first word in the MWE. Starting with proper nouns, the blue bars indicate POS tags that tend to begin nominal MWEs (noun, adjective, determiner, etc.). Red bar POS tags are characteristic of verbal MWEs. The remaining bars are prepositional (dark green) and other miscellaneous tags, which collectively comprise no more than 10% of the MWEs in each subcorpus.It is worth noting that in this plot, subcorpora within the same domain are sometimes more diver-</figDesc><table /><note>13 https://github.com/coastalcph/rungsted 14 http://hdl.handle.net/11234/1-1548</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Main results on the test set. Scores are tridomain combined F 1 percentages. Resource conditions are described in §6.2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Per-domain evaluation results. Figures are F 1 percentages. The best value in each section and column is in bold. Refer to table 4 for the identities of the systems.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table5offers a more detailed breakdown by domain and subscore (MWEs vs. supersenses vs. combined). The best scores are about 57% for both MWEs and supersenses. Systems S214 and S249 are the clear winners: the former is better in the surprise TED domain-particularly TED MWEs (by nearly 4 points). The latter is slightly better in TWEETS, and the systems are quite close in REVIEWS (the domain with the most training data).S214 and S249 were in the open condition, taking advantage of additional resources. The best system in the closed condition is S248, which is very similar to S249-and recall that its predictions, overall, are not statistically worse.Table 5 reveals one striking difference, however: in MWE scores for TWEETS, S249 bests S248 by nearly 7 points.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://dimsum16.github.io/ 2 Though our data set is limited to English, the representation is applicable to other languages: see §2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Evaluations used English SemCor<ref type="bibr" target="#b7">(Ciaramita and Altun, 2006;</ref><ref type="bibr" target="#b26">Paaß and Reichartz, 2009)</ref>, English-Italian MultiSem-Cor<ref type="bibr" target="#b29">(Picca et al., 2008</ref><ref type="bibr" target="#b28">(Picca et al., , 2009</ref> Attardi et al., 2010), the Italian Syntactic-Semantic Treebank and Italian Wikipedia(Attardi et al., 2010;<ref type="bibr" target="#b36">Rossi et al., 2013)</ref>, Chinese Cilin<ref type="bibr" target="#b31">(Qiu et al., 2011)</ref>, Arabic Wikipedia, and the Danish CLARIN Reference Corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/dimsum16/dimsum-data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">This computation on the basis of links is a slight simplification of the MUC coreference measure<ref type="bibr" target="#b49">(Vilain et al., 1995)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Sam Gibbon for the lexical semantic annotation, which was generously supported by Carlsberg infrastructure grant No. CF14-0694.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><forename type="middle">Dei</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><forename type="middle">Di</forename><surname>Pietro</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">556</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A resource and tool for super-sense tagging of Italian texts</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stelios Piperidis, Mike Rosner, and Daniel Tapias</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<meeting><address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Proc. of LREC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiword expressions</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Nitin</forename><surname>Indurkhya</surname></persName>
			<persName><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Taylor and Francis Group</orgName>
		</respStmt>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">English Web Treebank</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<idno>LDC2012T13</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia, PA. URL http</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UTU at SemEval-2016 Task 10: Binary Classification for Expression Detection (BCED)</title>
		<author>
			<persName><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WIT 3 : Web Inventory of Transcribed and Translated Talks</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EAMT</title>
				<editor>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
			<persName><forename type="first">Andy</forename><surname>Way</surname></persName>
		</editor>
		<meeting>of EAMT<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MWUaware part-of-speech tagging with a CRF model and lexical resources</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Sigogne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World</title>
				<meeting>of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UFRGS&amp;LIF at SemEval-2016 Task 10: Rule-based MWE identification and predominantsupersense tagging</title>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing models for identifying multiword expressions</title>
		<author>
			<persName><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="227" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic factual question generation from text</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<ptr target="http://www.ark" />
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Pittsburgh; Pennsylvania</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UW-CSE at SemEval-2016 Task 10: Detecting multiword expressions and supersenses using double-chained conditional random fields</title>
		<author>
			<persName><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User review sites as a resource for large-scale sociolinguistic studies</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
				<meeting>of WWW<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying metaphorical word use with tree kernels</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Workshop on Metaphor in NLP</title>
				<meeting>of the First Workshop on Metaphor in NLP<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tagging performance correlates with author age</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
				<meeting>of ACL-IJCNLP<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="483" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What&apos;s in a preposition? Dimensions of sense disambiguation for an interesting word class</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
				<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="454" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">More or less supervised supersense tagging of Twitter</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of *SEM</title>
				<meeting>of *SEM<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ICL-HD at SemEval-2016 Task 10: Improving the detection of minimal semantic units and their meanings with an ontology and word embeddings</title>
		<author>
			<persName><forename type="first">Angelika</forename><surname>Kirilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Krauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<editor>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
			<persName><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</editor>
		<meeting>of SemEval<address><addrLine>San Diego, California, USA. Lingpeng Kong, Nathan Schneider; Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1001" to="1012" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supersense tagging for Danish</title>
		<author>
			<persName><forename type="first">Alonso</forename><surname>Héctor Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sussi</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanni</forename><surname>Nimb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Hartvig Sørensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Braasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolette</forename><surname>Sandford Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NODALIDA</title>
				<editor>
			<persName><forename type="first">Beáta</forename><surname>Megyesi</surname></persName>
		</editor>
		<meeting>of NODALIDA<address><addrLine>Vilnius, Lithuania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT</title>
				<meeting>of HLT<address><addrLine>Plainsboro, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The naist-ntt ted talk treebank</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kepa</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyaz</forename><forename type="middle">Ahmad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantza</forename><surname>Diaz De Ilarraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaja</forename><surname>Dobrovoljc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaž</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koldo</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berta</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Mȃrȃnduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verginica</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petya</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName><surname>Øvrelid</surname></persName>
		</author>
		<ptr target="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548,LIN-DAT/CLARINdigitallibraryat" />
		<editor>Sampo Pyysalo, Loganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Kiril Simov, Aaron Smith, Jan Štěpánek, Alane Suhr, Zsolt Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Uematsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zdeněk Žabokrtský, Daniel Zeman, and Hanzhi Zhu</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting semantic constraints for estimating supersenses with CRFs</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Paaß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Reichartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Ninth SIAM International Conference on Data Mining</title>
				<meeting>of the Ninth SIAM International Conference on Data Mining<address><addrLine>Sparks, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="485" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1104.2086</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bridging languages by SuperSense entity tagging</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Picca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Campora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NEWS</title>
				<meeting>of NEWS<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
	<note>Suntec</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supersense Tagger for Italian</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Picca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stelios Piperidis, and Daniel Tapias</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<meeting><address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-01" />
			<biblScope unit="page" from="2386" to="2390" />
		</imprint>
	</monogr>
	<note>Proc. of LREC</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning part-of-speech taggers with inter-annotator agreement loss</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
				<meeting>of EACL<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining contextual and structural information for supersense tagging of Chinese unknown words</title>
		<author>
			<persName><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing: Proceedings of the 12th International Conference (CICLing&apos;11)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6608</biblScope>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A broad evaluation of techniques for automatic acquisition of multiword expressions</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Vitor De Araujo</surname></persName>
		</author>
		<author>
			<persName><surname>Villavicencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2012 Student Research Workshop</title>
				<meeting>of ACL 2012 Student Research Workshop<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">Proc. of the Third ACL Workshop on Very Large Corpora</title>
				<meeting>of the Third ACL Workshop on Very Large Corpora<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Description and results of the SuperSense tagging task</title>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Stefano Dei Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Di Pietro</surname></persName>
		</author>
		<author>
			<persName><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluation of Natural Language and Speech Tools for Italian</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
			<persName><forename type="first">Francesco</forename><surname>Cutugno</surname></persName>
			<persName><forename type="first">Mauro</forename><surname>Falcone</surname></persName>
			<persName><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Scherbakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<title level="m">VectorWeavers at</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Task 10: From incremental meaning to semantic unit (phrase by phrase)</title>
		<author>
			<persName><forename type="first">-</forename><surname>Semeval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative lexical semantic segmentation with gaps: running the MWE gamut</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Danchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supersense tagging for Arabic: the MT-in-the-middle attack</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="661" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coarse lexical semantic annotation with supersenses: an Arabic case study</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Comprehensive annotation of multiword expressions in a social web corpus</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Onuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kazour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Danchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Mordowanec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrietta</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
			<persName><forename type="first">Hrafn</forename><surname>Loftsson</surname></persName>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
			<persName><forename type="first">Asuncion</forename><surname>Moreno</surname></persName>
			<persName><forename type="first">Jan</forename><surname>Odijk</surname></persName>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>of LREC<address><addrLine>Reykjavík, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="455" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A corpus and model integrating multiword expressions and supersenses</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1537" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hierarchy with, of, and for preposition supersenses</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of The 9th Linguistic Annotation Workshop</title>
				<meeting>of The 9th Linguistic Annotation Workshop<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="112" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">WHUNlp at SemEval-2016 Task 10: A pilot study in detecting minimal semantic units and their meanings using supervised models</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ISI: Automatic classification of relations between nominals using a maximum entropy classifier</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-lingual metaphor detection using common semantic features</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mukomel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Workshop on Metaphor in NLP</title>
				<meeting>of the First Workshop on Metaphor in NLP<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Augmenting English adjective senses with supersenses</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
			<persName><forename type="first">Hrafn</forename><surname>Loftsson</surname></persName>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
			<persName><forename type="first">Asuncion</forename><surname>Moreno</surname></persName>
			<persName><forename type="first">Jan</forename><surname>Odijk</surname></persName>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>of LREC<address><addrLine>Reykjavík, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4359" to="4365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MUC-6</title>
				<meeting>of MUC-6<address><addrLine>Columbia, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Dennis Connolly, and Lynette Hirschman</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting noun compounds and light verb constructions: a contrastive study</title>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">István</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World</title>
				<meeting>of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dependency parsing for identifying Hungarian light verb constructions</title>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">János</forename><surname>Zsibrita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">István</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sixth International Joint Conference on Natural Language Processing</title>
				<meeting>of the Sixth International Joint Conference on Natural Language essing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MELB-YB: Preposition sense disambiguation using rich semantic features</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
				<meeting>of SemEval<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
