<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<email>navigli@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Sapienza</orgName>
								<orgName type="institution">Università di Roma Viale Regina Elena</orgName>
								<address>
									<postCode>295 -00161</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
							<email>vannella@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Sapienza</orgName>
								<orgName type="institution">Università di Roma Viale Regina Elena</orgName>
								<address>
									<postCode>295 -00161</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification. Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query. The task enables the end-to-end evaluation and comparison of systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word ambiguity is a pervasive issue in Natural Language Processing. Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see <ref type="bibr" target="#b16">(Navigli, 2009;</ref><ref type="bibr" target="#b17">Navigli, 2012)</ref> for a survey).</p><p>Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, provides fine-grained distinctions which make the disambiguation task quite difficult even for humans <ref type="bibr" target="#b6">(Edmonds and Kilgarriff, 2002;</ref><ref type="bibr" target="#b19">Snyder and Palmer, 2004)</ref>, although disagreements can be solved to some extent with graph-based methods <ref type="bibr" target="#b15">(Navigli, 2008)</ref>. On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses. In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output. Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.</p><p>Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively. A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds. Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification <ref type="bibr">(Navigli and Crisafulli, 2010;</ref><ref type="bibr" target="#b5">Di Marco and Navigli, 2013)</ref>. In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.</p><p>The Semeval-2013 task described in this paper 1 adopts the evaluation framework of Di Marco and <ref type="bibr" target="#b5">Navigli (2013)</ref>, and extends it to both WSD and WSI systems. The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic <ref type="bibr" target="#b0">(Agirre and Soroa, 2007;</ref><ref type="bibr" target="#b13">Manandhar et al., 2010)</ref>, and enabling a fair comparison between the two disambiguation paradigms. Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning). For instance, consider the target query apple and the following 3 search result snippets:</p><p>1. Apple Inc., formerly Apple Computer, Inc., is...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The science of apple growing is called pomology...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Apple designs and creates iPod and iTunes... Participating systems were requested to produce a clustering that groups snippets conveying the same meaning of the input query apple, i.e., ideally {1, 3} and {2} in the above example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task setup</head><p>For each ambiguous query the task required participating systems to cluster the top ranking snippets returned by a search engine (we used the Google Search API). WSI systems were required to identify the meanings of the input query and cluster the snippets into semantically-related groups according to their meanings. Instead, WSD systems were requested to sense-tag the given snippets with the appropriate senses of the input query, thereby implicitly determining a clustering of snippets (i.e., one cluster per sense).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>We created a dataset of 100 ambiguous queries. The queries were randomly sampled from the AOL search logs so as to ensure that they had been used in real search sessions. Following previous work on the topic <ref type="bibr" target="#b5">Di Marco and Navigli, 2013)</ref> we selected those queries for which a sense inventory exists as a disambiguation page in the English Wikipedia 2 . This guaranteed that the selected queries consisted of either a single word or a multiword expression for which we had a collaborativelyedited list of meanings, including lexicographic and encyclopedic ones. We discarded all queries made up of &gt; 4 words, since the length of the great majority of queries lay in the range <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>. In Table <ref type="table" target="#tab_0">1</ref> we compare the percentage distribution of 1-to 4-word queries in the AOL query logs against our dataset of queries. Note that we increased the percentage of 3-and 4-word queries in order to have a significant coverage of those lengths. Anyhow, in both cases most queries contained from 1 to 2 words. Note that the reported percentage distributions of query length is different from recent statistics for two reasons: first, over the years users have increased the average number of words per query in order to refine their searches; second, we selected only queries which were either single words (e.g., apple) or multi-word expressions (e.g., mortal kombat), thereby discarding several long queries composed of different words (such as angelina jolie actress).</p><p>Finally, we submitted each query to Google search and retrieved the 64 top-ranking results returned for each query. Therefore, overall the dataset consists of 100 queries and 6,400 results. Each search result includes the following information: page title, URL of the page and snippet of the page text. We show an example of search result for the apple query in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Annotation</head><p>For each query q we used Amazon Mechanical Turk 3 to annotate each query result with the most suitable sense. The sense inventory for q was obtained by listing the senses available in the Wikipedia disambiguation page of q augmented with additional options from the classes obtained from the section headings of the disambiguation page plus the OTHER catch-all meaning. For instance, consider the apple query. We show its disambiguation page in Figure <ref type="figure" target="#fig_0">2</ref>. The sense inventory for apple was made up of the senses listed in that page (e.g., MALUS, APPLE INC., APPLE BANK, etc.) plus the set of generic classes OTHER PLANTS AND PLANT PARTS, OTHER COMPANIES, OTHER FILMS, plus OTHER.</p><p>For each query we ensured that three annotators tagged each of the 64 results for that query with the most suitable sense among those in the sense inventory (selecting OTHER if no sense was appropriate). Specifically, each Turker was provided with the following instructions: "The goal is annotating the search result snippets returned by Google for a given query with the appropriate meaning among those available (obtained from the Wikipedia disambiguation page for the query). You have to select the meaning that you consider most appropriate". No constraint on the age, gender and citizenship of the annotators was imposed. However, in order to avoid random tagging of search results, we provided 3 gold-standard result annotations per query, which could be shown to the Turker more than once during the annotation process. In the case (s)he failed to annotate the gold items, the annotator was automatically excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inter-Annotator Agreement and Adjudication</head><p>In order to determine the reliability of the Turkers' annotations, we calculated the individual values of Fleiss' kappa κ <ref type="bibr" target="#b8">(Fleiss, 1971)</ref> for each query q and then averaged them:</p><formula xml:id="formula_0">κ = q∈Q κ q |Q| ,<label>(1)</label></formula><p>where κ q is the Fleiss' kappa agreement of the three annotators who tagged the 64 snippets returned by the Google search engine for the query q ∈ Q, and Q is our set of 100 queries. We obtained an average value of κ = 0.66, which according to Landis and Koch (1977) can be seen as substantial agreement, with a standard deviation σ = 0.185.</p><p>In Table <ref type="table" target="#tab_2">2</ref> we show the agreement distribution of our 6400 snippets, distinguishing between full agreement (3 out of 3), majority agreement (2 out of 3), and no agreement. Most of the items were annotated with full or majority agreement, indicating that the manual annotation task was generally doable for the layman. We manually checked all the cases of majority agreement, correcting only 7.92% of the majority adjudications, and manually adjudicated all the snippets for which there was no agreement. We observed during adjudication that in many cases the disagreement was due to the existence of subtle sense distinctions, like between MORTAL KOM-BAT (VIDEO GAME) and MORTAL <ref type="bibr">KOMBAT (2011 VIDEO GAME)</ref>  per query on average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scoring</head><p>Following Di Marco and Navigli (2013), we evaluated the systems' outputs in terms of the snippet clustering quality (Section 3.1) and the snippet diversification quality (Section 3.2). Given a query q ∈ Q and the corresponding set of 64 snippet results, let C be the clustering output by a given system and let G be the gold-standard clustering for those results. Each measure M (C, G) presented below is calculated for the query q using these two clusterings. The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of M (C, G) obtained for each single test query q ∈ Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clustering Quality</head><p>The first evaluation concerned the quality of the clusters produced by the participating systems. Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely:</p><p>• Rand Index <ref type="bibr" target="#b18">(Rand, 1971</ref>);</p><p>• Adjusted Rand Index <ref type="bibr" target="#b10">(Hubert and Arabie, 1985)</ref>;</p><p>• Jaccard Index <ref type="bibr" target="#b11">(Jaccard, 1901)</ref>;</p><p>• F1 measure <ref type="bibr" target="#b21">(van Rijsbergen, 1979)</ref>.</p><p>The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows:</p><formula xml:id="formula_1">RI(C, G) = TP + TN TP + FP + FN + TN , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where TP is the number of true positives, i.e., snippet pairs which are in the same cluster both in C and G, TN is the number of true negatives, i.e., pairs which are in different clusters in both clusterings, and FP and FN are, respectively, the number of false positives and false negatives. RI ranges between 0 and 1, where 1 indicates perfect correspondence. Adjusted Rand Index (ARI) is a development of Rand Index which corrects the RI for chance agreement and makes it vary according to expectaction:</p><formula xml:id="formula_3">H H H H H H G C C 1 C 2 • • • C m Sums G 1 n 11 n 12 • • • n 1m a 1 G 2 n 21 n 22 • • • n 2m a 2 . . . . . . . . . . . . . . . . . . G g n g1 n g2 • • • n gm a g Sums b 1 b 2 • • • b m N</formula><formula xml:id="formula_4">ARI(C, G) = RI(C, G) − E(RI(C, G)) max RI(C, G) − E(RI(C, G))</formula><p>.</p><p>(3) where E(RI(C, G)) is the expected value of the RI. Using the contingency table reported in Table <ref type="table" target="#tab_3">3</ref> we can quantify the degree of overlap between C and G, where n ij denotes the number of snippets in common between G i and C j (namely, n ij = |G i ∩ C j |), a i and b j represent, respectively, the number of snippets in G i and C j , and N is the total number of snippets, i.e., N = 64. Now, the above equation can be reformulated as:</p><formula xml:id="formula_5">ARI(C,G)= ij ( n ij 2 ) −[ i ( a i 2 ) j ( b j 2 ) ]/ ( N 2 ) 1 2 [ i ( a i 2 ) + j ( b j 2 ) ]−[ i ( a i 2 ) j ( b j 2 ) ]/ ( N 2 )</formula><p>.</p><p>(4) The ARI ranges between −1 and +1 and is 0 when the index equals its expected value.</p><p>Jaccard Index (JI) is a measure which takes into account only the snippet pairs which are in the same cluster both in C and G, i.e., the true positives (TP), while neglecting true negatives (TN), which are the vast majority of cases. JI is calculated as follows:</p><formula xml:id="formula_6">JI(C, G) = TP TP + FP + FN .<label>(5)</label></formula><p>Finally, the F1 measure calculates the harmonic mean of precision (P) and recall (R). Precision determines how accurately the clusters of C represent the query meanings in the gold standard G, whereas recall measures how accurately the different meanings in G are covered by the clusters in C. We follow <ref type="bibr" target="#b4">Crabtree et al. (2005)</ref> and define the precision of a cluster C j ∈ C as follows:</p><formula xml:id="formula_7">P (C j ) = |C s j | |C j | ,<label>(6)</label></formula><p>where C s j is the intersection between C j ∈ C and the gold cluster G s ∈ G which maximizes the cardinality of the intersection. The recall of a query sense s is instead calculated as:</p><formula xml:id="formula_8">R(s) = | C j ∈C s C s j | n s ,<label>(7)</label></formula><p>where C s is the subset of clusters of C whose majority sense is s, and n s is the number of snippets tagged with query sense s in the gold standard. The total precision and recall of the clustering C are then calculated as:</p><formula xml:id="formula_9">P = C j ∈C P (C j )|C j | C j ∈C |C j | ; R = s∈S R(s)n s s∈S n s (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where S is the set of senses in the gold standard G for the given query (i.e., |S| = |G|). The two values of P and R are then combined into their harmonic mean, namely the F1 measure:</p><formula xml:id="formula_11">F 1(C, G) = 2P R P + R . (<label>9</label></formula><formula xml:id="formula_12">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering Diversity</head><p>Our second evaluation is aimed at determining the impact of the output clustering on the diversification of the top results shown to a Web user. To this end, we applied an automatic procedure for flattening the clusterings produced by the participating systems to a list of search results. Given a clustering C = (C 1 , C 2 , . . . , C m ), we add to the initially empty list the first element of each cluster C j (j = 1, . . . , m); then we iterate the process by selecting the second element of each cluster C j such that |C j | ≥ 2, and so on. The remaining elements returned by the search engine, but not included in any cluster of C, are appended to the bottom of the list in their original order. Note that systems were asked to sort snippets within clusters, as well as clusters themselves, by relevance.</p><p>Since our goal is to determine how many different meanings are covered by the top-ranking search results according to the output clustering, we used the measures of S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at recall r) <ref type="bibr" target="#b22">(Zhai et al., 2003)</ref>.</p><p>S-recall@K determines the ratio of different meanings for a given query q in the top-K results returned:</p><formula xml:id="formula_13">S-recall@K = |{sense(r i ) : i ∈ {1, . . . , K}}| g ,<label>(10)</label></formula><p>where sense(r i ) is the gold-standard sense associated with the i-th snippet returned by the system, and g is the total number of distinct senses for the query q in our gold standard.</p><p>S-precision@r instead determines the ratio of different senses retrieved for query q in the first K r snippets, where K r is the minimum number of top results for which the system achieves recall r. The measure is defined as follows:</p><formula xml:id="formula_14">S-precision@r = | ∪ Kr i=1 sense(r i )| K r .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compared the participating systems with two simple baselines:</p><p>• SINGLETONS: each snippet is clustered as a separate singleton cluster (i.e., |C| = 64).</p><p>• ALL-IN-ONE: all snippets are clustered into a single cluster (i.e., |C| = 1).</p><p>These baselines are important in that they make explicit the preference of certain quality measures towards clusterings made up with a small or large number of clusters.</p><p>4 Systems 5 teams submitted 10 systems, out of which 9 were WSI systems, while 1 was a WSD system, i.e., using the Wikipedia sense inventory for performing the disambiguation task. All systems could exploit the information provided for each search result, i.e., URL, page title and result snippet. WSI systems were requested to use unannotated corpora only.  We asked each team to provide information about their systems. In Table <ref type="table" target="#tab_5">4</ref> we report the resources used by each system. The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus <ref type="bibr">(Graff, 2003, 1st edition)</ref>, whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky <ref type="bibr" target="#b1">(Baroni et al., 2009)</ref> and a distributional thesaurus obtained from the Leipzig Corpora Collection 6 <ref type="bibr" target="#b3">(Biemann et al., 2007)</ref>. SATTY-APPROACH1 just uses snippets.</p><p>The only participating WSD system, RAKESH, uses the YAGO hierarchy <ref type="bibr" target="#b20">(Suchanek et al., 2008)</ref> together with DBPedia abstracts <ref type="bibr">(Bizer et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We show the results of RI and ARI in Table <ref type="table" target="#tab_7">5</ref>. The best performing systems are those from the HDP team, with considerably higher RI and ARI. The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH. SINGLETONS perform well with RI, but badly when chance agreement is taken into account.</p><p>As for F1 and JI, whose values are shown in Table <ref type="table" target="#tab_9">6</ref>, the two HDP systems again perform best in terms of F1, and are on par with UKP-WSI-WACKY-LLR in terms of JI. The third best approach in terms of F1 is again SATTY-APPROACH1, which however per-  To get more insights into the performance of the various systems, we calculated the average number of clusters per clustering produced by each system and compared it with the gold standard average. We also computed the average cluster size, i.e., the average number of snippets per cluster. The statistics are shown in Table <ref type="table" target="#tab_10">7</ref>. Interestingly, the best performing systems are those with the cluster number and average number of clusters closest to the gold standard ones. This finding is also confirmed by Figure <ref type="figure" target="#fig_1">3</ref>, where we draw each system according to its average values regarding cluster number and size: again the distance from the gold standard is meaningful.</p><p>We now move to the diversification perfor-    Our annotation experience showed that the Wikipedia sense inventory, augmented with our generic classes, is a good choice for semantically tagging search results, in that it covers most of the meanings a Web user might be interested in. In fact, only 20% of the snippets was annotated with the OTHER class.</p><p>Future work might consider large-scale multilingual lexical resources, such as BabelNet <ref type="bibr" target="#b14">(Navigli and Ponzetto, 2012)</ref>, both as sense inventory and for performing the search result clustering and diversification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Wikipedia disambiguation page of Apple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average cluster size (ACS) vs. average number of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of search result for the apple query, including: page title, URL and snippet.</figDesc><table><row><cell>query length</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>AOL logs</cell><cell cols="4">45.89 40.98 10.98 2.32</cell></row><row><cell>our dataset</cell><cell cols="4">40.00 40.00 15.00 5.00</cell></row><row><cell cols="5">Table 1: Percentage distribution of AOL query lengths</cell></row><row><cell cols="5">(first row) vs. the queries sampled for our task (second</cell></row><row><cell>row).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, or between THE DA VINCI CODE and INACCURACIES IN THE DA VINCI CODE.The average number of senses associated with the search results of each query was 7.69 (higher than in previous datasets, such as AMBI-ENT 4 +MORESQUE 5 , which associates 5.07 senses</figDesc><table><row><cell></cell><cell cols="3">Full agr. Majority Disagr.</cell></row><row><cell>% snippets</cell><cell>66.70</cell><cell>25.85</cell><cell>7.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Percentage of snippets with full agreement, majority agreement and full disagreement.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Contingency table for the clusterings G and C.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Resources used for WSI/WSD.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results for Rand Index (RI) and Adjusted Rand Index (ARI), sorted by RI. forms badly in terms of JI. The SINGLETONS baseline clearly obtains the best F1 performance, but the worst JI results. The ALL-IN-ONE baseline outperforms all other systems with the JI measure, because TN are not considered, which favours large clusters.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results for Jaccard Index (JI) and F1 measure.</figDesc><table><row><cell></cell><cell>System</cell><cell cols="2"># cl. ACS</cell></row><row><cell></cell><cell>GOLD STANDARD</cell><cell cols="2">7.69 11.56</cell></row><row><cell></cell><cell>HDP-CLUSTERS-LEMMA</cell><cell cols="2">6.63 11.07</cell></row><row><cell></cell><cell cols="3">HDP-CLUSTERS-NOLEMMA 6.54 11.68</cell></row><row><cell></cell><cell>SATTY-APPROACH1</cell><cell>9.90</cell><cell>6.46</cell></row><row><cell></cell><cell>UKP-WSI-WP-PMI</cell><cell cols="2">5.86 30.30</cell></row><row><cell>WSI</cell><cell>DULUTH.SYS7.PK2 UKP-WSI-WP-LLR2</cell><cell cols="2">3.01 25.15 4.17 21.87</cell></row><row><cell></cell><cell>UKP-WSI-WACKY-LLR</cell><cell cols="2">3.64 32.34</cell></row><row><cell></cell><cell>DULUTH.SYS9.PK2</cell><cell cols="2">3.32 19.84</cell></row><row><cell></cell><cell>DULUTH.SYS1.PK2</cell><cell cols="2">2.53 26.45</cell></row><row><cell cols="2">WSD RAKESH</cell><cell>9.07</cell><cell>2.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Average number of clusters (# cl.) and average cluster size (ACS).</figDesc><table><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">gold-standard</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell cols="2">hdp-lemma</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">hdp-nolemma</cell></row><row><cell>average number of clusters</cell><cell>10 15 20 25 30</cell><cell></cell><cell></cell><cell cols="2">sys1.pk2 sys7.pk2 sys9.pk2 rakesh satty-approach1 ukp-wsi-wacky-llr ukp-wsi-wp-llr2 ukp-wsi-wp-pmi</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">average cluster size (ACS)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cs.york.ac.uk/semeval-2013/task11/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://en.wikipedia.org/wiki/Disambiguation page</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.mturk.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://credo.fub.it/ambient 5 http://lcl.uniroma1.it/moresque</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234.</p><p>We thank Antonio Di Marco and David A. Jurgens for their help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and 9, respectively. Here we find that, again, the HDP team obtains the best performance, followed by RAKESH. We note however that not all systems optimized the order of clusters and cluster snippets by relevance.</p><p>We also graph the diversification performance trend of S-recall@K and S-precision@r in Figures 4 and 5 for K = 1, . . . , 25 and r ∈ {40, 50, . . . , 100}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Directions</head><p>One of the aims of the SemEval-2013 task on Word Sense Induction &amp; Disambiguation within an End User Application was to enable an objective comparison of WSI and WSD systems when integrated into Web search result clustering and diversification. The task is a hard one, in that it involves clustering, but provides clear-cut evidence that our end-to-end application framework overcomes the limits of previous in-vitro evaluations. Indeed, the systems which create good clusters and better diversify search results, i.e., those from the HDP team, achieve good performance across all the proposed measures, with no contradictory evidence.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 02: Evaluating word sense induction and discrimination systems</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4 th International Workshop on Semantic Evaluations (SemEval-2007)</title>
				<meeting>the 4 th International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The WaCky Wide Web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Full-subtopic retrieval with keyphrase-based search results clustering</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano D'</forename><surname>Amico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Web Intelligence</title>
				<meeting>Web Intelligence<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Leipzig corpora collection -monolingual corpora of standard size</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Quasthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Corpus Linguistic</title>
		<title level="s">and Sebastian Hellmann</title>
		<meeting>Corpus Linguistic<address><addrLine>Birmingham, UK. Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören Auer, Christian Becker, Richard Cyganiak</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154" to="165" />
		</imprint>
	</monogr>
	<note>Dbpedia -a crystallization point for the web of data</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving web clustering by cluster selection</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Crabtree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Andreae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence</title>
				<meeting>the 2005 IEEE/WIC/ACM International Conference on Web Intelligence<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clustering and diversifying web search results with graphbased word sense induction</title>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">Di</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on evaluating word sense disambiguation systems</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="291" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychological Bulletin</title>
				<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">English Gigaword</title>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Report, LDC2003T05, Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparing Partitions</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des alpes et des jura</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bulletin de la Société Vaudoise des Sciences Naturelles</title>
				<imprint>
			<date type="published" when="1901" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing word senses to improve web search result clustering</title>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><forename type="middle">S</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Uppsala, Sweden; Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
	<note>Proceedings of the 5th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ba-belNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structural approach to the automatic adjudication of word sense disagreements</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="310" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: a survey</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A quick tour of word sense disambiguation, induction and related approaches</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOF-SEM)</title>
				<meeting>the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOF-SEM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The English all-words task</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3 rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3)</title>
				<meeting>the 3 rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YAGO: A large ontology from wikipedia and wordnet</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Information Retrieval</title>
		<author>
			<persName><forename type="first">Cornelis</forename><surname>Joost Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
	<note type="report_type">Butterworths</note>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
				<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
