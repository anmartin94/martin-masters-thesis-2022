<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The SENSEVAL-3 English Lexical Sample Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Texas Dallas</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute University of Southern California Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
							<email>adam.kilgarriff@itri.brighton.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Information Technology Research Institute</orgName>
								<orgName type="institution">University of Brighton</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The SENSEVAL-3 English Lexical Sample Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation.</p><p>This task is a follow-up to similar tasks organized during the SENSEVAL-1 <ref type="bibr">(Kilgarriff and Palmer, 2000)</ref> and <ref type="bibr">SENSEVAL-2 (Preiss and Yarowsky, 2001)</ref> evaluations.</p><p>The main changes in this year's evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Building a Sense Tagged Corpus with Volunteer Contributions over the Web</head><p>The sense annotated corpus required for this task was built using the Open Mind Word Expert system <ref type="bibr" target="#b1">(Chklovski and Mihalcea, 2002)</ref> 1 . To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web. Sense tagged examples are collected using a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set of sentences from a large textual corpus. These examples are presented to the contributors, who are asked to select the most appropriate sense for the target word in each sentence. The selection is made using checkboxes, which list all possible senses of the current target word, plus two additional choices, "unclear" and "none of the above." Although users are encouraged to select only one meaning per word, the selection of two or more senses is also possible. The results of the classification submitted by other users are not presented to avoid artificial biases.</p><p>Similar to the annotation scheme used for the English lexical sample at SENSEVAL-2, we use a "tag until two agree" scheme, with an upper bound on the number of annotations collected for each item set to four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Source Corpora</head><p>The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC). Earlier versions of OMWE also included data from the Penn Treebank corpus, the Los Angeles Times collection as provided during TREC conferences (http://trec.nist.gov), and Open Mind Common Sense (http://commonsense.media.mit.edu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sense Inventory</head><p>The sense inventory used for nouns and adjectives is WordNet 1.7.1 <ref type="bibr" target="#b2">(Miller, 1995)</ref>, which is consistent with the annotations done for the same task during SENSEVAL-2. Verbs are instead annotated with senses from Wordsmyth (http://www.wordsmyth.net/). The main reason motivating selection of a different sense inventory is the   <ref type="table" target="#tab_1">1</ref> presents the number of words under each part of speech, and the average number of senses for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Word Expressions</head><p>For this evaluation exercise, we decided to isolate the task of semantic tagging from the task of identifying multi-word expressions; we applied a filter that removed all examples pertaining to multi-word expressions prior to the tagging phase. Consequently, the training and test data sets made available for this task do not contain collocations as possible target words, but only single word units. This is a somewhat different definition of the task as compared to previous similar evaluations; the difference may have an impact on the overall performance achieved by systems participating in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sense Tagged Data</head><p>The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature. <ref type="bibr">Kilgarriff (2002)</ref> mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item. About 12% of that tagging consisted of multi-word expressions and proper nouns, which are usually not ambiguous, and which are not considered during our data collection process. So far we measured a 62.8% inter-tagger agreement between the first two taggings for single word tagging, plus close-to-100% precision in tagging multi-word expressions and proper nouns (as mentioned earlier, this represents about 12% of the annotated data). This results in an overall agreement of about 67.3% which is reasonable and closely comparable with previous figures. Note that these figures are collected for the entire OMWE data set build so far, which consists of annotated data for more than 350 words.</p><p>In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance <ref type="bibr" target="#b0">(Carletta, 1996)</ref>, was also determined. We measure two figures: micro-average , where number of senses, agreement by chance, and are determined as an average for all words in the set, and macro-average , where inter-tagger agreement, agreement by chance, and are individually determined for each of the words in the set, and then combined in an overall average. With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro-statistic of 0.58. For macro-estimations, we assume that word senses follow the distribution observed in the OMWE annotated data, and under this assumption, the macro-is 0.35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participating Systems</head><p>27 teams participated in this word sense disambiguation task. Tables <ref type="table" target="#tab_3">2 and 3</ref> list the names of the participating systems, the corresponding institutions, and the name of the first author -which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.</p><p>There were no restrictions placed on the number of submissions each team could make. A total number of 47 submissions were received for this task. Tables <ref type="table" target="#tab_3">2 and 3</ref> show all the submissions for each team, gives a brief description of their approaches, and lists the precision and recall obtained by each system under fine and coarse grained evaluations.</p><p>The precision/recall baseline obtained for this task under the "most frequent sense" heuristic is 55.2% (fine grained) and 64.5% (coarse grained). The performance of most systems (including several unsupervised systems, as listed in Table <ref type="table">3</ref>) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.</p><p>Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The English lexical sample task in SENSEVAL-3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense. The objective of this task was to: (1) Determine feasibility of reliably finding the  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Summary of the sense inventory</cell></row><row><cell>weak verb performance of systems participating in</cell></row><row><cell>the English lexical sample in SENSEVAL-2, which</cell></row><row><cell>may be due to the high number of senses defined for</cell></row><row><cell>verbs in the WordNet sense inventory. By choos-</cell></row><row><cell>ing a different set of senses, we hope to gain insight</cell></row><row><cell>into the dependence of difficulty of the sense disam-</cell></row><row><cell>biguation task on sense inventories.</cell></row><row><cell>Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>knowledge sources (part-of-speech of neighbouring words, Nat.U. Singapore (Lee) words in context, local collocations, syntactic relations), in an SVM classifier. 72.4 72.4 78.8 78.8 htsa4 Similar to htsa3, with different correction function of a-priori frequencies. 72.4 72.4 78.8 78.8 BCU comb An ensemble of decision lists, SVM, and vectorial similarity, improved Basque Country U. with a variety of smoothing techniques. The features consist 72.3 72.3 78.9 78.9 (Agirre &amp; Martinez) of local collocations, syntactic dependencies, bag-of-words, domain features. htsa1 Similar to htsa3, but with smaller number of features.</figDesc><table><row><cell></cell><cell></cell><cell>Fine</cell><cell></cell><cell cols="2">Coarse</cell></row><row><cell>System/Team</cell><cell>Description</cell><cell>P</cell><cell>R</cell><cell>P</cell><cell>R</cell></row><row><cell>htsa3</cell><cell>A Naive Bayes system, with correction of the a-priori frequencies, by</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U.Bucharest (Grozea) IRST-Kernels</cell><cell>dividing the output confidence of the senses by Kernel methods for pattern abstraction, paradigmatic and syntagmatic info. ¢ ¡ ¤ £ ¦ ¥  § © £ © ( ! ) # "$</cell><cell cols="4">72.9 72.9 79.3 79.3</cell></row><row><cell cols="2">ITC-IRST (Strapparava) and unsupervised term proximity (LSA) on BNC, in an SVM classifier.</cell><cell cols="4">72.6 72.6 79.5 79.5</cell></row><row><cell>nusels</cell><cell cols="5">A combination of 72.2 72.2 78.7 78.7</cell></row><row><cell>rlsc-comb</cell><cell>A regularized least-square classification (RLSC), using local and topical</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U.Bucharest (Popescu)</cell><cell>features, with a term weighting scheme.</cell><cell cols="4">72.2 72.2 78.4 78.4</cell></row><row><cell>htsa2</cell><cell>Similar to htsa4, but with smaller number of features.</cell><cell cols="4">72.1 72.1 78.6 78.6</cell></row><row><cell>BCU english</cell><cell>Similar to BCU comb, but with a vectorial space model learning.</cell><cell cols="4">72.0 72.0 79.1 79.1</cell></row><row><cell>rlsc-lin</cell><cell>Similar to rlsc-comb, with a linear kernel, and a binary weighting scheme.</cell><cell cols="4">71.8 71.8 78.4 78.4</cell></row><row><cell>HLTC HKUST all</cell><cell>A voted classifier combining a new kernel PCA method, a Maximum Entropy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HKUST (Carpuat)</cell><cell>model, and a boosting-based model, using syntactic and collocational features</cell><cell cols="4">71.4 71.4 78.6 78.6</cell></row><row><cell>TALP</cell><cell>A system with per-word feature selection, using a rich feature set. For</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U.P.Catalunya</cell><cell>learning, it uses SVM, and combines two binarization procedures:</cell><cell cols="4">71.3 71.3 78.2 78.2</cell></row><row><cell>(Escudero et al.)</cell><cell>one vs. all, and constraint learning.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MC-WSD</cell><cell>A multiclass averaged perceptron classifier with two components: one</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Brown U.</cell><cell>trained on the data provided, the other trained on this data, and on</cell><cell cols="4">71.1 71.1 78.1 78.1</cell></row><row><cell>(Ciaramita &amp; Johnson)</cell><cell>WordNet glosses. Features consist of local and syntactic features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HLTC HKUST all2</cell><cell>Similar to HLTC HKUST all, also adds a Naive Bayes classifier.</cell><cell cols="4">70.9 70.9 78.1 78.1</cell></row><row><cell>NRC-Fine</cell><cell>Syntactic and semantic features, using POS tags and pointwise mutual infor-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NRC (Turney)</cell><cell>mation on a terabyte corpus. Five basic classifiers are combined with voting.</cell><cell cols="4">69.4 69.4 75.9 75.9</cell></row><row><cell>HLTC HKUST me</cell><cell>Similar to HLTC HKUST all, only with a maximum entropy classifier.</cell><cell cols="4">69.3 69.3 76.4 76.4</cell></row><row><cell>NRC-Fine2</cell><cell>Similar to NRC-Fine, with a different threshold for dropping features</cell><cell cols="4">69.1 69.1 75.6 75.6</cell></row><row><cell>GAMBL</cell><cell>A cascaded memory-based classifier, using two classifiers based on global</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U. Antwerp (Decadt)</cell><cell>and local features, with a genetic algorithm for parameter optimization.</cell><cell cols="4">67.4 67.4 74.0 74.0</cell></row><row><cell>SinequaLex</cell><cell>Semantic classification trees, built on short contexts and document se-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sinequa Labs (Crestan)</cell><cell>mantics, plus a decision system based on information retrieval techniques.</cell><cell cols="4">67.2 67.2 74.2 74.2</cell></row><row><cell>CLaC1</cell><cell>A Naive Bayes approach using a context window around the target word,</cell><cell cols="4">67.2 67.2 75.1 75.1</cell></row><row><cell>Concordia U. (Lamjiri)</cell><cell>which is dynamically adjusted</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SinequaLex2</cell><cell>A cumulative method based on scores of surrounding words.</cell><cell cols="4">66.8 66.8 73.6 73.6</cell></row><row><cell>UMD SST4</cell><cell>Supervised learning using Support Vector Machines, using local and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U. Maryland (Cabezas)</cell><cell>wide context features, and also grammatical and expanded contexts.</cell><cell cols="4">66.0 66.0 73.7 73.7</cell></row><row><cell>Prob1</cell><cell>A probabilistic modular WSD system, with individual modules based on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cambridge U. (Preiss)</cell><cell>separate known approaches to WSD (26 different modules)</cell><cell cols="4">65.1 65.1 71.6 71.6</cell></row><row><cell>SyntaLex-3</cell><cell>A supervised system that uses local part of speech features and bigrams,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">U.Toronto (Mohammad) in an ensemble classifier using bagged decision trees.</cell><cell cols="4">64.6 64.6 72.0 72.0</cell></row><row><cell>UNED</cell><cell>A similarity-based system, relying on the co-occurrence of nouns and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNED (Artiles)</cell><cell>adjectives in the test and training examples.</cell><cell cols="4">64.1 64.1 72.0 72.0</cell></row><row><cell>SyntaLex-4</cell><cell>Similar to SyntaLex-3, but with unified decision trees.</cell><cell cols="4">63.3 63.3 71.1 71.1</cell></row><row><cell>CLaC2</cell><cell>Syntactic and semantic (WordNet hypernyms) information of neighboring</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>words, fed to a Maximum Entropy learner. See also CLaC1</cell><cell cols="4">63.1 63.1 70.3 70.3</cell></row><row><cell>SyntaLex-1</cell><cell>Bagged decision trees using local POS features. See also SyntaLex-3.</cell><cell cols="4">62.4 62.4 69.1 69.1</cell></row><row><cell>SyntaLex-2</cell><cell>Similar to SyntaLex-1, but using broad context part of speech features.</cell><cell cols="4">61.8 61.8 68.4 68.4</cell></row><row><cell>Prob2</cell><cell>Similar to Prob1, but invokes only 12 modules.</cell><cell cols="4">61.9 61.9 69.3 69.3</cell></row><row><cell>Duluth-ELSS</cell><cell>An ensemble approach, based on three bagged decision trees, using</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">U.Minnesota (Pedersen) unigrams, bigrams, and co-occurrence features</cell><cell cols="4">61.8 61.8 70.1 70.1</cell></row><row><cell>UJAEN</cell><cell>A Neural Network supervised system, using features based on semantic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U.Jaén (García-Vega)</cell><cell>relations from WordNet extracted from the training data</cell><cell cols="4">61.3 61.3 69.5 69.5</cell></row><row><cell>R2D2</cell><cell>A combination of supervised (Maximum Entropy, HMM Models, Vector</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U. Alicante (Vazquez)</cell><cell>Quantization, and unsupervised (domains and conceptual density) systems.</cell><cell cols="4">63.4 52.1 69.7 57.3</cell></row><row><cell>IRST-Ties</cell><cell>A generalized pattern abstraction system, based on boosted wrapper</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ITC-IRST (Strapparava) induction, using only few syntagmatic features.</cell><cell cols="4">70.6 50.5 76.7 54.8</cell></row><row><cell>NRC-Coarse</cell><cell cols="5">Similar to NRC-Fine; maximizes the coarse score, by training on coarse senses. 48.5 48.5 75.8 75.8</cell></row><row><cell>NRC-Coarse2</cell><cell>Similar to NRC-Coarse, with a different threshold for dropping features.</cell><cell cols="4">48.4 48.4 75.7 75.7</cell></row><row><cell>DLSI-UA-LS-SU</cell><cell>A maximum entropy method and a bootstrapping algorithm ("re-training") with,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U.Alicante (Vazquez)</cell><cell>iterative feeding of training cycles with new high-confidence examples.</cell><cell cols="4">78.2 31.0 82.8 32.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance and short description of the supervised systems participating in the SENSEVAL-3 English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both fine grained and coarse grained scoring. Corresponding team and reference to system description (in this volume) are indicated for the first system for each team.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Open Mind Word Expert can be accessed at http://teachcomputers.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible. In particular, we are grateful to Gwen Lenker -our most productive contributor. We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise. Without them, all these comparative analyses would not be possible.</p><p>We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.</p><p>We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>appropriate sense for words with various degrees of polysemy, using different sense inventories; and (2) Determine the usefulness of sense annotated data collected over the Web (as opposed to other traditional approaches for building semantically annotated corpora).</p><p>The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation. Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers. The training and test data sets used in this exercise are available online from http://www.senseval.org and http://teach-computers.org.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Assessing agreement on classification tasks: The kappa statistic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="254" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computer and the Humanities. Special issue: SENSEVAL. Evaluating Word Sense Disambiguation programs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2002 Workshop on</title>
				<editor>
			<persName><forename type="first">July</forename><forename type="middle">A</forename><surname>Philadelphia</surname></persName>
			<persName><forename type="first">M</forename><surname>Kilgarriff</surname></persName>
			<persName><surname>Palmer</surname></persName>
		</editor>
		<meeting>the ACL 2002 Workshop on</meeting>
		<imprint>
			<date type="published" when="2000-04" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
	<note>Word Sense Disambiguation: Recent Successes and Future Directions</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">Proceedings of SENSEVAL-2</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Preiss</surname></persName>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</editor>
		<meeting>SENSEVAL-2<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics Workshop</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
