<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 4: Sentiment Analysis in Twitter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM Watson Health Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computing Research Institute</orgName>
								<orgName type="institution">Hamad bin Khalifa University</orgName>
								<address>
									<country>Qatar, Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 4: Sentiment Analysis in Twitter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses the fourth year of the "Sentiment Analysis in Twitter Task". SemEval-2016 Task 4 comprises five subtasks, three of which represent a significant departure from previous editions. The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet. The three new subtasks focus on two variants of the basic "sentiment classification in Twitter" task. The first variant adopts a five-point scale, which confers an ordinal character to the classification task. The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature. The task continues to be very popular, attracting a total of 43 teams. * Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSI-TIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy. Sentiment classification has become a ubiquitous enabling technology in the Twittersphere. Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others <ref type="bibr">(Martínez-Cámara et al., 2014;</ref><ref type="bibr" target="#b35">Mejova et al., 2015)</ref>.</p><p>As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns <ref type="bibr" target="#b39">(Nakov et al., 2013;</ref><ref type="bibr" target="#b46">Rosenthal et al., 2014;</ref><ref type="bibr" target="#b47">Rosenthal et al., 2015;</ref><ref type="bibr" target="#b41">Nakov et al., 2016b)</ref>.</p><p>Previous editions of the SemEval task involved binary (POSITIVE vs. NEGATIVE) or single-label multi-class classification (SLMC) when a NEU-TRAL 1 class is added (POSITIVE vs. NEGATIVE vs. NEUTRAL). SemEval-2016 Task 4 represents a significant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and in combination (Subtask E):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Ordinal Classification</head><p>We replace the two-or three-point scale with a fivepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is now ubiquitous in the corporate world where human ratings are involved: e.g., Amazon, TripAdvisor, and Yelp, all use a five-point scale for rating sentiment towards products, hotels, and restaurants.</p><p>Moving from a categorical two/three-point scale to an ordered five-point scale means, in machine learning terms, moving from binary to ordinal classification (a.k.a. ordinal regression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Quantification</head><p>We replace classification with quantification, i.e., supervised class prevalence estimation. With regard to Twitter, hardly anyone is interested in whether a specific person has a positive or a negative view of the topic. Rather, applications look at estimating the prevalence of positive and negative tweets about a given topic. Most (if not all) tweet sentiment classification studies conducted within political science <ref type="bibr" target="#b8">(Borge-Holthoefer et al., 2015;</ref><ref type="bibr" target="#b32">Kaya et al., 2013;</ref><ref type="bibr" target="#b34">Marchetti-Bowick and Chambers, 2012)</ref>, economics <ref type="bibr" target="#b7">(Bollen et al., 2011;</ref><ref type="bibr" target="#b42">O'Connor et al., 2010)</ref>, social science <ref type="bibr" target="#b16">(Dodds et al., 2011)</ref>, and market research <ref type="bibr" target="#b10">(Burton and Soboleva, 2011;</ref><ref type="bibr" target="#b43">Qureshi et al., 2013)</ref>, use Twitter with an interest in aggregate data and not in individual classifications.</p><p>Estimating prevalences (more generally, estimating the distribution of the classes in a set of unlabelled items) by leveraging training data is called quantification in data mining and related fields. Previous work has argued that quantification is not a mere byproduct of classification, since (a) a good classifier is not necessarily a good quantifier, and vice versa, see, e.g., <ref type="bibr" target="#b23">(Forman, 2008)</ref>; (b) quantification requires evaluation measures different from classification. Quantification-specific learning approaches have been proposed over the years; Sections 2 and 5 of <ref type="bibr" target="#b19">(Esuli and Sebastiani, 2015)</ref> contain several pointers to such literature.</p><p>Note that, in Subtasks B to E, tweets come labelled with the topic they are about and participants need not classify whether a tweet is about a given topic. A topic can be anything that people express opinions about; for example, a product (e.g., iPhone6), a political candidate (e.g., Hillary Clinton), a policy (e.g., Obamacare), an event (e.g., the Pope's visit to Palestine), etc.</p><p>The rest of the paper is structured as follows. In Section 2, we give a general overview of SemEval-2016 Task 4 and the five subtasks. Section 3 focuses on the datasets, and on the data generation procedure. In Section 4, we describe in detail the evaluation measures for each subtask. Section 5 discusses the results of the evaluation and the techniques and tools that the top-ranked participants used. Section 6 concludes, discussing the lessons learned and some possible ideas for a followup at SemEval-2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>SemEval-2016 Task 4 consists of five subtasks: 1. Subtask A: Given a tweet, predict whether it is of positive, negative, or neutral sentiment.</p><p>2. Subtask B: Given a tweet known to be about a given topic, predict whether it conveys a positive or a negative sentiment towards the topic.</p><p>3. Subtask C: Given a tweet known to be about a given topic, estimate the sentiment it conveys towards the topic on a five-point scale ranging from HIGHLYNEGATIVE to HIGHLYPOS-ITIVE.</p><p>4. Subtask D: Given a set of tweets known to be about a given topic, estimate the distribution of the tweets in the POSITIVE and NEGATIVE classes.</p><p>5. Subtask E: Given a set of tweets known to be about a given topic, estimate the distribution of the tweets across the five classes of a fivepoint scale, ranging from HIGHLYNEGATIVE to HIGHLYPOSITIVE.</p><p>Subtask A is a rerun -it was present in all three previous editions of the task. In the 2013-2015 editions, it was known as Subtask B. <ref type="bibr">2</ref> We ran it again this year because it was the most popular subtask in the three previous task editions. It was the most popular subtask this year as well -see Section 5. Subtask B is a variant of SemEval-2015 Task 10 Subtask C <ref type="bibr" target="#b47">(Rosenthal et al., 2015;</ref><ref type="bibr" target="#b41">Nakov et al., 2016b)</ref>, with POSITIVE, NEUTRAL, and NEGATIVE as the classification labels.</p><p>Subtask E is similar to SemEval-2015 Task 10 Subtask D, which consisted of the following problem: Given a set of messages on a given topic from the same period of time, classify the overall sentiment towards the topic in these messages as strongly positive, weakly positive, neutral, weakly negative, or strongly negative. Note that in SemEval-2015 Task 10 Subtask D, exactly one of the five classes had to be chosen, while in our Subtask E, a distribution across the five classes has to be estimated.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section, we describe the process of collection and annotation of the training, development and testing tweets for all five subtasks. We dub this dataset the Tweet 2016 dataset in order to distinguish it from datasets generated in previous editions of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tweet Collection</head><p>We provided the datasets from the previous editions 3 (see Table <ref type="table" target="#tab_3">2</ref>) of this task <ref type="bibr" target="#b39">(Nakov et al., 2013;</ref><ref type="bibr" target="#b46">Rosenthal et al., 2014;</ref><ref type="bibr" target="#b47">Rosenthal et al., 2015;</ref><ref type="bibr" target="#b41">Nakov et al., 2016b)</ref>   We employed the following annotation procedure. As in previous years, we first gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned named entity recognition system <ref type="bibr" target="#b45">(Ritter et al., 2011)</ref>. The collected tweets were greatly skewed towards the neutral class. In order to reduce the class imbalance, we removed those that contained no sentiment-bearing words. We used SentiWordNet 3.0 <ref type="bibr" target="#b4">(Baccianella et al., 2010)</ref> as a repository of sentiment words. Any word listed in SentiWordNet 3.0 with at least one sense having a positive or a negative sentiment score greater than 0.3 was considered sentiment-bearing. <ref type="bibr">4</ref> The training and development tweets were collected from July to October 2015. The test tweets were collected from October to December 2015. We used the public streaming Twitter API to download the tweets. <ref type="bibr">5</ref> We then manually filtered the resulting tweets to obtain a set of 200 meaningful topics with at least 100 tweets each (after filtering out near-duplicates). We excluded topics that were incomprehensible, ambiguous (e.g., Barcelona, which is the name both of a city and of a sports team), or too general (e.g., Paris, which is the name of a big city). We then discarded tweets that were just mentioning the topic but were not really about the topic.</p><p>Note that the topics in the training and in the test sets do not overlap, i.e., the test set consists of tweets about topics different from the topics the training and development tweets are about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation</head><p>The 2016 data consisted of four parts: TRAIN (for training models), DEV (for tuning models), DEVTEST (for development-time evaluation), and TEST (for the official evaluation). The first three datasets were annotated using Amazon's Mechanical Turk, while the TEST dataset was annotated on CrowdFlower.</p><p>Instructions: Given a Twitter message and a topic, identify whether the message is highly positive, positive, neutral, negative, or highly negative (a) in general and (b) with respect to the provided topic. If a tweet is sarcastic, please select the checkbox "The tweet is sarcastic". Please read the examples and the invalid responses before beginning if this is the first time you are working on this HIT. Annotation with Amazon's Mechanical Turk. A Human Intelligence Task (HIT) consisted of providing all required annotations for a given tweet message. In order to qualify to work on our HITs, a Mechanical Turk annotator (a.k.a. "Turker") had to have an approval rate greater than 95% and to have completed at least 50 approved HITs. Each HIT was carried out by five Turkers and consisted of five tweets to be annotated. A Turker had to indicate the overall polarity of the tweet message (on a five-point scale) as well as the overall polarity of the message towards the given target topic (again, on a five-point scale). The annotation instructions along with an example are shown in Figure <ref type="figure" target="#fig_0">1</ref>. We made available to the Turkers several additional examples, which are shown in Table <ref type="table" target="#tab_5">3</ref>.</p><p>We rejected HITs with the following problems:</p><p>• one or more responses do not have the overall sentiment marked;</p><p>• one or more responses do not have the sentiment towards the topic marked;</p><p>• one or more responses appear to be randomly selected.</p><p>Annotation with CrowdFlower. We annotated the TEST data using CrowdFlower, as it allows better quality control of the annotations across a number of dimensions. Most importantly, it allows us to find and exclude unreliable annotators based on hidden tests, which we created starting with the highestconfidence and highest-agreement annotations from Mechanical Turk. We added some more tests manually. Otherwise, we setup the annotation task giving exactly the same instructions and examples as in Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consolidation of annotations.</head><p>In previous years, we used majority voting to select the true label (and discarded cases where a majority had not emerged, which amounted to about 50% of the tweets). As this year we have a five-point scale, where the expected agreement is lower, we used a two-step procedure. If three out of the five annotators agreed on a label, we accepted the label. Otherwise, we first mapped the categorical labels to the integer values −2, −1, 0, 1, 2. Then we calculated the average, and finally we mapped that average to the closest integer value. In order to counter-balance the tendency of the average to stay away from −2 and 2, and also to prefer 0, we did not use rounding at ±0.5 and ±1.5, but at ±0.4 and ±1.4 instead.</p><p>To give the reader an idea about the degree of agreement, we will look at the TEST dataset as an example. It included 20,632 tweets. For 2,760, all five annotators assigned the same value, and for another 9,944 there was a majority value. For the remaining 7,928 cases, we had to perform averaging as described above.</p><p>The consolidated statistics from the five annotators on a three-point scale for Subtask A are shown in Table <ref type="table" target="#tab_6">4</ref>. Note that, for consistency, we annotated the data for Subtask A on a five-point scale, which we then converted to a three-point scale.</p><p>The topic annotations on a two-point scale for Subtasks B and D are shown in Table <ref type="table" target="#tab_7">5</ref>, while those on a five-point scale for Subtasks C and E are in Table 6. Note that, as for Subtask A, the two-point scale annotation counts for Subtasks B and D derive from summing the POSITIVEs with the HIGH-LYPOSITIVEs, and the NEGATIVEs with the HIGH-LYNEGATIVEs from Table <ref type="table" target="#tab_8">6</ref>; moreover, this time we also remove the NEUTRALs.     As we use the same test tweets for all subtasks, the submission of results by participating teams was subdivided in two stages: (i) participants had to submit results for Subtasks A, C, E, and (ii) only after the submission deadline for A, C, E had passed, we distributed to participants the unlabelled test data for Subtasks B and D.</p><p>Otherwise, since for Subtasks B and D we filter out the NEUTRALs, we would have leaked information about which the NEUTRALs are, and this information could have been used in Subtasks C and E.</p><p>Finally, as the same tweets can be selected for different topics, we ended up with some duplicates; arguably, these are true duplicates for Subtask A only, as for the other subtasks the topics still differ. This includes 25 duplicates in TRAIN, 3 in DEV, 2 in DE-VTEST, and 116 in TEST. There is a larger number in TEST, as TEST is about twice as large as TRAIN, DEV, and DEVTEST combined. This is because we wanted a large TEST set with 100 topics and 200 tweets per topic on average for Subtasks C and E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Measures</head><p>This section discuss the evaluation measures for the five subtasks of our SemEval-2016 Task 4. A document describing the evaluation measures in detail 6 <ref type="bibr" target="#b40">(Nakov et al., 2016a)</ref>, and a scoring software implementing all the five "official" measures, were made available to the participants via the task website together with the training data. <ref type="bibr">7</ref> For Subtasks B to E, the datasets are each subdivided into a number of "topics", and the subtask needs to be carried out independently for each topic. As a result, each of the evaluation measures will be "macroaveraged" across the topics, i.e., we compute the measure individually for each topic, and we then average the results across the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subtask A: Message polarity classification</head><p>Subtask A is a single-label multi-class (SLMC) classification task. Each tweet must be classified as belonging to exactly one of the following three classes C={POSITIVE, NEUTRAL, NEGATIVE}.</p><p>We adopt the same evaluation measure as the 2013-2015 editions of this subtask, F P N 1 :</p><formula xml:id="formula_0">F P N 1 = F P 1 + F N 1 2 (1)</formula><p>F P 1 is the F 1 score for the POSITIVE class:</p><formula xml:id="formula_1">F P 1 = 2π P ρ P π P + ρ P (2)</formula><p>Here, π P and ρ P denote precision and recall for the POSITIVE class, respectively: π P = P P P P + P U + P N</p><p>(3) ρ P = P P P P + U P + N P (4) where P P , U P , N P , P U , P N are the cells of the confusion matrix shown in Table <ref type="table" target="#tab_10">7</ref>.  F N 1 is defined analogously, and the measure we finally adopt is F P N 1 as from Equation <ref type="formula">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subtask B: Tweet classification according to a two-point scale</head><p>Subtask B is a binary classification task. Each tweet must be classified as either POSITIVE or NEGATIVE.</p><p>For this subtask we adopt macroaveraged recall:</p><formula xml:id="formula_2">ρ P N = 1 2 (ρ P + ρ N ) = 1 2 ( P P P P + N P + N N N N + P N )<label>(5)</label></formula><p>In the above formula, ρ P and ρ N are the positive and the negative class recall, respectively. Note that U terms are entirely missing in Equation <ref type="formula" target="#formula_2">5</ref>; this is because we do not have the NEUTRAL class for SemEval-2016 Task 4, subtask A.</p><p>ρ P N ranges in [0, 1], where a value of 1 is achieved only by the perfect classifier (i.e., the classifier that correctly classifies all items), a value of 0 is achieved only by the perverse classifier (the classifier that misclassifies all items), while 0.5 is both (i) the value obtained by a trivial classifier (i.e., the classifier that assigns all tweets to the same classbe it POSITIVE or NEGATIVE), and (ii) the expected value of a random classifier. The advantage of ρ P N over "standard" accuracy is that it is more robust to class imbalance. The accuracy of the majority-class classifier is the relative frequency (aka "prevalence") of the majority class, that may be much higher than 0.5 if the test set is imbalanced. Standard F 1 is also sensitive to class imbalance for the same reason. Another advantage of ρ P N over F 1 is that ρ P N is invariant with respect to switching POSITIVE with NEG-ATIVE, while F 1 is not. See <ref type="bibr" target="#b53">(Sebastiani, 2015)</ref> for more details on ρ P N .</p><p>As we noted before, the training dataset, the development dataset, and the test dataset are each subdivided into a number of topics, and Subtask B needs to be carried out independently for each topic. As a result, the evaluation measures discussed in this section are computed individually for each topic, and the results are then averaged across topics to yield the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subtask C: Tweet classification according to a five-point scale</head><p>Subtask C is an ordinal classification (OCalso known as ordinal regression) task, in which each tweet must be classified into exactly one of the classes in C={HIGHLYPOSITIVE, POS-ITIVE, NEUTRAL, NEGATIVE, HIGHLYNEGA-TIVE}, represented in our dataset by numbers in {+2,+1,0,−1,−2}, with a total order defined on C. The essential difference between SLMC (see Section 4.1 above) and OC is that not all mistakes weigh equally in the latter. For example, misclassifying a HIGHLYNEGATIVE example as HIGHLYPOSITIVE is a bigger mistake than misclassifying it as NEGA-TIVE or NEUTRAL.</p><p>As our evaluation measure, we use macroaveraged mean absolute error (M AE M ):</p><formula xml:id="formula_3">M AE M (h, T e) = 1 |C| |C| j=1 1 |T e j | x i ∈T e j |h(x i )−y i |</formula><p>where y i denotes the true label of item x i , h(x i ) is its predicted label, T e j denotes the set of test documents whose true class is c j , |h(x i ) − y i | denotes the "distance" between classes h(x i ) and y i (e.g., the distance between HIGHLYPOSITIVE and NEGATIVE is 3), and the "M" superscript indicates "macroaveraging". The advantage of M AE M over "standard" mean absolute error, which is defined as:</p><formula xml:id="formula_4">M AE µ (h, T e) = 1 |T e| x i ∈T e |h(x i ) − y i | (6)</formula><p>is that it is robust to class imbalance (which is useful, given the imbalanced nature of our dataset). On perfectly balanced datasets M AE M and M AE µ are equivalent.</p><p>Unlike the measures discussed in Sections 4.1 and 4.2, M AE M is a measure of error, and not accuracy, and thus lower values are better. See <ref type="bibr" target="#b3">(Baccianella et al., 2009)</ref> for more detail on M AE M .</p><p>Similarly to Subtask B, Subtask C needs to be carried out independently for each topic. As a result, M AE M is computed individually for each topic, and the results are then averaged across all topics to yield the final score. The difference between binary classification (as from Section 4.2) and binary quantification is that errors of different polarity (e.g., a false positive and a false negative for the same class) can compensate each other in the latter. Quantification is thus a more lenient task since a perfect classifier is also a perfect quantifier, but a perfect quantifier is not necessarily a perfect classifier.</p><p>We adopt normalized cross-entropy, better known as Kullback-Leibler Divergence (KLD). KLD was proposed as a quantification measure in <ref type="bibr" target="#b22">(Forman, 2005)</ref>, and is defined as follows:</p><formula xml:id="formula_5">KLD(p, p, C) = c j ∈C p(c j ) log e p(c j ) p(c j )<label>(7)</label></formula><p>KLD is a measure of the error made in estimating a true distribution p over a set C of classes by means of a predicted distributionp. Like M AE M in Section 4.3, KLD is a measure of error, which means that lower values are better. KLD ranges between 0 (best) and +∞ (worst).</p><p>Note that the upper bound of KLD is not finite since Equation 7 has predicted prevalences, and not true prevalences, at the denominator: that is, by making a predicted prevalencep(c j ) infinitely small we can make KLD infinitely large. To solve this problem, in computing KLD we smooth both p(c j ) andp(c j ) via additive smoothing, i.e.,</p><formula xml:id="formula_6">p s (c j ) = p(c j ) + ( c j ∈C p(c j )) + • |C| = p(c j ) + 1 + • |C| (<label>8</label></formula><formula xml:id="formula_7">)</formula><p>where p s (c j ) denotes the smoothed version of p(c j ) and the denominator is just a normalizer (same for thep s (c j )'s); the quantity = 1 2•|T e| is used as a smoothing factor, where T e denotes the test set.</p><p>The smoothed versions of p(c j ) andp(c j ) are used in place of their original versions in Equation <ref type="formula" target="#formula_5">7</ref>; as a result, KLD is always defined and still returns a value of 0 when p andp coincide.</p><p>KLD is computed individually for each topic, and the results are averaged to yield the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Subtask E: Tweet quantification according to a five-point scale</head><p>Subtask E is an ordinal quantification (OQ) task, in which (as in OC) each tweet belongs exactly to one of the classes in C={HIGHLYPOSITIVE, POSI-TIVE, NEUTRAL, NEGATIVE, HIGHLYNEGATIVE}, where there is a total order on C. As in binary quantification, the task is to compute an estimatep(c j ) of the relative frequency p(c j ) in the test tweets of all the classes c j ∈ C.</p><p>The measure we adopt for OQ is the Earth Mover's Distance <ref type="bibr" target="#b49">(Rubner et al., 2000)</ref> (also known as the Vasersteȋn metric <ref type="bibr" target="#b51">(Rüschendorf, 2001)</ref>), a measure well-known in the field of computer vision. EM D is currently the only known measure for ordinal quantification. It is defined for the general case in which a distance d(c , c ) is defined for each c , c ∈ C. When there is a total order on the classes in C and d(c i , c i+1 ) = 1 for all i ∈ {1, ..., (C − 1)} (as in our application), the Earth Mover's Distance is defined as</p><formula xml:id="formula_8">EM D(p, p) = |C|−1 j=1 | j i=1p (c i ) − j i=1 p(c i )| (9)</formula><p>and can be computed in |C| steps from the estimated and true class prevalences.</p><p>Like KLD in Section 4.4, EM D is a measure of error, so lower values are better; EM D ranges between 0 (best) and |C| − 1 (worst). See  for more details on EM D.</p><p>As before, EM D is computed individually for each topic, and the results are then averaged across all topics to yield the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants and Results</head><p>A total of 43 teams (see Table <ref type="table" target="#tab_1">15</ref> at the end of the paper) participated in SemEval-2016 Task 4, representing 25 countries; the country with the highest participation was China (5 teams), followed by Italy, Spain, and USA (4 teams each). The subtask with the highest participation was Subtask A (34 teams), followed by Subtask B (19 teams), Subtask D (14 teams), Subtask C (11 teams), and Subtask E (10 teams).</p><p>It was not surprising that Subtask A proved to be the most popular -it was a rerun from previous years; conversely, none among Subtasks B to E had previously been offered in precisely the same form. Quantification-related subtasks (D and E) generated 24 participations altogether, while subtasks with an ordinal nature (C and E) attracted 21 participations. Only three teams participated in all five subtasks; conversely, no less than 23 teams took part in one subtask only (with a few exceptions, Subtask A). Many teams that participated in more than one subtask used essentially the same system for all of them, with little tuning to the specifics of each subtask.</p><p>Few trends stand out among the participating systems. In terms of the supervised learning methods used, there is a clear dominance of methods based on deep learning, including convolutional neural networks and recurrent neural networks (and, in particular, long short-term memory networks); the software libraries for deep learning most frequently used by the participants are Theano and Keras. Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is scarce.</p><p>The use of distant supervision is ubiquitous; this is natural, since there is an abundance of freely available tweets labelled according to sentiment (possibly with silver labels only, e.g., emoticons), and it is intuitive that their use as additional training data could be helpful. Another ubiquitous technique is the use of word embeddings, usually generated via either word2vec <ref type="bibr" target="#b36">(Mikolov et al., 2013)</ref> or GloVe <ref type="bibr">(Pennington et al., 2014)</ref>; most authors seem to use general-purpose, pre-trained embeddings, while some authors also use customized word embeddings, trained either on the Tweet 2016 dataset or on tweet datasets of some sort.</p><p>Nothing radically new seems to have emerged with respect to text preprocessing; as in previous editions of this task, participants use a mix of by now obvious techniques, such as negation scope detection, elongation normalization, detection of amplifiers and diminishers, plus the usual extraction of word n-grams, character n-grams, and POS ngrams. The use of sentiment lexicons (alone or in combination with each other; general-purpose or Twitter-specific) is obviously still frequent.</p><p>In the next five subsections, we discuss the results of the participating systems in the five subtasks, focusing on the techniques and tools that the top-ranked participants have used. We also focus on how the participants tailored (if at all) their approach to the specific subtask. When discussing a specific subtask, we will adopt the convention of adding to a team name a subscript which indicates the position in the ranking for that subtask that the team obtained; e.g., when discussing Subtask E, "Finki 2 " indicates team "Finki, which placed 2nd in the ranking for Subtask E". The papers describing the participants' approach are quoted in Table <ref type="table" target="#tab_1">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Subtask A: Message polarity classification</head><p>Table <ref type="table" target="#tab_12">8</ref> ranks the systems submitted by the 34 teams who participated in Subtask A "Message Polarity Classification" in terms of the official measure F P N 1 . We further show the result for two other measures, ρ P N (the measure that we adopted for Subtask B) and accuracy (Acc = T P +T N T P +T N +F P +F N ). We also report the result for a baseline classifier that assigns to each tweet the POSITIVE class. For Subtask A evaluated using F P N 1 , this is the equivalent of the majority class classifier for (binary or SLMC) classification evaluated via vanilla accuracy, i.e., this is the "smartest" among the trivial policies that attempt to maximize F P N 1 .  score. In each column the rankings according to the corresponding measure are indicated with a subscript. Teams marked as "(*)" are late submitters, i.e., their original submission was deemed irregular by the organizers, and a revised submission was entered after the deadline.</p><p>All 34 participating systems were able to outperform the baseline on all three measures, with the exception of one system that scored below the baseline on Acc. The top-scoring team (SwissCheese 1 ) used an ensemble of convolutional neural networks, differing in their choice of filter shapes, pooling shapes and usage of hidden layers. Word embeddings generated via word2vec were also used, and the neural networks were trained by using distant supervision. Out of the 10 top-ranked teams, 5 teams (SwissCheese 1 , SENSEI-LIF 2 , UNIMELB 3 , INESC-ID 4 , INSIGHT-1 8 ) used deep NNs of some sort, and 7 teams (SwissCheese 1 , SENSEI-LIF 2 , UNIMELB 3 , INESC-ID 4 , aueb.twitter.sentiment 5 , I2RNTU 7 , INSIGHT-1 8 ) used either generalpurpose or task-specific word embeddings, generated via word2vec or GloVe.</p><p>Historical results. We also tested the participating systems on the test sets from the three previous editions of this subtask. Participants were not allowed to use these test sets for training. Results (measured on F P N 1 ) are reported in Table <ref type="table">9</ref>. The top-performing systems on Tweet 2016 are also top-ranked on the test datasets from previous years. There is a general pattern: the top-ranked system in year x outperforms the top-ranked system in year (x − 1) on the official dataset of year (x − 1). Topranked systems tend to use approaches that are universally strong, even when tested on out-of-domain test sets such as SMS, LiveJournal, or sarcastic tweets (yet, for sarcastic tweets, there are larger differences in rank compared to systems rankings on Tweet 2016). It is unclear where improvements come from: (a) the additional training data that we made available this year (in addition to <ref type="bibr">Tweettrain-2013</ref><ref type="bibr">Tweettrain- , which was used in 2013</ref><ref type="bibr">Tweettrain- -2015</ref>, thus effectively doubling the amount of training data, or (b) because of advancement of learning methods.</p><p>We further look at the top scores achieved by any system in the period 2013-2016. The results are shown in Table <ref type="table" target="#tab_1">10</ref>. Interestingly, the results for a test set improve in the second year it is used (i.e., the year after it was used as an official test set) by 1-3 points absolute, but then do not improve further and stay stable, or can even decrease a bit. This might be due to participants optimizing their systems primarily on the test set from the preceding year.    <ref type="table">9</ref>: Historical results for Subtask A "Message Polarity Classification". The systems are ordered by their score on the Tweet 2016 dataset; the rankings on the individual datasets are indicated with a subscript. The meaning of "(*)" is as in Table <ref type="table" target="#tab_12">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subtask B: Tweet classification according to a two-point scale</head><p>Table <ref type="table" target="#tab_1">11</ref> ranks the 19 teams who participated in Subtask B "Tweet classification according to a twopoint scale" in terms of the official measure ρ P N . Two other measures are reported, F P N 1 (the measure adopted for Subtask A) and accuracy (Acc). We also report the result of a baseline that assigns to each tweet the positive class. This is the "smartest" among the trivial policies that attempt to maximize ρ P N . This baseline always returns ρ P N = 0.500.</p><p>Note however that this is also (i) the value returned by the classifier that assigns to each tweet the negative class, and (ii) the expected value returned by the random classifier; for more details see (Sebastiani, 2015, Section 5), where ρ P N is called K.</p><p>The top-scoring team (Tweester 1 ) used a combination of convolutional neural networks, topic modeling, and word embeddings generated via word2vec. Similar to Subtask A, the main trend among all participants is the widespread use of deep learning techniques.   The systems are ordered by their ρ P N score (higher is better). The meaning of "(*)" is as in Table <ref type="table" target="#tab_12">8</ref>.</p><p>Out of the 10 top-ranked participating teams, 5 teams (Tweester 1 , LYS 2 , INSIGHT-1 5 , UNIMELB 7 , Finki 10 ) used convolutional neural networks; 3 teams (thecerealkiller 3 , UNIMELB 7 , Finki 10 ) submitted systems using recurrent neural networks; and 7 teams (Tweester 1 , LYS 2 , INSIGHT-1 5 , UNIMELB 7 , Finki 10 ) incorporated in their participating systems either general-purpose or task-specific word embeddings (generated via toolkits such as GloVe or word2vec).</p><p>Conversely, the use of classifiers such as support vector machines, which were dominant until a few years ago, seems to have decreased, with only one team (TwiSE 8 ) in the top 10 using them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subtask C: Tweet classification according</head><p>to a five-point scale  <ref type="formula">6</ref>). We also report the result of a baseline system that assigns to each tweet the middle class (i.e., NEUTRAL); for ordinal classification evaluated via M AE M , this is the majority-class classifier for (binary or SLMC) classification evaluated via vanilla accuracy, i.e., this is <ref type="bibr" target="#b3">(Baccianella et al., 2009)</ref> the "smartest" among the trivial policies that attempt to maximize M AE M .  The top-scoring team (TwiSE 1 ) used a singlelabel multi-class classifier to classify the tweets according to their overall polarity. In particular, they used logistic regression that minimizes the multinomial loss across the classes, with weights to cope with class imbalance. Note that they ignored the given topics altogether.   <ref type="table" target="#tab_12">8</ref>.</p><p>Only 2 of the 11 participating teams tuned their systems to exploit the ordinal (as opposed to binary, or single-label multi-class) nature of this subtask. The two teams who did exploit the ordinal nature of the problem are PUT 3 , which uses an ensemble of ordinal regression approaches, and ISTI-CNR 7 , which uses a tree-based approach to ordinal regression. All other teams used general-purpose approaches for single-label multi-class classification, in many cases relying (as for Subtask B) on convolutional neural networks, recurrent neural networks, and word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Subtask D: Tweet quantification according</head><p>to a two-point scale </p><formula xml:id="formula_9">RAE(p,p, C) = 1 |C| c∈C |p(c) − p(c)| p(c)<label>(11)</label></formula><p>where the notation is the same as in Equation <ref type="formula" target="#formula_5">7</ref>.</p><p>We also report the result of a "maximum likelihood" baseline system (dubbed Baseline 1 ). This system assigns to each test topic the distribution of the training tweets (the union of TRAIN, DEV, DE-VTEST) across the classes. This is the "smartest" among the trivial policies that attempt to maximize KLD. We also report the result of a further (less smart) baseline system (dubbed Baseline 2 ), i.e., one that assigns a prevalence of 1 to the majority class (which happens to be the POSITIVE class) and a prevalence of 0 to the other class.</p><p>The top-scoring team (Finki 1 ) adopts an approach based on "classify and count", a classificationoriented (instead of quantification-oriented) approach, using recurrent and convolutional neural networks, and GloVe word embeddings.</p><p>Indeed, only 5 of the 14 participating teams tuned their systems to the fact that it deals with quantification (as opposed to classification). Among the teams who do rely on quantification-oriented approaches, teams LYS 2 and HSENN 14 used an existing structured prediction method that directly optimizes KLD; teams QCRI 5 and ISTI-CNR 11 use existing probabilistic quantification methods; team NRU-HSE 7 uses an existing iterative quantification method based on cost-sensitive learning. Interestingly, team TwiSE 2 uses a "classify and count" approach after comparing it with a quantificationoriented method (similar to the one used by teams LYS 2 and HSENN 14 ) on the development set, and concluding that the former works better than the latter. All other teams used "classify and count" approaches, mostly based on convolutional neural networks and word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Subtask E: Tweet quantification according to a five-point scale</head><p>Table <ref type="table" target="#tab_1">14</ref> lists the results obtained by the 10 participating teams on Subtask E "Tweet quantification according to a five-point scale". We also report the result of a "maximum likelihood" baseline system (dubbed Baseline 1 ), i.e., one that assigns to each test topic the same distribution, namely the distribution of the training tweets (the union of TRAIN, DEV, DEVTEST) across the classes; this is the "smartest" among the trivial policies (i.e., those that do not require any genuine work) that attempt to maximize EM D.</p><p>We further report the result of less smart baseline system (dubbed Baseline 2 ) -one that assigns a prevalence of 1 to the majority class (which coincides with the POSITIVE class) and a prevalence of 0 to all other classes.   <ref type="table" target="#tab_12">8</ref>.</p><p>Only 3 of the 10 participants tuned their systems to the specific characteristics of this subtask, i.e., to the fact that it deals with quantification (as opposed to classification) and to the fact that it has an ordinal (as opposed to binary) nature.</p><p>In particular, the top-scoring team (QCRI 1 ) used a novel algorithm explicitly designed for ordinal quantification, that leverages an ordinal hierarchy of binary probabilistic quantifiers.</p><p>Team NRU-HSE 4 uses an existing quantification approach based on cost-sensitive learning, and adapted it to the ordinal case.</p><p>Team ISTI-CNR 6 instead used a novel adaptation to quantification of a tree-based approach to ordinal regression.</p><p>Teams LYS 7 and HSENN 9 also used an existing quantification approach, but did not exploit the ordinal nature of the problem.</p><p>The other teams mostly used approaches based on "classify and count" (see Section 5.4), and viewed the problem as single-label multi-class (instead of ordinal) classification; some of these teams (notably, team Finki 2 ) obtained very good results, which testifies to the quality of the (general-purpose) features and learning algorithm they used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We described SemEval-2016 Task 4 "Sentiment Analysis in Twitter", which included five subtasks including three that represent a significant departure from previous editions. The three new subtasks focused, individually or in combination, on two variants of the basic "sentiment classification in Twitter" task that had not been previously explored within SemEval. The first variant adopts a five-point scale, which confers an ordinal character to the classification task. The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature. In contrast, previous years' subtasks have focused on the correct labeling of individual tweets. As in previous years <ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref>, the 2016 task was very popular and attracted a total of 43 teams.</p><p>A general trend that emerges from SemEval-2016 Task 4 is that most teams who were ranked at the top in the various subtasks used deep learning, including convolutional NNs, recurrent NNs, and (generalpurpose or task-specific) word embeddings. In many cases, the use of these techniques allowed the teams using them to obtain good scores even without tuning their system to the specifics of the subtask at hand, e.g., even without exploiting the ordinal nature of the subtask -for Subtasks C and E -or the quantification-related nature of the subtask -for Subtasks D and E. Conversely, several teams that have indeed tuned their system to the specifics of the subtask at hand, but have not used deep learning techniques, have performed less satisfactorily. This is a further confirmation of the power of deep learning techniques for tweet sentiment analysis.</p><p>Concerning Subtasks D and E, if quantificationbased subtasks are proposed again, we think it might be a good idea to generate, for each test topic t i , multiple "artificial" test topics t 1 i , t 2 i , ..., where class prevalences are altered with respect to the ones of t i by means of selectively removing from t i tweets belonging to a certain class. In this way, the evaluation can take into consideration (i) class prevalences in the test set and (ii) levels of distribution drift (i.e., of the divergence of the test distribution from the training distribution) that are not present in the "naturally occurring" data.</p><p>By varying the amount of removed tweets at will, one may obtain many test topics, thus augmenting the magnitude of the experimentation at will while at the same time keeping constant the amount of manual annotation needed.</p><p>In terms of possible follow-ups of this task, it might be interesting to have a subtask whose goal is to distinguish tweets that are NEUTRAL about the topic (i.e., do not express any opinion about the topic) from tweets that express a FAIR opinion (i.e., lukewarm, intermediate between POSITIVE and NEGATIVE) about the topic.</p><p>Another possibility is to have a multi-lingual tweet sentiment classification subtask, where training examples are provided for the same topic for two languages (e.g., English and Arabic), and where participants can improve their performance on one language by leveraging the training examples for the other language via transfer learning. Alternatively, it might be interesting to include a cross-lingual tweet sentiment classification subtask, where training examples are provided for a given language (e.g., English) but not for the other (e.g., Arabic); the second language could be also a surprise language, which could be announced at the last moment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The instructions provided to the Mechanical Turk annotators, followed by a screenshot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 4</head><label>4</label><figDesc>Subtask D: Tweet quantification according to a two-point scale Subtask D also assumes a binary quantification setup, in which each tweet is classified as POSITIVE or NEGATIVE. The task is to compute an estimatê p(c j ) of the relative frequency (in the test set) of each of the classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 0.637 2 0.716 4 0.566 1 0.695 7 0.671 1 0.633 1 2 SENSEI-LIF 0.706 3 0.634 3 0.744 1 0.467 8 0.741 1 0.662 2 0.630 2 3 UNIMELB 0.687 6 0.593 9 0.706 6 0.449 11 0.683 9 0.651 4 0.617 3 4 INESC-ID 0.723 1 0.609 6 0.727 2 0.554 2 0.702 4 0.657 3 0.610 4 5 aueb.twitter.sentiment 0.666 7 0.618 5 0.708 5 0.410 17 0.695 7 0.623 7 0.605 5 6 SentiSys 0.714 2 0.633 4 0.723 3 0.515 4 0.726 2 0.644 5 0.598 6 7 I2RNTU 0.693 5 0.597 7 0.680 7 0.469 6 0.696 6 0.638 6 0.596 7 8 INSIGHT-1 0.602 16 0.582 12 0.644 15 0.391 23 0.559 23 0.595 16 0.593 8 9 TwiSE 0.610 15 0.540 16 0.645 13 0.450 10 0.649 13 0.621 8 0.586 9 10 ECNU (*) 0.643 9 0.593 9 0.662 8 0.425 14 0.663 10 0.606 11 0.585 10 11 NTNUSentEval 0.623 11 0.641 1 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>As per the above discussion, Subtasks B to E are new. Conceptually, they form a 2×2 matrix, as shown in Table1, where the rows indicate the goal of the task (classification vs. quantification) and the columns indicate the granularity of the task (twovs. five-point scale).</figDesc><table><row><cell></cell><cell cols="2">Granularity</cell></row><row><cell></cell><cell cols="2">Two-point Five-point</cell></row><row><cell></cell><cell>(binary)</cell><cell>(ordinal)</cell></row><row><cell>Goal</cell><cell cols="2">Classification Quantification Subtask D Subtask E Subtask B Subtask C</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A 2×2 matrix summarizing the similarities and the differences between Subtasks B-E.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>for training and development. In addition we created new training and testing datasets.</figDesc><table><row><cell>Dataset</cell><cell>POSITIVE</cell><cell>NEGATIVE</cell><cell>NEUTRAL</cell><cell>Total</cell></row><row><cell>Twitter2013-train Twitter2013-dev Twitter2013-test SMS2013-test Twitter2014-test Twitter2014-sarcasm LiveJournal2014-test Twitter2015-test</cell><cell cols="4">3,662 1,466 4,600 9,728 575 340 739 1,654 1,572 601 1,640 3,813 492 394 1,207 2,093 982 202 669 1,853 33 40 13 86 427 304 411 1,142 1,040 365 987 2,392</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics about data from the 2013-2015 editions of the SemEval task on Sentiment Analysis in Twitter, which could</figDesc><table /><note>be used for training and development for SemEval-2016 Task 4.3 For Subtask A, we did not allow training on the testing datasets from 2013-2015, as we used them for progress testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Why would you still wear shorts when it's this cold?! I love how Britain see's a bit of sun and they're like 'OOOH LET'S STRIP!</figDesc><table><row><cell>Tweet</cell><cell>Overall Sentiment</cell><cell>Topic Sentiment</cell></row><row><cell></cell><cell>POSITIVE</cell><cell>Britain: NEGATIVE</cell></row><row><cell>Saturday without Leeds United is like Sunday dinner it</cell><cell>NEGATIVE</cell><cell>Leeds United: HIGHLYPOSITIVE</cell></row><row><cell>doesn't feel normal at all (Ryan)</cell><cell></cell><cell></cell></row><row><cell>Who are you tomorrow? Will you make me smile or just</cell><cell>NEUTRAL</cell><cell>Demi Lovato: POSITIVE</cell></row><row><cell>bring me sorrow? #HottieOfTheWeek Demi Lovato</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>List of example tweets and annotations that were provided to the annotators.</figDesc><table><row><cell></cell><cell>POSITIVE</cell><cell>NEUTRAL</cell><cell>NEGATIVE</cell><cell>Total</cell></row><row><cell>TRAIN DEV DEVTEST TEST</cell><cell cols="4">3,094 844 994 7,059 10,342 3,231 20,632 863 2,043 6,000 765 391 2,000 681 325 2,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>2016 data statistics (Subtask A).</figDesc><table><row><cell></cell><cell>Topics</cell><cell>POSITIVE</cell><cell>NEGATIVE</cell><cell>Total</cell></row><row><cell>TRAIN DEV DEVTEST TEST</cell><cell cols="4">60 3,591 20 986 20 1,153 100 8,212 2,339 10,551 755 4,346 339 1,325 264 1,417</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>2016 data statistics (Subtasks B and D).</figDesc><table><row><cell></cell><cell>Topics</cell><cell>HIGHLYPOSITIVE</cell><cell>POSITIVE</cell><cell>NEUTRAL</cell><cell>NEGATIVE</cell><cell>HIGHLYNEGATIVE</cell><cell>Total</cell></row><row><cell>TRAIN DEV DEVTEST TEST</cell><cell cols="7">60 437 3,154 20 53 933 20 148 1,005 100 382 7,830 10,081 2,201 138 20,632 1,654 668 87 6,000 675 296 43 2,000 583 233 31 2,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>2016 data statistics (Subtasks C and E).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The confusion matrix for Subtask A. Cell XY stands for "the number of tweets that the classifier labeled X and the gold standard labells as Y ". P , U , N stand for POSITIVE, NEUTRAL, NEGATIVE, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Results for Subtask A "Message Polarity Classification" on the Tweet 2016 dataset. The systems are ordered by their F P N 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>651 10 0.427 13 0.719 3 0.599 13 0.583 11 12 MDSENT 0.589 19 0.509 20 0.587 20 0.386 24 0.606 18 0.593 17 0.580 12 CUFE 0.642 10 0.596 8 0.662 8 0.466 9 0.697 5 0.598 14 0.580 12 14 THUIR 0.616 12 0.575 14 0.648 11 0.399 20 0.640 15 0.617 10 0.576 14 PUT 0.565 21 0.511 19 0.614 19 0.360 27 0.648 14 0.597 15 0.576 14 16 LYS 0.650 8 0.579 13 0.647 12 0.407 18 0.655 11 0.603 12 0.575 16 17 IIP 0.598 17 0.465 23 0.645 13 0.405 19 0.640 15 0.619 9 0.574 17 18 UniPI 0.592 18 0.585 11 0.627 17 0.381 25 0.654 12 0.586 18 0.571 18 19 DIEGOLab16 (*) 0.611 14 0.506 21 0.618 18 0.497 5 0.594 20 0.584 19 0.554 19 20 GTI 0.612 13 0.524 17 0.639 16 0.468 7 0.623 17 0.584 19 0.539 20 21 OPAL 0.567 20 0.562 15 0.556 23 0.395 21 0.593 21 0.531 21 0.505 21 22 DSIC-ELIRF 0.494 25 0.404 26 0.546 26 0.342 29 0.517 24 0.531 21 0.502 22 23 UofL 0.490 26 0.443 24 0.547 25 0.372 26 0.574 22 0.502 25 0.499 23 ELiRF 0.462 28 0.408 25 0.514 28 0.310 33 0.493 25 0.493 26 0.499 23 25 ISTI-CNR 0.538 22 0.492 22 0.572 21 0.327 30 0.598 19 0.508 24 0.494 25 26 SteM 0.518 23 0.315 29 0.571 22 0.320 32 0.405 28 0.517 23 0.478 26 27 Tweester 0.506 24 0.340 28 0.529 27 0.540 3 0.379 29 0.479 28 0.455 27 28 Minions 0.489 27 0.521 18 0.554 24 0.420 16 0.475 26 0.481 27 0.415 28 29 Aicyber 0.418 29 0.361 27 0.457 29 0.326 31 0.440 27 0.432 29 0.402 29 30 mib 0.394 30 0.310 30 0.415 31 0.352 28 0.359 31 0.413 31 0.401 30 31 VCU-TSA 0.383 31 0.307 31 0.444 30 0.425 14 0.336 32 0.416 30 0.372 31 32 SentimentalITists 0.339 33 0.238 33 0.393 32 0.288 34 0.323 34 0.343 33 0.339 32 33 WR 0.355 32 0.284 32 0.393 32 0.430 12 0.366 30 0.377 32 0.330 33 34 CICBUAPnlp 0.193 34 0.193 34 0.335 34 0.393 22 0.326 33 0.303 34 0.303 34</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Historical results for the best systems for Subtask A "Message Polarity Classification" over the years2013-2016.    </figDesc><table><row><cell># System 1 Tweester 2 LYS 3 thecerealkiller 4 ECNU (*) 5 INSIGHT-1 6 PUT 7 UNIMELB 8 TwiSE 9 GTI 10 Finki 11 pkudblab 12 CUFE 13 ISTI-CNR 14 SwissCheese 15 SentimentalITists 0.624 15 0.643 15 0.802 13 ρ P N F P N Acc 1 0.797 1 0.799 1 0.862 3 0.791 2 0.720 10 0.762 17 0.784 3 0.762 5 0.823 9 0.768 4 0.770 4 0.843 5 0.767 5 0.786 3 0.864 2 0.763 6 0.732 8 0.794 14 0.758 7 0.788 2 0.870 1 0.756 8 0.752 6 0.826 8 0.736 9 0.731 9 0.811 11 0.720 10 0.748 7 0.848 4 0.689 11 0.716 11 0.832 7 0.679 12 0.708 12 0.834 6 0.671 13 0.690 13 0.811 11 0.648 14 0.674 14 0.820 10 16 PotTS 0.618 16 0.610 17 0.712 18 17 OPAL 0.616 17 0.633 16 0.792 15 18 WR 0.522 18 0.502 18 0.577 19 19 VCU-TSA 0.502 19 0.448 19 0.775 16 Baseline 0.500 0.438 0.778</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Results for Subtask B "Tweet classification according to a two-point scale" on the Tweet 2016 dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>ranks the 11 teams who participated in Sub-</cell></row><row><cell>task C "Tweet classification according to a five-point</cell></row><row><cell>scale" in terms of the official measure M AE M ; we</cell></row><row><cell>also show M AE</cell></row></table><note>µ (see Equation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Results for Subtask C "Tweet classification according to a five-point scale" on the Tweet 2016 dataset. The systems are ordered by their M AE M score (lower is better). The meaning of "(*)" is as in Table8.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 13 :</head><label>13</label><figDesc>Results</figDesc><table><row><cell>for Subtask D "Tweet quantification accord-ing to a two-point scale" on the Tweet 2016 dataset. The sys-tems are ordered by their KLD score (lower is better). The meaning of "(*)" is as in Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">ranks the 14 teams who participated in Sub-</cell></row><row><cell cols="4">task D "Tweet quantification according to a two-</cell></row><row><cell cols="4">point scale" on the official measure KLD. Two</cell></row><row><cell cols="4">other measures are reported, absolute error (AE):</cell></row><row><cell>AE(p,p, C) =</cell><cell>1 |C| c∈C</cell><cell>|p(c) − p(c)|</cell><cell>(10)</cell></row><row><cell cols="3">and relative absolute error (RAE):</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Results for Subtask E "Tweet quantification according to a five-point scale" on the Tweet 2016 dataset. The systems are ordered by their EM D score (lower is better). The meaning of "(*)" is as in Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We merged OBJECTIVE under NEUTRAL, as previous attempts to have annotators distinguish between the two have consistently resulted in very low inter-annotator agreement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we retired the expression-level subtask A, which was present in SemEval 2013-2015<ref type="bibr" target="#b39">(Nakov et al., 2013;</ref><ref type="bibr" target="#b46">Rosenthal et al., 2014;</ref><ref type="bibr" target="#b47">Rosenthal et al., 2015;</ref><ref type="bibr" target="#b41">Nakov et al., 2016b)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Filtering based on an existing lexicon does bias the dataset to some degree; however, the text still contains sentiment expressions outside those in the lexicon.5  We distributed the datasets to the task participants in a similar way: we only released the annotations and the tweet IDs, and the participants had to download the actual tweets by themselves via the API, for which we provided a script: https://github.com/aritter/twitter download</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://alt.qcri.org/semeval2016/task4/ 7 An earlier version of the scoring script contained a bug, to the effect that for Subtask B it was computing F P N 1 , and not ρ P N . This was detected only after the submissions were closed, which means that participants to Subtask B who used the scoring system (and not their own implementation of ρ P N ) for parameter optimization, may have been penalized in the ranking as a result.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, their affiliation (Column 3) and nationality (Column 4), the subtasks they have participated in (Column 1), and the paper they have contributed (Column 5). Teams whose "Affiliation" column is typeset on more that one row include researchers with different affiliations. Teams marked with a (**) include some of the SemEval 2016 Task 4</p><p>organizers. An empty entry for the "Paper" column indicates that the team have not contributed a system description paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UofL at SemEval-2016 Task 4: Multi domain word2vec for Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Abdelwahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Elmaghraby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">INESC-ID at SemEval-2016 Task 4: Reducing the problem of outof-embedding words</title>
		<author>
			<persName><forename type="first">Ramón</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UniPI at SemEval-2016 Task 4: Convolutional neural networks for sentiment classification</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Sartiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation measures for ordinal regression</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Intelligent Systems Design and Applications</title>
				<meeting>the 9th IEEE International Conference on Intelligent Systems Design and Applications<address><addrLine>Pisa, IT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="283" to="287" />
		</imprint>
	</monogr>
	<note>ISDA 2009</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference on Language Resources and Evaluation</title>
				<meeting>the 7th Conference on Language Resources and Evaluation<address><addrLine>Valletta, MT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OPAL at SemEval-2016 Task 4: the Challenge of Porting a Sentiment Analysis System to the &quot;Real&quot; World</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TwiSE at SemEval-2016 Task 4: Twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Content and network dynamics behind Egyptian political polarization on Twitter</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Borge-Holthoefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW 2015)</title>
				<meeting>the 18th ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW 2015)<address><addrLine>Vancouver, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="700" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VCU-TSA at SemEval-2016 Task 4: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Briones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasun</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive or reactive? Marketing with Twitter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soboleva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Marketing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="491" to="499" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UDLAP at SemEval-2016 Task 4: Sentiment quantification using a graph based representation</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofelia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darnes</forename><surname>Vilariño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Báez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Minions at SemEval-2016 Task 4: Or how to boost a student&apos;s self esteem</title>
		<author>
			<persName><forename type="first">Marius-Valentin</forename><surname>Calin-Cristian Ciubotariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Hrisca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Gliga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Darabana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Trandabat</surname></persName>
		</author>
		<author>
			<persName><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">mib at SemEval-2016 Task 4: Exploiting lexicon based features for sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Vittoria</forename><surname>Cozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinella</forename><surname>Petrocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">QCRI at SemEval-2016 Task 4: Probabilistic methods for binary and ordinal quantification</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US. Forthcoming</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SwissCheese at SemEval-2016 Task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Gonzenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Uzdilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><forename type="middle">De</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kameron</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Danforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aicyber at SemEval-2016 Task 4: i-vector based sentence representation</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentiment quantification</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="75" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing text quantifiers for multivariate loss functions</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery and Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ISTI-CNR at SemEval-2016 Task 4: Quantification on an ordinal scale</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SentimentalITists at SemEval-2016 Task 4: Building a Twitter sentiment analyzer in your backyard</title>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Florean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Bejenaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Apostol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ciobanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Trandabat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counting positives accurately despite inaccurate classification</title>
		<author>
			<persName><forename type="first">George</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th European Conference on Machine Learning (ECML 2005)</title>
				<meeting>the 16th European Conference on Machine Learning (ECML 2005)<address><addrLine>Porto, PT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="564" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Quantifying counts and costs via classification. Data Mining and Knowledge Discovery</title>
		<author>
			<persName><forename type="first">George</forename><surname>Forman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="164" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">IIP at SemEval-2016 Task 4: Prioritizing classes in ensemble classification for sentiment analysis of tweets</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Friedrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MDSENT at SemEval-2016 Task 4: Supervised system for message polarity classification</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prodromos Malakasiotis, and Ion Androutsopoulos. 2016. aueb.twitter.sentiment at SemEval-2016 Task 4: A weighted ensemble of SVMs for Twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Giorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Rousas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CICBUAPnlp at SemEval-2016 Task 4: Discovering Twitter polarity using enhanced embeddings</title>
		<author>
			<persName><forename type="first">Helena</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darnes</forename><surname>Vilariño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Pinto</forename><surname>Avendaño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">YZU-NLP at SemEval-2016 Task 4: Ordinal sentiment classification using a recurrent convolutional network</title>
		<author>
			<persName><forename type="first">Hussam</forename><surname>Hamdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<editor>
			<persName><surname>Us. Yunchao</surname></persName>
			<persName><forename type="first">Liang-Chih</forename><surname>He</surname></persName>
			<persName><forename type="first">Chin-Sheng</forename><surname>Yu</surname></persName>
			<persName><forename type="first">K</forename><forename type="middle">Robert</forename><surname>Yang</surname></persName>
			<persName><forename type="first">Weiyi</forename><surname>Lai</surname></persName>
			<persName><surname>Liu</surname></persName>
		</editor>
		<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego; San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">NTNUSentEval at SemEval-2016 Task 4: Combining general classifiers for fast Twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">Valerij</forename><surname>Brage Ekroll Jahren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Fredriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Gambäck</surname></persName>
		</author>
		<author>
			<persName><surname>Bungum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GTI at SemEval-2016 Task 4: Training a naive Bayes classifier using features of an unsupervised system</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Juncal-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milagros</forename><surname>Tamaraálvarez-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Fernández-Gavilanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">Javier</forename><surname>Costa-Montenegro</surname></persName>
		</author>
		<author>
			<persName><surname>González-Castaño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">NRU-HSE at SemEval-2016 Task 4: The open quantification library with two iterative methods</title>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Porshnev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirill</forename><surname>Rudakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning using Twitter data for improving sentiment classification of Turkish political news</title>
		<author>
			<persName><forename type="first">Mesut</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guven</forename><surname>Fidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail Hakki</forename><surname>Toroslu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on Computer and Information Sciences (ISCIS 2013)</title>
				<meeting>the 28th International Symposium on Computer and Information Sciences (ISCIS 2013)<address><addrLine>Paris, FR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PUT at SemEval-2016 Task 4: The ABC of Twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Lango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dariusz</forename><surname>Brzezinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerzy</forename><surname>Stefanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eugenio Martínez-Cámara, Maria Teresa Martín-Valdivia, Luis Alfonso Ureña López, and Arturo Montejo Ráez</title>
		<author>
			<persName><forename type="first">Micol</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bowick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012)</title>
				<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012)<address><addrLine>Avignon, FR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
	<note>Sentiment analysis in Twitter</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Twitter: A Digital Socioscope</title>
		<author>
			<persName><forename type="first">Yelena</forename><surname>Mejova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Macy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)</title>
				<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)<address><addrLine>Atlanta, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DSIC-ELIRF at SemEval-2016 Task 4: Message polarity classification in Twitter using a support vector machine approach</title>
		<author>
			<persName><surname>Victor Martinez Morant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><forename type="middle">F</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><surname>Pla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CUFE at SemEval-2016 Task 4: A gated recurrent model for sentiment classification</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Nabil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Atyia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 2: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation</title>
				<meeting>the 7th International Workshop on Semantic Evaluation<address><addrLine>Atlanta, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluation measures for the SemEval-2016 Task 4 &quot;Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://alt.qcri.org/semeval2016/task4/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Developing a successful SemEval task in sentiment analysis of Twitter and other social media texts. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="35" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tweester at SemEval-2016 Task 4: Sentiment analysis in Twitter using semantic-affective model adaptation</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramnath</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Us</forename><forename type="middle">Elisavet</forename><surname>Smith ; Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasia</forename><surname>Palogiannidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenia</forename><surname>Kolovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippos</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Iosif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="middle">Jeffrey</forename><surname>Us</surname></persName>
			<persName><forename type="first">Richard</forename><surname>Pennington</surname></persName>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Socher</surname></persName>
			<persName><surname>Manning</surname></persName>
		</editor>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>San Diego; Doha, QA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clustering with error estimation for monitoring reputation of companies on Twitter</title>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">A</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Colm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>'riordan</surname></persName>
		</author>
		<author>
			<persName><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Asia Information Retrieval Societies Conference (AIRS 2013)</title>
				<meeting>the 9th Asia Information Retrieval Societies Conference (AIRS 2013)<address><addrLine>Singapore, SN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="170" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SteM at SemEval-2016 Task 4: Applying active learning to improve sentiment classification</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Räbiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mishal</forename><surname>Kazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yücel</forename><surname>Saygın</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Spiliopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 9: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, IE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 10: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SENSEI-LIF at SemEval-2016 Task 4: Polarity embedding fusion for robust sentiment analysis</title>
		<author>
			<persName><forename type="first">Mickael</forename><surname>Rouvier</surname></persName>
		</author>
		<author>
			<persName><surname>Benoit Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a metric for image retrieval</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wasserstein metric</title>
		<author>
			<persName><forename type="first">Ludger</forename><surname>Rüschendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopaedia of Mathematics</title>
				<editor>
			<persName><forename type="first">Michiel</forename><surname>Hazewinkel</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, NL</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DIEGOLab16 at SemEval-2016 Task 4: Sentiment analysis in Twitter using centroids, clusters, and sentiment lexicons</title>
		<author>
			<persName><forename type="first">Abeed</forename><surname>Sarker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An axiomatically derived measure for the evaluation of classification algorithms</title>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM International Conference on the Theory of Information Retrieval (ICTIR 2015)</title>
				<meeting>the 5th ACM International Conference on the Theory of Information Retrieval (ICTIR 2015)<address><addrLine>Northampton, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">PotTS at SemEval-2016 Task 4: Sentiment analysis of Twitter using characterlevel convolutional neural networks</title>
		<author>
			<persName><forename type="first">Uladzimir</forename><surname>Sidarenka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Finki at SemEval-2016 Task 4: Deep learning architecture for Twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Stojanovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjorgji</forename><surname>Strezoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjorgji</forename><surname>Madjarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yerai</forename><surname>Doval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">LYS at SemEval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exploiting neural activation values for Twitter sentiment classification and quantification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">UNIMELB at SemEval-2016 Task 4: An ensemble of neural networks and a word2vec based model for sentiment classification</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">I2RNTU at SemEval-2016 Task 4: Classifier fusion for polarity classification in Twitter</title>
		<author>
			<persName><surname>Vikrant Yadav ; San</surname></persName>
		</author>
		<author>
			<persName><surname>Diego</surname></persName>
		</author>
		<author>
			<persName><surname>Us. Zhengchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisi</forename><surname>Fuxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">thecerealkiller at SemEval-2016 Task 4: Deep learning based system for classifying sentiment of tweets on two point scale</title>
				<meeting><address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2016 Task 4: An empirical investigation of traditional NLP features and word embedding features for sentence-level and topic-level sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Yunxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
