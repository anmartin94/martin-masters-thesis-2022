<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
							<email>mpontiki@ilsp.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language and Speech Processing</orgName>
								<orgName type="institution">Athena R.C</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
							<email>galanisd@ilsp.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language and Speech Processing</orgName>
								<orgName type="institution">Athena R.C</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language and Speech Processing</orgName>
								<orgName type="institution">Athena R.C</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
							<email>suresh@cs.york.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language and Speech Processing</orgName>
								<orgName type="institution">Athena R.C</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence-or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.</p><p>1 A subset of the datasets has been annotated with aspects at the sentence level.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>The rise of e-commerce, as a new shopping and marketing channel, has led to an upsurge of review sites for a variety of services and products. In this context, Aspect Based Sentiment Analysis (ABSA) -i.e., mining opinions from text about specific entities and their aspects-can help consumers decide what to purchase and businesses to better monitor their reputation and understand the needs of the market <ref type="bibr" target="#b10">(Pavlopoulos 2014)</ref>. Given a target of interest (e.g., Apple Mac mini), an ABSA method can summarize the content of the respective reviews in an aspect-sentiment table like the one in Fig <ref type="figure" target="#fig_0">1</ref>. Some review sites also generate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews.</p><p>Several ABSA methods have been proposed for various domains, like consumer electronics <ref type="bibr">Liu {2004a, 2004b})</ref>, restaurants <ref type="bibr" target="#b3">(Ganu et al., 2009)</ref> and movies <ref type="bibr" target="#b19">(Thet et al., 2010)</ref>. The available methods can be divided into those that adopt domain-independent solutions <ref type="bibr" target="#b7">(Lin and He, 2009)</ref>, and those that use domain-specific knowledge to improve their results <ref type="bibr" target="#b19">(Thet et al., 2010)</ref>. Typically, most methods treat aspect extraction and sentiment classification separately <ref type="bibr" target="#b1">(Brody and Elhadad, 2010)</ref>, but there are also approaches that model the two problems jointly <ref type="bibr" target="#b6">(Jo and Oh, 2011)</ref>. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages <ref type="bibr" target="#b10">(Pavlopoulos 2014)</ref>. For example, the datasets of <ref type="bibr" target="#b8">McAuley et al. (2012)</ref> provide aspects and respective ratings at the review level (i.e., aspects and ratings associated with entire reviews, not particular sentences) 1 about Beers, Pubs, Toys and Games, and Audiobooks. The reviews are obtained from sites that allow users to evaluate a product not only in terms of its overall quality, but also focusing on specific predefined aspects (e.g. "smell" and "taste" for Beers, "fun" and "educational value" for Toys and Games). The IGGSA Shared Tasks on German Sentiment Analysis <ref type="bibr" target="#b13">(Ruppenhofer et al., 2014)</ref> provided human annotated datasets of political speeches <ref type="bibr">(STEPS task)</ref> and reviews about products (StAR task) like coffee machines and washers. The StAR task focused on the extraction of evaluative phrases (e.g., "bad") and aspect expressions (e.g., "washer"). The STEPS dataset includes annotations for evaluative phrases, opinion targets, and the corresponding sources (opinion holders). The extraction of opinion targets and holders has also been addressed in the context of the Multilingual Opinion Analysis Task <ref type="bibr" target="#b14">(Seki et al., 2007;</ref><ref type="bibr" target="#b15">Seki et al., 2008;</ref><ref type="bibr" target="#b16">Seki et al., 2010)</ref> and the Sentiment Slot Filling 2 Task of the Knowledge Base Population Track <ref type="bibr" target="#b9">(Mitchell, 2013)</ref>. However, these tasks deal with the identification of opinion targets in general, not in the context of ABSA.</p><p>SemEval-2014 Task 4 (SE-ABSA14) provided datasets annotated with aspect terms (e.g., "hard disk", "pizza") and their polarity for laptop and restaurant reviews, as well as coarser aspect categories (e.g., PRICE) and their polarity only for restaurants 3 <ref type="bibr" target="#b12">(Pontiki et al., 2014)</ref>. The task attracted 165 submissions from 32 teams that experimented with a variety of features (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has been adopted for the creation of aspect-level sentiment datasets in other languages, like Czech <ref type="bibr" target="#b17">(Steinberger et al., 2014)</ref>.</p><p>SemEval-2015 Task 12 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks (aspect category extraction, aspect term extraction, polarity classification) into a principled unified framework (described in Section 2). In addition, SE-ABSA15 included an aspect level polarity classification subtask for the hotels domain in which no training data were provided (out-of-domain ABSA). The annotation schema and the provided datasets are described in Section 3. The evaluation measures and the baseline methods are described in Section 4, while the evaluation scores and the main characteristics of the developed systems are presented in Section 5. The paper concludes with a general assessment of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Set-Up</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ABSA Framework: From SE-ABSA14 to SE-ABSA15</head><p>In SE-ABSA14, given a sentence from a user review about a target entity e (e.g., a laptop), the goal was to identify all aspects (explicit terms or categories) and the corresponding polarities. Following <ref type="bibr" target="#b0">Liu (2006) &amp;</ref><ref type="bibr" target="#b20">Zhang and</ref><ref type="bibr" target="#b20">Liu (2014)</ref>, an aspect (term or category) indicated: (a) a part/component of e (e.g., battery), (b) an attribute of e (e.g., price), or (c) an attribute of a part/component of e (e.g., battery life). In SE-ABSA15, an aspect category is defined as a combination of an entity type E and an attribute type A. This definition of aspect makes more explicit the difference between entities and the particular facets that are being evaluated. E can be the reviewed entity e itself (e.g., laptop), a part/component of it (e.g., battery or customer support), or another relevant entity (e.g., the manufacturer of e), while A is a particular attribute (e.g., durability, quality) of E. E and A are concept names (classes) from a given domain ontology and do not necessarily occur as terms in a sentence. For example, in "They sent it back with a huge crack in it and it still didn't work; and that was the fourth time I've sent it to them to get fixed" the reviewer is evaluating the quality (A) of the customer support (E) without explicitly mentioning it.</p><p>In contrast to SE-ABSA14, in the current framework aspect terms correspond to explicit mentions of the entities E (e.g., service, pizza) or attributes A (e.g., price, quality). However, only the extraction of the explicit mentions of E is required (see Section 2.2). Another difference is that the datasets of SE-ABSA15 consist of whole reviews, not isolated sentences. Correctly identifying the E, A pairs of a sentence and their polarities often requires examining a wider part or the whole review.</p><p>In this setting, the ABSA problem has been formalized into a principled unified framework in which all the identified constituents of the expressed opinions (i.e., opinion target expressions, aspects and sentiment polarities) meet a set of guidelines/specifications and are linked to each other within tuples. The extracted tuples directly reflect the intended meaning of the texts and, thus, can be used to generate structured aspect-based opinion summaries from user reviews in realistic applications (e.g., review sites).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Description</head><p>SE-ABSA15 consisted of the following subtasks. Participants were free to choose the subtasks, slots and domains they wished to participate in.</p><p>Subtask 1: In-domain ABSA. Given a review text about a laptop or restaurant, identify all the opinion tuples with the following types (tuple slots) of information:</p><p>Slot 1: Aspect Category. The goal is to identify every entity E and attribute A pair towards which an opinion is expressed in the given text. E and A should be chosen from predefined inventories of entity types (e.g., LAPTOP, MOUSE, RESTAURANT, FOOD) and attribute labels (e.g., <ref type="bibr">DESIGN, PRICE, QUALITY)</ref>. The E, A inventories for each domain are described in section 3.</p><p>Slot 2: Opinion Target Expression (OTE). The task is to extract the OTE, i.e., the linguistic expression used in the given text to refer to the reviewed entity E of each E#A pair. The OTE is defined by its starting and ending offsets. When there is no explicit mention of the entity, the slot takes the value "NULL". The identification of Slot 2 values was required only in the restaurants domain.</p><p>Slot 3: Sentiment Polarity. Each identified E#A pair has to be assigned one of the following polarity labels: positive, negative, neutral (mildly positive or mildly negative sentiment).</p><p>Two examples of opinion tuples with Slot 1-3 values from the restaurants domain are shown below. Such tuples can be used to generate aspectsentiment tables like the one of Fig 1.</p><p>a. The food was delicious but do not come here on an empty stomach. → {category= "FOOD#QUALITY", target= "food", from: "4", to: "8", polarity= "positive"}, {category= "FOOD#STYLE_OPTIONS" 4 , target = "food", from: "4", to: "8", polarity= "negative"} b. Prices are in line. → {category: "RESTAURANT#PRICES", target= "NULL", from: "-", to: "-", polarity: "neutral"} Subtask 2: Out-of-domain ABSA. In this subtask, participants had the opportunity to test their systems in a previously unseen domain (hotel reviews) for which no training data was made available. The gold annotations for Slots 1 and 2 were provided and the teams had to return the sentiment polarity values (Slot 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Datasets for three domains (laptops, restaurants, hotels) were provided; consult Table <ref type="table" target="#tab_0">1</ref>  Note that in the domain of hotels no training data were provided (Out-of-Domain ABSA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation Schema and Guidelines</head><p>Given a review text about a laptop, a restaurant or a hotel, the task of the annotators was to identify opinions expressed towards specific entities and their attributes and to assign the respective aspect category (Slot 1) and polarity (Slot 3) labels. The category (E#A) values had to be chosen from predefined inventories of entities and attributes for each domain; the inventories were described in detail in the respective annotation guidelines 5 . In particular, the entity E could be assigned 22 possible labels for the laptops domain (e.g., LAPTOP, SOFTWARE, SUPPORT), 6 labels for the restaurants domain (e.g., RESTAURANT, FOOD), and 7 labels for the hotels domain (e.g., HOTEL, ROOMS). The attribute A could be assigned 9 possible labels for the laptops domain (e.g., USABILITY), 5 labels for the restaurants domain (e.g., QUALITY), and 8 labels for the hotels domain (e.g., COMFORT). The full inventories of the aspect category labels for each domain are provided below in appendices A-C. Quite often reviews contain opinions towards entities that are not directly related to the entity being reviewed, for example, restaurants/hotels that the reviewer has visited in the past, other laptops or products (and their components) of the same or a competitive brand. Such entities as well as comparative opinions are considered to be out of the scope of SE-ABSA15. In these cases, no opinion annotations were provided.</p><p>The {E#A, polarity} annotations had to be assigned at the sentence level taking into account the context of the whole review. For example, "Laptop still did not work, blue screen within a week..." (Previous sentence: "Horrible customer supportthey lost my laptop for a month-got it back 3 months later") had to be assigned a negative opinion about the customer support, not about the operation of the laptop, as implied by the previous sentence. Similarly, in "I was so happy with my new Mac." (Next sentences: "For two months... Then the hard drive failed."), even though the reviewer says how happy he/she was with the laptop, he/she is expressing a negative opinion.</p><p>For the polarity slot the possible values were: positive, negative, and neutral. Contrary to SE-ABSA14, the "neutral" label applies only to mildly positive or mildly negative sentiment, thus it does not indicate objectivity (e.g., "Food was okay, nothing great." → {FOOD#QUALITY, "Food", neu-tral}). Another difference is that this year the "conflict" label was not used, since -due to the adopted fine-grained aspect classification schema-it is very rare to encounter (in a sentence) both a positive and a negative opinion about the same attribute A of an entity E. In the few cases where this happened, the dominant sentiment was chosen (e.g., "The OS takes some getting used to but the learning curve is so worth it!" → {OS#USABILITY, positive}).</p><p>For the restaurants and the hotels domain the annotators also had to tag the OTE (explicit mention) for each identified entity E (Slot 2). Such mentions can be named entities (e.g., "The Four Seasons"), common nouns (e.g., "place", "steak", "bed") or multi-word terms (e.g., "vitello alla marsala", "conference/banquet room"). Similarly to SE-ABSA14, the identified OTEs were annotated as they appeared, even if misspelled. When an evaluated entity E was only implicitly inferred or referred to (e.g., through pronouns), the OTE slot was assigned the value "NULL" (e.g. "Everything was wonderful." → {RESTAURANT#GENERAL, NULL, positive}).</p><p>In the laptops domain we did not provide OTE annotations, since most entities are instantiated through a limited set of expressions (e.g., MEMORY: "memory", "ram", CPU: "processing power", "processor", "cpu") as opposed to the restaurants domain, where for example, the entity "FOOD" is instantiated through a variety of food types and dishes (e.g. "pizza", "Lobster Cobb Salad"). Furthermore, LAPTOP, which is the majority category label in laptops (see Section 3.3), is instantiated mostly through pronominal mentions, while the explicit mentions are limited to nouns like laptop, computer, product, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Process and Statistics</head><p>Each dataset was annotated by a linguist (annotator A) using BRAT <ref type="bibr" target="#b18">(Stenetorp et al., 2012)</ref>, a webbased annotation tool, which was configured appropriately for the needs of the task. Then, one of the organizers (annotator B) validated/inspected the resulting annotations. When B was not confident or disagreed with A, a decision was made collaboratively between them and a third annotator. The main disagreements encountered during the annotation process are summarized below: Slot 1. In the laptops domain the main difficulty was that in some negative evaluations the annotators were unsure about the actual problem/target. For example, in "Sometimes the screen even goes black on this computer", the black screen may be related to the graphics, the laptop operation (e.g., motherboard issue) or the screen itself. The decision for such cases was to assign the E#A pair that reflected what the reviewer is saying and not the possible interpretations that a technician would give. So, if someone reports screen issues without providing further details, then the opinion is considered to be about the screen 6 . Another issue was when an attribute could be inferred from an explicitly evaluated attribute. For example, DESIGN affects USABILITY (e.g., "With the switch being at the top you need to memorize the key combination rather than just flicking a switch"). In such cases annotators assigned both attribute labels. The annotation in the restaurants domain was easier, due to the less fine-grained schema. A common problem was that (as in SE-ABSA14) the distinction between the GENERAL and MISCELLANEOUS and between the RESTAURANT and AMBIENCE labels was not always clear.</p><p>Slot 2. The annotators found it easier to identify explicit references to the target entities as opposed to the more general aspect terms of SE-ABSA14. However, the problem of distinguishing aspect terms when they appear in conjunctions or disjunctions remains. In this case the maximal phrase (e.g. the entire conjunction or disjunction) is annotated (e.g. "Greek or Cypriot dishes" instead of "Greek dishes", "Cypriot dishes"). Slot 3. Most cases in which the annotators had difficulty deciding the correct polarity label fall into one of the following categories: (a) Change of sentiment over time. Some reviewers tend to start their review by saying how excited they were at first (e.g., with the laptop) and continue by reporting problems or negative evaluations. (b) Negative fact vs. positive opinion. Some reviewers do mention particular deficiencies of a laptop or a restau-rant saying, however, at the same time that they do not bother (e.g., "Overheats but put a pillow and problem solved!"). (c) Mildly positive and negative sentiments are both denoted by the "neutral" label. In some cases the annotators reported that it would be helpful to have a more fine-grained schema (e.g., "negative", "somewhat negative", "neutral", "somewhat positive", "positive"). Finally, in some cases it is difficult to decide a polarity label without knowing the reviewer's intention (e.g., "50% of the food was very good").</p><p>The annotation process resulted in 5,761 opinion tuples in total that correspond to more than 15,000 label assignments (E, A, OTE, polarity); consult Table <ref type="table" target="#tab_2">2</ref>  The distribution of the category annotations in the restaurants domain (Fig. <ref type="figure">2</ref>) is similar across the training and test set. In the laptops domain, 81 E,A combinations (different pairs) were annotated in the training set and 58 in the test set. LAPTOP is the majority entity class in both sets; 62.36% in training, 72.81% in test data. Figure <ref type="figure">3</ref> presents the distribution for all the attributes of the LAPTOP entity in the training and test sets. Again, the category distributions are similar. The remaining 37.64% of the annotations in the laptops training data correspond to 72 categories with frequencies ranging from 6.53% to 0.05%. In the test set, the remaining 27.19% of the annotations correspond to 49 categories with frequencies from 2.32 % to 0.11%.</p><p>Regarding polarity, positive is the majority class in all domains (Table <ref type="table">3</ref>). The polarity distribution is balanced in the laptops domain, while in the restaurants domain there is a significant imbalance between the positive and negative classes across the training and the test sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Datasets Format and Availability</head><p>The datasets 7 of the SE-ABSA15 task were provided in an XML format. They are available under a non-commercial, no redistribution license through META-SHARE 8 , a repository devoted to the sharing and dissemination of language resources <ref type="bibr" target="#b11">(Piperidis, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Measures and Baselines</head><p>Similarly to SE-ABSA14, the evaluation ran in two phases. In Phase A, the participants were asked to return the {category, OTE} tuples for the restaurants domain and only the category slot (Slot1) for the laptops domain. Subsequently, in Phase B, the participants were given the gold annotations for the reviews of Phase A and they were asked to return the polarity (Slot3). Each participating team was allowed to submit up to two runs per slot and domain in each phase; one constrained (C), where only the provided training data could be used, and one unconstrained (U), where other resources (e.g., publicly available lexica) and additional data of any kind could be used for training. In the latter case, the teams had to report the resources they used. To evaluate aspect category (Slot1) and OTE extraction (Slot2) in Phase A, we used the F-1 measure. To evaluate sentiment polarity (Slot 3) in Phase B, we used accuracy. Furthermore, we implemented and provided three baselines (see below) for the respective slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Measures</head><p>Slot 1: F-1 scores are calculated by comparing the category annotations that a system returned (for all the sentences) to the gold category annotations (using micro-averaging). These category annotations are extracted from the values of Slot 1 (category). Duplicate occurrences of categories (for the same sentence) are ignored.</p><p>Slot 2: F-1 scores are calculated by comparing the targets that a system returned (for all the sentences) to the corresponding gold targets (using micro-averaging). The targets are extracted using their starting and ending offsets. The calculation for each sentence considers only distinct targets and discards NULL targets, since they do not correspond to explicit mentions.</p><p>Slot 1&amp;2 (jointly): Again F-1 scores are calculated by comparing the {category, OTE} tuples of a system to the gold ones (using micro-averaging).</p><p>Slot 3: To evaluate sentiment polarity detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted polarity labels of aspect categories, divided by the total number of aspect categories. Recall that we use the gold aspect categories in Phase B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Slot 1: For category (E#A) extraction, a Support Vector Machine (SVM) with a linear kernel was trained. In particular, n unigram features are extracted from the respective sentence of each tuple that is encountered in the training data. The category value (e.g., SERVICE#GENERAL) of the tuple is used as the correct label of the feature vector. Similarly, for each test sentence s, a feature vector is built and the trained SVM is used to predict the probabilities of assigning each possible category to s (e.g., {SERVICE#GENERAL, 0.2}, {RESTAURANT# GENERAL, 0.4}. Then, a threshold 9 t is used to decide which of the categories will be assigned 10 to s. As features, we use the 1,000 most frequent unigrams of the training data excluding stop-words.</p><p>Slot 2: The baseline uses the training reviews to create for each category c (e.g., SERVICE# GENERAL) a list of OTEs (e.g., SERVICE#GENERAL → {"staff", "waiter"}). These are extracted from the (training) opinion tuples whose category value is c. Then, given a test sentence s and an assigned category c, the baseline finds in s the first occurrence of each OTE of c's list. The OTE slot is filled with the first of the target occurrences found in s. If no target occurrences are found, the slot is assigned the value NULL.</p><p>Slot 3: For polarity prediction we trained a SVM classifier with a linear kernel. Again, as in Slot 1, n unigram features are extracted from the respective sentence of each tuple of the training data. In addition, an integer-valued feature 11 that indicates the category of the tuple is used. The correct label for the extracted training feature vector is the corresponding polarity value (e.g., positive). Then, for each tuple {category, OTE} of a test sentence s, a feature vector is built and it is classified using the trained SVM. Furthermore, for Slot 3 we also used a majority baseline that assigns the most frequent polarity (in the training data) to all test tuples.</p><p>The baseline systems and evaluation scripts are available for download as a single zip from the SE-ABSA15 website 12 . They are implemented in Java and can be used via a Linux shell script. The baselines use the LibSVM package <ref type="bibr">13</ref>  <ref type="bibr" target="#b2">(Chang and Lin, 2011)</ref> for SVM training and prediction. The scores of the baselines in the test datasets are presented in Tables 4-8 along with the system scores. <ref type="bibr">9</ref> The threshold t was tuned on a subset of the training data (for each domain) using a trial and error approach. We use the -b 1 option of LibSVM to obtain probabilities. <ref type="bibr">11</ref> Each category (E#A pair) has been assigned a distinct integer value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>In total, the task attracted 92 submissions from 16 teams. The evaluation results per phase and slot are presented below. For the teams that submitted more than one unconstrained runs per slot and domain, we included in the tables only the run with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results of Phase A</head><p>The aspect category identification slot attracted 6 teams for the laptops dataset and 9 teams for the restaurants dataset (consult Table <ref type="table">4</ref>). As expected, the systems achieved significantly higher scores (+12%) in the restaurants domain since in this domain the classification schema is less fine-grained; it contains 6 entity types and 5 attribute classes that result in 12 possible combinations, as opposed to the laptops domain where the 22 entities and 9 attribute labels give rise to more than 80 combinations. The best F-1 scores in both domains, 50.86% for laptops and 62.68% for restaurants, were achieved by the unconstrained submission of the NLANGP team, which modeled aspect category extraction as a multiclass classification problem with features based on n-grams, parsing, and word clusters learnt from Amazon and Yelp data (for laptops and restaurants, respectively). The system of Sentiue (scores: 50% on laptops, 54.10% on restaurants) used a separate MaxEnt classifier with bag-of-word-like features (e.g. words, lemmas) for each entity and for each attribute. Subsequently, heuristics are applied to the output of the classifiers to determine which categories will be assigned to each sentence. Finally, as expected, the scores are significantly lower when systems have to link the extracted OTEs to the relevant aspect categories (Slot1&amp;2 jointly). As shown in Table <ref type="table">6</ref>, the best F-1 score (42.90%) was achieved by the NLANGP team that simply combined the output for each slot to construct the corresponding tuples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of Phase B</head><p>The sentiment polarity slot attracted 10 teams for the laptops and 12 teams for the restaurants domain (see Most teams performed (slightly) better in the laptops domain. This is probably due to the fact that in the restaurants domain the positive polarity is significantly more frequent in the training than in the test data, which may have led to biased models. Nevertheless, most system scores indicate robustness across the two domains, with Sentiue achieving the most stable performance: 79.34% in laptops and 78.69% in restaurants.</p><p>A similar score was obtained also by Sentiue in the hidden domain (78.76%). The (hidden) hotels domain (subtask 2) attracted 9 teams. Lsislif achieved the best score based on a Liblinear model developed for the restaurants domain. LT3 achieved the second best score (80.53%) with an SVM model trained on the restaurants training data. The model used features based on unigrams, sentiment lexica (by Bing Liu, General Inquirer) and PMI scores learnt from TripAdvisor data. The team of EliXa (79.64%) used a multiclass SVM and features based on word clusters, lemmas, ngrams, POS tagging, and well known sentiment lexica. The system of Sentiue (78.76%) is somewhat similar; it uses BOW, POS tags, lemmas, and sentiment lexica. The results of some systems (LT3, EliXa, V3) suggest that the hidden domain was easier, but other systems (e.g., ECNU, wnlp) achieved significantly lower scores in the hidden domain, compared to the in-domain ABSA scores.  <ref type="table">8</ref>. Accuracy scores for slot 3 (polarity extraction). * indicate unconstrained systems. The evaluated run of UMDuluthC team was submitted after the deadline had expired but before the release of the gold polarity labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The SE-ABSA15 task is a continuation of SE-ABSA14 task. The SE-ABSA15 task provided a new definition of aspect -that makes explicit the difference between entities and the particular facets that are being evaluated-within a new principled, unified ABSA framework and output representation, which may be used in realistic applications (e.g., review sites). We also provided benchmark datasets containing manually annotated reviews from three domains (restaurants, laptops, hotels) and baselines for the respective SE-ABSA15 slots. The task attracted 93 submissions from 16 teams that were evaluated in three slots: aspect categories, opinion target expressions, and polarity classifica-tion. Future work includes applying the new framework and annotation schema to other languages (e.g., Spanish, Greek) and enhancing it with information about topics or events, opinion holders, and annotations for linguistic phenomena like metaphor and irony.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1.Table summarizing the average sentiment for each aspect of an entity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>12 http://alt.qcri.org/semeval2015/task12/index.php?id=dataand-tools 13 http://www.csie.ntu.edu.tw/~cjlin/libsvm/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>for more information. Datasets provided for ABSA.</figDesc><table><row><cell></cell><cell>Laptops</cell><cell>Restaurants</cell><cell>Hotels</cell></row><row><cell></cell><cell></cell><cell>Training data</cell><cell></cell></row><row><cell>Review texts</cell><cell>277</cell><cell>254</cell><cell>-</cell></row><row><cell>Sentences</cell><cell>1739</cell><cell>1315</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Test data</cell><cell></cell></row><row><cell>Review texts</cell><cell>173</cell><cell>96</cell><cell>30</cell></row><row><cell>Sentences</cell><cell>761</cell><cell>685</cell><cell>266</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Aspect category (E#A) distribution in the restaurants domain. REST = restaurant, SERV = service, AMB = ambience, LOC = location, GEN=general, PRIC = price, S&amp;O = style&amp;options, MISC= miscellaneous Figure 3. LAPTOP#ATTRIBUTE categories distribution in the laptops domain. LP= laptop, O&amp;P= operation &amp;performance, QUAL= quality, D&amp;F= design &amp;features, USAB=usability, CONN=connectivity, PORT=portability.</figDesc><table><row><cell>35.13%</cell><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell>Test</cell></row><row><cell>32.07%</cell><cell>17.40% 20.71%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>16.26% 16.20%</cell><cell>11.06% 9.11%</cell><cell>5.62% 4.73%</cell><cell cols="2">3.75% 3.26% 2.90% 4.50% 3.67% 4.14%</cell><cell>2.06% 1.57% 1.42% 0.71% 0.95% 1.21%</cell><cell>0.91% 0.59%</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>for more information. Number of tuples annotated per dataset.</figDesc><table><row><cell cols="2">Laptops</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">training test</cell><cell>total</cell></row><row><cell>{E#A, polarity}</cell><cell>1974</cell><cell>949</cell><cell>2923</cell></row><row><cell cols="2">Restaurants</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">training test</cell><cell>total</cell></row><row><cell>{E#A, OTE, polarity}</cell><cell>1654</cell><cell>845</cell><cell>2499</cell></row><row><cell cols="2">Hotels</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">training test</cell><cell>total</cell></row><row><cell>{E#A, OTE, polarity}</cell><cell>-</cell><cell>339</cell><cell>339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 4. F-1 scores for aspect category extraction (slot 1). * indicate unconstrained systems.The OTE slot, which was used only in the restaurants domain, attracted 14 teams; consult Table5. The best F1 score (70.05%) was achieved by the</figDesc><table><row><cell>Baseline</cell><cell>48.06</cell><cell>Baseline</cell><cell>51.32</cell></row><row><cell cols="4">unconstrained submission of EliXa that addressed</cell></row><row><cell cols="4">the problem using an averaged perceptron with a</cell></row><row><cell cols="4">BIO tagging scheme. The features EliXa used in-</cell></row><row><cell cols="4">cluded n-grams, token classes, n-gram prefixes and</cell></row><row><cell cols="4">suffixes, and word clusters learnt from additional</cell></row><row><cell cols="4">data (Yelp for Brown and Clark clusters; Wikipe-</cell></row><row><cell cols="4">dia for word2vec clusters). Similarly, NLANGP</cell></row><row><cell cols="4">(67.11%) was based on a Conditional Random</cell></row><row><cell cols="4">Fields (CRF) model with features based on word</cell></row><row><cell cols="4">strings, head words (obtained from parse trees),</cell></row><row><cell cols="4">name lists (e.g. extracted using frequency), and</cell></row><row><cell cols="2">Brown clusters.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Restaurants</cell><cell></cell></row><row><cell>Team</cell><cell>F1</cell><cell>Team</cell><cell>F1</cell></row><row><cell>EliXa</cell><cell>70.05*</cell><cell>UMDuluthC</cell><cell>50.36</cell></row><row><cell>NLANGP</cell><cell>67.11*</cell><cell>UMDuluthT</cell><cell>50.36</cell></row><row><cell>IHS-RD.</cell><cell>63.12</cell><cell>LT3</cell><cell>49.97*</cell></row><row><cell>Lsislif</cell><cell>62.22</cell><cell>UFRGS</cell><cell>49.32*</cell></row><row><cell>NLANGP</cell><cell>61.49</cell><cell>V3</cell><cell>45.67*</cell></row><row><cell>wnlp</cell><cell>57.63</cell><cell>Sentiue</cell><cell>39.82*</cell></row><row><cell>SIEL</cell><cell>53.38*</cell><cell>CU-BDDA</cell><cell>36.01</cell></row><row><cell>TJUdeM</cell><cell>52.44*</cell><cell>CU-BDDA</cell><cell>33.86*</cell></row><row><cell>Baseline</cell><cell></cell><cell>48.06</cell><cell></cell></row><row><cell cols="4">Table 5. Results for OTE extraction (slot 2). * indicate</cell></row><row><cell></cell><cell cols="2">unconstrained systems.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Laptops</cell><cell>Restaurants</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Team</cell><cell>F1</cell><cell>Team</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NLANGP 50.86*</cell><cell>NLANGP</cell><cell>62.68*</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sentiue</cell><cell>50.00*</cell><cell>NLANGP</cell><cell>61.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IHS-RD.</cell><cell>49.59</cell><cell>UMDuluthC</cell><cell>57.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NLANGP 49.06</cell><cell>UMDuluthT</cell><cell>57.19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TJUdeM</cell><cell>46.49</cell><cell>SIEL</cell><cell>57.14*</cell></row><row><cell></cell><cell></cell><cell></cell><cell>UFRGS</cell><cell>44.95</cell><cell>Sentiue</cell><cell>54.10*</cell></row><row><cell></cell><cell></cell><cell></cell><cell>UFRGS</cell><cell>44.73*</cell><cell>LT3</cell><cell>53.67*</cell></row><row><cell></cell><cell></cell><cell></cell><cell>V3</cell><cell>24.94*</cell><cell>TJUdeM</cell><cell>52.44*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UFRGS</cell><cell>52.09*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UFRGS</cell><cell>51.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IHS-RD.</cell><cell>49.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IHS-RD.</cell><cell>49.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>V3</cell><cell>41.85*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 Table 7 .</head><label>77</label><figDesc>). The best accuracy scores in both domains, 79.34% for laptops and 78.69% for restaurants, were achieved by Sentiue with a MaxEnt Accuracy scores for slot 3 (polarity extraction). * indicate unconstrained systems. The evaluated run of SIEL team was submitted after the deadline had expired, but before the release of the gold polarity labels.</figDesc><table><row><cell cols="4">classifier along with features based on n-grams,</cell></row><row><cell cols="4">POS tagging, lemmatization, negation words and</cell></row><row><cell cols="4">publicly available sentiment lexica (MPQA, Bing</cell></row><row><cell cols="4">Liu's lexicon, AFINN). The system of ECNU</cell></row><row><cell cols="4">(scores: 78.29% laptops, 78.10% restaurants) used</cell></row><row><cell cols="4">features based on n-grams, PMI scores, POS tags,</cell></row><row><cell cols="4">parse trees, negation words and scores based on 7</cell></row><row><cell cols="4">sentiment lexica. The lsislif team (77.87% laptops,</cell></row><row><cell cols="4">75.50% restaurants) relied on a logistic regression</cell></row><row><cell cols="4">model (Liblinear) with various features: syntactic</cell></row><row><cell cols="4">(e.g., unigrams, negation), semantic (Brown dic-</cell></row><row><cell cols="4">tionary), sentiment (e.g., MPQA, SentiWordnet).</cell></row><row><cell>Laptops</cell><cell></cell><cell cols="2">Restaurants</cell></row><row><cell>Team</cell><cell>Acc.</cell><cell>Team</cell><cell>Acc.</cell></row><row><cell>Sentiue</cell><cell cols="2">79.34* Sentiue</cell><cell>78.69*</cell></row><row><cell>ECNU</cell><cell>78.29</cell><cell>ECNU</cell><cell>78.10*</cell></row><row><cell>Lsislif</cell><cell>77.87</cell><cell>Lsislif</cell><cell>75.50</cell></row><row><cell>ECNU</cell><cell cols="2">74.49* LT3</cell><cell>75.02*</cell></row><row><cell>LT3</cell><cell cols="2">73.76* UFRGS</cell><cell>71.71</cell></row><row><cell>TJUdeM</cell><cell cols="2">73.23* Wnlp</cell><cell>71.36</cell></row><row><cell>EliXa</cell><cell cols="2">72.91* UMDuluthC</cell><cell>71.12</cell></row><row><cell>Wnlp</cell><cell>72.07</cell><cell>EliXa</cell><cell>70.05*</cell></row><row><cell>EliXa</cell><cell>71.54</cell><cell>ECNU</cell><cell>69.82</cell></row><row><cell>V3</cell><cell cols="2">68.38* V3</cell><cell>69.46*</cell></row><row><cell>UFRGS</cell><cell>67.33</cell><cell>TJUdeM</cell><cell>68.87*</cell></row><row><cell>SINAI</cell><cell>65.85</cell><cell>EliXa</cell><cell>67.33</cell></row><row><cell>SINAI</cell><cell cols="2">51.84* SINAI</cell><cell>60.71*</cell></row><row><cell></cell><cell></cell><cell>SIEL</cell><cell>70.76*</cell></row><row><cell>SVM+ BOW</cell><cell>69.96</cell><cell>SVM+ BOW</cell><cell>63.55</cell></row><row><cell>Baseline</cell><cell></cell><cell>Baseline</cell><cell></cell></row><row><cell>Majority Base-</cell><cell>57.00</cell><cell>Majority Base-</cell><cell>53.72</cell></row><row><cell>line</cell><cell></cell><cell>line</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 3 The SE-ABSA14 inventory of categories for the restaurants domain is similar to the one of<ref type="bibr" target="#b3">Ganu et al. (2009)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Opinions evaluating the food quantity (e.g. portions size) are assigned the label "FOOD#STYLE_OPTIONS".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The detailed annotation guidelines are available at: http://alt.qcri.org/semeval2015/task12/index.php?id=data-andtools</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">"Blue screen" is an exception since it is well-known that it refers to the laptop operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The data are available at http://metashare.ilsp.gr:8080/.8 META-SHARE (http: //www.metashare.org/) was implemented in the framework of the META-NET Network of Excellence (http://www.meta-net.eu/).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Konstantina Papanikolaou, who carried out a critical part of the annotation process, Thomas Keefe for his help during the initial phases of the annotation process, Juli </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond the stars: Improving rating predictions using review text content</title>
		<author>
			<persName><forename type="first">Gayatree</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelie</forename><surname>Marian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WebDB</title>
				<meeting>WebDB<address><addrLine>Providence, Rhode Island, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD, pages 168-177</title>
				<meeting>KDD, pages 168-177<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
				<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management</title>
				<meeting>the 18th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>pag- es 1020-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining, ICDM &apos;12</title>
				<meeting>the 12th IEEE International Conference on Data Mining, ICDM &apos;12<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the TAC2013 Knowledge Base Population Evaluation: English Sentiment Slot Filling</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Analysis Conference (TAC)</title>
				<meeting>the Text Analysis Conference (TAC)<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Greece</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Informatics, Athens University of Economics and Business</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The META-SHARE language resources sharing infrastructure: Principles, challenges, solutions</title>
		<author>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC-2012</title>
				<meeting>LREC-2012<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">IGGSA Shared Tasks on German Sentiment Analysis (GESTALT)</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">Maria</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proceedings of the 12th Edition of the KONVENS Conference</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of opinion analysis pilot task at ntcir-6</title>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Kirk</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><surname>Hsin-Hsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noriko</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-6 Workshop Meeting</title>
				<meeting>NTCIR-6 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of multilingual opinion analysis task at NTCIR-7</title>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Kirk</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access</title>
				<meeting>the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="185" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of Multilingual Opinion Analysis Task at NTCIR-8: A Step Toward Cross Lingual Opinion Analysis</title>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</title>
				<meeting>the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="209" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect-Level Sentiment Analysis in Czech</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Brychcín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Konkol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th workshop on computational approaches to subjectivity, sentiment and social media analysis</title>
				<meeting>the 5th workshop on computational approaches to subjectivity, sentiment and social media analysis<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BRAT: a web-based tool for NLP-assisted text annotation</title>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Topi´c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis of movie reviews on discussion boards</title>
		<author>
			<persName><forename type="first">Jin-Cheon</forename><surname>Tun Thura Thet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">S G</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><surname>Khoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Information Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="823" to="848" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aspect and Entity Extraction for Opinion Mining</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining and Knowledge Discovery for Big Data: Methodologies, Challenges, and Opportunities</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Usability F</surname></persName>
		</author>
		<author>
			<persName><surname>Design&amp; Features G</surname></persName>
		</author>
		<author>
			<persName><surname>Portability H</surname></persName>
		</author>
		<author>
			<persName><surname>Connectivity I</surname></persName>
		</author>
		<author>
			<persName><surname>Miscellaneous</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
