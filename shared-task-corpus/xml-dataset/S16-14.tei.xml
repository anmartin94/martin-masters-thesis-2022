<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 14: Semantic Taxonomy Enrichment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
							<email>jurgens@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 14: Semantic Taxonomy Enrichment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manually constructed taxonomies provide a crucial resource for many NLP technologies, yet these resources are often limited in their lexical coverage due to their construction procedure. While multiple approaches have been proposed to enrich such taxonomies with new concepts, these techniques are typically evaluated by measuring the accuracy at identifying relationships between words, e.g., that a dog is a canine, rather relationships between specific concepts. Task 14 provides an evaluation framework for automatic taxonomy enrichment techniques by measuring the placement of a new concept into an existing taxonomy: Given a new word and its definition, systems were asked to attach or merge the concept into an existing WordNet concept. Five teams submitted 13 systems to the task, all of which were able to improve over the random baseline system. However, only one participating system outperformed the second, morecompetitive baseline that attaches a new term to the first word in its gloss with the appropriate part of speech, which indicates that techniques must be adapted to exploit the structure of glosses.</p><p>the new synset as a hyponym of S in the Word-Net's subsumption hierarchy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic networks and ontologies are key resources in Natural Language Processing. Of these resources, WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation <ref type="bibr" target="#b20">(Navigli, 2009)</ref>, Query expansion and Information Retrieval <ref type="bibr" target="#b26">(Varelas et al., 2005;</ref><ref type="bibr" target="#b8">Fang, 2008)</ref>, sentiment analysis <ref type="bibr" target="#b7">(Esuli and Sebastiani, 2006)</ref>, and semantic similarity measurement <ref type="bibr" target="#b5">(Budanitsky and Hirst, 2006a;</ref><ref type="bibr" target="#b21">Pilehvar et al., 2013)</ref>. The performances of these WordNet-based techniques are directly affected by the lexical coverage of WordNet's vocabulary, especially if applied to specific domains and social media texts. However, the manual maintenance of WordNet is an expensive endeavour which requires significant effort and time. As a result, WordNet is not updated frequently and omits many lemmas and senses, such as those from domain specific lexicons (e.g., DNA replication, regular expression, and long shot), creative slang usages (e.g., homewrecker), or those for technology or entities that came into recent existence (e.g., selfie, mp3).</p><p>Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy <ref type="bibr" target="#b22">(Poprat et al., 2008;</ref><ref type="bibr" target="#b23">Snow et al., 2006;</ref><ref type="bibr" target="#b24">Toral et al., 2008;</ref><ref type="bibr" target="#b28">Yamada et al., 2011;</ref><ref type="bibr" target="#b13">Jurgens and Pilehvar, 2015)</ref>. However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words <ref type="bibr" target="#b15">(Lau et al., 2012)</ref>; however, these senses were not re-integrated into the taxonomy.</p><p>Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire new automated approaches for using the definitions in these resource to expand WordNet with new concepts. Accordingly, the task provides a high-quality dataset of one thousand definitions from a wide range of domains to be added to the WordNet hierarchy, either by adding them as new concepts or integrating them as new lemmas of an existing concept. The task provides a robust evaluation framework for measuring the accuracy of ontology expansion techniques. More broadly, the techniques developed as a part of Task 14 can play an important role in the construction of new automatically-built ontologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The goal of Task 14 is to evaluate systems that enrich semantic taxonomies with new word senses drawn from other lexicographic resources. The task provides systems with a set of word senses that are not defined in WordNet. 1 Each word sense comprises three parts: a lemma, part of speech tag, and definition. For example, the noun geoscience is a word sense in our dataset which is associated with the definition "Any of several sciences that deal with the Earth". The word sense is drawn from Wiktionary. <ref type="bibr">2</ref> For each of these word senses, a system's task is to identify a point in the WordNet's subsumption (i.e., is-a) hierarchy which is the most plausible point for placing the new word sense. In other words, a system's task is to find the most semantically similar WordNet synset to the given new word sense.</p><p>Operations Once the target synset is identified, a system has to decide how to integrate the new word sense. For a given new word sense s and a target synset S we define two possible operations:</p><p>• MERGE: when s refers to the same concept that is conceptualized by the synset S. As a result of this operation s is added to the set of synonymous word senses in S.</p><p>• ATTACH: when s refers to a more specific concept than S. In other words, S is a generalization of the new word sense s (i.e., its hypernym). This operation creates a new synset containing the sole word sense s and attaches</p><p>Table <ref type="table">1</ref> shows example new word senses together with the target synset and the operation. Note that after both these operations, the polysemy of the lemma of s is increased by one. Also, the total number of synsets in the enriched WordNet increases by one after an ATTACH operation whereas it remains unchanged after MERGE, since in the latter case, a new word sense is added to an existing synset. Our datasets contain instances from noun and verb parts of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subtasks</head><p>For each item in our datasets, we provide the source dictionary from which the corresponding word sense (i.e., a word and its definition) is obtained. The participating systems were allowed to use the source dictionary in order to draw additional information or exploit its structural properties. Based on their usage of the source dictionary, we classify the participating systems into two categories:</p><p>• Resource-aware: the participating systems could use the URLs provided in the dataset to gather additional information (e.g., hyperlinks, wiki-markup) for performing the integration and may use additional information from any dictionary, including the one from which the target word sense had been obtained, e.g., Wiktionary.</p><p>• Constrained: the system might use any resource other than dictionaries.</p><p>We allowed each team to submit up to three runs per system type to let them explore different configurations, features, or parameter settings in the official rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Tasks</head><p>Task 14 directly relates to three branches of prior tasks in SemEval. First, two recent tasks have evaluated automatic methods for constructing taxonomies <ref type="bibr" target="#b3">(Bordea et al., 2015;</ref><ref type="bibr" target="#b4">Bordea et al., 2016)</ref>. In these tasks, participants are presented with word pairs -but no glosses-and tasked with organizing the words into hypernym relationships. Task 14 provides the next step in such evaluations by explicitly incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense.</p><p>Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity <ref type="bibr" target="#b1">(Agirre et al., 2012;</ref><ref type="bibr">Agirre et al., 2013, STS)</ref> and Cross-Level Semantic Similarity <ref type="bibr">(Jurgens et al., 2014, CLSS)</ref>. Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets <ref type="bibr" target="#b10">(Hovy et al., 2006)</ref> and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate.</p><p>Third, prior tasks on Word Sense Induction (WSI) have evaluated methods that automatically discover the different meanings of a word <ref type="bibr" target="#b16">(Manandhar et al., 2010;</ref><ref type="bibr" target="#b12">Jurgens and Klapaftis, 2013;</ref><ref type="bibr" target="#b19">Navigli and Vannella, 2013)</ref>. However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Data</head><p>Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to contain concepts from a wide variety of domains and to include glosses typical of those seen if performing an automated integration from online sources, such as those from heavily-curated sources such as Wiktionary and more idiosyncratic online glossaries with a single author.  Novel concepts were limited to nouns and verbs, as only these parts of speech have fully-developed taxonomies in WordNet. <ref type="bibr">3</ref> Table <ref type="table" target="#tab_2">2</ref> shows the distribution of training and test items according to their parts of speech and the intended operation, highlighting the fact that most new items are novel concepts that require a new synset to be added, rather than new lemmas to be included in an existing synset.</p><p>For each item, in addition to the target synset and the operation, we also provide the resource from which the new word sense was obtained. Glosses were provided as purely text data, with the hope was that systems may use the source URL provided with each gloss to identify additional page structure that could prove useful for concept integration (e.g., hyperlinks, wiki-markup, page topics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Process</head><p>The two authors independently annotated each of the 1000 items, identifying the appropriate synset and operation. In a small number of cases, neither author could determine an appropriate integration for an item; such items were discarded and replaced with more-easily annotated items. Ultimately, all disagreements were discussed and adjudicated to determine the final dataset.</p><p>Annotators initially agreed on the annotation for 37.5% of the items. While this rate seems low at first glance, most disagreements were due to one annotator finding a more refined integration of the item, e.g,. DNA vs. Mutant Gene, which is expected given the large search space of over 82K noun and 13.7K verb synsets from which to find the appropriate hypernym or synonym synset. Indeed, most disagreements were very close in meaning; in fact, dis-agreements had an average semantic similarity between their synsets of 0.74 according to the <ref type="bibr" target="#b27">Wu and Palmer (1994)</ref> measure. Hence, the moderate exactmatch agreement is an underestimate of the true semantic agreement between annotators. Furthermore, several of the remaining dissimilar pairs were instances where similar concepts were distantly located in WordNet's structure.</p><p>The annotation proved difficult for three categories of concepts, not all of which were successfully integrated. First, many technical domains include unique processes and techniques specific to their field, e.g., Lautering (noun) -The process of separating the sweet wort (pre-boil) from the spent grains in a lauter tun or with other straining apparatus.</p><p>However, some techniques and processes do not have a correspondence to any existing synsets, leaving their closest appropriate hypernym as a sense of process or technique. This difficulty is reflected in the current structure of WordNet, where process#n#1 already has the dissimilar concepts of "fingerprinting," "computation," and "modus operandi" all as direct hyponyms, highlighting the challenge of placing some concepts. Where possible, we opted to avoid attaching new concepts to general senses, either by finding a more specific concept or leaving them out of the dataset entirely.</p><p>Second, in rare occasions, WordNet does not contain an intermediate concept necessary for the appropriate integration. For example, integrating the new concept Root (noun) -The administrative account (UID 0) on a *nix system that has all privileges; cf. superuser. requires first having a concept of a computer account, which is not currently present in WordNet. These gaps are particularly evident for action nouns, where most verbs do not have a corresponding noun gerund. While the novel concept may still be attached to a more-distant hypernym of the appropriate location, this situation points to the need for an iterative integration process where intermediary concepts are first inserted.</p><p>Third, concepts that express a negated or partial state often do not have associated concepts in the more-specific depths of the taxonomy. For example, annotators had difficulty finding appropriate synsets that were not too general for the following two concepts: NaN (noun) -Not a number; applied to numeric values that represent an undefined or unrepresentable value, such as zero divided by itself Neomort (noun) -A brain-dead human being that could be kept on life support for organ transplantation, medical and nursing education, and drug research.</p><p>Without additional synsets for representing partial or negated state, these concepts would need to be attached to very general synsets such as value#n#1 or person#n#1.</p><p>The challenge of agreeing upon a specific location for a new concept underscores the need for automated approaches developed as a part of this task. As an ontology grows in size, it becomes less obvious where a new concept could be integrated, despite an annotator's familiarity with the concepts contained therein. Our annotation process relied on two annotators whose collective experience was necessary to identify the appropriate location. However, for larger ontologies such as BabelNet <ref type="bibr" target="#b18">(Navigli and Ponzetto, 2012)</ref>, which contains several orders of magnitude more concepts than WordNet, automated integration approaches will be necessary as it is infeasible for a single human annotator to recall the appropriate insertion point among millions of concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We evaluated the performance of the participating system according to two criteria: (1) the accuracy by which the placements were performed, and (2) the percentage of items for which a decision was made (Recall).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Accuracy (Wu&amp;P)</head><p>Our first criteria verifies the ability of a system to correctly identify the attachment or merge point in the WordNet hierarchy. Checking for exact matches would penalize equally both a placement in the near proximity of the intended synset and a random placement far in the network. A system's automatically-made attachment to the WordNet hierarchy is expected to be as close as possible to the correct attachment point given by the gold-standard data. We therefore evaluate the systems according to a fuzzy measure of accuracy which is sensitive to the distance between the intended target synset and the one outputted by the system. However, we recognize that links in the taxonomy do not necessarily represent uniform semantic distances, since siblings that are deep in the hierarchy tend to be more related to one another. Hence, a direct edge-counting approach might not provide a reliable basis for the evaluation of the attachment accuracy. Interestingly, the attachment accuracy evaluation can be cast as a WordNet-based semantic similarity measurement in which the goal is to compute the similarity between two concepts based on the structural properties of WordNet <ref type="bibr" target="#b6">(Budanitsky and Hirst, 2006b)</ref>, most important of which is the distance between the two. Therefore, we measure accuracy using the <ref type="bibr">Wu and Palmer (1994, Wu&amp;P)</ref> semantic similarity measure, defined as:</p><formula xml:id="formula_0">2 • depth LCS depth 1 + depth 2 (1)</formula><p>where depth 1 and depth 2 are the depths of the two concepts in WordNet's subsumption hierarchy (hypernymy/hyponymy relations) and Depth LCS is the depth of their least common subsumer, i.e., the most specific concept which is an ancestor of both the concepts. For each instance in the test set for which the system made a prediction, we measure the Wu&amp;P similarity of the output attachment and the corresponding correct synset. An accurate system is expected to have a high similarity score when aggregated over all instances in the test set. Please note that picking the correct target synset with an incorrect operation is analogous to increasing the distance by one edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Lemma Match</head><p>A key challenge in the integration task is identifying the appropriate word in the gloss that denotes the hypernym (if it exists) and then disambiguating which sense of that word is the appropriate concept for attachment or merger. For example, given the  item Grief (verb) -To deliberately harass and annoy or cause grief to other players of a game in order to interfere with their enjoyment of it a system may correctly identify that the verb harass is the hypernym but select the wrong sense to which grief should be attached. Such a mistake would be penalized heavily according to the Wu&amp;P measure and mask that the system is accurate at identifying hypernyms in glosses. Therefore, we include a second unofficial metric, lemma match, that measures the percentage of items for which the system has selected a synset with at least one word in common with the correct synset where the item should be integrated; i.e., how often the system picked the right word but wrong sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Recall</head><p>Some word senses may be more difficult to place in the WordNet hierarchy than others due to a variety of reasons, such an entry with a gloss that contains many out of vocabulary words. Therefore, we allow a system to decline to place these senses in order to avoid making placements with low confidence. As an evaluation metric, we report Recall as the percentage of items for which a decision was made by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System ranking</head><p>A system's performance is computed by the F1 score of Wu&amp;P and Recall. The official ranking of the systems was done according to their F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Systems</head><p>Five teams submitted 13 systems, where each team's systems were variations on a common architecture.</p><p>No system utilized resource-specific features beyond the gloss (e.g., the Wiktionary markup) and so all systems were ultimately submitted in the constrained category. Systems were compared against two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Participants</head><p>The MSejrKU systems build definitional representations based on skip-gram vectors trained on Wikipedia data and incorporates syntactic features.</p><p>Words in a candidate gloss are disambiguated using the method of <ref type="bibr" target="#b0">Agirre and Soroa (2009)</ref> and then a classifier predicts the goodness of fit for a candidate attachment synset related to those in the gloss.</p><p>The Duluth systems perform string matching to compare a definition with each of the glosses in WordNet. Given a new definition, systems differ in which words are included from the WordNet synset for comparison: Duluth2 uses only the words in the definition after stopword removal, while Duluth1 extends Duluth2 by including words from the hypernyms of the compared synset. Duluth3 extends Du-luth1 with words from the hyponyms but also takes the step of breaking each definition into character tri-grams to capture surface-form regularities. The UMNDuluth team performs a similar approach but weights gloss similarity by favoring specific kinds of terms, such as those that are longer and those that appear in WordNet.</p><p>The TALN systems project the definition of the novel term into a vector space using SENSEMBED <ref type="bibr" target="#b11">(Iacobacci et al., 2015)</ref>. Then this vector is compared with the vectors for senses in WordNet to find the closest match. System variations address issues when words have no associated vectors and how to select between candidate attachments.</p><p>The JRC system uses a form of second-order similarity by representing each definition as a vector over the synsets that contain its words. New terms are attached by finding the WordNet synset whose definition has maximal cosine similarity.</p><p>The VCU systems adopt multiple approaches based on textual similarity. Run1 uses a secondorder expansion by representing a definition using frequency of words related to those in the definition. Run2 compares glosses using Lesk relatedness measure. Run3 performs no pre-processing and compares the words in the glosses directly as first-order vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>The first baseline, Random synset captures the expected performance of a system at chance when attaching the new concept to a randomly picked synset from WordNet with the appropriate part of speech. This baseline provides the lower bound in expected similarity for an attachment.</p><p>The second baseline captures our observation that  glosses are reasonably well structured such that the word expressing the hypernym concept appears early in the gloss (if at all). Therefore, given new word sense s with definition d s and part of speech tag p, the First word, first sense (FWFS) baseline picks the first occurring word w in d s with part of speech p as the hypernym (i.e., the first noun if the word sense to be attached is a noun and the first verb otherwise).</p><p>The new word sense is then attached to the synset containing the first sense of w. For example, given the item Immunoglobin (noun) -Any protein that functions as an antibody the FWFS baseline attaches the item to the first sense of the noun protein in WordNet, i.e., protein#n#1. Despite the wide variety of domains seen in the data, 65% of all integrations in the gold standard data connect the first sense of the target word, suggesting that in the absence of specific information to disambiguate a word in the gloss, its first (most frequent) sense is relatively high precision back-off strategy.</p><p>For the FWFS baseline, glosses are POS-tagged using <ref type="bibr">CoreNLP (Manning et al., 2014)</ref> and we include a minimal heuristic that prevents attaching to "a" or "an," both of which are nouns in WordNet. In the rare event that no word can be found with the same part of speech, the item is attached to either the general concepts of entity#n#1 or be#v#1, depending on the part of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>All of the thirteen participating systems improved over the Random synset baseline. Table <ref type="table" target="#tab_6">4</ref> shows the evaluation results for Task 14's participating systems. However, only one of the systems, System2 of MSejrKU, could slightly outperform the FWFS baseline, showing the competitiveness of this simple baseline, which takes advantage of the inherent structure of definitions. Indeed, the lower ranked systems frequently performed holistic comparisons between gloss texts, which frequently include terms from later in the gloss that do not aid in identifying the closest meaning.</p><p>While its performance is relatively high among participants, the FWFS baseline should not be mistaken for a satisfactory solution; many of the attachments made by the baseline are overly general and do not take advantage of the remainder of the gloss's content, which can identify the correct, more-specific concept to which the item should be attached. For example, with the item Hot reactor (noun) -A person whose blood pressure and heart rate increase abnormally in response to stress the baseline naively attaches to person#n#1, while a more sophisticated solution could use the additional text in gloss to identify an appropriate hyponym of person#n#1 to which the item may be attached, e.g., sick person#n#1. Thus, we speculate that the gloss similarity used by participants may still prove highly useful by first identifying the appropriate general concept in the gloss (e.g., as the baseline does) and then searching its hyponyms for a better match.</p><p>Examining the performances of systems in Table <ref type="table" target="#tab_6">4</ref>, we see that no system performed significantly better on the Lemma Match measure than Wu&amp;P, with both measures being highly correlated at r=0.96. This suggests that when the appropriate hypernym lemma was present in a gloss, systems struggled most with selecting it as the correct candidate lemma in the gloss, rather than identifying which synset of that lemma was the correct attachment.</p><p>Given the variety of genres and sources from which new definitions were drawn, we performed a follow-up analysis to examine the impact of the genre on system performance. Figure <ref type="figure" target="#fig_0">1</ref> shows the distribution of scores per genre. Surprisingly, Religious definitions cause the most variance among systems and also saw the highest and lowest system scores per genre. Religious definitions were drawn from more sources beyond just Wiktionary and thus, such variance may reflect systems' robustness to different writing styles. Systems performed worst on the Finance and Jargon domains; however, both genres had little training data relative to testing data, suggesting that systems had difficulty generalizing from few examples. Nevertheless, systems still performed well for the Legal genre which was held out as a surprise dataset with no training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Semantic taxonomies are core components of many NLP systems and multiple approaches have been proposed for how to extend such taxonomies automatically with new concepts. We have introduced SemEval-2016 Task 14 as a framework and dataset for evaluating the accuracy of systems at integrating new definitions as concepts into an ontology using WordNet 3.0 as a base resource. Five teams submitted 13 systems for participation, with all teams performing better than chance but only one team surpassing a simple baseline that leverages knowledge of the expected word order in a definition to guess the correct hypernym concept. Our results point towards significant opportunity for improving taxonomy enrichment. In future work, we intend to integrate the best insights of this task into the next version of CROWN, 4 an automatically constructed extension of WordNet with concepts from online glossaries and lexicographic resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The distribution of Wu&amp;P scores of the participating systems per genre. Whiskers show minimum and maximum scores and lines denote the median.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>shows the distribution of instances in the Task 14's training and test datasets across different genres. The dataset consists of a total of 1000 items, split into training and test datasets containing 400 and 600 items, respectively.</figDesc><table><row><cell cols="2">Training</cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell cols="2">M A</cell><cell>Total</cell><cell cols="2">M A</cell><cell>Total</cell></row><row><cell cols="3">Noun 27 322 349</cell><cell cols="3">26 490 516</cell></row><row><cell>Verb 6</cell><cell>45</cell><cell>51</cell><cell>6</cell><cell>78</cell><cell>84</cell></row><row><cell cols="3">Total 33 367 400</cell><cell cols="3">32 568 600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The distribution of items in the task's datasets according to the part of speech and the target operation, i.e., Merge (M) and Attach (A).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The distribution of instances across different genres in the training and test data sets of Task 14.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results showing the Lemma Match (LM), Wu&amp;P, and Recall measures.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use WordNet 3.0. 2 http://www.wiktionary.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We do note that<ref type="bibr" target="#b25">Tsvetkov et al. (2014)</ref> have proposed a taxonomy for adjectives, for which our methodology could be applied.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalizing pagerank for word sense disambiguation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<ptr target="https://github.com/davidjurgens/crown" />
	</analytic>
	<monogr>
		<title level="m">ings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 17: Taxonomy extraction evaluation (texeval)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating WordNet-based measures of Lexical Semantic Relatedness</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="47" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating WordNet-based measures of lexical semantic relatedness</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="47" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SENTI-WORDNET: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Conference on Language Resources and Evaluation</title>
				<meeting>the 5th Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A re-examination of query expansion using lexical resources</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
				<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OntoNotes: the 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference of the NAACL</title>
				<meeting>the human language technology conference of the NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SensEmbed: learning sense embeddings for word and relational similarity</title>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SemEval-2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second joint conference on lexical and computational semantics (* SEM)</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reserating the awesometastic: An automatic extension of the WordNet taxonomy for novel terms</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1459" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 3: Cross-level semantic similarity</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8 th International Workshop on Semantic Evaluation</title>
				<meeting>the 8 th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
	<note>conjunction with COLING 2014</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word sense induction for novel sense detection</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><surname>Sameer S Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
				<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ba-belNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (* SEM)</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A survey</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mohammad Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1341" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a BioWordNet by using WordNet&apos;s data formats and WordNet&apos;s software infrastructure: a failure story</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poprat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Beisswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing</title>
				<meeting>the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing<address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
	<note>Columbus</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic taxonomy induction from heterogeneous evidence</title>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Named Entity WordNet</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Muoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Monachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC)</title>
				<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="741" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Augmenting English adjective senses with supersenses</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic similarity methods in WordNet and their application to information retrieval on the web</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Varelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Epimenidis</forename><surname>Voutsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paraskevi</forename><surname>Raftopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Euripides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Petrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual ACM International Workshop on Web Information and Data Management</title>
				<meeting>the 7th Annual ACM International Workshop on Web Information and Data Management</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd Annual Meeting on Association for Computational Linguistics</title>
				<meeting>the 32Nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extending WordNet with hypernyms and siblings acquired from wikipedia</title>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Kazama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Stijn De Saeger</surname></persName>
		</author>
		<author>
			<persName><surname>Kawada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Natural Language Processing</title>
				<meeting>the 5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="874" to="882" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
