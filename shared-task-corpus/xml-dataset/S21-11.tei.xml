<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2021 Task 11: NLPCONTRIBUTIONGRAPH -Structuring Scholarly NLP Contributions for a Research Knowledge Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>D'souza</surname></persName>
							<email>jennifer.dsouza@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">TIB Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">SÃ¶ren</forename><surname>Auer</surname></persName>
							<email>auer@tib.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">TIB Leibniz Information Centre for Science and Technology</orgName>
								<address>
									<settlement>Hannover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
							<email>tpederse@d.umn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Duluth</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2021 Task 11: NLPCONTRIBUTIONGRAPH -Structuring Scholarly NLP Contributions for a Research Knowledge Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. The SemEval-2021 Shared Task NLPCONTRI-BUTIONGRAPH (a.k.a. 'the NCG task') tasks participants to develop automated systems that structure contributions from NLP scholarly articles in the English language. Being the firstof-its-kind in the SemEval series, the task released structured data from NLP scholarly articles at three levels of information granularity, i.e. at sentence-level, phrase-level, and phrases organized as triples toward Knowledge Graph (KG) building. The sentencelevel annotations comprised the few sentences about the article's contribution. The phraselevel annotations were scientific term and predicate phrases from the contribution sentences. Finally, the triples constituted the research overview KG. For the Shared Task, participating systems were then expected to automatically classify contribution sentences, extract scientific terms and relations from the sentences, and organize them as KG triples.</p><p>Overall, the task drew a strong participation demographic of seven teams and 27 participants. The best end-to-end task system classified contribution sentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While the absolute performance to generate triples remains low, in the conclusion of this article, the difficulty of producing such data and as a consequence of modeling it is highlighted.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional search models over scholarly communication are now changing toward Knowledge Graph (KG) models operating on structured fine-grained scholarly content offering enhanced contextual search results. Several initiatives exist to this end: Google Scholar, Web of Science <ref type="bibr" target="#b4">(Birkle et al., 2020)</ref>, Microsoft Academic Graph <ref type="bibr" target="#b27">(Wang et al., 2020)</ref>, OpenAIRE Research Graph <ref type="bibr">(Manghi et al., 2019)</ref>, Open Research Knowledge Graph <ref type="bibr" target="#b2">(Auer, 2018)</ref>, Semantic Scholar <ref type="bibr" target="#b11">(Fricke, 2018)</ref> to name just a few. These KG models differ in their content, their level of detail, etc., as they represent diverse aspects of scholarly communication.</p><p>Text, of course, is of seminal importance to Science. It is as important as experimentation itself; unpublished research lacks validity. Seen in another angle, it is hard to imagine a medium other than discourse that can convey a comprehensive picture of the scholarly investigation. For the wider research audience, it is interesting to read the full "stories" of Science.</p><p>Nonetheless, since scientific literature is growing at a rapid rate <ref type="bibr" target="#b16">(Johnson et al., 2018)</ref> and researchers today are faced with this publications deluge <ref type="bibr" target="#b17">(Landhuis, 2016)</ref>, it is increasingly tedious, if not practically impossible to keep up with the research progress even within one's own narrow discipline. In this regard, among the existing scholarly knowledge structuring initiatives, the Open Research Knowledge Graph (ORKG)  is posited as a solution to the problem of keeping track of research progress minus the cognitive overload that reading dozens of full papers impose. It aims to build a comprehensive KG that publishes the research contributions of scholarly publications per paper, where the contributions are interconnected via the graph even across papers. The ORKG digital library (DL) framework can be accessed here https://www.orkg.org.</p><p>Motivated by the availability of a nextgeneration DL, we present the SemEval-2021 NLP-CONTRIBUTIONGRAPH (NCG) Shared Task as a step in the easier knowledge acquisition of contri-butions for researchers -the automated structuring of the unstructured article contributions. To this end, via the NCG task, we have formalized the building of such a scholarly contributions-focused graph over NLP scholarly articles as an automated task. In the subsequent paper, we detail our task in terms of its resources, organization, participants, and evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The NCG Shared Task comprised a dataset of NLP scholarly articles annotated for their contributions. The contributions were structured to be integrable within KG infrastructures such as the ORKG <ref type="bibr" target="#b15">(Jaradeh et al., 2019)</ref> that capture research overviews. The contributions were annotated in three different information granularities, i.e. (1) Contribution sentences: a set of sentences about the article's contribution; (2) Scientific terms and relations: a set of terms and relational predicates in the contribution sentences; and (3) Triples: semantic statements that pair the terms with a predicate, modeled toward subject-predicate-object RDF statements for KG building. This latter set of annotations formed the actual graph. Inspired after article sections, the Triples were organized under three (mandatory) or more of 12 total information units (IUs), viz. RESEARCHPROBLEM, APPROACH, MODEL, CODE, DATASET, EXPERIMENTALSETUP, HY-PERPARAMETERS, BASELINES, RESULTS, TASKS, EXPERIMENTS, and ABLATIONANALYSIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Annotation Scheme</head><p>A trial annotation stage preceded the annotation of the Shared Task dataset. In this stage, an annotation scheme was prescribed. This involved specifying the annotation data granularities and the 12 IUs for organizing the triples. Observations were also obtained about the position in the articles where the authors generally stated the contribution. The trial annotations were conducted in two steps: a pilot annotation step (D' <ref type="bibr">Souza and Auer, 2020)</ref> followed by an adjudication step (D' <ref type="bibr">Souza and Auer, 2021)</ref>. The resulting scheme itself was called the NLPCONTRIBUTIONGRAPH (NCG) scheme.</p><p>For the trial stage, a relatively small dataset of 50 articles uniformly distributed across five NLP tasks, i.e. machine translation, named entity recognition, question answering, relation classification, and text classification, were selected.</p><p>Overall, after the pilot annotation task the follow-ing core question was answered. Could a scheme be defined such that it would encompass all annotation decisions of the task? In reality, it was found that the scheme could only define high-level annotation decisions such as: where in the article could the contribution information generally be found? E.g., the title, the abstract, a few lines in the Introduction, the first few lines of the Results section. This still entailed making subjective decisions such as if the model is not described in the Introduction then the first few lines of the model description section would need to be annotated. The scheme also specified the 12 IUs for organizing the structured triples. The choice of the specific IU for organizing the triples was based on the closest section title. After the two-step trial annotation stage, the intra-annotation agreement between the pilot and adjudication steps, in terms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for triple statements indicating that with increased granularity of the information, the annotation adjudication was greater (2021).</p><p>The trial annotations were made by a postdoctoral researcher in Computational Linguistics. The same experienced annotator also annotated the full dataset. Next, we explain the NCG data with a focus on the KG and then offer two supporting examples as illustrations of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Understanding our Knowledge Graph</head><p>The NCG KG used two levels of knowledge systematization: 1) At the root, it defined a dummy node called CONTRIBUTION. And following the root node, 2) it defined the 12 nodes introduced earlier and generically referred to as Information Units or IUs. Each scholarly article's annotated contribution triple statements were organized under three (mandatory) or more of these IU nodes, depending on whether they applied to the article. Next, we provide details about each IU.</p><p>RESEARCHPROBLEM The research challenge addressed by a contribution. In other words, a focus of the research investigation or the issue for which a research solution was proposed.</p><p>APPROACH or MODEL The contribution of the paper as the solution proposed for the research problem. This unit was called APPROACH when the solution was proposed as an abstraction, and was called MODEL if the solution was proposed in practical implementation terms. Further, in case the solution was not referred to as approach or model in the article, the reference was normalized as either APPROACH or MODEL. E.g., references like "method" or "application" were normalized as APPROACH; on the other hand, references like "system" or "architecture," were normalized to MODEL. This unit captured only proposed system highlights. CODE The contribution resource; the link to the software on an open-source hosting platform such as Gitlab or Github or on the author's website.</p><p>DATASET Like CODE, this a contributed resource in the form of a dataset.</p><p>EXPERIMENTALSETUP or HYPERPARAME-TERS Details about the platform including both hardware (e.g., GPU) and software (e.g., Tensorflow library) for implementing the machine learning solution; and of variables, that determine the network structure (e.g., number of hidden units) and how the network is trained (e.g., learning rate), for tuning the software to the task objective. It was called EXPERIMENTALSETUP only when hardware details were provided.</p><p>BASELINES The systems that a proposed AP-PROACH or MODEL were compared with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>The main findings or outcomes reported in an article for the RESEARCHPROBLEM.</p><p>TASKS The APPROACH or MODEL, particularly in multi-task settings, are tested on more than one task, in which case, this unit was defined to capture all the experimental tasks. Unlike the earlier units, the TASKS IU was a container for more than one of the earlier mentioned IUs. Specifically, each task listed in TASKS could include one or more of the EXPERIMENTALSETUP, HYPERPARAMETERS, and RESULTS as sub-information units.</p><p>Furthermore, since it is common in NLP for tasks to be defined over datasets, experimental tasks are often synonymous with the experimental datasets, therefore this unit was also applied in articles where the datasets were explicitly listed instead of the task names.</p><p>EXPERIMENTS The second container information unit, like TASKS, defined to include one or more of the previous discussed units as subinformation units. This unit encapsulated several TASKS themselves and consequently, the units that TASKS encapsulated, i.e. EXPERIMENTALSETUP and RESULTS, or a combination of APPROACH, EXPERIMENTALSETUP and RESULTS.</p><p>ABLATIONANALYSIS A form of RESULTS that describes the performance of components in an APPROACH or MODEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Examples</head><p>Below, we show two examples of two different IUs, viz. RESEARCHPROBLEM and MODEL, respectively, as illustrations of our data.  <ref type="bibr" target="#b6">(Cho et al., 2014)</ref>. We show two formats of our data: the JSON format (see Fig. <ref type="figure" target="#fig_0">1</ref>) with all three annotated information granularities; and the triples format (see Table <ref type="table" target="#tab_1">1</ref>) showing only the annotated data for a KG. In the JSON data, the dummy root node CONTRIBUTION is left unspecified, however, it is specified in the triples. For this data, three phrases that named the research problem were annotated. The phrases were attached to the dummy root node by the predicate "has research problem." Further, in the JSON data, following the predicate "from sentence," the selected contribution sentences are listed.  tences <ref type="bibr" target="#b14">(Hu et al., 2014)</ref>. See Fig. <ref type="figure">2</ref> for the JSON format and Table <ref type="table" target="#tab_3">2</ref> for the triples data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data Statistics</head><p>Overall, the NCG Shared Task dataset had 50 articles in the trial data, 237 articles in the training data, and 155 articles in the test data. The trial data articles uniformly spanned five tasks, the training data spanned 24 tasks, and the test data spanned 10 tasks. For the Shared Task itself, participants were encouraged to merge the trial and training datasets. Thus, the overall training data had 287 articles representing 29 unique tasks. The training and test tasks were mutually exclusive except for one, i.e. 'natural language inference.' Table <ref type="table" target="#tab_5">3</ref> shows further detailed statistics of the NCG dataset in terms of each of the annotated information granularities.</p><p>Our full dataset is publicly released online (D' <ref type="bibr">Souza et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Our comprehensive NCG Shared Task formalism was as follows. Given a scholarly article A in plaintext format, the goal was to extract (1) a set of contribution sentences C sent = {C sent 1 , ..., C sent N }, (2) a set of scientific knowledge terms and predicates from C sent referred to as entities E = {e 1 , ..., e N }, and (3) to organize the entities E as a set of (subject,predicate,object) triple statements T = {t 1 , ..., t N } toward KG building organized under three or more of the 12 total IUs. Task Evaluation Phases. The task comprised three evaluation phases, thereby enabling detailed system evaluations.</p><p>Evaluation Phase 1: End-to-end Pipeline. In this phase, systems were tested for the comprehensive end-to-end KG building task described in the formalism above. Given a test set of articles A in plaintext format, the participating systems were expected to return: (1) a set of contribution sentences C sent , (2) a set of scientific knowledge terms and predicates from C sent , i.e. entities E, and (3) the entities in E organized in a set of triple statements T toward KG building. System outputs were evaluated for the three aspects and overall.</p><p>Evaluation Phase 2, Part 1: Phrases and Triples. In this phase, systems were tested only for their capacity to extract phrases and organize them as triples. Given a test set of articles A in plain-text format and contribution sentences C sent from each article, each system was expected to return: (1) the entities E, and (2) the set of triple statements T .</p><p>Evaluation Phase 2, Part 2: Triples. In this phase, systems were tested only for the triples formation task. Thus, given gold entities E for the set of C sent , systems were expected to form triple statements T .  In the Evaluation phases that lasted from Jan 10 till Feb 1, 2021, we provided the participants with masked versions of the test set based on the current evaluation phase. The test set annotations in each phase were uploaded to CodaLab and were not available to the participants. To obtain results, the participants were expected to upload their system outputs to Codalab where they were automatically evaluated by our script and reference data stored on the platform. In each evaluation phase, teams were restricted to make only 10 submissions and only one result, i.e. the top-scoring result, was shown on the leaderboard. Before the task began, our participants were onboarded via our task website https://ncg-task.github.io/.</p><p>Further, participants were encouraged to discuss their task-related questions via our task Google groups page at https://groups.google.com/forum/#! forum/ncg-task-semeval-2021.</p><p>The NCG Data Collection of Articles Our base collection of scholarly articles was downloaded from the publicly available leaderboard of tasks in AI called https://paperswithcode.com/. While paperswithcode predominantly represents the NLP and Computer Vision research fields in AI, we restricted ourselves just to its NLP papers. From their overall collection of articles, the tasks and articles in our final data were randomly selected. The raw articles' pdfs needed to undergo a two-step preprocessing before the annotation task. 1) For pdf-to-text conversion, the GROBID parser <ref type="bibr">(GRO, 2008</ref><ref type="bibr">(GRO, -2020</ref> was applied; following which, 2) for plaintext pre-processing in terms of tokenization and sentence splitting, the Stanza toolkit <ref type="bibr" target="#b24">(Qi et al., 2020)</ref> was used. The resulting pre-processed articles could then be annotated in plaintext format. Note, our data consists of articles in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>The NCG Task participating team systems were evaluated for classifying contribution sentences, extracting scientific terms and relations, and extracting triples (see specific details in Section 3). The results from the three evaluations parts were also cumulatively averaged as a single score to rank the teams. Finally, for the evaluations, the standard precision, recall, and F1-score metrics were leveraged.</p><p>This completes our discussion of the NCG task in terms of its dataset definition and overall organization description. In the remainder of the paper, we shift our focus to the participating teams. Specifically, we describe the participating systems and examine their results for the NCG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participating System Descriptions</head><p>The NCG Shared Task received public entries from 7 participating teams in all. In this section, we briefly describe the teams' systems in terms of the three parts of the NCG task, i.e. contribution sentence classification, scientific terms and relations extraction, and triples extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Contribution Sentence Classification</head><p>To identify the contribution sentences from articles, systems adopted one of two strategies: a binary classification objective, or a multi-class classification objective. In the first strategy, sentences were either classified as contribution sentences or not. In the second strategy, sentences were classified in a 13-class classification task as one of the 12 IUs or as a non-contribution sentence. Next, we describe these strategies. Note, the asterisk superscripts against team names, where present, correspond to * * * 3rd best, * * 2nd best, and * 1st best systems in the Shared Task, respectively. These binary and multi-class sentence classifiers, were also adapted to our following dataset characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Contribution sentences data imbalance</head><p>Characteristically, of all the sentences in training data scholarly articles, only 10% were annotated as contribution sentences. Thus, our dataset presented an imbalanced classification task.</p><p>Teams YNU-HPCC, DULUTH, KnowGraph@IITK * * * and UIUC BioNLP * trained their classifiers on the given data. While INNOVATORS and ITNLP * * downsampled the non-contribution sentences.</p><p>INNOVATORS established a threshold based on cumulative contributing sentence bigram scores as a filter; ITNLP fixed the ratio of positive to negative samples as an integer and tuned the value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Differing tasks coverage between the training and the test datasets</head><p>Since only one task was in common between the training and the test datasets, this meant that systems trained only on the training data would be applied on articles from nine new tasks as test data.</p><p>To this end, Team ECNUICA hypothesized that if the classifier could see, i.e. somehow be trained on, the test data tasks, its performance could be boosted. They, thus, adopted the strategy of retraining their classification ensemble with silverlabeled test data instances. This followed the standard setup of training the classifier on the actual training data, applying it to the test data, and incrementally retraining the classifier leveraging the few confidently classified test instances. The instances were marked as silver training data only when all three ensemble classifiers predicted the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scientific Terms and Relations Extraction</head><p>After identifying the contribution sentences, systems then had to extract their scientific terms and relational predicates.</p><p>Sequence Labeling Systems Majority, i.e. six, of the seven participating systems adopted a sequence labeling approach.</p><p>1. Team YNU-HPCC used a pre-trained BERT model for sequence labeling of each token, obtaining embeddings for each token in the sequence, with softmax and argmax top layers which were shared across all tokens.</p><p>2. Team DULUTH trained a feature-based maximum-entropy Markov model (MEMM) to predict scientific terms in the contribution sentences.</p><p>3. Team ECNUICA extracted entities using RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> with a CRF layer and a BIO sequence labeling scheme. The input sequences to RoBERTa are modified with sub-title information.</p><p>4. Team KnowGraph@IITK * * * extracted phrases in the sentence by adding BiLSTM layers to the SciBERT + CRF model as a sequence labeler. To mark phrase boundaries, they used the BILUO scheme.</p><p>5. Team ITNLP * * employed the standard BERTbased model, however, in a sequence labeling setting. They trained ten different models by 10-fold cross-validation and used a voting count threshold scheme to extract the final set of entities.</p><p>6. Team UIUC BioNLP * used a BERT-CRF model for phrase extraction and type classification <ref type="bibr" target="#b26">(Souza et al., 2019)</ref>. They employed the BIO scheme to distinguish the scientific terms vs. predicate phrases.</p><p>Rule-based System Team INNOVATORS leveraged an unsupervised rule-based approach for phrase extraction. Using spaCy (Honnibal et al., 2020), they obtained dependency parses for each sentence. They then implemented a set of dependency tree node traversal heuristics for phrase extraction based on the dependency parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Triples Extraction</head><p>1. Team YNU-HPCC first classified the scientific terms in subject, predicate, and object roles using three binary BERT classifiers. These triples from each contribution sentence were then organized as the 12 IUs leveraging a 12-class contribution sentence classifier. This team, however, did not participate in the end-to-end evaluation task.</p><p>2. Team DULUTH applied Stanford Core NLP's dependency parser <ref type="bibr" target="#b5">(Chen and Manning, 2014)</ref> to generate a dependency parse for each contribution sentence. They used the dependency parse structures to assign subject, relation, and object phrase roles to the extracted scientific terms. These were then organized as triples per IU obtained by their 13-class sentence classifier. The overall end-to-end pipeline system score achieved by this system is 28.38%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Team INNOVATORS implemented a set of rules based on the dependency parses to form triples from the extracted scientific terms.</head><p>They used a CNN-based architecture for classifying the contribution sentences as the 12 IUs. Their end-to-end score was 32.05%.</p><p>4. Team ECNUICA approached the triples formation task in two steps: i) they formed triple candidates based on the scientific term sequence order in the sentence. Additionally, they employed a set of predefined predicates when the predicates were not directly found in the sentence. ii) They then employed a</p><p>SciBERT-based binary classifier to classify the triples as true or false candidates. Their overall end-to-end system score was 33.35%.</p><p>5. Team KnowGraph@IITK * * * addressed the RESEARCHPROBLEM, CODE, BASELINES and ABLATIONANALYSIS IUs by a heuristicsbased approach. For the remaining eight IUs triples, they followed a 3-step approach: i) identify predicates from the scientific terms using a binary SciBERT+BiLSTM classifier; and ii) formed triples by arranging the terms and predicates in exact order as they appear in the original sentence; and iii) employ an 8-class SciBERT + BiLSTM classifier to classify the triples. Their overall end-to-end system score was 37.83%.</p><p>6. Team ITNLP * * extracted triples as follows: i) they formed all possible triples candidates from the classified scientific terms; and ii) employed a binary BERT classifier for true or false candidates. Prior to BERT classification, they perform the negative candidate triples downsampling as follows: by artificially generating them using random replacement (RR) of one of the arguments of the true triples with a false argument; and by random selection (RS) of triples where no argument is a valid pair of another. Additionally, each of their system components obtained boosted performances with the Friendly Adversarial Training strategy . Their overall end-to-end system score was 47.03%.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Shared Task Results</head><p>In this section, we present the results of the seven participating teams' systems.</p><p>The results in Table <ref type="table" target="#tab_8">4</ref> show the cumulative scores of the participating teams in each of the three evaluation phases in our Shared Task. We refer the reader to Section 3 for a detailed description of the three evaluation phases. In each phase, Teams were officially ranked by these scores. Next, we examine the scores by the individual extraction task deadline due to an error in their submission offsets for phrases. Thus, they are officially 2nd after the ITNLP team within the Shared Task timeline for Phase 1.</p><p>tasks that constituted building the NLPCONTRIBU-TIONGRAPH per article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Contribution Sentences Classification</head><p>As a first step toward building the NLPCONTRI-BUTIONGRAPH, systems were evaluated for identifying contribution sentences. This was done only in the Evaluation Phase 1 of the Shared Task, i.e. the phase that tested the end-to-end systems. These results are shown in Table <ref type="table" target="#tab_9">5</ref> under column "Sentences." This subtask attained a high score of 57%. The top two teams, i.e. UIUC BioNLP * and ITNLP * * , differed by only 1 point. Comparing these performances to a baseline, a default system would return all titles as candidate contribution sentences. This results in a score of 10.78% F1 at 90% precision and 5.7% recall. In contrast to the 1 sentence per article result in the default computation, our actual data averages at 17 sentences per article. Thus the default score was computed on a significantly underestimated data sample as also reflected by its low recall. Nevertheless, the top systems significantly outperform this default score with both systems averaging at 20 sentences per article. The least score was also significantly better than the default at 38.1% F1 at an average of 12 sentences per article.</p><p>With F1 less than 60%, the task shows itself challenging. Some teams ascribed this to the dataset characteristic that contribution sentences constituted only a minority of the sentences in the article (&lt;10%) and thus, overall, presented imbalanced data. To address this they downsampled the data. However, from the two participant systems that used a downsampling strategy, it could not be conclusively verified as an effective strategy since these systems performed on opposite ends of the performance spectrum. On the other hand, incorporating the closest section header and sentence position as features in the BERT model showed itself an effective and reliable strategy for sentence classification. This modeled the dataset better since the sentences were annotated from a few sections and the sentences were usually close to the section header. The system UIUC BioNLP * that incorporated such features outperformed all other systems including the ones with the downsampling strategy, i.e. ITNLP * * and INNOVATORS.</p><p>Finally, how did bootstrapping the test data as silver-labeled data impact model performance? Team ECNUICA that adopted this strategy did not obtain a balanced harmonic mean between their precision and recall achieving the highest recall among all teams of 82.48% and the lowest precision of 26.21%. Thus this strategy did not show itself too effective and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Scientific Terms and Relations Extraction</head><p>These results are shown in Table <ref type="table" target="#tab_9">5</ref> under column "Phrases" for the end-to-end systems. The highest F1 obtained on this task was 46.41%. However, this score was impacted by the pipeline setup such that the low performance in sentence classification impacted the performance in this stage. We conducted a separate evaluation phase to control for this as-pect. In other words, we examined how would the systems perform only on extracting terms and relations given gold contribution sentences? These results are shown in Figure <ref type="figure">3</ref> (a). In fact, the bar chart offers a perspective on the significant differences in system performances when applied on automatically extracted sentences versus gold data. The systems showed the same performance ranking order in both settings. This is a somewhat expected result since none of the systems implemented any specific noisy sentence handling strategy in which case performance differences may have risen. In conclusion, the best result was 46.4% F1 in the end-to-end setting and was 78.6% F1 when given gold sentences.</p><p>Notably, the pipeline systems were 10 points lower for extracting phrases than for sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Triples Extraction</head><p>The final extraction task to build the NCG per article was to form triples from the extracted terms and relations. These results for the pipeline systems are shown in Table <ref type="table" target="#tab_9">5</ref> under column "Triples." The best performance was 22.28% F1 and the 2nd best was significantly lower at 13.79% F1. To evaluate system performances purely for extracting triples, thereby cancelling out the effect of the pipeline setup, additional evaluations were conducted wherein gold data were incrementally made available to the system. These results are shown in Figure <ref type="figure">3 (b)</ref>. Given only the gold sentences, the best team attained 43.44% F1; given gold terms and relations in addition, they achieved 61.29% F1. A score of 61.29% F1 is a strong performance on a still fairly difficult task given the annotation decision subjectivity that may have crept into the data thereby producing considerable variations in annotation patterns. This is discussed in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifying only the Information Unit Labels</head><p>We conducted a meta-evaluation for identifying the set of IU labels per article. These results are shown in Table <ref type="table" target="#tab_9">5</ref> under column "Information Units." The top two teams were tied at 72.93% F1 with the second best score at 60.54% F1. Like sentence classification, a default system could be implemented for this task as one that output just the three mandatory IUs, i.e. RESEARCHPROBLEM, MODEL, and RESULTS for all articles. The scores from this default system were 69.01% F1, 81.67% precision, and 59.76% recall. It is 9 points better than the 2nd best. When given gold sentences, systems could be evaluated for identifying just the IUs since the classification were dependent on the underlying sentences. These results are shown in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>A notable exception in the results is that the IU classification score by Team INNOVATORS remained unchanged regardless of pipelined or gold sentences as input. This is because their downsampling heuristic once designed did not rely on the underlying data when filtering. It is likely that the new gold sentences information was not used at all.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Finally, we conclude our Shared Task paper with a discussion on the perceived limitations of our dataset that can potentially be addressed in future work. Thereby, a new dataset will present new opportunities to evaluate systems on this novel task.</p><p>Single Annotator Annotations The NCG Shared Task dataset was annotated by a single annotator. Further, the design of the annotation scheme was supported by only an intra-annotator consensus agreement score for that annotator. Since this work is the first-of-its-kind in proposing an initial scheme, and given the complex nature of this annotation task with the need to design a model within a realistic timeframe, our annotation procedure is well-suited. However, as discussed in our related work <ref type="bibr" target="#b10">(D'Souza and Auer, 2021)</ref>, in the next stage, we advocate for a blind, multi-stage, and multi-annotator annotation process for the NCG scheme, recognizing it as a potentially better annotation model. We find that such a process while incorporating multiple worldviews could better address annotation inconsistencies that may have crept in in our current dataset.</p><p>Non-uniform Distribution of Articles As discussed earlier, our combined training dataset had 29 tasks and the test data had 10 tasks. However, these tasks did not have a uniform distribution of articles in our data. In the training data, the number of articles per task ranged from a maximum of 101 in one task, i.e. "natural language inference," to a minimum of one article in seven tasks -58.62% of the training data tasks had less than 5 articles. The test dataset, on the other hand, followed a more uniform distribution than the training data ranging from a maximum of 32 articles to a minimum of seven articles at an average of 15.5% articles per task. While our training dataset had over 200 articles, it may not have been sufficiently representative to learn uniform patterns. Thus in a new version of the dataset, a more uniform representation of the tasks will be attempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have detailed the NLPCONTRIBUTIONGRAPH Shared Task that entailed structuring research contributions in NLP articles as structured KGs. This task is the first-of-its-kind to be organized in the SemEval series. It attracted a strong participation demographic of 27 participants and seven teams -BERT transformer models were a popular choice among the participant systems in two different capacities, i.e. as classifiers or sequence labelers. Our task also saw the use of traditional parsers such a dependency syntax parsing technology. Further, some systems leveraged a hybrid approach including a combination of heuristics and machine learning. While the end-to-end task performance was low showing the task considerably challenging, each individual subtask toward obtaining an NCG, i.e. contribution sentence classification, scientific terms and relations extraction, and triples formation, demonstrated high performances in the subtask-only evaluation setting, i.e. when given gold data from the previous stage. The best system adopted a hybrid approach which seemed the most effective strategy for building the NCG.</p><p>The NCG dataset is publicly available (D <ref type="bibr">'Souza et al., 2021)</ref> and a KG overview of a structured form of our paper is here https://www.orkg.org/ orkg/comparison/R74774.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Per Information Unit Evaluations</head><p>Table <ref type="table" target="#tab_13">7</ref> shows triple extraction F1 scores for each of the IUs. The scores from each of the three evaluation phases in our Shared Task are separated by slash symbols. Recall that from the second evaluation phase, the gold data were made available to the systems starting with sentences (Evaluation Phase 2, Part 1: Phrases and Triples) followed by the terms and relations additionally (Evaluation Phase 2, Part 2: Triples).</p><p>Comparing the performances across IUs, we see the CODE IU was the easiest to extract. In Phase 1, the best F1 was 83.33%. In both Phase 2, Part 1 and Part 2, the best F1 was 100.0%. This is an expected result for CODE to be easiest to extract since it had the simplest annotation patterns; an example is depicted in Fig. <ref type="figure" target="#fig_3">5</ref>.  Table <ref type="table" target="#tab_12">6</ref> shows the average number of triples per IU reflecting, in a sense, their complexity. We hypothesize that the more the triples, the more complex the extraction task. Comparing these numbers with the results in Table <ref type="table" target="#tab_13">7</ref>, we see that 5th ranked IU, i.e. MODEL, showed the next easiest to extract after CODE, at 38.14% F1, in the end-to-end setting. Following which, we see that the 2nd ranked IU, i.e. RESEARCHPROBLEM, obtained an F1 of 35.79%. Nevertheless, confirming our hypothesis, we found a negative correlation (r -0.65) between the training data triples size per IU and the end-to-end system performances, i.e. for IUs with fewer triples the extraction score is higher for most IUs. The negative correlations were progressively stronger from Part 1 to Part 2 in Evaluation Phase 2 (r -0.75 and r -0.79), respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Normalized APPROACH and EXPERIMENTALSETUP Evaluations</head><p>In Table <ref type="table" target="#tab_14">8</ref>, we revisit overall scores from Table <ref type="table" target="#tab_9">5</ref> for two evaluation aspects in the end-to-end system evaluations, i.e. only for extracting Information Units and Triples. We revisit just these two aspects because they were impacted when we obtained normalizations of four IU labels into two, respectively, i.e. APPROACH and MODEL as APPROACH and EXPERIMENTALSETUP and HYPERPARAMETERS as EXPERIMENTALSETUP. By this, we can observe system performances on a simplified version of our task. Observing "Triples" F1, we see that the ordering of the system performance without and with normalization remain unchanged -the best score obtained a 3 points boost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotated data in JSON format for the RESEARCHPROBLEM Information Unit for the paper "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: (a) Phrases and (b) Triples extraction results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Information Unit identification results in Evaluation Phase 1: End-to-end Pipeline with Pipelined Sentences (blue bars) and Evaluation Phase 2, Part 1 and Part 2 with Gold Sentences (red bars)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Annotated data in JSON format for the CODE Information Unit for the paper "Deep Joint Entity Disambiguation with Local Neural Attention."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Annotated RESEARCHPROBLEM Information Unit contribution data as triples. This data is obtained from the JSON data shown in Fig 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Annotated MODEL Information Unit contribution data as triples. This data is obtained from the JSON data illustrated in Fig.2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>NLPCONTRIBUTIONGRAPH Shared Task 2021 Overall Corpus Statistics For the convenience of the participants, the task was divided into four phases. In the Practice phase, which began on Aug 16, 2020, we released the participant kit that included the full training dataset along with the Python code of the official scoring program https: //github.com/ncg-task/scoring-program.</figDesc><table><row><cell cols="2">4 Task Setup</cell></row><row><cell cols="2">Online Competition We used the CodaLab</cell></row><row><cell cols="2">platform for running the competition on-</cell></row><row><cell>line</cell><cell>https://competitions.codalab.org/</cell></row><row><cell cols="2">competitions/25680.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Binary Classifiers Team YNU-HPCC<ref type="bibr" target="#b21">(Ma et al., 2021)</ref> employed BERT as a binary classifier to classify the contribution sentences. Team IN-NOVATORS<ref type="bibr" target="#b0">(Arora et al., 2021</ref>) also employed a BERT-based binary classifier wherein each instance was a set of 10 sentences with additional sentences as context features to the model. Team KnowGraph@IITK * * *<ref type="bibr" target="#b25">(Shailabh et al., 2021)</ref> used the standard SciBERT + BiLSTM architecture<ref type="bibr" target="#b3">(Beltagy et al., 2019)</ref> as a binary sentence classifier. Team UIUC BioNLP * employed BERT-based binary sentence classifier with features that handled sentence characteristics w.r.t. their context in the article -specifically, its closest preceding topmost and innermost section headers and its position in the article.</figDesc><table /><note>Multi-class Classifiers Team DULUTH (Martin and Pedersen, 2021) framed a 13-class multiclass classification task. They employed de-BERTa<ref type="bibr" target="#b12">(He et al., 2020)</ref> as their classifier. Team ECNUICA employed three pretrained transformer models, viz. RoBERTa<ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, SciBERT<ref type="bibr" target="#b3">(Beltagy et al., 2019)</ref>, and BERT<ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> as an ensemble classifier. They formulated a multi-class classification task as well. The features to BERT models are the original sentence, contextual information as previous and next sentence to the original sentence, and a sub-title of the paragraph with the separator token([SEP]) in between. Team ITNLP * *) employed a BERT-based multi-class classifier that leveraged sentence context and the paragraph heading as additional features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The seven NLPCONTRIBUTIONGRAPH participating teams with their averaged F1 scores over individual subtasks per evaluation phase.</figDesc><table><row><cell></cell><cell></cell><cell cols="10">Column "1" -Evaluation Phase 1: End-to-end Pipeline F1; Column "2.1" -Evaluation Phase</cell></row><row><cell cols="10">2, Part 1: Phrases and Triples F1; and Column "2.2" -Evaluation Phase 2, Part 2: Triples Extraction F1.</cell><cell></cell></row><row><cell cols="10">**system submission had error in phrase offsets for task submission; actual task performance was 49.72 F1.</cell><cell></cell></row><row><cell>Model</cell><cell>F1</cell><cell>Sentences P</cell><cell>R</cell><cell>F1</cell><cell>Phrases P</cell><cell>R</cell><cell cols="2">Information Units F1 P R</cell><cell>F1</cell><cell>Triples P</cell><cell>R</cell></row><row><cell>UIUC BioNLP</cell><cell cols="10">57.27 53.61 61.46 46.41 42.69 50.83 72.93 66.67 80.49 22.28 22.3</cell><cell>22.26</cell></row><row><cell>ITNLP</cell><cell cols="5">56.19 51.74 61.46 45.22 41.6</cell><cell cols="6">49.55 72.93 66.67 80.49 13.79 13.39 14.23</cell></row><row><cell>KnowGraph@IITK</cell><cell>46.8</cell><cell cols="2">39.69 57.01</cell><cell>35.4</cell><cell cols="4">28.99 45.44 60.54 44.13 96.34</cell><cell>8.57</cell><cell>6.53</cell><cell>12.45</cell></row><row><cell>ECNUICA</cell><cell cols="8">39.78 26.21 82.48 32.03 20.73 70.37 54.05 42.86 73.17</cell><cell>6.78</cell><cell>4.28</cell><cell>16.29</cell></row><row><cell>INNOVATORS</cell><cell cols="8">39.87 39.32 40.45 15.63 13.27 19.01 71.72 82.54 63.41</cell><cell>0.97</cell><cell cols="2">14.29 0.5</cell></row><row><cell>DULUTH</cell><cell>38.1</cell><cell cols="2">44.83 33.12</cell><cell>7.08</cell><cell cols="2">13.07 4.86</cell><cell>64.41 60.0</cell><cell>69.51</cell><cell>3.94</cell><cell>9.2</cell><cell>2.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Evaluation Phase 1: End-to-end Pipeline Results</figDesc><table><row><cell>F1 score</cell><cell>20 40 60 100 80 0</cell><cell>46.4 45.2 78.6 78.4 74.5 77.7 35.4 32 UIUC ITNLP KnowGraph@IITK ECNUICA</cell><cell>15.6 37.4 Pipelined 7.1 0 21.3 26.2 Phrases Only Phrases INNOVATORS DULUTH YNU-HPCC</cell><cell>80 60 40 20 0</cell><cell>61.29 43.44 22.28 13.79 40.82 20.53 UIUC ITNLP</cell><cell>8.57 27.92 14.65 KnowGraph@IITK</cell><cell>44.73 7.52 25.7 ECNUICA</cell><cell>0.97 0.97 4.13 INNOVATORS</cell><cell>3.94 27.62 6.14 DULUTH</cell><cell>7.32 2.01 0 YNU-HPCC</cell></row><row><cell cols="3">(a) Evaluation Phase 1:</cell><cell>End-to-end Pipeline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Pipelined Phrases (blue bars) and Evaluation Phase 2,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Part 1: Phrases and Triples Only Phrases (red bars) results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Average no. of triples per Information Unit</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Per Information Unit F1 scores per evaluation phase of the seven participating teams. The three scores in each row are from the three evaluation phases in the Shared Task as follows [Evaluation Phase 1: End-to-end Pipeline]/[Evaluation Phase 2, Part 1: Phrases and Triples]/[Evaluation Phase 2, Part 2: Triples].Best scores are in bold. /83.98 66.67/76.77 80.49/92.68 22.28/25.01 22.3/25.08 22.26/24.94 ITNLP 72.93/82.49 66.67/76.84 80.49/89.02 13.79/14.26 13.39/13.98 14.23/14.56 KnowGraph@IITK 60.54/72.32 44.13/57.04 96.34/98.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell>Information Units P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>Triples</cell><cell>R</cell></row><row><cell>UIUC BioNLP</cell><cell cols="3">72.9378</cell><cell>8.57/10.0</cell><cell cols="2">6.53/7.87</cell><cell>12.45/13.72</cell></row><row><cell>ECNUICA</cell><cell cols="2">54.05/56.76 42.86/45.0</cell><cell>73.17/76.83</cell><cell>6.78/6.72</cell><cell cols="2">4.28/4.24</cell><cell>16.29/16.12</cell></row><row><cell>INNOVATORS</cell><cell>71.72/80.0</cell><cell cols="2">82.54/92.06 63.41/70.73</cell><cell>0.97/0.97</cell><cell cols="3">14.29/14.29 0.5/0.5</cell></row><row><cell>DULUTH</cell><cell cols="2">64.41/77.11 60.0/76.19</cell><cell>69.51/78.05</cell><cell>3.94/4.05</cell><cell cols="2">9.2/10.42</cell><cell>2.51/2.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Evaluation Phase 1: End-to-end Pipeline Results with (APPROACH, MODEL) IUs normalized to AP-PROACH and (EXPERIMENTALSETUP, HYPERPARAMETERS) IUs normalized to EXPERIMENTALSETUP. Best scores are in bold. Scores before the slash are from original dataset and scores after the slash are from the normalized dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Per the task timeline, i.e. within the Phase 1 end-to-end system evaluation, the team achieved 38.28% F1 within the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their comments and suggestions. This work was co-funded by the European Research Council for the project ScienceGRAPH (Grant agreement ID: 819536) and by the TIB Leibniz Information Centre for Science and Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">INNOVATORS at SemEval-2021 Task 11: A Dependency Parsing and BERT-based model for Extracting Contribution Knowledge from Scientific Papers</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tirthankar</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving Access to Scientific Literature with Knowledge Graphs</title>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allard</forename><surname>Oelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer D'</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kheir</forename><forename type="middle">Eddine</forename><surname>Farfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Prinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitalis</forename><surname>Wiens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><forename type="middle">Yaser</forename><surname>Jaradeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bibliothek Forschung und Praxis</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="516" to="529" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards an Open Research Knowledge Graph</title>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Auer</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1157185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SciB-ERT: Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Web of science as a data source for research on scientific and scholarly activity</title>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Birkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Pendlebury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schnell</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NLPContributions: An Annotation Scheme for Machine Reading of Scholarly Contributions in Natural Language Processing Literature</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE 2020) colocated with the ACM/IEEE Joint Conference on Digital Libraries in 2020 (JCDL 2020)</title>
				<meeting>the 1st Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE 2020) colocated with the ACM/IEEE Joint Conference on Digital Libraries in 2020 (JCDL 2020)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2021. Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of Natural Language Processing Contributions-A Trial Dataset</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><surname>Auer</surname></persName>
		</author>
		<idno type="DOI">10.2478/jdis-2021-0023</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Science</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SemEval-2021 Task 11: NLPContributionGraph -Structuring Scholarly NLP Contributions for a Research Knowledge Graph</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><surname>Pederson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic scholar</title>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Fricke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Medical Library Association: JMLA</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DeBERTa: Decodingenhanced BERT with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<idno type="DOI">10.5281/zenodo.1212303</idno>
		<editor>Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network Architectures for Matching Natural Language Sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open research knowledge graph: next generation infrastructure for semantic scholarly knowledge</title>
		<author>
			<persName><forename type="first">Mohamad</forename><forename type="middle">Yaser</forename><surname>Jaradeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allard</forename><surname>Oelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kheir</forename><forename type="middle">Eddine</forename><surname>Farfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Prinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GÃ¡bor</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>KismihÃ³k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ¶ren</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Knowledge Capture</title>
				<meeting>the 10th International Conference on Knowledge Capture</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mabe</surname></persName>
		</author>
		<title level="m">The STM report. An overview of scientific and scholarly publishing</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>5th edition October</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scientific literature: information overload</title>
		<author>
			<persName><forename type="first">Esther</forename><surname>Landhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">535</biblScope>
			<biblScope unit="issue">7612</biblScope>
			<biblScope unit="page" from="457" to="458" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ECNUICA at SemEval-2021 Task 11: Schema based Information Extraction Pipeline</title>
		<author>
			<persName><forename type="first">Jiaju</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Qin Chen, and Liang He</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UIUC BioNLP at SemEval-2021 Task 11: A Cascade of Neural Models for Structuring Scholarly NLP Contributions</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janina</forename><surname>Sarol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Kilicoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">YNU-HPCC at SemEval-2021 Task 11: Using a BERT Model to Extract Contributions from NLP Scholarly Articles</title>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Manghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessia</forename><surname>Bardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Shirrwagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><forename type="middle">La</forename><surname>Bruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Foufoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aenne</forename><surname>LÃ¶hden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelie</forename><surname>BÃ¤cker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Mannocci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Horst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Baglioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Czerniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Kiatropoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Argiro</forename><surname>Kokogiannaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><forename type="middle">De</forename><surname>Bonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Artini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Ottonello</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3516918</idno>
		<title level="m">Antonis Lempesis, Lars Holm Nielsen, Alexandros Ioannidis, Chiara Bigarella</title>
				<imprint/>
	</monogr>
	<note>and Friedrich Summan. 2019. OpenAIRE Research Graph Dump</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Duluth at SemEval-2021 Task 11: Applying DeBERTa to Contributing Sentence Selection and Dependency Parsing for Entity Extraction</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stanza: A Python natural language processing toolkit for many human languages</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">KnowGraph@IITK at SemEval-2021 Task 11: Building Knowledge Graph for NLP Research</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shailabh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajal</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">FÃ¡bio</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Lotufo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10649</idno>
		<title level="m">Portuguese named entity recognition using BERT-CRF</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ITNLP at SemEval-2021 Task 11: Boosting BERT with Sampling and Adversarial Training for Knowledge Extraction</title>
		<author>
			<persName><forename type="first">Genyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fifteenth Workshop on Semantic Evaluation<address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attacks which do not kill training make adversarial learning stronger</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11278" to="11287" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
