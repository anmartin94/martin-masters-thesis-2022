<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2020 Task 4: Commonsense Validation and Explanation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
							<email>shuailongliang@mymail.sutd.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yili</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yilong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhangyue@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2020 Task 4: Commonsense Validation and Explanation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons. Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not. The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense. In the third subtask, a participating system needs to generate the reason. We finally attracted 39 teams participating at least one of the three subtasks. For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans. However, for Subtask C, there is still a relatively large gap between systems and human performance. The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation; The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decades, computer' ability in processing natural language has significantly improved. However, its intelligence for understanding common sense expressed in language is still limited. For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: "John put a turkey into a fridge" while "John put an elephant into the fridge" does not, but it is non-trivial for a computer to tell the difference. Arguably, commonsense reasoning plays a central role in a natural language understanding system <ref type="bibr" target="#b6">(Davis, 2017)</ref>. It is essential to gauge how well computers can understand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. <ref type="bibr">1</ref> Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution <ref type="bibr" target="#b16">(Levesque et al., 2012;</ref><ref type="bibr" target="#b30">Morgenstern and Ortiz, 2015)</ref>, subsequent event prediction <ref type="bibr" target="#b43">(Roemmele et al., 2011)</ref>, ordinal common-sense inference <ref type="bibr" target="#b62">(Zhang et al., 2017)</ref>, situations with adversarial generations <ref type="bibr" target="#b61">(Zellers et al., 2018)</ref>, event validation <ref type="bibr" target="#b56">(Wang et al., 2018)</ref>, reading comprehension <ref type="bibr" target="#b31">(Mostafazadeh et al., 2016;</ref><ref type="bibr" target="#b35">Ostermann et al., 2018b;</ref><ref type="bibr" target="#b34">Ostermann et al., 2018a)</ref>, dialogue <ref type="bibr" target="#b2">(Cui et al., 2020)</ref> and QA <ref type="bibr" target="#b5">(Davis, 2016;</ref><ref type="bibr" target="#b53">Talmor et al., 2018;</ref><ref type="bibr" target="#b28">Mihaylov et al., 2018)</ref>. They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.</p><p>The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., "John put an elephant into the fridge" and "John put a turkey into the fridge", respectively. The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense. For example, for the nonsensical statement, "John put an elephant into the fridge", the three options are "An elephant is much bigger than a fridge", "Elephants are usually white while fridges are usually white", and "An elephant cannot eat a fridge." A system needs to identify the correct reason. In addition, the third task requires the participating systems to generate the reason automatically. We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.</p><p>There are 39 teams submitting valid systems to at least one subtask. In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects. However, for Subtask C, there is still a relatively large between system and human performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>Formally, each instance in our dataset is composed of eight sentences:</p><formula xml:id="formula_0">{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. s 1</formula><p>and s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not. They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense. For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense. So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options. For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense. Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.</p><p>• Subtask A: Validation Task: Select the statement of the two that does not make sense. s 1 : John put a turkey into a fridge. s 2 : John put an elephant into the fridge.</p><p>In this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .</p><p>• Subtask B: Explanation (Multi-Choice) Task: Select the best reason that explains why the given statement does not make sense. Nonsensical statement (s n ): John put an elephant into the fridge. o 1 : An elephant is much bigger than a fridge. o 2 : Elephants are usually white while fridges are usually white. o 3 : An elephant cannot eat a fridge.</p><p>In this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .</p><p>• Subtask C: Explanation (Generation) Task: Generate the reason why this statement does not make sense. Nonsensical statement (s n ): John put an elephant into the fridge. Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge. r 2 : A fridge is much smaller than an elephant. r 3 : Most of the fridges aren't large enough to contain an elephant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0</head><p>The reason is not grammatically correct, or not comprehensible at all, or not related to the statement at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>The reason is just the negation of the statement or a simple paraphrase. Obviously, a better explanation can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>The reason is relevant and appropriate, though it may contain a few grammatical errors or unnecessary parts. Or like case 1, but it's hard to write a proper reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The reason is appropriate and is a solid explanation of why the statement does not make sense.</p><p>Table <ref type="table">1</ref>: Rubrics used in human evaluation in Subtask C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Metrics</head><p>The Subtasks A and B are evaluated using accuracy. Subtask C is evaluated with the BLEU score <ref type="bibr" target="#b37">(Papineni et al., 2002)</ref>. In addition, for Subtask C, we further perform human evaluation. We randomly select 100 instances from the test set and evaluate system outputs on Amazon Mechanical Turk. We ask three different crowd-sourcing workers to score each generated reason with a scale ranging from 0 to 3, inclusively, according the rubrics listed in Table <ref type="table">1</ref>.</p><p>Then we calculate the average score of the three scores as our final human evaluation score. Formally, the human evaluation score of system k is</p><formula xml:id="formula_1">score k = 100 i=1 3 j=1 score ijk 100 * 3 ,<label>(1)</label></formula><p>where score ijk means the score from the j th annotator for system k on the i th instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Construction</head><p>Our data construction is mainly performed on Amazon Mechanical Turk, which consists of two steps:</p><p>• Step 1: In this step, we construct datasets for Subtask A and Subtask B. Specifically, we ask a crowd-sourcing worker to write a sensical statement s c and a nonsensical statement s n . For the nonsensical statement s n , the worker further writes three sentences, o 1 , o 2 , o 3 ; one of them, denoted as o c , explains why the nonsensical statement does not make sense; two of them, denoted as o n1 and o n2 , serve as the confusing choices. (Refer to Section 3.1 for details.)</p><p>•</p><p>Step 2: We then make three reference reasons, r 1 , r 2 , r 3 for Subtask C. We use o c as one of three references, and collect two more references in this step. We ask two different crowd-sourcing workers to write each of them. Note that instead of letting the same worker in step 1 to write these two references, we asked two more workers. The reason is to encourage diversity of the reference. (Refer to Section 3.2 for details.)</p><p>Finally, each instance of the dataset have 8 sentences:</p><formula xml:id="formula_2">{s 1 , s 2 , o 1 , o 2 , o 3 , r 1 , r 2 , r 3 }. Note that one sentence in o 1 , o 2 , o 3 is repeated in r 1 , r 2 , r 3</formula><p>, but for convenience of description, we denote it differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>Step 1: Collecting Data for Subtask A and B Annotation Guidelines. When writing instances, workers were asked to follow several principles: (1) Try to avoid complex knowledge and focus on daily common sense. Make the questions as understandable as possible, so that a literate person is able to give the right answers.</p><p>(2) The confusing reason options, o n1 and o n2 , should better contain more content words or information such as entities and activities in the nonsensical statements s n . For example, the confusing reasons of "John put an elephant into the fridge" should better contain both "elephant" and "fridge". (3) The confusing reasons, o n1 and o n2 , should be related to the statements s n and the correct reason o c and not deviate from the context; otherwise it may be easily captured by pretrained models like BERT <ref type="bibr" target="#b53">(Talmor et al., 2018)</ref>. ( <ref type="formula">4</ref>  and o 3 should only be related to the incorrect statements s n rather than the correct statements s c , because we want further studies to be able to estimate nonsensical statements s n without the correct statement s c . (5) The confusing reasons, o n1 and o n2 , should make sense themselves. Otherwise, the models may simply ignore the incorrect options o n1 , o n2 without considering the casual semantics. This concern is raised from and motivated by the fact that models can achieve high performance in the ROC Story Cloze Task, when only looking at the alternative endings and ignoring the story content <ref type="bibr" target="#b47">(Schwartz et al., 2017)</ref>. ( <ref type="formula">6</ref>) We ask the annotators to make the nonsensical statement s n contain about the same number of words as the sensical statement s c , and the correct reason o c have similar length with other two options. We drop the instances which do not meet such requirements.</p><p>Use of Inspirational Materials. It is not easy for all crowd-sourcing workers to write instances from scratch. To address this issue, we also provide them with external reading materials to stimulate inspiration, such as the sentences of the Open Mind Common Sense (OMCS) project <ref type="bibr" target="#b12">(Havasi et al., 2010)</ref>. For example, "he was sent to a (restaurant)/(hospital) for treatment after a car crash" can be inspired by the two sentences "restaurants provide food" and "hospitals provide medical care".</p><p>Quality Control. To ensure the quality of the data, we manually check the instances and drop or request a rewriting of the low-quality ones. If one worker writes too many low-quality instances, we will remove her or him from our annotator pool. With such process, we finally accept around 30% submitted instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Step 2: Collecting Data for Subtask C Annotation Guidelines. To collect data for Subtask C, each worker is given a nonsensical statement s n and a sensical statement s c and asked to write a reason to explain why the nonsensical statement s n does not make sense. They shall follow the following rules: (1) Do not explain why the sensical statement s c makes sense. (2) Avoid mentioning the sensical statement s c . (3) Write the reason, rather than simply add the word "not" or "can't" to the nonsensical statement s n to form an explanation. (4) Write the reason, don't use patterns like "XXX is not for YYY" to create an explanation. (5) Do not try to justify why the nonsensical statement s n makes sense. (6) Write only one sentence, do not be overly formal. (7) Refrain from using "because" at the beginning of a sentence. (8) Do not try to correct the statement s n , but just give the reason. Quality Control. As the same as in Step 1, after the annotators write the reasons in Step 2, the first two authors of the paper perform the check process again. We reject low-quality reasons (that violate the rules significantly) and low-quality annotators (who write many low-quality reasons with the number above a threshold).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Summary and Analysis</head><p>For SemEval-2020, we created 11,997 instances (i.e., 11,997 8-sentence tuples). We further split the instances into three subsets with 10,000 (the training set), 997 (the development set), and 1,000 (the test set) instances, respectively. We randomly assign the label of the correct options in subtask A and B to avoid unbalanced correct labels. We conduct three more data analysis experiments to evaluate data quality, including sentence length, common words and repetition.</p><p>Average Length. In  sensical statements and nonsensical statements almost have the same average lengths in the three sets (the differences are equal or smaller than 1%), which is balanced. However, there is an obvious gap between the correct reasons and confusing reasons in terms of the average lengths (roughly 4% in the training set and 10% in the dev/test set). Common Word Analysis. The most common words are important for showing the differences between sentences. We only present those words which have obvious different frequencies between sensical statements and nonsensical statements or between correct/referential reasons and confusing reasons. So, we skip most uninformative words, including 'a', 'an', 'the', 'to', 'in', 'on', 'of', 'for', 'and', 'is', 'are' and 'be'. After removing those words, we can list the top-5 common words in each type of sentence in the training/dev+test sets. For sensical statements s c and nonsensical statements s n , there are no significant differences between the training, dev, and test set. However, there is an obvious gap in the correct reasons o c and confusing reasons o n in negative words such as "not", "no", and "cannot". In the training data, negative words are about 3 times more common in the correct option o c than in the confusing options o n . In the dev+test data, the gap is about 40%, which indicates that the dev+test data has a higher quality than the training data. However, as discussed in <ref type="bibr" target="#b33">(Niven and Kao, 2019)</ref>, spurious statistical cues can affect BERT's results. We conjure that the negative words are also spurious effective clues, which make the Subtask B potentially easier. Repetition. The dev+test set have 12 instances (0.6%) that repeat the same nonsensical statements in the training data and 36 instances (1.8%) that repeat the same correct reasons with the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cautions of using the data</head><p>The following advice is given to all task participants and future users: (1) Feel free to use whatever additional data they deem appropriate for the tasks to train their model. (2) Do not use the input of Subtask B/C to help Subtask A and do not use the option o of Subtask B to help Subtask C. Otherwise the task will be artificially easy. This is because of two reasons: a) The nonsensical statements s n of Subtask B and Subtask C is exactly the nonsensical statements s c of Subtask A and, participants can use the input of the Subtask B/C to directly obtain the answer of Subtask A and the option answers o of Subtask B will also reduce the difficulty of Subtask A; b) the correct reason o c of Subtask B is also one of the reference reason o c in Subtask C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Systems and Results</head><p>In this section, we show the evaluation results of all the submitted systems for the three subtasks. Since most systems share similar model architecture for subtasks A and B, we discuss the two subtasks together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subtask A and Subtask B</head><p>The formal evaluation results of Subtask A and B are shown in Table <ref type="table" target="#tab_5">4 and 5</ref>. There are in total 39 valid submissions for Subtask A and 27 valid submissions for Subtask B. Most top-performing submissions Figure <ref type="figure">1</ref>: The most commonly used model architectures used in the three subtasks. This figure is mostly based on Team Solomon's system. For Subtask B and C, the connector can be simply "No, ", to help in constraining the model to learn a choice that explains the unreasonability of the statement. For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input. For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference. adopted the pretrained language models such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019c)</ref>, XLNET <ref type="bibr" target="#b60">(Yang et al., 2019)</ref> and ALBERT <ref type="bibr" target="#b15">(Lan et al., 2019)</ref> as the encoder of the model, and then finetune on the training set of the task. See Figure <ref type="figure">1</ref> for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet <ref type="bibr" target="#b51">(Speer et al., 2017)</ref>, or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features.</p><p>• CN-HIT-IT.NLP  ranks top in Subtask A. They use a variant of K-BERT <ref type="bibr">(Liu et al., 2019a)</ref> as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements.</p><p>• ECNU-SenseMaker <ref type="bibr" target="#b64">(Zhao et al., 2020)</ref>    • Qiaoning <ref type="bibr" target="#b24">(Liu, 2020)</ref> and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa.</p><p>• BUT-FIT <ref type="bibr" target="#b13">(Jon et al., 2020)</ref>, LMVE , Lijunyi  use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system.</p><p>• UAICS <ref type="bibr" target="#b3">(Cusmuliuc et al., 2020)</ref>, DEEPYANG <ref type="bibr" target="#b0">(Bai and Zhou, 2020)</ref>, YNU-oxz <ref type="bibr" target="#b36">(Ou et al., 2020)</ref> Table <ref type="table">6</ref>: Subtask C results of all the submitted systems. Those marked with * did not submit a system description paper, and those marked with + means they do not include Subtask C in their system description paper.</p><p>It can be seen from the results that pretrained language models such as RoBERTa can achieve rather high performance, e.g., the team Solomon achieves 96.0% and 94.0% on Subtask A and Subtask B, respectively, without using further resources. This shows that large-scale pretrained language models do contain commonsense knowledge to deal with the Subtask A and the Subtask B in this challenge. Additionally finetuning the pretrained language models on commonsense-related text such as OMCS, which we use as inspirational materials, can push the results even higher, close to human performance. The best-performing teams on Subtask A and Subtask B both adopt K-BERT, which incorporates the external knowledge base (i.e. ConceptNet) to complement the pretrained language models with knowledge triples. This shows that knowledge-graph-enhanced approaches, such as K-BERT can effectively incorporate external knowledge. However, the high number may also indicate data leaking to some extent, since in the data creation stage, both ConceptNet and OMCS are used as references for the annotator to write the data instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subtask C</head><p>The results for Subtask C are shown in Table <ref type="table">6</ref>. There are in total 17 valid submissions for Subtask C. There are generally two approaches: (1) sequence-to-sequence approach, where the source side is the non-sensical statement, and the reason is the target sequence. (2) language model generation approach, which uses large-scale pretrained auto-regressive language models such as GPT-2 <ref type="bibr" target="#b39">(Radford et al., 2019)</ref> for reason generation, where the non-sensical sentence acts as prompt. An example of the language model generation approach is shown in Figure <ref type="figure">1</ref>, which is most commonly used and achieves relatively good results. Below we describe in detail the systems and their main features.</p><p>• BUT-FIT <ref type="bibr" target="#b13">(Jon et al., 2020)</ref> experiments with both the sequence-to-sequence approach and the language generation approach. For the sequence-to-sequence approach, they use BART <ref type="bibr" target="#b17">(Lewis et al., 2019)</ref> with beam-search decoding to achieves the highest BLEU among all the teams. For the language generation approach, the nonsensical statement is used as a prompt. At the training stage, the statement and the explanation are concatenated together, and a GPT-2 is trained on these sequences with a next token prediction objective. At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated.</p><p>• KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART. To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary. 2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation. This approach proves effective and makes their system second-best for human evaluations.</p><p>• ANA <ref type="bibr" target="#b14">(Konar et al., 2020)</ref> has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use <ref type="bibr">CoS-E (Rajani et al., 2019)</ref> and OpenBook <ref type="bibr" target="#b28">(Mihaylov et al., 2018)</ref> at the task-specific training stage.</p><p>• Solomon <ref type="bibr" target="#b52">(Srivastava et al., 2020)</ref>, JUSTers <ref type="bibr" target="#b9">(Fadel et al., 2020)</ref>, SWAGex (Rim and Okazaki, 2020), UI <ref type="bibr" target="#b8">(Doxolodeo and Mahendra, 2020)</ref>  Large-scale pretrained language models such as BART and GPT-2 dominates the submissions. The two systems with the highest human evaluations, namely ANA and KaLM, use additional resources such as Wiktionary, OMCS, and other commonsense datasets. This again shows that additional knowledge from structured databases can help with the generation of the reasons. From Table <ref type="table">6</ref> we can see that BLEU does not correlate well with Human Evaluation, especially for the top-performing systems. According to a further experiment of BUT-FIT, the naive baseline of "copying source sentence as the reason" can give a BLEU of 17.23, which can rank No. 4 among all the submissions. This indicates that BLEU, which focuses on the surface token overlap, has difficulty in evaluating the generated text reliably. The top-performed system achieves the human evaluation score of 2.10, showing the power of pretrained language models, but considering the human performance of 2.58, we still have a long way to go to generate human acceptable reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention. In the Winograd Schema Challenge (WSC) <ref type="bibr" target="#b16">(Levesque et al., 2012;</ref><ref type="bibr" target="#b30">Morgenstern and Ortiz, 2015)</ref>, a model needs to solve hard co-reference resolution problems based on commonsense knowledge. For example, "The trophy would not fit in the brown suitcase because it was too big. What was too big (trophy or suitcase)?" The Choice of Plausible Alternatives (COPA) <ref type="bibr" target="#b43">(Roemmele et al., 2011</ref>) emphasizes on events and consequences. Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives. All premises and alternatives are simple sentences. For example, the premise can be "The man broke his toe. What was the CAUSE of this?" and the two candidate answers are "(1) He got a hole in his sock." and "(2) He dropped a hammer on his foot." Several subsequent datasets are inspired by COPA. The JHU Ordinal Common-sense Inference (JOCI) <ref type="bibr" target="#b62">(Zhang et al., 2017)</ref> aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation. Situations with Adversarial Generations (SWAG) <ref type="bibr" target="#b61">(Zellers et al., 2018)</ref> request a system to choose the most likely-to-happen alternative after a specific situation. Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused. Besides, our dataset is not limited to events or situations. It concerns a broader commonsense setting, which includes events, descriptions, assertion etc. Some datasets are inspired by reading comprehension. The Story Cloze Test and ROCStories Corpora <ref type="bibr" target="#b31">(Mostafazadeh et al., 2016;</ref><ref type="bibr" target="#b48">Sharma et al., 2018)</ref> aim to figure out the right ending from two candidate sentences after a four-sentence story. For a narrative text, MCScript <ref type="bibr" target="#b34">(Ostermann et al., 2018a)</ref> gives various types of questions and pairs of answer candidates for each question. Most questions require knowledge beyond the facts mentioned in the text. Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want. Some other datasets evolve from QA problems and care more about factual commonsense knowledge. SQUABU <ref type="bibr" target="#b5">(Davis, 2016)</ref> provides a small hand-constructed test of commonsense and scientific questions. CommonsenseQA <ref type="bibr" target="#b53">(Talmor et al., 2018)</ref> asks crowd workers to create questions from ConceptNet <ref type="bibr" target="#b51">(Speer et al., 2017)</ref>, which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBookQA <ref type="bibr" target="#b28">(Mihaylov et al., 2018)</ref> provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions. The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences. MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area <ref type="bibr" target="#b2">(Cui et al., 2020)</ref>. Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.</p><p>Some datasets focus on non-sentential eventual plausibility <ref type="bibr" target="#b56">(Wang et al., 2018;</ref><ref type="bibr" target="#b38">Porada et al., 2019)</ref>, such as "gorilla-ride-camel". In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as "China's territory is larger than Japan's". And some datasets concentrate on limited attributes or actions of world knowledge, such as physics <ref type="bibr" target="#b10">(Forbes and Choi, 2017)</ref>. Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task "Tom's mom become (happy)/(upset) when Tom gets high grades in the exam" is about social and emotional common sense. For our first task, those statements that conforms to commonsense can also be phrased as being plausible. Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.</p><p>More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks. Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time <ref type="bibr" target="#b19">(Liu and Singh, 2004;</ref><ref type="bibr" target="#b11">Havasi et al., 2007;</ref><ref type="bibr" target="#b50">Speer and Havasi, 2013;</ref><ref type="bibr" target="#b51">Speer et al., 2017)</ref>. ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.</p><p>Before organizing this shared-task, a pilot study <ref type="bibr" target="#b57">(Wang et al., 2019)</ref> has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences. In our task here, we also provide training data with human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation. In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences. The task attracted around 40 participating teams, out of which 31 teams submit their system papers. The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.</p><p>We attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences. 2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge <ref type="bibr" target="#b65">(Zhou et al., 2019)</ref>, which helps achieve considerably better performance. 3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers. The top-3 systems also use OMCS, which potentially help them to attain better performances. 4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty. 5) Some instances have obvious patterns. For example, there are tens of instances that contain "put XXX into YYY", and "XXX is bigger than YYY", making the problems simpler. 6) Hundreds of crowd-sourcing workers write instances. It is likely for workers to think about the shared commonsense knowledge, such as "XXX is bigger/shorter/quicker/slower than YYY".</p><p>We consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation. 2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns. 3) Subtask A can be turned into a more difficult form. Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not. 4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and CUHK (Wang et al., 2020) use GPT or GPT-2 finetuned on the task training data. JBNU (Na and Lee, 2020) uses UniLM, which incorporates three LM tasks: unidirectional LM, bidirectional LM and sequence-to-sequence prediction LM, and only use one of the reference correct reasons. UI does not use the training data and treats the generation as a Cloze task. SSN-NLP (S, 2020) uses the seq2seq NMT framework without a pretrained LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average length of different types of sentences of Training/Dev/Test set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>, we present the average length of each type of sentence in the training/dev/test set. The sentences in the development and test set have shorter lengths than those in the training set. This is because we check the development and test more carefully and more strictly, thus removing longer and more incomprehensible instances, which lowers the average lengths of the dev/test set. The</figDesc><table><row><cell>Types of Sentences</cell><cell></cell><cell cols="3">Word:Word Frequency(‰)</cell></row><row><cell>Sensical Statements</cell><cell>can:1.54</cell><cell>his:1.45</cell><cell>with:1.285</cell><cell cols="2">my:1.247 people:0.912</cell></row><row><cell cols="3">Nonsensical Statements can:1.604 his:1.411</cell><cell>with:1.299</cell><cell cols="2">my:1.254 people:0.873</cell></row><row><cell>Correct Reasons</cell><cell cols="4">not:4.545 cannot:1.731 people:1.579 can:1.389</cell><cell>no:1.337</cell></row><row><cell>Confusing Reasons</cell><cell cols="2">can:2.095 people:1.671</cell><cell>not:1.49</cell><cell>have:1.152</cell><cell>than:0.81</cell></row><row><cell cols="3">Referential Reasons not:5.711 cannot:1.65</cell><cell>can:1.09</cell><cell cols="2">people:1.088 have:0.978</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Training set</cell><cell></cell></row><row><cell>Types of Sentences</cell><cell></cell><cell cols="3">Word:Word Frequency(‰)</cell></row><row><cell cols="3">Sensical Statements can:1.343 my:1.322</cell><cell>his:1.291</cell><cell>put:1.259</cell><cell>with:0.978</cell></row><row><cell cols="3">Nonsensical Statements can:1.458 my:1.335</cell><cell>put:1.304</cell><cell>his:1.15</cell><cell>with:0.975</cell></row><row><cell>Correct Reasons</cell><cell cols="2">not:2.325 can:1.709</cell><cell cols="3">no:1.614 people:1.362 than:1.31</cell></row><row><cell>Confusing Reasons</cell><cell cols="3">can:2.332 people:1.619 not:1.609</cell><cell>have:1.094</cell><cell>than:1.079</cell></row><row><cell cols="4">Referential Reasons not:4.264 cannot:1.212 can:1.203</cell><cell>it:1.179</cell><cell>have:1.093</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) Dev+Test set</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-5 common words and their frequencies in different types of sentences in the training and dev+test set. 1.000‰ means this word appear once in every 1000 words.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding.</figDesc><table><row><cell>Team</cell><cell cols="3">Acc. Rank Team</cell><cell cols="3">Acc. Rank Team</cell><cell cols="2">Acc. Rank</cell></row><row><cell>Human</cell><cell>99.1</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CN-HIT-IT.NLP</cell><cell>97.0</cell><cell>1</cell><cell>panaali*</cell><cell>92.5</cell><cell>14</cell><cell>Lijunyi</cell><cell>83.0</cell><cell>27</cell></row><row><cell cols="2">ECNU-SenseMaker 96.7</cell><cell>2</cell><cell cols="2">ZhengxianFan* 92.4</cell><cell>15</cell><cell>ehsantaher*</cell><cell>82.5</cell><cell>28</cell></row><row><cell>IIE-NLP-NUT</cell><cell>96.4</cell><cell>3</cell><cell>LMVE</cell><cell>90.4</cell><cell>16</cell><cell>TakeLab*</cell><cell>81.2</cell><cell>29</cell></row><row><cell>nlpx*</cell><cell>96.4</cell><cell>3</cell><cell>Warren*</cell><cell>90.4</cell><cell>16</cell><cell>Vicki*</cell><cell>79.8</cell><cell>30</cell></row><row><cell>Solomon</cell><cell>96.0</cell><cell>5</cell><cell>TMLab*</cell><cell>89.2</cell><cell>18</cell><cell>TR</cell><cell>79.7</cell><cell>31</cell></row><row><cell>Qiaoning</cell><cell>95.9</cell><cell>6</cell><cell>UAICS</cell><cell>89.1</cell><cell>19</cell><cell>KDE SenseForce</cell><cell>79.6</cell><cell>32</cell></row><row><cell>BUT-FIT</cell><cell>95.8</cell><cell>7</cell><cell>JUST</cell><cell>89.1</cell><cell>19</cell><cell>Hitachi*</cell><cell>78.4</cell><cell>33</cell></row><row><cell>olenet*</cell><cell>95.5</cell><cell>8</cell><cell>eggy*</cell><cell>89.0</cell><cell>21</cell><cell>CUHK</cell><cell>72.4</cell><cell>34</cell></row><row><cell>KaLM</cell><cell>95.3</cell><cell>9</cell><cell>UI</cell><cell>88.2</cell><cell>22</cell><cell>paramitamirza*</cell><cell>69.2</cell><cell>35</cell></row><row><cell>CS-NET</cell><cell>94.8</cell><cell>10</cell><cell>Armins*</cell><cell>87.1</cell><cell>23</cell><cell>UoR</cell><cell>67.6</cell><cell>36</cell></row><row><cell>fkerem*</cell><cell>94.4</cell><cell>11</cell><cell>DEEPYANG</cell><cell>85.1</cell><cell>24</cell><cell>chenggguang*</cell><cell>62.3</cell><cell>37</cell></row><row><cell>JUSTers</cell><cell>92.9</cell><cell>12</cell><cell>WUY*</cell><cell>84.2</cell><cell>25</cell><cell cols="2">praveenjoshi007* 55.9</cell><cell>38</cell></row><row><cell>CS-NLP</cell><cell>92.7</cell><cell>13</cell><cell>YNU-oxz</cell><cell>83.6</cell><cell>26</cell><cell>dania*</cell><cell>21.6</cell><cell>39</cell></row></table><note>Like CN-HIT-IT.NLP, their model is also based on K-BERT. In addition, they use unstructured text from ConceptNet and Subtask C to pretrain the language model.• IIE-NLP-NUT<ref type="bibr" target="#b59">(Xing et al., 2020)</ref> uses RoBERTa as the encoder, and conduct a second pretraining on the original RoBERTa model with the textual corpus from Open Mind Common Sense(Singh et   </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Subtask A results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems.</figDesc><table><row><cell>Team</cell><cell cols="3">Acc. Rank Team</cell><cell cols="3">Acc. Rank Team</cell><cell cols="2">Acc. Rank</cell></row><row><cell>Human</cell><cell>97.8</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ECNU-SenseMaker 95.0</cell><cell>1</cell><cell>JBNU</cell><cell>91.4</cell><cell>10</cell><cell cols="2">Masked Reasoner 73.5</cell><cell>19</cell></row><row><cell>CN-HIT-IT.NLP</cell><cell>94.8</cell><cell>2</cell><cell>Qiaoning</cell><cell>90.8</cell><cell>11</cell><cell>KDE SenseForce</cell><cell>72.8</cell><cell>20</cell></row><row><cell>IIE-NLP-NUT</cell><cell>94.3</cell><cell>3</cell><cell>CS-NET</cell><cell>89.0</cell><cell>12</cell><cell>SSN-NLP</cell><cell>68.3</cell><cell>21</cell></row><row><cell>Solomon</cell><cell>94.0</cell><cell>4</cell><cell>WUY*</cell><cell>85.3</cell><cell>13</cell><cell>TakeLab*</cell><cell>66.8</cell><cell>22</cell></row><row><cell>LMVE</cell><cell>93.8</cell><cell>5</cell><cell>SWAGex</cell><cell>84.6</cell><cell>14</cell><cell>UoR</cell><cell>65.9</cell><cell>23</cell></row><row><cell>CS-NLP</cell><cell>93.7</cell><cell>6</cell><cell>TMLab*</cell><cell>82.0</cell><cell>15</cell><cell>dania*</cell><cell>55.5</cell><cell>24</cell></row><row><cell>KaLM</cell><cell>93.2</cell><cell>7</cell><cell>UI</cell><cell>80.5</cell><cell>16</cell><cell>CUHK</cell><cell>51.2</cell><cell>25</cell></row><row><cell>BUT-FIT</cell><cell>93.1</cell><cell>8</cell><cell cols="2">ehsantaher* 79.3</cell><cell>17</cell><cell>bhu*</cell><cell>36.4</cell><cell>26</cell></row><row><cell>JUSTers</cell><cell>92.3</cell><cell>9</cell><cell>uzh*</cell><cell>75.8</cell><cell>18</cell><cell cols="2">praveenjoshi007* 32.6</cell><cell>27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation.</figDesc><table /><note>Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020),JUSTers<ref type="bibr" target="#b9">(Fadel et al., 2020)</ref>, CS-NLP<ref type="bibr" target="#b45">(Saeedi et al., 2020)</ref>, UI<ref type="bibr" target="#b8">(Doxolodeo and Mahendra, 2020)</ref>, TR<ref type="bibr" target="#b54">(Teo, 2020)</ref> UoR<ref type="bibr" target="#b26">(Markchom et al., 2020)</ref>, Masked Reasoner<ref type="bibr" target="#b25">(Lu, 2020)</ref> have similar model architecture, with RoBERTa as the encoder. In addition,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> Wiktionary version: enwiktionary-20200220   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Science Foundation of China (Grant No. 61976180), the Westlake University, and the Bright Dream Joint Institute for Intelligent Robotics. The research of Xiaodan Zhu is supported by NSERC. Yue Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepyang at semeval-2020 task 4: Using the hidden layer of bert model for differentiating common sense</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the AI2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>abs/1803.05457</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual: A dataset for multi-turn dialogue reasoning</title>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Conference of the Association for Computational Linguistics</title>
				<meeting>the 58th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uaics at semeval-2020 task 4 -using a bidirectional transformer for task a</title>
		<author>
			<persName><forename type="first">Ciprian-Gabriel</forename><surname>Cusmuliuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia-Georgiana</forename><surname>Coca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cs-net at semeval-2020 task 4: Siamese bert for comve</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Ranjan Soumya Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Routray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How to write science questions that are easy for people and hard for computers</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Logical formalizations of commonsense reasoning: a survey</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="651" to="723" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ui at sem-eval 2020 task 4 : Commonsense validation and explanationby exploiting contradiction</title>
		<author>
			<persName><forename type="first">Kerenza</forename><surname>Doxolodeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahmad</forename><surname>Mahendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Justers at semeval-2020 task 4: Evaluating transformer models against commonsense validation and explanation</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Verb physics: Relative physical knowledge of actions and objects</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent advances in natural language processing</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open mind common sense: Crowd-sourcing for common sense</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Moeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">But-fit at semeval-2020 task 4: Multilingual commonsense</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fajcik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Docekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Smrz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ana at semeval-2020 task 4: multitask learning for commonsense reasoning (union)</title>
		<author>
			<persName><forename type="first">Anandh</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amine</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osmar</forename><surname>Zaiane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR&apos;12</title>
				<meeting>the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR&apos;12</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lijunyi at semeval-2020 task 4: An albert model based on different training sizes and depths for commonsense validation and explanation</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyan</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conceptnet-a practical commonsense reasoning tool-kit</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07606</idno>
		<title level="m">Qi Ju, Haotang Deng, and Ping Wang. 2019a. K-bert: Enabling language representation with knowledge graph</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lmve at semeval-2020 task 4: Commonsense validation and explanation using pretraining language model</title>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qiaoning at semeval-2020 task 4: Commonsense validation and explanation system based on ensemble of language model</title>
		<author>
			<persName><forename type="first">Pai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Masked reasoner at semeval-2020 task 4: Fine-tuning roberta for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Daming</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uor at semeval-2020 task 4: Pre-trained sentence transformer models for commonsense validation and explanation</title>
		<author>
			<persName><forename type="first">Thanet</forename><surname>Markchom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandresh</forename><surname>Pravin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhi</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kde senseforce</title>
		<author>
			<persName><forename type="first">Khanddorj</forename><surname>Mendbayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaki</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Teamjust at semeval-2020 task 4: Commonsense validation and explanation using ensembling techniques</title>
		<author>
			<persName><forename type="first">Roweida</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malak</forename><surname>Abdullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The winograd schema challenge: Evaluating progress in commonsense reasoning</title>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
				<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4024" to="4025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jbnu at semeval-2020 task 4: Bert and unilm for commonsense validation and explanation</title>
		<author>
			<persName><forename type="first">Jong-Hyeon</forename><surname>Seung-Hoon Na</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
				<meeting>The 14th International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<idno>abs/1907.07355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mcscript: A novel dataset for assessing machine comprehension using script knowledge</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). European Language Resource Association</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). European Language Resource Association</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semeval-2018 task 11: Machine comprehension using commonsense knowledge</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
				<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="747" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ynu-oxz at semeval-2020 task 4: Commonsense validation using bert with bidirectional gru</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Can a gorilla ride a camel? learning semantic plausibility from text</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Porada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02361</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Event2mind: Commonsense inference on events, intents, and reactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Swagex at semeval-2020 task 4: Commonsense explanation as next event prediction</title>
		<author>
			<persName><forename type="first">Wiem</forename><surname>Ben Rim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011-03" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ssn-nlp at semeval-2020 task 4: Text classification and generation on common sense context using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kayalvizhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cs-nlp team at semeval-2020 task 4: Evaluation of state-of-the-art nlp deep learning architectures on commonsense reasoning task</title>
		<author>
			<persName><forename type="first">Sirwe</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliakbar</forename><surname>Panahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyran</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvis C</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">ATOMIC: an atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno>abs/1811.00146</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The effect of different writing tasks on linguistic style: A case study of the roc story cloze task</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
				<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tackling the story ending biases in the story cloze test</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Bakhshandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="752" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Open mind common sense: Knowledge acquisition from the general public</title>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travell</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">Li</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OTM Confederated International Conferences&quot; On the Move to Meaningful Internet Systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1223" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Conceptnet 5: A large semantic network for relational knowledge</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The People&apos;s Web Meets NLP</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="161" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Team solomon at semeval-2020 task 4: Be reasonable: Exploiting large-scale language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Vertika</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeep</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeon</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><forename type="middle">R R</forename><surname>Hyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><forename type="middle">Jaiswal</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>abs/1811.00937</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tr at semeval-2020 task 4: Exploring the limits of language-model-based common sense validation</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
				<meeting>The 14th International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Kalm at semeval-2020 task 4: Knowledge-aware language models for comprehension and generation</title>
		<author>
			<persName><forename type="first">Jiajing</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Modeling semantic plausibility by injecting world knowledge</title>
		<author>
			<persName><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00619</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Does it make sense? and why? a pilot study for sense making and explanation</title>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="4020" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cuhk at semeval-2020 task 4: Commonsense explanation, reasoning and prediction with multi-task learning</title>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sak Kwong</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Iie-nlp-nut at semeval-2020 task 4: Guiding plm with prompt template reconstruction strategy for comve</title>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ordinal common-sense inference</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="379" to="395" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cn-hit-it.nlp at semeval-2020 task 4: Enhanced language representation with multiple knowledge triples</title>
		<author>
			<persName><forename type="first">Yice</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ecnu-sensemaker at semeval-2020 task 4: Leveraging heterogeneousknowledge resources for commonsense validation and explanation</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th International Workshop on Semantic Evaluation</title>
				<meeting>The 14th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Evaluating commonsense in pre-trained language models</title>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11931</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
