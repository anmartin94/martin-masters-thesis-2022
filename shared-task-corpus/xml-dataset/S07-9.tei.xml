<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
							<email>lluism@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Center Technical University of Catalonia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Villarejo</surname></persName>
							<email>luisv@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Center Technical University of Catalonia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
							<email>amarti@ub.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Centre de Llenguatge i Computació</orgName>
								<address>
									<addrLine>CLiC Universitat de</addrLine>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
							<email>mtaule@ub.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Centre de Llenguatge i Computació</orgName>
								<address>
									<addrLine>CLiC Universitat de</addrLine>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish). In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish. Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Multilevel Semantic Annotation of Catalan and Spanish task is split into the following three subtasks:</p><p>Noun Sense Disambiguation (NSD): Disambiguation of all frequent nouns ("all words" style).</p><p>Named Entity Recognition (NER): The annotation of (possibly embedding) named entities with basic entity types.</p><p>Semantic Role Labeling (SRL): Including also two subtasks, i.e., the annotation of verbal predicates with semantic roles (SR), and verb tagging with semantic-class labels (SC).</p><p>All semantic annotation tasks are performed on exactly the same corpora for each language. We presented all the annotation levels together as a complex global task, since we were interested in approaches which address these problems jointly, possibly taking into account cross-dependencies among them. However, we were also accepting systems approaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of the languages.</p><p>In Section 2 we describe the methodology followed to develop the linguistic corpora for the task. Sections 3 and 4 summarize the task setting and the participant systems, respectively. Finally, Section 5 presents a comparative analysis of the results. For any additional information on corpora, resources, formats, tagsets, annotation manuals, etc. we refer the reader to the official website of the task 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Linguistic corpora</head><p>The corpora used in this SemEval task are a subset of CESS-ECE, a multilingual Treebank, composed of a Spanish (CESS-ESP) and a Catalan (CESS-CAT) corpus of 500K words each <ref type="bibr" target="#b5">(Martí et al., 2007b)</ref>. These corpora were enriched with different kinds of semantic information: argument structure, thematic roles, semantic class, named entities, and WordNet synsets for the 150 most frequent nouns. The annotation process was carried out in a semiautomatic way, with a posterior manual revision of all automatic processes.</p><p>A sequential approach was adopted for the annotation of the corpus, beginning with the basic levels of analysis, i.e., POS tagging and chunking (automatically performed) and followed by the more complex levels: syntactic constituents and functions (manually tagged) and semantic annotation (manual and semiautomatic processes with manual completion and posterior revision). Furthermore, some experiments concerning inter-annotator agreement were carried out at the syntactic <ref type="bibr" target="#b2">(Civit et al., 2003)</ref> and semantic levels <ref type="bibr" target="#b3">(Màrquez et al., 2004)</ref> in order to evaluate the quality of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntactic Annotation</head><p>The syntactic annotation consists of the labeling of constituents, including elliptical subjects, and syntactic functions. The surface order was maintained and only those constituents directly attached to any kind of 'Sentence' root node were considered ('S', 'S.NF', 'S.F', 'S*'). The syntactic functions are: subject (SUJ), direct object (OD), indirect object (OI), attribute (ATR), predicative (CPRED), agent complement (CAG), and adjunct (CC). Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive any thematic role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lexical Semantic Information: WordNet</head><p>We selected the 150 most frequent nouns in the whole corpus and annotated their occurrences with WordNet synsets. No other word categories were treated (verbs, adjectives and adverbs). We used a steady version of Catalan and Spanish WordNets, linked to WordNet 1.6. Each noun either matched a WordNet synset or a special label indicating a specific circumstance (for instance, the tag C2S indicates that the word does not appear in the dictionary). All this process was carried out manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Named Entities</head><p>The corpora were annotated with both strong and weak Named Entities. Strong NEs correspond to single lexical tokens (e.g., "[U.S.] LOC "), while weak NEs include, by definition, some strong entities (e.g., "The [president of [US] LOC ] P ER "). <ref type="bibr" target="#b0">(Arévalo et al., 2004)</ref>. Thus, NEs may embed. Six basic semantic categories were distinguished: Person, Organization, Location, Date, Numerical expression, and Others <ref type="bibr" target="#b1">(Borrega et al., 2007)</ref>.</p><p>Two golden rules underlie the definition of NEs in Spanish and Catalan. On the one hand, only a noun phrase can be a NE. On the other hand, its referent must be unique and unambiguous. Finally, another hard rule (although not 100% reliable) is that only a definite singular noun phrase might be a NE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Thematic Role Labeling / Semantic Class</head><p>Basic syntactic functions were tagged with both arguments and thematic roles, taking into account the semantic class related to the verbal predicate <ref type="bibr">(Taulé et al., 2006b)</ref>. We characterized predicates by means of a limited number of Semantic Classes based on Event Structure Patterns, according to four basic event classes: states, activities, accomplishments, and achievements. These general classes were split into 17 subclasses, depending on thematic roles and diathesis alternations.</p><p>Similar to PropBank, the set of arguments selected by the verb are incrementally numbered expressing the degree of proximity of an argument in relation to the verb (Arg0, Arg1, Arg2, Arg3, Arg4). In our proposal, each argument includes the thematic role in its label (e.g., Arg1-PAT). Thus, we have two different levels of semantic description: the argument position and the specific thematic role. This information was previously stored in a verbal lexicon for each language. In these lexicons, a semantic class was established for each verbal sense, and the mapping between their syntactic functions with the corresponding argument structure and thematic roles was declared. These classes resulted from the analysis of 1,555 verbs from the Spanish corpus and 1,077 from the Catalan. The annotation process was performed in two steps: firstly, we annotated automatically the unambiguous correspondences between syntactic functions and thematic roles <ref type="bibr" target="#b4">(Martí et al., 2007a)</ref>; secondly, we manually checked the outcome of the previous process and completed the rest of thematic role assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Subset for SemEval-2007</head><p>The corpora extracted from CESS-ECE to conform SemEval-2007 datasets are: (a) SemEval-CESS-ESP (Spanish), made of 101,136 words (3,611 sentences), with 29% of the corpus coming from the Spanish EFE News Agency and 71% coming from Lexesp, a Spanish balanced corpus; (b) SemEval-CESS-CAT (Catalan), consisting of 108,207 words (3,202 sentences), with 71% of the corpus consistinf of Catalan news from EFE News Agency and 29% coming from the Catalan News Agency (ACN).</p><p>These corpora were split into training and test subsets following a a 90%-10% proportion. Each test set was also partitioned into two subsets: 'indomain' and 'out-of-domain' test corpora. The first is intended to be homogeneous with respect to the training corpus and the second was extracted from a part of the CESS-ECE corpus annotated later and not involved in the development of the resources (e.g., verbal dictionaries). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task setting</head><p>Data formats are similar to those of CoNLL-2004/2005 shared tasks on SRL (column style presentation of levels of annotation), in order to be able to share evaluation tools and already developed scripts for format conversion.</p><p>In Figure <ref type="figure" target="#fig_2">1</ref> you can find an example of a fully annotated sentence in the column-based format. There is one line for each token, and a blank line after the last token of each sentence. The columns, separated by blank spaces, represent different annotations of the sentence with a tagging along words. For structured annotations (parse trees, named entities, and arguments), we use the Start-End format. Columns 1-6 correspond to the input information; columns 7 and above contain the information to be predicted. We can group annotations in five main categories: All these annotations in column format are extracted automatically from the syntactic-semantic trees from the CESS-ECE corpora, which were distributed with the datasets. Participants were also provided with the whole Catalan and Spanish Word-Nets (v1.6), the verbal lexicons used in the role labeling annotation, the annotation guidelines as well as the annotated corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participant systems</head><p>About a dozen teams expressed their interest in the task. From those, only 5 registered and downloaded datasets, and finally, only two teams met the deadline and submitted results. ILK2 (Tilburg University) presented a system addressing Semantic Role Labeling, and UPC* (Technical University of Catalonia) presented a system addressing all subtasks independently 3 . The ILK2 SRL system is based on memory-based classification of syntactic constituents using a rich feature set. UPC* used several machine learning algorithms for addressing the different subtasks (AdaBoost, SVM, Perceptron). For SRL, the system implements a re-ranking strategy using global features. The candidates are generated using a state-of-the-art SRL base system.</p><p>Although the task targeted at systems addressing all subtasks jointly none of the participants did it. <ref type="bibr">4</ref> We believe that the high complexity of the whole task together with the short period of time available were the main reasons for this failure. From this point of view, the conclusions are somehow disappointing. However, we think that we have contributed with a very valuable resource for the future research and, although not complete, the current systems provide also valuable insights about the task and are very good baselines for the systems to come.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In the following subsections we present an analysis of the results obtained by participant systems in the <ref type="figure">INPUT--------------------------------------------------------------&gt; OUTPUT-----------------------------------BASIC_INPUT_INFO-----&gt; EXTRA_INPUT_INFO---------------------------&gt; NE NS-------&gt; SR------------------------</ref>  <ref type="figure">-------------------------------------------------------------------------------------------------------------</ref>  three subtasks. Results on the test set are presented along 2 dimensions: (a) language ('ca'=Catalan; 'es'=Spanish); (b) corpus source ('in'=in-domain corpus; 'out'=out-of-domain corpus). We will use a language.source pair to denote a particular test set. Finally, '*' will denote the addition of the two subcorpora, either in the language or source dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NSD</head><p>Results on the NSD subtask are presented in  The left part of the table ("all words") contains results on the complete test sets, while the right part ("selected words") contains the results restricted to the set of words with trained SVM classifiers. This set covers 31.0% of the word occurrences in the training set and 28.2% in the complete test set.</p><p>The main observation is that training/test corpora contain few sense variations. Sense distributions are very skewed and, thus, the simple baseline shows a very high accuracy (almost 85%). The UPC* system only improves BSL accuracy by one point. This can be partly explained by the small size of the wordbased training corpora. Also, this improvement is diminished because UPC* only treated a subset of words. However, looking at the right-hand side of the table, the improvement over the baseline is still modest (∼3 points) when focusing only on the treated words. As a final observation, no significant differences are observed across languages and corpora sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NER</head><p>Results on the NER subtask are presented in Table <ref type="table">2</ref>. This time, BSL stands for a baseline system consisting of collecting a gazetteer with the strong NEs appearing in the training set and assigning the longest matches of these NEs in the test set. Weak entities are simply ignored by BSL. UPC* presented a system which treats strong and weak NEs in a pipeline of two processors. Classifiers trained with multiclass AdaBoost are used to predict the strong and weak NEs. See authors' paper for details. Table <ref type="table">2</ref>: Overall results on the NER subtask UPC* system largely overcomes the baseline, mainly due to the low recall of the latter. By languages, results on Catalan are significantly better than those on Spanish. We think this is attributable mainly to corpora variations across languages. By corpus source, "in-domain" results are slightly better, but the difference is small (1.78 points). Overall, the results for the NER task are in the mid seventies, a remarkable result given the small training set and the complexity of predicting embedded NEs.</p><p>Detailed results on concrete entity types are presented in Table <ref type="table" target="#tab_5">3</ref> (sorted by decreasing F 1 ). As expected, DAT and NUM are the easiest entities to recognize since they can be easily detected by simple patterns and POS tags. On the contrary, entity types requiring more semantic information present fairly lower results. ORG PER and LOC are in the seventies, while OTH is by far the most difficult class, showing a very low recall. This is not surprising since OTH agglutinates a wide variety of entity cases which are difficult to characterize as a whole.  Another interesting analysis is to study the differences between strong and weak entities (see Table <ref type="table" target="#tab_7">4</ref>) . Contrary to our first expectations, results on weak entities are much better (up to 11 F 1 points higher). Weak NEs are simpler for two reasons: (a) there exist simple patters to characterize them, with-out the need of fully recognizing their internal strong NEs; (b) there is some redundancy in the corpus when tagging many equivalent weak NEs in embedded noun phrases. It is worth noting that the low results for strong NEs come from classification rather than recognition (recognition is almost 100% given the "proper noun" PoS tag), thus the recall for weak entities is not diminished by the errors in strong entity classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SRL</head><p>SRL is the most complex and interesting problem in the task. We had two participants ILK2 and UPC*, which participated in both subproblems, i.e., labeling arguments of verbal predicates with thematic roles (SR), and assigning semantic class labels to target verbs (SC). Detailed results of the two systems are presented in Tables <ref type="table" target="#tab_9">5 and 6</ref>.  The ILK2 system outperforms UPC* in both SR and SC. For SR, both systems use a traditional architecture of labeling syntactic tree nodes with thematic roles using supervised classifiers. We would attribute the overall F 1 difference (2.68 points) to a better feature engineering by ILK2, rather than to differences in the Machine Learning techniques used. Overall results in the eighties are remarkably high given the training set size and the granularity of the thematic roles (though we have to take into account that systems work with gold parse trees). Again, the results are comparable across languages and slightly better in the "in-domain" test set.  In the SC subproblem, the differences are similar (2.60 points). In this case, ILK2 trained specialized classifiers for the task, while UPC* used heuristics based on the SR outcomes. As a reference, the baseline consisting of tagging each verb with its most frequent semantic class achieves F 1 values of 64.01, 63.97, 41.00, and 57.42 on ca.in, ca.out, es.in, es.out, respectively. Now, the results are significantly better in Catalan, and, surprisingly, the 'out' test corpora makes F 1 to raise. The latter is an anomalous situation provoked by the 'es.in' tset. <ref type="bibr">5</ref> Table <ref type="table" target="#tab_13">7</ref> shows the global SR results by numbered arguments and adjuncts Interestingly, tagging adjuncts is far more difficult than tagging core arguments (this result was also observed for English in previous works). Moreover, the global difference between ILK2 and UPC* systems is explained by their ability to tag adjuncts (70.22 vs. 58.37). In the core arguments both systems are tied. Also in the same table we can see the overall results on a simplified SR setting, in which the thematic roles are eliminated from the SR labels keeping only the argument number (like other evaluations on PropBank). The results are only ∼2 points higher in this setting.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BASIC INPUT INFO(columns 1-3). The basic input information, including: (a) WORD (column 1) words of the sentence; (b) TN (column 2) target nouns of the sentence, marked with '*' (those that are to be assigned WordNet synsets); (c) TV (column 3) target verbs of the sentence, marked with '*' (those that are to be annotated with semantic roles).EXTRA INPUT INFO (columns 4-6). The extra input information, including: (a) LEMMA (column 4) lemmas of the words; (b) POS (column 5) part-of-speech tags; (c) SYNTAX (column 6) Full syntactic tree. NE (column 7). Named Entities. NS (column 8). WordNet sense of target nouns. SR (columns 9 and above). Information on semantic roles, including: (a) SC (column 9). Semantic class of the verb; (b) PROPS (columns 10 and above). For each target verb, a column representing the argument structure. Core numbered arguments include the thematic role labels. ArgM's are the adjuncts. Columns are ordered according to the textual order of the predicates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of an annotated sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Overall accuracy on the NSD subtask</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Detailed results on the NER subtask: UPC* team; Test corpus *.*</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Results on strong vs. weak named entities:</cell></row><row><cell>UPC* team; Test corpus *.*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Overall results on the SRL subtask: semantic role labeling (SR)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>86.57 86.57 90.25 88.50 89.37 es.* 81.05 81.05 81.05 84.30 83.63 83.83 *.in 81.17 81.17 81.17 84.68 83.11 83.89 *.out 86.72 86.72 86.72 90.04 89.08 89.56 *.* 83.86 83.86 83.86 87.12 85.81 86.46</figDesc><table><row><cell></cell><cell></cell><cell>UPC*</cell><cell></cell><cell></cell><cell>ILK2</cell><cell></cell></row><row><cell>Test</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>ca.*</cell><cell>86.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Overall results on the SRL subtask: semantic class tagging (SC)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>87.73 89.05 89.42 88.58 88.99 Adj 64.72 53.16 58.37 72.54 68.04 70.22 A-TR 92.91 90.15 91.51 91.31 90.45 90.88</figDesc><table><row><cell></cell><cell></cell><cell>UPC*</cell><cell></cell><cell></cell><cell>ILK2</cell><cell></cell></row><row><cell>Test</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Arg</cell><cell>90.41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Global results on numbered arguments (Arg), adjuncts (Adj), and numbered arguments without thematic role tag (A-TR). Test corpus *.* Finally, Table 8 compares overall SR results on known vs. new predicates. As expected, the re-sults on the verbs not appearing in the training set are lower, but the performance decrease is not dramatic (3-6 F 1 points) indicating that generalization to new predicates is fairly good. Known 84.39 78.43 81.30 84.88 83.46 84.16 New 81.31 75.56 78.33 79.34 77.81 78.57</figDesc><table><row><cell></cell><cell></cell><cell>UPC*</cell><cell></cell><cell></cell><cell>ILK2</cell><cell></cell></row><row><cell>Test</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Global results on semantic role labeling for known versus new predicates. Test corpus *.*</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.lsi.upc.edu/∼nlp/semeval/msacs.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For historical reasons we referred to these splits as '3LB' and 'CESS-ECE', respectively. Participants in the task are actually using these names, but we opted for using a more simple notation in this paper (see Section 5).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Some members of this team are also task organizers. This is why we mark the team name with an asterisk.4  The UPC* team tried some inter-task features to improve SRL but initial results were not successful.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">By chance, the genre of this part of corpus is mainly literary. We are currently studying how this is affecting performance results on all subtasks and, particularly, semantic class tagging.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MICE: a Module for Named-Entities Recognition and Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arévalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Corpus Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
			<publisher>John Benjamins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What do we mean when we speak about Named Entities?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Borrega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Corpus Linguistics (forthcoming)</title>
				<meeting>Corpus Linguistics (forthcoming)<address><addrLine>Birmingham, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Qualitative and Quantitative Analysis of Annotatotrs: Agreement in the Development of Cast3LB</title>
		<author>
			<persName><forename type="first">M</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ageno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bufí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd Workshop on Treebanks and Linguistics Theories (TLT-2003)</title>
				<meeting>2nd Workshop on Treebanks and Linguistics Theories (TLT-2003)<address><addrLine>Vaxjo, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="33" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Quality of Lexical Resources for Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Villarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th EsTAL Conference</title>
				<meeting>the 4th EsTAL Conference<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3230</biblScope>
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
	<note>Advances in natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anotación semiautomática con papeles temáticos de los corpus CESS-ECE</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revista de la SEPLN -Monografía TIMM (forthcoming)</title>
				<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CESS-ECE: A multilingual and Multilevel Annotated Corpus. E-pub</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertran</surname></persName>
		</author>
		<ptr target="http://www.lsi.upc.edu/∼mbertran/cess-ece" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic Classes in CESS-LEX: Semantic Annotation of CESS-ECE</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Castellví</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Treebanks and Linguistic Theories (TLT-2006)</title>
				<meeting>the Fifth Workshop on Treebanks and Linguistic Theories (TLT-2006)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
