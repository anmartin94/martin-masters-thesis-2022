<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Report of NEWS 2018 Named Entity Transliteration Shared Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nancy</forename><surname>Chen</surname></persName>
							<email>nancychen@alum.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
							<email>rbanchs@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@suda.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Duan</surname></persName>
							<email>xiangyuduan@suda.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
							<email>haizhou.li@nus.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Report of NEWS 2018 Named Entity Transliteration Shared Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report presents the results from the Named Entity Transliteration Shared Task conducted as part of The Seventh Named Entities Workshop (NEWS 2018) held at ACL 2018 in Melbourne, Australia. Similar to previous editions of NEWS, the Shared Task featured 19 tasks on proper name transliteration, including 13 different languages and two different Japanese scripts. A total of 6 teams from 8 different institutions participated in the evaluation, submitting 424 runs, involving different transliteration methodologies. Four performance metrics were used to report the evaluation results. The NEWS shared task on machine transliteration has successfully achieved its objectives by providing a common ground for the research community to conduct comparative evaluations of state-of-the-art technologies that will benefit the future research and development in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Test Set Accuracy F-score MRR MAP EDI NEWS18 0.0010 (1) 0.2111 (4) 0.0044 (3) 0.0010 (1) EDI NEWS18 0.0010 (1) 0.2063 (5) 0.0041 (4) 0.0010 (1) UALB NEWS18 0.0010 (1) 0.2056 (6) 0.0010 (5) 0.0010 (1) UALB NEWS18 0.0010 (1) 0.2042 (7) 0.0051 (1) 0.0010 (1) UALB NEWS18 0.0010 (1) 0.2034 (8) 0.0051 (1) 0.0010 (1) UALB NEWS18 0.0010 (1) 0.2012 (9) 0.0044 (2) 0.0010 (1) SINGA NEWS18 0.0010 (1) 0.2167 (1) 0.0010 (5) 0.0010 (1) SINGA NEWS18 0.0010 (1) 0.2167 (1) 0.0010 (5) 0.0010 (1) SINGA NEWS18 0.0010 (1) 0.2167 (1) 0.0010 (5) 0.0010 (1) UJUS NEWS18 0.0010 (1) 0.2145 (2) 0.0010 (5) 0.0010 (1) UJUS NEWS18 0.0010 (1) 0.2137 (3) 0.0010 (5) 0.0010 (1) UJUS NEWS18 0.0010 (1) 0.1928 (11) 0.0010 (5) 0.0010 (1) UJUS NEWS18 0.0000 (2) 0.1987 (10) 0.0000 (6) 0.0000 (2)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Names play an important role in the performance of most natural language processing and information retrieval applications. They are also critical in cross-lingual applications such as machine translation and cross-language information retrieval, as it has been shown that system performance correlates positively with the quality of name conversion across languages <ref type="bibr">(Demner-Fushman and Oard 2002</ref><ref type="bibr">, Mandl and Womser-Hacker 2005</ref><ref type="bibr">, Hermjakob et al. 2008</ref><ref type="bibr" target="#b10">, Udupa et al. 2009</ref>. Bilingual dictionaries constitute the traditional source of information for name conversion across languages, however they offer very limited support as in most languages names are continuously emerging and evolving.</p><p>All of the above points to the critical need for robust machine transliteration methods and systems. Significant efforts has been conducted by the research community to address the problem of machine transliteration (Knight and Graehl 1998, <ref type="bibr" target="#b1">Meng et al. 2001</ref><ref type="bibr">, Li et al. 2004</ref><ref type="bibr" target="#b14">, Zelenko and Aone 2006</ref><ref type="bibr" target="#b8">, Sproat et al. 2006</ref><ref type="bibr" target="#b6">, Sherif and Kondrak 2007</ref><ref type="bibr">, Hermjakob et al. 2008</ref><ref type="bibr" target="#b0">, Al-Onaizan and Knight 2002</ref><ref type="bibr">, Goldwasser and Roth 2008</ref><ref type="bibr">, Goldberg and Elhadad 2008</ref><ref type="bibr">, Klementiev and Roth 2006</ref><ref type="bibr" target="#b4">, Oh and Choi 2002</ref><ref type="bibr" target="#b11">, Virga and Khudanpur 2003</ref><ref type="bibr" target="#b13">, Wan and Verspoor 1998</ref><ref type="bibr">, Kang and Choi 2000</ref><ref type="bibr">, Gao et al. 2004</ref><ref type="bibr">, Li et al. 2009a</ref><ref type="bibr">, Li et al. 2009b</ref>). These efforts fall into three main categories: grapheme-based, phoneme-based and hybrid methods. Grapheme based methods <ref type="bibr">(Li et al. 2004</ref>) treat transliteration as a direct orthographic mapping and only uses orthographyrelated features while phoneme-based methods (Knight and Graehl 1998) make use of phonetic correspondences to generate the transliteration. The hybrid approach refers to the combination of several different models or knowledge sources to support the transliteration generation process. Recently, neural network approaches have been explored with varying successes, depending on the size of the training data.</p><p>The first machine transliteration shared task <ref type="bibr">(Li et al. 2009a</ref><ref type="bibr">, Li et al. 2009b</ref>) was organized and conducted as part of NEWS 2009 at ACL-IJCNLP 2009. It was the first time that common benchmarking data in diverse language pairs was provided for evaluating state-of-the-art machine transliteration. While the focus of the 2009 shared task was on establishing the quality metrics and on setting up a baseline for transliteration quality based on those metrics, the 2010 shared task <ref type="bibr">(Li et al. 2010a</ref><ref type="bibr">, Li et al. 2010b</ref>) fo-cused on expanding the scope of the transliteration generation task to about a dozen languages and on exploring the quality of the task depending on the direction of transliteration.</p><p>In NEWS 2011 <ref type="bibr" target="#b15">(Zhang et al. 2011a</ref><ref type="bibr" target="#b16">, Zhang et al. 2011b</ref>), the focus was on significantly increasing the hand-crafted parallel corpora of named entities to include 14 different language pairs from 11 language families, and on making them available as the common dataset for the shared task.</p><p>The NEWS 2018 Shared Task on Named Entity Transliteration has been a continued effort for evaluating machine transliteration performance following the NEWS edition of 2012 <ref type="bibr" target="#b17">(Zhang et al. 2012)</ref>, 2015 <ref type="bibr">(Zhang et al. 2015)</ref> and 2016 <ref type="bibr">(Duan et al. 2016)</ref>.</p><p>In this paper, we present in full detail the results of NEWS 2018 Named Entity Transliteration Shared Task. The rest of the paper is structured as follows. Section 2 provides as short review of the main characteristics of the machine transliteration task and the corpora used for it. Section 3 reviews the four metrics used for the evaluations. Section 4 reports specific details about participation in the shared task, and section 5 presents and discusses the evaluation results. Finally, section 6 presents our main conclusions and future plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shared Task on Transliteration</head><p>Transliteration, sometimes also called Romanization, especially if Latin Scripts are used for target strings <ref type="bibr">(Halpern 2007)</ref>, deals with the conversion of names between two languages and/or script systems. Within the context of this transliteration shared task, we are aiming not only at addressing the name conversion process but also its practical utility for downstream applications, such as machine translation and cross-language information retrieval.</p><p>In this context, we adopt the same definition of transliteration as proposed during <ref type="bibr">NEWS 2009</ref><ref type="bibr">(Li et al. 2009a</ref>): transliteration is understood as the conversion of a given name in the source language (a text string in the source writing system or orthography) to a name in the target language (another text string in the target writing system or orthography) conditioned to the following specific requirements regarding the name representation in the target language:</p><p>• it is phonetically equivalent to the source name,</p><p>• it conforms to the phonology of the target language, and • it matches the user intuition on its equivalence with respect to the source language name. Following previous editions of NEWS some back-transliteration tasks are considered. Backtransliteration attempts to restore transliterated names back into their original source language. NEWS 2018 included a total of six backtransliteration tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shared Task Description</head><p>As in previous editions of the workshop series, the shared task in NEWS 2018 consists of developing machine transliteration systems in one or more of the specified language pairs. Each language pair of the shared task consists of a source and a target language, implicitly specifying the transliteration direction. Training and development data in each of the language pairs was made available to all registered participants for developing their transliteration systems.</p><p>At the evaluation time, hand-crafted test sets of source names were released to the participants, who were required to produce a ranked list of transliteration candidates in the target language for each source name. The system outputs were tested against their corresponding reference sets (which may include multiple correct transliterations for some source names). The performance of a system is quantified using multiple metrics (defined in Section 3).</p><p>In this edition of the workshop, only standard runs (restricted to the train and development data provided) were considered. No other data or linguistic resources were allowed for standard runs. This ensures parity between systems and enables meaningful comparison of performance of various algorithmic approaches in a given language pair. Participants were allowed to submit one or more standard runs for each task they participated in. If more than one standard runs were submitted, it was required to select one as the "primary" run by publishing it into the leaderboard. The primary runs are the ones used to compare results across different systems.</p><p>The NEWS 2018 Shared Task was run on Co-daLab (http://codalab.org/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shared Task Corpora</head><p>Two specific constraints were considered when selecting languages for the shared task: language diversity and data availability. To make the shared task interesting and to attract wider participation, it is important to ensure a reasonable variety of linguistic diversity, orthography and geography. Following NEWS 2016, the tasks were grouped into five categories based on the specific organizations providing the datasets. The 19 tasks for NEWS 2018 are shown in Tables 1.a-e. In addition to the 14 tasks from NEWS 2016, five new tasks (highlighted in italics) have been included this year. This year, new evaluation data was generated and used. In Tables 1.a-e, Type refers to the type of task (transliteration, back-transliteration or mixed); Origin refers to the origin of the names; and Source/Target refer to the source/target scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics and Rationale</head><p>The participants have been asked to submit standard and, optionally, non-standard runs. One of the standard runs must be named as the primary submission, which was the one used for the performance summary. Each run must contain a ranked list of up to ten candidate transliterations for each source name. The submitted results are compared to the ground truth (reference transliterations) using four evaluation metrics capturing different aspects of transliteration performance. The four considered evaluation metrics are • Word Accuracy in Top-1 (ACC),</p><p>• Fuzziness in Top-1 (Mean F-score) <ref type="bibr" target="#b5">(Powers 2011</ref>), • Mean Reciprocal Rank (MRR) <ref type="bibr" target="#b12">(Voorhees 1999)</ref>, and • Mean Average Precision (MAP ref ) <ref type="bibr" target="#b5">(Powers 2011</ref>).</p><p>In the next subsections, we present a brief description of the four considered evaluation metrics. The following notation is further assumed:</p><p>• N: Total number of names (source words) in the test set, • n i : Number of reference transliterations for ith name in the test set (n i ≥ 1), • r i,j : j-th reference transliteration for i-th name in the test set, • c i,k : k-th candidate transliteration (system output) for i-th name in the test set (1 ≤ k ≤ 10), • K i : Number of candidate transliterations produced by a transliteration system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Accuracy in Top-1 (ACC)</head><p>Also known as Word Error Rate, it measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system. ACC = 1 means that all top candidates are correct transliterations; i.e. they match one of the references, and ACC = 0 means that none of the top candidates are correct.</p><formula xml:id="formula_0">= ! ! 1 ∃ !,! ∶ !,! = !,! ; 0 ℎ ! !!!</formula><p>(Eq.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fuzziness in Top-1 (Mean F-score)</head><p>The Mean F-score measures how different, on average, the top transliteration candidate is from its closest reference. F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references.</p><p>Precision and Recall are calculated based on the length of the Longest Common Subsequence (LCS) between a candidate and a reference:</p><formula xml:id="formula_1">, = ! ! + − , (Eq.2)</formula><p>where ED is the edit distance and |x| is the length of x. For example, the longest common subsequence between "abcd" and "afcde" is "acd" and its length is 3. The best matching reference, i.e. the reference for which the edit distance has the minimum, is taken for calculation. If the best matching reference is given by</p><formula xml:id="formula_2">!,! = arg ! !,! , !,! (Eq.3)</formula><p>the Recall, Precision and F-score for the i-th word are calculated as:</p><formula xml:id="formula_3">! = !"# ! !,! ,! !,! ! !,!</formula><p>(Eq.4)</p><formula xml:id="formula_4">! = !"# ! !,! ,! !,! ! !,!</formula><p>(Eq.5)</p><formula xml:id="formula_5">! = 2 ! ! ×! ! ! ! !! ! (Eq.6)</formula><p>The lengths are computed with respect to distinct Unicode characters, and no distinctions are made for different character types of a language (e.g. vowel vs. consonant vs. combining diereses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mean Reciprocal Rank (MRR)</head><p>Measures traditional MRR for any right answer produced by the system, from among the candidates. 1/MRR tells approximately the average rank of the correct transliteration. MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists.</p><formula xml:id="formula_6">! = ! ! ! ∃ !,! , !,! : !,! = !,! ; 0 ℎ (Eq.7) = ! ! ! ! !!!</formula><p>(Eq.8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mean Average Precision (MAP ref )</head><p>This metric measures tightly the precision in the n-best candidates for i-th source name, for which reference transliterations are available. If all of the references are produced, then the MAP is 1. If we denote the number of correct candidates for the i-th source word in k-best list as num(i,k), then MAP ref is given by:</p><formula xml:id="formula_7">!"# = ! ! ! ! ! ( , ) ! ! !!! ! !!! (Eq.9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participation in the Shared Task</head><p>A total of six teams from eight different institutions participated in the NEWS 2018 Shared Task. More specifically, the participating teams were from University of Alberta (UALB), University of Edinburgh (EDI), University of Jadavpur and Universitat des Saarlandes (UJUS), Universite du Quebec a Montreal (UQAM), and team SINGA (from National University of Singapore and Singapore University of Technology and Design) and WIPO (World Intellectual Property Organization) <ref type="bibr">1</ref> .</p><p>In total, we received 424 standard runs. Table <ref type="table" target="#tab_2">2</ref> summarizes the number of standard runs and the teams participated in each task.  Table <ref type="table" target="#tab_2">2</ref> shows that the most popular task continues to be the transliteration from English to Chinese <ref type="bibr" target="#b17">(Zhang et al. 2012)</ref>, followed by Chinese to English, English to Hindi, and English to Tamil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task Results and Analysis</head><p>In this section, we present the official results of the shared task along with brief descriptions of 1 This last team did not submit a system paper, but we are including their submission result for the sake of completeness.</p><p>the different participant systems and some recommendations for future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Shared Task Results</head><p>Figure <ref type="figure">1</ref> summarizes the results of the NEWS 2018 Shared Task. In the figure, only F-scores over the NEWS 2018 evaluation test set for all primary standard submissions are depicted. A total of 66 primary standard submissions were received.</p><p>Most language pairs are able to achieve close to 80% or more in terms of F-score for at least some systems. An intriguing observation from Figure <ref type="figure">1</ref> is that for the language pair English-Chinese, the back-transliteration task from Chinese to English performs at least 15% better than the transliteration task from English to Chinese.</p><p>It also can be observed from the table that results for the T-EnPe and the B-PeEn tasks (western names) are significantly low. This resulted from a mismatch on scripting conventions used for the Persian language between the original train and development sets and the newly developed test set.</p><p>A much more comprehensive presentation of results for the NEWS 2018 Shared Task is provided in the Appendix at the end of this paper, where the resulting scores are reported for all received submissions for all four metrics, including non-primary submissions. All results are presented in 19 tables, each of which reports the scores for one transliteration task. In the tables, all primary standard runs are highlighted in bolditalic fonts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Participant Systems</head><p>This year, the SINGA team (Snigdha et al. 2018) provided two baseline systems using Sequitur and Moses (phrase-based machine translation). All other systems used some version of neural modeling. It is interesting to note that non-neural systems by SINGA, while not the highest in performance, are generally comparable to neural systems or system combinations which include neural models.</p><p>Regarding the systems participating in this year evaluation, the UALB's system <ref type="bibr" target="#b3">(Najafi et al. 2018)</ref>   <ref type="bibr" target="#b9">(Sutskever et al. 2014)</ref>, and RL-NMT <ref type="bibr" target="#b3">(Najafi et al., 2018)</ref>. They showed improvements of up to 8% absolute over a baseline system by using system combination.</p><p>Figure <ref type="figure">1</ref>: Mean F-scores (Top-1) on the evaluation set for all primary submissions and tasks.</p><p>The UJUS system <ref type="bibr">(Kundu et al. 2018</ref>) used an RNN-based NMT framework and a CNN-based NMT framework, where both byte-pair encoding and character-based segmentation were employed for both cases. They also adopted an ensemble method to choose the hypothesis that has the highest frequency of occurrence to further improve accuracy.</p><p>The EDI system (Grundkiewicz et al.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Issues and Recommendations</head><p>In this section, we report some issues encountered during the shared task execution along with recommendations for future improvement of the Shared Task on Named Entity Transliteration. 2 • As mentioned in section 5.1, scripting discrepancies between the train/dev data and the test data occurred for Persian characters in the T-EnPe and B-PeEn tasks. Specifically, the newly developed test set happens to contain a mixture of the Persian and Arabic scripts, which includes visually similar characters that have distinct encodings. This dataset will be revised to resolve this problem for the next evaluation campaign. • Some of the datasets for the shared task are available under specific licensing agreements that have to be undertaken directly by the participants from the data providers. The organizing team will explore alternative means to offer all the datasets in the shared task under a unique centralized licensing agreement, which should be ideally free of cost for the participants. • Some of the participants experienced failures and delays during submissions to the Co-daLab system. Most of these problems are due to server overloads. The organizing team will contact CodaLab support to see how these problems can be fully resolved, or at least minimized, in the future editions of the shared task. • Participants also believe that better publicity for the shared task would result in increased participation in the task. NEWS workshop organizers receive a significant number of request for dataset and information about the shared task throughout the year. However, the total number of participants in the shared task does not reflect such actual interests from the research community on the data and 2</p><p>The organizers would like to thank all the participants, especially the University of Alberta team, for their valuable feedback and suggestions.</p><p>the tasks. Publicity strategies and shared task timelines will be revised accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The Shared Task on Named Entity Transliteration in NEWS 2018 has shown that the research community has a continued interest in this area. This report summarizes the results of the NEWS 2018 Shared Task.</p><p>We are pleased to report a comprehensive set of machine transliteration approaches and their evaluation results from 6 teams from 8 different institutions that participated in the shared task. This year, we received 424 runs in total. Most of the current state-of-the-art in machine transliteration is represented in the systems that have participated in the shared task.</p><p>Encouraged by the continued success of the NEWS workshop series, we plan to continue this event in the future to further promoting machine transliteration research and development.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>was based on multiple system combinations. They presented experimental results involving five different well-known transliteration approaches: DirecTL+ (Jiampojamarn et al. 2009), Sequitur (Bisani and Ney 2008), OpenNMT (Klein et al. 2017), BaseNMT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2018) system uses a deep attention RNN encoder decoder model, which employed neural machine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Number of standard (Std) runs submitted, and teams participating in each task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>translation techniques such as dropout regularization, model ensembling, and re-scoring with right-left models. The EDI system is competitive, outperforming other teams in most of the tasks it participated in.The UQAM system(Le et al. 2018) aligned the sequences in the English Vietnamese language pair before an RNN based machine transliteration system was trained.</figDesc><table><row><cell>B)EnPe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WIPO</cell><cell>SINGA</cell><cell>UQAM</cell><cell>UJUS</cell><cell>EDI</cell><cell>UALB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>M. Bisani, H. Ney. 2008. Joint sequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434-451. CJKI. 2010. CJK Institute. http://www.cjk.org/. D. Demner-Fushman, D.W. Oard. 2002. The effect of bilingual term list size on dictionary-based cross-language information retrieval. In Proc. 36-th Hawaii Int'l. Conf. System Sciences, volume 4, page 108.2. W. Gao, K.F. Wong, W. Lam. 2004. Phoneme-based transliteration of foreign names for OOV problem. In Proc. IJCNLP, pages 374-381, Sanya, Hainan, China. Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177-180, Prague, Czech Republic. A. Kumaran, T. Kellner. 2007. A generic framework for machine transliteration. In Proc. SIGIR, pages 721-722. . T. Le and F. Sadat. 2018. Low-Resource Machine Transliteration Using Recurrent Neural Networks of Asian Languages. In Proc. Named Entities Workshop at ACL 2018. H. Li, M. Zhang, J. Su. 2004. A joint source-channel model for machine transliteration. In Proc. 42nd ACL Annual Meeting, pages 159-166, Barcelona, Spain. H. Li, A. Kumaran, V. Pervouchine, M. Zhang. 2009a. Report of NEWS 2009 machine transliteration shared task. In Proc. Named Entities Workshop at ACL 2009.</figDesc><table><row><cell></cell><cell>H. Li, A. Kumaran, M. Zhang, V. Pervouchine. 2009b.</cell></row><row><cell>Y. Goldberg, M. Elhadad. 2008. Identification of translit-erated foreign words in Hebrew script. In Proc. CICLing, volume LNCS 4919, pages 466-477.</cell><cell>ACL-IJCNLP 2009 Named Entities Workshop -Shared Task on Transliteration. In Proc. Named Entities Work-shop at ACL 2009.</cell></row><row><cell>D. Goldwasser, D. Roth. 2008. Transliteration as con-strained optimization. In Proc. EMNLP, pages 353-362.</cell><cell>H. Li, A. Kumaran, M. Zhang, V. Pervouchine. 2010a. Re-port of news 2010 transliteration generation shared task. In Proc. Named Entities Workshop at ACL 2010.</cell></row></table><note>R. Grundkiewicz and K. Heafield. 2018. Neural Machine Translation Techniques for Named Entity Transliteration. In Proc. Named Entities Workshop at ACL 2018. J. Halpern. 2007. The challenges and pitfalls of Arabic romanization and arabization. In Proc. Workshop on Comp. Approaches to Arabic Scriptbased Lang. U. Hermjakob, K. Knight, H. Daum. 2008. Name translation in statistical machine translation: Learning when to transliterate. In Proc. ACL, Columbus, OH, USA, June. S. Jiampojamarn, C. Cherry, G. Kondrak. 2010. Integrating joint n-gram features into a discriminative training framework. In Proceedings of NAACL-2010, Los Angeles, CA, June. Association for Computational Linguistics. B.J. Kang, K.S. Choi. 2000. English-Korean automatic transliteration/ backtransliteration system and character alignment. In Proc. ACL, pages 17-18, Hong Kong. G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. In Proc. ACL. A. Klementiev, D. Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proc. 21st Int'l Conf Computational Linguistics and 44th Annual Meeting of ACL, pages 817-824, Sydney, Australia, July. K. Knight, J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4). P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: S. Kundu, S. Paul, S. Pal. 2018. A Deep Learning Based Approach to Transliteration. In Proc. Named Entities Workshop at ACL 2018. NH. Li, A. Kumaran, M. Zhang, V. Pervouchine. 2010b. Whitepaper of news 2010 shared task on transliteration generation. In Proc. Named Entities Workshop at ACL 2010. T. Mandl, C. Womser-Hacker. 2005. The effect of named entities on effectiveness in cross-language information retrieval evaluation. In Proc. ACM Symp. Applied Comp., pages 1059-1064.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A1 :Table A3 :</head><label>A1A3</label><figDesc>Results for the Persian to English transliteration task (T-PeEn) on EvaluationTest. Numbers in parentheses refer to the ranking of the submitted system.</figDesc><table><row><cell>SINGA SINGA Team SINGA UALB</cell><cell>NEWS18 0.6560 (5) NEWS18 0.3333 (2) Test Set Accuracy NEWS18 0.4340 (8) NEWS18 0.2450 (11)</cell><cell>0.9437 (6) 0.8426 (8) F-score 0.8587 (10) 0.6286 (9)</cell><cell>0.6560 (8) 0.3333 (12) MRR 0.4340 (13) 0.3329 (8)</cell><cell>0.5663 (8) 0.3333 (2) MAP 0.4340 (8) 0.2450 (11)</cell></row><row><cell>SINGA SINGA EDI SINGA UALB SINGA SINGA EDI SINGA UALB SINGA EDI UJUS SINGA</cell><cell>NEWS18 0.2460 (11) NEWS18 0.3153 (8) NEWS18 0.4610 (1) NEWS18 0.0260 (16) NEWS18 0.2450 (11) NEWS18 0.2460 (11) NEWS18 0.3143 (9) NEWS18 0.4560 (2) NEWS18 0.0220 (18) NEWS18 0.0070 (12) NEWS18 0.3113 (10) NEWS18 0.4560 (2) NEWS18 0.2000 (13) NEWS18 0.3110 (3)</cell><cell>0.9019 (11) 0.8417 (9) 0.9006 (1) 0.6764 (22) 0.6286 (9) 0.9019 (11) 0.8407 (11) 0.8994 (2) 0.6690 (23) 0.2646 (13) 0.8369 (17) 0.8994 (2) 0.7560 (16) 0.6093 (10)</cell><cell>0.4812 (11) 0.3153 (13) 0.5927 (1) 0.0260 (21) 0.3329 (8) 0.2460 (12) 0.4167 (6) 0.5907 (2) 0.0220 (22) 0.0070 (14) 0.3113 (14) 0.5907 (2) 0.2000 (15) 0.3788 (4)</cell><cell>0.3153 (8) 0.4610 (1) 0.0260 (16) 0.2450 (11) 0.2363 (11) 0.3143 (9) 0.4560 (2) 0.0220 (18) 0.0070 (12) 0.2060 (12) 0.3113 (10) 0.4560 (2) 0.2000 (13) 0.3110 (3)</cell></row><row><cell>SINGA UALB UJUS SINGA</cell><cell>NEWS18 0.3013 (12) NEWS18 0.4120 (3) NEWS18 0.1780 (14) NEWS18 0.3110 (3)</cell><cell>0.8377 (15) 0.8812 (5) 0.7399 (20) 0.6093 (10)</cell><cell>0.3013 (15) 0.5312 (3) 0.1780 (16) 0.3110 (9)</cell><cell>0.3013 (12) 0.4120 (3) 0.1780 (14) 0.3110 (3)</cell></row><row><cell>SINGA UALB UJUS SINGA</cell><cell>NEWS18 0.3013 (12) NEWS18 0.4080 (4) NEWS18 0.0940 (15) NEWS18 0.2910 (4)</cell><cell>0.8377 (15) 0.8840 (3) 0.6774 (21) 0.5877 (11)</cell><cell>0.3013 (15) 0.5295 (4) 0.0940 (17) 0.2910 (10)</cell><cell>0.3013 (12) 0.4080 (4) 0.0940 (15) 0.2910 (4)</cell></row><row><cell cols="5">Team EDI EDI EDI SINGA SINGA SINGA SINGA UJUS UJUS UJUS Table A2: Results for the Persian to English transliteration task (B-PeEn) on Evaluation Test. Num-Test Set Accuracy F-score MRR MAP NEWS18 0.0033 (2) 0.3590 (2) 0.0086 (2) 0.0030 (2) NEWS18 0.0022 (3) 0.3235 (4) 0.0053 (3) 0.0019 (3) NEWS18 0.0000 (4) 0.0014 (8) 0.0000 (4) 0.0000 (4) NEWS18 0.0000 (4) 0.0098 (5) 0.0000 (4) 0.0000 (4) NEWS18 0.0000 (4) 0.0077 (6) 0.0000 (4) 0.0000 (4) NEWS18 0.0000 (4) 0.0074 (7) 0.0000 (4) 0.0000 (4) NEWS18 0.0000 (4) 0.0074 (7) 0.0000 (4) 0.0000 (4) NEWS18 0.0088 (1) 0.3662 (1) 0.0088 (1) 0.0078 (1) NEWS18 0.0000 (4) 0.3573 (3) 0.0000 (4) 0.0000 (4) NEWS18 0.0000 (4) 0.3573 (3) 0.0000 (4) 0.0000 (4) bers in parentheses refer to the ranking of the submitted system. Team Test Set ACC F-score MRR MAP ref EDI NEWS1 8 0.0010 (12) 0.4817 (12) 0.0022 (13) 0.0012 (13) EDI NEWS18 0.0000 (13) 0.0014 (13) 0.0000 (14) 0.0000 (14) Team Test Set Accuracy F-score MRR SINGA NEWS18 0.0010 (26) 0.3856 (28) 0.0010 (27) 0.0010 (26) UALB NEWS18 0.4070 (5) 0.8827 (4) 0.5284 (5) 0.4070 (5) UJUS NEWS18 0.0080 (20) 0.5863 (25) 0.0080 (24) 0.0080 (20) SINGA NEWS18 0.2860 (5) 0.5836 (12) 0.2860 (11) 0.2860 (5) MAP EDI NEWS18 0.2367 (1) 0.8405 (1) 0.3291 (1) SINGA NEWS18 0.0000 (27) 0.7784 (26) 0.0000 (28) 0.0000 (27) UALB NEWS18 0.3780 (10) 0.8701 (9) 0.5093 (7) 0.3780 (10) UJUS NEWS18 0.0080 (20) 0.5088 (26) 0.0080 (24) 0.0080 (20) SINGA NEWS18 0.2730 (8) 0.6770 (6) 0.2730 (12) 0.2730 (8) 0.2367 (1) EDI NEWS18 0.2155 (2) 0.8361 (2) 0.3148 (2) UJUS NEWS18 0.2993 (13) 0.8401 (13) 0.2993 (16) 0.2993 (13) UALB NEWS18 0.3580 (12) 0.8680 (13) 0.4511 (10) 0.3580 (12) Table A8: Results for the English to Vietnamese transliteration task (T-EnVi) on Evaluation Test. SINGA NEWS18 0.2590 (10) 0.6747 (7) 0.2590 (13) 0.2590 (10) 0.2155 (2) EDI NEWS18 0.0544 (21) 0.4591 (26) 0.0687 (23) UJUS NEWS18 0.2963 (14) 0.8429 (7) 0.2963 (17) 0.2963 (14) UALB NEWS18 0.3400 (14) 0.8714 (7) 0.4746 (8) 0.3400 (14) Numbers in parentheses refer to the ranking of the submitted system. Table A10: Results for the English to Korean Hangul transliteration task (T-EnKo) on Evaluation 0.0544 (21) EDI NEWS18 0.0504 (22) 0.4577 (27) 0.0658 (24) UJUS NEWS18 0.2923 (16) 0.8408 (10) 0.2923 (19) 0.2923 (16) UALB NEWS18 0.3350 (15) 0.8701 (10) 0.4698 (9) 0.3350 (15) Test. Numbers in parentheses refer to the ranking of the submitted system. 0.0504 (22) UALB NEWS18 0.2135 (3) 0.8348 (3) 0.3078 (3) UJUS NEWS18 0.2833 (17) 0.8359 (18) 0.2833 (20) 0.2833 (17) UALB NEWS18 0.3270 (17) 0.8635 (14) 0.3270 (20) 0.3270 (17) Team Test Set Accuracy F-score MRR MAP 0.2135 (3) UALB NEWS18 0.2105 (4) 0.8314 (5) 0.3016 (5) 0.2105 (4) UALB NEWS18 0.2064 (5) 0.8332 (4) 0.3019 (4) 0.2064 (5) UALB NEWS18 0.1974 (6) 0.8271 (7) 0.2873 (6) 0.1974 (6) UALB NEWS18 0.1934 (7) 0.8304 (6) 0.2638 (9) UJUS NEWS18 0.2773 (18) 0.8347 (19) 0.2773 (21) 0.2773 (18) UJUS NEWS18 0.2553 (20) 0.8195 (24) 0.2553 (22) 0.2553 (20) UJUS NEWS18 0.2502 (22) 0.8275 (22) 0.2502 (23) 0.2502 (22) UJUS NEWS18 0.2472 (23) 0.8223 (23) 0.2472 (24) 0.2472 (23) UALB NEWS18 0.3270 (17) 0.8635 (14) 0.3270 (20) 0.3270 (17) SINGA NEWS18 0.4070 (5) 0.8793 (6) 0.4070 (11) 0.4070 (5) SINGA NEWS18 0.4060 (6) 0.8682 (12) 0.4060 (12) EDI NEWS18 0.1670 (1) 0.7740 (4) 0.2547 (2) 0.1670 (2) EDI NEWS18 0.1650 (3) 0.7728 (6) 0.2533 (3) Team Test Set ACC F-score MRR MAP ref 0.1650 (4) EDI NEWS18 0.1640 (5) 0.7760 (1) 0.2487 (4) 0.1640 (5) UALB NEWS18 0.3904 (1) 0.8098 (1) 0.5157 (1) 0.3893 (1) 0.4060 (6) SINGA NEWS18 0.3950 (7) 0.8684 (11) 0.5126 (6) 0.3950 (7) EDI NEWS18 0.1610 (6) 0.7712 (8) 0.2479 (5) UALB NEWS18 0.3844 (2) 0.8078 (3) 0.5116 (2) 0.3825 (2) 0.1610 (6) 0.1934 (7) UALB NEWS18 0.1853 (8) 0.8175 (11) 0.2700 (8) UJUS NEWS18 0.2312 (24) 0.7982 (25) 0.2312 (25) 0.2312 (24) SINGA NEWS18 0.3950 (7) 0.8684 (11) 0.3950 (13) 0.3950 (7) UALB NEWS18 0.1660 (2) 0.7740 (5) 0.2352 (6) 0.1660 (3) UALB NEWS18 0.3814 (3) 0.8093 (2) 0.5110 (3) 0.3815 (3) 0.1853 (8) UALB NEWS18 0.1813 (11) 0.8217 (9) 0.1813 (13) Table A5: Results for the English to Hindi transliteration task (M-EnHi) on Evaluation Test. Numbers SINGA NEWS18 0.3930 (8) 0.8626 (16) 0.3930 (14) 0.3930 (8) UALB NEWS18 0.1660 (2) 0.7654 (9) 0.2310 (9) 0.1660 (3) UALB NEWS18 0.3684 (4) 0.8029 (5) 0.4979 (4) 0.3688 (4) 0.1813 (11) UALB NEWS18 0.1793 (12) 0.8159 (13) 0.2586 (10) in parentheses refer to the ranking of the submitted system. SINGA NEWS18 0.3820 (9) 0.8713 (8) 0.3820 (15) 0.3820 (9) UALB NEWS18 0.1640 (5) 0.7712 (7) 0.2340 (7) 0.1640 (5) UALB NEWS18 0.3644 (5) 0.8030 (4) 0.4977 (5) 0.3625 (5) 0.1793 (12) SINGA NEWS18 0.1833 (9) 0.8260 (8) 0.1833 (11) 0.1833 (9) SINGA NEWS18 0.1833 (9) 0.8260 (8) 0.1833 (11) 0.1833 (9) SINGA NEWS18 0.1833 (9) 0.8173 (12) 0.1833 (11) 0.1833 (9) SINGA NEWS18 0.1823 (10) 0.8126 (14) 0.2735 (7) Team Test Set Accuracy F-score MRR MAP EDI NEWS18 0.3404 (1) 0.8673 (1) 0.4588 (1) 0.3404 (1) EDI NEWS18 0.3343 (2) 0.8638 (2) 0.4504 (2) 0.3343 (2) SINGA NEWS18 0.0010 (20) 0.3629 (24) 0.0010 (23) 0.0010 (20) SINGA NEWS18 0.0000 (21) 0.8215 (22) 0.0000 (24) 0.0000 (21) UJUS NEWS18 0.3820 (9) 0.8618 (18) 0.3820 (15) UALB NEWS18 0.1610 (6) 0.7745 (2) 0.2335 (8) 0.1610 (6) UALB NEWS18 0.1600 (7) 0.7606 (13) 0.2306 (10) UALB NEWS18 0.3594 (6) 0.8009 (7) 0.4924 (6) 0.3583 (6) 0.1600 (7) UALB NEWS18 0.1550 (8) 0.7596 (15) 0.1550 (16) 0.1550 (8) UALB NEWS18 0.3504 (7) 0.8024 (6) 0.4897 (7) 0.3490 (7) 0.3820 (9) UJUS NEWS18 0.3780 (10) 0.8621 (17) 0.3780 (16) 0.3780 (10) UALB NEWS18 0.1530 (9) 0.7627 (10) 0.2242 (11) 0.1530 (9) UALB NEWS18 0.3463 (8) 0.7936 (8) 0.3463 (11) 0.3428 (8) 0.1823 (10) SINGA NEWS18 0.1813 (11) 0.7996 (20) 0.1813 (13) EDI NEWS18 0.0251 (23) 0.4087 (28) 0.0361 (25) 0.0251 (23) UJUS NEWS18 0.3760 (11) 0.8606 (19) 0.3760 (17) 0.3760 (11) UALB NEWS18 0.1480 (10) 0.7615 (11) 0.2000 (15) 0.1480 (10) UALB NEWS18 0.3293 (11) 0.7803 (15) 0.4258 (10) 0.3296 (11) 0.1813 (11) SINGA NEWS18 0.1601 (16) 0.8176 (10) 0.1601 (18) EDI NEWS18 0.0221 (24) 0.4091 (27) 0.0342 (26) 0.0221 (24) UJUS NEWS18 0.3430 (13) 0.8631 (15) 0.3430 (18) 0.3430 (13) UALB NEWS18 0.1450 (11) 0.7586 (17) 0.2177 (12) 0.1450 (11) UALB NEWS18 0.3203 (12) 0.7828 (12) 0.4602 (8) 0.3209 (13) 0.1601 (16) SINGA NEWS18 0.1581 (17) 0.7930 (23) 0.1581 (19) 0.1581 (17) SINGA NEWS18 0.0000 (23) 0.7668 (24) 0.0000 (25) 0.0000 (23) UJUS NEWS18 0.1823 (10) 0.8076 (17) 0.1823 (12) 0.1823 (10) UJUS NEWS18 0.1793 (12) 0.8100 (16) 0.1793 (14) UALB NEWS18 0.3042 (3) 0.8569 (3) 0.4198 (3) 0.3042 (3) UALB NEWS18 0.3022 (4) 0.8563 (4) 0.4152 (4) 0.3022 (4) UALB NEWS18 0.2912 (5) 0.8528 (5) 0.4077 (5) 0.2912 (5) UALB NEWS18 0.2831 (8) 0.8486 (6) 0.4043 (6) 0.2831 (8) UJUS NEWS18 0.3340 (16) 0.8540 (20) 0.3340 (19) 0.3340 (16) UJUS NEWS18 0.2550 (18) 0.8291 (21) 0.2550 (21) 0.2550 (18) UJUS NEWS18 0.1180 (19) 0.7507 (23) 0.1180 (22) 0.1180 (19) Table A7: Results for the English to Bangla (Bengali) transliteration task (M-EnBa) on Evaluation UALB NEWS18 0.1450 (11) 0.7578 (19) 0.1450 (17) 0.1450 (11) UALB NEWS18 0.1400 (16) 0.7590 (16) 0.2076 (14) UALB NEWS18 0.3033 (15) 0.7806 (14) 0.3033 (16) 0.3003 (16) 0.1400 (16) SINGA NEWS18 0.1430 (13) 0.7578 (18) 0.2115 (13) SINGA NEWS18 0.3393 (9) 0.7829 (11) 0.3393 (12) 0.3363 (9) 0.1430 (13) SINGA NEWS18 0.1430 (13) 0.7578 (18) 0.1430 (19) SINGA NEWS18 0.3313 (10) 0.7851 (9) 0.3313 (13) 0.3286 (12) 0.1430 (13) 0.1793 (12) UJUS NEWS18 0.1702 (13) 0.8039 (18) 0.1702 (15) UALB NEWS18 0.2510 (15) 0.8391 (11) 0.3433 (10) 0.2510 (15) Test. Numbers in parentheses refer to the ranking of the submitted system. SINGA NEWS18 0.1420 (14) 0.7542 (21) 0.1420 (20) 0.1420 (14) SINGA NEWS18 0.3313 (10) 0.7848 (10) 0.4536 (9) 0.3322 (10) 0.1702 (13) UJUS NEWS18 0.1641 (14) 0.8109 (15) 0.1641 (16) UALB NEWS18 0.2369 (16) 0.8405 (10) 0.3691 (8) 0.2369 (16) SINGA NEWS18 0.1410 (15) 0.7604 (14) 0.1410 (21) 0.1410 (15) SINGA NEWS18 0.3183 (13) 0.7807 (13) 0.3183 (14) 0.3153 (14) 0.1641 (14) UJUS NEWS18 0.1631 (15) 0.7954 (22) 0.1631 (17) UALB NEWS18 0.2339 (17) 0.8385 (12) 0.2339 (20) 0.2339 (17) Team Test Set Accuracy F-score MRR MAP SINGA NEWS18 0.1390 (17) 0.7511 (22) 0.1390 (22) 0.1390 (17) SINGA NEWS18 0.3043 (14) 0.7745 (16) 0.3043 (15) 0.3008 (15) 0.1631 (15) UJUS NEWS18 0.1541 (18) 0.8006 (19) 0.1541 (20) 0.1541 (18) UJUS NEWS18 0.1460 (19) 0.7995 (21) 0.1460 (21) UJUS NEWS18 0.1339 (20) 0.7591 (25) 0.1339 (22) SINGA NEWS18 0.2781 (9) 0.8279 (20) 0.2781 (13) 0.2781 (9) EDI NEWS18 0.5020 (1) 0.8893 (1) 0.6046 (1) 0.5020 (1) UJUS NEWS18 0.1300 (20) 0.7449 (25) 0.1300 (25) 0.1300 (20) in parentheses refer to the ranking of the submitted system. SINGA NEWS18 0.2841 (7) 0.8439 (8) 0.2841 (12) 0.2841 (7) EDI NEWS18 0.5020 (1) 0.8893 (1) 0.6046 (1) 0.5020 (1) UJUS NEWS18 0.1350 (19) 0.7484 (23) 0.1350 (24) 0.1350 (19) Table A4: Results for the English to Tamil transliteration task (M-EnTa) on Evaluation Test. Numbers SINGA NEWS18 0.2841 (7) 0.8439 (8) 0.2841 (12) 0.2841 (7) UQAM NEWS18 0.0120 (19) 0.7423 (19) 0.0195 (23) 0.0120 (19) UJUS NEWS18 0.1440 (12) 0.7551 (20) 0.1440 (18) 0.1440 (12) 0.1339 (20) SINGA NEWS18 0.2851 (6) 0.8422 (9) 0.3899 (7) 0.2851 (6) UQAM NEWS18 0.0240 (17) 0.7480 (18) 0.0309 (19) 0.0240 (17) UJUS NEWS18 0.1450 (11) 0.7610 (12) 0.1450 (17) 0.1450 (11) 0.1460 (19) UALB NEWS18 0.2199 (19) 0.8362 (14) 0.3502 (9) 0.2199 (19) SINGA NEWS18 0.2851 (6) 0.8453 (7) 0.2851 (11) 0.2851 (6) UQAM NEWS18 0.0260 (16) 0.7831 (15) 0.0311 (18) 0.0260 (16) UQAM NEWS18 0.0240 (17) 0.7502 (17) 0.0292 (20) 0.0240 (17) SINGA NEWS18 0.1380 (18) 0.7481 (24) 0.1380 (23) 0.1380 (18) SINGA NEWS18 0.0380 (26) 0.4580 (29) 0.0380 (31) 0.0380 (26) SINGA NEWS18 0.2913 (16) 0.7737 (17) 0.2913 (17) 0.2880 (17)</cell></row><row><cell cols="5">EDI UALB UALB UALB UALB UALB UALB UALB UALB SINGA SINGA Team SINGA EDI UJUS EDI SINGA EDI UJUS EDI SINGA UALB UJUS EDI UJUS UALB UJUS UALB UJUS UALB UJUS UALB UJUS UALB Table A9: Results for the English to Thai transliteration task (T-EnTh) on Evaluation Test. Numbers in NEWS18 0.0000 (13) 0.0000 (14) 0.0000 (14) 0.0000 (14) NEWS1 8 0.6880 (1) 0.9515 (1) 0.7755 (3) 0.6081 (1) NEWS18 0.6820 (2) 0.9498 (3) 0.7777 (2) 0.6050 (2) NEWS18 0.6800 (3) 0.9508 (2) 0.7780 (1) 0.6049 (3) NEWS18 0.6450 (6) 0.9462 (5) 0.7476 (6) 0.5786 (4) NEWS18 0.6440 (7) 0.9429 (7) 0.7546 (4) 0.5748 (5) NEWS18 0.6380 (8) 0.9420 (8) 0.7516 (5) 0.5721 (6) NEWS18 0.5070 (9) 0.9174 (9) 0.5070 (10) 0.4368 (9) NEWS18 0.3930 (10) 0.9094 (10) 0.5075 (9) 0.3486 (10) NEWS1 8 0.6580 (4) 0.9476 (4) 0.6580 (7) 0.5701 (7) NEWS18 0.6560 (5) 0.9437 (6) 0.6560 (8) 0.5663 (8) Test Set Accuracy F-score MRR NEWS18 0.2711 (10) 0.8313 (18) 0.2711 (14) 0.2711 (10) NEWS18 0.4940 (2) 0.8858 (3) 0.5935 (3) 0.4940 (2) NEWS18 0.1270 (21) 0.7383 (26) 0.1270 (26) 0.1270 (21) MAP NEWS18 0.3333 (2) 0.8515 (1) 0.4455 (1) NEWS18 0.2691 (11) 0.8362 (13) 0.2691 (15) 0.2691 (11) NEWS18 0.4900 (3) 0.8885 (2) 0.5967 (2) 0.4900 (3) NEWS18 0.1080 (22) 0.7164 (27) 0.1080 (27) 0.1080 (22) 0.3333 (2) NEWS18 0.3283 (3) 0.8501 (2) 0.4426 (2) NEWS18 0.0000 (25) 0.7809 (26) 0.0000 (27) 0.0000 (25) NEWS18 0.4540 (5) 0.8719 (5) 0.5447 (4) 0.4540 (5) NEWS18 0.0760 (23) 0.6632 (28) 0.0760 (28) 0.0760 (23) 0.3283 (3) NEWS18 0.0400 (25) 0.4488 (27) 0.0568 (26) NEWS18 0.2671 (12) 0.8298 (19) 0.2671 (16) 0.2671 (12) NEWS18 0.4360 (7) 0.8641 (7) 0.5345 (6) 0.4360 (7) NEWS18 0.0750 (24) 0.3984 (30) 0.0750 (29) 0.0750 (24) 0.0400 (25) NEWS18 0.3243 (4) 0.8472 (4) 0.4287 (4) NEWS18 0.2651 (13) 0.8338 (17) 0.2651 (17) 0.2651 (13) NEWS18 0.4280 (9) 0.8605 (8) 0.5266 (7) 0.4280 (9) NEWS18 0.0700 (25) 0.3947 (31) 0.0700 (30) 0.0700 (25) 0.3243 (4) NEWS18 0.3233 (5) 0.8472 (5) 0.3935 (8) NEWS18 0.2641 (14) 0.8345 (16) 0.2641 (18) 0.2641 (14) NEWS18 0.4200 (10) 0.8592 (9) 0.5228 (8) 0.4200 (10) 0.3233 (5) UALB NEWS18 0.3223 (6) 0.8474 (3) 0.4291 (3) UJUS NEWS18 0.2369 (16) 0.8192 (21) 0.2369 (19) 0.2369 (16) UALB NEWS18 0.3960 (11) 0.8533 (12) 0.4952 (9) 0.3960 (11) parentheses refer to the ranking of the submitted system. 0.3223 (6) UALB NEWS18 0.3193 (7) 0.8438 (6) 0.4235 (5) UJUS NEWS18 0.2239 (18) 0.8087 (24) 0.2239 (21) 0.2239 (18) UALB NEWS18 0.3960 (11) 0.8525 (13) 0.4897 (10) 0.3960 (11) 0.3193 (7) UALB NEWS18 0.3033 (11) 0.8374 (16) 0.4083 (7) UJUS NEWS18 0.2179 (20) 0.8346 (15) 0.2179 (22) 0.2179 (20) UALB NEWS18 0.3400 (12) 0.8448 (14) 0.4047 (14) 0.3400 (12) Team Test Set Accuracy F-score MRR MAP 0.3033 (11) UALB NEWS18 0.2943 (15) 0.8407 (12) 0.2943 (18) UJUS NEWS18 0.2108 (21) 0.8169 (23) 0.2108 (23) 0.2108 (21) UALB NEWS18 0.0080 (20) 0.6021 (24) 0.0080 (24) 0.0080 (20) UALB NEWS18 0.3400 (1) 0.7113 (1) 0.4301 (1) 0.3400 (1) 0.2943 (15) UALB NEWS18 0.2683 (19) 0.8347 (20) 0.3873 (9) UJUS NEWS18 0.1867 (22) 0.8169 (22) 0.1867 (24) 0.1867 (22) SINGA NEWS18 0.4580 (4) 0.8583 (11) 0.4580 (11) 0.4580 (4) UALB NEWS18 0.3400 (1) 0.7110 (2) 0.4273 (2) 0.3400 (1) 0.2683 (19) UALB NEWS18 0.2543 (21) 0.8290 (21) 0.3741 (10) UJUS NEWS18 0.1867 (22) 0.7911 (25) 0.1867 (24) 0.1867 (22) SINGA NEWS18 0.4500 (6) 0.8730 (4) 0.4500 (12) 0.4500 (6) UALB NEWS18 0.3190 (2) 0.6954 (3) 0.4106 (3) 0.3190 (2) 0.2543 (21) UALB NEWS18 0.0000 (27) 0.0509 (29) 0.0000 (28) Table A6: Results for the English to Kannada transliteration task (M-EnKa) on Evaluation Test. Num-SINGA NEWS18 0.4500 (6) 0.8730 (4) 0.4500 (12) 0.4500 (6) UALB NEWS18 0.2790 (6) 0.6775 (5) 0.3688 (5) 0.2790 (6) 0.0000 (27) SINGA NEWS18 0.3343 (1) 0.8383 (14) 0.3343 (11) bers in parentheses refer to the ranking of the submitted system. SINGA NEWS18 0.4500 (6) 0.8658 (6) 0.5377 (5) 0.4500 (6) UALB NEWS18 0.2780 (7) 0.6822 (4) 0.3669 (6) 0.2780 (7) 0.3343 (1) SINGA NEWS18 0.4500 (6) 0.8658 (6) 0.4500 (12) 0.4500 (6) UALB NEWS18 0.2680 (9) 0.6672 (8) 0.3368 (7) 0.2680 (9)</cell></row></table><note>Results for the English to Persian transliteration task (T-EnPe) on EvaluationTest. Numbers in parentheses refer to the ranking of the submitted system.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A11 :</head><label>A11</label><figDesc>Results for the English to Japanese Katakana transliteration task (T-EnJa) on EvaluationTest. Numbers in parentheses refer to the ranking of the submitted system. Results for the Thai to English back-transliteration task (B-ThEn) on Evaluation Test. Numbers in parentheses refer to the ranking of the submitted system. Results for the English to Japanese Kanji back-transliteration task (B-JnJk) on Evaluation Test. Numbers in parentheses refer to the ranking of the submitted system.</figDesc><table><row><cell>SINGA UJUS UJUS</cell><cell>NEWS18 NEWS18 NEWS18</cell><cell>0.1759 (3) 0.2352 (15) 0.0610 (14)</cell><cell>0.8000 (4) 0.8005 (17) 0.7129 (18)</cell><cell>0.1759 (11) 0.2352 (18) 0.0610 (22)</cell><cell>0.1721 (4) 0.2350 (15) 0.0610 (14)</cell></row><row><cell>SINGA UJUS UJUS</cell><cell>NEWS18 NEWS18 NEWS18</cell><cell>0.1683 (5) 0.2289 (17) 0.0610 (14)</cell><cell>0.7986 (5) 0.7987 (18) 0.6788 (26)</cell><cell>0.1683 (13) 0.2289 (20) 0.0610 (22)</cell><cell>0.1649 (9) 0.2289 (17) 0.0610 (14)</cell></row><row><cell>SINGA UJUS UJUS</cell><cell>NEWS18 NEWS18 NEWS18</cell><cell>0.1683 (5) 0.2212 (19) 0.0593 (15)</cell><cell>0.7964 (10) 0.7906 (19) 0.6945 (23)</cell><cell>0.1683 (13) 0.2212 (22) 0.0593 (23)</cell><cell>0.1654 (8) 0.2210 (20) 0.0593 (15)</cell></row><row><cell>SINGA UJUS UJUS</cell><cell>NEWS18 NEWS18 NEWS18</cell><cell>0.1644 (7) 0.2114 (20) 0.0508 (16)</cell><cell>0.8002 (3) 0.7881 (20) 0.6818 (25)</cell><cell>0.2484 (8) 0.2114 (23) 0.0508 (24)</cell><cell>0.1620 (11) 0.2113 (21) 0.0508 (16)</cell></row><row><cell>SINGA UJUS</cell><cell>NEWS18 NEWS18</cell><cell>0.1644 (7) 0.1989 (22)</cell><cell>0.8002 (3) 0.7776 (21)</cell><cell>0.1644 (15) 0.1989 (24)</cell><cell>0.1611 (13) 0.1987 (23)</cell></row><row><cell>SINGA UJUS</cell><cell>NEWS18 NEWS18</cell><cell>0.0000 (15) 0.1689 (23)</cell><cell>0.7373 (24) 0.7588 (23)</cell><cell>0.0000 (22) 0.1689 (26)</cell><cell>0.0000 (22) 0.1689 (24)</cell></row><row><cell>UJUS UJUS</cell><cell>NEWS18 NEWS18</cell><cell>0.1663 (6) 0.0984 (25)</cell><cell>0.7884 (15) 0.2960 (28)</cell><cell>0.1663 (14) 0.0984 (27)</cell><cell>0.1630 (10) 0.0982 (26)</cell></row><row><cell cols="6">UJUS UJUS UJUS UJUS Table A15: Team UJUS UALB UJUS UALB UJUS UALB UJUS UALB Table A12: Results for the English to Hebrew transliteration task (T-EnHe) on Evaluation Test. Num-NEWS18 0.1644 (7) 0.7825 (17) 0.1644 (15) 0.1611 (13) NEWS18 0.1644 (7) 0.7812 (18) 0.1644 (15) 0.1616 (12) NEWS18 0.1625 (8) 0.7843 (16) 0.1625 (16) 0.1592 (14) NEWS18 0.1606 (9) 0.7789 (19) 0.1606 (17) Test Set ACC F-score MRR MAP ref 0.1573 (16) NEWS18 0.1453 (11) 0.7519 (23) 0.1453 (18) NEWS18 0.5930 (1) 0.7678 (1) 0.6669 (1) 0.3740 (3) 0.1424 (18) NEWS18 0.1377 (12) 0.7560 (22) 0.1377 (19) NEWS18 0.5690 (2) 0.7543 (2) 0.6492 (2) 0.3840 (1) 0.1348 (19) NEWS18 0.1300 (13) 0.7746 (20) 0.1300 (20) NEWS18 0.5650 (3) 0.7529 (3) 0.6459 (3) 0.3813 (2) 0.1286 (20) NEWS18 0.1205 (14) 0.7691 (21) 0.1205 (21) NEWS18 0.5530 (4) 0.7388 (4) 0.6399 (4) 0.3546 (4) 0.1185 (21) UALB NEWS18 0.4660 (5) 0.6919 (5) 0.4660 (6) 0.1941 (6)</cell></row><row><cell cols="4">bers in parentheses refer to the ranking of the submitted system UALB NEWS18 0.3850 (6) 0.6622 (6)</cell><cell>0.4837 (5)</cell><cell>0.2442 (5)</cell></row><row><cell>Team EDI EDI UALB UALB UALB UALB UALB UALB UALB UALB UALB SINGA Team WIPO EDI Table A16: Team EDI EDI EDI EDI UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB UALB SINGA UALB SINGA UALB SINGA SINGA SINGA SINGA SINGA SINGA SINGA SINGA UJUS SINGA UJUS SINGA UJUS SINGA UJUS UJUS UJUS UJUS UJUS UJUS UJUS UJUS UJUS UJUS UJUS</cell><cell>Test Set NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 Test Set NEWS18 NEWS18 Test Set NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18 NEWS18</cell><cell>ACC 0.1836 (1) 0.1778 (2) 0.1702 (4) 0.1702 (4) 0.1683 (5) 0.1683 (5) 0.1683 (5) 0.1625 (8) 0.1606 (9) 0.1530 (10) 0.1530 (10) 0.1778 (2) ACC 0.2820 (4) 0.3040 (1) ACC 0.3030 (2) 0.1525 (2) 0.3010 (3) 0.1068 (3) 0.2820 (4) 0.1729 (1) 0.2750 (5) 0.0915 (5) 0.2750 (5) 0.0881 (6) 0.2710 (6) 0.0864 (7) 0.2600 (12) 0.0864 (7) 0.2560 (13) 0.0780 (9) 0.2460 (14) 0.0780 (9) 0.2280 (18) 0.0678 (12) 0.2750 (5) 0.0644 (13) 0.2700 (7) 0.0644 (13) 0.2700 (7) 0.0949 (4) 0.2670 (8) 0.0949 (4) 0.2630 (9) 0.0915 (5) 0.2620 (10) 0.0915 (5) 0.2610 (11) 0.0864 (7) 0.2610 (11) 0.0678 (12) 0.2440 (15) 0.0000 (17) 0.2400 (16) 0.0831 (8) 0.2370 (17) 0.0780 (9) 0.1870 (19) 0.0746 (10) 0.1590 (20) 0.0712 (11) 0.1540 (21) 0.0678 (12) 0.0644 (13)</cell><cell>F-score 0.8042 (1) 0.8033 (2) 0.7983 (6) 0.7983 (6) 0.7952 (12) 0.7946 (13) 0.7940 (14) 0.7965 (9) 0.7969 (8) 0.7962 (11) 0.7962 (11) 0.7982 (7) F-score 0.6686 (4) 0.6791 (1) F-score 0.6776 (3) 0.7532 (1) 0.6785 (2) 0.7454 (2) 0.6680 (5) 0.7240 (10) 0.6634 (6) 0.7316 (8) 0.6634 (6) 0.7337 (4) 0.6627 (7) 0.7331 (5) 0.6516 (10) 0.7319 (6) 0.6513 (12) 0.7316 (7) 0.6435 (19) 0.7300 (9) 0.6288 (21) 0.7234 (11) 0.6512 (13) 0.7194 (13) 0.6515 (11) 0.7129 (17) 0.6515 (11) 0.7135 (16) 0.6461 (17) 0.7135 (16) 0.6489 (15) 0.7339 (3) 0.6509 (14) 0.6757 (27) 0.6603 (8) 0.6730 (28) 0.6566 (9) 0.7205 (12) 0.6443 (18) 0.6819 (24) 0.6475 (16) 0.7157 (14) 0.6358 (20) 0.7147 (15) 0.6086 (22) 0.7122 (19) 0.3497 (24) 0.7026 (21) 0.3495 (25) 0.7031 (20) 0.7006 (22)</cell><cell>MRR 0.2855 (1) 0.2776 (2) 0.1702 (12) 0.1702 (12) 0.2741 (3) 0.2600 (6) 0.2555 (7) 0.2627 (5) 0.2636 (4) 0.2211 (9) 0.2211 (9) 0.1778 (10) MRR 0.4040 (4) 0.4364 (2) MRR 0.4267 (3) 0.2306 (1) 0.4383 (1) 0.1803 (3) 0.3854 (5) 0.2181 (2) 0.3771 (6) 0.0915 (14) 0.3771 (6) 0.1505 (5) 0.2710 (12) 0.1498 (6) 0.3664 (8) 0.1494 (7) 0.3646 (9) 0.1477 (8) 0.3108 (10) 0.1436 (9) 0.2280 (21) 0.1148 (11) 0.2750 (11) 0.1261 (10) 0.3736 (7) 0.1035 (12) 0.2700 (13) 0.1560 (4) 0.2670 (14) 0.0949 (13) 0.2630 (15) 0.0915 (14) 0.2620 (16) 0.0915 (14) 0.2610 (17) 0.0864 (15) 0.2610 (17) 0.0678 (20) 0.2440 (18) 0.0000 (25) 0.2400 (19) 0.0831 (16) 0.2370 (20) 0.0780 (17) 0.1870 (22) 0.0746 (18) 0.1590 (23) 0.0712 (19) 0.1540 (24) 0.0678 (20) 0.0644 (21)</cell><cell>MAP ref 0.1807 (1) 0.1750 (2) 0.1663 (6) 0.1663 (6) 0.1673 (5) 0.1659 (7) 0.1659 (7) 0.1611 (13) 0.1587 (15) 0.1501 (17) 0.1501 (17) 0.1740 (3) MAP ref 0.2820 (4) MAP ref 0.3040 (1) 0.1521 (2) 0.3030 (2) 0.1068 (3) 0.3010 (3) 0.1725 (1) 0.2820 (4) 0.0915 (5) 0.2750 (5) 0.0881 (6) 0.2750 (5) 0.0864 (7) 0.2710 (6) 0.0864 (7) 0.2600 (12) 0.0780 (9) 0.2560 (13) 0.0780 (9) 0.2460 (14) 0.0678 (12) 0.2280 (18) 0.0644 (13) 0.2750 (5) 0.0644 (13) 0.2700 (7) 0.0949 (4) 0.2700 (7) 0.0949 (4) 0.2670 (8) 0.0915 (5) 0.2630 (9) 0.0915 (5) 0.2620 (10) 0.0864 (7) 0.2610 (11) 0.0678 (12) 0.2610 (11) 0.0000 (17) 0.2440 (15) 0.0831 (8) 0.2400 (16) 0.0780 (9) 0.2370 (17) 0.0746 (10) 0.1870 (19) 0.0712 (11) 0.1590 (20) 0.0678 (12) 0.1540 (21) 0.0644 (13)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A17 :</head><label>A17</label><figDesc>Results for the Hebrew to English back-transliteration task (B-HeEn) on Evaluation Test. Numbers in parentheses refer to the ranking of the submitted system.</figDesc><table><row><cell>SINGA</cell><cell>NEWS18</cell><cell>0.1750 (14)</cell><cell>0.7850 (16)</cell><cell>0.1750 (16)</cell><cell>0.1750 (14)</cell></row><row><cell>SINGA</cell><cell>NEWS18</cell><cell>0.0100 (25)</cell><cell>0.3440 (26)</cell><cell>0.0100 (26)</cell><cell>0.0100 (25)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1910 (10)</cell><cell>0.8003 (9)</cell><cell>0.1910 (11)</cell><cell>0.1910 (10)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1900 (11)</cell><cell>0.8014 (7)</cell><cell>0.1900 (13)</cell><cell>0.1900 (11)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1820 (12)</cell><cell>0.7962 (10)</cell><cell>0.1820 (14)</cell><cell>0.1820 (12)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1600 (15)</cell><cell>0.7849 (17)</cell><cell>0.1600 (17)</cell><cell>0.1600 (15)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1540 (16)</cell><cell>0.7875 (13)</cell><cell>0.1540 (18)</cell><cell>0.1540 (16)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1520 (17)</cell><cell>0.7850 (15)</cell><cell>0.1520 (19)</cell><cell>0.1520 (17)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1490 (18)</cell><cell>0.7879 (12)</cell><cell>0.1490 (20)</cell><cell>0.1490 (18)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1470 (19)</cell><cell>0.7857 (14)</cell><cell>0.1470 (21)</cell><cell>0.1470 (19)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1330 (20)</cell><cell>0.7656 (22)</cell><cell>0.1330 (23)</cell><cell>0.1330 (20)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1280 (21)</cell><cell>0.7670 (21)</cell><cell>0.1280 (24)</cell><cell>0.1280 (21)</cell></row><row><cell>UJUS</cell><cell>NEWS18</cell><cell>0.1120 (23)</cell><cell>0.7736 (19)</cell><cell>0.1120 (25)</cell><cell>0.1120 (23)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A19 :</head><label>A19</label><figDesc>Results for the Chinese to English back-transliteration task (B-ChEn) on Evaluation Test.Numbers in parentheses refer to the ranking of the submitted system.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The organizers of the NEWS 2018 Shared Task would like to thank the Institute for Infocomm Research (Singapore), National University of Singapore, Artificial Intelligence Laboratory at the Ho Chi Minh City University of Science (AILab, VNU-HCMUS, Vietnam), Microsoft Research India, the Computer Science &amp; Engineering Department of Jadavpur University (India), the CJK Institute (Japan), the National Electronics and Computer Technology Center (NECTEC, Thailand) and Sarvnaz Karim (RMIT, Australia) for providing the corpora and technical support. Without those, the Shared Task would not be possible. In addition, we want to thank Grandee Lee and Snigdha Singhania for their help and support with CodaLab and the baseline systems, respectively. We also want to thank all programme committee members for their valuable comments that improved the quality of the shared task papers and, finally, we wish to thank all participants for their active participation, which have made again the NEWS 2018 edition of the Shared Task on Named Entity Transliteration a successful competition.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UJUS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine transliteration of names in arabic text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-2002Workshop: Computational Apporaches to Semitic Languages</title>
				<meeting>ACL-2002Workshop: Computational Apporaches to Semitic Languages<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generate phonetic cognates to handle name entities in English-Chinese cross-language spoken document retrieval</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
				<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Msri</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/india" />
		<title level="m">Microsoft Research India</title>
				<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of Assorted Models of Transliteration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Hauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyeuan</forename><surname>Rashed Rubby Riyadh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregorz</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Named Entities Workshop at ACL</title>
				<meeting>Named Entities Workshop at ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An English-Korean transliteration model using pronunciation and contextual rules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
				<meeting>COLING<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness &amp; Correlation&quot; (PDF)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Substringbased transliteration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Annual Meeting of the ACL</title>
				<meeting>45th Annual Meeting of the ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="944" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical Machine Transliteration Baselines for NEWS 2018</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Named Entities Workshop at ACL</title>
				<meeting>Named Entities Workshop at ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named entity transliteration with comparable corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int&apos;l Conf Computational Linguistics and 44th Annual Meeting of ACL</title>
				<meeting>21st Int&apos;l Conf Computational Linguistics and 44th Annual Meeting of ACL<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">They are out there, if you know where to look&quot;: Mining transliterations of OOV query terms for cross-language information retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saravanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LNCS: Advances in Information Retrieval</title>
				<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5478</biblScope>
			<biblScope unit="page" from="437" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transliteration of proper names in cross-lingual information retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Virga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL MLNER</title>
				<meeting>ACL MLNER<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<title level="m">Proceedings of the 8th Text Retrieval Conference. TREC-8 Question Answering Track Report</title>
				<meeting>the 8th Text Retrieval Conference. TREC-8 Question Answering Track Report</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic English-Chinese name transliteration for development of multilingual resources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1352" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative methods for transliteration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="612" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Whitepaper of news 2011 shared task on machine transliteration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Named Entities Workshop at IJCNLP</title>
				<meeting>Named Entities Workshop at IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Report of news 2011 machine transliteration shared task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Named Entities Workshop at IJCNLP</title>
				<meeting>Named Entities Workshop at IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Report of NEWS 2012 Machine Transliteration Shared Task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
