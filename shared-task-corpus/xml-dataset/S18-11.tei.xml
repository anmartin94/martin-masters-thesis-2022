<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University Saarbrücken</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University Saarbrücken</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University Saarbrücken</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University Saarbrücken</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University Saarbrücken</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report summarizes the results of the Se-mEval 2018 task on machine comprehension using commonsense knowledge. For this machine comprehension task, we created a new corpus, MCScript. It contains a high number of questions that require commonsense knowledge for finding the correct answer. 11 teams from 4 different countries participated in this shared task, most of them used neural approaches. The best performing system achieves an accuracy of 83.95%, outperforming the baselines by a large margin, but still far from the human upper bound, which was found to be at 98%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developing algorithms for understanding natural language is not trivial. Natural language comes with its own complexity and inherent ambiguities. Ambiguities can occur, for example, at the level of word meaning, syntactic structure, or semantic interpretation. Traditionally, Natural Language Understanding (NLU) systems have resolved ambiguities using information from the textual context (e.g. neighboring words and sentences), for example via distributional methods <ref type="bibr" target="#b13">(Lenci, 2008)</ref>. However, many times context may be absent or may lack sufficient information to resolve the ambiguity. In such cases, it would be beneficial to include commonsense knowledge about the world in an NLU system. For example, consider example (1).</p><p>(1) The waitress brought Rachel's order. She ate the food with great pleasure.</p><p>Looking at the example in isolation, the person eating the food could be either Rachel or the waitress. Using commonsense knowledge, or, more specifically, script knowledge about the RESTAU-RANT scenario, helps to resolve the referent of the pronoun: Rachel ordered the food. The person who orders the food is the customer. So Rachel should eat the food, she thus refers to Rachel.</p><p>This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems. In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc. <ref type="bibr" target="#b37">(Schank and Abelson, 1975)</ref>. The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system <ref type="bibr" target="#b0">(Bower et al., 1979;</ref><ref type="bibr" target="#b35">Schank, 1982;</ref><ref type="bibr" target="#b24">Modi et al., 2017)</ref>. From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding <ref type="bibr" target="#b36">(Schank, 1990)</ref>, information extraction <ref type="bibr" target="#b29">(Rau et al., 1989)</ref>, and drawing inferences from texts <ref type="bibr" target="#b19">(Miikkulainen, 1993)</ref>.</p><p>Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript , which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus <ref type="bibr" target="#b33">(Rudinger et al., 2015a</ref>) is a similar dataset centered around the EATING IN A RESTAURANT scenario.</p><p>In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering <ref type="bibr" target="#b23">(Modi and Titov, 2014)</ref>, paraphrasing <ref type="bibr" target="#b30">(Regneri et al., 2010;</ref><ref type="bibr" target="#b43">Wanzare et al., 2017)</ref>, event prediction (namely, the narrative cloze task) <ref type="bibr">Jurafsky, 2008, 2009;</ref><ref type="bibr" target="#b34">Rudinger et al., 2015b;</ref><ref type="bibr" target="#b21">Modi, 2016)</ref> or story completion (e.g. the story cloze task T It was a long day at work and I decided to stop at the gym before going home. I ran on the treadmill and lifted some weights. I decided I would also swim a few laps in the pool. Once I was done working out, I went in the locker room and stripped down and wrapped myself in a towel. I went into the sauna and turned on the heat. I let it get nice and steamy. I sat down and relaxed. I let my mind think about nothing but peaceful, happy thoughts. I stayed in there for only about ten minutes because it was so hot and steamy. When I got out, I turned the sauna off to save energy and took a cool shower. I got out of the shower and dried off. After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home.</p><p>Q1 Where did they sit inside the sauna? a. on the floor b. on a bench Q2 How long did they stay in the sauna? a. about ten minutes b. over thirty minutes Figure <ref type="figure">1</ref>: An example for a text from MCScript with 2 reading comprehension questions. <ref type="bibr" target="#b25">(Mostafazadeh et al., 2016)</ref>). These tasks test a system's ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks.</p><p>Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension. The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text. In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts. For example, consider the short narrative in (1). For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text.</p><p>The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets. In Section 3, we describe the dataset used for our shared task. Section 4.2 gives details about the setup of our task. In Section 5, information about participating systems is given. Results are presented and discussed in Sections 6 and 8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, a number of datasets have been proposed for machine comprehension. One example is MCTest <ref type="bibr" target="#b32">(Richardson et al., 2013)</ref>, a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE <ref type="bibr" target="#b12">(Lai et al., 2017)</ref>, which contains 100,000 questions on reading examination data. <ref type="bibr" target="#b28">Rajpurkar et al. (2016)</ref> have proposed the Stanford Question Answering Dataset (SQuAD), a data set of 100,000 questions on Wikipedia articles collected via crowdsourcing. In that dataset, the answer to a question corresponds to a segment/span from the reading passage. Since Wikipedia articles mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. <ref type="bibr" target="#b44">Weston et al. (2015)</ref> have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reasoning to solve different tasks. In contrast to our dataset, the artificial texts in BAbI are not reflective of a typically occurring narrative text.</p><p>Two recently published datasets that also have a larger focus on commonsense reasoning are NewsQA and TriviaQA. NewsQA <ref type="bibr" target="#b41">(Trischler et al., 2017)</ref> contains newswire texts from CNN with crowdsourced questions and answers. During the question collection, workers were only presented with the title of the text, and a short summary. This method ensures that literal repetitions of the text are avoided and the generation of non-trivial questions requiring background knowledge is supported. The NewsQA text collection differs from MCScript in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant.</p><p>TriviaQA <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref> contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with webcrawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>In 3.1, we now briefly describe the machine comprehension dataset used for the shared task, MC-Script. Parts of the following Section are taken from <ref type="bibr" target="#b26">Ostermann et al. (2018)</ref>. For a more detailed description of the resource collection and a more thorough discussion of the dataset, we refer to the original paper. Section 3.2 gives details about script data collections that were made available to the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Machine Comprehension Data -MCScript</head><p>For our shared task, we use the MCScript data set <ref type="bibr" target="#b26">(Ostermann et al., 2018)</ref>. It is a collection of narrative texts, questions of various types referring to these texts, and pairs of answer candidates for each question. It comprises 2,119 such texts and a total of 13,939 questions. The texts in the data set talk about everyday activities and cover 110 script scenarios of differing complexity. For the text collection, we followed : All texts are simple and explicit in the description of script events and script participants. The data set was crowdsourced via Amazon Mechanical turk 1 . In the crowdsourcing experiments, participants were asked to write questions independent of a concrete narrative, but only based on short descriptions of a scenario. By doing so, the collected questions were related to the scenario only and could be answered from different texts, independent of story details.</p><p>The scenario-based questions were paired randomly with texts from the same scenario. The subsequent answer collection was divided up into two steps: First, crowdsourcing workers had to annotate whether a question could be answered based on the given text. If it could be answered, they had to explicitly mark whether it could be answered from the text directly or based on commonsense knowledge. Second, they had to write a plausible correct and incorrect answer, if the question was answerable. Afterwards, all texts, questions and answers were manually validated by trained annotators, and corrected, if necessary.</p><p>Due to the design of the data acquisition process, a substantial subset of questions (27.4%) require commonsense inference about everyday activities. Figure <ref type="figure" target="#fig_0">2</ref> gives an overview of the distribution of question types on the data. Yes/No questions form the largest group, with 29%, followed by questions asking for details of a narration or scenario (what/which and who).</p><p>For the task, the corpus was split into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). For 5 scenarios, all texts were held out for the test set, in order to avoid that models overfit and memorize the scenarios in the training data. Texts, questions, and answers contain on average 196.0 words, 7.8 words, and 3.6 words, respectively. There are 6.7 questions per text on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Script and Commonsense Knowledge Data</head><p>We also encouraged participants to make use of existing script data collections. Thus, we provided several existing collections of script data together with the machine comprehension corpus: DeScript <ref type="bibr" target="#b42">(Wanzare et al., 2016)</ref>, RKP <ref type="bibr" target="#b30">(Regneri et al., 2010)</ref> and the OMCS stories <ref type="bibr" target="#b39">(Singh et al., 2002)</ref>. The three datasets contain sequences of short, telegraphstyled descriptions of all events that need to be conducted in a scenario (event sequence descriptions, ESDs). The data sets contain ESDs for different numbers of scenarios, with a total coverage of over 200 scenarios. The complexity of scenarios varies from simple activities, such as opening a window, to more complex ones, such as attending a wedding.</p><p>For 90 of the 110 scenarios in MCScript, there exist multiple ESDs per scenario in at least one of the 3 script data collections.</p><p>We also advised participants to make use of other representations for script knowledge, such as narrative chains <ref type="bibr" target="#b3">(Chambers and Jurafsky, 2008)</ref>, or event embeddings <ref type="bibr" target="#b23">(Modi and Titov, 2014)</ref>. Some participants also made use of ConceptNet <ref type="bibr" target="#b40">(Speer et al., 2017)</ref> as a resource for commonsense knowledge. ConceptNet is a large-scale knowledge graph that is built from several handcrafted and crowdsourced sources, and that encodes various types of commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Shared Task Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Method</head><p>In our evaluation, we measured how well a system was capable of correctly answering questions that may involve commonsense knowledge. As evaluation metric, we used accuracy, calculated as the ratio between correctly answered questions and all questions in our evaluation data. We also evaluated systems with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We provide results of two baseline systems as lower bounds for comparison: a rule-based baseline (Sliding Window) and a neural end-to-end system (Attentive Reader). Both baselines are described in more detail below. For details about the tuning of hyperparameters, we refer to <ref type="bibr" target="#b26">Ostermann et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sliding Window</head><p>The sliding window baseline is a simple rule-based method that answers a question on a text by predicting the answer option with the highest similarity to the text. The intuition underlying this method is that answers similar to a text should be more plausible than answer options that are different from the text (independent of the question). In our baseline implementation, we compute similarity using a sliding window that compares each answer option to any possible "window" of w tokens of the text. For comparison, each window and each answer is represented by an average vector, computed over the components of word embeddings corresponding to the words in the window and answer, respectively. For each possible window, we compute similarity as the cosine similarity between the window and the answer representation. The answer with the higher maximum similarity (over possible windows) is predicted to be the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Reader</head><p>The attentive reader is an established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus <ref type="bibr" target="#b8">(Hermann et al., 2015;</ref><ref type="bibr" target="#b5">Chen et al., 2016)</ref>. It is a neural networkbased approach, which scores answers to a question on a text by finding ("paying attention to") and scoring relevant passages in the text. The scoring and attention mechanisms are learned directly ("end-toend") from text-question-answer triples, without the need for manual rule writing or feature engineering. As a baseline for the shared task, we use the model formulation by <ref type="bibr" target="#b5">Chen et al. (2016)</ref> and <ref type="bibr" target="#b12">Lai et al. (2017)</ref>, who employ bilinear weight functions to compute both attention and answer-text fit. Bi-directional gated recurrent units (GRUs) are used to encode questions, texts and answers into hidden representations. For a question q and an answer a, the last state of the GRU, q and a, are used as representations, while the text is encoded as a sequence of hidden states t 1 ...t n . We compute an attention score s j for each hidden state t j using the question representation q, a weight matrix W a , and an attention bias b. The text representation t is computed as a weighted average of hidden  representations:</p><formula xml:id="formula_0">s j =sof tmax j (t j W a q + b) t = j s j t j (1)</formula><p>The probability p of answer a being correct is predicted using another bilinear weight matrix W s , followed by an application of the softmax function over both answer options for the question:</p><formula xml:id="formula_1">p(a|t, q) = sof tmax(t W s a)<label>(2)</label></formula><p>5 Participants</p><p>We ran our shared task through the CodaLab platform 3 . 24 teams submitted results during the evaluation period, out of which 11 teams provided system descriptions: 8 teams from China, and one team each from Spain, Russia and the US. The full leader board containing all 24 submissions can be found on the shared task website. Except for one team, all participating models rely on recurrent neural network techniques to encode texts, questions and/or answers. The one team that did not apply neural methods proposed an alternative approach based on clustering techniques and scoring word overlap. Only 3 of the 11 teams made explicit use of commonsense knowledge: Two approaches used ConceptNet, either in the form of features extracted from ConceptNet relations or in the form of pretrained Numberbatch embeddings <ref type="bibr" target="#b40">(Speer et al., 2017)</ref>. One participating system made use of script knowledge in the form of event sequence descriptions. Resources commonly used by participants include pretrained word embeddings such as GloVe <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> or word2vec <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref>, and preprocessing pipelines such as NLTK 4 . In the following, we provide short summaries of the participants' systems and we give an overview of models and resources used by them (Table <ref type="table" target="#tab_1">1</ref>). Neural-network based models Apart from IUCM, all participating systems are neural endto-end models that employ recurrent and/or convolutional neural network architectures. Systems mainly differ with respect to details of the architecture and the form of how words are represented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-neural methods</head><p>Yuanfudao (Liang Wang, 2018) applies a threeway attention mechanism to model interactions between the text, question and answers, on top of BiL-STMs. Each word in a text, question, and answer is represented by a vector of GloVe embeddings and additional information from part-of-speech tagging, name entity recognition, and relation extraction  The accuracy of participating systems and the two baselines in total, on commonsense-based questions (CS), text-based questions (TXT) and on out-of-domain questions (from the 5 held-out testing scenarios). The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line. The last line shows the human upper bound <ref type="bibr" target="#b26">(Ostermann et al., 2018)</ref> as comparison.</p><p>(based on ConceptNet). The model is pretrained on another large machine comprehension dataset, namely the RACE corpus.</p><p>MITRE <ref type="bibr" target="#b18">(Merkhofer et al., 2018</ref>) use a combination of 3 systems -two LSTMs with attention mechanisms, and one logistic regression model using patterns based on the vocabulary of the training set. The two neural models use different word embeddings -one trained on GoogleNews, another one trained on Twitter, which were enriched with word overlap features. Interestingly, the simple logistic regression model achieves competitive performance and would have ranked 4th as an individual system.</p><p>Jiangnan <ref type="bibr" target="#b45">(Xia, 2018)</ref> applies a BiLSTM over GloVe and CoVe embeddings <ref type="bibr" target="#b17">(McCann et al., 2017)</ref> with an additional attention mechanism. The attention mechanism computes soft word alignment between words in the question and the text or answer. Manual features, including part-of-speech tags, named entitity types, and term frequencies, are employed to enrich word token representations.</p><p>ELiRF-UPV (José -Ángel <ref type="bibr" target="#b10">González et al., 2018)</ref> employs a BiLSTM with attention to find similarities between texts, questions, and answers. Each word is represented based on Numberbatch embeddings, which encode information from ConceptNet.</p><p>YNU Deep <ref type="bibr" target="#b7">(Ding and Zhou, 2018)</ref> test different LSTMs and BiLSTMs variants to encode questions, answers and texts. A simple attention mechanism is applied between question-answer and text-answer pairs. The final submission is an ensemble of five model instances.</p><p>ZMU <ref type="bibr" target="#b14">(Li and Zhou, 2018</ref>) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings.</p><p>ECNU <ref type="bibr" target="#b38">(Sheng et al., 2018)</ref> use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader <ref type="bibr" target="#b6">(Dhingra et al., 2017)</ref>).</p><p>YNU AI1799 <ref type="bibr" target="#b16">(Liu et al., 2018)</ref> submitted an ensemble of neural network models based on LSTMs, RNNs, and BiLSTM/CNN combinations, with attention mechanisms. In addition to word2vec embeddings, positional embeddings are used that are generated based on word embeddings.   <ref type="table">3</ref>: Accuracy of participating systems and the baselines on the six most frequent question types. The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line.</p><p>YNU-HPCC <ref type="bibr" target="#b47">(Yuan et al., 2018</ref>) use an ensemble of neural networks with stacked CNN and LSTM layers and attention.</p><p>CSReader <ref type="bibr" target="#b9">(Jiang and Sun, 2018)</ref> use GRUs to encode questions and texts. Answer and text are combined by using an attention mechanism that models soft word alignments, inspired by work on Natural Language Inference <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. Two answer classifiers based on these representations are ensembled for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Tables <ref type="table" target="#tab_3">2 and 3</ref> give detailed results for all participating systems. We performed pairwise significance tests using an approximate randomization test <ref type="bibr" target="#b46">(Yeh, 2000)</ref> over texts. At an accuracy of 84%, the best participating team Yuanfudao performed significantly better (p&lt;0.05) than the second best system, MITRE (82%).</p><p>Except for when questions, Yuanfudao also achieved the best performance at each question type. However, individual differences in results over the 2nd place system were not found to be significant. The top three participating teams, Yuanfudao, MITRE and Jiangnan, all significantly outperform the remaining teams on text-based questions (&gt;80% vs. &lt;74%) as well as on yes/no, what, where and when questions.</p><p>In comparison to our baselines, all teams but Innopolis significantly outperform Sliding Window. Results of the Attentive Reader are in line with those of the participating systems ranked 7-9: ECNU, YNU AI1799 and YNU HPCC. The six top-ranked systems all significantly outperform both of our baselines. On out-of-domain questions, only the top 3 performing models significantly outperform the Attentive Reader baseline, while all models significantly outperform the Sliding Window approach.</p><p>For commonsense-based questions as well as for questions on why and who, results are considerably less consistent: while the top ranked system significantly outperforms teams ranked 7th or lower, most pairwise differences between the top teams are not statistically significant. This implies that the set of correctly answered questions considerably varies between systems, either due to randomness or because they excel at different inference problems.</p><p>We found that 19.3 % of the questions in the test set were answered correctly by each participating system. These questions mainly contain text-based questions with an answer that is literally given in the text. Also, there are many commonsense-based questions with a standardized correct answer, as shown in Example 2. Only few of the stories in MCScript cover a long timespan, so the answer to such questions is always similar.</p><p>(2) Q: How long did it take to pump up the tires? a. just a few minutes b. a few hours</p><p>In contrast, only 1% of questions could not be answered by any of the participating models. Answer-ing these questions mainly requires complicated inference steps, such as counting or plausibility judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We briefly highlight some of the findings by the shared task participants.</p><p>External knowledge sources. One of the main goals of this shared task was to provide an extrinsic evaluation framework for models of commonsense knowledge. However, only three participants actually made use of resources of commonsense knowledge.</p><p>Most prominent is the use of ConceptNet, a large-scale knowledge graph that is built from several handcrafted and crowdsourced sources. It was employed by two of the top 5 scoring models: Yuanfudao use it to learn their own ConceptNetbased relation embeddings. ELiRF-UPV make use of Numberbatch word embeddings, which are learned based on ConceptNet data. Ablation analyses conducted by Yuanfudao indicate that the addition of ConceptNet increases overall accuracy by almost 1% absolute. In contrast, only one participant used crowdsourced script data from the DeScript corpus in their final submission, IUCM. They found that the use of script data, instead of or in addition to texts, improved performance by up to 0.3% absolute.</p><p>CSReader tried to extend their neural model with script data from OMCS, but report that it did not result in an improvement.</p><p>No participant made use of narrative chains or other forms of structured/learned representations of scripts or events (such as event embeddings).</p><p>Pretraining. Most participants made use of pretraining in the form of word embeddings such as word2vec or GloVe, that were build on large data collections. Yuanfudao used the RACE dataset, which is the largest available multiple-choice machine comprehension corpus, for pretraining the complete model for several epochs. In their ablation analysis, they found pretraining to have the largest effect on model performance, with improvements in accuracy of up to 1.4% absolute. This result underlines that the comparably small size of MCScript naturally restricts how much neural approaches can learn from the data without overfitting.</p><p>Word representations. For representing tokens, most participants used word2vec embeddings, GloVe embeddings, or combinations thereof. The participating teams used different dimensionality sizes, and some of them refitted the vectors while others did not, leading to differing outcomes for both embedding types. In summary, none of both representations seems to clearly outperform the other.</p><p>In contrast, participants consistently found that extending word representations with additional features improves results: For example, Yuanfudao and Jiangnan use predicted part-of-speech tags and named entity information, as well as term frequency, and report improvements of up to 1% absolute in accuracy. Also, some participants report the use of word overlap features. Most notably, MITRE found that a logistic regression classifier based on overlap features can achieve performance competitive with neural approaches.</p><p>In general, additional features seem to be beneficial, since they provide more explicit or additional information that can be leveraged by neural networks and other classifiers.</p><p>Preprocessing. Several participants reported that lemmatizing and stop word removal further improved their results. A prominent example is the submission by MITRE, who use a stemmer to derive root forms for all words, in order to compute overlap and co-occurrence statistics between answers and text/questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This shared task provides an evaluation framework for commonsense knowledge in a machine comprehension setting. We create the MCScript corpus, which provides 2,119 stories and 13,939 answers for 110 everyday activities of different complexity. In contrast to previous datasets, MCScript was created in a way that results in a relatively large amount of common sense questions, i.e. questions which can not be answered directly from the text but require some form of common-sense knowledge about the scenario under consideration to be answered correctly.</p><p>24 teams submitted systems during the the evaluation period of the shared task, of which 11 teams submitted task description papers. The bestperforming system achieves an overall accuracy of 84%, which outperforms the two baselines by a large margin; yet, there gap to the human upper bound (98%) is still relative large.</p><p>Although participants were explicitly encouraged to use additional common-sense knowledge resources like DeScript of OMCS, only 3 systems (including the best-performing system) actually used such additional resources. The evaluation results suggest that additional common-sense knowledge is in fact beneficial for overall accuracy. However, the positive effect is relatively small, which might be due to the fact that our dataset has been created in a way that leads to relatively "easy" stories, and that the systems are able to learn a certain amount of common sense knowledge directly from the stories. In future work, it would be interesting to see if the results of our shared task carry over to other, presumably more complex stories like for instance personal blog stories from the Spinn3r corpus <ref type="bibr" target="#b2">(Burton et al., 2011)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of question types in MCScript, from Ostermann et al. (2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>IUCM (Reznikova and Derczynski, 2018) applied an unsupervised approach that assigns the correct answer to a question based on text overlap. Text overlap is computed based on the given passage and text sources of the same topic. Different clustering and topic modeling techniques are used to identify such text sources in MCScript and DeScript.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of techniques and resourced used by the participating systems.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">IUCM cluster MCScript texts and try to find answers also in other texts, that are topically similar. In that sense, MCScript itself is used to represent commonsense knowledge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://competitions.codalab.org/ competitions/17184</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.nltk.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. Also, we thank all teams for their participation and the effort they put into their submissions and the discussions, making this shared task a success.</p><p>This research was funded by the German Research Foundation (DFG) as part of SFB 1102 Information Density and Linguistic Encoding and EXC 284 Multimodal Computing and Interaction.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scripts in memory for text</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Bower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="220" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The icwsm 2011 spinn3r dataset</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Kasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Weblogs and Social Media (ICWSM</title>
				<meeting>the Annual Conference on Weblogs and Social Media (ICWSM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Schemas and their Participants</title>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">YNU Deep at SemEval-2018 Task 11: An Ensemble of Attentionbased BiLSTM Model for Machine Comprehension</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CSReader at SemEval-2018 Task 11: Multiple Choice Question Answering as Textual Entailment</title>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ELiRF-UPV at SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">José -Ángel</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><forename type="middle">F</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Encarna</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName><surname>Pla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributional semantics in linguistic and cognitive research</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian journal of linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ZMU at SemEval-2018 Task 11: Machine Comprehension Task using Deep Learning Models</title>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">YNU AI1799 at SemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge of Different model ensemble</title>
		<author>
			<persName><forename type="first">Qingxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MITRE at SemEval-2018 Task 11: Commonsense Reasoning without Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Merkhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Strickhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Subsymbolic natural language processing: An integrated model of scripts, lexicon, and memory</title>
		<author>
			<persName><surname>Risto Miikkulainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Event Embeddings for Semantic Script Modeling</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">InScript: Narrative texts annotated with script information</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatjana</forename><surname>Anikina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
				<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inducing Neural Models of Script Knowledge</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling semantic expectations: Using script knowledge for referent prediction</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Asad Sayeed, and Manfred Pinkal</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Language Resources and Evaluation</title>
				<meeting>the 11th International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Information extraction and text summarization using linguistic knowledge acquisition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><surname>Zernik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Script Knowledge with Web Experiments</title>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">IUCM at SemEval-2018 Task 11: Similar-Topic Texts as a Knowledge Source</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Reznikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to predict script events from domainspecific text</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics (* SEM 2015)</title>
				<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Script induction as language modeling</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dynamic memory: A theory of learning in people and computers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><surname>Schank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tell me a story: A new look at real and artificial memory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><surname>Schank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<pubPlace>Scribner New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scripts, Plans, and Knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><surname>Robert P Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international joint conference on Artificial intelligence-Volume</title>
				<meeting>the 4th international joint conference on Artificial intelligence-Volume</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2018 Task 11: Using Deep Learning Method to Address Machine Comprehension Task</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open Mind Common Sense: Knowledge Acquisition from the General Public</title>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travell</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">Li</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On the move to Meaningful Internet Systems 2002: CoopIS, DOA, and ODBASE</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1223" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">NewsQA: A Machine Comprehension Dataset</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
				<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeScript: A Crowdsourced Database for the Acquisition of Highquality Script Knowledge</title>
		<author>
			<persName><forename type="first">Lilian</forename><surname>Wanzare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Zarcone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
				<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering</title>
		<author>
			<persName><forename type="first">Lilian</forename><surname>Wanzare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Zarcone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
				<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
				<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Jiangnan at SemEval-2018 Task 11: Attention-Based Reading Comprehension System</title>
		<author>
			<persName><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
				<meeting>the 18th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">YNU-HPCC at Semeval-2018 Task 11: Using an Attention-based CNN-LSTM for Machine Comprehension using Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluations</title>
				<meeting>the 12th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
