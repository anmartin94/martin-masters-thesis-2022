<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
							<email>mihai.surdeanu@barcelonamedia.org</email>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
							<email>richard@cs.lth.se</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Meyers</surname></persName>
							<email>meyers@cs.nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
							<email>lluism@lsi.upc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
							<email>joakim.nivre@vxu.se</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Barcelona Media Innovation Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Catalonia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Växjö University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies. This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year's syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. The CoNLL-2008 shared task 1 proposes a unified dependency-based c 2008.</p><p>Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.</p><p>1 http://www.yr-bcn.es/conll2008</p><p>formalism, which models both syntactic dependencies and semantic roles. Using this formalism, this shared task merges both the task of syntactic dependency parsing and the task of identifying semantic arguments and labeling them with semantic roles. Conceptually, the 2008 shared task can be divided into three subtasks: (i) parsing of syntactic dependencies, (ii) identification and disambiguation of semantic predicates, and (iii) identification of arguments and assignment of semantic roles for each predicate. Several objectives were addressed in this shared task:</p><p>• SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies. While SRL on top of a dependency treebank has been addressed before <ref type="bibr" target="#b17">(Hacioglu, 2004)</ref>, our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates.</p><p>• Based on the observation that a richer set of syntactic dependencies improves semantic processing <ref type="bibr" target="#b20">(Johansson and Nugues, 2007)</ref>, the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks. For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations.</p><p>• A practical framework is provided for the joint learning of syntactic and semantic dependencies.</p><p>Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly setting. The evaluation is separated into two different challenges: a closed challenge, where systems have to be trained strictly with information contained in the given training corpus, and an open challenge, where systems can be developed making use of any kind of external tools and resources. The participants could submit results in either one or both challenges.</p><p>This paper is organized as follows. Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges. Section 3 introduces the corpora used and our constituent-to-dependency conversion procedure. Section 4 summarizes the results of the submitted systems. Section 5 discusses the approaches implemented by participants. Section 6 analyzes the results using additional non-official evaluation measures. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>In this section we provide the definition of the shared task, starting with the format of the shared task data, followed by a description of the evaluation metrics used and a discussion of the two shared task challenges, i.e., closed and open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Format</head><p>The data format used in this shared task was highly influenced by the formats used in the 2004-2007 shared tasks. The data follows these general rules:</p><p>• The files contain sentences separated by a blank line.</p><p>• A sentence consists of one or more tokens and the information for each token is represented on a separate line.</p><p>• A token consists of at least 11 fields. The fields are separated by one or more whitespace characters (spaces or tabs). Whitespace characters are not allowed within fields.</p><p>Table <ref type="table" target="#tab_1">1</ref> describes the fields stored for each token in the closed-track data sets. Columns 1-3 and 5-8 are available at both training and test time.</p><p>Column 4, which contains gold-standard part-ofspeech (POS) tags, is not given at test time. The same holds for columns 9 and above, which contain the syntactic and semantic dependency structures that the systems should predict.</p><p>The PPOS and PPOSS fields were automatically predicted using the SVMTool POS tagger <ref type="bibr" target="#b16">(Giménez, 2004)</ref>. To predict the tags in the training set, a 5-fold cross-validation procedure was used. The LEMMA and SPLIT LEMMA fields were predicted using the built-in lemmatizer in WordNet <ref type="bibr">(Fellbaum, 1998)</ref> based on the most frequent sense for the form and part-of-speech tag.</p><p>Since NomBank uses a sub-word analysis in some hyphenated words (such as [finger] ARG -[pointing] PRED ), the data format represents the parts in hyphenated words as separate tokens (columns 6-8). However, the format also represents how the parts originally fit together before splitting (columns 2-5). Padding characters (" ") are used in columns 2-5 to ensure the same number of rows for all columns corresponding to one sentence. All syntactic and semantic dependencies are annotated relative to the split word forms <ref type="bibr">(columns 6-8)</ref>.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the columns available to the systems participating in the open challenge: namedentity labels as in the <ref type="bibr">CoNLL-2003 Shared Task (Tjong Kim San and</ref><ref type="bibr" target="#b34">De Meulder, 2003)</ref> and from the BBN Wall Street Journal Entity Corpus, 2 WordNet supersense tags, and the output of an offthe-shelf dependency parser <ref type="bibr" target="#b29">(Nivre et al., 2007b)</ref>. Columns 1-3 were predicted using the tagger of <ref type="bibr" target="#b10">Ciaramita and Altun (2006)</ref>. Because the BBN corpus shares lexical content with the Penn Treebank, we generated the BBN tags using a 2-fold cross-validation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Measures</head><p>We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Official Evaluation Measures</head><p>The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LAS), (ii) semantic dependencies are evaluated using a labeled F 1 score, and (iii) the overall task is scored with a macro average of the two previous scores. We describe all these scoring measures next.</p><p>The LAS score is defined similarly as in the previous two shared tasks, as the percentage of to-   kens for which a system has predicted the correct HEAD and DEPREL columns (see Table <ref type="table" target="#tab_1">1</ref>). Same as before, our scorer also computes the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and label accuracy, i.e., the percentage of tokens with correct DEPREL.</p><p>The semantic propositions are evaluated by converting them to semantic dependencies, i.e., we create a semantic dependency from every predicate to all its individual arguments. These dependencies are labeled with the labels of the corresponding arguments. Additionally, we create a semantic dependency from each predicate to a virtual ROOT node. The latter dependencies are labeled with the predicate senses. This approach guarantees that the semantic dependency structure conceptually forms a single-rooted, connected (but not necessarily acyclic) graph. More importantly, this scoring strategy implies that if a system assigns the incorrect predicate sense, it still receives some points for the arguments correctly assigned. For example, for the correct proposition: verb.01: ARG0, ARG1, ARGM-TMP the system that generates the following output for the same argument tokens: verb.02: ARG0, ARG1, ARGM-LOC receives a labeled precision score of 2/4 because two out of four semantic dependencies are incorrect: the dependency to ROOT is labeled 02 in-stead of 01 and the dependency to the ARGM-TMP is incorrectly labeled ARGM-LOC. Using this strategy we compute precision, recall, and F 1 scores for both labeled and unlabeled semantic dependencies.</p><p>Finally, we combine the syntactic and semantic measures into one global measure using macro averaging. We compute macro precision and recall scores by averaging the labeled precision and recall for semantic dependencies with the LAS for syntactic dependencies:</p><formula xml:id="formula_0">3 LM P = W sem * LP sem + (1 − W sem ) * LAS (1) LM R = Wsem * LRsem + (1 − Wsem) * LAS (2)</formula><p>where LM P is the labeled macro precision and LP sem is the labeled precision for semantic dependencies. Similarly, LM R is the labeled macro recall and LR sem is the labeled recall for semantic dependencies. W sem is the weight assigned to the semantic task. <ref type="bibr">4</ref> The macro labeled F 1 score, which was used for the ranking of the participating systems, is computed as the harmonic mean of LM P and LM R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Additional Evaluation Measures</head><p>We used several additional evaluation measures to further analyze the performance of the participating systems.</p><p>The first additional measure used is Exact Match, which reports the percentage of sentences that are completely correct, i.e., all the generated syntactic dependencies are correct and all the semantic propositions are present and correct. While this score is significantly lower than any of the official scores, it will award systems that performed joint learning or optimization for all subtasks.</p><p>In the same spirit but focusing on the semantic subtasks, we report the Perfect Proposition F 1 score, where we score entire semantic frames or propositions. This measure is similar to the PProps accuracy score from the 2005 shared task <ref type="bibr" target="#b1">(Carreras and Màrquez, 2005)</ref>, with the caveat that this year this score is implemented as an F 1 measure, because predicates are not provided in the test data. Hence, propositions may be over or under generated at prediction time.</p><p>Lastly, we analyze systems based on the ratio between labeled F 1 score for semantic dependencies and the LAS for syntactic dependencies. In other words, this measure normalizes the semantic scores relative to the performance of the parsing component. This measure estimates the true overall performance of the semantic subtasks, independent of the syntactic parser. <ref type="bibr">5</ref> For example, this score addresses the situations where the semantic labeled F 1 score of one system is artificially low because the corresponding syntactic component does not perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Closed and Open Challenges</head><p>Similarly to the CoNLL-2005 shared task, this shared task evaluation is separated into two challenges:</p><p>Closed Challenge -systems have to be built strictly with information contained in the given training corpus, and tuned with the development section. In addition, the PropBank and NomBank lexical frames can be used. These restrictions mean that constituent-based parsers or SRL systems can not be used in this challenge because the constituent-based annotations are not provided in our training set. The aim of this challenge is to compare the performance of the participating systems in a fair environment.</p><p>Open Challenge -systems can be developed making use of any kind of external tools and resources. The only condition is that such tools or resources must not have been developed with the annotations of the test set, both for the input and output annotations of the data. In this challenge, we are interested in learning methods which make use of any tools or resources that might improve the performance. For example, we encourage the use of semantic information, as provided by NE recognition or word-sense disambiguation (WSD) systems (such state-of-the-art annotations are provided by the organizers, see Table <ref type="table" target="#tab_2">2</ref>). Also, in this challenge participants are encouraged to use constituent-based parsers and SRL systems, as long as these systems were trained only with the sections of Penn Treebank used in the shared task training corpus. To encourage the participation of the groups that are only interested in SRL, the organizers provide also the output of a state-of-theart dependency parser as input in this challenge. The comparison of different systems in this setting may not be fair, and thus ranking of systems is not necessarily important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The corpora used in the shared task evaluation were generated through a process that merges several input corpora and converts them from the constituent-based formalism to dependencies. This section starts with an introduction of the input corpora used, followed by a description of the constituent-to-dependency conversion process. The section concludes with an overview of the shared task corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Corpora</head><p>Input to our merging procedures includes the Penn Treebank, BBN's named entity corpus, PropBank and NomBank. In this section, we will provide brief descriptions of these annotations in terms of both form and content. All annotations are currently being distributed by the Linguistic Data Consortium, with the exception of NomBank, which is freely downloadable. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Penn Treebank 3</head><p>The Penn Treebank 3 corpus <ref type="bibr" target="#b23">(Marcus et al., 1993)</ref> consists of hand-coded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kuĉera, 1964) (test only). These hand parses are notated in-line and sometimes involve changing the strings of the input data. For example, in file wsj 0309, the token fearlast in the text corresponds to the two tokens fear and last in the annotated data. In a similar way, cannot is regularly split to can and not. It is significant that the other annotations assume the tokenization of the Penn Treebank, as this makes it easier for us to merge the annotation. The Penn Treebank syntactic annotation includes phrases, parts of speech, empty category representations of various filler/gap constructions and other phenomena, based on a theoretical perspective similar to that of Government and Binding Theory <ref type="bibr" target="#b7">(Chomsky, 1981)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">BBN Pronoun Coreference and Entity</head><p>Type Corpus BBN's NE annotation of the Wall Street Journal corpus <ref type="bibr" target="#b36">(Weischedel and Brunstein, 2005)</ref> takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC <ref type="bibr" target="#b5">(Chinchor and Robinson, 1998)</ref> and ACE 7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Proposition Bank I (PropBank)</head><p>The PropBank annotation <ref type="bibr" target="#b30">(Palmer et al., 2005)</ref> classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types 7 http://projects.ldc.upenn.edu/ace/ of ARGM (TMP, ADV, etc.). <ref type="bibr">8</ref> Rather than using PropBank directly, we used the version created for the CoNLL-2005 shared task <ref type="bibr" target="#b1">(Carreras and Màrquez, 2005)</ref>. PropBank's pointers to subtrees are converted into the list of leaves of those subtrees, minus the empty categories. On occasion, arguments of verbs end up being two non-adjacent substrings. For example, the argument of claims in the following sentence is indicated in bold: This sentence, Mary claims, is self-referential. The CoNLL-2005 format handles this by marking both strings A1 (This sentence and is self-referential), but adding a C-prefix to the argument tag on the second argument. Another difference between the PropBank annotation and the CoNLL-2005 version of it is their treatments of filler gap constructions involving empty categories. PropBank annotation includes the whole chain of empty categories, as well as the antecedent of the empty category (the filler of the gap). In contrast, the CoNLL-2005 version only includes the filler of the gap and if there is no filler, the argument is omitted, e.g., no ARG0 (subject) for leave would be included in I said to leave because the subject of leave is unspecified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">NomBank</head><p>NomBank annotation <ref type="bibr" target="#b26">(Meyers et al., 2004)</ref> uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun and verb argument structure; differences in treatment of nouns and verbs in the Penn Treebank; and differences in the sophistication of previous research about noun and verb argument structure. Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ARGM. These limitations were necessary to make the NomBank task consistent and tractable. In addition, long distance dependencies of nouns, e.g., the relation between Mary and walk in Mary took dozens of walks is handled as follows: Mary is marked as the ARG0 of walk and took + dozens + of is marked as a support chain in NomBank. In contrast, verbal long distance dependencies can be handled by means of empty categories in the Penn Treebank, e.g., the relation be-tween John and walked in John seemed t to walk. Support chains are needed because nominal long distance dependencies are not captured under the Penn Treebank's system of empty categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conversion to Dependencies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Syntactic Dependencies</head><p>There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank <ref type="bibr" target="#b23">(Marcus et al., 1993)</ref>. Since dependency syntax represents grammatical structure by means of labeled binary head-dependent relations rather than phrases, the task of the conversion procedure is to identify and label the head-dependent pairs. The idea underpinning constituent-to-dependency conversion algorithms <ref type="bibr" target="#b22">(Magerman, 1994;</ref><ref type="bibr" target="#b11">Collins, 1999;</ref><ref type="bibr" target="#b37">Yamada and Matsumoto, 2003)</ref> is that head-dependent pairs are created from constituents by selecting one word in each phrase as the head and setting all other as its dependents. The dependency labels are then inferred from the phrase-subphrase or phrase-word relations.</p><p>Our conversion procedure <ref type="bibr" target="#b20">(Johansson and Nugues, 2007)</ref> differs from this basic approach by exploiting the rich structure of the constituent format used in Penn Treebank 3:</p><p>• Grammatical function labels that often can be directly used in the dependency framework.</p><p>• Long-distance grammatical relations represented by means of empty categories and secondary edges, which can be used to create (often nonprojective) dependency links.</p><p>Of the grammatical function tags available in the Treebank, we removed the HLN, NOM, TPC, and TTL tags since they represent structural properties of single phrases rather than binary relations. For compatibility between the WSJ and Brown corpora, we removed the ETC, UNF, and IMP tags from Brown and the CLR tag from WSJ. Algorithms 1 and 2 show the constituent-todependency conversion algorithm and function labeling, respectively. The first steps apply structural transformations to the constituent trees. Next, a head word is assigned to each constituent. After this, grammatical functions are inferred, allowing a dependency tree to be created.</p><p>To find head children (used in assign-heads), a system of rules is used Algorithm 1: Pseudocode for constituent-todependency conversion.</p><p>procedure constituents-to-dependencies(T ) import-glarf(T ) reattach-traces(T ) split-small-clauses(T ) assign-heads(T.root) assign-functions(T ) return create-dependency-tree(T )  (Table <ref type="table" target="#tab_3">3</ref>). The first column in the table indicates the phrase type, the second is the search direction, and the third is a priority list of phrase types to look for. For instance, to find the head of an S phrase, we look from right to left for a VP. If no VP is found, look for anything with a PRD function tag, and so on.</p><formula xml:id="formula_1">procedure import-glarf(T ) Import a GLARF surface dependency graph G for each multi-word name N in G for each token d in N Set the function tag of d to NAME for each dependency link h → L d in G if L ∈ { APPOSITE,</formula><p>Moreover, since the grammatical structure in-   <ref type="bibr">(Meyers et al., 2007)</ref>.</p><p>The parts of GLARF's NP analysis that are most relevant to this task include: (i) identifying apposites (APPO, e.g., that book depends on gift in Mary's gift, a book about cheese; (ii) the identification of name boundaries taken from BBN's NE annotation, e.g., identifying that Smith depends on Mary which depends on appointment in the Mary Smith appointment; (iii) identifying TITLE and POSTHON dependencies, e.g., determining that Ms. and III depend on Mary in Ms. Mary Smith III. These identifications were carried out by hand-coded rules that have been fine tuned as part of GLARF, over the past several years. For example, identifying apposition constructions requires identifying that both the head and the apposite can stand alone -proper nouns (John Smith), plural nouns (books), and singular common nouns with determiners (the book) are stand-alone cases, whereas singular nouns without determiners (green book) do not qualify.</p><p>We split Treebank tokens at a hyphen (-) or a forward slash (/) if the segments on either side of these delimiters are: (a) a word in a dictionary (COMLEX Syntax or any of the dictionaries available on the NOMLEX website); (b) part of a markable Named Entity; 9 or (c) a prefix from the list: co, pre, post, un, anti, ante, ex, extra, fore, non, over, pro, re, super, sub, tri, bi, uni, ultra. For example, York-based was split into 3 segments: (1) York, (2) -and (3) based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Semantic Dependencies</head><p>When encoding the semantic dependencies, it was necessary to convert the underlying constituent analysis of PropBank and NomBank into a dependency analysis. Because semantic predicates are already assigned to individual tokens in both PropBank (the version used for the CoNLL-2005 shared task) and NomBank, constituent-todependency conversion is thus necessary only for semantic arguments. Conceptually, this conversion can be handled using similar heuristics as described in Section 3.2.1. However, in order to avoid replicating this effort and to ensure compatibility between syntactic and semantic dependencies, we decided to generate semantic dependencies using only argument boundaries and the syntactic dependencies generated in Section 3.2.1, i.e., ignoring syntactic constituents. Given this input, we identify the head of a semantic argument using the following heuristic:</p><p>The head of a semantic argument is assigned to the token inside the argument boundaries whose head is a token outside the argument boundaries. This heuristic works remarkably well: over 99% of the PropBank arguments in the training corpus have a single token whose head is located outside of the argument boundaries. As a simple example, consider the following annotated text: [sold] PRED [1214 cars] ARG1 [in the U.S.] ARGM-LOC . Using the above heuristic, the head of the ARG1 argument is set to cars, because it has an OBJ dependency to sold, and the head of the ARGM-LOC argument is set to in, because it modifies sold through a LOC dependency.</p><p>While this heuristic processes the vast majority of arguments, there are several cases that require special treatment. We discuss these situations in the remainder of this section.</p><p>Arguments with several syntactic heads For 0.7% of the semantic arguments, the above heuristic detects several syntactic heads for the given boundary. For example, in the text [it] ARG0</p><p>[expects] PRED [its U.S. sales to remain steady at about 1200 cars] ARG1 , the above heuristic assigns two syntactic heads to ARG1: sales, which modifies expects through an OBJ dependency, and to, which modifies expects through a PRD dependency. These situations are caused by the constituent-to-dependency conversion process described in Section 3.2.1, which in some cases interprets syntax differently than the original Treebank annotation, e.g., the raising phenomenon for the PRD dependency in the above example. In such cases, we split the original argument into a sequence of discontinuous arguments, e.g., the ARG1 in the above example becomes [its U.S. sales] ARG1 [to remain steady at about 1200 cars] C-ARG1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging discontinuous arguments</head><p>While in the above case we split arguments, there are situations where we can merge arguments that were initially discontinuous in PropBank or Nom-Bank. This typically happens when the Prop-Bank/NomBank predicate is infixed inside one of its arguments. For example, in the text [Milliondollar conferences] ARG1 were [held] PRED [to chew on subjects such as... ] C-ARG1 , PropBank lists multiple constituents as aggregately filling the ARG1 slot of held. These cases are detected automatically because the least common ancestor of the argument pieces is actually one of the argument segments. In the above example, to chew on subjects such as... depends on Million-dollar conferences because to modifies conferences through a NMOD dependency. In these situations, we treat the least common ancestor, e.g., conferences in the above text, as the true argument. This heuristic allowed us to merge 1665 (or 0.6% of total) arguments that were initially discontinuous in the Prop-Bank training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empty categories</head><p>PropBank and NomBank both encode chains of empty categories. As with the 2005 shared task <ref type="bibr" target="#b1">(Carreras and Màrquez, 2005)</ref>, we used the head of the antecedent of empty categories as arguments rather than empty categories. Furthermore, empty category arguments with no antecedents were ignored. <ref type="bibr">10</ref> For example, given The man wanted t to make a speech, we assume that the A0 of make and speech is man, rather than the chain consisting of the empty category represented as t and man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation disagreements</head><p>NomBank and Penn Treebank annotators sometimes disagree about constituent structure. Nom-  Bank annotators are in effect assuming that the constituents provided form a phrase. In this case, the constituents are adjacent to each other. For example, consider the NP the human rights discussion. In this case, the Penn Treebank would treat each of the four words the, human, rights, discussion as daughters of a single NP node. However, NomBank would treat human rights as a single ARG1 of discussion. Since noun noun modification constructions are head final, we can easily determine (via GLARF) that rights is the markable dependent of discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support chains</head><p>Finally, NomBank's encoding of support chains is handled as chains of dependencies in the data (although these are not scored). For example, given Mary took dozens of walks, where Mary is the ARG0 of walks, the support chain took + dozens + of is represented as a sequence of dependencies: of depends on Mary, dozens depends on of and took depends on dozens. Each of these dependencies is labeled SU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overview of Corpora</head><p>The syntactic dependency types are divided into atomic types that consist of a single label, and nonatomic types consisting of more than one label. There are 38 atomic and 70 non-atomic labels in the corpus. There are three types of non-atomic labels: those consisting of a PRD or OPRD concatenated with an adverbial label such as LOC or TMP; gapping labels such as GAP-SBJ; and combined adverbial tags such as LOC-TMP. Table <ref type="table" target="#tab_7">4</ref> shows statistics for the atomic syntactic dependencies: label type, the frequency of the label in the complete corpus, and a description of the label. Table <ref type="table" target="#tab_9">5</ref> shows the corresponding statistics for non-atomic dependencies, excluding gapping dependencies. The non-atomic labels are rare, which made it difficult to learn these relations ef-   fectively. Table <ref type="table" target="#tab_10">6</ref> shows the table for non-atomic labels containing a gapping label.</p><p>A dependency link w i → w j is said to be projective if all words occurring between w i and w j in the surface word order are dominated by w i (where dominance is the transitive closure of the direct link relation). Nonprojective links are impossible to handle for the search procedures in many types of dependency parsers. It has been previously observed that the majority of dependencies in all languages are projective, and this is particularly true for English -in the complete corpus, only 4118 links (0.4%) are nonprojective. 3312 sentences, or 7.6%, contain at least one nonprojective link.   Even to make love, he says, you need experience; split noun phrases such as hold a hearing tomorrow on the topic; and all other types of nonprojective links.</p><p>Lastly, Tables <ref type="table" target="#tab_13">8 and 9</ref> summarizes statistics for semantic predicates and roles. Table <ref type="table" target="#tab_13">8</ref> shows the number of non-support predicates with a given POS tag in the whole corpus (we used GPOS or PPOSS for predicates inside hyphenated words). The last line shows the number of predicates with a POS tag that does not start with NN or VB. This last table entry is generated by POS tagger mistakes when producing the PPOSS tags, or by errors in our NomBank/PropBank conversion software. <ref type="bibr">11</ref> Nevertheless, the overall picture given by the table indicates that predicates are almost perfectly distributed between nouns and verbs: there are 98525 nominal and 98553 verbal predicates.</p><p>Table <ref type="table" target="#tab_14">9</ref> shows the number of arguments with a given role label. For brevity we list only labels that are instantiated at least 10 times in the whole corpus. The total number of arguments labeled with a role label with frequency lower than 10 is listed in the last line in the table. The table indicates that, while the top three most common role labels are "core" labels (A1, A0, A2), modifier arguments (AM-* ) account for approximately 20% of the total number of arguments. On the other hand, discontinuous arguments are not common: only 0.7% of the total number of arguments have a continuation label (C-* ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions and Results</head><p>Nineteen groups submitted test runs in the closed challenge and five groups participated in the open challenge. Three of the latter groups participated only in the open challenge, and two of these submitted results only for the semantic subtask. These results are summarized in Tables <ref type="table" target="#tab_1">10 and 11</ref>.</p><p>Table <ref type="table" target="#tab_1">10</ref> summarizes the official results -i.e., results at evaluation deadline -for the closed challenge. Note that several teams corrected bugs and/or improved their systems and they submitted post-evaluation scores (accounted in the shared task website). The table indicates that most of the top results cluster together: three systems had a labeled macro F 1 score on the WSJ+Brown corpus around 82 points (che, ciaramita, and zhao); five systems scored around 79 labeled macro F 1 points (yuret, samuelsson, zhang, henderson, and watanabe). Remarkably, the top-scoring system (johansson) is in a class of its own, with scores 2-3 points higher than the next system. This is most likely caused by the fact that Johansson and Nugues ( <ref type="formula">2008</ref>) implemented a thorough system that addressed all facets of the task with state-ofthe-art methods: second-order parsing model, argument identification/classification models separately tuned for PropBank and NomBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5).</p><p>Table <ref type="table" target="#tab_1">11</ref> lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of the top five groups in the closed challenge submitted results in the open challenge. Only one of the systems that participated in both challenges (zhang) improved the results submitted in the closed challenge. <ref type="bibr" target="#b38">Zhang et al. (2008)</ref> achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus. The improvements measured were relatively small for the in-domain WSJ corpus (0.2 labeled macro F 1 points) but larger for the out-of-domain Brown corpus (approximately 1 labeled macro F 1 point).</p><p>Tables <ref type="table" target="#tab_1">10 and 11</ref> indicate that in both challenges the results on the out-of-domain corpus (Brown) are much lower than the results measured in-domain (WSJ). The difference is around 7-8 LAS points for the syntactic subtask and 12-14 labeled F 1 points for semantic dependencies. Overall, this yields a drop of approximately 10 labeled macro F 1 points for most systems. This performance decrease on out-of-domain corpora is consistent with the results reported in CoNLL-2005 on SRL (using the same Brown corpus). These results indicate that domain adaptation is a problem that is far from being solved for both syntactic and semantic analysis of text. Furthermore, as the scores on the syntactic and semantic subtasks indicate, domain adaptation becomes even harder as the task to be solved gets more complex.</p><p>We describe the participating systems in the next section. Then, in Section 6, we revert to result analysis using different evaluation measures and different views of the data.   . Teams are denoted by the last name of the first author of the corresponding paper in the proceedings or the last name of the person who registered the team if no paper was submitted. Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of the labeled F 1 score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approaches</head><p>Table <ref type="table" target="#tab_9">5</ref> summarizes the properties of the systems that participated in the closed the open challenges. The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly. For example, <ref type="bibr" target="#b31">Riedel and Meza-Ruiz (2008)</ref> perform predicate and argument identification and classification jointly, whereas <ref type="bibr" target="#b9">Ciaramita et al. (2008)</ref> implemented a pipeline architecture of three components. We use the || to indicate that several differ-ent architectures that span multiple subtasks were deployed in parallel. This summary of system architectures indicates that it is common that systems combine several components in the semantic or syntactic subtasks -e.g., nine systems jointly performed predicate/argument identification and classificationbut only four systems combined components between the syntactic and semantic subtasks: <ref type="bibr" target="#b18">Henderson et al. (2008)</ref>, who implemented a generative history-based model (Incremental Sigmoid Belief Networks with vectors of latent variables) where syntactic and semantic structures are separately generated but using a synchronized derivation (sequence of actions); <ref type="bibr" target="#b32">Samuelsson et al. (2008)</ref>, who, within an ensemble-based architecture, implemented a joint syntactic-semantic model using MaltParser with labels enriched with semantic information; Lluís and Màrquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, <ref type="bibr" target="#b33">Sun et al. (2008)</ref>, who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, <ref type="bibr" target="#b19">Johansson and Nugues (2008)</ref>, who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, <ref type="bibr" target="#b4">Chen et al. (2008)</ref> search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learning/Opt. column in the table. The system of Riedel and Meza-Ruiz ( <ref type="formula">2008</ref>) deserves a special mention: even though Riedel and Meza-Ruiz did not implement a syntactic parser, they are the only group that performed the complete SRL subtask -i.e., predicate identification and classification, argument identification and classification -jointly, simultaneously for all the predicates in a sentence. They implemented a joint SRL model using Markov Logic Networks and they selected the overall best solution using inference based on the cutting-plane algorithm.</p><p>Although some of the systems that implemented joint approaches obtained good results, the top five systems in the closed challenge are essentially systems with pipeline architectures. Furthermore, <ref type="bibr" target="#b19">Johansson and Nugues (2008)</ref> and <ref type="bibr" target="#b31">Riedel and Meza-Ruiz (2008)</ref> showed that joint learning/optimization improves the overall results, but the improvement is not large. These initial efforts indicate at least that the joint modeling of this problem is not a trivial task.</p><p>The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year's shared task <ref type="bibr" target="#b28">(Nivre et al., 2007)</ref>, the vast majority of parsing models fall in two classes: transition-based ("trans" in the table) or graph-based ("graph") models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras ( <ref type="formula">2007</ref>) -MST C , Eisner (2000) -MST E , or <ref type="bibr">Chu-Liu/Edmonds (Mc-Donald et al., 2005;</ref><ref type="bibr" target="#b8">Chu and Liu, 1965;</ref><ref type="bibr" target="#b12">Edmonds, 1967)</ref> -MST CL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). <ref type="bibr" target="#b32">Samuelsson et al. (2008)</ref> perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. <ref type="bibr" target="#b19">Johansson and Nugues (2008)</ref> use a single parsing model, but this model is extended with second-order features.</p><p>The PA Arch. and PA Inference columns summarize the architectures and inference strategies used for the identification and classification of predicates and arguments. The columns indicate that most systems modeled the SRL problem as a token-by-token classification problem ("class" in the table) with a corresponding greedy inference strategy. Some systems (e.g., yuret, samuelsson, henderson, lluis) incorporate SRL within parsing, in which case we report the corresponding parsing architecture and inference approach. <ref type="bibr" target="#b35">Vickrey and Koller (2008)</ref> simplify the sentences to be labeled using a set of hand-crafted rules before deploying a classification model on top of a constituent-based representation. Unlike in the case of parsing, few systems (yuret, samuelssson, and morante) combine several PA models and the combination is limited to simple voting strategies (see the PA Comb. column).</p><p>Finally, the ML Methods column lists the Machine Learning (ML) methods used. The column indicates that maximum entropy (ME) was the most popular method (12 distinct systems relied on it). Support Vector Machines (SVM) (eight systems) and the Perceptron algorithm (three systems) were also popular ML methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Section 4 summarized the results in the closed and open challenges using the official evaluation measures. In this section, we analyze the submitted runs using different evaluation measures, e.g., Exact Match or Perfect Proposition F 1 scores, and different views of the data, e.g., only nonprojective dependencies or NomBank versus Prop-Bank frames.  Because some open-challenge systems did not implement syntactic parsing, these systems are sorted by labeled F1 score of the semantic dependencies on the WSJ+Brown corpus. Only the systems that have a corresponding paper in the proceedings are included. Systems that participated in both challenges are listed only in the closed challenge. Acronyms used: D -syntactic dependencies, P -predicate, A -argument, I -identification, C -classification. Overall arch. stands for the complete system architecture; D Arch. stands for the architecture of the syntactic parser; D Comb. indicates if the final parser output was generated using parser combination; D Inference stands for the type of inference used for syntactic parsing; PA Arch. stands the type of architecture used for PAIC; PA Comb. indicates if the PA output was generated through system combination; PA Inference stands for the the type of inference used for PAIC; Joint Learning/Opt. indicates if some form of joint learning or optimization was implemented for the syntactic + semantic global task; ML methods lists the ML methods used throughout the complete system.</p><p>Exact Match  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Exact Match and Perfect Propositions</head><p>Table <ref type="table" target="#tab_1">13</ref> lists the Exact Match and Perfect Proposition F 1 scores for test runs submitted in both challenges. Both these scores measure the capacity of a system to correctly parse structures with granularity much larger than a simple dependency, i.e., entire sentences for Exact Match and complete propositions for Perfect Proposition F 1 (see Section 2.2.2 for a formal definition of these evaluation measures). The table indicates that these values are much smaller than the scores previously reported, e.g., labeled macro F 1 . This is to be expected: the probability of an incorrectly parsed unit (sentence or proposition) is much larger given its granularity. However, the main purpose of this analysis is to investigate if systems that focused on joint learning or optimization performed better than others with respect to these global measures. This indeed seems to be the case for at least two systems. The system of <ref type="bibr" target="#b19">Johansson and Nugues (2008)</ref>, which jointly optimizes the labeled F 1 score (for semantic dependencies) and then the labeled macro F 1 score (for the complete task), increases its distance from the next ranked system: its Perfect Proposition F 1 score is over 6 points higher than the score of the second system in Table <ref type="table" target="#tab_1">13</ref>. The system of <ref type="bibr" target="#b18">Henderson et al. (2008)</ref>, which was designed for joint learning of the complete task, improves its rank from eighth to fifth compared to the official results (Table <ref type="table" target="#tab_1">10</ref>).  from position thirteenth to ninth and choi from sixteenth to eighth. This indicates that these systems were penalized in the official ranking mainly due to the relative poor performance of their parsers. Note that this experiment is relevant only for systems that implemented pipeline architectures, where the semantic components are in fact separated from the syntactic ones; this excludes the systems that blended syntax with SRL: henderson, sun, and lluis. Furthermore, systems that had significantly lower scores in syntax will receive an unreasonable boost in ranking according to this measure. Fortunately, there was only one such outlier in this evaluation (neumann), shown in gray in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Nonprojectivity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">PropBank versus NomBank</head><p>Table <ref type="table" target="#tab_1">16</ref> lists the labeled F 1 scores for semantic dependencies for two different views of the testing data sets: for propositions centered around verbal predicates, i.e., from PropBank, and for propositions centered around nominal predicates, i.e., from NomBank. Table <ref type="table" target="#tab_1">15</ref>: Ratio of the labeled F 1 score for semantic dependencies and LAS for syntactic dependencies. Systems are sorted in descending order of this ratio score on the WSJ+Brown corpus. We only show systems that participated in both the syntactic and semantic subtasks.</p><p>The table indicates that, generally, systems performed much worse on nominal predicates than on verbal predicates. This is to be expected considering that there is significant body of previous work that analyzes the SRL problem on Prop-Bank, but minimal work for NomBank. On average, the difference between the labeled F 1 scores for verbal predicates and nominal predicates on the WSJ+Brown corpus is 7.84 points. Furthermore, the average difference between labeled F 1 scores on the Brown corpus alone is 12.36 points. This indicates that the problem of SRL for nominal predicates is more sensitive to domain changes than the equivalent problem for verbal predicates. Our conjecture is that, because there is very little syntactic structure between nominal predicates and their arguments, SRL models for nominal predicates select mainly lexical features, which are more brittle than syntactic or other non-lexicalized features.</p><p>Remarkably, there is one system (baldridge) which performed better on the WSJ+Brown for nominal predicates than verbal predicates. Unfortunately, this group did not submit a systemdescription paper so it is not clear what was their approach.  Systems can mitigate the inherent differences between verbal and nominal predicates with different models for the two sub-problems. This was indeed the approach taken by two out of the top three systems (johansson and che). <ref type="bibr" target="#b19">Johansson and Nugues (2008)</ref> developed different models for verbal and nominal predicates and implemented separate feature selection processes for each model. <ref type="bibr" target="#b3">Che et al. (2008)</ref> followed the same method but they also implemented separate domain constraints for inference for the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The previous four CoNLL shared tasks popularized and, without a doubt, boosted research in semantic role labeling and dependency parsing. This year's shared task introduces a new task that essentially unifies the problems addressed in the past four years under a unique, dependency-based formalism. This novel task is attractive both from a research perspective and an application-oriented perspective:</p><p>• We believe that the proposed dependencybased representation is a better fit for many applications (e.g., Information Retrieval, Information Extraction) where it is often suffi-cient to identify the dependency between the predicate and the head of the argument constituent rather than extracting the complete argument constituent.</p><p>• It was shown that the extraction of syntactic and semantic dependencies can be performed with state-of-the-art performance in linear time <ref type="bibr" target="#b9">(Ciaramita et al., 2008)</ref>. This can give a significant boost to the adoption of this technology in real-world applications.</p><p>• We hope that this shared task will motivate several important research directions. For example, is the dependency-based representation better for SRL than the constituent-based formalism? Does joint learning improve syntactic and semantic analysis?</p><p>• Surface (string related patterns, syntax, etc.) linguistic features can often be detected with greater reliability than deep (semantic) features. In contrast, deep features can cover more ground because they regularize across differences in surface strings. Machine learning systems can be more effective by using evidence from both deep and surface features jointly <ref type="bibr" target="#b39">(Zhao, 2005)</ref>.</p><p>Even though this shared task was more complex than the previous shared tasks, 22 different teams submitted results in at least one of the challenges. Building on this success, we hope to expand this effort in the future with evaluations on multiple languages and on larger out-of-domain corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A-POS, N-POS, POST-HON, Q-POS, RED-RELATIVE, SUFFIX, T-POS, TITLE } or if h and d are inside a split word Set the function tag of d to L in T if h and d are part of a larger constituent Add an NX constituent to T that brackets h and d procedure reattach-traces(T ) for each empty category t in T if t is linked to a constituent C via a secondary edge label L and L ∈ { * ICH * , * T * , * RNR * } disconnect C disconnect the secondary edge attach C to the parent of t procedure split-small-clauses(T ) for each verb phrase C in T if C has a child S and the phrase label of S is S and S is not preceded by a '' or , tag and S has a subject child s disconnect s attach s to C set the function tag of s to OBJ set the function tag of S to OPRD procedure assign-heads(N ) for each child C of N assign-heads(C) if is-coordinated(N ) e ← index of first CC or CONJP or , or : else e ← index of last child of N find head child H between 1 and e according to head rules (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Column format in the closed-track data. The columns in the lower part of the table are unseen at test time and are to be predicted by systems.</figDesc><table><row><cell cols="2">Number Name</cell><cell>Description</cell></row><row><cell>1</cell><cell>CONLL2003</cell><cell>Named entity labels using the tag set from the CoNLL-2003 shared task.</cell></row><row><cell>2</cell><cell>BBN</cell><cell>NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.</cell></row><row><cell>3</cell><cell>WNSS</cell><cell>WordNet super senses.</cell></row><row><cell>4</cell><cell>MALT HEAD</cell><cell>Head of the syntactic dependencies generated by MaltParser.</cell></row><row><cell>5</cell><cell cols="2">MALT DEPREL Label of syntactic dependencies generated by MaltParser.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Column format in the open-track data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 )</head><label>3</label><figDesc>N.head ← H.head procedure is-coordinated(N )if N has the label UCP return True if N has a CC or CONJP child which is not leftmost return True if N has a , or : child c, and c is not leftmost or rightmost or crossed by an apposition link, return True else return False procedure create-dependency-tree(T ) D ← {} for each token t in T let C be the highest constituent that t is the head of let P be the parent of C let L be the function tag of C D ← D ∪ P.head → L t return D</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ADJP ← NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS SBAR RB ADVP → RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN CONJP → CC RB IN FRAG → (NN * | NP) W * SBAR (PP | IN) (ADJP | JJ) ADVP</figDesc><table><row><cell></cell><cell>RB</cell></row><row><cell>INTJ LST</cell><cell>← * → LS :</cell></row><row><cell cols="2">NAC, NP, NX, WHNP ← (NN * | NX) NP-ε JJR CD JJ JJS RB QP NP PP, WHPP → IN TO VBG VBN RP FW</cell></row><row><cell>PRN PRT</cell><cell>→ S * N * W * PP|IN ADJP|JJ * ADVP|RB * → RP</cell></row><row><cell>QP</cell><cell>← $ IN NNS NN JJ RB DT CD NCD QP JJR JJS</cell></row><row><cell>RRC</cell><cell>→ VP NP ADVP ADJP PP</cell></row><row><cell>S SBAR</cell><cell>← VP * -PRD S SBAR ADJP UCP NP ← S SQ SINV SBAR FRAG IN DT</cell></row><row><cell>SBARQ</cell><cell>← SQ S SINV SBARQ FRAG</cell></row><row><cell>SINV SQ UCP VP</cell><cell>← VBZ VBD VBP VB MD VP * -PRD S SINV ADJP NP ← VBZ VBD VBP VB MD * -PRD VP SQ → * → VBD VBN MD VBZ VB VBG VBP VP * -PRD ADJP NN NNS NP</cell></row><row><cell>WHADJP</cell><cell>← CC WRB JJ ADJP</cell></row><row><cell>WHADVP</cell><cell>→ CC WRB</cell></row><row><cell>X</cell><cell>→ *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Head rules. Set the function tag of C to L procedure infer-function(C) let c be the head of C, P the parent of C, and p the head ofP if C is an object return OBJ if C is PRN return PRN if h is punctuation return P if C is coordinated with P return COORD if C is PP, ADVP, or SBAR and P is VP return ADV if C is PRT and P is VP return PRT if Cis VP and P is VP, SQ, or SINV return VC if C is TO and P is VP return IM if P is SBAR and p is IN return SUB if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV if P is NP, NX, NAC, or WHNP return NMOD if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD if P is PP or WHPP return PMOD else return DEP side noun phrases (NP) is under-specified in the Penn Treebank, we imported dependencies inside NPs and hyphenated words from a version of the Penn Treebank mapped into GLARF, the Grammatical and Logical Argument Representation Framework</figDesc><table><row><cell>Algorithm 2: Pseudocode for the function la-</cell></row><row><cell>beling procedure.</cell></row></table><note>procedure assign-functions(T )for each constituent C in T if C has no function tag from Penn or GLARF L ← infer-function(C)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Statistics for atomic syntactic labels.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Statistics for non-atomic syntactic labels excluding gapping labels.</figDesc><table><row><cell>Label</cell><cell>Frequency</cell></row><row><cell>GAP-SBJ</cell><cell>116</cell></row><row><cell>GAP-OBJ</cell><cell>102</cell></row><row><cell>DEP-GAP</cell><cell>83</cell></row><row><cell>GAP-TMP</cell><cell>69</cell></row><row><cell>GAP-PRD</cell><cell>66</cell></row><row><cell>GAP-LGS</cell><cell>44</cell></row><row><cell>GAP-LOC</cell><cell>42</cell></row><row><cell>DIR-GAP</cell><cell>37</cell></row><row><cell>GAP-PMOD</cell><cell>22</cell></row><row><cell>GAP-VC</cell><cell>20</cell></row><row><cell>EXT-GAP</cell><cell>16</cell></row><row><cell>ADV-GAP</cell><cell>15</cell></row><row><cell>GAP-NMOD</cell><cell>13</cell></row><row><cell cols="2">GAP-LOC-PRD 6</cell></row><row><cell>DTV-GAP</cell><cell>6</cell></row><row><cell>AMOD-GAP</cell><cell>6</cell></row><row><cell>GAP-MNR</cell><cell>5</cell></row><row><cell>GAP-PRP</cell><cell>4</cell></row><row><cell>EXTR-GAP</cell><cell>3</cell></row><row><cell>GAP-SUB</cell><cell>1</cell></row><row><cell>GAP-PUT</cell><cell>1</cell></row><row><cell>GAP-OPRD</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Statistics for non-atomic labels containing a gapping label.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc>shows statistics for different types of nonprojective links: nonprojectivity caused by wh-movement, such as in Where are you going? or What have you done?; split clauses such as</figDesc><table><row><cell>Type</cell><cell>Frequency</cell></row><row><cell>wh-movement</cell><cell>1709</cell></row><row><cell>Split clause</cell><cell>734</cell></row><row><cell cols="2">Split noun phrase 590</cell></row><row><cell>Other</cell><cell>1085</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Statistics for nonprojective links.</figDesc><table><row><cell>POS</cell><cell>Frequency</cell></row><row><cell>NN</cell><cell>68477</cell></row><row><cell>NNS</cell><cell>30048</cell></row><row><cell cols="2">VBD 24106</cell></row><row><cell>VB</cell><cell>23650</cell></row><row><cell cols="2">VBN 19339</cell></row><row><cell cols="2">VBG 14245</cell></row><row><cell>VBZ</cell><cell>10883</cell></row><row><cell>VBP</cell><cell>6330</cell></row><row><cell cols="2">Other 83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Statistics for predicates, by POS tags.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Statistics for semantic roles.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Official results in the closed challenge (post-evaluation scores are available on the shared task website). Teams are denoted by the last name of the first author of the corresponding paper in the proceedings or the last name of the person who registered the team if no paper was submitted. Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of the labeled macro F 1 score on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task.</figDesc><table><row><cell></cell><cell cols="2">Labeled Macro F 1</cell><cell></cell><cell cols="3">Labeled Attachment Score</cell><cell cols="2">Labeled F 1</cell><cell></cell></row><row><cell></cell><cell cols="2">(complete task)</cell><cell></cell><cell cols="3">(syntactic dependencies)</cell><cell cols="3">(semantic dependencies)</cell></row><row><cell></cell><cell>WSJ+Brown</cell><cell>WSJ</cell><cell>Brown</cell><cell>WSJ+Brown</cell><cell>WSJ</cell><cell>Brown</cell><cell>WSJ+Brown</cell><cell>WSJ</cell><cell>Brown</cell></row><row><cell>vickrey</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.17 (1)</cell><cell>77.38</cell><cell>66.23</cell></row><row><cell>riedel</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.59 (2)</cell><cell>75.72</cell><cell>65.38</cell></row><row><cell>zhang</cell><cell>79.61 (1)</cell><cell>80.61</cell><cell>71.45</cell><cell>87.32 (1)</cell><cell>88.14</cell><cell>80.80</cell><cell>71.89 (3)</cell><cell>73.08</cell><cell>62.11</cell></row><row><cell>li</cell><cell>77.84 (2)</cell><cell>78.87</cell><cell>69.51</cell><cell>86.69 (2)</cell><cell>87.42</cell><cell>80.80</cell><cell>68.99 (4)</cell><cell>70.32</cell><cell>58.22</cell></row><row><cell>wang</cell><cell>76.19 (3)</cell><cell>78.39</cell><cell>59.89</cell><cell>84.56 (3)</cell><cell>85.50</cell><cell>77.06</cell><cell>67.12 (5)</cell><cell>70.41</cell><cell>42.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Official results in the open challenge (post-evaluation scores are available on the shared task website)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Summary of system architectures that participated in the closed and open challenges. The closed-challenge systems are sorted by macro labeled F1 score on the WSJ+Brown corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Exact Match and Perfect Proposition F 1 scores for runs submitted in the closed and open challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect Proposition F 1 score. The number in parentheses next to the WSJ+Brown scores indicates the system rank according to the corresponding scoring measure.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14</head><label>14</label><figDesc>shows the unlabeled F1 scores for prediction of nonprojective syntactic dependencies. Since nonprojectivity is quite rare, many teams chose to ignore this issue. The table shows only those systems that submitted well-formed dependency trees, and whose output contained at least one nonprojective link. The small number of nonprojective links in the training set makes it hard to learn to predict such links, and this is also reflected in the figures. In general, the figures for nonprojective wh-movements and split clauses are higher, and they are also the most common types. Also, they are detectable by fairly simple patterns, such as the presence of a wh-word or a pair of commas.</figDesc><table><row><cell>System</cell><cell>All</cell><cell cols="3">wh-mov. SpCl SpNP</cell></row><row><cell>choi</cell><cell>25.43</cell><cell>49.49</cell><cell>45.47</cell><cell>8.72</cell></row><row><cell>lee</cell><cell>46.26</cell><cell>50.30</cell><cell cols="2">64.84 20.69</cell></row><row><cell>nugues</cell><cell>46.15</cell><cell>58.96</cell><cell cols="2">59.26 11.32</cell></row><row><cell cols="2">samuelsson 24.47</cell><cell>38.15</cell><cell>0</cell><cell>9.83</cell></row><row><cell>titov</cell><cell>42.32</cell><cell>50.56</cell><cell>48.71</cell><cell>0</cell></row><row><cell>zhang</cell><cell>13.39</cell><cell>5.71</cell><cell>12.33</cell><cell>7.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14 :</head><label>14</label><figDesc>Unlabeled F1-measures for nonprojective links. Results are given for all links, whmovements, split clauses, and split noun phrases.</figDesc><table><row><cell>6.3 Normalized SRL Performance</cell></row><row><cell>Table 6.3 lists the scores for the semantic sub-</cell></row><row><cell>task measured as the ratio of the labeled F 1 score</cell></row><row><cell>and LAS. As previously mentioned, this score es-</cell></row><row><cell>timates the performance of the SRL component</cell></row><row><cell>independent of the performance of the syntactic</cell></row><row><cell>parser. This analysis is not a substitute for the</cell></row><row><cell>actual experiment where the SRL components are</cell></row><row><cell>evaluated using correct syntactic information but,</cell></row><row><cell>nevertheless, it indicates several interesting facts.</cell></row><row><cell>First, the ranking of the top three systems in Ta-</cell></row><row><cell>ble 10 changes: the system of Che et al. (2008)</cell></row><row><cell>is now ranked first, and the system of Johansson</cell></row><row><cell>and Nugues (2008) is second. This shows that Che</cell></row><row><cell>et al. have a relatively stronger SRL component,</cell></row><row><cell>whereas Johansson and Nugues developed a bet-</cell></row><row><cell>ter parser. Second, several other systems improved</cell></row><row><cell>their ranking compared to Table 10: e.g., chen</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 16 :</head><label>16</label><figDesc>Labeled F 1 scores for frames centered around verbal and nominal predicates. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">LDC catalog number LDC2005T33.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We can do this because the LAS for syntactic dependencies is a special case of precision and recall, where the predicted number of dependencies is equal to the number of gold dependencies.4  We assign equal weight to the two tasks, i.e., W sem = 0.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">A correct evaluation of the stand-alone SRL systems would require the usage of gold syntactic dependencies, but these were not provided for the testing corpora.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://nlp.cs.nyu.edu/meyers/NomBank. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">PropBank I is used here. Later versions of PropBank mark instances of be in addition to other verbs. PropBank's use of the terms roleset and ARGM correspond approximately to sense and adjunct in common usage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The CoNLL-2008 website contains a Named Entity Token gazetteer to aid in this segmentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Under our approach to filler gap constructions, the filler is a shared argument (as in Relational Grammar, most Feature Structure and Dependency Grammar frameworks), in contrast with the Penn Treebank's empty category antecedent approach (more closely resembling the various Chomskian approaches).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">In very few situations, we select incorrect head tokens for multi-word predicates.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to thank the following people who helped us with the generation of the data sets: Jesús Giménez, for generating the predicted POS tags with his SVMTool POS tagger, and Massimiliano Ciaramita, for generating columns 1, 2 and 3 in the open-challenge corpus with his semantic tagger.</p><p>We also thank the following people who helped us with the organization of the shared task: Paola Merlo and James Henderson for the idea and the implementation of the Exact Match measure, Sebastian Riedel for his dependency visualization software, 12 Hai Zhao, for the the idea of the F 1 ratio score, and Carlos Castillo, for help with the shared task website. Last but not least, we thank the organizers of the previous four shared tasks: Sabine Buchholz, Xavier Carreras, Ryan McDonald, Amit Dubey, Johan Hall, Yuval Krymolowski, Sandra Kübler, Erwin Marsi, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. This shared task would not have been possible without their previous effort.</p><p>Mihai </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Experiments with a Higher-Order Projective Dependency Parser</title>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2007 Shared Task</title>
				<meeting>of CoNLL-2007 Shared Task</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2005</title>
				<meeting>of CoNLL-2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
				<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Cascaded Syntactic and Semantic Dependency Parsing System</title>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic Model for Syntactic and Semantic Dependency Parsing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno>MUC-7</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Named Entity Task Definition</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of Seventh Message Understanding Conference (MUC-7)</title>
				<meeting>of Seventh Message Understanding Conference (MUC-7)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lectures on Government and Binding</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Foris Publications</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the Shortest Arborescence of a Directed Graph</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science Sinica</title>
				<imprint>
			<date type="published" when="1965" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeSRL: A Linear-Time Semantic Role Labeling System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dell'orletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Broad Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimum Branchings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Research of the National Bureau of Standards</title>
				<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bilexical Grammars and Their Cubic-Time Parsing Algorithms. New Developments in Parsing Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Brown Corpus. Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuĉera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SVMTool: A general POS tagger generator based on Support Vector Machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
				<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling Using Dependency Trees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hacioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
				<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependencybased Syntactic-Semantic Analysis with PropBank and NomBank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extended Constituent-to-Dependency Conversion for English</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NODALIDA</title>
				<meeting>of NODALIDA</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Joint Model for Parsing Syntactic and Semantic Dependencies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Natural Language Parsing as Statistical Pattern Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Magerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: the Penn Treebank</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-Projective Dependency Parsing using Spanning Tree Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ribarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-EMNLP</title>
				<meeting>of HLT-EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Covering Treebanks with GLARF</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL/EACL 2001 Workshop on Sharing Tools and Resources for Research and Education</title>
				<meeting>of the ACL/EACL 2001 Workshop on Sharing Tools and Resources for Research and Education</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The NomBank Project: An Interim Report</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL/HLT 2004 Workshop Frontiers in Corpus Annotation</title>
				<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-X Shared Task</title>
				<meeting>of CoNLL-X Shared Task</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 Shared Task on Dependency Parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
				<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Malt-Parser: A language-independent system for datadriven dependency parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An Annotated Corpus of Semantic Roles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective Semantic Role Labelling with Markov Logic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Meza-Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mixing and Blending Syntactic and Semantic Dependencies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Samuelsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer Maximum Entropy Markov Models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong Kim San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F. De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2003</title>
				<meeting>of CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Applying Sentence Simplification to the CoNLL-2008 Shared Task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">BBN pronoun coreference and entity type corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Statistical Dependency Analysis with Support Vector Machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
				<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2008 Shared Task</title>
				<meeting>of CoNLL-2008 Shared Task</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Information Extraction from Multiple Syntactic Sources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>NYU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
