<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Basque Country Donostia, Basque Country</orgName>
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Google Inc. Mountain View, CA e George Washington University Washington</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Basque Country Donostia, Basque Country</orgName>
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Basque Country Donostia, Basque Country</orgName>
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic Textual Similarity (STS) seeks to measure the degree of semantic equivalence between two snippets of text. Similarity is expressed on an ordinal scale that spans from semantic equivalence to complete unrelatedness. Intermediate values capture specifically defined levels of partial similarity. While prior evaluations constrained themselves to just monolingual snippets of text, the 2016 shared task includes a pilot subtask on computing semantic similarity on cross-lingual text snippets. This year's traditional monolingual subtask involves the evaluation of English text snippets from the following four domains: Plagiarism Detection, Post-Edited Machine Translations, Question-Answering and News Article Headlines. From the questionanswering domain, we include both questionquestion and answer-answer pairs. The cross-lingual subtask provides paired Spanish-English text snippets drawn from the same sources as the English data as well as independently sampled news data. The English subtask attracted 43 participating teams producing 119 system submissions, while the crosslingual Spanish-English pilot subtask attracted 10 teams resulting in 26 systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Textual Similarity (STS) assesses the degree to which the underlying semantics of two segments of text are equivalent to each other. This assessment is performed using an ordinal scale that The authors of this paper are listed in alphabetic order. ranges from complete semantic equivalence to complete semantic dissimilarity. The intermediate levels capture specifically defined degrees of partial similarity, such as topicality or rough equivalence, but with differing details. The snippets being scored are approximately one sentence in length, with their assessment being performed outside of any contextualizing text. While STS has previously just involved judging text snippets that are written in the same language, this year's evaluation includes a pilot subtask on the evaluation of cross-lingual sentence pairs. The systems and techniques explored as a part of STS have a broad range of applications including Machine Translation (MT), Summarization, Generation and Question Answering (QA). STS allows for the independent evaluation of methods for computing semantic similarity drawn from a diverse set of domains that would otherwise be only studied within a particular subfield of computational linguistics. Existing methods from a subfield that are found to perform well in a more general setting as well as novel techniques created specifically for STS may improve any natural language processing or language understanding application where knowing the similarity in meaning between two pieces of text is relevant to the behavior of the system.</p><p>Paraphrase detection and textual entailment are both highly related to STS. However, STS is more similar to paraphrase detection in that it defines a bidirectional relationship between the two snippets being assessed, rather than the non-symmetric propositional logic like relationship used in textual entailment (e.g., P → Q leaves Q → P unspecified). STS also expands the binary yes/no catego-Score English Cross-lingual Spanish-English</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>The two sentences are completely equivalent, as they mean the same thing.</p><p>The bird is bathing in the sink. Birdie is washing itself in the water basin.</p><p>El pájaro se esta bañando en el lavabo. Birdie is washing itself in the water basin. 4</p><p>The two sentences are mostly equivalent, but some unimportant details differ.</p><p>In May 2010, the troops attempted to invade Kabul.</p><p>The US army invaded Kabul on May 7th last year, 2010.</p><p>En mayo de 2010, las tropas intentaron invadir Kabul.</p><p>The US army invaded Kabul on May 7th last year, 2010. 3</p><p>The two sentences are roughly equivalent, but some important information differs/missing. John said he is considered a witness but not a suspect. "He is not a suspect anymore." John said.</p><p>John dijo queél es considerado como testigo, y no como sospechoso. "He is not a suspect anymore." John said. 2</p><p>The two sentences are not equivalent, but share some details.</p><p>They flew out of the nest in groups.</p><p>They flew into the nest together.</p><p>Ellos volaron del nido en grupos. They flew into the nest together. 1</p><p>The two sentences are not equivalent, but are on the same topic.</p><p>The woman is playing the violin.</p><p>The young lady enjoys listening to the guitar.</p><p>La mujer está tocando el violín.</p><p>The young lady enjoys listening to the guitar. 0</p><p>The two sentences are completely dissimilar.</p><p>John went horse back riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.</p><p>Al amanecer, Juan se fue a montar a caballo con un grupo de amigos. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. rization of both paraphrase detection and textual entailment to a finer grained similarity scale. The additional degrees of similarity introduced by STS are directly relevant to many applications where intermediate levels of similarity are significant. For example, when evaluating machine translation system output, it is desirable to give credit for partial semantic equivalence to human reference translations.</p><p>Similarly, a summarization system may prefer short segments of text with a rough meaning equivalence to longer segments with perfect semantic coverage.</p><p>STS is related to research into machine translation evaluation metrics. This subfield of machine translation investigates methods for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME-TEOR, TER). <ref type="bibr">1</ref> The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation.</p><p>The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models <ref type="bibr" target="#b1">(Agirre et al., 2012;</ref><ref type="bibr" target="#b2">Agirre et al., 2013;</ref><ref type="bibr" target="#b3">Agirre et al., 2014;</ref><ref type="bibr" target="#b4">Agirre et al., 2015)</ref>. During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet <ref type="bibr" target="#b24">(Miller, 1995;</ref><ref type="bibr" target="#b10">Christiane Fellbaum, 1998)</ref>, FrameNet <ref type="bibr" target="#b6">(Baker et al., 1998)</ref>, OntoNotes <ref type="bibr" target="#b16">(Hovy et al., 2006)</ref>, web discussion forums, and Q&amp;A data sets). This year's evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluation set on Q&amp;A forum question-question similarity and revisit news headlines and Q&amp;A answer-answer similarity. The 2016 task includes both a traditional monolingual subtask with English data and a pilot cross-lingual subtask that pairs together Spanish and English texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Overview</head><p>STS presents participating systems with paired text snippets of approximately one sentence in length. The systems are then asked to return a numerical score indicating the degree of semantic similarity between the two snippets. Canonical STS scores fall on an ordinal scale with 6 specifically defined degrees of semantic similarity (see Table <ref type="table" target="#tab_0">1</ref>). While the underlying labels and their interpretation are ordinal, systems can provide real valued scores to indicate their semantic similarity prediction.</p><p>Participating systems are then evaluated based on the degree to which their predicted similarity scores correlate with STS human judgements. Algorithms are free to use any scale or range of values for the scores they return. They are not punished for outputting scores outside the range of the interpretable human annotated STS labels. This evaluation strategy is motivated by a desire to maximize the flexibility in the design of machine learning models and systems for STS. It reinforces the assumption that computing textual similarity is an enabling component for other natural language processing applications, rather than being an end in itself.</p><p>Table <ref type="table" target="#tab_0">1</ref> illustrates the ordinal similarity scale the shared task uses. Both the English and the crosslingual Spanish-English STS subtasks use a 6 point similarity scale. A similarity label of 0 means that two texts are completely dissimilar; this can be interpreted as two sentences with no overlap in their meanings. The next level up, a similarity label of 1, indicates that the two snippets are not equivalent but are topically related to each other. A label of 2 indicates that the two texts are still not equivalent but agree on some details of what is being said. The labels 3 and 4, both indicate that the two sentences are approximately equivalent. However, a score of 3 implies that there are some differences in important details, while a score of 4 indicates that the differing details are not important. The top score of 5, denotes that the two texts being evaluated have complete semantic equivalence.</p><p>In the context of the STS task, meaning equivalence is defined operationally as two snippets of text that mean the same thing when interpreted by a reasonable human judge. The operational approach to sentence level semantics was popularized by the recognizing textual entailment task <ref type="bibr" target="#b12">(Dagan et al., 2010)</ref>. It has the advantage that it allows the labeling of sentence pairs by human annotators without any training in formal semantics, while also being more useful and intuitive to work with for downstream systems. Beyond just sentence level semantics, the operationally defined STS labels also reflect both world knowledge and pragmatic phenomena.</p><p>As in prior years, 2016 shared task participants are allowed to make use of existing resources and tools (e.g., WordNet, <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref>'s word2vec). Participants are also allowed to make unsupervised use of arbitrary data sets, even if such data overlaps with the announced sources of the evaluation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">English Subtask</head><p>The English subtask builds on four prior years of English STS tasks. Task participants are allowed to use all of the trial, train and evaluation sets released during prior years as training and development data. As shown in table 2, this provides 14,250 paired snippets with gold STS labels. The 2015 STS task annotated between 1,500 and 2,000 pairs per data set that were then filtered based on annotation agreement and to achieve better data set balance. The raw annotations were released after the evaluation, providing an additional 5,500 pairs with noisy STS annotations (8,500 total for 2015). <ref type="bibr">2</ref> The   <ref type="bibr">(2012,</ref><ref type="bibr">2013,</ref><ref type="bibr">2014,</ref><ref type="bibr">2015)</ref> and test (2016) data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>The data for the English evaluation sets are collected from a diverse set of sources. Data sources are selected that correspond to potentially useful domains for application of the semantic similarity methods explored in STS systems. This section details the pair selection heuristics as well as the individual data sources we use for the evaluation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Selection Heuristics</head><p>Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP   Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap <ref type="bibr" target="#b19">(Lin, 1998)</ref>. As shown in equation ( <ref type="formula" target="#formula_0">1</ref>), surface level lexical similarity between two snippets s 1 and s 2 is computed as a log probability weighted sum of the words common to both snippets divided by a log probability weighted sum of all the words in the two snippets. sim l (s 1 , s 1 ) = 2 × w∈s 1 ∩s 2 log P (w)</p><formula xml:id="formula_0">w∈s 1 log P (w) + w∈s 2 log P (w)<label>(1)</label></formula><p>Unigram probabilities are estimated over the evaluation set data sources and are computed without any smoothing.</p><p>Word Embedding Similarity As our second heuristic, we compute the cosine between a simple embedding space representation of the two text snippets. Equation (2) illustrates the construction of the snippet embedding space representation, v(s), as the sum of the embeddings for the individual words, v(w), in the snippet. The cosine similarity can then be computed as in equation ( <ref type="formula">3</ref>).</p><formula xml:id="formula_1">v(s) = w∈s v(w) (2) sim v (s 1 , s 2 ) = v(s 1 )v(s 2 ) ||v(s 1 )||||v(s 2 )|| (3)</formula><p>Three hundred dimensional word embeddings are obtained by running the GloVe package <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref> with default parameters over all the data collected from the 2016 evaluation sources. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Newswire Headlines</head><p>The newswire headlines evaluational set is collected from the Europe Media Monitor (EMM) <ref type="bibr" target="#b7">(Best et al., 2005)</ref> using the same extraction approach taken for STS 2015 <ref type="bibr" target="#b4">(Agirre et al., 2015)</ref> over the date range July 28, 2014 to April 10, 2015. The EMM clusters identify related news stories. To construct the STS pairs, we extract 749 pairs of headlines that appear in same cluster and 749 pairs of headlines associated with stories that appear in different clusters. For both groups, we use a surface level string similarity metric found in the Perl package String::Similarity <ref type="bibr" target="#b25">(Myers, 1986)</ref> to select an equal number of pairs with high and low surface similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Plagiarism</head><p>The plagiarism evaluation set is based on Clough and Stevenson (2011)'s Corpus of Plagiarised Short Answers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles. <ref type="bibr">4</ref> The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Postediting</head><p>The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system <ref type="bibr" target="#b18">(Koehn et al., 2007)</ref> paired with postedited corrections of those translations. <ref type="bibr">5</ref> The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form.</p><p>4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Theorem from probability theory, E. What is dynamic programming? <ref type="bibr">5</ref> The corpus also includes English news data machine translated into Spanish and the postedited corrections of these translations. We use the English-Spanish data in the cross-lingual task.</p><p>number of changes necessary to produce a publishable translation. STS pairs for this evaluation set are selected both using the surface form and embedding space pairing heuristics and by including the existing explicit pairs of each machine translation with its postedited correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Question-Question &amp; Answer-Answer</head><p>The question-question and answer-answer evaluation sets are extracted from the Stack Exchange Data Dump <ref type="bibr">(Stack Exchange, Inc., 2016)</ref>. The data include long form Question-Answer pairs on a diverse set of topics ranging from highly technical areas such as programming, physics and mathematics to more casual topics like cooking and travel.</p><p>Pairs are constructed using questions and answers from the following less technical Stack Exchange sites: academia, cooking, coffee, diy, english, fitness, health, history, lifehacks, linguistics, money, movies, music, outdoors, parenting, pets, politics, productivity, sports, travel, workplace and writers. Since both the questions and answers are long form, often being a paragraph in length or longer, heuristics are used to select a one sentence summary of each question and answer. For questions, we use the title of the question when it ends in a question mark. <ref type="bibr">6</ref> For answers, a one sentence summary of each question is constructed using LexRank <ref type="bibr" target="#b13">(Erkan and Radev, 2004)</ref> as implemented by the Sumy 7 package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-lingual Subtask</head><p>The pilot cross-lingual subtask explores the expansion of STS to paired snippets of text in different languages. The 2016 shared task pairs snippets in Spanish and English, with each pair containing exactly one Spanish and one English member. A trial set of 103 pairs was released prior to the official evaluation window containing pairs of sentences randomly selected from prior English STS evaluations, but with one of the snippets being translated into Spanish by human translators. <ref type="bibr">8</ref> The similarity scores associated with this set are taken from the manual STS annotations within the original English data. Participants are allowed to use the labeled STS pairs from any of the prior STS evaluations. This includes STS pairs from all four prior years of the English STS subtasks as well as data from the 2014 and 2015 Spanish STS subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Collection</head><p>The cross-lingual evaluation data is partitioned into two evaluation sets: news and multi-source. The news data set is manually harvested from multilingual news sources, while the multi-source dataset is sampled from the same sources as the 2016 English data, with one of the snippets being translated into Spanish by human translators. <ref type="bibr">9</ref> As shown in Table <ref type="table" target="#tab_4">3</ref>, the news set has 301 pairs, while the multi-source set has 294 pairs. For the news evaluation set, participants are provided with exactly the 301 pairs that will be used for the final evaluation. For the multisource dataset, we take the same approach as the English subtask and release 2,973 pairs for annotation by participant systems, without providing information on what pairs will be included in the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Cross-lingual News</head><p>The cross-lingual news dataset is manually culled from less mainstream news sources such as Russia a disadvantage, and rather systems that actually explore semantic information receive due credit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Multi-source</head><p>The raw multi-source data sets annotated by participating systems are constructed by first sampling 250 pairs from each of the following four data sets from the English task: Answer-Answer, Plagiarism, Question-Question and Headlines. One sentence from each sampled pair is selected at random for translation into Spanish by human translators. <ref type="bibr">11</ref> An additional 1973 pairs are drawn from the English-Spanish section of EAMT 2011. We include all pairings of English source sentences with their human post-edited Spanish translations, resulting in 1000 pairs. We also include pairings of English source sentences with their Spanish machine translations. This only produced an additional 973 pairs, since 27 of the pairs are already generated by human postedited translations that exactly match their corresponding machine translations. The gold standard data are selected by randomly drawing 60 pairs belonging to each data set within the raw multi-source data, except for EMM where only 54 pairs were drawn. <ref type="bibr">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Annotation</head><p>Annotation of pairs with STS scores is performed using crowdsourcing on Amazon Mechanical Turk. <ref type="bibr">13</ref> This section describes the templates and annotation parameters we use for the English and cross-lingual Spanish-English pairs, as well as how the gold standard annotations are computed from multiple annotations from crowd workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English Subtask</head><p>The annotation instructions for the English subtask are modified from prior years in order to accommodate the annotation of question-question pairs. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the new instructions. References to statements are replaced with snippets. The new instructions remove the wording suggesting that anno-11 Inspection of the data suggests the translation service provider may have used a postediting based process.</p><p>12 Our annotators work on batches of 7 pairs. Drawing 54 pairs from the EMM data results in a total number pairs that is cleanly divisible by 7.</p><p>13 https://www.mturk.com/</p><p>tators "picture what is being described" and provide tips for navigating the annotation form quickly. The annotation form itself is also modified from prior years to make use of radio boxes to annotate the similarity scale rather than drop-down lists.</p><p>The English STS pairs are annotated in batches of 20 pairs. For each batch, annotators are paid $1 USD. Five annotations are collected per pair. Only workers with the MTurk master qualification are allowed to perform the annotation, a designation by the MTurk platform that statistical identifies workers who perform high quality work across a diverse set of MTurk tasks. Gold annotations are selected as the median value of the crowdsourced annotations after filtering out low quality annotators. We remove annotators with correlation scores &lt; 0.80 using a simulated gold annotation computed by leaving out the annotations from the worker being evaluated. We also exclude all annotators with a kappa score &lt; 0.20 against the same simulated gold standard.</p><p>The official task evaluation data are selected from pairs having at least three remaining labels after excluding the low quality annotators. For each evaluation set, we attempt to select up to 42 pairs for each STS label. Preference was given to pairs with a higher number of STS labels matching the median label. After the final pairs are selected, they are spot checked with some of the pairs having their STS score corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-lingual Spanish-English Subtask</head><p>The Spanish-English pairs are annotated using a slightly modified template from the 2014 and 2015 Spanish STS subtask. Given the multilingual nature of the subtask, the guidelines consist of alternating instructions in either English or Spanish, in order to dissuade monolingual annotators from participating (see Figure <ref type="figure" target="#fig_1">2</ref>). The template is also modified to use the same six point scale used by the English subtask, rather than the five point scale used in the Spanish subtasks in the past (which did not attempt to distinguish between differences in unimportant details). Judges are also presented with the cross-lingual example pairs and explanations listed on Table <ref type="table" target="#tab_0">1</ref>.</p><p>The cross-lingual pairs are annotated in batches of 7 pairs. Annotators are paid $0.30 USD per batch and each batch receives annotations from 5 workers. The annotations are restricted to workers who have completed 500 HITs on the MTurk platform and have less than 10% of their lifetime annotations rejected. The gold standard is computed by averaging over the 5 annotations collected for each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">System Evaluation</head><p>This section reports the evaluation results for the 2016 STS English and cross-lingual Spanish-English subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Participation</head><p>Participating teams are allowed to submit up to three systems. <ref type="bibr">14</ref> For the English subtask, there were 119 systems from 43 participating teams. The crosslingual Spanish-English subtask saw 26 submissions from 10 teams. For the English subtask, this is a 45% increase in participating teams from 2015. The Spanish-English STS pilot subtask attracted approximately 53% more participants than the monolingual Spanish subtask organized in 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metric</head><p>On each test set, systems are evaluated based on their Pearson correlation with the gold standard STS labels. The overall score for each system is computed as the average of the correlation values on the individual evaluation sets, weighted by the number of data points in each evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baseline</head><p>Similar to prior years, we include a baseline built using a very simple vector space representation. For this baseline, both text snippets in a pair are first tokenized by white-space. The snippets are then projected to a one-hot vector representation such that each dimension corresponds to a word observed in one of the snippets. If a word appears in a snippet one or more times, the corresponding dimension in the vector is set to one and is otherwise set to zero. The textual similarity score is then computed as the cosine between these vector representations of the two snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">English Subtask</head><p>The rankings for the English STS subtask are given in Tables <ref type="table" target="#tab_6">4 and 5</ref>. The baseline system ranked 100th. Table <ref type="table" target="#tab_8">6</ref> provides the best and median scores for each of the individual evaluation sets as well as overall. <ref type="bibr">15</ref> The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set.</p><p>The best overall performance is obtained by Samsung Poland NLP Team's EN1 system, which achieves an overall correlation of 0.778 <ref type="bibr" target="#b29">(Rychalska et al., 2016)</ref>. This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH's Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687.    EN2 achieves the best correlation on the questionquestion data at 0.747. The most difficult data sets this year are the two Q&amp;A evaluation sets: answer-answer and questionquestion. Difficulty on the question-question data was expected as this is the first year that questionquestion pairs are formally included in the evaluation of STS systems. <ref type="bibr">16</ref> Interestingly, the baseline system has particular problems on this data, achieving a correlation of only 0.038. This suggests that surface overlap features might be less informative on this data, possibly leading to prediction errors by the systems that include them. An answer-answer evaluation set was previously included in the 2015 STS task. Answer-answer data is included again in this year's evaluation specifically because of the poor performance observed on this type of data in 2015.</p><p>In Table <ref type="table" target="#tab_8">6</ref>, it can be seen that the difficult Q&amp;A data sets are also the data sets that exhibit the biggest difference between the top performing system for that evaluation set and typical system performance, as capture by the median on the same data. For the easier data sets, news headlines, plagiarism, and postediting, there is only a relatively modest gap between the best system for that data set and typical system performance ranging from about 0.05 to 0.06. However, on the difficult Q&amp;A data set the difference between the best system and typical systems jumps to 0.212 for answer-answer and 0.170 for question-question. This suggests these harder data sets may be better at discriminating between different approaches with most systems now being fairly competent on assessing easier pairs. 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Methods</head><p>Participating systems vary greatly in the approaches they take to solving STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet <ref type="bibr" target="#b29">(Rychalska et al., 2016)</ref>. To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of <ref type="bibr" target="#b31">Sultan et al. (2015)</ref>'s very successful STS model enhanced with additional features found to work well in the literature.</p><p>The team in second place overall, UWB, combines a large number of diverse similarity models and features <ref type="bibr" target="#b9">(Brychcin and Svoboda, 2016)</ref>. Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, May-oNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model <ref type="bibr" target="#b0">(Afzal et al., 2016)</ref>. Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word <ref type="bibr" target="#b28">(Resnik, 1995)</ref> with a deep learning method known as Deep Structured Semantic Model (DSSM) <ref type="bibr" target="#b17">(Huang et al., 2013)</ref>.</p><p>The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam <ref type="bibr" target="#b32">(Tian and Lan, 2016;</ref><ref type="bibr" target="#b27">Przybyła et al., 2016)</ref>.</p><p>The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model <ref type="bibr" target="#b15">(He et al., 2016)</ref>. The team extends a multi-perspective convolutional neural network (MPCNN) <ref type="bibr" target="#b14">(He et al., 2015)</ref> with a simple word level attentional mecha-nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering.</p><p>Finally, the best performing system on the postediting data, RICOH's Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Cross-lingual Spanish-English Subtask</head><p>The rankings for the cross-lingual Spanish-English STS subtask are provided in Table <ref type="table" target="#tab_10">7</ref>. Recall that the multi-source data is drawn from the same sources as the monolingual English STS pairs. It is interesting to note that the performance of cross-lingual systems on this evaluation set does not appear to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems.</p><p>In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB <ref type="bibr" target="#b9">(Brychcin and Svoboda, 2016)</ref>. The unsupervised UWB system builds on the word alignment based STS method proposed by <ref type="bibr" target="#b31">Sultan et al. (2015)</ref>. However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that achieves 0.808.</p><p>Approximately half of the submissions are able to achieve a correlation above 0.8 on the news data. On the multi-source data, the overall correlation trend is lower, but with half the systems still obtaining a score greater than 0.6. Due to the diversity of the material embedded in the multi-source data, it seems to amount to a more difficult testing scenario.</p><p>Nonetheless, there are cases of: 1) systems performing much worse on the news data set: the FBK HLT-MT systems experience an approximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Methods</head><p>In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team <ref type="bibr" target="#b21">(Lo et al., 2016</ref>) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric <ref type="bibr" target="#b20">(Lo et al., 2014)</ref>, and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team <ref type="bibr" target="#b5">(Ataman et al., 2016)</ref> proposes a model combining cross-lingual word embeddings with features from QuEst <ref type="bibr">(Specia et al., 2013)</ref>, a tool for machine translation quality estimation. The RTM system <ref type="bibr" target="#b8">(Biçici, 2016</ref>) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess sentences originating in different languages. <ref type="bibr">18</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented the results of the 2016 STS shared task. This year saw a significant increase in participation. There are 119 submissions from 43 participating teams for the English STS subtask. This is a 45% increase in participating teams over 2015. The pilot cross-lingual Spanish-English STS subtask has 26 submissions from 10 teams, which is impressive given that this is the first year such a challenging subtask was attempted. Interestingly, the cross-lingual STS systems appear to perform competitively to monolingual systems on pairs drawn from the same sources. This suggests that it would be interesting to perform a more direct comparison between cross-lingual and monolingual systems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotation instructions for the STS English subtask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Annotation instructions for the STS Spanish-English subtask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Similarity scores with explanations and examples for the English and the cross-lingual Spanish-English subtasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>English subtask: Train</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Spanish-English subtask: Trial and test data sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>STS 2016: English Rankings. Late or corrected systems are marked with a * symbol.</figDesc><table><row><cell>Team</cell><cell>Run</cell><cell>ALL</cell><cell>Ans.-Ans.</cell><cell>HDL</cell><cell>Plagiarism</cell><cell>Postediting</cell><cell>Ques.-Ques.</cell><cell>Run Rank</cell><cell>Team Rank</cell></row><row><cell>UTA MLNLP SimiHawk NORMAS UTA MLNLP LIPN-IIMAS NORMAS JUNITMZ NSH LIPN-IIMAS USFD HHU ASOBEK ASOBEK LIPN-IIMAS Telkom University BIT 3CFEE ASOBEK JUNITMZ VRep Meiji WSL teamC JUNITMZ VRep VRep UNCC Telkom University NORMAS STS Organizers meijiuniversity teamb meijiuniversity teamb meijiuniversity teamb Amrita CEN VENSESEVAL UNCC UNCC WHU NLP 3CFEE WHU NLP DalGTM DalGTM DalGTM WHU NLP IHS-RD-Belarus</cell><cell>150-1 TreeLSTM SV-2 150-3 SOPA ECV-3 Backpropagation-1 Run2 SOPA1000 Word2Vec DeepLDA T11 M11 SOPA100 WA WeightedVecSim grumlp F1 Recurrent-1 withLeven Run1 FeedForward-1 withStopRem noStopRem Run-3 CS RF-1 baseline 4featuresˆLCS pos 5features 4featuresˆLCS SEWE-2 Run1 Run-1 Run-2 CNN20 bowkst CNN10 Run1 Run2 Run3 CNN5 Run3</cell><cell>0.64500 0.64140 0.64078 0.63698 0.63087 0.63072 0.62708 0.62941 0.62466 0.62254 0.62078 0.61782 0.61430 0.61321 0.60912 0.59560 0.59603 0.59556 0.59493 0.58292 0.58169 0.58109 0.57805 0.55894 0.55789 0.51602 0.50895 0.51334 0.45748 0.45656 0.45621 0.40951 0.44258 0.40359 0.37956 0.11100 0.33174 0.09814 0.05354 0.05136 0.05027 0.04713</cell><cell>0.43042 0.52277 0.36583 0.41871 0.44901 0.27637 0.48023 0.34172 0.44893 0.27675 0.47211 0.52277 0.47916 0.43216 0.28859 0.37565 0.36521 0.42692 0.44218 0.30617 0.53250 0.40859 0.29487 0.34684 0.31111 0.06623 0.16095 0.41133 0.26182 0.26498 0.26588 0.30309 0.41932 0.19537 0.18100 -0.02488 0.20076 0.05402 -0.00557 -0.00810 -0.00780 0.00041</cell><cell>0.72133 0.74083 0.68864 0.72485 0.62411 0.72245 0.70749 0.74977 0.59721 0.64217 0.58821 0.63741 0.68652 0.58499 0.69988 0.55925 0.72092 0.67898 0.66120 0.68745 0.64567 0.66524 0.68185 0.67856 0.59592 0.72668 0.58800 0.54073 0.60223 0.60000 0.59868 0.43164 0.62738 0.49344 0.50924 0.16026 0.42288 0.19650 0.29896 0.28962 0.28501 0.13780</cell><cell>0.71620 0.67628 0.74647 0.70296 0.69109 0.72496 0.72075 0.75858 0.75936 0.78755 0.62503 0.78521 0.77779 0.74727 0.69090 0.75594 0.74210 0.75717 0.73708 0.69762 0.74233 0.76752 0.69730 0.69768 0.64672 0.50534 0.62134 0.69601 0.55931 0.55231 0.55687 0.63336 0.64538 0.38881 0.26190 0.16854 0.50150 0.08949 -0.07016 -0.07032 -0.07134 0.03264</cell><cell>0.74471 0.70655 0.80234 0.69652 0.79864 0.79797 0.77196 0.82471 0.76157 0.75057 0.84743 0.84245 0.84089 0.75560 0.74654 0.77835 0.76327 0.81950 0.69279 0.73033 0.54783 0.66522 0.72966 0.74088 0.75544 0.73999 0.72016 0.82615 0.66989 0.66894 0.66690 0.66465 0.27274 0.59235 0.56176 0.24153 0.19111 0.16152 -0.04933 -0.04818 -0.05014 0.12669 0.83761</cell><cell>0.62006 0.55265 0.61300 0.65543 0.59779 0.65312 0.43751 0.46548 0.56285 0.68833 0.57099 0.26352 0.24804 0.55310 0.64009 0.51643 0.37179 0.26181 0.43092 0.49639 0.42797 0.38711 0.49029 0.30908 0.48409 0.56194 0.46743 0.03844 0.16276 0.16518 0.16103 -0.03174 0.22581 0.34548 0.38317 0.00176 0.35970 -0.02989 0.08924 0.08987 0.09223 -0.08105</cell><cell>76 77 78 79 80 81 82 82* 83 84 85 86 87 88 89 90 90* 91 92 93 94 95 96 97 98 99 100 100  † 101 102 103 104 104* 105 106 107 107* 108 109 110 111 112 113</cell><cell>29 30 31 32 33 34* 34 35 36 37  † 37 38 38* 39 40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>STS 2016: English Rankings (continued). As before, late or corrected systems are marked with a * symbol. The baseline run by the organizers is marked with a † symbol (at rank 100).</figDesc><table><row><cell cols="2">Evaluation Set Best</cell><cell>Median</cell><cell>∆</cell></row><row><cell>ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques.</cell><cell cols="3">0.77807 0.69347 0.08460 0.69235 0.48067 0.21169 0.82749 0.76439 0.06311 0.84138 0.79445 0.04694 0.86690 0.81272 0.05418 0.74705 0.57673 0.17032</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Best and median scores by evaluation set for the English subtask as well as the difference between the two, ∆.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>STS 2016: Cross-lingual Spanish-English Rankings</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> Answers-forums: 2000; Answers-students: 1500; Belief:  2000; Headlines: 1500; Images: 1500.   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The evaluation source data contained only 10,352,554 tokens. This is small relative to the data sets used to train embedding space models that typically make use of &gt; 1B tokens. However, the resulting embeddings are found to be functionally</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Questions with titles not ending in a "?" are discarded. 7 https://pypi.python.org/pypi/sumy 8 SDL was used to translate the trial data set, http:// sdl.com.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The evaluation set was translated into Spanish by Gengo: http://gengo.com/ Today 10 , in order to pose a more natural challenge in terms of machine translation accuracy. Articles on the same or differing topics are collected, with particular effort being spent to find articles on the same or somewhat similar story (many times written by the same author in English and Spanish), that exhibit a natural writing pattern in each language by itself and do not amount to an exact translation. When compared across the two languages, such articles exhibit different sentence structure and length. Additional paragraphs are also included by the writer that would cater to the readers' interests. For example, in the case of articles written about the Mexican drug lord Joaquin "El Chapo" Guzman, who was recently captured, the English articles typically have less extraneous details, focusing more on facts, while the articles written in Spanish provide additional background information with more narrative. Such articles allow for the manual extraction of high quality pairs that enable a wider variety of testing scenarios: from exact translations, to paraphrases exhibiting a different sentence structure, to somewhat similar sentences, to sentences sharing common vocabulary but no topic similarity, and ultimately to completely unrelated sentences. This ensures that semantic similarity systems that rely heavily on lexical features (which have been also typically used in STS tasks to derive test and train datasets) are at 10 English: https://www.rt.com/, Spanish: https: //actualidad.rt.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">For the English subtask, the SimiHawk team was granted permission to make 4 submissions as the team has 3 submissions using distinct machine learning models (feature engineered, LSTM, and Tree LSTM) and they asked to separately submit an ensemble of the three methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">The pair selection criteria from prior years did not explicitly exclude the presence of questions or question-question pairs. Of the 19,189 raw pairs from prior English STS evaluations as trial, train or evaluation data, 831 (4.33%) of the pairs include a question mark within at least one member of the pair, while 319 of the pairs (1.66%) include a question mark within both members.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">To see how much of this is related to the Q&amp;A domain in particular, we will investigate including difficult non-Q&amp;A evaluation data in future STS competitions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">The GWU NLP team includes one of the STS organizers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based in part upon work supported by DARPA-BAA-12-47 DEFT grant to George Washington University, by DEFT grant #12475008 to the University of Michigan, and by a MINECO grant to the University of the Basque Country (TUNER project TIN2015-65308-C5-1-R). Aitor Gonzalez Agirre is supported by a doctoral grant from MINECO (PhD grant FPU12/06243). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MayoNLP at SemEval-2016 Task 1: Semantic textual similarity based on lexical semantic net and deep learning semantic model</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
				<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic Textual Similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
				<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>Denver, CO; San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of the 9th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FBK HLT-MT at SemEval-2016 Task 1: Cross-lingual semantic similarity measurement using quality estimation features and compositional bilingual word embeddings</title>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ataman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco Turchi1 Matteo</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet Project</title>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;98 Proceedings of the 17th international conference on Computational linguistics</title>
				<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Europe Media Monitor -System description</title>
		<author>
			<persName><forename type="first">Clive</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Blackler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUR Report 22173-En</title>
				<meeting><address><addrLine>Ispra, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Teófilo Garcia, and David Horby</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RTM at SemEval-2016 Task 1: Predicting semantic similarity with referential translation machines and related statistics</title>
		<author>
			<persName><forename type="first">Ergun</forename><surname>Biçici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UWB at SemEval-2016 Task 1: Semantic textual similarity using lexical, syntactic, and semantic information</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Brychcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Svoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Developing a corpus of plagiarised short answers. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rational, evaluation and approaches</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="105" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="Portu" to=" gal" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UMD-TTIC-UW at SemEval-2016 Task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL</title>
				<meeting>the Human Language Technology Conference of the North American Chapter of the ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">Hideo</forename><surname>Itoh ; San Diego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Usa</forename><surname>Philipp Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
				<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07<address><addrLine>Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>RICOH at SemEval-2016 Task 1: Irbased semantic textual similarity estimation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning, ICML &apos;98</title>
				<meeting>the Fifteenth International Conference on Machine Learning, ICML &apos;98<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">XMEANT: Better semantic MT evaluation without reference translations</title>
		<author>
			<persName><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Saers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CNRC at SemEval-2016 Task 1: Experiments in crosslingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ano(nd) difference algorithm and its variations</title>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NaCTeM at SemEval-2016 Task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Przybyła</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nhung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Shardlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Kontonatsios</surname></persName>
		</author>
		<author>
			<persName><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 14th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;95</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Samsung Poland NLP Team at SemEval-2016 Task 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensemble methods to measure semantic similarity</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Pakulska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krystyna</forename><surname>Chodorowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Andruszkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
			<persName><forename type="first">Kashif</forename><surname>Shah</surname></persName>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Jose</surname></persName>
			<persName><forename type="first">Trevor</forename><surname>De Souza</surname></persName>
			<persName><surname>Cohn</surname></persName>
		</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting objective annotations for measuring translation post-editing effort</title>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="https://archive.org/details/stackexchange" />
	</analytic>
	<monogr>
		<title level="m">15th Conference of the European Association for Machine Translation, EAMT</title>
				<meeting><address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Stack Exchange, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
	<note>2016. Stack exchange data dump</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DLS@CU: Sentence similarity from word alignment and semantic vector composition</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Md Arafat Sultan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Sumner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ECNU at SemEval-2016 Task 1: Leveraging word embedding from macro and micro views to boost performance for semantic textual similarity</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
