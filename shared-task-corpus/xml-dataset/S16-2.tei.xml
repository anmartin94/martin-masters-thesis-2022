<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2016 Task 2: Interpretable Semantic Textual Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
							<email>montse.maritxalar@ehu.eus</email>
						</author>
						<author>
							<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>June 16-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">IXA NLP group, University of the Basque Country Manuel Lardizabal</orgName>
								<address>
									<addrLine>1, 20.018 Donostia</addrLine>
									<settlement>Basque Country</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2016 Task 2: Interpretable Semantic Textual Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The final goal of Interpretable Semantic Textual Similarity (iSTS) is to build systems that explain which are the differences and commonalities between two sentences. The task adds an explanatory level on top of STS, formalized as an alignment between the chunks in the two input sentences, indicating the relation and similarity score of each alignment. The task provides train and test data on three datasets: news headlines, image captions and student answers. It attracted nine teams, totaling 20 runs. All datasets and the annotation guideline are freely available 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Textual Similarity (STS) <ref type="bibr" target="#b2">(Agirre et al., 2015)</ref> measures the degree of equivalence in the underlying semantics of paired snippets of text. The idea of Interpretable STS (iSTS) is to explain why two sentences may be related/unrelated, by supplementing the STS similarity score with an explanatory layer.</p><p>Our final goal would be to enable interpretable systems, that is, systems that are able to explain which are the differences and commonalities between two sentences. For instance, let's assume the following two sentences drawn from a corpus of news headlines: 12 killed in bus accident in Pakistan 10 killed in road accident in NW Pakistan * * Authors listed in alphabetical order 1 http://at.qrci.org/semeval2016/task2/</p><p>The output of such a system would be something like the following:</p><p>The two sentences talk about accidents with casualties in Pakistan, but they differ in the number of people killed (12 vs. 10) and level of detail: the first one specifies that it is a bus accident, and the second one specifies that the location is NW Pakistan.</p><p>While giving such explanations comes naturally to people, constructing algorithms and computational models that mimic human level performance represents a difficult Natural Language Understanding (NLU) problem, with applications in dialogue systems, interactive systems and educational systems.</p><p>In the iSTS 2015 pilot task <ref type="bibr" target="#b2">(Agirre et al., 2015)</ref>, we defined a first step of such an ambitious system, which we follow in 2016. Given the input (a pair of sentences), participant systems need first to identify the chunks in each sentence, and then, align chunks across the two sentences, indicating the relation and similarity score of each alignment. The relation can be one of equivalence, opposition, specificity, similarity or relatedness, and the similarity score can range from 1 to 5. Unrelated chunks are left unaligned. An optional tag can be added to alignments for the cases where there is a difference in factuality or polarity. See Figure <ref type="figure">1</ref> for the manual alignment of the two sample sentences. The alignments between chunks in Figure <ref type="figure">1</ref> can be used to produce the kind of explanations shown in the previous example.</p><p>In previous work, <ref type="bibr" target="#b5">Brockett (2007)</ref> and <ref type="bibr" target="#b20">Rus et al. (2012)</ref> produced a dataset where corresponding Figure <ref type="figure">1</ref>: Example of a manual alignment of two sentences: "12 killed in bus accident in Pakistan" and "10 killed in road accident in NW Pakistan". Each aligned pair of chunks included information on the type of alignment, and the score of alignment. words (including some multiword expressions like named-entities) were aligned. Although this alignment is useful, we wanted to move forward to the alignment of segments, and decided to align chunks <ref type="bibr" target="#b0">(Abney, 1991)</ref>. <ref type="bibr" target="#b5">Brockett (2007)</ref> did not provide any label to alignments, while Rus et al. ( <ref type="formula">2012</ref>) defined a basic typology. In our task, we provided a more detailed typology for the aligned chunks as well as a similarity/relatedness score for each alignment. Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them.</p><p>In a different strand of work, <ref type="bibr" target="#b17">Nielsen et al. (2009)</ref> defined a textual entailment model where the "facets" (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by <ref type="bibr" target="#b13">Levy et al. (2013)</ref>. Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks.</p><p>The SemEval Semantic Textual Similarity (STS) task in 2015 contained a subtask on Interpretable STS <ref type="bibr" target="#b2">(Agirre et al., 2015)</ref>, showing that the task is feasible, with high inter-annotator agreement and system scores well above baselines. The datasets comprised news headlines and image captions.</p><p>For 2016, the pilot subtask has been updated into a standalone task. The restriction from the iSTS 2015 task to allow only one-to-one alignments has been now lifted, and we thus allow any number of chunks to be aligned to any number of chunks. Annotation guidelines have been revised accordingly, including an updated chunking criterium for subordinate clauses and a better explanation of the instruc-tions.</p><p>The 2015 datasets were re-annotated and released as training data. New pairs from news headlines and image captions have been annotated and used for test. In addition, a new dataset of sentence pairs from the education domain has been produced, including train and test data.</p><p>The paper is organized as follows. We first provide the description of the task, followed by the evaluation metrics and the baseline system. Section 5 describes the participation, Section 6 the results, and Section 7 comments on the systems, tools and resources used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The dataset was produced using sentence pairs from news headlines, image captions and answers from students. Headlines have been mined from several news sources by European Media Monitor, and collected by us using their RSS feed 2 . We saw a pair of headlines from this corpus in the introduction.</p><p>The Image descriptions dataset is a subset of the Flickr dataset presented in <ref type="bibr" target="#b19">(Rashtchian et al., 2010)</ref>, which consisted of 8108 hand-selected images from Flickr, depicting actions and events of people or animals, with five captions per image. The image captions of the dataset are released under a Creative Commons Attribution-Share Alike license. This is a sample pair from this dataset:</p><p>A man sleeps with a baby in his lap A man asleep in a chair holding a baby The Answer-Students corpus consists of the interactions between students and the BEETLE II tutorial dialogue system. The BEETLE II system is an intelligent tutoring engine that teaches students in basic electricity and electronics.  spend from three to five hours reading the material, building and observing circuits in the simulator and interacting with a dialogue-based tutor. They used the keyboard to interact with the system, and the computer tutor asked them questions and provided feedback via a text-based chat interface. The data from 73 undergraduate volunteer participants at south-eastern US university were recorded and annotated to form the BEETLE human-computer dialogue corpus <ref type="bibr" target="#b6">(Dzikovska et al., 2010;</ref><ref type="bibr" target="#b7">Dzikovska et al., 2012)</ref>, and later used in a SemEval 2015 task <ref type="bibr" target="#b8">(Dzikovska et al., 2013)</ref>. In the present corpus, we include sentence pairs composed of a student answer and the reference answer of a teacher. We have rejected those answers containing pronouns whose antecedent is not in the sentence (pronominal coreference), as the question is not included in the train data and, therefore, it is not possible to deduce which is the antecedent. There are also some dataset-specific details that are mentioned in the same section. The next pair sentences are an example of the Answer-Students corpus.</p><p>because switch z is in bulb c's closed path there is a path containing both Z and C</p><p>All datasets have been previously used in STS tasks. Table <ref type="table" target="#tab_1">1</ref> shows details of the datasets, including train-test splits. The Headlines and Images datasets are tokenized, as in the STS release. The Answer-Students dataset was not tokenized, and was used as in the STS release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation</head><p>The manual annotation has been performed following the annotation guidelines 3 . Please refer to those 3 http://alt.qcri.org/ semeval2016/task2/data/uploads/ guidelines for further details. The general annotation procedure is as follows:</p><p>1. First identify the chunks in each sentence separately.</p><p>2. Align chunks in order, from the clearest and strongest correspondences to the most unclear or weakest ones.</p><p>3. For each alignment, provide a similarity/relatedness score.</p><p>4. For each alignment, choose one (or more) alignment label.</p><p>Chunk annotation was based on those used in the CoNLL 2000 chunking task <ref type="bibr" target="#b22">(Tjong Kim Sang and Buchholz, 2000)</ref>. The annotators were provided with the output of an automatic chunker 4 trained on the CoNLL corpora 5 , which they corrected manually.</p><p>Independently of the labels, and before assigning any label, the annotators need to provide a similarity/relatedness score for each alignment from 5 (maximum similarity/relatedness) to 0 (no relation at all), as follows: 5 if the meaning of both chunks is equivalent <ref type="bibr">[4,</ref><ref type="bibr">3]</ref> if the meaning of both chunks is very similar or closely related [2,1] if the meaning of both chunks is slightly similar or somehow related 0 (represented as NIL) if the meaning of the chunk is completely unrelated. Note that 0 is not possible for an aligned pair, as that would mean that the two chunks would be left unaligned. Note also that if the score is 5, then the label assigned later should be equivalence (EQUI, see below). After assigning the label, the annotator should check for the following: if a chunk is not aligned it should have NIL score, equivalent chunks annotationguidelinesinterpretablests2016v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.pdf</head><p>(EQUI) should have a 5 score. The rest of the labels should have a score larger than 0 but lower than 5.</p><p>We will now describe the alignment types, but first note that the interpretation of the whole sentence, including common sense inference, has to be taken into account. This means that we need to take into account the context in order to know whether the aligned chunks refer to the same instance (or set of instances) or not. Instances may refer to physical or abstract object instances (for NPs) or real world event instances (for verb chains):</p><p>• EQUI: both chunks have the same meaning, they are semantically equivalent in this context. • OPPO: the meanings of the chunks are in opposition to each other, lying in an inherently incompatible binary relationship. • SPE1: both chunks have similar meanings, but chunk in sentence 1 is more specific. • SPE2: like SPE1, but it is the chunk in sentence 2 which is more specific. In addition, the meaning of the chunks can be very close, either because they have a similar meaning, or because their meanings have some other relation. In those cases, we use SIMI or REL as follows:</p><p>• SIMI: both chunks have similar meanings, they share similar attributes and there is no EQUI, OPPO, SPE1 or SPE2 relation. • REL: both chunks are not considered similar but they are closely related by some relation not mentioned above (i.e. no EQUI, OPPO, SPE1, SPE2, or SIMI relation). • NOALI: this chunk has not any corresponding chunk in the other sentence. Therefore, it is left unaligned. The above seven labels are exclusive, and each alignment should have one such label.</p><p>In addition to one of the labels above, there are two labels which can be used either in isolation or together, that is, you can use none, one or both:</p><p>• FACT: the factuality in the aligned chunks (i.e. whether the statement is or is not a fact or a speculation) is different. • POL: the polarity in the aligned chunks (i.e. the expressed opinion, which can be positive, negative, or neutral) is different. Note that NOALI can also be FACT or POL, meaning that the respective chunk adds a factuality or polarity nuance to the sentence.</p><p>Listing 1 shows the annotation format for a given sentence pair from the training set (note that each alignment is reported in one line as follows: tokenid-sent1 &lt;==&gt; token-id-sent2 // label // score // comment).</p><p>Finally, there are some specific criteria related to the Answer-Students corpus that have been followed during the annotation process. For instance, in the Answer-Students example in the previous section, switch z (first sentence) and Z (second sentence) are considered equivalent as, in this dataset, X, Y, and Z always refer to switches X, Y, and Z. The same criteria is followed when annotating bulb c and C as equivalent, as A, B and C are always used to refer to bulb A, B and C. In the same way closed path and a path are equivalent, as paths are always considered to be closed. For further details related to such a corpus specific criteria refer to the annotation guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics</head><p>The official evaluation is based on <ref type="bibr" target="#b16">(Melamed, 1998)</ref>, which uses the F1 of precision and recall of token alignments (in the context of alignment for Machine Translation). <ref type="bibr" target="#b9">Fraser and Marcu (2007)</ref> argue that F1 is a better measure than other alternatives such as the Alignment Error Rate. The idea is that, for each pair of chunks that are aligned, we consider that any pairs of tokens in the chunks are also aligned with some weight. The weight of each token-token alignment is the inverse of the number of alignments of each token (so-called fan out factor, Melamed, 1998). Precision is measured as the ratio of token-token alignments that exist in both system and gold standard files, divided by the number of alignments in the system. Recall is measured similarly, as the ratio of token-token alignments that exist in both system and gold-standard, divided by the number of alignments in the gold standard. Precision and recall are evaluated separately for all alignments of all pairs. Participating runs were evaluated using four different metrics: F1 where alignment type and score are ignored (alignment F1, F for short); F1 where alignment types need to match, but scores are ignored (type F1, +T for short); F1 where alignment type is ignored, but each alignment is penalized Listing 1: Annotation format &lt;s e n t e n c e i d ="6" s t a t u s =""&gt; 12 k i l l e d i n b u s a c c i d e n t i n P a k i s t a n 10 k i l l e d i n r o a d a c c i d e n t i n NW P a k i s t a n . . . &lt;a l i g n m e n t &gt; 1 &lt;==&gt; 1 / / SIMI / / 4 / / 12 &lt;==&gt; 10 2 &lt;==&gt; 2 / / EQUI / / 5 / / k i l l e d &lt;==&gt; k i l l e d 3 4 5 &lt;==&gt; 3 4 5 / / SPE1 / / 4 / / i n b u s a c c i d e n t &lt;==&gt; i n r o a d a c c i d e n t 6 7 &lt;==&gt; 6 7 8 / / SPE2 / / 4 / / i n P a k i s t a n &lt;==&gt; i n NW P a k i s t a n &lt;/ a l i g n m e n t &gt; &lt;/ s e n t e n c e &gt; when scores do not match 6 (score F1, +S for short); and, F1 where alignment types need to match, and each alignment is penalized when scores do not match (type and score F1, +TS for short). The type and score F1 is the main overall metric.</p><p>Note that our evaluation procedure does not explicitly evaluate the chunking results. The method implicitly penalizes chunking errors via the induced token-token alignments, using a soft penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baseline System</head><p>The baseline system consists of a cascade concatenation of several procedures. First, input sentences are tokenized using simple regular expressions. Additionally, we collect chunks coming either from the gold standard or from the chunking done by ixapipes-chunk <ref type="bibr" target="#b1">(Agerri et al., 2014)</ref>. This is followed by a lower-cased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus enabling us to know which chunks were left unaligned). Finally, in the last phase, the baseline system uses a rule-based algorithm to directly assign labels and scores: to chunks with the highest link weight assign label = "EQUI" and score = 5, to the rest of aligned chunks (with lower weights) assign label = "NOALI" and score = NIL, and, to unaligned chunks assign label = "NOALI" and score = NIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participation</head><p>The pilot task presented two scenarios: raw text and gold standard chunks. In the first scenario, given a pair of sentences, participants had to identify the composing chunks, and then align them; after that they would assign a relatedness tag and a similarity score to each alignment. In the gold standard scenario, participants were provided with the gold standard chunks.</p><p>In both scenarios the datasets were provided with tokenized text, with exception of Answer-Students, which was not tokenized 7 .</p><p>The task allowed up to a total of three submissions for each team on each of the evaluation scenarios. The organizers provided a script to check if the run files are well formed.</p><p>Nine teams participated on the gold chunks scenario, and out of them six teams also participated in the system chunks scenario. Regarding the datasets, all the teams gave their results for the three datasets, except Venseseval who sent results only for Headlines and Images.</p><p>The iUBC team includes some of the organizers of the interpretable STS task. It is marked by the symbol * in the result tables, and it is not taken into account in the rankings. The organizers took measures to prevent developers of that team to access the test data or any other information, so the team participated in identical conditions to the rest of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" target="#tab_3">2</ref> provides the overall type and score (+TS) performance per dataset, and the mean accross the three datasets. Results for Headlines, Images and Answer-Students datasets are shown in the Appendix, tables 3, 4 and 5, respectively. Each row of the tables corresponds to a run configuration named TeamID RunID. Note that task results are separately written with respect to the scenario. A unique baseline was used for both evaluation scenarios and its performance is jointly presented with the scores obtained by participants.</p><p>The results of the present edition corroborate last years' results regarding the difficulty of the system chunks scenario. Indeed, it is considerably more challenging than the gold chunks scenario.</p><p>With regard to the datasets, the Answer-Students ended up being more challenging than the other datasets for five out of eight teams, but FBK-HLT-NLP, IISCNLP and iUBC teams give their best results for such a scenario.</p><p>Compared to last year, the best results for Images and Headlines in the +TS metric have improved in both SYS and GS scenarios: 4 and 6 points for Headlines (in SYS and GS, respectively), and 5 and 7 points for Images (in SYS and GS, respectively). In order to check whether the datasets where easier this year, we checked the performance of the baseline. The differences are small: this year the Images dataset seems slightly easier (3 and 4 point difference for SYS and GS scenarios), and the Headlines dataset is only slightly more difficult (1 point difference for SYS and GS scenarios). The improvement in results for this year seems to be due to better system performance.</p><p>The complexity of the evaluation (cf. tables 3, 4 and 5) was incremental for the four available metrics, which obviously, were lower for the system chunks. Both type and score are bounded by the alignment results and it is thus natural that alignment results are higher. Comparing type and score results, the type results are generally lower, possibly due to the harder task of guessing the correct label.</p><p>The final results are bounded by both type and score, and the systems doing best in type are the ones doing best overall. From the results we can see that labeling the type was the most challenging.</p><p>Regarding the overall test results for type and score (+TS) across datasets, UWB <ref type="bibr" target="#b12">(Konopík et al., 2016)</ref> and DTSim <ref type="bibr" target="#b4">(Banjade et al., 2016)</ref> obtained the best results for the gold chunks scenario, and DT-Sim and FBK-HLT-NLP <ref type="bibr" target="#b15">(Magnolini et al., 2016)</ref> for the system chunks scenario. In addition, DTSim obtained the best overall results even though they have not good results for the Answer-Students dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Systems, tools and resources</head><p>Most of the teams reported input text processing such as lemmatization and part of speech tagging, and in some cases named-entity recognition and syntactic parsing. Additional resources such as Word-Net, distributional embeddings, paraphrases from PPDB and global STS sentence scores were also used. Participants also revealed that most of their systems were built using some kind of distributional or knowledge-based similarity metrics. We noticed, for instance, that WordNet or word embeddings were used by several teams to compute word similarity.</p><p>Looking at the learning approaches, both supervised and unsupervised approaches have been applied, as well are mainly manual rule-based combinations.</p><p>Next, we briefly introduce the participant teams, whit slightly more details for the top performing systems.</p><p>• UWB <ref type="bibr" target="#b12">(Konopík et al., 2016)</ref>: UWB used three separate supervised classifiers to perform alignment, scoring and typing. They defined a similarity function based on a distribution similarity paradigm: vector composition, lexical semantic vectors and iDF weighting. They introduced a modified method to create word vectors, and  combine unique words from the chunks of both sentences into one single vocabulary which is then used to produce similarity measures. They claim that the following three differences have significant influence on the final results: modified lexical semantic vectors (+3% of the mean of T+S F1 scores), shared words (+2%) and POS tags difference (+2%). • DTSim <ref type="bibr" target="#b4">(Banjade et al., 2016)</ref>: This team builds on the NeroSim system <ref type="bibr" target="#b3">(Banjade et al., 2015)</ref>, which participated in the 2015 task with good results using a system based on manual rules blended semantic similarity features. The team explored several chunking algorithms and in-cluded new rules. Concretely, they expanded the rules for SIMI and EQUI. They mainly improved the chunker and concluded that a Conditional Random Fields (CRFs) based chunking tool is the best approach for chunking. The input sequence to their chunking model are POS tags, and the chunker yielded the highest average accuracies on both the training and test datasets. • FBK-HLT-NLP <ref type="bibr" target="#b15">(Magnolini et al., 2016)</ref>: This teams built a multi-layer perceptron to solve alignment, scoring and typing. The perceptron shares some layers for the three tasks, and other layers are separate. They use a variety of features, including WordNet and word embeddings. The system performs better in the system chunks scenario than in the gold chunks one. Therefore, there is no specific advantage of using chunked sentence pairs and their system is very powerful. The Answer-Students dataset has better performance than Headlines and Images. They obtain better results training a single system for the three datasets (compared to training a classifier separately for each dataset). • Inspire <ref type="bibr" target="#b11">(Kazmi and Schüller, 2016)</ref>: The authors propose a system based on logic programming which extends the basic ideas of NeroSim <ref type="bibr" target="#b3">(Banjade et al., 2015)</ref>. The rule based system makes use of several resources to prepare the input and uses Answer Set Programming to determine chunk boundaries. • IISCNLP (Tekumalla and Sharmistha, 2016):</p><p>The system uses an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forest Classifier. This system is an adaptation of a pre-existing textual entailment system, VENSES, which first performs a semantic analysis of the text including argument structure and then looks for bridging information between chunks using several knowledge resources.</p><p>• iUBC <ref type="bibr" target="#b14">(Lopez-Gazpio et al., 2016)</ref>: A two layer architecture is used to produce the similarity type and score of pairs of chunks. The top layer consists of two models: a classifier and a regressor. The bottom layer consists of a recurrent neural network that processes input and feeds composed semantic feature vectors to the top layer. Both layers are trained at the same time by propagating gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Last year, the Interpretable STS task was introduced as a pilot subtask of the STS task. At the present edition, it has been presented as an independent task that has attracted nine teams. In addition to the image caption and news headlines datasets, this year participants were challenged with a new dataset from the Educational area. Concretely, the Answer-Students corpus, which consists of the interactions between students of electronics and the BEETLE II tutorial dialogue system. Compared to the results last year <ref type="bibr" target="#b2">(Agirre et al., 2015)</ref>, the results have improved in the two datasets that happened both years, Images and Headlines. The Answer-Students dataset is the most challenging, and among the three subtasks (alignment, type and score) guessing the correct type of the aligned chunks is the most difficult one. Teams that did best on type get the best overall score.</p><p>All datasets and the annotation guideline are available in http://alt.qcri.org/ semeval2016/task2/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>12] &lt;=&gt; [10] : (SIMILAR 4) [killed] &lt;=&gt; [killed] : (EQUIVALENT 5) [in bus accident] &lt;=&gt; [in road accident] : (MORE-SPECIFIC 4) [in Pakistan] &lt;=&gt; [in NW Pakistan] : (MORE-GENERAL 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Details of the datasets, including number of pairs, source, and relation to STS datasets. HDL stands for Headlines, and Student to Student-Answers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall test results for type and score (+TS) across datasets. Each row correspond to a system run, and each column to a dataset: (I) for Images, (H) for Headlines, (AS) for Answer-Students, Mean for the mean across the three datasets, and R for the rank. The " * " symbol denotes runs that include task organizers. Additionally, the table shows results for the baseline, average of participants (AVG) and maximum score of participants (MAX).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>•</head><label></label><figDesc>Vrep (Henry and Sands, 2016): features are extracted to create a learned rule-based classifier to assign a label. It uses semantic and syntactic (form of the chunks) relationship features. • Rev (Ping Ping et al., 2016): The system consists of rules based on the analysis of the Headlines dataset considering lexical overlapping, part of speech tags and synonymy.</figDesc><table /><note>• Venseseval:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://emm.newsexplorer.eu/NewsExplorer/home/en/latest.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/ixa-ehu/ ixa-pipe-chunk 5 http://www.clips.ua.ac.be/conll2000/ chunking/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The penalization is the difference between the scores divided by five.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In fact The Answer-Students dataset was only partially tokenized. In order to be consistent with the gold standard, participants had to follow the partial tokenization, separating tokens at blanks alone.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based in part upon work supported a MINECO grant to the University of the Basque Country (TUNER project TIN2015-65308-C5-1-R). Aitor Gonzalez Agirre and Iñigo Lopez-Gazpio are by doctoral grants from MINECO. The IXA group is funded by the Basque Government (A type Research Group).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>F alignment (F), F alignment with type penalty (+T), F alignment with score penalty (+S) and F alignment with type and score penalty (+TS), and R for the rank. The " * " symbol denotes runs that include task organizers. Additionally, the table shows results for the baseline, average of participants (AVG) and maximum score of participants (MAX). Table <ref type="table">5</ref>: Test results in Answer-Students for both scenarios. Each row correspond to a system run, and each column to one evaluation metric: F alignment (F), F alignment with type penalty (+T), F alignment with score penalty (+S) and F alignment with type and score penalty (+TS), and R for the rank. The " * " symbol denotes runs that include task organizers. Additionally, the table</p><p>shows results for the baseline, average of participants (AVG) and maximum score of participants (MAX).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parsing by chunks</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principle-based parsing: Computation and psycholinguistics</title>
				<editor>
			<persName><forename type="first">Robert</forename><surname>Berwick</surname></persName>
			<persName><forename type="first">Steven</forename><surname>Abney</surname></persName>
			<persName><forename type="first">Carol</forename><surname>Tenny</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="257" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ixa pipeline: Efficient and ready to use multilingual NLP tools</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josu</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Language Resources and Evaluation Conference (LREC 2014)</title>
				<meeting>the 9th Language Resources and Evaluation Conference (LREC 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nerosim: A system for measuring and interpreting semantic textual similarity</title>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Banjade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nabin</forename><surname>Bikram Niraula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Stefanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipesh</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName><surname>Gautam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dtsim at semeval-2016 task 2: Interpretable semantic textual similarity</title>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Banjade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nabin</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nobal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Niraula</surname></persName>
		</author>
		<author>
			<persName><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aligning the RTE 2006 corpus</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Microsoft Research</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Myroslava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Bental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">B</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwendolyn</forename><forename type="middle">E</forename><surname>Steinhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaine</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">B</forename><surname>Farrow</surname></persName>
		</author>
		<author>
			<persName><surname>Callaway</surname></persName>
		</author>
		<title level="m">Sustaining TEL: From Innovation to Learning and Practice: 5th European Conference on Technology Enhanced Learning, EC-TEL 2010</title>
				<meeting><address><addrLine>Barcelona, Spain; Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-09-28" />
			<biblScope unit="page" from="620" to="625" />
		</imprint>
	</monogr>
	<note>Proceedings, chapter Intelligent Tutoring with Natural Language Support in the Beetle II System</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards effective tutorial feedback for explanation questions: A dataset and baselines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Myroslava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
				<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Myroslava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2013: The First Joint Conference on Lexical and Computational Semantics</title>
				<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring word alignment quality for statistical machine translation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="303" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vrep at semeval-2016 task 1 and task 2</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Sands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inspire at semeval-2016 task 2: Interpretable semantic textual similarity alignment based on answer set programming</title>
		<author>
			<persName><forename type="first">Mishal</forename><surname>Kazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uwb at semeval-2016 task 2: Interpretable semantic textual similarity with distributional semantics for chunks</title>
		<author>
			<persName><forename type="first">Miloslav</forename><surname>Konopík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Pražák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Brychcín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing partial textual entailment</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="451" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">iubc at semeval-2016 task 2: Rnns and lstms for interpretable sts</title>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fbk-hlt-nlp at semeval-2016 task 2: A multitask, deep learning approach for interpretable semantic textual similarity</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Magnolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Feltracco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)</title>
				<meeting>the 10th International Workshop on Semantic Evaluation (Se-mEval 2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Manual annotation of translational equivalence: The blinker project</title>
		<author>
			<persName><forename type="first">Melamed</forename><surname>I Dan</surname></persName>
		</author>
		<idno>cmp-lg/9805005</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing entailment in intelligent tutoring systems</title>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="479" to="501" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rev at semeval-2016 task 2: Aligning chunks by lexical, part of speech and semantic equivalence</title>
		<author>
			<persName><forename type="first">Karin</forename><surname>Tan Ping Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collecting image annotations using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT 2010</title>
				<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The SIMILAR corpus: A resource to foster the qualitative understanding of semantic similarity of texts</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobal</forename><surname>Baggett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Niraula</surname></persName>
		</author>
		<author>
			<persName><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Relations II: Enhancing Resources and Applications, The 8th Language Resources and Evaluation Conference (LREC 2012)</title>
				<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="23" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IISC-NLP at semeval-2016 task 2: Interpretable STS with ILP based multiple chunk aligner</title>
		<author>
			<persName><forename type="first">Sita</forename><surname>Lavanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharmistha</forename><surname>Tekumalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2000 shared task: Chunking</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning</title>
				<meeting>the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
