<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2013 Task 12: Multilingual Word Sense Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<email>navigli@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Sapienza</orgName>
								<orgName type="institution">Università di Roma Viale Regina Elena</orgName>
								<address>
									<postCode>295 -00161</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
							<email>jurgens@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Sapienza</orgName>
								<orgName type="institution">Università di Roma Viale Regina Elena</orgName>
								<address>
									<postCode>295 -00161</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
							<email>vannella@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Sapienza</orgName>
								<orgName type="institution">Università di Roma Viale Regina Elena</orgName>
								<address>
									<postCode>295 -00161</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2013 Task 12: Multilingual Word Sense Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the SemEval-2013 task on multilingual Word Sense Disambiguation. We describe our experience in producing a multilingual sense-annotated corpus for the task. The corpus is tagged with BabelNet 1.1.1, a freely-available multilingual encyclopedic dictionary and, as a byproduct, WordNet 3.0 and the Wikipedia sense inventory. We present and analyze the results of participating systems, and discuss future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics <ref type="bibr" target="#b21">(Navigli, 2009;</ref><ref type="bibr" target="#b22">Navigli, 2012)</ref>. Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition <ref type="bibr" target="#b9">(Kilgarriff, 1998;</ref><ref type="bibr" target="#b8">Kilgarriff and Palmer, 2000)</ref>, later tasks <ref type="bibr" target="#b1">(Edmonds and Cotton, 2001;</ref><ref type="bibr" target="#b25">Snyder and Palmer, 2004;</ref><ref type="bibr" target="#b12">Mihalcea et al., 2004)</ref> focused on WordNet <ref type="bibr" target="#b14">(Miller et al., 1990;</ref><ref type="bibr">Fellbaum, 1998)</ref> as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself <ref type="bibr" target="#b20">(Navigli et al., 2007)</ref> and from OntoNotes <ref type="bibr" target="#b23">(Pradhan et al., 2007)</ref>.</p><p>In recent years, with the exponential growth of the Web and, consequently, the increase of non-English speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation <ref type="bibr" target="#b10">(Lefever and Hoste, 2010)</ref> and cross-lingual lexical substitution <ref type="bibr" target="#b13">(Mihalcea et al., 2010)</ref> were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution <ref type="bibr" target="#b11">(McCarthy and Navigli, 2009)</ref>. The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary.</p><p>The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources <ref type="bibr" target="#b6">(Hovy et al., 2013)</ref>. Over the past few years, a wide-coverage multilingual "encyclopedic" dictionary, called BabelNet, has been developed <ref type="bibr" target="#b16">(Navigli and Ponzetto, 2012a)</ref>. BabelNet 1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for both lexicographic (e.g., apple as fruit) and encyclopedic meanings (e.g., Apple Inc. as company). In this paper we describe our task and disambiguation dataset and report on the system results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Setup</head><p>The task required participating systems to annotate nouns in a test corpus with the most appropriate sense from the BabelNet sense inventory or, alternatively, from two main subsets of it, namely the WordNet or Wikipedia sense inventories. In contrast to previous all-words WSD tasks we did not focus on the other three open classes (i.e., verbs, adjectives and adverbs) since BabelNet does not currently provide non-English coverage for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Corpus</head><p>The test set consisted of 13 articles obtained from the datasets available from the 2010, 2011 and 2012 editions of the workshop on Statistical Machine Translation (WSMT). <ref type="bibr">2</ref> The articles cover different domains, ranging from sports to financial news.</p><p>The same article was available in 4 different languages (English, French, German and Spanish). In order to cover Italian, an Italian native speaker manually translated each article from English into Italian, with the support of an English mother tongue advisor. In Table <ref type="table" target="#tab_1">1</ref> we show for each language the number of words of running text, together with the number of multiword expressions and named entities annotated, from the 13 articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sense Inventories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">BabelNet inventory</head><p>To semantically annotate all the single-and multiword expressions, as well as the named entities, occurring in our test corpus we used BabelNet 1.1.1 <ref type="bibr" target="#b16">(Navigli and Ponzetto, 2012a)</ref>. BabelNet is a multilingual "encyclopedic dictionary" and a semantic network currently covering 6 languages, namely: English, Catalan, French, German, Italian and Spanish. BabelNet is obtained as a result of a novel integration and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia -i.e., Wikipedia -to the most popular computational lexicon -i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2 http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (MT).</p><p>Its lexicon includes lemmas which denote both lexicographic meanings (e.g., balloon) and encyclopedic ones (e.g., Montgolfier brothers). The basic meaning unit in BabelNet is the Babel synset, modeled after the WordNet synset <ref type="bibr" target="#b14">(Miller et al., 1990;</ref><ref type="bibr">Fellbaum, 1998)</ref>. A Babel synset is a set of synonyms which express a concept in different languages. For instance, { Globus aerostàtic CA , Balloon EN , Aérostation FR , Ballon DE , Pallone aerostatico IT , . . . , Globo aerostático ES } is the Babel synset for the balloon aerostat, where the language of each synonym is provided as a subscript label. Thanks to their multilingual nature, we were able to use Babel synsets as interlingual concept tags for nouns occurring within text written in any of the covered languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">WordNet and Wikipedia inventories</head><p>Since BabelNet 1.1.1 is a superset of the Word-Net 3.0 and Wikipedia sense inventories, 3 once text is annotated with Babel synsets, it turns out to be annotated also according to either WordNet or Wikipedia, or both. In fact, in order to induce the WordNet annotations, one can restrict to those lexical items annotated with Babel synsets which contain WordNet senses for the target lemma; similarly, for Wikipedia, we restrict to those items tagged with Babel synsets which contain Wikipedia pages for the target lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BabelNet sense inventory validation</head><p>Because BabelNet is an automatic integration of WordNet and Wikipedia, the resulting Babel synsets may contain WordNet and Wikipedia entries about different meanings of the same lemma. The underlying cause is a wrong mapping between the two original resources. For instance, in BabelNet 1.1 the WordNet synset { arsenic, As, atomic number 33 } was mapped to the Wikipedia page AS (RO-MAN COIN), and therefore the same Babel synset mixed the two meanings.</p><p>In order to avoid an inconsistent semantic tagging of text, we decided to manually check all the mappings in BabelNet 1.1 between Wikipedia pages  and WordNet senses involving lemmas in our English test set for the task. Overall, we identified 8306 synsets for 978 lemmas to be manually checked. We recruited 8 annotators in our research group and assigned each lemma to two annotators. Each annotator was instructed to check each Babel synset and determine whether any of the following three operations was needed:</p><p>• Delete a mapping and separate the WordNet sense from the Wikipedia page (like in the arsenic vs. AS (ROMAN COIN) example above);</p><p>• Add a mapping between a WordNet sense and a Wikipedia page (formerly available as two separate Babel synsets);</p><p>• Merge two Babel synsets which express the same concept.</p><p>After disagreement adjudication carried out by the first author, the number of delete, add and merge operations was 493, 203 and 43, respectively, for a total of 739 operations (i.e., 8.8% of synsets corrected). As a result of our validation of BabelNet 1.1, we obtained version 1.1.1, which is currently available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sense Annotation</head><p>To ensure high quality annotations, the annotation process was completed in three phases. Because BabelNet is a superset of both the WordNet and Wikipedia sense inventories, all annotators used the BabelNet 1.1.1 sense inventory for their respective language. These BabelNet annotations were then projected into WordNet and Wikipedia senses. Annotation was performed by one native speaker each for English, French, German and Spanish and, for Italian, by two native speakers who annotated different subsets of the corpus.</p><p>In the first phase, each annotator was instructed to inspect each instance to check that (1) the lemma was tagged with the correct part of speech, (2) lemmas were correctly annotated as named entity or multiword expressions, and (3) the meaning of the instance's lemma had an associated sense in Ba-belNet. Based on these criteria, annotators removed dozens of instances from the original data.</p><p>In the second phase, each instance in the English dataset was annotated using BabelNet senses. To reduce the time required for annotation in the other languages, the sense annotations for the English dataset were then projected onto the other four Table <ref type="table">2</ref>: Statistics when using the English sense annotations to project the correct sense of a lemma in another language of the sentence-aligned test data.</p><p>languages using the sense translation API of Babel-Net <ref type="bibr" target="#b19">(Navigli and Ponzetto, 2012d)</ref>. The projection operated as follows, using the aligned sentences in the English and non-English texts. For an instance in the non-English text, all of the senses for that instance's lemma were compared with the sense annotations in the English sentence. If any of that lemma's senses was used in the English sentence, then that sense was selected for the non-English instance. The matching procedure operates at the sentence-aligned level because the instances themselves are not aligned; i.e., different languages have different numbers of instances per sentence, which are potentially ordered differently due to languagespecific construction. Ultimately, this projection labeled approximately 50-70% of the instances in the other four languages. Given the projected senses, the annotators for the other four languages were then asked to (1) correct the projected sense labels and</p><p>(2) annotate those still without senses. 4 These annotations were recorded in text in a stand-off file; no further annotation tools were used. The resulting sense projection proved highly useful for selecting the correct sense. Table <ref type="table">2</ref> shows the number of corrections made by the annotators to the projected senses, who changed only 22-37% of the labels. While simple, the projection method offers significant potential for generating good quality sense-annotated data from sentence-aligned multilingual text.</p><p>In the third phase, an independent annotator reviewed the labels for the high-frequency lemmas for all languages to check for systematic errors and discuss possible changes to the labeling. This review resulted in only a small number of changes to less than 5% of the total instances, except for German which had a slightly higher percentage of changes.</p><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the sense annotation statistics for the test set. Annotators were allowed to use multiple senses in the case of ambiguity, but encouraged to use a single sense whenever possible. In rare cases, a lemma was annotated with senses from a different lemma. For example, WordNet does not contain a sense for "card" that corresponds to the penalty card meaning (as used in sports such as football). In contrast, BabelNet has a sense for "penalty card" from Wikipedia which, however, is not mapped to the lemma "card". In such cases, we add both the closest meaning from the original lemma (e.g., the rectangual piece of paper sense in WordNet) and the most suitable sense that may have a different lemma form (e.g., PENALTY CARD).</p><p>Previous annotation studies have shown that, when a fine-grained sense inventory is used, annotators will often label ambiguous instances with multiple senses if allowed <ref type="bibr" target="#b2">(Erk and McCarthy, 2009;</ref><ref type="bibr" target="#b7">Jurgens and Klapaftis, 2013)</ref>. Since BabelNet is a combination of a fine-grained inventory (WordNet) and contains additional senses from Wikipedia, we analyzed the average number of BabelNet sense annotations per instance, shown in column six of Table <ref type="table" target="#tab_1">1</ref>. Surprisingly, Table <ref type="table" target="#tab_1">1</ref> suggests that the rate of multiple sense annotation varies significantly between languages.</p><p>BabelNet may combine multiple Wikipedia pages into a single BabelNet synset. As a result, when Wikipedia is used as a sense inventory, instances are annotated with all of the Wikipedia pages associated with each BabelNet synset. Indeed, Table <ref type="table" target="#tab_1">1</ref> shows a markedly increased multi-sense annotation rate for three languages when using Wikipedia.</p><p>As a second analysis, we considered the observed level of polysemy for each of the unique lemmas. The last column of Table <ref type="table" target="#tab_1">1</ref> shows the average number of different senses seen for each lemma across the test sets. In all languages, often only a single sense of a lemma was used. Because the test set is constructed based on topical documents, infrequent lemmas mostly occurred within a single document where they were used with a consistent interpreta-tion. However, we note that in the case of lemmas that were only seen with a single sense, this sense does not always correspond to the most frequent sense as seen in SemCor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Task 12 uses the standard definitions of precision and recall for WSD evaluation (see, e.g., <ref type="bibr" target="#b21">(Navigli, 2009)</ref>). Precision measures the percentage of the sense assignments provided by the system that are identical to the gold standard; Recall measures the percentage of instances that are correctly labeled by the system. When a system provides sense labels for all instances, precision and recall are equivalent. Systems using BabelNet and WordNet senses are compared against the Most Frequent Sense (MFS) baseline obtained by using the WordNet most frequent sense. For the Wikipedia sense inventory, we constructed a pseudo-MFS baseline by selecting (1) the Wikipedia page associated with the highest ranking WordNet sense, as ranked by SemCor frequency, or (2) when no synset for a lemma was associated with a WordNet sense, the first Wikipedia page sorted using BabelNet's ordering criteria, i.e., lexicographic sorting. We note that, in the second case, this procedure frequently selected the page with the same name as the lemma itself. For instance, the first sense of Dragon Ball is the cartoon with title DRAGON BALL, followed by two films <ref type="bibr">(DRAGON BALL (1990 FILM)</ref> and DRAGON BALL EVOLU-TION).</p><p>Systems were scored separately for each sense inventory. We note that because the instances in each test set are filtered to include only those that can be labeled with the respective inventory, both the Wikipedia and WordNet test sets are subsets of the instances in the BabelNet test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participating Systems</head><p>Three teams submitted a total of seven systems for the task, with at least one participant attempting all of the sense inventory and language combinations. Six systems participated in the WSD task with BabelNet senses; two teams submitted four systems using WordNet senses; and one team submitted three systems for Wikipedia-based senses. Notably, all systems used graph-based approaches for sense disambiguation, either using WordNet or BabelNet's synset graphs. We summarize the teams' systems as follows.</p><p>DAEBAK! DAEBAK! submitted one system called PD (Peripheral Diversity) based on BabelNet path indices from the BabelNet synset graph. Using a ±5 sentence window around the target word, a graph is constructed for all senses of co-occurring lemmas following the procedure proposed by <ref type="bibr" target="#b15">Navigli and Lapata (2010)</ref>. The final sense is selected based on measuring connectivity to the synsets of neighboring lemmas. The MFS is used as a backoff strategy when no appropriate sense can be picked out.</p><p>GETALP GETALP submitted three systems, two for BabelNet and one for WordNet, all based on the ant-colony algorithm of <ref type="bibr" target="#b24">(Schwab et al., 2012)</ref>, which uses the sense inventory network structure to identify paths connecting synsets of the target lemma to the synsets of other lemmas in context. The algorithm requires setting several parameters for the weighting of the structure of the contextbased graph, which vary across the three systems. The BN1 system optimizes its parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs.</p><p>UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource <ref type="bibr" target="#b4">(Gutiérrez et al., 2011)</ref>, which enriches the WordNet semantic network using edges from multiple lexical resources, such as WordNet Domains and the eXtended WordNet. WSD was then performed using the ISR-WN network in combination with the algorithm of <ref type="bibr" target="#b5">Gutiérrez (2012)</ref>, which is an extension of the Personalized PageRank algorithm for WSD <ref type="bibr" target="#b0">(Agirre and Soroa, 2009)</ref> which includes senses frequency. The algorithm requires initializing the PageRank algorithm with a set of seed synsets (vertices) in the network; this initialization represents the key variation among UMCC's three approaches. The RUN-1 system performs WSD using all noun instances from the sentence context. In contrast, the RUN-2 works at the discourse level and initializes the PageRank using the synsets of all  nouns in the document. Finally, the RUN-3 system initializes using all words in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>All teams submitted at least one system using the BabelNet inventory, shown in Table <ref type="table" target="#tab_4">3</ref>. The UMCC-DLSI systems were consistently able to outperform the MFS baseline (a notoriously hard-to-beat heuristic) in all languages except German. Additionally, the DAEBAK! system outperformed the MFS baseline on French and Italian. The UMCC-DLSI RUN-2 system performed the best for all languages. Notably, this system leverages the single-sense per discourse heuristic <ref type="bibr" target="#b26">(Yarowsky, 1995)</ref>, which uses the same sense label for all occurrences of a lemma in a document. UMCC-DLSI submitted the only three systems to use Wikipedia-based senses. Table <ref type="table" target="#tab_6">4</ref> shows their performance. Of the three sense inventories, Wikipedia had the most competitive MFS baseline, scoring at least 0.694 on all languages. Notably, the Wikipedia-based system has the lowest recall of all systems. Despite having superior precision to the MFS baseline, the low recall brought the resulting F1 measure below the MFS.</p><p>Two teams submitted four total systems for Word-Net, shown in Table <ref type="table" target="#tab_7">5</ref>. The UMCC-DLSI RUN-2 system was again the top-performing system, underscoring the benefit of using discourse information in selecting senses. The other two UMCC-DLSI systems also surpassed the MFS baseline. Though still performing worse than the MFS baseline, when using the WordNet sense graph, the GETALP system sees a noticeable improvement of 0.14 over its per- formance on English data when using the WordNet sense graph. The disambiguation task encompasses multiple types of entities. Therefore, we partitioned the Ba-belNet test data according to the type of instance being disambiguated; Table <ref type="table" target="#tab_8">6</ref> highlights the results per instance type, averaged across all languages. <ref type="bibr">5</ref> Both multiword expressions and named entities are less polysemous, resulting in a substantially higher MFS baseline that no system was able to outperform on the two classes. However, for instances made of a single term, both of the UMCC-DLSI systems were able to outperform the MFS baseline.</p><p>BabelNet adds many Wikipedia senses to the existing WordNet senses, which increases the poly-      <ref type="table">7</ref>: System performance when the system's annotations are restricted to only those senses that it also uses in the aligned sentences of at least two other languages. semy of most instances. As a further analysis, we consider the relationship between the polysemy of an instance's target and system performance. Instances were grouped according to the number of BabelNet senses that their lemma had; following, systems were scored on each grouping. Figure <ref type="figure" target="#fig_0">1</ref> shows the performance of the best system from each team on each polysemy-based instance grouping, with a general trend of performance decay as the number of senses increases. Indeed, all systems' performances are negatively correlated with the degree of polysemy, ranging from -0.401 (UMCC-DLSI RUN-1) to -0.654 (GETALP BN-1) when measured using Pearson's correlation. All systems' correlations are significant at p &lt; 0.05.</p><p>Last, we note that all systems operated by senseannotating each language individually without taking advantage of either the multilingual structure of BabelNet or the sentence alignment of the test data. For example, the sense projection method used to create the initial set of multilingual annotations on our test data (cf. Table <ref type="table">2</ref>) suggests that the sense translation API could be used as a reliable source for estimating the correctness of an annotation; specifically, given the sense annotations for each language, the translation API could be used to test whether the sense is also present in the aligned sentence in the other languages.</p><p>Therefore, we performed a post-hoc analysis of the benefit of multilingual sense alignment using the results of the four systems that submitted for all languages in BabelNet. For each language, we filter the sense annotations such that an annotation for an instance is retained only if the system assigned the same sense to some word in the aligned sentence from at least two other languages.</p><p>Table <ref type="table">7</ref> shows the resulting performance for the four systems. As expected, the systems exhibit significantly lower recall due to omitting all languagespecific instances. However, the resulting precision is significantly higher than the original performance, shown in Table <ref type="table" target="#tab_4">3</ref>. Additionally, we analyzed the set of instances reported for each system and confirmed that the improvement is not due to selecting only monosemous lemmas. Despite the GETALP system having the lower performance of the four systems when all instances are considered, the system obtains the highest precision for the English dataset. Furthermore, the UMCC-DLSI systems still obtain moderate recall, while enjoying 0.106-0.155 absolute improvements in precision across all languages. While the resulting F1 is lower due to a loss of recall, we view this result as a solid starting point for other methods to sense-tag the remaining instances. Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages <ref type="bibr" target="#b17">(Navigli and Ponzetto, 2012b;</ref><ref type="bibr" target="#b18">Navigli and Ponzetto, 2012c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Directions</head><p>Following recent SemEval efforts with word senses in multilingual settings, we have introduced a new task on multilingual WSD that uses the recently released BabelNet 1.1.1 sense inventory. Using a data set of 13 articles in five languages, all nominal instances were annotated with BabelNet senses. Because BabelNet is a superset of WordNet and Wikipedia, the task also facilitates analysis in those sense inventories.</p><p>Three teams submitted seven systems, with all systems leveraging the graph-based structure of WordNet and BabelNet. Several systems were able to outperform the competitive MFS baseline, except in the case of Wikipedia, but current performance leaves significant room for future improvement. In addition, we believe that future research could leverage sense parallelism available in sentence-aligned multilingual corpora, together with enriched information available in future versions of BabelNet. All of the resources for this task, including the newest 1.1.1 version of BabelNet, were released on the task website. <ref type="bibr">6</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: F1 measure according to the degree of instance polysemy, reported when at least ten instances have the specified polysemy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics for the sense annotations of the test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>System performance, reported as F1, for all five languages in the test set when using BabelNet senses. Top performing systems are marked in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The F1 measure for each system across all five languages in the test set when using Wikipedia-based senses.</figDesc><table><row><cell>Team</cell><cell cols="3">System Precision Recall</cell><cell>F1</cell></row><row><cell>GETALP</cell><cell>WN-1</cell><cell>0.406</cell><cell cols="2">0.406 0.406</cell></row><row><cell cols="2">UMCC-DLSI RUN-1</cell><cell>0.639</cell><cell cols="2">0.635 0.637</cell></row><row><cell cols="2">UMCC-DLSI RUN-2</cell><cell>0.649</cell><cell cols="2">0.645 0.647</cell></row><row><cell cols="2">UMCC-DLSI RUN-3</cell><cell>0.642</cell><cell cols="2">0.639 0.640</cell></row><row><cell>MFS</cell><cell></cell><cell>0.630</cell><cell cols="2">0.630 0.630</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>System performance when using WordNet senses. Top performing systems are marked in bold.</figDesc><table><row><cell>Team</cell><cell cols="4">System Single term Multiword expression Named Entity</cell></row><row><cell>DAEBAK!</cell><cell>PD</cell><cell>0.502</cell><cell>0.801</cell><cell>0.910</cell></row><row><cell>GETALP</cell><cell>BN-1</cell><cell>0.232</cell><cell>0.724</cell><cell>0.677</cell></row><row><cell>GETALP</cell><cell>BN-2</cell><cell>0.235</cell><cell>0.740</cell><cell>0.656</cell></row><row><cell cols="2">UMCC-DLSI RUN-1</cell><cell>0.582</cell><cell>0.806</cell><cell>0.865</cell></row><row><cell cols="2">UMCC-DLSI RUN-2</cell><cell>0.584</cell><cell>0.809</cell><cell>0.864</cell></row><row><cell>MFS</cell><cell></cell><cell>0.511</cell><cell>0.853</cell><cell>0.920</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>System F1 per instance type, averaged across all submitted languages, with the highest system scores in bold.</figDesc><table><row><cell></cell><cell></cell><cell>English</cell><cell></cell><cell>French</cell><cell></cell><cell>German</cell><cell></cell><cell>Italian</cell><cell></cell><cell>Spanish</cell></row><row><cell>Team</cell><cell cols="2">System Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell><cell>Prec. Rec.</cell><cell>F1</cell></row><row><cell>DAEBAK</cell><cell>PD</cell><cell cols="10">0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570</cell></row><row><cell>GETALP</cell><cell>BN-2</cell><cell cols="10">0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282</cell></row><row><cell cols="12">UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643</cell></row><row><cell cols="12">UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://babelnet.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For version 1.1.1 we used the English Wikipedia database dump from October 1, 2012.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">During the second phase, annotators were also allowed to add and remove instances that were missed during the first phase, which resulted in small number of changes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We omit the UMCC-DLSI Run-3 system from analysis, as it participated in only a single language.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.cs.york.ac.uk/semeval-2013/ task12/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234.</p><p>A large group of people assisted with SemEval-2013 Task 12, and without whose help this task would not have been possible. In particular, we would like to thank Philipp Cimiano, Maud Erhmann, Sascha Hinte, Jesús Roque Campaña Gómez, and Andreas Soos for their assistance in sense annotation; our fellow LCL team members: Moreno De Vincenzi, Stefano Faralli, Tiziano Flati, Marc Franco Salvador, Andrea Moro, Silvia Necşulescu, and Taher Pilehvar for their invaluable assistance in creating BabelNet 1.1.1, preparing and validating sense annotations, and sense-tagging the Italian corpus; last, we thank Jim McManus for his help in producing the Italian test data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalizing PageRank for Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Senseval-2: Overview</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Second International Workshop on Evaluating Word Sense Disambiguation Systems</title>
				<meeting>The Second International Workshop on Evaluating Word Sense Disambiguation Systems<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graded word sense assignment</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching the integration of semantic resources based on wordnet</title>
		<author>
			<persName><forename type="first">Yoan</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Fernández Orquín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrés</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
				<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Análisis semántico multidimensional aplicado a la desambiguación del lenguaje natural</title>
		<author>
			<persName><forename type="first">Yoan</forename><surname>Gutiérrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaboratively built semi-structured content and artificial intelligence: The story so far</title>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="2" to="27" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation</title>
				<meeting>the 7th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on senseval</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Senseval: An exercise in evaluating word sense disambiguation programs</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Language Resources and Evaluation</title>
				<meeting>the First International Conference on Language Resources and Evaluation<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1255" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 3: Cross-lingual word sense disambiguation</title>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronique</forename><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
				<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The English lexical substitution task. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="139" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Senseval-3 English lexical sample task</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3) at ACL-04</title>
				<meeting>the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3) at ACL-04<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 2: Cross-lingual lexical substitution</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
				<meeting>the 5th international workshop on semantic evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">WordNet: an online lexical database</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><forename type="middle">D</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An experimental study on graph connectivity for unsupervised Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="678" to="692" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ba-belNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BabelRelate! a joint multilingual approach to computing semantic relatedness</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joining forces pays off: Multilingual Joint Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
				<meeting>EMNLP-CoNLL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1399" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual WSD with just a few lines of code: the BabelNet API</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2007 Task 07: Coarsegrained English all-words task</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orin</forename><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
				<meeting>the 4th International Workshop on Semantic Evaluations<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A survey</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A quick tour of Word Sense Disambiguation, Induction and related approaches</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOF-SEM)</title>
				<meeting>the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOF-SEM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SemEval-2007 Task-17: English lexical sample, SRL and all words</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
				<meeting>the 4th International Workshop on Semantic Evaluations<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ant colony algorithm for the unsupervised word sense disambiguation of texts: Comparison and evaluation</title>
		<author>
			<persName><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Goulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andon</forename><surname>Tchechmedjiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Blanchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24 th International Conference on Computational Linguistics (COLING)</title>
				<meeting>the 24 th International Conference on Computational Linguistics (COLING)<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2004 SENSEVAL-3 Workshop</title>
				<meeting>ACL 2004 SENSEVAL-3 Workshop<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33 rd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 33 rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
