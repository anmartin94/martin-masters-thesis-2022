<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A First Evaluation of Logic Form Identification Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
							<email>vasile@cs.iusb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University South Bend</orgName>
								<address>
									<postCode>46634</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A First Evaluation of Logic Form Identification Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a first experience with evaluating sytems that address the issue of Logic Form Identification (LFi). A Gold Standard approach was used in which experts provide solutions to test data. The expert solutions, the gold standard, are then compared against outputs from participanting systems and different metrics observed. We proposed a few novel metrics, including precision and recall, that are further used to provide comparative results. The test data included 4155 arguments and 2398 predicates grouped in 300 sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of a Logic Form Identification (LFi) task is to evaluate the performance of different methods addressing the issue of LFi. The Logic Form (LF) that we use is a flat, scope-free first order logic representation that embeds lexical and syntactic information. Given a set of English sentences, participating systems were supposed to return the sentences in Logic Form as in the example below. The general approach adopted for evaluation was a gold standard approach in which the test data is first correctly mapped onto its corresponding LF by a team of experts and then this correct LF is automatically compared against outputs provided by participating sytems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Guidelines</head><p>The Logic Form of a sentence is the conjunction of individual predicates, where the relationships among them are expressed via shared arguments. Predicates are generated for all content words such as nouns, verb, adjectives and adverbs. Pronouns are treated as independent nouns. Prepositions and conjunctions are also mapped onto predicates that capture the relation between the prepositional object and the constituent to which it is attached and the relation among the coordinated entities, respectively.</p><p>There are two types of arguments: e -for events, x -for entities. For the sentence presented above we have two events -e1, e2 corresponding to each verb/action in the sentence and four <ref type="bibr">entities -x1, x2, x3, x4</ref> corresponding to the heads of the base noun phrases (NP). Each verb predicate has the second argument set to the corresponding logical subject and the third argument to its direct object. The remaining slots for a verb predicate are filled with the arguments of their indirect and prepositional objects. In the example presented above the predicate eat has arguments after ; (semicolon) which indicates its adjuncts. The distinction between complements and adjuncts is a novelty to the LF proposed by the authors in this paper. For this first trial, we did not make the distinction between the two and thus the accepted representation would be eat:v (e2, x3, x2, x4) -see below. To ease the task, the notation was relaxed by adopting few simplifications similar, to some extent, to the simplications in <ref type="bibr" target="#b3">(Moldovan and Rus, 2001)</ref>: determiners, plurals, negation, auxiliaries and verb tenses, punctuation are ingnored. Collocations, such as New York, should be considered a single predicate as well as verbs having particles (e.g. give up). For cases when an argument is underspecified, such as the logical subject in Jim was told to say something, an artificial argument should be generated.</p><p>The advantages of the LF notation are mantfold:</p><p>it allows a simple syntax/semantics interface it is user friendly it has positional syntactic arguments that ease other NLP tasks such as textual interpretationa and textual inference if predicates are disambiguated with respect to a general ontology such as WordNet it leads to concept predicates it is easily customizable (for example to distinguish between arguments and adjuncts)</p><p>For details about the principles of Logic Forms read Chapter 2 in <ref type="bibr" target="#b4">(Rus, 2002)</ref>, <ref type="bibr" target="#b4">(Rus and Moldovan, 2002)</ref> and <ref type="bibr" target="#b2">(Hobbs, 1986)</ref>.</p><p>The LF notation proposed for the LFi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differences. A web page for the LFi task is available at http://www.cs.iusb.edu/ vasile/logic/indexLF.html and a discussion group, called logicform, was opened at yahoo.groups.com which can also be consulted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Data</head><p>The test data was compiled so that the impact of external tools that different sytems might use in the LF identification process be minimal. For example, it is well-known that the accuracy of automatic syntactic parsing drops drastically for sentences larger than 40 words and thus we kept the size of the collected sentences below the 40 words threshold. The average sentence size in the test data is 9.89 words.</p><p>Special attention was paid to covering linguistic phenomena such as: coordination, compound nouns, ditransitives, multiword expressions (give up, as well as, etc.), relative clauses and others. Different sources were used to look up such cases: Treebank, WordNet and the web.</p><p>The size of the test set (4155 arguments, 2398 predicates, 300 sentences) allows a better evaluation of the vertical scalability (coverage of as many linguistics problems as possible) of sytems rather than their horizontal scalability (handling large data sets without significant deterioration of performance displayed on small sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Annotation Guidelines</head><p>The annotation part is the most critical part of any evaluation exercise. For the Logic Form Identification task the following steps were applied to obtain the correct LF for the test data:</p><p>1. logic forms for the test data were automatically obtained using an extended version of the LF derivation engine developed in <ref type="bibr" target="#b4">(Rus, 2002)</ref> for LFi of WordNet glosses. As part of this step, sentences were preprocessed: tokenized (separating punctuation from words) using the Penn Treebank guidelines, tagged with Brill's tagger <ref type="bibr" target="#b0">(Brill, 1992)</ref> and then parsed with Collins' statistical parser <ref type="bibr" target="#b1">(Collins, 1996)</ref>.</p><p>2. a first manual checking of the previously generated LF was done.</p><p>3. a second manual checking was done by another annotator.</p><p>4. quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency.</p><p>5. annotators agreement was done with a human moderator solving conflicting cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Metrics</head><p>Two performance measures to evaluate Logic Form Identification methods were developed by Rus in <ref type="bibr" target="#b4">(Rus and Moldovan, 2002)</ref> for the particular task of LFi for WordNet glosses (the definitions of concepts are shorter than regular sentences in terms of number of words, etc.). Each measure has advantages in some context. Predicate level performance is defined as the number of predicates with correct arguments divided by the total number of predicates. This measure focuses on the derivation method, though at a coarse-grained level because it does not capture the capability of a method to successfully identify a specific argument, e.g. the subject of a verb.</p><p>Gloss level performance is the number of entire glosses correctly transformed into logic forms divided by the total number of glosses attempted. This measure catches contextual capabilities of a method in that it gives an idea of how well a method performs at gloss level. It is a more appropriate measure when one tries to see the impact of using full glosses in logic forms to applications such as planning. This measure is specific to the particular task of LFi for concept definitions and thus is not suited for general open text tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us consider the following gloss from Word-Net:</head><p>Abbey is a convent ruled by an abbess. and let us suppose that some system, say Sys is able to generate the following logic form (please note that the subject of rule event is missing):</p><formula xml:id="formula_0">abbey(x1) &amp; be(e1, x1, x2) &amp; convent(x2) &amp; rule(e2, , x2) &amp; by(e2, x3) &amp; abbess(x3)</formula><p>Since one of the arguments is missing the predicate level performance is 5/6 (there are 6 predicates and for five of them the system generated all the arguments correctly) and the gloss level performance is 0/1 (this measure awards cases where all the predicates in the statement have all their arguments correctly assigned).</p><p>None of the two measures can distinguish between two systems, where one misses the subject of the rule event and the other misses both the subject and object (both systems will miss one predicate).</p><p>We propose two new, finer metrics in the next section, that are more suitable for a less restrictive LFi task: precision and recall. Both precision and recall can be defined at argument and predicate level, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Argument Level</head><p>We define Precision at argument level as the number of correctly identified arguments divided by the number of all identified arguments. Recall at argument level is the number of correctly identified arguments divided by the number of arguments that were supposed to be identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicate Level</head><p>Precision at predicate level is the number of correctly and fully identified predicates (with ALL arguments correctly identified) divided by the number of all attempted predicates. Recall at predicate level is the number of correctly and fully identified predicates (with ALL arguments correctly identified) divided by the number of all predicates that were supposed to be identified.</p><p>Let us suppose that some system outputs the following logic form for the above example: where x4 is incorrectly indentified as the direct object of eating event. In the correct output there are 11 slots to be filled and the predicate eat should have 4 arguments. The previously defined measures for the sample output are given in In addition, we report a more global measure called exact sentence which is defined as the number of sentences whose logic form was fully identified (all predicates and arguments correctly found) divided by the number of sentences attempted. This is similar to gloss level performance measure presented before. We proposed and computed several variants for it which are described below.</p><p>Sentence-Argument (Sent-A): How many sentences have ALL arguments correctly detected out of all attempted sentences.</p><p>Sentence-Predicate (Sent-P): How many sentences have ALL predicates correctly detected out of all attempted sentences.</p><p>Sentence-Argument-Predicate Sent-AP: How many sentences have ALL arguments correctly detected out of sentences which have ALL predicates correctly detected</p><p>Sentence-Argument-Predicate-Sentences Sent-APSent: How many sentences have ALL arguments and ALL predicates correctly detected out of all attempted sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Extra Resources</head><p>A package of trial data was provided to interested participants. The trial package contains two data files: (1) English sentences and (2) their corresponding logic form. A software evaluator was available for download on the web page of the task. We compiled a dictionary of collocations from WordNet which was also freely available for download. It includes 62,611 collocations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Submission Format</head><p>Each team was supposed to submit a file containing on each line the answer to a input sentence using the following pattern:</p><p>InstitutionShortName Y000 Sentence# Score :: Logic Form</p><p>Here is an example:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Input: The Earth provides the food we eat every day. Output: Earth:n (x1) provide:v (e1, x1, x2) food:n (x2) we(x3) eat:v (e2, x3, x2; x4) day:n (x4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Sample Output: Earth:n (x1) provide:v (e1, x1, x2) food:n (x2) we(x3) eat:v (e2, x3, x4) day:n (x4) Correct Output: Earth:n (x1) provide:v (e1, x1, x2) food:n (x2) we(x3) eat:v (e2, x3, x2, x4) day:n (x4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>:</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field Y000 was generated as is, for all lines. It will be used in future trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>Initially, there were 27 teams registered to participate in the Logic Form Identification task and 6 submissions were received by the deadline. One of the submissions was discarded since the file contained no valid data and another one was not included in the comparative results shown in Table <ref type="table">2</ref> since it used manual parsing (parsing is not a necessary step in obtaining the LF). The part of speech info attached to some predicates was ignored when computing the scores. We plan to use it in further trials.</p><p>If one looks at the results in the table one may notice their consistency. At Argument level precision and recall range from 0.729 to 0.776 and from 0.655 to 0.777, respectively. The same trend can be observed at Predicate level (the results are slighlty better). At a more coarse-grain level (Sentence level) the results vary more but still one can distinguish a certain degree of consistency: the Sent-A measure ranges from 0.160 to 0.256 and the Sent-AP measure varies from 0.386 to 0.510.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In the first attempt to systematically evaluate LFi systems we managed to provide a gold standard, a first software evaluator and proposed few performance metrics that can be used by researchers in the community to further study their approaches.</p><p>Among the drawbacks, it is worth mentioning the lack of training data which we plan to offer in the future.</p><p>The results reported by different systems constitute a lower bound of their approaches since the test data comprised raw sentences and thus the reported performances include errors coming from tokenization, part of speech tagging and parsing, wherever parsing was used.</p><p>Due to a tight schedule it was not possible to analyze the different approaches adopted by different systems but we hope the ACL meeting will provide the necessary background information and discussions to foster the development of such a study.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple rule-based part of speech tagger</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Applied Natural Language Processing</title>
				<meeting>the Third Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="152" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new statistical parser based on bigram lexical dependencies</title>
		<author>
			<persName><forename type="first">Michael John</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Collins</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics</title>
				<editor>
			<persName><forename type="first">Arivind</forename><surname>Joshi</surname></persName>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<meeting>the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of the TACITUS project</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">R</forename><surname>Hobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Logic Form transformation of wordNet and its Applicability to question answering</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">July. Association for Computational Linguistics</title>
				<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Logic Form for WordNet Glosses and Applications</title>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal for Tools with Artificial Intelligence. IEEE Computer Society</title>
		<imprint>
			<date type="published" when="2002-05" />
			<publisher>IEEE Press</publisher>
		</imprint>
		<respStmt>
			<orgName>Southern Methodist University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Phd thesis</note>
	<note>High precision logic form transformation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
