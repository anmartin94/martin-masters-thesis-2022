<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Hajič</surname><genName>jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
							<email>joakim.nivre@lingfil.uu.se</email>
							<affiliation key="aff1">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>5 Google</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<email>martin.potthast@uni-weimar.de</email>
							<affiliation key="aff4">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<addrLine>7 UiT</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">The Arctic University of Norway</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francis</forename><surname>Tyers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of the Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Memduh</forename><surname>Gökırmak</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Istanbul Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaroslava</forename><surname>Hlaváčová</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Václava</forename><surname>Kettnerová</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zdeňka</forename><surname>Urešová</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stina</forename><surname>Ojala</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Missilä</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Turku</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dima</forename><surname>Taji</surname></persName>
							<affiliation key="aff9">
								<orgName type="institution">New York University Abu Dhabi</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nizar</forename><surname>Habash</surname></persName>
							<affiliation key="aff9">
								<orgName type="institution">New York University Abu Dhabi</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Herman</forename><surname>Leung</surname></persName>
							<affiliation key="aff10">
								<orgName type="institution">City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<addrLine>14</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">University of Turin</orgName>
								<address>
									<addrLine>15</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff13">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<addrLine>16 IBM Research, 17 Nuance Communications, 18 INRIA -Paris 7, 19</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff14">
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<addrLine>20 DFKI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Simi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valeria</forename><surname>De Paiva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kira</forename><surname>Droganova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Héctor</forename><forename type="middle">Martínez</forename><surname>Alonso</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ç</forename><surname>Agrı Çöltekin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Istanbul Technical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vivien</forename><surname>Macketanz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aljoscha</forename><surname>Burchardt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kim</forename><surname>Harris</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katrin</forename><surname>Marheinecke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Georg</forename><surname>Rehm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tolga</forename><surname>Kayadelen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Elkahky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saran</forename><surname>Lertpradit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Mandl</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Fernandez Alcalde</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jana</forename><surname>Strnadová</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Esha</forename><surname>Banerjee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Stella</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Atsuko</forename><surname>Shimada</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sookyoung</forename><surname>Kwak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Mendonça</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Lando</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rattima</forename><surname>Nitisaroj</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a realworld setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks <ref type="bibr" target="#b6">(Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b31">Nivre et al., 2007)</ref> were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to-kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues. <ref type="bibr">1</ref> The focus of the 2017 task was learning syntactic dependency parsers that can work in a realworld setting, starting from raw text, and that can work over many typologically different languages, even surprise languages for which there is little or no training data, by exploiting a common syntactic annotation standard. This task has been made possible by the Universal Dependencies initiative (UD) <ref type="bibr" target="#b29">(Nivre et al., 2016)</ref>, which has developed treebanks for 50+ languages with crosslinguistically consistent annotation and recoverability of the original raw texts.</p><p>Participating systems had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation. No gold-standard annotation (tokenization, sentence segmentation, lemmas, morphology) was available in the input text. However, teams wishing to concentrate just on parsing were able to use segmentation and morphology predicted by the baseline UDPipe system <ref type="bibr" target="#b43">(Straka et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>In general, we wanted the participating systems to be able to use any data that is available free of charge for research and educational purposes (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. <ref type="bibr" target="#b31">Nivre et al. (2007)</ref>), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available.</p><p>In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora <ref type="bibr" target="#b45">(Tiedemann, 2012)</ref> to morphological transducers. Some of the resources were proposed by the participating teams.</p><p>We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Data: UD 2.0</head><p>Training and development data come from the Universal Dependencies (UD) 2.0 collection <ref type="bibr">(Nivre et al., 2017b)</ref>. Unlike previous UD releases, the test data was not included in UD 2.0. It was kept hidden until the evaluation phase of the shared task terminated. In some cases, the underlying texts had been known from previous UD releases but the annotation had not (UD 2.0 follows new annotation guidelines that are not backwardcompatible).</p><p>64 UD treebanks in 45 languages were available for training. 15 languages had two or more training treebanks from different sources, often also from different domains.</p><p>56 treebanks contained designated development data. Participants were asked not to use it for training proper but only for evaluation, development, tuning hyperparameters, doing error analysis etc. The 8 remaining treebanks were small and had only training data (and even these were extremely small in some cases, especially for Kazakh and Uyghur). For those treebanks cross-validation had to be used during development, but the entire dataset could be used for training once hyperparameters were determined.</p><p>Participants received the training and development data with gold-standard tokenization, sentence segmentation, POS tags and dependency relations; and for some languages also lemmas and/or morphological features.</p><p>Cross-domain and cross-language training was allowed and encouraged. Participants were free to train models on any combination of the training treebanks and apply it to any test set. They were even allowed to use the training portions of the 6 UD 2.0 treebanks that were excluded from evaluation (see Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supporting Data</head><p>To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw texts</head><p>The supporting raw data was gathered from CommonCrawl, which is a publicly available web crawl created and maintained by the non-profit CommonCrawl foundation. <ref type="bibr">2</ref> The data is publicly available in the Amazon cloud both as raw HTML and as plain text. It is collected from a number of independent crawls from 2008 to 2017, and totals petabytes in size.</p><p>We used cld2 3 as the language detection engine because of its speed, available Python bindings and large coverage of languages. Language detection was carried out on the first 1024 bytes of each plaintext document. Deduplication was carried out using hashed document URLs, a simple strategy found in our tests to be effective for coarse duplicate removal. The data for each language was capped at 100,000 tokens per a single input file.</p><p>Automatic tokenization, morphology and parsing The raw texts were further processed in order to generate automatic tokenization, segmentation, morphological annotations and dependency trees.</p><p>At first, basic cleaning was performed -paragraphs with erroneous encoding or less than 16 characters were dropped, remaining paragraphs converted to Normalization Form KC (NFKC) 4 and again deduplicated. Then the texts were segmented and tokenized, multi-word tokens split into words, and sentences with less than 5 words dropped. Because we wanted to publish the resulting corpus, we shuffled the sentences and also dropped sentences with more than 80 words at this point for licensing reasons. The segmentation and tokenization was obtained using the baseline UDPipe models described in Section 5. These models were also used to further generate automatic morphological annotations (lemmas, UPOS, XPOS and FEATS) and dependency trees.</p><p>The resulting corpus contains 5.9 M sentences and 90 G words in 45 languages and is available in CoNLL-U format . The perlanguage sizes of the corpus are listed in Table <ref type="table" target="#tab_1">1</ref> Precomputed word embeddings We also precomputed word embeddings using the segmented and tokenized plain texts. Because UD words can contain spaces, these in-word spaces were con-  verted to Unicode character NO-BREAK SPACE (U+00A0). <ref type="bibr">5</ref> The dimensionality of the word embeddings was chosen to be 100 after thorough discussion -more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that <ref type="bibr" target="#b2">Andor et al. (2016)</ref> achieved state-of-the-art results with 64 dimensions.</p><p>The word embeddings were precomputed using word2vec <ref type="bibr" target="#b26">(Mikolov et al., 2013)</ref> with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Test Data: UD 2.0</head><p>The main part of test data comprises test sets corresponding to 63 of the 64 training treebanks. <ref type="bibr">6</ref> Test sets from two different treebanks of one language were evaluated separately as if they were different languages. Every test set contained at least 10,000 words or punctuation marks. UD 2.0 treebanks that were smaller than 10,000 words were excluded from the evaluation. Among the treebanks that were able to provide the required amount of test data, there are 8 treebanks so small that the remaining data could not be split to training and development portions; for two of them, the data left for training is only a tiny sample (529 words in Kazakh, 1662 in Uyghur). There was no upper limit on the test data; the largest treebank had a test set comprising 170K words.</p><p>Although the 63 test sets correspond to UD 2.0 treebanks, they were not released with UD 2.0. They were kept hidden and only published after the evaluation phase of the shared task <ref type="bibr">(Nivre et al., 2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">New Parallel Test Sets</head><p>In addition, there were test sets for which no corresponding training data sets exist: 4 "surprise" languages (described in Section 2.5) and 14 test sets of a new Parallel UD (PUD) treebank (described in this section). These test sets were created for this shared task, i.e., not included in any previous UD release.</p><p>The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia; 7 usually only a few sentences per source document. 750 sentences were originally English, the remaining 250 sentences come from German, French, Italian and Spanish texts. They were translated by professional translators to 14 languages (i.e., 15 languages with the original: Arabic, Chinese, English, French, German, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Thai and Turkish; but four languages-Chinese, Indonesian, Korean and Thai-were excluded from the shared task due to consistency issues). Translators were instructed to prefer translations closer to original grammatical structure, provided it is still a fluent sentence in the target language. In some cases, picking a correct translation was difficult because the translators did not see the context of the original document. The translations were organized at DFKI and text &amp; form, Germany; they were then tokenized, morphologically and syntactically annotated at Google following guidelines based on <ref type="bibr" target="#b25">McDonald et al. (2013)</ref>, and finally converted to proper UD v2 annotation style by volunteers from the UD community using the Udapi framework <ref type="bibr" target="#b34">(Popel et al., 2017)</ref>. <ref type="bibr">8</ref> Three additional translations (Czech, Finnish and Swedish) were contributed and annotated natively in UD v2 by teams from Charles University, University of Turku and Uppsala University, respectively.</p><p>The Google dependency representation predates Universal Dependencies, deriving from the scheme used by <ref type="bibr" target="#b25">McDonald et al. (2013)</ref>, i.e., Stanford Dependencies 2.0 with the option to make copula verbs heads (de Marneffe and Manning, 2008, section 4.7) and Google Universal POS tags <ref type="bibr" target="#b32">(Petrov et al., 2011)</ref>. Various tree transformations were needed to convert it to UD. <ref type="bibr">9</ref> For example, prepositions and copula verbs are phrasal heads in Google annotation but must be dependent function words in UD. Similarly, some POS tags differ in the two schemes; particularly hard were conjunc-tions, where the Google tag set does not distinguish coordinators (CCONJ in UD) from subordinators (SCONJ). Some bugs, for example where verbs had multiple subjects or objects, or where function words were not leaves, were detected automatically 10 and fixed manually.</p><p>Finally, the most prominent consistency issues lay in tokenization and word segmentation, especially in languages where it interacts with morphology or where the writing system does not clearly mark word boundaries. The tokenizers used before manual annotation were not necessarily compatible with existing UD treebanks, yet in the shared task it was essential to make the segmentation consistent with the training data. We were able to fix some problems, such as unmarked multi-word tokens in European languages, <ref type="bibr">11</ref> and we were even able to re-segment Japanese (note that this often involved new dependency relations); on the other hand, we had to exclude Korean for not being able to fix it in time.</p><p>Many transformations were specific to individual languages. For example, in the original tokenization of Arabic, the definite article al-was separated from the modified word, which is comparable to the D3 tokenization scheme <ref type="bibr" target="#b16">(Habash, 2010)</ref>. This scheme was inconsistent with the tokenization of the Arabic training data, hence it had to be changed. Text-level normalization further involved removal of the shadda diacritical mark (marking consonant gemination), which is optional in Arabic orthography and does not occur in the training data. On the POS level, the active and passive participles and verbal nouns (masdars) were annotated as verbs. For Arabic, however, these should be mapped to NOUN. Once we changed the tags, we also had to modify the surrounding relations to those used with nominals.</p><p>Like some UD treebanks, the parallel data contains information on document boundaries. They are projected as empty lines to the raw text presented to parsers, and they can be exploited to improve sentence segmentation. Note that due to the way the sentences were collected, the paragraphs are rather short. <ref type="bibr">12</ref> The fact that the data is parallel was not exploited in this task. Participating systems were told the language code so they could select an appropriate model. All parallel test sets were in languages that have at least one training treebank in UD 2.0 (although the domain may differ).</p><p>After the evaluation phase these parallel test sets were published together with the main test data; in the future they will become part of regular UD releases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Surprise Languages</head><p>The second type of additional test sets were surprise languages, which had not been previously released in UD. Names of surprise languages (Buryat, Kurmanji Kurdish, North Sámi and Upper Sorbian) and small samples of gold-standard data (about 20 sentences) were published one week before the beginning of the evaluation phase. Crawled raw texts were provided too, though in much smaller quantity than for the other languages. The point of having surprise languages was to encourage participants to pursue truly multilingual approaches to parsing, utilizing data from other languages.</p><p>As with all other test sets, the systems were able to use segmentation and part-of-speech tags predicted by the baseline UDPipe system (in this case UDPipe was trained and applied in a 10-fold cross-validation manner directly on the test data; hence this is the only annotation that the participants were given but could not produce with their own models).</p><p>Note that the smallest non-surprise languages (Kazakh, Uyghur) were asking for multilingual approaches as well, given that the amount of their own training data was close to zero. The difference was that participants at least knew in advance what these languages were and had more time to determine the most suitable training model. On the other hand, the segmentation and tagging models for these languages were only trained on the tiny training data, i.e., they were much worse than the models for the surprise languages. In this sense parsing of Kazakh and Uyghur was even harder than parsing the surprise languages.</p><p>When compared to the training data available in UD 2.0, the genetically closest language to Kazakh and Uyghur is Turkish; but it uses a dif-UD Arabic treebank. This gave an advantage to systems that were able to take paragraph boundaries into account, including those that re-used the baseline segmentation.</p><p>ferent writing system, and the Turkish dataset itself is not particularly large. For Kurmanji Kurdish, the closest relative is Persian, again with different script and other reservations. Buryat is a Mongolic language written in Cyrillic script and does not have any close relative in UD. North Sámi is an Finno-Ugric language; Finnish and Estonian UD data could be expected to be somewhat similar. Finally, Upper Sorbian is a West Slavic language spoken in Germany; among the many Slavic languages in UD, Czech and Polish are its closest relatives.</p><p>In summary, the test data consisted of 81 files in 49 languages (55 test sets from "big" UD 2.0 treebanks, 8 "small" treebanks, 14 parallel test sets and 4 surprise-language test sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics</head><p>The standard evaluation metric of dependency parsing is the labeled attachment score (LAS), i.e., the percentage of nodes with correctly assigned reference to parent node, including the label (type) of the relation. When parsers are applied to raw text, the metric must be adjusted to the possibility that the number of nodes in gold-standard annotation and in the system output vary. Therefore, the evaluation starts with aligning system nodes and gold nodes. A dependency relation cannot be counted as correct if one of the nodes could not be aligned to a gold node. LAS is then re-defined as the harmonic mean (F 1 ) of precision P and recall R, where</p><formula xml:id="formula_0">P = #correctRelations #systemNodes (1) R = #correctRelations #goldNodes (2) LAS = 2P R P + R (3)</formula><p>Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 81 test files and a macroaverage of all these scores serves as the main metric for system ranking in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token Alignment</head><p>UD defines two levels of token/word segmentation. The lower level corresponds to what is usually understood as tokenization. However, unlike some popular tokenization schemes, it does not include any normalization of the non-whitespace characters. We can safely assume that any two tokenizations of a text differ only in whitespace while the remaining characters are identical. There is thus a 1-1 mapping between gold and system nonwhitespace characters, and two tokens are aligned if all their characters match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Word Alignment</head><p>The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu "to" and the article dem "the".</p><p>Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs are involved. Fortunately, the CoNLL-U file format used in UD clearly marks all MWTs so we can detect them both in system output and in gold data. Whenever one or more MWTs have overlapping spans of surface character offsets, the longest common subsequence algorithm is used to align syntactic words within these spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Segmentation</head><p>Words are aligned and dependencies are evaluated in the entire file without considering sentence segmentation. Still, the accuracy of sentence boundaries has an indirect impact on LAS: any missing or extra sentence boundary necessarily makes one or more dependency relations incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Invalid Output</head><p>If a system fails to produce one of the 81 files or if the file is not valid CoNLL-U format, the score of that file (counting towards the system's macroaverage) is zero.</p><p>Formal validity is defined more leniently than for UD-released treebanks. For example, a nonexistent dependency type does not render the whole file invalid, it only costs the system one incorrect relation. However, cycles and multi-root sentences are disallowed. A file is also invalid if there are character mismatches that could make the token alignment algorithm fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CLAS</head><p>Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison <ref type="bibr" target="#b30">(Nivre and Fang, 2017)</ref>. It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation.</p><p>As CLAS is still experimental, we have designated full LAS as our main evaluation metric; nevertheless, a large evaluation campaign like this is a great opportunity to study the behavior of the new metric, and we present both scores in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methodology</head><p>Key goals of any empirical evaluation are to ensure a blind evaluation, its replicability, and its reproducibility. To facilitate these goals, we employed the cloud-based evaluation platform TIRA <ref type="bibr" target="#b35">(Potthast et al., 2014)</ref>, <ref type="bibr">13</ref> which implements the evaluation as a service paradigm <ref type="bibr" target="#b17">(Hanbury et al., 2015)</ref>. In doing so, we depart from the traditional submission of system output to shared tasks, which lacks in these regards, toward the submission of working software. Naturally, software submissions bring about additional overhead for both organizers and participants, whereas the goal of an evaluation platform like TIRA is to reduce this overhead to a bearable level. Still being an early prototype, though, TIRA fulfills this goal only with some reservations. Nevertheless, the scale of the CoNLL 2017 UD Shared Task also served as a test of scalability of the evaluation as a service paradigm in general as well as that of TIRA in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Blind Evaluation</head><p>Traditionally, evaluations in shared tasks are halfblind (the test data are shared with participants while the ground truth is withheld), whereas outside shared tasks, say, during paper-writing, evaluations are typically pseudo-blind (the test data and ground truth are accessible, yet, ignored until the to-be-evaluated software is ready). In both cases, remaining blind to the test data is one of the cornerstones of evaluation, and has a significant impact on the validity of evaluation results. While outside shared tasks, one can only trust that paper authors do not spoil their evaluation by implicitly or explicitly exploiting their knowledge of the test data, within shared tasks, another factor comes into play, namely the fact that shared tasks are also competitions.</p><p>Dependent on its prestige, winning a shared task comes along with a lot of visibility, so that supplying participants with the test data up front bears risks of mistakes that spoil the ground truth, and of cheating. Here, TIRA implements a proper solution which ensures blind evaluation, an airlock for data. On demand, software deployed at TIRA is locked in the datalock together with the test data, where it can process the data and have its output recorded. Otherwise, all communication channels to the outside are closed or tightly moderated to prevent data leakage. However, closing down all communication channels also has its downsides, since participants cannot check up on their running software anymore, or have to ask organizers to do so, which increases the turnaround time to fix bugs. Participants were only able to learn whether they achieved a non-zero score on each of the 81 test files; a zero score signaled a bug, in which case the task moderator would make the diagnostic output visible to the participants. Such interaction was only possible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Replicability and Reproducibility</head><p>The replicability of an evaluation depends on whether the same results can be obtained from re-running an experiment using the same setup, whereas reproducibility refers to achieving results that are commensurate with a reference evaluation, for instance, when exchanging the test data with alternative test data. Both are important aspects of an evaluation, the former pertaining to its reliability, and the latter to its validity. Ensuring both requires that a to-be-evaluated software is preserved in working condition for as long as possible. Traditionally, shared tasks do not take charge of participant software preservation, mostly because the software remains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within.</p><p>Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub. 14</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Resource Allocation</head><p>The allocation of an appropriate amount of computing resources (especially CPUs and RAM, whereas disk space is cheap enough) to each participant proved to be difficult, since minimal requirements were unknown. When asked, participants typically request liberal amounts of resources, just to be on the safe side, whereas assigning too much up front would not be economical nor scale well. We hence applied a least commitment strategy with an initial assignment of 1 CPU and 4 GB RAM. More resources were granted on request, the limit being the size of the underlying hardware. When it comes to exploiting available resources, a lot depends on programming prowess, whereas more resources do not necessarily translate into better performance. This is best exemplified by the fact that with 4 CPUs and 16 GB RAM, the winning team Stanford used only a quarter the amount of resources of the second and third winners, respectively. The team on fourth (sixth) place was even more frugal, getting by with 1 CPU and 8 GB RAM (4 GB RAM). All of the aforementioned teams' approaches exceed the LAS level of 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">UDPipe</head><p>We prepared a set of baseline models using UD-Pipe <ref type="bibr" target="#b43">(Straka et al., 2016</ref>) version 1.1. A slightly improved version-UDPipe 1.2-was submitted by <ref type="bibr" target="#b44">Straka and Straková (2017)</ref> as one of the competing systems. <ref type="bibr" target="#b44">Straka and Straková (2017)</ref> describe both these versions in more detail.</p><p>The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack-knifing (cross-validation), are available on-line <ref type="bibr" target="#b41">(Straka, 2017)</ref>.</p><p>UDPipe baseline models are able to reconstruct nearly all annotation from CoNLL-U files -they can generate segmentation, tokenization, multiword token splitting, morphological annotation (lemmas, UPOS, XPOS and FEATS) and dependency trees. Participants were free to use any part of the model in their systems -for all test sets, we provided UDPipe processed variants in addition to raw text inputs. We provided the UD-Pipe processed variant even for surprise languages -however, only segmentation, tokenization and morphology, generated by 10-fold jack-knifing, as described in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline UDPipe Shared Task System</head><p>We further used the baseline models as a baseline system in the shared task. We used the corresponding models for the UD 2.0 test data.</p><p>For the new parallel treebanks, we used UD 2.0 baseline models of the corresponding languages. If there were several treebanks for one language, we arbitrarily chose the one named after the language only (e.g., we chose ru and not ru syntagrus). Unfortunately, we did not explicitly mention this choice to the participants and this arbitrary choice had a large impact on resultssome contestant systems fell below UDPipe baseline just because of choosing different treebanks to train on for the parallel treebanks. (On the other hand, there was no guarantee that the models selected in the baseline system would be optimal.)</p><p>For each surprise language, we also chose one baseline model to apply. Even if most words are unknown to the baseline model, universal POS tags can be used to drive the parsing, making the baseline model act similar to a delexicalized parser. We chose a baseline model to maximize Team LAS 1. Stanford <ref type="bibr">(Dozat et al.)</ref> 76.30 2. C2L2 <ref type="bibr">(Shi et al.)</ref> 75.00 3. IMS <ref type="bibr">(Björkelund et al.)</ref> 74.42 4. HIT-SCIR <ref type="bibr">(Che et al.)</ref> 72.11 5. LATTICE <ref type="bibr">(Lim and Poibeau)</ref> 70.93 6. NAIST SATO <ref type="bibr">(Sato et al.)</ref> 70.14 7. Koç University <ref type="bibr">(Kırnap et al.)</ref> 69   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SyntaxNet</head><p>Another set of baseline models was prepared by <ref type="bibr" target="#b1">Alberti et al. (2017)</ref> based on improved version of the SyntaxNet system <ref type="bibr" target="#b2">(Andor et al., 2016)</ref>. Pretrained models were provided for UD 2.0 data. However, no SyntaxNet models were prepared for the surprise languages, therefore, the Syn-taxNet baseline is not part of the official results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Official Parsing Results</head><p>Table <ref type="table" target="#tab_3">2</ref> gives the main ranking of participating systems by the LAS F 1 score macro-averaged over all 81 test files. The table also shows the performance of the baseline UDPipe system; the baseline is relatively strong and only 12 of the 32 systems managed to outperform it.</p><p>We used bootstrap resampling to compute 95% confidence intervals: they are in the range ±0.11 to ±0.15 (% LAS) for all systems except the three lowest-scoring ones. We used paired bootstrap resampling to compute whether the difference in LAS is significant (p &lt; 0.05) for each pair of systems. 15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Secondary Metrics</head><p>In addition to the main LAS ranking, we evaluated the systems along multiple other axes, which may  shed more light on their strengths and weaknesses. This section provides an overview of selected secondary metrics for systems matching or surpassing the baseline; a large number of additional results is available at the shared task website. <ref type="bibr">16</ref> The website also features a LAS ranking of unofficial system runs, i.e. those that were not marked by their teams as primary runs, or were even run after the official evaluation phase closed and test data were unblinded. At least two differences from the official results are remarkable; both seem to be partially inflicted by the blind evaluation on TIRA and the inability of the participants to see the diagnostic messages from their software. In the first case, the Dynet library seems to pro-  duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team <ref type="bibr" target="#b10">(de Lhoneux et al., 2017)</ref> the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 (6th place).</p><p>Table <ref type="table" target="#tab_4">3</ref> ranks the systems by CLAS instead of LAS (see Section 3.5). The scores are lower than LAS but differences in system ranking are minimal, possibly indicating that optimization towards one of the metrics does not make the parser bad with respect to the other.</p><p>Table <ref type="table" target="#tab_6">4</ref> evaluates detection of tokens, syntactic words and sentences. Half of the systems simply trusted the segmentation offered by the baseline system. 7 systems were able to improve baseline segmentation. For most languages and in aggregate, the ability to improve parsing scores through better segmentation was probably negligible, but for a few languages, such as Chinese and Vietnamese, the UDPipe baseline segmentation was not so strong and several teams, notably IMS, appear to have improved their LAS by several percent through use of improved segmentation.</p><p>The systems were not required to generate any morphological annotation (part-of-speech tags, features or lemmas). Some parsers do not even need morphology and learn to predict syntactic dependencies directly from text. Nevertheless, systems that did output POS tags, and had them at least as good as the baseline system, are evaluated in Table <ref type="table" target="#tab_8">5</ref>. Note that as with segmentation, morphology predicted by the baseline system was available and some systems simply copied it to the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Partial Results</head><p>Table <ref type="table" target="#tab_10">6</ref> gives the LAS F 1 score averaged over the 55 "big" treebanks (training data larger than test data, development data available). Higher scores reflect the fact that models for these test sets are easier to learn: enough data is available, no cross-lingual or cross-domain learning is necessary (the parallel test sets are not included here). When compared to Table <ref type="table" target="#tab_3">2</ref>, four new teams now surpass the baseline, LyS-FASTPARSE being the best among them. The likely explanation is that the systems can learn good models but are not so good at picking the right model for unknown domains and languages.</p><p>Table <ref type="table" target="#tab_12">7</ref> gives the LAS F 1 score on the four surprise languages only. The globally best system, Stanford, now falls back to the fourth rank while C2L2 (Cornell University) apparently employs the most successful strategy for underresourced languages. Another immediate observation is that our surprise languages are very hard to parse; accuracy under 50% is hardly useful for any downstream processing. However, there are significant language-by-language differences, the best score on Upper Sorbian surpassing 60%. This proba-  bly owes to the presence of many Slavic treebanks in training data, including some of the largest datasets in UD.</p><p>In contrast, the results on the 8 small nonsurprise treebanks (Table <ref type="table" target="#tab_13">8</ref>) are higher on average, but again the variance is huge. Uyghur (best score 43.51) is worse than three surprise languages, and Kazakh (best score 29.22) is the least parsable test set of all (see Table <ref type="table" target="#tab_1">10</ref>). These two treebanks are outliers in the size of training data (529 words Kazakh and 1662 words Uyghur, while the other "small" treebanks have between 10K and 20K words). However, the only "training data" of the surprise languages are samples of 147 to 460 words, yet they seem to be easier for some systems. It would be interesting to know whether the more successful systems took a similar approach to Kazakh and Uyghur as to the surprise languages.</p><p>Table <ref type="table">9</ref> gives the average LAS on the 14 new parallel test sets (PUD). Three of them (Turkish, Arabic and Hindi) proved difficult to parse for any model trained on the UD 2.0 training data; it seems likely that besides domain differences, inconsistent application of the UD annotation guidelines played a role, too.</p><p>See Table <ref type="table" target="#tab_1">10</ref> for a ranking of all test sets by the best LAS achieved on them by any parser. Note that this cannot be directly interpreted as a   ranking of languages by their parsing difficulty: many treebanks have high ranks simply because the corresponding training data is large. The table also gives a secondary ranking by CLAS and indicates the system that achieved the best LAS / CLAS (mostly the same system won by both metrics). Finally, the best score of word and sentence segmentation is given (without indicating the best-scoring system). Vietnamese proved to be the hardest language in terms of word segmentation; it is not surprising given that its writ- Table <ref type="table">9</ref>: Average attachment score on the 14 parallel test sets (PUD).</p><p>ing system allows spaces inside words. Second hardest was Hebrew, probably due to a large number of multi-word tokens. In both cases the poor segmentation correlates with poor parsing accuracy. Sentence segmentation was particularly difficult for treebanks without punctuation, i.e., most of the classical languages and spoken data (the best score achieved on the Spoken Slovenian Treebank is only 21.41%). On the other hand, the paragraph boundaries available in some treebanks made sentence detection significantly easier (the extreme being Arabic PUD with one sentence per paragraph; some systems were able to exploit this anomaly and get 100% correct segmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis of Submitted Systems</head><p>Table <ref type="table" target="#tab_1">11</ref> gives an overview of 29 of the systems evaluated in the shared task. The overview is based on a post-evaluation questionnaire to which 29 of 32 teams responded. The abbreviations used in Table <ref type="table" target="#tab_1">11</ref> are explained in Table <ref type="table" target="#tab_1">12</ref>.</p><p>As we can see from Table <ref type="table" target="#tab_1">11</ref>, the typical system uses the baseline models for segmentation and morphological analysis (including part-of-speech tagging), employs a single parsing model with pretrained word embeddings provided by the organizers, and does not make use of any additional data. For readability, all the cells corresponding to use of baseline models (and lack of additional data) have been shaded gray.</p><p>Only 7 teams have developed their own word and sentence segmenters, while an additional 5  When it comes to part-of-speech tags and morphology, 7 teams use their own systems and 4 use modified versions of the baseline, while 2 teams predict tags jointly with parsing and 3 teams do not predict morphology at all. For parsing, most teams use a single parsing model -transition-based, graph-based or even rule-based -but 4 teams build ensemble systems in one way or the other. It is worth noting that, whereas the C2L2 and IMS systems are ensembles, the winning Stanford system is not, which makes its performance even more impressive.</p><p>The majority of parsers incorporate pre-trained word embeddings. Only 3 parsers use word embeddings without pre-training, and only 4 parsers do not incorporate word embeddings at all. Except for training word embeddings, the additional data provided (or permitted) appears to have been used very sparingly.</p><p>When it comes to the surprise languages (and some of the other low-resource languages), the dominant approach is to use a cross-lingual parser, single-or multi-source, and often delexicalized. Finally, for the parallel test sets, most teams have picked a model trained on a single treebank from the same language, but at least 4 teams have trained models on multiple treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The CoNLL 2017 Shared Task on UD parsing was novel in several respects. Besides using crosslinguistically consistent linguistic representations and emphasizing end-to-end processing of text, as discussed in the introduction, it was unusual also in featuring a very large number of languages, in integrating cross-lingual learning for resourcepoor languages, and in using a multiply parallel test set.</p><p>It was the first large-scale evaluation on data annotated in the Universal Dependencies style. For most UD languages the results represent a new state of the art for dependency parsing. The numbers are not directly comparable to some older work for various reasons (different annotation schemes, gold-standard POS tags, tokenization etc.) but the way the task was organized should ensure their reproducibility and comparability in the future. Furthermore, parsing results are now more comparable across languages than ever before.  Two new language resources were produced whose usefulness reaches far beyond the task itself: A UD-style parallel treebank in 18 languages, and a large, web-crawled parsebank in 48 languages, over 90 billion words in total.</p><p>The analysis of the shared task results has so far only scratched the surface, and we refer to the system description papers for more in-depth analysis of individual systems and their performance. For many previous CoNLL shared tasks, the task itself has only been the starting point of a long and fruitful research strand, enabled by the resources created for the task. We hope and believe that the 2017 UD parsing task will join this tradition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The supporting data overview: the number of words (M = million; K = thousand) for each language.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ranking of the participating systems by the main evaluation metric, the labeled attachment F 1 -score, macro-averaged over 81 test sets.</figDesc><table><row><cell>Pairs of systems with significantly (p &lt; 0.05) dif-</cell></row><row><cell>ferent LAS are separated by a line. Names of</cell></row><row><cell>several teams are abbreviated in the table: LyS-</cell></row><row><cell>FASTPARSE, OpenU NLP Lab, Orange -Deskiñ</cell></row><row><cell>andÚFAL -UDPipe 1.2. Citations refer to the</cell></row><row><cell>corresponding system-description papers in this</cell></row><row><cell>volume.</cell></row><row><cell>the accuracy on the released sample for each sur-</cell></row><row><cell>prise language, resulting in Finnish FTB, Polish,</cell></row><row><cell>Finnish FTB and Slovak models for the surprise</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average CLAS F 1 score.</figDesc><table /><note>languages Buryat, Kurmanji, North Sámi and Upper Sorbian, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Tokenization, word segmentation and</cell></row><row><cell>sentence segmentation (ordered by word F 1</cell></row><row><cell>scores; out-of-order scores in the other two</cell></row><row><cell>columns are bold).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Universal POS tags, features and lemmas (ordered by UPOS F 1 scores).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Average attachment score on the 55 "big" treebanks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: Average attachment score on the 4 sur-</cell></row><row><cell cols="2">prise languages: Buryat (bxr), Kurmanji (kmr),</cell></row><row><cell cols="2">North Sámi (sme) and Upper Sorbian (hsb).</cell></row><row><cell>Team</cell><cell>LAS F 1</cell></row><row><cell>1. C2L2 (Ithaca)</cell><cell>61.49</cell></row><row><cell>2. Stanford (Stanford)</cell><cell>61.02</cell></row><row><cell>3. IMS (Stuttgart)</cell><cell>58.76</cell></row><row><cell>4. LATTICE (Paris)</cell><cell>54.78</cell></row><row><cell>5. HIT-SCIR (Harbin)</cell><cell>54.77</cell></row><row><cell>6. fbaml (Palo Alto)</cell><cell>54.64</cell></row><row><cell>7. RACAI (Bucureşti)</cell><cell>54.26</cell></row><row><cell>8. TurkuNLP (Turku)</cell><cell>54.19</cell></row><row><cell>9.ÚFAL -UDPipe 1.2 (Praha)</cell><cell>53.76</cell></row><row><cell>10. NAIST SATO (Nara)</cell><cell>53.52</cell></row><row><cell>11. Koç University (İstanbul)</cell><cell>53.36</cell></row><row><cell>12. darc (Tübingen)</cell><cell>52.46</cell></row><row><cell>13. UALING (Tucson)</cell><cell>52.27</cell></row><row><cell>14. Wanghao-ftd-SJTU (Shanghai)</cell><cell>52.13</cell></row><row><cell>15. BASELINE UDPipe 1.1</cell><cell>51.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Average attachment score on the 8</cell></row><row><cell>small treebanks: French ParTUT, Galician Tree-</cell></row><row><cell>Gal, Irish, Kazakh, Latin, Slovenian SST, Uyghur</cell></row><row><cell>and Ukrainian.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Treebank ranking by best parser LAS. Bold CLAS is higher than the preceding one. Best F 1 of word and sentence segmentation is also shown. ISO 639 language codes are optionally followed by a treebank code. teams have retrained or improved the baseline models, or combined them with other techniques.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Classification of participating systems. The second column repeats the main system ranking.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects-for example SANCL(Petrov and Mc- Donald, 2012)  or SPMRL<ref type="bibr" target="#b39">(Seddah et al., 2013</ref><ref type="bibr" target="#b38">(Seddah et al., , 2014</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Using udpipe --output=horizontal.6  We had to withdraw the test set from the Italian ParTUT treebank because it turned out to significantly overlap with the training data of the larger Italian treebank in UD 2.0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The two domains are encoded in sentence ids but this information is not visible to the systems participating in the shared task.8 http://udapi.github.io/ 9 using ud.Google2ud from the Udapi framework</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">using ud.MarkBugs from the Udapi framework 11 using Udapi's ud.de.AddMwt for German, and similarly for Spanish (es), French (fr) and Portuguese (pt). For all languages, we applied ud.ComplyWithText to make sure the concatenation of tokens matches exactly the original raw text.12 A special case is Arabic where we artificially marked every sentence as a separate paragraph, to make it more consistent with somewhat unusual segmentation of the existing</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">http://www.tira.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://github.com/CoNLL-UD-2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">using Udapi's eval.Conll17, marked by the presence or absence of vertical lines in Table2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to all the contributors to Universal Dependencies; without their effort a task like this simply wouldn't be possible.</p><p>The work described herein, including data preparation for the CoNLL 2017 UD Shared Task, has been supported by the following grants and projects: "CRACKER," H2020 Project No. 645357 of the European Commission; "MANYLA," Grant No. GA15-10472S of the Grant Agency of the Czech Republic; FIN-CLARIN.</p><p>The data for the CoNLL 2017 UD Shared Task are available via the LINDAT/CLARIN repository, which is part of a research infrastructure project funded by the Ministry of Education, Youth and Sports of the Czech Republic, Project. No. LM2015071.</p><p>The parallel evaluation set was made possible by contributions from DFKI (sentence selection and translation), Google (initial treebanking) and UD volunteers (translation to additional languages, annotation and conversion to UD v2). </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Initial explorations of CCG supertagging for Universal Dependency parsing</title>
		<author>
			<persName><forename type="first">Heval</forename><surname>Burak Kerim Akkuş</surname></persName>
		</author>
		<author>
			<persName><surname>Azizoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Bogatyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chayut</forename><surname>Thanapirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<idno>abs/1703.04929</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-1231.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIMSI@CoNLL&apos;17: UD shared task</title>
		<author>
			<persName><forename type="first">Lauriane</forename><surname>Aufrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IMS at the CoNLL 2017 UD shared task: CRFs and perceptrons meet neural networks</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Faleńska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL</title>
				<meeting>the CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/W/W06/W06-29.pdf#page=165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X). Association for Computational Linguistics</title>
				<meeting>the 10th Conference on Computational Natural Language Learning (CoNLL-X). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The HIT-SCIR system for end-to-end parsing of Universal Dependencies</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delexicalized transfer parsing for low-resource languages using transformed and combined treebanks</title>
		<author>
			<persName><forename type="first">Ayan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaffar</forename><surname>Affan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ParisNLP entry at the CoNLL UD shared task 2017: A tale of a #parsingtragedy</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From raw text to Universal Dependencies -look, no tags!</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliyahu</forename><surname>Basirat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the CoNLL 2017 shared task</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RACAI&apos;s natural language processing pipeline for Universal Dependencies</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Daniel Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiberiu</forename><surname>Boroş</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tufiş</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rule-based system for cross-lingual parsing of Romance languages with universal dependencies</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gamallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-1989" />
		<title level="m">CoNLL 2017 shared taskautomatically annotated raw texts and word embeddings. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to Arabic natural language processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nizar</surname></persName>
		</author>
		<author>
			<persName><surname>Habash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Evaluation-as-a-Service: Overview and Outlook</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torben</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.07454" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-model and crosslingual dependency analysis</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munshi</forename><surname>Asadullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Corpus selection approaches for multilingual parsing from raw text to Universal Dependencies</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hornby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyeul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast and lightweight system for multilingual dependency parsing</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A semi-universal pipelined approach to the CoNLL 2017 UD shared task</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayasu</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsumasa</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TurkuNLP: Delexicalized pre-training of word embeddings for dependency parsing</title>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing with context embeddings</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Kırnap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkay</forename><surname>Furkanönder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A system for multilingual dependency parsing based on bidirectional LSTM feature representations</title>
		<author>
			<persName><forename type="first">Kyungtae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Núria</forename><surname>Bertomeu Castelló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungmee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geneva DINN parser: a neural network dependency parser ten years later</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Universal Dependencies 2.0. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Mustafina</surname></persName>
		</author>
		<author>
			<persName><surname>Müürisep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huyền Nguyễn Thị</forename><surname>Lương Nguyễn Thị</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stina</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petya</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Øvrelid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cenel-Augusto</forename><surname>Passarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Perrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jussi</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Piitulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauma</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Pretkalniņa ; Livy Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Rituma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shadi</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiba</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Saulīte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojgan</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Seraji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Shakurova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Sichinava</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-1983" />
		<editor>Zsolt Szántó, Dima Taji, Takaaki Tanaka, Reut Tsarfaty, Francis Tyers, Sumire Uematsu, Larraitz Uria, Gertjan van Noord, Viktor Varga, Veronika Vincze, Jonathan North Washington, ZdeněkŽabokrtský, Amir Zeldes, Daniel Zeman, and Hanzhi Zhu</editor>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, MáriaŠimková, Kiril Simov, Aaron Smith, Alane Suhr; Prague</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Prokopis Prokopidis, Tiina Puolakainen, Sampo Pyysalo, Alexandre Rademaker, Loganathan Ramasamy</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal Dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation</title>
				<meeting>the 10th International Conference on Language Resources and Evaluation<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1659" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal dependency evaluation</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiao-Ting</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies</title>
				<meeting>the NoDaLiDa 2017 Workshop on Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="86" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D/D07/D07-" />
		<imprint>
			<date type="published" when="2007" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno>abs/1104.2086</idno>
		<ptr target="http://arxiv.org/abs/1104.2086" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<ptr target="http://www.petrovi.de/data/sancl12.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)</title>
				<meeting>the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Udapi: Universal API for universal dependencies</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdeněkžabokrtský</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vojtek</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W17/W17-0412.pdf" />
	</analytic>
	<monogr>
		<title level="j">Workshop on Universal Dependencies. Göteborgs universitet</title>
		<imprint>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving the reproducibility of PAN&apos;s shared tasks: Plagiarism detection, author identification, and author profiling</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11382-1_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11382-1_22" />
	</analytic>
	<monogr>
		<title level="m">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 5th International Conference of the CLEF Initiative</title>
				<editor>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
			<persName><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</editor>
		<meeting><address><addrLine>Paul Clough, Mark Sanderson, Mark Hall, Allan Hanbury, and Elaine Toms; Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="268" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A non-DNN feature engineering approach to dependency parsing -FBAML at CoNLL 2017 shared task</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial training for crossdomain universal dependency parsing</title>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hitoshi</forename><surname>Manabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages</title>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-6111" />
	</analytic>
	<monogr>
		<title level="m">First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
				<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Overview of the SPMRL 2013 shared task: Cross-framework evaluation of parsing morphologically rich languages</title>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koldo</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Przepiórkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Woliński</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-4917" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages. Association for Computational Linguistics</title>
				<meeting>the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
	<note>Alina Wróblewska, and Eric Villemonte de la Clérgerie</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining global models for parsing Universal Dependencies</title>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<title level="m">CoNLL 2017 shared task -UD-Pipe baseline models and supplementary materials</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="http://hdl.handle.net/" />
		<title level="m">LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
				<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">UD-Pipe: trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation</title>
				<meeting>the 10th International Conference on Language Resources and Evaluation<address><addrLine>Portorož</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="Slove" to=" nia" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bente</forename><surname>Mehmet Ugur Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</title>
				<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
	<note>Nicoletta Calzolari (Conference Chair)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">UParse: the Edinburgh system for the CoNLL 2017 UD shared task</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A non-projective greedy dependency parser with bidirectional LSTMs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A transition-based system for universal dependency parsing</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The parse is darc and full of errors: Universal dependency parsing with transition-based and graph-based algorithms</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Sofroniev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Schill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</title>
				<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
