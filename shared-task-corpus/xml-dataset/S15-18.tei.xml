<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Computer and Information Science</orgName>
								<orgName type="institution" key="instit1">University of Oslo</orgName>
								<orgName type="institution" key="instit2">Potsdam University</orgName>
								<orgName type="institution" key="instit3">Linköping University</orgName>
								<orgName type="institution" key="instit4">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvie</forename><surname>Cinková</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zdeňka</forename><surname>Urešová</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for the Study of Language and Information</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background and Motivation</head><p>Syntactic dependency parsing has seen great advances in the past decade, but tree-oriented parsers are ill-suited for producing meaning representations, i.e. moving from the analysis of grammatical structure to sentence semantics. Even if syntactic parsing arguably can be limited to tree structures, this is not the case in semantic analysis, where a node will often be the argument of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 18 at SemEval 2015, Broad-Coverage Semantic Dependency Parsing (SDP 2015), 1 seeks to stimulate the parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom?</p><p>Extending the very similar predecessor task SDP 2014 <ref type="bibr" target="#b23">(Oepen et al., 2014)</ref>, we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; <ref type="bibr" target="#b16">Marcus et al., 1993)</ref> for English, as well as comparable resources for Chinese and Czech. Figure <ref type="figure" target="#fig_0">1</ref> below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence:</p><p>(1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice.</p><p>Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacuous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees.</p><p>Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; <ref type="bibr" target="#b10">Gildea &amp; Jurafsky, 2002)</ref>. <ref type="bibr">2</ref> However, we require parsers to identify 'full-A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:  sentence' semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed 'semantic parsing', which <ref type="bibr" target="#b15">Kate and Wong (2010)</ref> define as "the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application." In contrast to much work in this tradition, our SDP target representations aim to be task-and domain-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Target Representations</head><p>We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure <ref type="figure" target="#fig_0">1</ref>), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction-often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques.</p><p>tivations for the SDP task series. Please see <ref type="bibr" target="#b23">Oepen et al. (2014)</ref> and <ref type="bibr" target="#b21">Miyao et al. (2014)</ref> for additional background.</p><p>DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed Deep-Bank, of Sections 00-21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar <ref type="bibr" target="#b7">(Flickinger, 2000;</ref><ref type="bibr" target="#b8">Flickinger et al., 2012</ref>  <ref type="bibr" target="#b20">(Miyao, 2006)</ref>. Our PAS semantic dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; <ref type="bibr" target="#b28">Xue et al., 2005)</ref>. Top nodes in this representation denote semantic heads.</p><p>PSD: Prague Semantic Dependencies The Prague Czech-English Dependency Treebank (PCEDT; <ref type="bibr" target="#b12">Hajič et al., 2012)</ref> 5 is a set of parallel dependency trees over the WSJ texts from the PTB, and their Czech translations. Our PSD bi-lexical dependencies have been extracted from what is called the tectogrammatical annotation layer (t-trees). Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Format</head><p>The SDP target representations can be characterized as labeled, directed graphs. Nodes are labeled with five pieces of information: word form, lemma, part of speech, a Boolean flag indicating whether the node represents a top predicate, and optional frame (or sense) information-for example the distinction between causative vs. inchoative predicates like increase. Edges are labeled with semantic relations that hold between source and target. All data provided for the task uses a column-based file format that extends the format of the SDP 2014 task by a new frame column (thus making it a little more SRL-like). More details about the file format are available at the task website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Sets</head><p>All three target representations for English are annotations of the same text, Sections 00-21 of the WSJ Corpus, as well as of a balanced sample of twenty files from the Brown Corpus <ref type="bibr" target="#b9">(Francis &amp; Kučera, 1982)</ref>. For this task, we have synchronized these resources at the sentence and tokenization levels and excluded from the SDP 2015 training and testing data any sentences for which (a) one or more of the treebanks lacked a gold-standard analysis; (b) a oneto-one alignment of tokens could not be established across all three representations; or (c) at least one of the graphs was cyclic. Of the 43,746 sentences in these 22 first sections of WSJ text, DeepBank lacks analyses for some 11%, and the Enju Tree-bank has gaps for a little more than four percent. <ref type="bibr">6</ref> Finally, 139 of the WSJ graphs obtained through the above conversions were cyclic. In total, we were left with 35,657 sentences (or 802,717 tokens; eight percent more than for SDP 2014 7 ) as training data (Sections 00-20), 1,410 in-domain testing sentences (31,948 tokens) from WSJ Section 21, and 1,849 outof-domain testing sentences (31,583 tokens) from the Brown Corpus.</p><p>Besides the additions of out-of-domain test data and frame (or sense) identifiers for English, another extension beyond the SDP 2014 task concerns the inclusion of additional languages, albeit only for select target representations. Our training data included an additional 31,113 Chinese sentences (649,036 tokens), taken from Release 7.0 of the CTB, for the PAS target representation, and 42,076 Czech sentences (985,302 tokens), drawing on the translations of the WSJ Corpus in PCEDT 2.0, for the PSD target representation. Additional out-of-domain Czech test data was drawn from the Prague Dependency Treebank 3.0 (PDT; <ref type="bibr" target="#b0">Bejček et al., 2013)</ref>. For these additional languages, the task comprised 1,670 sentences (38,397 tokens) of in-domain Chinese test data, and 1,670 sentences (38397 tokens) and 5,226 sentences (87,927 tokens) of in-and out-of-domain Czech data, respectively.</p><p>Quantitative Comparison As a first attempt at contrasting our three target representations, Table <ref type="table" target="#tab_3">1</ref> shows some high-level statistics of the graphs comprising the training and testing data. <ref type="bibr">8</ref> In terms of distinctions drawn in dependency labels (1), there are clear differences between the representations, with PSD appearing linguistically most fine-   <ref type="formula">2</ref>) in our setup correspond to tokens analyzed as semantically vacuous, which (as seen in Figure <ref type="figure" target="#fig_0">1</ref>) include most punctuation marks in PSD and DM, but not PAS. Furthermore, PSD (unlike the other two) analyzes some high-frequency determiners as semantically vacuous.</p><formula xml:id="formula_0">EN i-d CS i-d ZH i-d EN o-o-d CS o-o-d DM PAS PSD PSD PAS DM PAS PSD PSD<label>(</label></formula><p>Conversely, PAS on average has more edges per (nonsingleton) nodes than the other two (3), which likely reflects its approach to the analysis of functional words (see below).</p><p>Judging from both the percentage of actual trees (4), the proportions of noncrossing graphs (5), projective graphs (6), and the proportions of reentrant nodes (8), PSD is more 'tree-oriented' than the other two, which at least in part reflects its approach to the analysis of modifiers and determiners (again, see below). We view the small percentages of graphs without at least one top node (9) and of graphs with at least two non-singleton components that are not interconnected ( <ref type="formula">7</ref>) as tentative indicators of general well-formedness. Intuitively, there should always be a 'top' predicate, and the whole graph should 'hang together'. Only DM exhibits non-trivial (if small) degrees of topless and fragmented graphs, which may indicate imperfections in DeepBank annotations or room for improvement in the conversion from full logical forms to bi-lexical dependencies, but possibly also exceptions to our intuitions about semantic dependency graphs.  Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table <ref type="table" target="#tab_3">1</ref> reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database <ref type="bibr" target="#b3">(Cinková, 2006)</ref>. Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015.</p><p>Finally, in Table <ref type="table" target="#tab_5">2</ref> we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F 1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand.</p><p>Linguistic Comparison Among other aspects, <ref type="bibr" target="#b14">Ivanova et al. (2012)</ref> categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure <ref type="figure" target="#fig_0">1</ref> and the discussion of Table <ref type="table" target="#tab_3">1</ref> above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks; 9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as 'local' top nodes (i.e. the semantic head of the corresponding sub-graph): For example, the infinitival marker in Figure <ref type="figure" target="#fig_0">1</ref> takes the verb as its argument, but the 'upstairs' predicate impossible links directly to the verb, rather than to the infinitival marker as an intermediate.</p><p>At the same time, DM and PAS pattern alike in their approach to modifiers, e.g. attributive adjectives, adverbs, and prepositional phrases. Unlike in PSD (or common syntactic dependency schemes), these are analyzed as semantic predicates and, thus, contribute to higher degrees of node reentrancy and non-top (structural) roots. Roughly the same holds for determiners, but here our PSD projection of Prague tectogrammatical trees onto bi-lexical dependencies leaves 'vanilla' articles (like a and the) as singleton nodes.</p><p>The analysis of coordination is distinct in the three representations, as also evident in Figure <ref type="figure" target="#fig_0">1</ref>. By design, DM opts for what is often called the Mel'čukian analysis of coordinate structures <ref type="bibr" target="#b18">(Mel'čuk, 1988)</ref>, with a chain of dependencies rooted at the first conjunct (which is thus considered the head, 'standing in' for the structure at large); in the DM approach, coordinating conjunctions are not integrated with the graph but rather contribute different types of dependencies. In PAS, the final coordinating conjunction is the head of the structure and each coordinating conjunction (or intervening punctuation mark that acts like one) is a two-place predicate, taking left and right conjuncts as its arguments. Conversely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph. <ref type="bibr">10</ref> A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure <ref type="figure" target="#fig_1">2</ref>. Much noun phraseinternal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of <ref type="bibr" target="#b27">Vadas and Curran (2007)</ref>. In the four-way nominal compounding example of Figure <ref type="figure" target="#fig_1">2</ref>, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns an underspecified, construction-specific dependency type; its compound dependency, then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a preposition, as in the above paraphrase). The DM and PSD annotations of this specific example happen to diverge in their bracketing decisions, where the DM analysis corresponds to <ref type="bibr">[...]</ref>   the concept employee stock (in contrast to 'common stock'). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see <ref type="bibr" target="#b22">Nakov, 2013</ref>, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between 'sentence' (or 'conventional') meaning, on the one hand, and 'speaker' (or 'occasion') meaning, on the other hand <ref type="bibr" target="#b25">(Quine, 1960;</ref><ref type="bibr" target="#b11">Grice, 1968;</ref><ref type="bibr" target="#b1">Bender et al., 2015)</ref>. In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Task Setup</head><p>English training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit-including graph input, basic statistics, and scoring-were released to candidate participants in early August 2014. In mid-November, cross-lingual training data, a minor update to the English data, and optional syntactic 'companion' analyses (see below) were provided. Anytime between mid-December 2014 and mid-January 2015, participants could request an input-only version of the test data, with just columns (1) to (4) pre-filled; participants then had six days to run their systems on these inputs, fill in columns (5), ( <ref type="formula">6</ref>), (7), and upwards, and submit their results (from up to two different runs) for scoring. Upon completion of the testing phase, we have shared the gold-standard test data, official scores, and system results for all submissions with participants and are currently preparing all data for general release through the Linguistic Data Consortium.</p><p>Evaluation Systems participating in the task were evaluated based on the accuracy with which they can produce semantic dependency graphs for previously unseen text, measured relative to the gold-standard testing data. For comparability with SDP 2014, the primary measures for this evaluation were labeled and unlabeled precision and recall with respect to predicted dependencies (predicate-role-argument triples) and labeled and unlabeled exact match with respect to complete graphs. In both contexts, identification of the top node(s) of a graph was considered as the identification of additional, 'virtual' dependencies from an artificial root node (at position 0). Below we abbreviate these metrics as (a) labeled precision, recall, and F 1 : LP, LR, LF; (b) unlabeled precision, recall, and F 1 : UP, UR, UF; and (c) labeled and unlabeled exact match: LM, UM.</p><p>The 'official' ranking of participating systems is determined based on the arithmetic mean of the labeled dependency F 1 scores (i.e. the geometric mean of labeled precision and labeled recall) on the three target representations (DM, PAS, and PSD). Thus, to be competitive in the overall ranking, a system had to submit semantic dependencies for all three target representations.</p><p>In addition to these metrics, we apply two additional metrics that aim to capture fragments of semantics that are 'larger' than individual dependencies but 'smaller' than the semantic dependency graph for the complete sentence, viz. what we call (a) complete predications and (b) semantic frames. A complete predication is comprised of the set of all core arguments to one predicate, which for the DM and PAS target representations corresponds to all outgoing dependency edges, and for the PSD target representation to only those outgoing dependencies marked by an '-arg' suffix on the edge label. Pushing the units of evaluation one step further towards inter-   pretation, a semantic frame is comprised of a complete predication combined with the frame (or sense) identifier of its predicate. Both complete-predicate and semantic-frame evaluation are restricted to predicates corresponding to verbal parts of speech (as determined by the gold-standard part of speech), and semantic frames are further restricted to those target representations for which frame or sense information is available in our data (English DM and PSD). As with the other metrics, we score precision, recall, and F 1 , which we abbreviate as PP, PR, and PF for complete predications, and FP, FR, and FF for semantic frames.</p><p>Closed vs. Open vs. Gold Tracks Much like in 2014, the task distinguished a closed track and an open track, where systems in the closed track could only be trained on the gold-standard semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example-provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data. <ref type="bibr">11</ref> To simplify participation in the open track, the organizers prepared ready-touse 'companion' syntactic analyses, sentence-and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies <ref type="bibr" target="#b4">(de Marneffe et al., 2006)</ref> produced by the parser of <ref type="bibr" target="#b2">Bohnet and Nivre (2012)</ref>. Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by <ref type="bibr" target="#b14">Ivanova et al. (2012)</ref>, derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Submissions and Results</head><p>From almost 40 teams who had registered for the task, twelve teams obtained the test data, and test runs were submitted for six systems-including one 'inofficial' submission by a sub-set of the task organizers <ref type="bibr" target="#b21">(Miyao et al., 2014)</ref>. Each team submitted up to two test runs per track. In total, there were seven runs submitted to the English closed track, five to the open track and two to the gold track; seven runs were submitted to the Chinese closed track, two to the open track; and five runs submitted to the Czech closed track, two to the open track. One team submitted only to the open and gold tracks, three teams submitted only to the closed track, one team submitted to open and closed tracks in English but only to the closed tracks in the other two languages. The main results are summarized and ranked in Tables <ref type="table" target="#tab_8">3  and 4</ref>. The ranking is based on the average LF score across all three target representations. Besides LF, LP and LR we also indicate the F 1 score of prediction of semantic frames (FF), or, where frame (or sense) identifiers are not available, of complete predications (PF). In cases where a team submitted two runs to a track, only the highest-ranked score is included in the table.</p><p>In the English closed track, the average LF scores across target representations range from 85.33 to 80.74. Comparing the results for different target representations, the average LF scores across systems are 89.13 for PAS, 87.09 for DM, and 74.24 for PSD.</p><p>The scores for semantic frames show a much larger variation across representations and systems. <ref type="bibr">12</ref> The Lisbon team is the only one that submitted to both the open and the closed tracks; with the additional resources allowed in the open track, they were able to improve over all closed-track submissions. Similarly, the perfect Stanford dependencies in the gold track helped the Turku team a lot in PAS and somewhat in DM and PSD; interestingly, they did not obtain the best results in the latter two representations, but their cross-representation average was still the best. The In-House system is ranked low because its submission was incomplete (no of-the-shelf parser for PSD being available); however, for DM and PAS they yielded the best open-track scores.</p><p>We see very similar trends for the out-of-domain data, though the scores are a few points lower.</p><p>Chinese PAS seems to be more difficult than English (cross-system average LF being 81.05, as opposed to English 90.07). The Czech and English in-domain data are actually parallel translations and the Czech PSD average LF is slightly higher <ref type="bibr">(77.11,</ref><ref type="bibr">as opposed to English 74.90)</ref>. The Turku open-track system shined in the Czech out-of-domain data, presumably because the additional dependency parser they used was trained on data from the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Overview of Approaches</head><p>Table <ref type="table" target="#tab_11">5</ref> shows a summary of the tracks in which each submitted system participated, and Table <ref type="table" target="#tab_12">6</ref> shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track.</p><p>The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of <ref type="bibr" target="#b5">Du et al. (2014)</ref>     <ref type="bibr" target="#b5">Du et al. (2014)</ref> and contributed to their best-performing system in the 2014 SDP task.</p><p>In addition to applying the Mate parser to the treetransformed data of <ref type="bibr" target="#b5">Du et al. (2014)</ref>, Riga developed a high-precision but low-recall semantic parser. This method applies a decision tree classifier (C6.0) to edge detection. C6.0 learns patterns of semantic dependencies, which means it outputs highly reliable prediction when a learned pattern applies, while in most cases it cannot produce any predictions. These two types of parsers are finally combined by parser ensemble. They also applied C6.0 to frame (or sense) label prediction for DM and PSD. Graph parsing and frame prediction are performed independently.</p><p>Peking proposed a novel method for graph-to-tree transformation, namely weighted tree approximation. The intuition behind this method is that the core part of graph-to-tree transformation is the extraction of an essential tree-forming subset of edges from semantic dependency graphs, but it is not trivial to determine a reasonable subset. Therefore, the idea of weighted tree approximation is to define an edge score to quantify importance of each edge, and extract tree-forming edges that maximizes the sum of edge scores globally. After defining edge scores, treeforming edges with optimal scores can be extracted by applying decoding methods like maximum spanning tree and the Eisner algorithm. They applied this method as well as the previous method proposed in <ref type="bibr" target="#b5">Du et al. (2014)</ref> with several variations on encoding edge labels, finally obtaining nine tree parsers. In the final submission, outputs from these parsers are combined by the parser ensemble technique. For predicate disambiguation, they independently applied a sequence labeling technique.</p><p>Turku took a completely different approach. They consider each predicate separately, and apply sequence labeling for each predicate individually, to recognize arguments of the target predicate. That is, the task is reduced to assign each word an argument tag (e.g. ARG1) or a negative 'pseudo-'label indicating it is not an argument of the target predicate. Outputs from sequence labeling for each predicate are combined to derive final semantic dependencies. Top node recognition and frame label prediction are performed separately. Turku is the only team who participated in the Gold track; they used gold syntactic dependencies as features for sequence labeling.</p><p>Lisbon and In-House applied their parsers from SDP 2014 without substantive changes. The Lisbon parser (TurboSemanticParser) computes globally optimal semantic dependencies using rich second order features on semantic dependencies, such as siblings and grand parents. This optimization is impractical in general, but they achieve tractable parsing time by applying dual decomposition. In-House uses deep parsers with specifically developed linguistically motivated grammars, namely the LinGO English Resource Grammar and the Enju grammar. As described in Section 2, these same grammars were used for deriving the training and test data sets of this task, i.e. these components of the In-House ensemble exclusively support the DM and PAS target representations, respectively.</p><p>Peking and Lisbon tend to attain high scores in their participated tracks in LF. Riga ranked third in LF in the closed tracks (both in-domain and out-ofdomain), while it achieved higher scores than others in FF. This might be due to high-precision rules obtained by their model, although this does not apply in the cross-lingual track. The Turku results in the gold track achieved considerably higher scores, which indicate that better syntactic parsing will help improve semantic dependency parsing. <ref type="bibr">13</ref> It is difficult to describe a tendency in the out-of-domain track; all the systems scree three to five points lower scores than the in-domain track, indicating that domain variation is still a significant challenge in semantic dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have described the motivation, design, and outcomes of the SDP 2015 task on semantic dependency parsing, i.e. retrieving bi-lexical predicate-argument relations between all content words within an English sentence. We have converted to a common format three existing annotations (DM, PAS, and PSD) over the same text and have put this to use in training and testing data-driven semantic dependency parsers. In contrast to SDP 2014 the task was extended by cross-domain testing and evaluation at the level of 'complete' predications and semantic frame (or sense) disambiguation. Furthermore, we provided comparable annotations of Czech and Chinese texts to enable cross-linguistic comparison. To start further probing of the role of syntax in the recovery of predicate-argument relations, we added a third (idealized) 'gold' track, where syntactic dependencies are provided directly from available syntactic annotations of the underlying treebanks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample semantic dependency graphs for Example (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Analysis of nominal compounding in DM, PAS, and PSD, respectively .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DELPH-IN Minimal Recursion Semantics-derived bi-lexical dependencies (DM).A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>top</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>BV</cell><cell></cell><cell>ARG2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ARG3</cell><cell cols="2">ARG1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ARG1</cell><cell>ARG1</cell><cell cols="2">ARG1</cell><cell></cell><cell></cell><cell cols="2">ARG1</cell><cell>mwe</cell><cell></cell><cell>ARG2</cell><cell>conj</cell><cell>_and_c</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>n:x</cell><cell>_ n:x _</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) top</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARG2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ARG1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ARG2</cell></row><row><cell></cell><cell cols="2">ARG1</cell><cell></cell><cell>ARG2</cell><cell>ARG2</cell><cell></cell><cell></cell><cell></cell><cell>ARG2</cell><cell>ARG1</cell><cell></cell><cell></cell><cell></cell><cell>ARG1</cell></row><row><cell></cell><cell></cell><cell>ARG1</cell><cell>ARG1</cell><cell>ARG1</cell><cell cols="2">ARG1</cell><cell>ARG1</cell><cell></cell><cell>ARG1</cell><cell>ARG1</cell><cell>ARG1</cell><cell></cell><cell>ARG1</cell><cell>ARG2</cell><cell>ARG2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(b) Enju Predicate-Argument Structures (PAS).</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADDR-arg</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PAT-arg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADDR-arg</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ACT-arg</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADDR-arg</cell><cell></cell><cell></cell><cell cols="2">CONJ.m</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PAT-arg</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ADDR-arg</cell><cell></cell><cell></cell><cell></cell><cell cols="2">APPS.m</cell></row><row><cell></cell><cell>RSTR</cell><cell></cell><cell></cell><cell>EXT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RSTR</cell><cell>APPS.m</cell><cell></cell><cell></cell><cell></cell><cell>CONJ.m</cell><cell>CONJ.m</cell></row><row><cell cols="3">A similar technique</cell><cell>is</cell><cell cols="3">almost impossible to</cell><cell>apply</cell><cell cols="7">to other crops , such as cotton , soybeans</cell><cell>and</cell><cell>rice .</cell></row><row><cell>_</cell><cell>_</cell><cell>_</cell><cell>ev-w218f2</cell><cell>_</cell><cell>_</cell><cell cols="3">_ ev-w119f2 _</cell><cell>_</cell><cell cols="2">_ _ _ _</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_</cell><cell>_ _</cell></row></table><note>top (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Contrastive high-level graph statistics across target representations, languages, and domains. grained, and PAS showing the smallest label inventory. Unattached singleton nodes (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Pairwise F 1 similarities, including punctuation (upper right diagonals) or not (lower left).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>investments in stock for employees, i.e. grouping</figDesc><table><row><cell></cell><cell>ARG1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ARG1</cell><cell>ACT</cell><cell></cell></row><row><cell>compound compound compound</cell><cell>ARG1</cell><cell>PAT</cell><cell>REG</cell></row><row><cell>employee stock investment plans</cell><cell>employee stock investment plans</cell><cell cols="2">employee stock investment plans</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>88.29 89.52 87.09 58.39 95.58 95.94 95.21 87.99 76.57 78.24 74.97 56.85 Lisbon* 86.23 89.44 90.52 88.39 00.20 91.67 92.45 90.90 84.18 77.58 79.88 75.41 00.06 Peking 85.33 89.09 90.93 87.32 63.08 91.26 92.90 89.67 79.08 75.66 78.60 72.93 49.95 Lisbon 85.15 88.21 89.84 86.64 00.15 90.88 91.87 89.92 81.74 76.36 78.62 74.23 00.03 Riga 84.00 87.90 88.57 87.24 58.12 90.75 91.50 90.02 80.03 73.34 75.25 71.52 52.54 Turku* 83.47 86.17 87.80 84.60 54.67 90.62 91.38 89.87 80.60 73.63 76.10 71.32 53.20 Minsk 80.74 84.13 86.28 82.09 54.24 85.24 87.28 83.28 64.66 72.84 74.65 71.13 51.63 In-House* 61.61 92.80 92.85 92.75 83.79 92.03 92.07 91.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DM</cell><cell></cell><cell></cell><cell></cell><cell>PAS</cell><cell></cell><cell></cell><cell></cell><cell>PSD</cell><cell></cell></row><row><cell></cell><cell>LF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>FF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>PF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>FF</cell></row><row><cell>Turku"</cell><cell cols="9">86.81 99 87.24</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DM</cell><cell></cell><cell></cell><cell></cell><cell>PAS</cell><cell></cell><cell></cell><cell></cell><cell>PSD</cell><cell></cell></row><row><cell></cell><cell>LF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>FF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>PF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>FF</cell></row><row><cell>Turku"</cell><cell cols="13">83.50 82.11 84.26 80.07 42.89 92.92 93.52 92.33 83.80 75.47 77.77 73.31 42.37</cell></row><row><cell>Lisbon*</cell><cell cols="13">82.53 83.77 85.79 81.84 00.35 87.63 88.88 86.41 80.19 76.18 80.12 72.61 02.25</cell></row><row><cell>Lisbon</cell><cell cols="13">81.15 81.75 84.81 78.90 00.27 86.88 88.52 85.30 78.47 74.82 78.68 71.31 02.09</cell></row><row><cell>Peking</cell><cell cols="13">80.78 81.84 84.29 79.53 47.49 87.23 89.47 85.10 74.75 73.28 77.36 69.61 34.28</cell></row><row><cell>Riga</cell><cell cols="13">79.23 80.69 81.69 79.72 41.88 86.63 87.56 85.72 76.26 70.37 73.23 67.71 40.76</cell></row><row><cell>Turku*</cell><cell cols="13">78.85 79.01 81.54 76.63 39.15 85.95 86.95 84.98 76.38 71.59 74.92 68.55 38.75</cell></row><row><cell>Minsk</cell><cell cols="13">75.79 77.24 80.24 74.46 42.18 80.44 83.07 77.96 62.00 69.68 72.26 67.27 41.25</cell></row><row><cell cols="10">In-House* 59.24 89.69 89.80 89.58 76.39 88.03 88.10 87.96 81.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Results of the gold track (marked "), open track (marked *) and closed track (unmarked) submissions for the English in-domain (top) and out-of-domain (bottom) data. For each system, the second column (LF) indicates the averaged LF score across all representations, used to rank the systems. The best closed track scores are highlighted in italices.</figDesc><table><row><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>PF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>PF</cell><cell>LF</cell><cell>LP</cell><cell>LR</cell><cell>PF</cell></row><row><cell cols="4">Peking 83.43 84.75 82.15 66.09</cell><cell cols="4">Lisbon 79.33 83.52 75.54 55.91</cell><cell cols="4">Peking 64.37 69.41 60.02 48.82</cell></row><row><cell cols="4">Riga 82.47 83.12 81.84 66.05</cell><cell cols="4">Peking 78.45 83.61 73.89 55.36</cell><cell cols="4">Turku* 63.70 65.11 62.35 51.04</cell></row><row><cell cols="4">Lisbon 82.02 83.81 80.31 66.05</cell><cell cols="4">Riga 75.34 78.77 72.19 50.90</cell><cell cols="4">Lisbon 63.50 67.94 59.61 43.10</cell></row><row><cell cols="4">Turku* 79.64 80.81 78.51 62.04</cell><cell cols="4">Turku* 75.30 77.53 73.20 54.26</cell><cell cols="4">Riga 61.32 64.50 58.44 44.34</cell></row><row><cell cols="4">Minsk 77.68 79.27 76.15 58.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results of the open (Turku) and closed (other teams) tracks for the Chinese in-domain (left) and Czech in-(center) and out-of-domain (right) data. The systems are ranked according to their LF scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Summary of tracks in which submitted systems participated</figDesc><table><row><cell>Team</cell><cell>Approach</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Overview of approaches and additional resources used (if any).graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structures. In run-time, the tree parser is applied, and predicted trees are converted back into graph structures. Labels of tree edges encode additional information to recover original graph structures. This idea was applied in</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://alt.qcri.org/semeval2015/ task18/ for further technical details, information on how to obtain the data, and official results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In much previous SRL work, target representations typically draw on resources like PropBank and NomBank<ref type="bibr" target="#b24">(Palmer et al., 2005;</ref><ref type="bibr" target="#b19">Meyers et al., 2004)</ref>, which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena-for example negation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">However, non-scopal adverbs act as mere intersective modifiers, e.g. in a structure like Abrams sang loudly, the adverb is a predicate in DM, but the main verb nevertheless is the top node.4 See http://kmcs.nii.ac.jp/enju/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See http://ufal.mff.cuni.cz/pcedt2.0/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Additionally, some 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB idiosyncrasies like G.m.b, H. , S.p, A. , and U.S., . , and introducing a few new ones<ref type="bibr" target="#b6">(Fares et al., 2013)</ref>.7  In comparison to the SDP 2014 data, our DM graphs were extracted from a newer, improved release of DeepBank (Version 1.1), and its conversion to bi-lexical dependencies was moderately revised to provide more systematic analyses of contracted negated auxiliaries and comparatives. At the same time, the extraction of PSD graphs from the PCEDT t-trees was refined to include edges representing grammatical coreference, e.g. reentrancies introduced by control verbs.8 These statistics are obtained using the 'official' SDP toolkit. Our notions of singletons, roots, re-entrancies, and projectivity follow common graph terminology, but see<ref type="bibr" target="#b23">Oepen et al. (2014)</ref> for formal definitions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">In all formats, punctuation marks like dashes, colons, and sometimes commas can be contentful, i.e. at times occur as both predicates, arguments, and top nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">As detailed by<ref type="bibr" target="#b21">Miyao et al. (2014)</ref>, individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a 'shared' modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PSD.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">This restriction implies that typical off-the-shelf syntactic parsers have to be re-trained, as many data-driven parsers for English include WSJ Section 21 in their default training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Please see the task web page at the address indicated above for full labeled and unlabeled scores.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The SDP 2014 and 2015 task setups, however, somewhat artificially constrain the possible contributions of syntactic analysis, as all training and testing data (even in the closed track) includes high-quality parts of speech and lemmata.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Angelina Ivanova for help in DM data preparation and contrastive analysis, to Željko Agić and Bernd Bohnet for consultation and assistance in preparing our companion parses, to the Linguistic Data Consortium (LDC) for support in distributing the SDP data to participants, as well as to Emily M. Bender and two anonymous reviewers for feedback on an earlier version of this manuscript. We warmly thank the general SemEval 2015 chairs, Preslav Nakov and Torsten Zesch, for always being role-model organizers, equipped with an outstanding balance of structure, flexibility, and community spirit. Data preparation was supported through the ABEL high-performance computing facilities at the University of Oslo, and we acknowledge the Scientific Computing staff at UiO, the Norwegian Metacenter for Computational Science, and the Norwegian taxpayers. Part of the work was supported by the grants 15-10472S, GP13-03351P and 15-20031S of the Czech Science Foundation, and by the infrastructural funding by the Ministry of Education, Youth and Sports of the Czech Republic (LM2010013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Bejček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajičová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jínová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolářová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Zikánová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Š</forename></persName>
		</author>
		<idno>11858/ 00-097C-0000-0023-1AAF-3</idno>
		<ptr target="http://hdl.handle.net/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Prague dependency treebank 3.0. Re</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Layers of interpretation. On grammar and compositionality</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics</title>
				<meeting>the 11th International Conference on Computational Semantics<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled nonprojective dependency parsing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapting the PropBank lexicon to the valency theory of the Functional Generative Description</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cinková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
				<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>From PropBank to EngValLex</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">, M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
				<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Peking: Profiling syntactic tree parsing techniques for semantic graph parsing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
				<meeting>the 8th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>semeval 2014</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning for high-quality tokenization. Replicating variable tokenization schemes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics and intelligent text processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="231" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On building a more efficient grammar by exploiting types</title>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep-Bank. A dynamically annotated treebank of the Wall Street Journal</title>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories</title>
				<meeting>the 11th International Workshop on Treebanks and Linguistic Theories<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Edições Colibri</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Frequency analysis of English usage. Lexicon and grammar</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kučera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<pubPlace>New York, USA; Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Utterer&apos;s meaning, sentence-meaning, and word-meaning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="242" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Announcing Prague Czech-English Dependency Treebank 2.0</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajičová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Panevová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Žabokrtský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
				<meeting>the 8th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Turkey</forename><surname>Istanbul</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Who did what to whom? A contrastive study of syntacto-semantic dependencies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Øvrelid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Linguistic Annotation Workshop</title>
				<meeting>the Sixth Linguistic Annotation Workshop<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic parsing. The task, the state of the art and the future</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpora of English. The Penn Treebank</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Priberam: A turbo semantic parser with second order features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S C</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 8th international workshop on semantic evaluation</title>
				<meeting>the 8th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>semeval 2014</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dependency syntax. Theory and practice</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mel'čuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>SUNY Press</publisher>
			<pubPlace>Albany, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotating noun argument structure for NomBank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Language Resources and Evaluation</title>
				<meeting>the 4th International Conference on Language Resources and Evaluation<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="803" to="806" />
		</imprint>
	</monogr>
	<note>Portugal</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">From linguistic theory to syntactic analysis. Corpus-oriented grammar development and feature forest model. Unpublished doctoral dissertation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Tokyo, Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Tokyo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In-House. An ensemble of pre-existing off-the-shelf parsers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the interpretation of noun compounds: Syntax, semantics, and entailment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="330" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Se-mEval 2014 Task 8. Broad-coverage semantic dependency parsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Proposition Bank. A corpus annotated with semantic roles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Word and object</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V O</forename><surname>Quine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online graph planarisation for synchronous parsing of semantic and syntactic dependencies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Joint Conference on Artificial Intelligence</title>
				<meeting>the 21st International Joint Conference on Artificial Intelligence<address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adding Noun Phrase Structure to the Penn Treebank</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Meeting of the Association for Computational Linguistics</title>
				<meeting>the 45th Meeting of the Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank. Phrase structure annotation of a large corpus</title>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
