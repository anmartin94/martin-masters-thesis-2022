<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
							<email>gordon@ict.usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
							<email>kozareva@isi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
							<email>msroemme@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Creative Technologies Information Sciences Institute Department of Linguistics University of Southern</orgName>
								<orgName type="institution">California University of Southern California Indiana University</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CA Marina del Rey</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>CA, IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we describe the development of this task and its motivation. We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. <ref type="bibr" target="#b8">Lenat, 1995)</ref>, where logical formalizations of commonsense theories were hand-authored by expert logicians and evaluated using a handful of commonsense challenge problems <ref type="bibr" target="#b11">(Morgenstern, 2012)</ref>. Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions.</p><p>Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. <ref type="bibr" target="#b15">Speer et al. (2008)</ref> describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language commonsense facts in a crowd-sourced knowledge base <ref type="bibr" target="#b9">(Liu &amp; Singh, 2004)</ref>. <ref type="bibr" target="#b5">Gordon et al., (2010)</ref> describe a method for extracting millions of commonsense facts from parse trees of English sentences. <ref type="bibr" target="#b7">Jung et al. (2010)</ref> describe a novel approach to the extraction of commonsense knowledge about activities by mining online howto articles. We believe that these new NLP-based approaches hold enormous potential for overcoming the knowledge acquisition bottleneck that has limited progress in commonsense reasoning in previous decades.</p><p>Given the growth and enthusiasm for these new approaches, there is increasing need for a common metric for evaluation. A common evaluation suite would allow researchers to gauge the performance of new versions of their own systems, and to compare their approaches with those of other research groups. Evaluations for these new NLP-based approaches should themselves be based in natural language, and must be suitably large to truly evaluate the breadth of different reasoning approaches. Still, each evaluation should be focused on one dimension of the overall commonsense reasoning task, so as not to create a new challenge that no single research group could hope to succeed.</p><p>In SemEval-2012 Task 7, we presented a new evaluation for open-domain commonsense reason-ing, focusing specifically on commonsense causal reasoning about everyday events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Choice of Plausible Alternatives</head><p>Consider the following English sentence, describing a hypothetical state of the world:</p><p>The man lost his balance on the ladder.</p><p>In addition to parsing this sentence, resolving ambiguities, and constructing a semantic interpretation, human readers also imagine the causal antecedents and consequents that would follow if the statement were true. With such a brief description, readers are left with many questions. How high up on the ladder was this man? What was he doing on the ladder in the first place? How much experience does he have using ladders? Was he intoxicated? The answers to these questions help readers formulate hypotheses for the two central concerns when reasoning about events: What was the cause of this? and What happened as a result?</p><p>As computational linguists, we imagine that our automated natural language processing algorithms will also, eventually, need to engage in similar reasoning processes in order to achieve human-like performance on text understanding tasks. Progress toward the goal of deep semantic interpretation of text has been slow. However, the last decade of natural language processing research has shown that enormous gains can be achieved when there is a clear evaluation metric. A shared task with an automated scoring mechanism allows researchers to compare different approaches, tune system parameters to maximize performance, and assess progress toward broader research objectives. Developing an evaluation metric for causal reasoning poses a number of challenges. It is necessary to formulate a question with answers that can be automatically graded, but can still serve as a proxy for the complex, generative imagination of readers. <ref type="bibr" target="#b13">Roemmele et al. (2011)</ref> offered a solution in the form of a simple binary-choice question. Presented with an English sentence describing a premise, systems must select between two alternatives (also sentences) the one that more plausibly has a causal relation to the premise, as in the following example:</p><p>Premise: The man lost his balance on the ladder. What happened as a result? Alternative 1: He fell off the ladder.</p><p>Alternative 2: He climbed up the ladder.</p><p>Both of these alternatives are conceivable, and neither is entailed by the premise. However, human readers have no difficulty selecting the alternative that is the more plausible of the two. This question asks about a causal consequent, and a complimentary formulation asks for the causal antecedent, as in the following example: Premise: The man fell unconscious. What was the cause of this? Alternative 1: The assailant struck the man on the head. Alternative 2: The assailant took the man's wallet.</p><p>Roemmele et al. describe their efforts to author a collection of 1000 questions of these two types to create a new causal reasoning evaluation tool: the Choice of Plausible Alternatives (COPA). When presented to humans to select the correct alternative, the inter-rater agreement was extremely high (Cohen's kappa = 0.965). Where disagreements between two raters were found (in 26 of 1000 items), questions were removed and replaced with new ones with perfect agreement.</p><p>To develop an automated evaluation tool, the 1000 questions were randomly ordered and sorted into two equally sized sets of 500 questions to serve as development and test sets. The order of the correct alternative was also randomized, such that the expected accuracy of a random baseline would be 50%. Gold-standard answers for each split are used to automatically evaluate a given system's performance.</p><p>The distribution of the COPA evaluation includes an automated test of statistical significance of differences seen between two competing systems. This software tool implements a computeintensive randomized test of statistical significance using stratified shuffling, as described by <ref type="bibr" target="#b12">Noreen (1989)</ref>. By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance.</p><p>The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge <ref type="bibr">(Degan et al., 2006)</ref>, but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns the defeasible inferences that can be drawn from the interpretation of a sentence. In this respect, COPA overlaps in its aims with the task of recognizing causal relations in text through automated discourse processing (e.g. <ref type="bibr" target="#b10">Marcu, 1999)</ref>. Some progress in automated discourse processing has been made using supervised machine learning methods, where system learn the lexical-syntactic patterns that are most correlated with causal relations from a large annotated corpus <ref type="bibr" target="#b14">(Sagae, 2009)</ref>. Lacking a dedicated training corpus, the COPA evaluation encourages competitors to capture commonsense causal knowledge from any available corpus or existing knowledge repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SemEval-2012 Systems and Results</head><p>The COPA evaluation was accepted as Task 7 of the 6th International Workshop on Semantic <ref type="bibr">Evaluation (SemEval-2012)</ref>. In several respects, the COPA evaluation was different than the typical shared task offered as part of this series of workshops. First, the task materials were available and distributed long before the evaluation period began, and there were published results of previous systems using this evaluation. 1 Second, the task included no training data, only sets of development and test questions (500 each). Participants were encouraged to use any available text corpus or knowledge repositories in the construction of their systems. Success on the task would not be possible simply through the selection of machine learning algorithms and feature encodings. Instead, some creativity and ingenuity was needed to find a suitable source of commonsense causal information, and determine an automated mechanism for applying this information to COPA questions.</p><p>Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems <ref type="bibr" target="#b2">(Goodwin et al., 2012)</ref>, which they described to us as follows:</p><p>UTDHLT Bigram PMI: The team's first approach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic 1 http://www.ict.usc.edu/~gordon/copa.html <ref type="bibr" target="#b0">(Church &amp; Hanks, 1990</ref>) over all pairs of bigrams (at the token level) between the candidate alternative and the premise. PMI statistics were collected using 8.4 million documents from the LDC Gigaword corpus <ref type="bibr" target="#b6">(Graff &amp; Cieri, 2003)</ref>. A window of 100 terms was used for finding pairs of cooccurring bigrams, and a window/slop size of 2 for the bigram itself.</p><p>UTDHLT SVM Combined: The team's second approach augments the first by combining it with several other features and casting the task as a classification problem. To this end, they consider the PMI between events participating in a temporal link on a Time-ML annotated Gigaword corpus. That is, events that occur together frequently will have a higher PMI. They also consider the difference between the number of positive and negative polarity words between an alternative and premise using information from the Harvard Inquisitor. In addition, they used the count of matching causeeffect pairs extracted using patterns on dependency structures from the Gigaword corpus. Combining all of these sources of information, they trained a support vector machine (SVM) learning algorithm to classify the alternative that is most causally related to the premise.</p><p>These systems were assessed based on their accuracy on the 500 questions in the test split of the COPA evaluation, presented in Table <ref type="table">1</ref>. Both systems significantly outperformed the random baseline (50% accuracy), but the gains seen in the second approach were not significantly different than those of the first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% Table <ref type="table">1</ref>. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison to Previous Results</head><p>In order to better evaluate the success of these two systems, we compared these results with the published results of other systems that have used the COPA evaluation. Three other systems were considered.</p><p>PMI Gutenberg (W=5): Described in Roemmele et al. <ref type="bibr">(2011)</ref>, this approach calculated the PMI between words (unigrams) in the premise and each alternative, and selected the alternative with the stronger correlation. The PMI statistic was calculated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words.</p><p>PMI Story 1M (W=25): Described in <ref type="bibr" target="#b3">Gordon et al. (2011)</ref>, this approach was identical to that of <ref type="bibr" target="#b13">Roemmele et al. (2011)</ref> except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs <ref type="bibr" target="#b4">(Gordon &amp; Swanson, 2009)</ref>, with 1.9 GB of text. Using this corpus instead of Project Gutenberg, the best results were obtained by using a window of 25 words for the PMI statistic.</p><p>PMI Story 10M (W=25): Also described in <ref type="bibr" target="#b3">Gordon et al. (2011)</ref>, this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog stories. The story extraction technology used by <ref type="bibr" target="#b4">Gordon and Swanson (2009)</ref> was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 million personal stories (37GB of text). Again, the best results were obtained by using a window of 25 words for the PMI statistic.</p><p>Table <ref type="table">2</ref> compares the results of these three previous systems with the two SemEval-2012 systems. Although the last two of these three previous systems achieved higher scores than both of the SemEval-2012 submissions, the differences are not statistically significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The two systems from the University of Texas at Dallas make an important contribution to progress on open-domain commonsense reasoning. Some lessons are evident from the short descriptions of their systems that they provided to us.</p><p>As in each of the previously successful systems, this team focused their efforts on calculating correlational statistics between words in COPA questions using very large text corpora. In this case, the Gigaword corpus is used, and the calculation is based on bigrams rather than unigrams. We believe that the content of the news articles that comprise the Gigaword corpus is a step further away from the concerns of COPA questions than both the Project Gutenberg corpus and the weblog story corpora used in previous efforts. Indeed, the gains achieved by <ref type="bibr" target="#b3">Gordon et al. (2011)</ref> appear to be entirely due to the relationship between COPA questions and the personal stories that people write about in their public weblogs. However, the use of a large news corpus affords the use of more sophisticated analysis techniques that have been developed for this genre. Here, the Gigaword corpus is annotated using Time-ML relationships, which in turn are used to modify the PMI strength between words.</p><p>The use of bigrams is an additional enhancement explored by this team, as is the casting of COPA questions as a classification task using a diverse set of lexical and discourse features. Such an approach can facilitate the combining of diverse systems in the future, where correlational statistics are gathered from a diverse set of text corpora, each suited for specific domains of COPA questions or yielding complimentary feature sets.</p><p>Still, the modest COPA performance seen from all existing systems is somewhat discouraging. With the best systems performing in the 60-65% range, we remain much closer to random performance (50%) than human performance (99%). These results cast some doubt that the information necessary to answer COPA questions can be readily obtained from large text corpora. Certainly the use of simple correlational statistics between nearby words is not enough. In the best case, we might wish for perfect identification of causal relationships between events in an extremely large text corpus of narratives similar in content to COPA questions. Semantic similarity between these events and COPA sentences could be computed to gather evidence to select the best alternative. Even if it were possible to achieve this ideal, it is difficult to imagine that such an approach could mirror human performance on this task.</p><p>To move closer to human performance, systems may need to stretch beyond corpus statistics into the realm of automated reasoning. Just as human readers do when hearing that "the man lost his balance on the ladder," successful systems may need to treat COPA premises as novel world states, and imagine a broad range of interconnected causal antecedents and consequents. Useful knowledge bases will be those that have adequate coverage over commonsense concerns, but also adequate competency to support generative inference of the sort more commonly seen in deductive and abductive automated reasoning frameworks. This knowledge may or may not be represented as text, but any successful system must have the capacity to apply this knowledge to the understanding of COPA's textual premises and alternatives. We consider the successful application of commonsense inference to text understanding to be one of the grand challenges of natural language processing, and hope that the COPA evaluation continues to be a useful tool for benchmarking progress toward this goal.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The projects or efforts depicted were or are sponsored by the U. S. Army. The content or information presented does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information, and Lexicography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
			<persName><forename type="first">F</forename><surname>; D'alché-Buc</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UTDHLT: COPACETIC System for Choosing Plausible Alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation</title>
				<meeting>the 6th International Workshop on Semantic Evaluation<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commonsense Causal Reasoning Using Millions of Personal Stories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth Conference on Artificial Intelligence (AAAI-11)</title>
				<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying Personal Stories in Millions of Weblog Entries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Weblogs and Social Media</title>
				<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from the Web: Extracting General World Knowledge from Noisy Text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence</title>
				<meeting>the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">English Gigaword. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Construction of a Large-Scale Situation Ontology by Mining How-to Instructions from the Web</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Myaeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="110" to="124" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CYC: A Large-Scale Investment in Knowledge Infrastructure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ConceptNet: A Practical Commonsense Reasoning Toolkit. BT Technology Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A decision-based approach to rhetorical parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99)</title>
				<meeting><address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Common Sense Problem Page</title>
		<author>
			<persName><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
		<ptr target="http://www-formal.stanford.edu/leora/commonsense/" />
		<imprint>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computer-Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011-03-21" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies (IWPT)</title>
				<meeting>the 11th International Conference on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analo-gySpace: Reducing the Dimensionality of Common Sense Knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
