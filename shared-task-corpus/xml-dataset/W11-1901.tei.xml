<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
							<email>pradhan@bbn.com</email>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
							<email>lramshaw@bbn.com</email>
							<affiliation key="aff1">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
							<email>martha.palmer@colorado.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<postCode>80309</postCode>
									<settlement>Boulder</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
							<email>weischedel@bbn.com</email>
							<affiliation key="aff4">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
							<email>xuen@cs.brandeis.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<postCode>02453</postCode>
									<settlement>Waltham</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CoNLL-2011 shared task involved predicting coreference using OntoNotes data. Resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The importance of coreference resolution for the entity/event detection task, namely identifying all mentions of entities and events in text and clustering them into equivalence classes, has been well recognized in the natural language processing community. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by <ref type="bibr">McCarthy and Lenhert (1995)</ref> where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by <ref type="bibr" target="#b41">Soon et al. (2001)</ref>. Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward <ref type="bibr" target="#b23">(Morton, 2000;</ref><ref type="bibr" target="#b15">Harabagiu et al., 2001;</ref><ref type="bibr" target="#b21">McCallum and Wellner, 2004;</ref><ref type="bibr" target="#b8">Culotta et al., 2007;</ref><ref type="bibr" target="#b9">Denis and Baldridge, 2007;</ref><ref type="bibr" target="#b37">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b13">Haghighi and Klein, 2010)</ref>. Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited <ref type="bibr" target="#b32">(Ponzetto and Strube, 2005;</ref><ref type="bibr" target="#b33">Ponzetto and Strube, 2006;</ref><ref type="bibr" target="#b45">Versley, 2007;</ref><ref type="bibr" target="#b24">Ng, 2007)</ref>. Researchers continued finding novel ways of exploiting ontologies such as Word-Net. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia <ref type="bibr" target="#b33">(Ponzetto and Strube, 2006)</ref>. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs' distance, etc. A better idea of the progress in the field can be obtained by reading recent survey articles <ref type="bibr" target="#b25">(Ng, 2010)</ref> and tutorials <ref type="bibr" target="#b31">(Ponzetto and Poesio, 2009)</ref> dedicated to this subject.</p><p>Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC). These corpora were tagged with coreferring entities identified by noun phrases in the text. The de facto standard datasets for current coreference studies are the MUC <ref type="bibr">(Hirschman and Chin-chor, 1997;</ref><ref type="bibr" target="#b7">Chinchor, 2001;</ref><ref type="bibr" target="#b6">Chinchor and Sundheim, 2003)</ref> and the ACE 1 (G. <ref type="bibr" target="#b12">Doddington et al., 2000)</ref> corpora. The MUC corpora cover all noun phrases in text, but represent small training and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) <ref type="bibr">(Hirschman et al., 1998)</ref>. This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past <ref type="bibr" target="#b30">(Poesio, 2004;</ref><ref type="bibr" target="#b29">Poesio and Artstein, 2005;</ref><ref type="bibr" target="#b28">Passonneau, 2004)</ref>. There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation -both of which seek to take information access technology to the next level -we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge -possibly linked to knowledge sources -and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving the current state of the art. The computational learning community, in general, is also witnessing a move towards evaluations based on joint inference, with the two previous CoNLL tasks <ref type="bibr" target="#b44">(Surdeanu et al., 2008;</ref><ref type="bibr" target="#b14">Hajič et al., 2009)</ref> devoted to joint learning of syntactic and semantic dependencies. A principle ingredient for joint learning is the presence of multiple layers of semantic information.</p><p>One fundamental question still remains, and that is -what would it take to improve the state of the art in coreference resolution that has not been attempted so far? Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information. One of the many goals of the OntoNotes project 2 <ref type="bibr" target="#b17">(Hovy et al., 2006;</ref><ref type="bibr" target="#b48">Weischedel et al., 2011)</ref> is to explore whether it can fill this void and help push the progress further -not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that cov-ers entities and events not limited to noun phrases or a limited set of entity types. A small portion of this corpus from the newswire and broadcast news genres (∼120k) was recently used for a SEMEVAL task <ref type="bibr" target="#b40">(Recasens et al., 2010)</ref>. As mentioned earlier, the coreference layer in OntoNotes constitutes just one part of a multi-layered, integrated annotation of shallow semantic structure in text with high interannotator agreement, which also provides a unique opportunity for performing joint inference over a substantial body of data.</p><p>The remainder of this paper is organized as follows. Section 2 presents an overview of the OntoNotes corpus. Section 3 describes the coreference annotation in OntoNotes. Section 4 then describes the shared task, including the data provided and the evaluation criteria. Sections 5 and 6 then describe the participating system results and analyze the approaches, and Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The OntoNotes Corpus</head><p>The OntoNotes project has created a corpus of largescale, accurate, and integrated annotation of multiple levels of the shallow semantic structure in text. The idea is that this rich, integrated annotation covering many layers will allow for richer, cross-layer models enabling significantly better automatic semantic analysis. In addition to coreference, this data is also tagged with syntactic trees, high coverage verb and some noun propositions, partial verb and noun word senses, and 18 named entity types. However, such multi-layer annotations, with complex, cross-layer dependencies, demands a robust, efficient, scalable mechanism for storing them while providing efficient, convenient, integrated access to the the underlying structure. To this effect, it uses a relational database representation that captures both the inter-and intra-layer dependencies and also provides an object-oriented API for efficient, multitiered access to this data <ref type="bibr" target="#b35">(Pradhan et al., 2007a)</ref>. This should facilitate the creation of cross-layer features in integrated predictive models that will make use of these annotations.</p><p>Although OntoNotes is a multi-lingual resource with all layers of annotation covering three languages: English, Chinese and Arabic, for the scope of this paper, we will just look at the English portion. Over the years of the development of this corpus, there were various priorities that came into play, and therefore not all the data in the English portion is annotated with all the different layers of annotation. There is a core portion, however, which is roughly 1.3M words which has been annotated with all the layers. It comprises ∼450k words from newswire, ∼150k from magazine articles, ∼200k from broadcast news, ∼200k from broadcast conversations and ∼200k web data.</p><p>OntoNotes comprises the following layers of annotation:</p><p>• Syntax -A syntactic layer representing a revised Penn Treebank <ref type="bibr" target="#b20">(Marcus et al., 1993;</ref><ref type="bibr" target="#b0">Babko-Malaya et al., 2006</ref>).</p><p>• Propositions -The proposition structure of verbs in the form of a revised PropBank <ref type="bibr" target="#b26">(Palmer et al., 2005;</ref><ref type="bibr" target="#b0">Babko-Malaya et al., 2006)</ref>.</p><p>• Word Sense -Coarse grained word senses are tagged for the most frequent polysemous verbs and nouns, in order to maximize coverage. The word sense granularity is tailored to achieve 90% inter-annotator agreement as demonstrated by <ref type="bibr" target="#b27">Palmer et al. (2007)</ref>. These senses are defined in the sense inventory files and each individual sense has been connected to multiple WordNet senses. This provides a direct access to the WordNet semantic structure for users to make use of. There is also a mapping from the word senses to the PropBank frames and to VerbNet <ref type="bibr" target="#b18">(Kipper et al., 2000)</ref> and FrameNet <ref type="bibr" target="#b11">(Fillmore et al., 2003)</ref>.</p><p>• Named Entities -The corpus was tagged with a set of 18 proper named entity types that were well-defined and well-tested for interannotator agreement by <ref type="bibr">Weischedel and Burnstein (2005)</ref>.</p><p>• Coreference -This layer captures general anaphoric coreference that covers entities and events not limited to noun phrases or a limited set of entity types <ref type="bibr" target="#b36">(Pradhan et al., 2007b)</ref>. We will take a look at this in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coreference in OntoNotes</head><p>General anaphoric coreference that spans a rich set of entities and events -not restricted to a few types, as has been characteristic of most coreference data available until now -has been tagged with a high degree of consistency. Attributive coreference is tagged separately from the more common identity coreference. Two different types of coreference are distinguished in the OntoNotes data: Identical (IDENT), and Appositive (APPOS). Appositives are treated separately because they function as attributions, as described further below. The IDENT type is used for anaphoric coreference, meaning links between pronominal, nominal, and named mentions of specific referents. It does not include mentions of generic, underspecified, or abstract entities.</p><p>Coreference is annotated for all specific entities and events. There is no limit on the semantic types of NP entities that can be considered for coreference, and in particular, coreference is not limited to ACE types.</p><p>The mentions over which IDENT coreference applies are typically pronominal, named, or definite nominal. The annotation process begins by automatically extracting all of the NP mentions from the Penn Treebank, though the annotators can also add additional mentions when appropriate. In the following two examples (and later ones), the phrases notated in bold form the links of an IDENT chain.</p><p>(1) She had a good suggestion and it was unanimously accepted by all.</p><p>(2) Elco Industries Inc. said it expects net income in the year ending June 30, 1990, to fall below a recent analyst's estimate of $ 1.65 a share. The Rockford, Ill. maker of fasteners also said it expects to post sales in the current fiscal year that are "slightly above" fiscal 1989 sales of $ 155 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Verbs</head><p>Verbs are added as single-word spans if they can be coreferenced with a noun phrase or with another verb. The intent is to annotate the VP, but we mark the single-word head for convenience. This includes morphologically related nominalizations (3) and noun phrases that refer to the same event, even if they are lexically distinct from the verb (4). In the following two examples, only the chains related to the growth event are shown.</p><p>(3) Sales of passenger cars grew 22%. The strong growth followed year-to-year increases.</p><p>(4) Japan's domestic sales of cars, trucks and buses in October rose 18% from a year earlier to 500,004 units, a record for the month, the Japan Automobile Dealers' Association said. The strong growth followed year-to-year increases of 21% in August and 12% in September.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pronouns</head><p>All pronouns and demonstratives are linked to anything that they refer to, and pronouns in quoted speech are also marked. Expletive or pleonastic pronouns (it, there) are not considered for tagging, and generic you is not marked. In the following example, the pronoun you and it would not be marked. (In this and following examples, an asterisk (*) before a boldface phrase identifies entity/event mentions that would not be tagged as coreferent.)</p><p>(5) Senate majority leader Bill Frist likes to tell a story from his days as a pioneering heart surgeon back in Tennessee. A lot of times, Frist recalls, *you'd have a critical patient lying there waiting for a new heart, and *you'd want to cut, but *you couldn't start unless *you knew that the replacement heart would make *it to the operating room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generic mentions</head><p>Generic nominal mentions can be linked with referring pronouns and other definite mentions, but are not linked to other generic nominal mentions. This would allow linking of the bracketed mentions in ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>), but not (8).</p><p>(6) Officials said they are tired of making the same statements.</p><p>(7) Meetings are most productive when they are held in the morning. Those meetings, however, generally have the worst attendance.</p><p>(8) Allergan Inc. said it received approval to sell the PhacoFlex intraocular lens, the first foldable silicone lens available for *cataract surgery. The lens' foldability enables it to be inserted in smaller incisions than are now possible for *cataract surgery.</p><p>Bare plurals, as in ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>), are always considered generic. In example (9) below, there are two generic instances of parents. These are marked as distinct IDENT chains (with separate chains distinguished by subscripts X, Y and Z), each containing a generic and the related referring pronouns.</p><p>(9) Parents X should be involved with their X children's education at home, not in school. They X should see to it that their X kids don't play truant; they X should make certain that the children spend enough time doing homework; they X should scrutinize the report card. Parents Y are too likely to blame schools for the educational limitations of their Y children. If parents Z are dissatisfied with a school, they Z should have the option of switching to another.</p><p>In (10) below, the verb "halve" cannot be linked to "a reduction of 50%", since "a reduction" is indefinite.</p><p>(10) Argentina said it will ask creditor banks to *halve its foreign debt of $64 billion -the third-highest in the developing world . Argentina aspires to reach *a reduction of 50% in the value of its external debt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-modifiers</head><p>Proper pre-modifiers can be coreferenced, but proper nouns that are in a morphologically adjectival form are treated as adjectives, and not coreferenced. For example, adjectival forms of GPEs such as Chinese in "the Chinese leader", would not be linked. Thus we could coreference United States in "the United States policy" with another referent, but not American "the American policy. The firm balked at the price.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Copular verbs</head><p>Attributes signaled by copular structures are not marked; these are attributes of the referent they modify, and their relationship to that referent will be captured through word sense and propositional argument tagging.</p><p>(15) John X is a linguist. People Y are nervous around John X , because he X always corrects their Y grammar. Copular (or 'linking') verbs are those verbs that function as a copula and are followed by a subject complement. Some common copular verbs are: be, appear, feel, look, seem, remain, stay, become, end up, get. Subject complements following such verbs are considered attributes, and not linked. Since Called is copular, neither IDENT nor APPOS coreference is marked in the following case.</p><p>(16) Called Otto's Original Oat Bran Beer, the brew costs about $12.75 a case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Small clauses</head><p>Like copulas, small clause constructions are not marked. The following example is treated as if the copula were present ("John considers Fred to be an idiot"):</p><p>(17) John considers *Fred *an idiot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Temporal expressions</head><p>Temporal expressions such as the following are linked:</p><p>(18) John spent three years in jail. In that time... Deictic expressions such as now, then, today, tomorrow, yesterday, etc. can be linked, as well as other temporal expressions that are relative to the time of the writing of the article, and which may therefore require knowledge of the time of the writing to resolve the coreference. Annotators were allowed to use knowledge from outside the text in resolving these cases. In the following example, the end of this period and that time can be coreferenced, as can this period and from three years to seven years. (19) The limit could range from three years to seven years X , depending on the composition of the management team and the nature of its strategic plan. At (the end of (this period) X ) Y , the poison pill would be eliminated automatically, unless a new poison pill were approved by the then-current shareholders, who would have an opportunity to evaluate the corporation's strategy and management team at that time Y . In multi-date temporal expressions, embedded dates are not separately connected to to other mentions of that date. For example in Nov. 2, 1999, Nov. would not be linked to another instance of November later in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Appositives</head><p>Because they logically represent attributions, appositives are tagged separately from Identity coreference. They consist of a head, or referent (a noun phrase that points to a specific object/concept in the world), and one or more attributes of that referent. An appositive construction contains a noun phrase that modifies an immediately-adjacent noun phrase (separated only by a comma, colon, dash, or parenthesis). It often serves to rename or further define the first mention. Marking appositive constructions allows us to capture the attributed property even though there is no explicit copula. In cases where the two members of the appositive are equivalent in specificity, the left-most member of the appositive is marked as the head/referent. Definite NPs include NPs with a definite marker (the) as well as NPs with a possessive adjective (his). Thus the first element is the head in all of the following cases:</p><p>(24) The chairman, the man who never gives up     When the entity to which an appositive refers is also mentioned elsewhere, only the single span containing the entire appositive construction is included in the larger IDENT chain. None of the nested NP spans are linked. In the example below, the entire span can be linked to later mentions to Richard Godown. The sub-spans are not included separately in the IDENT chain.</p><p>(28) Richard Godown, president of the Industrial Biotechnology Association</p><p>Ages are tagged as attributes (as if they were ellipses of, for example, a 42-year-old):</p><p>(29) Mr.Smith head , 42 attribute ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Special Issues</head><p>In addition to the ones above, there are some special cases such as:</p><p>• No coreference is marked between an organization and its members.  • GPEs are linked to references to their governments, even when the references are nested NPs, or the modifier and head of a single NP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Annotator Agreement and Analysis</head><p>Table <ref type="table" target="#tab_5">1</ref> shows the inter-annotator and annotatoradjudicator agreement on all the genres of OntoNotes. We also analyzed about 15K disagreements in various parts of the data, and grouped them into one of the categories shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the distribution of these different types that were found in that sample. It can be seen that genuine ambiguity and annotator error are the biggest contributors -the latter of which is usually captured during adjudication, thus showing the increased agreement between the adjudicated version and the individual annotator version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CoNLL-2011 Coreference Task</head><p>This section describes the CoNLL-2011 Coreference task, including its closed and open track versions, and characterizes the data used for the task and how it was prepared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Why a Coreference Task?</head><p>Despite close to a two-decade history of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention. There are many different parameters involved in defining a coreference task. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem <ref type="bibr" target="#b41">(Soon et al., 2001)</ref> or one that is somewhat easier <ref type="bibr" target="#b8">(Culotta et al., 2007)</ref>. Given the space constraints, we refer the reader to <ref type="bibr" target="#b42">Stoyanov et al. (2009)</ref> for a detailed treatment of the issue.</p><p>Limitations in the size and scope of the available datasets have also constrained research progress. The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated. The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results.</p><p>The availability of the OntoNotes data offered an opportunity to define a coreference task based on a larger, more broad-coverage corpus. We have tried to design the task so that it not only can support the current evaluation, but also can provide an ongoing resource for comparing different coreference algorithms and approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Description</head><p>The CoNLL-2011 shared task was based on the English portion of the OntoNotes 4.0 data. The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains. The target coreference decisions could be made using automatically predicted information on the other structural layers including the parses, semantic roles, word senses, and named entities.</p><p>As is customary for CoNLL tasks, there were two tracks, closed and open. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Closed Track</head><p>In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by <ref type="bibr" target="#b2">Bergsma and Lin (2006)</ref>.</p><p>For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2. For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers. Participants were allowed to use either the gold-standard or predicted annotation for training their systems. They were also free to use the gold-standard data to train their own models for the various layers of annotation, if they judged that those would either provide more accurate predictions or alternative predictions for use as multiple views, or wished to use a lattice of predictions.</p><p>More so than previous CoNLL tasks, coreference predictions depend on world knowledge, and many state-of-the-art systems use information from external resources such as WordNet, which can add a layer that helps the system to recognize semantic connections between the various lexicalized mentions in the text. Therefore, the use of WordNet was allowed, even for the closed track. Since word senses in OntoNotes are predominantly 3 coarse-grained groupings of WordNet senses, systems could also map from the predicted or goldstandard word senses provided to the sets of underlying WordNet senses. Another significant piece of knowledge that is particularly useful for coreference but that is not available in the layers of OntoNotes is that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by <ref type="bibr" target="#b2">Bergsma and Lin (2006)</ref>, for use during both training and testing.</p><p>Following the recent CoNLL tradition, participants were allowed to use both the training and the development data for training the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Open Track</head><p>In addition to resources available in the closed track, the open track, systems were allowed to use external resources such as Wikipedia, gazetteers etc. This track is mainly to get an idea of a performance ceiling on the task at the cost of not getting a comparison across all systems. Another advantage of the open track is that it might reduce the barriers to participation by allowing participants to field existing research systems that already depend on external resources -especially if there were hard dependencies on these resources. They can participate in the task with minimal or no modification to their existing system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Coreference Task Data</head><p>Since there are no previously reported numbers on the full version of OntoNotes, we had to create a train/development/test partition. The only portion of OntoNotes that has a previously determined, widely used, standard split is the WSJ portion of the newswire data. For that subcorpus, we maintained the same partition. For all the other portions we created stratified training, development and test partitions over all the sources in OntoNotes using the procedure shown in Algorithm 1. The list of training, development and test document IDs can be found on the task webpage. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Preparation</head><p>This section gives details of the different annotation layers including the automatic models that were used to predict them, and describes the formats in which the data were provided to the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Manual Annotation Gold Layers</head><p>We will take a look at the manually annotated, or gold layers of information that were made available for the training data. Coreference The manual coreference annotation is stored as chains of linked mentions connecting multiple mentions of the same entity. Coreference is the only document-level phenomenon in OntoNotes, and the complexity of annotation increases nonlinearly with the length of a document. Unfortunately, some of the documents -especially ones in the broadcast conversation, weblogs, and telephone conversation genre -are very long which prohibited us from efficiently annotating them in entirety. These had to be split into smaller parts. We conducted a few passes to join some adjacent parts, but since some documents had as many as 17 parts, there are still multi-part documents in the corpus. Since the coreference chains are coherent only within each of these document parts, for this task, each such part is treated as a separate document. Another thing to note is that there were some cases of sub-token annotation in the corpus owing to the fact that tokens were not split at hyphens. Cases such as pro-WalMart had the sub-span WalMart linked with another instance of the same. The recent Treebank revision which split tokens at most hyphens, made a majority of these sub-token annotations go away. There were still some residual sub-token annotations. Since subtoken annotations cannot be represented in the CoNLL format, and they were a very small quantity -much less than even half a percent -we decided to ignore them.</p><p>For various reasons, not all the documents in OntoNotes have been annotated with all the differ-    ent layers of annotation, with full coverage. <ref type="bibr">6</ref> There is a core portion, however, which is roughly 1.3M words which has been annotated with all the layers. This is the portion that we used for the shared task.</p><p>The number of documents in the corpus for this task, for each of the different genres, are shown in Table <ref type="table" target="#tab_3">2</ref>. Tables <ref type="table" target="#tab_9">3 and 4</ref> shows the distribution of mentions by the syntactic categories, and the counts of entities, links and mentions in the corpus respectively. All of this data has been Treebanked and PropBanked either as part of the OntoNotes effort or some preceding effort.</p><p>For comparison purposes, Table <ref type="table" target="#tab_3">2</ref> also lists the number of documents in the MUC-6, MUC-7, and ACE <ref type="bibr">(2000)</ref><ref type="bibr">(2001)</ref><ref type="bibr">(2002)</ref><ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref> corpora. The MUC-6 data was taken from the Wall Street Journal, whereas the MUC-7 data was from the New York Times. The ACE data spanned many different genres similar to the ones in OntoNotes.</p><p>Parse Trees This represents the syntactic layer that is a revised version of the Penn Treebank. For purposes of this task, traces were removed from the syntactic trees, since the CoNLL-style data format, being indexed by tokens, does not provide any good means of conveying that information. Function tags were also removed, since the parsers that we used for the predicted syntax layer did not provide them. One thing that needs to be dealt with in conversational data is the presence of disfluencies (restarts, etc.). In the original OntoNotes parses, these are marked using a special EDITED 7 phrase tag -as was the case for the Switchboard Treebank. Given the frequency of disfluencies and the performance with which one can identify them automatically, 8 a probable processing pipeline would filter them out before parsing. Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Propositions</head><p>The propositions in OntoNotes constitute PropBank semantic roles. Most of the verb predicates in the corpus have been annotated with their arguments. Recent enhancements to the Prop-Bank to make it synchronize better with the Treebank <ref type="bibr" target="#b0">(Babko-Malaya et al., 2006)</ref> have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK-PCR) and selectional preferences (LINK-SLC). More details can be found in the addendum to the PropBank guidelines 9 in the OntoNotes 4.0 re-lease. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank. This functionality is part of the OntoNotes DB Tool. <ref type="bibr">10</ref> Word Sense Gold word sense annotation was supplied using sense numbers as specified in the OntoNotes list of senses for each lemma. <ref type="bibr">11</ref> The sense inventories that were provided in the OntoNotes 4.0 release were not all mapped to the latest version 3.0 of WordNet, so we provided a revised version of the sense inventories, containing mapping to WordNet 3.0, on the task page for the participants.</p><p>Named Entities Named Entities in OntoNotes data are specified using a catalog of 18 Name types.</p><p>Other Layers Discourse plays a vital role in coreference resolution. In the case of broadcast conversation, or telephone conversation data, it partially manifests in the form of speakers of a given utterance, whereas in weblogs or newsgroups it does so as the writer, or commenter of a particular article or thread. This information provides an important clue for correctly linking anaphoric pronouns with the right antecedents. This information could be automatically deduced, but since it would add additional complexity to the already complex task, we decided to provide oracle information of this metadata both during training and testing. In other words, speaker and author identification was not treated as an annotation layer that needed to be predicted. This information was provided in the form of another column in the .conll table. There were some cases of interruptions and interjections that ideally would associate parts of a sentence to two different speakers, but since the frequency of this was quite small, we decided to make an assumption of one speaker/writer per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Predicted Annotation Layers</head><p>The predicted annotation layers were derived using automatic models trained using cross-validation on other portions of OntoNotes data. As mentioned earlier, there are some portions of the OntoNotes corpus that have not been annotated for coreference but that have been annotated for other layers. For training 10 http://cemantix.org/ontonotes.html <ref type="bibr">11</ref> It should be noted that word sense annotation in OntoNotes is note complete, so only some of the verbs and nouns have word sense tags specified.  Parse Trees Predicted parse trees were produced using the Charniak parser <ref type="bibr" target="#b5">(Charniak and Johnson, 2005)</ref>. 12 Some additional tag types used in the OntoNotes trees were added to the parser's tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended.</p><p>The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation. Table <ref type="table" target="#tab_14">5</ref> shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set. We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has not seen that tag in training, we could not make use of it.</p><p>Word Sense We trained a word sense tagger using a SVM classifier and contextual word and part of speech features on all the training portion of the OntoNotes data. The OntoNotes 4.0 corpus comprises a total of 14,662 sense definitions across 4877 verb and noun lemmas <ref type="bibr">13</ref> . The distribution of senses per lemma is as shown in Table <ref type="table" target="#tab_12">6</ref>. Table <ref type="table" target="#tab_16">7</ref> shows the performance of this classifier over both the verbs and nouns in the CoNLL-2011 test set. Again this performance is not directly comparable to any reported in the literature before, and it seems lower then performances reported on previous versions of OntoNotes because this is over all the genres of OntoNotes, and aggregated over both verbs and nouns in the CoNLL-2011 test set.</p><p>Propositions To predict propositional structure, ASSERT 14 <ref type="bibr" target="#b34">(Pradhan et al., 2005)</ref>     could try to verify using just the WSJ portion of the data, but it would be hard as it is not only a subset of the documents that the performance has been reported on previously, but also the annotation has been significantly revised; it includes propositions for be verbs missing from the original PropBank, and the training data is a subset of the original data as well. Table <ref type="table" target="#tab_15">8</ref> shows the detailed performance numbers.</p><p>In addition to automatically predicting the arguments, we also trained a classifier to tag PropBank frameset IDs in the data using the same word sense module as mentioned earlier. OntoNotes 4.0 contains a total of 7337 framesets across 5433 verb lemmas. <ref type="bibr">15</ref> An overwhelming number of them are monosemous, but the more frequent verbs tend to be polysemous. Table <ref type="table" target="#tab_18">9</ref> gives the distribution of number of framesets per lemma in the PropBank layer of the OntoNotes 4.0 data.</p><p>During automatic processing of the data, we tagged all the tokens that were tagged with a part of speech VBx. This means that there would be cases where the wrong token would be tagged with propositions. The CoNLL-2005 scorer was used to generate the scores.</p><p>Named Entities BBN's IdentiFinder TM system was used to predict the named entities. Given the    Other Layers As noted above, systems were allowed to make use of gender and number predictions for NPs using the table from Bergsma and Lin <ref type="bibr" target="#b2">(Bergsma and Lin, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Data Format</head><p>In order to organize the multiple, rich layers of annotation, the OntoNotes project has created a database representation for the raw annotation layers along with a Python API to manipulate them <ref type="bibr" target="#b35">(Pradhan et al., 2007a)</ref>. In the OntoNotes distribution the data is organized as one file per layer, per document. The API requires a certain hierarchical structure with documents at the leaves inside a hierarchy of language, genre, source and section. It comes with various ways of cleanly querying and manipulating the data and allows convenient access to the sense inventory and propbank frame files instead of having to interpret the raw .xml versions. However, maintaining format consistency with earlier CoNLL tasks was deemed convenient for sites that already had tools configured to deal with that format. Therefore, in order to distribute the data so that one could make the best of both worlds, we created a new file type called .conll which logically served as another layer in addition to the .parse, .prop, .name and .coref layers. Each .conll file contained a merged representation of all the OntoNotes layers in the CoNLLstyle tabular format with one line per token, and with multiple columns for each token specifying the input annotation layers relevant to that token, with the final column specifying the target coreference layer. Because OntoNotes is not authorized to distribute the underlying text, and many of the layers contain inline annotation, we had to provide a skeletal form (.skel of the .conll file which was essentially the .conll file, but with the word column replaced with a dummy string. We provided an assembly script that participants could use to create a .conll file taking as input the .skel file and the top-level directory of the OntoNotes distribution that they had separately downloaded from the LDC <ref type="bibr">16</ref> Once the .conll file is created, it can be used to create the individual layers such as .parse, .name, .coref etc. using another set of scripts. Since the propositions and word sense layers are inherently standoff annotation, they were provided as is, and did not require that extra merging step. One thing thing that made this data creation process a bit tricky was the fact that we had dissected some of the trees for the conversation data to remove the EDITED phrases. Table <ref type="table" target="#tab_5">11</ref> describes the data provided in each of the column of the .conll format. Figure <ref type="figure" target="#fig_4">3</ref> shows a sample from a .conll file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation</head><p>This section describes the evaluation criteria used. Unlike for propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there are several established metrics, evaluating the accuracy of coreference continues to be contentious. Various al-   ternative metrics have been proposed, as mentioned below, which weight different features of a proposed coreference pattern differently. The choice is not clear in part because the value of a particular set of coreference predictions is integrally tied to the consuming application.</p><p>A further issue in defining a coreference metric concerns the granularity of the mentions, and how closely the predicted mentions are required to match those in the gold standard for a coreference prediction to be counted as correct.</p><p>Our evaluation criterion was in part driven by the OntoNotes data structures. OntoNotes coreference distinguishes between identity coreference and appositive coreference, treating the latter separately because it is already captured explicitly by other layers of the OntoNotes annotation. Thus we evaluated systems only on the identity coreference task, which links all categories of entities and events together into equivalent classes.</p><p>The situation with mentions for OntoNotes is also different than it was for MUC or ACE. OntoNotes data does not explicitly identify the minimum extents of an entity mention, but it does include handtagged syntactic parses. Thus for the official evaluation, we decided to use the exact spans of mentions for determining correctness. The NP boundaries for the test data were pre-extracted from the handtagged Treebank for annotation, and events triggered by verb phrases were tagged using the verbs themselves. This choice means that scores for the CoNLL-2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input, <ref type="bibr">17</ref> or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Metrics</head><p>As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns. Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric <ref type="bibr" target="#b46">(Vilain et al., 1995)</ref>, ii) The mention based B-CUBED metric <ref type="bibr" target="#b1">(Bagga and Baldwin, 1998)</ref> and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric <ref type="bibr" target="#b19">(Luo, 2005)</ref>. Very recently BLANC (BiLateral Assessment of Noun-Phrase Coreference) measure <ref type="bibr">(Recasens and Hovy,</ref><ref type="bibr">17</ref> as is the case in this evaluation with Gold Mentions 2011) has been proposed as well. Each of the metric tries to address the shortcomings or biases of the earlier metrics. Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure. The MUC measure if the oldest and most widely used. It focuses on the links (or, pairs of mentions) in the data. <ref type="bibr">18</ref> The number of common links between entities in K and R divided by the number of links in K represents the recall, whereas, precision is the number of common links between entities in K and R divided by the number of links in R. This metric prefers systems that have more mentions per entity -a system that creates a single entity of all the mentions will get a 100% recall without significant degradation in its precision. And, it ignores recall for singleton entities, or entities with only one mention. The B-CUBED metric tries to addresses MUCS's shortcomings, by focusing on the mentions and computes recall and precision scores for each mention. If K is the key entity containing mention M, and R is the response entity containing mention M, then recall for the mention M is computed as |K∩R| |K| and precision for the same is is computed as |K∩R| |R| . Overall recall and precision are the average of the individual mention scores. CEAF aligns every response entity with at most one key entity by finding the best one-to-one mapping between the entities using an entity similarity metric. This is a maximum bipartite matching problem and can be solved by the Kuhn-Munkres algorithm. This is thus a entity based measure. Depending on the similarity, there are two variationsentity based CEAF -CEAF e and a mention based CEAF -CEAF e . Recall is the total similarity divided by the number of mentions in K, and precision is the total similarity divided by the number of mentions in R. Finally, BLANC uses a variation on the Rand index <ref type="bibr" target="#b38">(Rand, 1971</ref>) suitable for evaluating coreference. There are a few other measures -one being the ACE value, but since this is specific to a restricted set of entities (ACE types), we did not consider it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Official Evaluation Metric</head><p>In order to determine the best performing system in the shared task, we needed to associate a single number with each system. This could have been one of the metrics above, or some combination of more than one of them. The choice was not simple, and while we consulted various researchers in the field, hoping for a strong consensus, their conclusion seemed to be that each metric had its pros and cons. We settled on the MELA metric by <ref type="bibr" target="#b10">Denis and Baldridge (2009)</ref>, which takes a weighted average of three metrics: MUC, B-CUBED, and CEAF. The rationale for the combination is that each of the three metrics represents a different important dimension, the MUC measure being based on links, the B-CUBED based on mentions, and the CEAF based on entities. For a given task, a weighted average of the three might be optimal, but since we don't have an end task in mind, we decided to use the unweighted mean of the three metrics as the score on which the winning system was judged. We decided to use CEAF e instead of CEAF m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Scoring Metrics Implementation</head><p>We used the same core scorer implementation 19 that was used for the SEMEVAL-2010 task, and which implemented all the different metrics. There were a couple of modifications done to this scorer after it was used for the SEMEVAL-2010 task.</p><p>1. Only exact matches were considered correct. Previously, for SEMEVAL-2010 non-exact matches were judged partially correct with a 0.5 score if the heads were the same and the mention extent did not exceed the gold mention.</p><p>2. The modifications suggested by <ref type="bibr" target="#b3">Cai and Strube (2010)</ref> were incorporated in the scorer.</p><p>Since there are differences in the version used for CoNLL and the one available on the download site, and it is possible that the latter would be revised in the future, we have archived the version of the scorer on the CoNLL-2011 task webpage. <ref type="bibr">20</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Systems and Results</head><p>About 65 different groups demonstrated interest in the shared task by registering on the task webpage. Of these, 23 groups submitted system outputs on the test set during the evaluation week. 18 groups submitted only closed track results, 3 groups only open track results, and 2 groups submitted both closed and open track results. 2 participants in the closed track, did not write system papers, so we don't use their results in the discussion. Their results will be reported on the task webpage.</p><p>The official results for the 18 systems that submitted closed track outputs are shown in Table <ref type="table" target="#tab_3">12</ref>, with those for the 5 systems that submitted open track results in Table <ref type="table" target="#tab_5">13</ref>. The official ranking score, the arithmetic mean of the F-scores of MUC, B-CUBED and CEAF e , is shown in the rightmost column. For convenience, systems will be referred to here using the first portion of the full name, which is unique within each table.</p><p>For completeness, the tables include the raw precision and recall scores from which the F-scores were derived. The tables also include two additional scores (BLANC and CEAF m ) that did not factor into the official ranking score. Useful further analysis may be possible based on these results beyond the preliminary results presented here.</p><p>As discussed previously in the task description, we will consider three different test input conditions: i) Predicted only (Official), ii) Predicted plus gold mention boundaries, and iii) Predicted plus gold mentions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predicted only (Official)</head><p>For the official test, beyond the raw source text, coreference systems were provided only with the predictions from automatic engines as to the other annotation layers (parses, semantic roles, word senses, and named entities).</p><p>In this evaluation it is important to note that the mention detection score cannot be considered in isolation of the coreference task as has usually been the case. This is mainly owing to the fact that there are no singleton entities in the OntoNotes data. Most systems removed singletons from the response as a post-processing step, so not only will they not get credit for the singleton entities that they correctly removed from the data, but they will be penalized for the ones that they accidentally linked with another mention. What this number does indicate is the ceiling on recall that a system would have got in absence of being penalized for making mistakes in coreference resolution. A close look at the Table <ref type="table" target="#tab_3">12</ref> indicates a possible outlier in case of the sapena system. The recall for this system is very high, and precision way lower than any other system. Further investigations uncovered that the reason for this aberrant behavior was that fact that this system opted to keep singletons in the response. By design, the scorer removes singletons that might be still present in the system, but it does so after the mention detection accuracy is computed.</p><p>The official scores top out in the high 50's. While this is lower than the figures cited in previous coref-           track using all predicted information erence evaluations, that is as expected, given that the task here includes predicting the underlying mentions and mention boundaries, the insistence on exact match, and given that the relatively easier appositive coreference cases are not included in this measure. The top-performing system (lee) had a score of 57.79 which is about 1.8 points higher than that of the second (sapena) and third (chang) ranking systems, which scored 55.99 and 55.96 respectively. Another 1.5 points separates them from the fourth best score of 54.53 (nugues). Thus the performance differences between the better-scoring systems were not large, with only about three points separating the top four systems.</p><p>This becomes even clearer if we merge in the results of systems that participated only in the open track but that made relatively limited use of outside resources. <ref type="bibr">21</ref> Comparing that way, the cai system scores in the same ball park as the second rank systems (sapena and chang). The uryupina system similarly scores very close to nugues's 54.53</p><p>Given that our choice of the official metric was somewhat arbitrary, if is also useful to look at the individual metrics, including the mention-based CEAF m and BLANC metrics that were not part of the official metric. The lee system which scored the best using the official metric does slightly worse than song on the MUC metric, and also does slightly worse than chang on the B-CUBED and BLANC metrics. However, it does much better than every other group on the entity-based CEAF e , and this is the primary reason for its 1.8 point advantage in the official score. If the CEAF e measure does indicate the accuracy of entities in the response, this suggests that the lee system is doing better on getting coherent entities than any other system. This could be partly due to the fact that that system is primarily a precision-based system that would tend to create purer entities. The CEAF e measure also seems to penalize other systems more harshly than do the other measures.</p><p>We cannot compare these results to the ones obtained in the SEMEVAL-2010 coreference task using a small portion of OntoNotes data because it was only using nominal entities, and had heuristically added singleton mentions to the OntoNotes data 22 <ref type="bibr">21</ref> The cai system specifically mentions that, and the only resource that the uryupina system used outside of the closed track setting was the Stanford named entity tagger. <ref type="bibr">22</ref> The documentation that comes with the SEMEVAL data package from LDC (LDC2011T01) states: "Only nominal mentions and identical (IDENT) types were taken from the OntoNotes coreference annotation, thus excluding coreference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicted plus gold mention boundaries</head><p>We also explored performance when the systems were provided with the gold mention boundaries, that is, with the exact spans (expressed in terms of token offsets) for all of the NP constituents in the human-annotated parse trees for the test data. Systems could use this additional data to ensure that the output mention spans in their entity chains would not clash with those in the answer set. Since this was a secondary evaluation, it was an optional element, and not all participants ran their systems on this task variation. The results for those systems that did participate in this optional task are shown in Tables 14 (closed track) and 15 (open track).</p><p>Most of the better scoring systems did supply these results. While all systems did slightly better here in terms of raw scores, the performance was not much different from the official task, indicating that mention boundary errors resulting from problems in parsing do not contribute significantly to the final output. <ref type="bibr">23</ref> One side benefit of performing this supplemental evaluation was that it revealed a subtle bug in the automatic scoring routine that we were using that could double-count duplicate correct mentions in a given entity chain. These can occur, for example, if the system considers a unit-production NP-PRP combination as two mentions that identify the exact same token in the text, and reports them as separate mentions. Most systems had a filter in their processing that selected only one of these duplicate mentions, but the kobdani system considered both as potential mentions, and its developers tuned their algorithm using that flawed version of the scorer.</p><p>When we fixed the scorer and re-evaluated all of the systems, the kobdani system was the only one whose score was affected significantly, dropping by about 8 points, which lowered that system's rank from second to ninth. It is not clear how much of this was owing to the fact that the system's paramrelations with verbs and appositives. Since OntoNotes is only annotated with multi-mention entities, singleton referential elements were identified heuristically: all NPs and possessive determiners were annotated as singletons excluding those functioning as appositives or as pre-modifiers but for NPs in the possessive case. In coordinated NPs, single constituents as well as the entire NPs were considered to be mentions. There is no reliable heuristic to automatically detect English expletive pronouns, thus they were (although inaccurately) also annotated as singletons."</p><p>23 It would be interesting to measure the overlap between the entity clusters for these two cases, to see whether there was any substantial difference in the mention chains, besides the expected differences in boundaries for individual mentions. eters had been tuned using the scorer with the bug, which double-credited duplicate mentions. To find out for sure, one would have to re-tune the system using the modified scorer.</p><p>One difficulty with this supplementary evaluation using gold mention boundaries is that those boundaries alone provide only very partial information. For the roughly 10% of mentions that the automatic parser did not correctly identify, while the systems knew the correct boundaries, they had no hierarchical parser or semantic role label information, and they also had to further approximate the already heuristic head word identification. This incomplete data complicated the systems' task and also complicates interpretation of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Predicted plus gold mentions</head><p>The final supplementary condition that we explored was if the systems were supplied with the manuallyannotated spans for exactly those mentions that did participate in the gold standard coreference chains. This supplies significantly more information than the previous case, where exact spans were supplied for all NPs, since the gold mentions list here will also include verb headwords that are linked to event NPs, but will not include singleton mentions, which do not end up as part of any chain. The latter constraint makes this test seem somewhat artificial, since it directly reveals part of what the systems are designed to determine, but it still has some value in quantifying the impact that mention detection has on the overall task and what the results are if the mention detection is perfect.</p><p>Since this was a logical extension of the task and since the data was available to the participants for the development set, a few of the sites did run experiments of this type. Therefore we decided to provide the gold mentions data to a few sites who had reported these scores, so that we could compute the performance on the test set. The results of these experiments are shown in Tables <ref type="table" target="#tab_5">16 and 17</ref>. The results show that performance does go up significantly, indicating that it is markedly easier for the systems to generate better entities given gold mentions. Although, ideally, one would expect a perfect mention detection score, it is the case that one of the two systemslee -did not get a 100% Recall. This could possibly be owing to unlinked singletons that were removed in post-processing.</p><p>The lee system developers also ran a further experiment where both gold mentions for the elements of the coreference chains and also gold annotations for all the other layers were available to the system. Surprisingly, the improvement in coreference performance from having gold annotation of the other layers was almost negligible. This suggests that either: i) the automatic models are predicting those layers well enough that switching to gold doesn't make much difference; ii) information from the other layers does not provide much leverage for coreference resolution; or iii) current coreference models are not capable of utilizing the information from these other layers effectively. Given the performance numbers on the individual layers cited earlier, (i) seems unlikely, and we hope that further research in how best to leverage these layers will result in models that can benefit from them more definitively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Head word based scoring</head><p>In order to check how stringent the official, exact match scoring is, we also performed a relaxed scoring. Unlike ACE and MUC, the OntoNotes data does not have manually annotated minimum spans that a mention must contain to be considered correct. However, OntoNotes does have manual syntactic analysis in the form of the Treebank. Therefore, we decided to approximate the minimum spans by using the head words of the mentions using the gold standard syntax tree. If the response mention contained the head word and did not exceed the true mention boundary, then it was considered correct -both from the point of view of mention detection, and coreference resolution. The scores using this relaxed strategy for the open and closed track submissions using predicted data are shown in Tables <ref type="table" target="#tab_5">18 and 19</ref>. It can be observed that the relaxed, head word based, scoring does not improve performance very much. The only exception was the klenner system whose performance increased from 51.77 to 55.28. Overall, the ranking remained quite stable, though it did change for some adjacent systems which had very close exact match scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Genre variation</head><p>In order to check how the systems did on various genres, we scored their performance per genre as well. Tables <ref type="table" target="#tab_1">20 and 21</ref> summarize genre based performance for the closed and open track participants respectively. System performance does not seem to vary as much across the different genres as is normally the case with language processing tasks, which could suggest that coreference is relatively genre insensitive, or it is possible that scores are two low for the difference to be apparent. Comparisons are difficult, however, because the spoken gen-  res were treated here with perfect speech recognition accuracy and perfect speaker turn information. Under more realistic application conditions, the spread in performance between genres might be greater.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Approaches</head><p>Tables <ref type="table" target="#tab_3">22 and 23</ref> summarize the approaches of the participating systems along with some of the important dimensions. Most of the systems broke the problem into two phases, first identifying the potential mentions in the text and then linking the mentions to form coreference chains. Most participants also used rule-based approaches for mention detection, though two did use trained models. While trained morels seem able to better balance precision and recall, and thus to achieve a higher F-score on the mention task itself, their recall tends to be quite a bit lower than that achievable by rule-based systems designed to favor recall. This impacts coreference scores because the full coreference system has no way to recover if the mention detection stage misses a potentially anaphoric mention.</p><p>Only one of the participating systems cai attempted to do joint mention detection and coreference resolution. While it did not happen to be among the top-performing systems, the difference in performance could be due to the richer features used by other systems rather than to the use of a joint model.</p><p>Most systems represented the markable mentions internally in terms of the parse tree NP constituent span, but some systems used shared attribute models, where the attributes of the merged entity are determined collectively by heuristically merging the attribute types and values of the different constituent mentions.</p><p>Various types of trained models were used for predicting coreference. It is interesting to note that some of the systems, including the best-performing one, used a completely rule-based approach even for this component.</p><p>Most participants appear not to have focused much on eventive coreference, those coreference chains that build off verbs in the data. This usually meant that mentions that should have linked to the eventive verb were instead linked in with some other entity. Participants may have chosen not to focus on events because they pose unique challenges while making up only a small portion of the data. Roughly 91% of mentions in the data are NPs and pronouns.</p><p>In the systems that used trained models, many systems used the approach described in <ref type="bibr" target="#b41">Soon et al. (2001)</ref> for selecting the positive and negative training examples, while others used some of the alternative approaches that have been introduced in the research literature more recently. Many of the trained systems also were able to improve their performance by using feature selection, though things varied some depending on the example selection strategy and the classifier used. Almost half of the trained systems used the feature selection strategy from <ref type="bibr" target="#b41">Soon et al. (2001)</ref> and found it beneficial. It is not clear whether the other systems did not explore this path, or whether it just did not prove as useful in their case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we described the anaphoric coreference information and other layers of annotation in the  nugues Closest Antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Closest-first clustering for pronouns and Best-first clustering for non-pronouns 1-best uryupina Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> mention pair model without ranking as in <ref type="bibr" target="#b41">Soon 2001</ref> santos Extended version of <ref type="bibr" target="#b41">Soon (2001)</ref>  sobha Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Pronominal: all preceding NPs in the sentence and preceding 4 sentences klenner --Incremental entity creation kobdani Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Best-first clustering. Threshold of 100 words used for long documents 1-best zhou Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> charton From the end of the document, until an antecedent is found, or 10 mentions Negative examples in between anaphor and closest antecedent MLP with score of 0.5 used for linking and 10 mentions yang Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Maximum 23 sentences to the left; Constrained clustering hao Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Beam search <ref type="bibr">(Luo, 2004)</ref> Packed forest</p><p>xinxin Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Best-first clustering followed by ILP optimization zhang Closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Negative examples in between anaphor and closest antecedent <ref type="bibr" target="#b41">(Soon, 2001)</ref> Window of 100 markables  OntoNotes corpus, and presented the results from an evaluation on learning such unrestricted entities and events in text. The following represent our conclusions on reviewing the results:</p><p>• Perhaps the most surprising finding was that the best-performing system (lee) was completely rule-based, rather than trained. This suggests that their rule-based approach was able to do a more effective job of combining the multiple sources of evidence than the trained systems.</p><p>The features for coreference prediction are certainly more complex than for many other language processing tasks, which makes it more challenging to generate effective feature combinations. The rule-based approach used by the best-performing system seemed to benefit from a heuristic that captured the most confident links before considering less confident ones, and also made use of the information in the guidelines in a slightly more refined manner than other systems. They also included appositives and copular constructions in their calculations. Although OntoNotes does not count those as instances of IDENT coreference, using that information may have helped their system discover additional useful links.</p><p>• It is interesting to note that the developers of the lee system also did the experiment of running their system using gold standard information on the individual layers, rather than automatic model predictions. The somewhat surprising result was that using perfect information for the other layers did not end up improving coreference performance much, if at all. It is not clear whether this means that: i) Automatic predictors for the individual layers are accurate enough already; ii) Information captured by those supplementary layers actually does not provide much leverage for resolving coreference; or iii) researchers have yet have found an effective way of capturing and utilizing the extra information provided by these layers.</p><p>• It does seem that collecting information about an entity by merging information across the various attributes of the mentions that comprise it can be useful, though not all systems that attempted this achieved a benefit.</p><p>• System performance did not seem to vary as much across the different genres as is normally the case with language processing tasks, which could suggest that coreference is relatively genre insensitive, or it is possible that scores are two low for the difference to be apparent. Comparisons are difficult, however, because the spoken genres were treated here with perfect speech recognition accuracy and perfect speaker turn information. Under more realistic application conditions, the spread in performance between genres might be greater.</p><p>• It is noteworthy that systems did not seem to attempt the kind of joint inference that could make use of the full potential of various layers available in OntoNotes, but this could well have been owing to the limited time available for the shared task.</p><p>• We had expected to see more attention paid to event coreference, which is a novel feature in this data, but again, given the time constraints and given that events represent only a small portion of the total, it is not surprising that most systems chose not to focus on it.</p><p>• Scoring coreference seems to remain a significant challenge. There does not seem to be an objective way to establish one metric in preference to another in the absence of a specific application. On the other hand, the system rankings do not seem terribly sensitive to the particular metric chosen. It is interesting that both versions of the CEAF metric -which tries to capture the goodness of the entities in the output -seem much lower than the other metric, though it is not clear whether that means that our systems are doing a poor job of creating coherent entities or whether that metric is just especially harsh.</p><p>Finally, it is interesting to note that the problem of coreference does not seem to be following the same kind of learning curve that we are used to with other problems of this sort. While performance has improved somewhat, it is not clear how far we will be able to go given the strategies at hand, or whether new techniques will be needed to capture additional information from the texts or from world knowledge. We hope that this corpus and task will provide a useful resource for continued experimentation to help resolve this issue.</p><p>(DARPA/IPTO) under the GALE program, DARPA/CMO Contract No. HR0011-06-C-0022. We would like to thank all the participants. Without their hard work, patience and perseverance this evaluation would not have been a success. We would also like to thank the Linguistic Data Consortium for making the OntoNotes 4.0 corpus freely and timely available to the participants. Emili Sapena, who graciously allowed the use of his scorer implementation, and made available enhancements and immediately fixed issues that were uncovered during the evaluation. Finally, we offer our special thanks to Lluís Màrquez and Joakim Nivre for their wonderful support and guidance without which this task would not have been successful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Description of various disagreement types</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The distribution of disagreements across the various types in Table 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The distribution of disagreements across the various types in Table1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.8 67.1 52.9 86.9 21.2 32.1 Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6 Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8 Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0 Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2 Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7 Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample portion of the .conll file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>46 84.75 82.55 72.84 74.57 73.70 69.71 69.71 69.71 70.45 60.75 65.24 78.01 76.57 77.26 73.83</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Participating system profiles -Part I. In the Task column, C/O represents whether the system participated in the closed, open or both tracks. In the Syntax column, a P represents that the systems used a phrase structure grammar representation of syntax, whereas a D represents that they used a dependency representation. in a union of of gold and predicted mentions. Mentions where the first is pronoun and other not are not considered Best link and All links strategy; with and without constraints -Best link without constraints was selected for the official run cai Weights are trained on part of the training dataRecursive 2-way Spectral clustering(Agarwal, 2005)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where in addition to their strategy, positive and negative examples from mentions in the sentence of the closest preceding antecedent are considered Limited number of preceding mentions 60 for automatic and 40 given gold boundaries; Aggressive-merge clustering (Mccarthy and Lenhert, 1995) song Pre-cluster pair models separate for each pair NP-NP, NP-PRP and PRP-PRP Pre-clusters, with singleton pronoun pre-clusters, and use closest-first clustering. Different link models based on the type of linking mentions -NP-PRP, PRP-PRP and NP-NP stoyanov Smart Pair Generation (SmartPG) where the type of antecedent is determined by the type of anaphor using a set of rules Single-link clustering by computing transitive closure between pairwise positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>( 20 )</head><label>20</label><figDesc>John head , a linguist attribute John head , a linguist attribute (22) A famous linguist attribute , he head studied at ... (23) a principal of the firm attribute , J. Smith head</figDesc><table><row><cell cols="2">The head of each appositive construction is distin-guished from the attribute according to the following heuristic specificity scale, in a decreasing order from top to bottom:</cell></row><row><cell>Type</cell><cell>Example</cell></row><row><cell cols="2">Proper noun Pronoun Definite NP Indefinite specific NP a man I know John He the man Non-specific NP man</cell></row><row><cell cols="2">This leads to the following cases:</cell></row><row><cell>(21)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Sheet1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Inter Annotator and Adjudicator agreement for the Coreference Layer in OntoNotes measured in terms of the MUC score.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 1 Procedure used to create OntoNotes training, development and test partitions. GENERATE PARTITIONS(ONTONOTES) returns TRAIN, DEV, TEST 1: TRAIN ← ∅ 2: DEV ← ∅ 3: TEST ← ∅ 4: for all SOURCE ∈ ONTONOTES do 5: if SOURCE = WALL STREET JOURNAL then</figDesc><table><row><cell>6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for TRAIN ← TRAIN ∪ SECTIONS 02 -21 DEV ← DEV ∪ SECTIONS 00, 01, 22, 24 TEST ← TEST ∪ SECTION 23 else if Number of files in SOURCE ≥ 10 then TRAIN ← TRAIN ∪ FILE IDS ending in 1 -8 DEV ← DEV ∪ FILE IDS ending in 0 TEST ← TEST ∪ FILE IDS ending in 9 else DEV ← DEV ∪ FILE IDS ending in 0 TEST ← TEST ∪ FILE ID ending in the highest number TRAIN ← TRAIN ∪ Remaining FILE IDS for the SOURCE end if end if 21: return TRAIN, DEV, TEST</cell></row></table><note>Procedure:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents. Each part was considered a separate document for evaluation purposes.</figDesc><table><row><cell>Syntactic category</cell><cell>Train Count</cell><cell>%</cell><cell>Development Count %</cell><cell>Test Count</cell><cell>%</cell></row><row><cell>NP PRP PRP$ NNP NML Vx Other Overall</cell><cell cols="5">60,345 59.71 8,463 59.31 8,629 53.09 25,472 25.21 3,535 24.78 5,012 30.84 8,889 8.80 1,208 8.47 1,466 9.02 2,643 2.62 468 3.28 475 2.92 900 0.89 151 1.06 118 0.73 1,915 1.89 317 2.22 314 1.93 893 0.88 126 0.88 239 1.47 101,057 100.00 14,268 100.00 16,253 100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Distribution of mentions in the data by their syntactic category.</figDesc><table><row><cell cols="2">Train Development Test</cell></row><row><cell>Entities/Chains 26,612 Links 74,652 Mentions 101,264</cell><cell>3,752 3,926 10,539 12,365 14,291 16,291</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Number of entities, links and mentions in the OntoNotes 4.0 data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Word sense polysemy over verb and noun lemmas in OntoNotes models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>was used, retrained also on all the training portion of the release 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90 Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98 Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87 Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11 Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43 Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11 Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94</figDesc><table><row><cell></cell><cell>N</cell><cell>POS</cell><cell>All Sentences R P</cell><cell>F</cell><cell>N</cell><cell>Sentence len &lt; 40 R P</cell><cell>F</cell></row><row><cell>Broadcast Conversation (BC)</cell><cell>2,194</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5 :</head><label>5</label><figDesc>Parser performance on the CoNLL-2011 test set</figDesc><table><row><cell></cell><cell cols="4">Frameset Accuracy Sentences Propositions Propositions Total Total % Perfect Argument ID + Class P R F</cell></row><row><cell>Broadcast Conversation (BC) Broadcast News (BN) Magazine (MZ) Newswire (NW) Weblogs and Newsgroups (WB)</cell><cell>0.92 0.91 0.89 0.93 0.92</cell><cell>2,037 1,252 780 1,898 929</cell><cell>5,021 3,310 2,373 4,758 2,174</cell><cell>52.18 82.55 64.84 72.63 53.66 81.64 64.46 72.04 47.16 79.98 61.66 69.64 39.72 80.53 62.68 70.49 39.19 81.01 60.65 69.37</cell></row><row><cell>Overall</cell><cell>0.91</cell><cell>6,896</cell><cell>17,636</cell><cell>46.82 81.28 63.17 71.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Performance on the propositions and framesets in the CoNLL-2011 test set.</figDesc><table><row><cell></cell><cell>Accuracy</cell></row><row><cell>Broadcast Conversation (BC) Broadcast News (BN) Magazine (MZ) Newswire (NW) Weblogs and Newsgroups (WB)</cell><cell>0.70 0.68 0.60 0.62 0.63</cell></row><row><cell>Overall</cell><cell>0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Word sense performance over both verbs and nouns in the CoNLL-2011 test set 4.0 data. Given time constraints, we had to perform two modifications: i) Instead of a single model that predicts all arguments including NULL arguments, we had to use the two-stage mode where the NULL arguments are first filtered out and the remaining NON-NULL arguments are classified into one of the argument types, and ii) The argument identification module used an ensemble of ten classifiers -each trained on a tenth of the training data and performed an unweighted voting among them. This should still give a close to state of the art performance given that the argument identification performance tends to start to be asymptotic around 10k training instances. At first glance, the performance on the newswire genre is much lower than what has been reported for WSJ Section 23. This could be attributed to two factors: i) the fact that we had to compromise on the training method, but more importantly because ii) the newswire in OntoNotes not only contains WSJ data, but also Xinhua news. One</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>Frameset polysemy across lemmas</figDesc><table><row><cell>ALL Named Entities</cell><cell>Overall BC BN MZ NW TC WB F F F F F F F 71.8 64.8 72.2 61.5 84.3 39.5 55.2</cell></row><row><cell>Cardinal Date Event Facility GPE Language</cell><cell>68.7 51.8 71.1 66.1 82.8 34.0 68.7 76.1 63.7 77.9 66.7 83.7 60.5 56.0 27.6 00.0 34.8 30.8 47.6 -13.3 41.9 55.0 16.7 23.1 66.7 00.0 22.9 87.9 87.5 90.3 73.7 92.9 65.9 88.7 41.2 -50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Named Entity performance on the CoNLL-2011 test set time constraints, we could not re-train it on the OntoNotes data and so an existing, pre-trained model was used, therefore the results are not a good indicator of the model's best performance. The pre-trained model had also used a somewhat different catalog of name types, which did not include the OntoNotes NORP type (for nationalities, organizations, religions, and political parties), so that category was never predicted. Table10shows the overall performance of the tagger on the CoNLL-2011 test set, as well as the performance broken down by individual name types. IdentiFinder performance has been reported to be in the low 90's on WSJ test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Document ID This is a variation on the document filename 2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc. 3 Word number This is the word index in the sentence 4 Word The word itself 5 Part of Speech Part of Speech of the word 6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the rows of that column. 7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a -8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7. 9 Word sense This is the word sense of the word in Column 3. 10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and</figDesc><table><row><cell>Column</cell><cell>Type Description</cell></row><row><cell>1</cell><cell></cell></row><row><cell>11 12:N N</cell><cell>Web Log data. Named Entities These columns identifies the spans representing various named entities. Predicate Arguments There is one column each of predicate argument structure information for the predicate mentioned in Column 7. Coreference Coreference chain information encoded in a parenthesis structure.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Format of the .conll file used on the shared task</figDesc><table><row><cell cols="5">#begin document (nw/wsj/07/wsj_0771); part 000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>0</cell><cell>''</cell><cell cols="2">'' (TOP(S(S *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>(ARG1 *</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell cols="3">1 Vandenberg NNP</cell><cell>(NP *</cell><cell>-</cell><cell>--</cell><cell cols="3">-(PERSON) (ARG1 *</cell><cell>*</cell><cell>*</cell><cell cols="2">* (8|(0)</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>2</cell><cell>and</cell><cell>CC</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>3</cell><cell cols="2">Rayburn NNP</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell cols="2">-(PERSON)</cell><cell>* )</cell><cell>*</cell><cell>*</cell><cell cols="2">* (23)|8)</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>4</cell><cell cols="2">are VBP</cell><cell>(VP *</cell><cell>be</cell><cell>01 1</cell><cell>-</cell><cell>*</cell><cell>(V * )</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>5</cell><cell cols="2">heroes NNS</cell><cell>(NP(NP * )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell cols="2">* (ARG2 *</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>6</cell><cell>of</cell><cell>IN</cell><cell>(PP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>7</cell><cell>mine</cell><cell>NN</cell><cell>(NP * ))))</cell><cell>-</cell><cell>-5</cell><cell>-</cell><cell>*</cell><cell>* )</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(15)</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>8</cell><cell>,</cell><cell>,</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>9</cell><cell>''</cell><cell>''</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>* )</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 10</cell><cell cols="2">Mr. NNP</cell><cell>(NP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>(ARG0 *</cell><cell>(ARG0 *</cell><cell>*</cell><cell>(15</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 11</cell><cell cols="2">Boren NNP</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell cols="2">-(PERSON)</cell><cell>*</cell><cell>* )</cell><cell>* )</cell><cell>*</cell><cell>15)</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 12</cell><cell cols="2">says VBZ</cell><cell>(VP *</cell><cell>say</cell><cell>01 1</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>(V * )</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 13</cell><cell>,</cell><cell>,</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="4">nw/wsj/07/wsj_0771 0 14 referring VBG</cell><cell>(S(VP *</cell><cell>refer</cell><cell>01 2</cell><cell>-</cell><cell>*</cell><cell cols="2">* (ARGM-ADV *</cell><cell>(V * )</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 15</cell><cell>as</cell><cell>RB</cell><cell>(ADVP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell cols="2">* (ARGM-DIS *</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 16</cell><cell>well</cell><cell>RB</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>* )</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 17</cell><cell>to</cell><cell>IN</cell><cell>(PP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(ARG1 *</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 18</cell><cell cols="2">Sam NNP</cell><cell>(NP(NP *</cell><cell>-</cell><cell>--</cell><cell cols="2">-(PERSON *</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(23</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 19</cell><cell cols="2">Rayburn NNP</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>* )</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 20</cell><cell>,</cell><cell>,</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 21</cell><cell>the</cell><cell>DT</cell><cell>(NP(NP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(ARG0 *</cell><cell>-</cell></row><row><cell cols="3">nw/wsj/07/wsj_0771 0 22 Democratic</cell><cell>JJ</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>(NORP)</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 23</cell><cell cols="2">House NNP</cell><cell>*</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>(ORG)</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 24</cell><cell>speaker</cell><cell>NN</cell><cell>* )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>* )</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 25</cell><cell>who</cell><cell cols="2">WP (SBAR(WHNP * )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell cols="2">* (R-ARG0 * )</cell><cell>-</cell></row><row><cell cols="4">nw/wsj/07/wsj_0771 0 26 cooperated VBD</cell><cell cols="2">(S(VP * cooperate</cell><cell>01 1</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(V * )</cell><cell>-</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 27</cell><cell>with</cell><cell>IN</cell><cell>(PP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>(ARG1 *</cell><cell>-</cell></row><row><cell cols="4">nw/wsj/07/wsj_0771 0 28 President NNP</cell><cell>(NP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell cols="5">nw/wsj/07/wsj_0771 0 29 Eisenhower NNP * )))))))))))</cell><cell>-</cell><cell>--</cell><cell cols="2">-(PERSON)</cell><cell>*</cell><cell>* )</cell><cell>* )</cell><cell>* )</cell><cell>23)</cell></row><row><cell cols="2">nw/wsj/07/wsj_0771 0 30</cell><cell>.</cell><cell>.</cell><cell>* ))</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>0</cell><cell>''</cell><cell>''</cell><cell>(TOP(S *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>1</cell><cell cols="2">They PRP</cell><cell>(NP * )</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell cols="2">* (ARG0 * )</cell><cell>*</cell><cell>(8)</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>2</cell><cell cols="2">allowed VBD</cell><cell>(VP *</cell><cell>allow</cell><cell>01 1</cell><cell>-</cell><cell>*</cell><cell>(V * )</cell><cell>*</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>3</cell><cell>this</cell><cell>DT</cell><cell>(S(NP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell cols="2">* (ARG1 *</cell><cell>(ARG1 *</cell><cell>(6</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>4</cell><cell>country</cell><cell>NN</cell><cell>* )</cell><cell>-</cell><cell>-3</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>* )</cell><cell>6)</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>5</cell><cell>to</cell><cell>TO</cell><cell>(VP *</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>6</cell><cell>be</cell><cell>VB</cell><cell>(VP *</cell><cell>be</cell><cell>01 1</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>(V * )</cell><cell>(16)</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>7</cell><cell>credible</cell><cell cols="2">JJ (ADJP * )))))</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>* )</cell><cell>(ARG2 * )</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>nw/wsj/07/wsj_0771 0</cell><cell>8</cell><cell>.</cell><cell>.</cell><cell>* ))</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>#end document</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>+F 3 3 lee 75.07 66.81 70.70 61.76 57.53 59.57 68.40 68.23 68.31 56.37 56.37 56.37 43.41 47.75 45.48 70.63 76.21 73.02 57.79 sapena 92.39 28.19 43.20 56.32 63.16 59.55 62.75 72.08 67.09 53.51 53.51 53.51 44.75 38.38 41.32 69.50 73.07 71.10 55.99 chang 68.08 61.96 64.88 57.15 57.15 57.15 67.14 70.53 68.79 54.40 54.40 54.40 41.94 41.94 41.94 71.19 77.09 73.71 55.96 nugues 69.87 68.08 68.96 60.20 57.10 58.61 66.74 64.23 65.46 51.45 51.45 51.45 38.09 41.06 39.52 71.99 70.31 71.11 54.53 santos 67.80 63.25 65.45 59.21 54.30 56.65 68.79 62.81 65.66 49.54 49.54 49.54 35.86 40.21 37.91 73.37 66.91 69.46 53.41 song 57.81 80.41 67.26 53.73 67.79 59.95 60.65 66.05 63.23 46.29 46.29 46.29 43.37 30.71 35.96 69.49 59.71 61.47 53.05 stoyanov 70.84 64.98 67.78 63.61 54.04 58.43 72.58 53.27 61.44 46.08 46.08 46.08 32.00 40.82 35.88 73.21 58.93 60.88 51.92 sobha 67.82 62.09 64.83 51.08 49.88 50.48 62.63 65.43 64.00 49.48 49.48 49.48 40.65 41.82 41.23 61.40 68.35 63.88 51.90 kobdani 62.06 60.04 61.03 55.64 51.50 53.49 69.66 62.43 65.85 42.70 42.70 42.70 32.33 35.40 33.79 61.86 63.51 62.61 51.04 zhou 61.08 63.59 62.31 45.65 52.79 48.96 57.14 72.91 64.07 47.53 47.53 47.53 43.19 36.79 39.74 61.10 73.94 64.72 50.92 charton 65.90 62.77 64.30 55.09 50.05 52.45 66.26 58.44 62.10 46.82 46.82 46.82 34.33 39.05 36.54 69.94 62.23 64.80 50.36 yang 71.92 57.53 63.93 59.91 46.43 52.31 71.64 55.14 62.32 46.55 46.55 46.55 30.28 42.39 35.33 71.11 61.75 64.63 49.99 hao 64.50 64.11 64.30 57.89 51.42 54.47 67.83 55.43 61.01 45.07 45.07 45.07 30.08 35.76 32.67 72.61 62.37 65.35 49.38 xinxin 65.49 58.71 61.92 48.54 44.85 46.62 61.59 62.28 61.93 44.75 44.75 44.75 35.19 38.62 36.83 63.04 65.83 64.27 48.46 zhang 55.35 68.25 61.13 42.03 55.62 47.88 52.57 73.05 61.14 44.46 44.46 44.46 42.00 30.28 35.19 62.84 69.22 65.21 48.07 kummerfeld 69.77 56.97 62.72 46.39 39.56 42.70 63.60 57.30 60.29 45.35 45.35 45.35 35.05 42.26 38.32 58.74 61.58 59.91 47.10 zhekova 67.49 37.60 48.29 28.87 20.66 24.08 67.14 56.67 61.46 40.43 40.43 40.43 31.57 41.21 35.75 52.77 57.05 53.77 40.43 irwin 17.06 61.09 26.67 12.45 50.60 19.98 35.07 89.90 50.46 31.68 31.68 31.68 45.84 17.38 25.21 51.48 56.83 51.12 31.88</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 12 :</head><label>12</label><figDesc>Performance of systems in the official, closed 67.64 67.40 56.73 58.90 57.80 64.60 71.03 67.66 53.37 53.37 53.37 42.71 40.68 41.67 69.77 73.96 71.62 55.71 uryupina 70.60 66.31 68.39 59.70 55.70 57.63 66.29 64.12 65.18 51.42 51.42 51.42 38.34 42.17 40.16 69.23 68.54 68.88 54.32 klenner 64.41 60.28 62.28 49.04 50.71 49.86 61.70 68.61 64.97 50.03 50.03 50.03 41.28 39.70 40.48 66.05 73.90 69.05 51.77 irwin 24.60 62.27 35.27 18.56 51.01 27.21 38.97 85.57 53.55 33.86 33.86 33.86 43.33 19.36 26.76 51.62 52.91 51.76 35.84</figDesc><table><row><cell>track using all predicted information</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Performance of systems in the official, open 70.74 72.42 64.33 60.05 62.12 68.26 65.17 66.68 53.84 53.84 53.84 39.86 44.23 41.93 72.53 71.04 71.75 56.91 chang 63.37 73.18 67.92 55.00 65.50 59.79 62.16 76.65 68.65 54.95 54.95 54.95 46.77 37.17 41.42 70.97 79.30 74.29 56.62 santos 65.82 69.90 67.80 57.76 61.39 59.52 64.49 70.27 67.26 51.87 51.87 51.87 41.42 38.16 39.72 72.72 71.97 72.34 55.50 kobdani 67.11 65.09 66.08 62.63 56.80 59.57 73.20 62.22 67.27 44.49 44.49 44.49 32.87 37.25 34.92 64.07 64.13 64.10 53.92 stoyanov 76.90 64.73 70.29 69.81 55.01 61.54 77.07 52.54 62.48 48.08 48.08 48.08 30.97 44.84 36.64 76.57 60.33 62.96 53.55 zhang 59.62 71.19 64.89 46.06 58.75 51.64 53.89 73.41 62.16 46.62 46.62 46.62 43.49 32.11 36.95 64.11 70.47 66.54 50.25 song 58.43 77.64 66.68 46.66 68.40 55.48 54.40 70.19 61.29 43.62 43.62 43.62 43.77 25.88 32.53 66.29 58.76 60.22 49.77 zhekova 69.19 57.27 62.67 33.48 37.15 35.22 55.47 68.23 61.20 41.31 41.31 41.31 38.29 34.65 36.38 53.45 63.33 54.79 44.27</figDesc><table><row><cell>track using all predicted information</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 14 :</head><label>14</label><figDesc>Performance of systems in the supplementary closed track using predicted information plus gold boundaries 72.33 75.39 66.93 63.91 65.39 70.09 71.49 70.78 59.78 59.78 59.78 46.34 49.62 47.92 73.38 79.00 75.83 61.36</figDesc><table><row><cell>Official</cell><cell>F 1 +F 3 +F 2</cell><cell>3</cell></row><row><cell></cell><cell cols="2">F</cell></row><row><cell>BLANC</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 3</cell></row><row><cell>CEAFe</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F</cell></row><row><cell>CEAFm</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 2</cell></row><row><cell>B-CUBED</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 1</cell></row><row><cell>MUC</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell>Mention Detection</cell><cell cols="2">P F R 78.71</cell></row><row><cell cols="2">System</cell><cell>lee</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 15 :</head><label>15</label><figDesc>Performance of systems in the supplementary open</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 16 :</head><label>16</label><figDesc>Performance of systems in the supplementary, closed track using predicted information plus gold mentions 90.93 74.79 89.68 81.56 67.46 86.88 75.95 70.73 70.73 70.73 77.75 51.05 61.64 76.65 85.85 80.35 73.05</figDesc><table><row><cell>Official</cell><cell>F 1 +F 3 +F 2</cell><cell>3</cell></row><row><cell></cell><cell cols="2">F</cell></row><row><cell>BLANC</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 3</cell></row><row><cell>CEAFe</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F</cell></row><row><cell>CEAFm</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 2</cell></row><row><cell>B-CUBED</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell></cell><cell cols="2">F 1</cell></row><row><cell>MUC</cell><cell cols="2">P</cell></row><row><cell></cell><cell cols="2">R</cell></row><row><cell>Mention Detection</cell><cell cols="2">F R P 83.37 100</cell></row><row><cell cols="2">System</cell><cell>lee</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 17 :</head><label>17</label><figDesc>Performance of systems in the supplementary, open track using predicted information plus gold mentions 29.07 44.55 56.99 63.91 60.25 62.89 72.31 67.27 53.90 53.90 53.90 45.22 38.70 41.71 69.71 73.32 71.32 56.41 chang 69.88 63.61 66.60 58.48 58.48 58.48 67.42 70.91 69.12 55.21 55.21 55.21 42.66 42.66 42.66 71.42 77.36 73.96 56.75 nugues 72.96 71.08 72.01 62.68 59.46 61.03 67.24 64.89 66.04 52.82 52.82 52.82 39.25 42.50 40.81 72.57 70.86 71.68 55.96 santos 70.39 65.67 67.95 61.28 56.20 58.63 69.25 63.16 66.07 50.47 50.47 50.47 36.51 41.15 38.69 73.92 67.32 69.93 54.46 song 59.24 82.39 68.92 54.92 69.29 61.27 60.89 66.27 63.46 46.97 46.97 46.97 44.49 31.15 36.65 69.73 59.87 61.61 53.79 stoyanov 74.43 68.28 71.22 67.18 57.08 61.72 74.06 53.45 62.09 47.40 47.40 47.40 32.78 42.52 37.02 74.10 59.34 61.31 53.61 sobha 71.06 65.06 67.93 53.91 52.64 53.27 63.17 66.14 64.62 50.80 50.80 50.80 41.77 43.03 42.39 61.91 69.15 64.49 53.43 kobdani 65.98 63.83 64.89 59.22 54.81 56.93 70.49 63.12 66.60 44.17 44.14 44.15 33.19 36.50 34.77 62.52 64.25 63.32 52.77 zhou 64.11 66.74 65.40 48.00 55.51 51.48 57.18 73.71 64.40 48.40 48.40 48.40 44.18 37.35 40.48 61.54 74.86 65.30 52.12 charton 71.01 67.64 69.28 59.24 53.82 56.40 67.10 59.02 62.80 48.91 48.91 48.91 35.96 41.39 38.48 70.65 62.71 65.34 52.56 yang 73.73 58.97 65.53 61.23 47.45 53.47 71.88 55.13 62.40 47.05 47.05 47.05 30.54 43.16 35.77 71.39 61.92 64.83 50.55 hao 66.79 66.38 66.59 59.55 52.89 56.02 68.27 55.46 61.20 45.95 45.95 45.95 30.76 36.81 33.51 73.22 62.73 65.78 50.24 xinxin 69.05 61.91 65.28 50.99 47.11 48.97 61.59 62.70 62.14 45.64 45.64 45.64 35.86 39.57 37.62 63.42 66.29 64.68 49.58 zhang 57.41 70.78 63.40 43.48 57.53 49.53 52.44 73.60 61.24 44.97 44.97 44.97 42.71 30.44 35.55 63.12 69.63 65.53 48.77 kummerfeld 71.05 58.01 63.87 47.42 40.44 43.65 63.73 57.39 60.39 45.76 45.76 45.76 35.30 42.72 38.66 58.89 61.77 60.07 47.57 zhekova 72.65 40.48 51.99 31.73 22.70 26.46 66.92 56.68 61.37 41.04 41.04 41.04 31.93 42.17 36.34 53.09 57.86 54.22 41.39 irwin 17.58 62.96 27.49 12.69 51.59 20.37 34.88 89.98 50.27 31.71 31.71 31.71 46.13 17.33 25.20 51.51 56.93 51.14 31.95</figDesc><table><row><cell>Official</cell><cell>F 1 +F 2 +F 3</cell><cell>3 58.73</cell></row><row><cell>BLANC</cell><cell cols="2">R P F 70.93 76.58 73.36</cell></row><row><cell>CEAFe</cell><cell cols="2">R P F 3 44.19 48.75 46.36</cell></row><row><cell>MUC B-CUBED CEAFm</cell><cell cols="2">R P F 1 R P F 2 R P F 63.29 58.96 61.05 68.84 68.72 68.78 57.28 57.28 57.28</cell></row><row><cell>Mention Detection</cell><cell cols="2">R P F 76.79 68.34 72.32 95.27</cell></row><row><cell cols="2">System</cell><cell>lee sapena</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 18 :</head><label>18</label><figDesc>Head word based performance of systems in the official, closed 69.82 69.57 58.39 60.63 59.49 64.88 71.53 68.04 54.36 54.36 54.36 43.74 41.58 42.64 70.13 74.39 72.01 56.72 uryupina 72.10 67.72 69.84 60.74 56.68 58.64 66.43 64.25 65.32 52.00 52.00 52.00 38.87 42.85 40.76 69.43 68.73 69.07 54.91 klenner 71.73 67.14 69.36 55.17 57.04 56.09 62.67 70.69 66.44 53.25 53.25 53.25 44.27 42.39 43.31 67.45 75.92 70.68 55.28 irwin 25.24 63.87 36.18 18.90 51.94 27.71 38.79 85.64 53.40 33.89 33.89 33.89 43.59 19.31 26.76 51.66 52.98 51.80 35.96</figDesc><table><row><cell>track using all predicted information</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 19 :</head><label>19</label><figDesc>Head word based performance of systems in the official, open</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 20 :</head><label>20</label><figDesc>Detailed look at the performance per genre for the official, closed track using automatic performance. MD represents MENTION DETECTION; BCUB represents B-CUBED; C m represents CEAF m ; C e represents CEAF e and O represents the OFFICIAL score.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 21 :</head><label>21</label><figDesc>Detailed look at the performance per genre for the official, open track using predicted information. MD represents MENTION DETECTION; BCUB represents B-CUBED; C m represents CEAF m ; C e represents CEAF e and</figDesc><table /><note>O represents the OFFICIAL score.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 23 :</head><label>23</label><figDesc>Participating system profiles -Part II. This focuses on the way positive and negative examples were generated and the decoding strategy used.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://projects.ldc.upenn.edu/ace/data/ 2 http://www.bbn.com/nlp/ontonotes</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://conll.bbn.com/download/conll-train.id http://conll.bbn.com/download/conll-dev.id http://conll.bbn.com/download/conll-test.id</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Given the nature of word sense annotation, and changes in project priorities, we could not annotate all the low frequency verbs and nouns in the corpus. Furthermore, PropBank annotation currently only covers verb predicates.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">There is another phrase type -EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data.8  A study by<ref type="bibr" target="#b4">Charniak and Johnson (2001)</ref> shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall.9  doc/propbank/english-propbank.pdf</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz13  The number of lemmas in Table6do not add up to this number because not all of them have examples in the training data, where the total number of instantiated senses amounts to 7933.14 http://cemantix.org/assert.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The number of lemmas in Table9do not add up to this number because not all of them have examples in the training data, where the total number of instantiated senses amounts to 4229.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">OntoNotes is deeply grateful to the Linguistic Data Consortium for making the source data freely available to the task participants.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">The MUC corpora did not tag single mention entities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3 20 http://conll.bbn.com/download/scorer.v4.tar.gz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the Defense Advanced Research Projects Agency</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Issues in synchronizing the English treebank and propbank</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Babko-Malaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szuting</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Frontiers in Linguistically Annotated Corpora</title>
				<imprint>
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for Scoring Coreference Chains</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
				<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bootstrapping path-based pronoun resolution</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation metrics for end-to-end coreference resolution systems</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL &apos;10</title>
				<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Edit detection and parsing for transcribed speech</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of North American Chapter of the Association of Computational Linguistics</title>
				<meeting>the Second Meeting of North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarse-tofine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Message understanding conference (MUC) 6</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
		<idno>LDC2003T13</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Message understanding conference (MUC) 7</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<idno>LDC2001T02</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">First-order probabilistic models for coreference resolution</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/NAACL</title>
				<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint determination of anaphoricity and coreference resolution using integer programming</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
				<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global joint models for coreference resolution and named entity classification</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
				<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Background to framenet</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program-tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Los Angeles</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Janštěpánek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text and knowledge mining for coreference resolution</title>
		<author>
			<persName><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
		<title level="m">Proceedings of the Seventh Message Understanding Conference</title>
				<meeting>the Seventh Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Coreference task definition (v3.0, 13 jul 97</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
				<meeting>HLT/NAACL<address><addrLine>New York City, USA, June</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A large-scale classification of english verbs. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="21" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional models of identity uncertainty with application to noun coreference</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using decision trees for coreference resolution</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coreference for nlp applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, October</title>
				<meeting>the 38th Annual Meeting of the Association for Computational Linguistics, October</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shallow semantics for coreference resolution</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI</title>
				<meeting>the IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1396" to="1411" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An annotated corpus of semantic roles</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Making fine-grained and coarse-grained sense distinctions, both manually and automatically</title>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computing reliability for coreference annotation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky</title>
				<meeting>the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The mate/gnome scheme for anaphoric annotation, revisited</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
				<meeting>SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">State-of-the-art nlp approaches to coreference resolution: Theory and practical recipes</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial Abstracts of ACL-IJCNLP 2009</title>
				<meeting><address><addrLine>Suntec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
	<note>Singapore</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic role labeling for coreference resolution</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Volume of the Proceedings of the 11th Meeting of the European Chapter of the Association for Computational Linguistics</title>
				<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04" />
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT/NAACL</title>
				<meeting>the HLT/NAACL<address><addrLine>New York City, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Support vector learning for semantic argument classification</title>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Krugler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="39" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">OntoNotes: A Unified Relational Semantic Representation</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Semantic Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="419" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unrestricted Coreference: Indentifying Entities and Events in OntoNotes</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Macbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Semantic Computing (ICSC)</title>
				<meeting>the IEEE International Conference on Semantic Computing (ICSC)</meeting>
		<imprint>
			<date type="published" when="2007-09-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Blanc: Implementing the rand index for coreference evaluation</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 1: Coreference resolution in multiple languages</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emili</forename><surname>Sapena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Véronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
				<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrase</title>
		<author>
			<persName><forename type="first">W</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conundrums in noun phrase coreference resolution: Making sense of the state-of-theart</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th</title>
				<meeting>the Joint Conference of the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m">Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting><address><addrLine>Suntec, Singapore, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
				<meeting><address><addrLine>Manchester, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Antecedent selection techniques for high-recall coreference resolution</title>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A model theoretic coreference scoring scheme</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Message Undersatnding Conference (MUC-6)</title>
				<meeting>the Sixth Message Undersatnding Conference (MUC-6)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<title level="m">BBN pronoun coreference and entity type corpus LDC catalog no.: LDC2005T33. BBN Technologies</title>
				<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">OntoNotes: A Large Training Corpus for Enhanced Processing</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing and Machine Translation</title>
				<editor>
			<persName><forename type="first">Joseph</forename><surname>Olive</surname></persName>
			<persName><forename type="first">Caitlin</forename><surname>Christianson</surname></persName>
			<persName><forename type="first">John</forename><surname>Mccary</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
