<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2015 Task 9: CLIPEval Implicit Polarity of Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Irene</forename><surname>Russo</surname></persName>
							<email>irene.russo@ilc.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="institution">ILC-CNR &quot;A. Zampolli&quot; Via G. Moruzzi</orgName>
								<address>
									<postCode>1 56124</postCode>
									<settlement>Pisa</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
							<email>t.caselli@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Vrije Universiteit</orgName>
								<address>
									<addrLine>Amsterdam De Boelelaan</addrLine>
									<postCode>1105, 1081 HV</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
							<affiliation key="aff2">
								<address>
									<postCode>18</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fondazione</forename><forename type="middle">Bruno</forename><surname>Kessler</surname></persName>
							<affiliation key="aff2">
								<address>
									<postCode>18</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2015 Task 9: CLIPEval Implicit Polarity of Events</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment analysis tends to focus on the polarity of words, combining their values to detect which portion of a text is opinionated. CLIPEval wants to promote a more holistic approach, looking at psychological researches that frame the connotations of words as the emotional values activated by them. The implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current research in sentiment analysis (SA, henceforth) is mostly focused on lexical resources that store polarity values. For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items. This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events -involving perspectives and points of view -are expressed.</p><p>In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions -mainly adjectives, adverbs and several nouns -leaving verbs on the foreground. Improvements have been proposed by taking into account syntax <ref type="bibr" target="#b7">(Greene and Resnik 2009)</ref> and by investigating the connotative polarity of words <ref type="bibr">(Cambria et al., 2009;</ref><ref type="bibr">Akkaya et al., 2009</ref><ref type="bibr">, Balhaur et al., 2011</ref><ref type="bibr" target="#b17">Russo et al. 2011;</ref><ref type="bibr" target="#b4">Cambria et al., 2012</ref><ref type="bibr">, Deng et al., 2013</ref>.</p><p>One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment. Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions.</p><p>Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) <ref type="bibr" target="#b9">(Lewinsohn and Amenson, 1978;</ref><ref type="bibr" target="#b10">MacPhillamy and Lewinsohn, 1982)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Measuring Emotional Connotations: Psychological Studies</head><p>For a long time research in psychology has been interested in a subjective cultural and/or emotional coloration in addition to the explicit or denotative meaning of any specific word or phrase. Starting with the work of Charles E. Osgood, who in the 50s developed a technique for measuring the connotative meaning of concepts and analyzed human attitudes <ref type="bibr" target="#b14">(Osgood et al., 1957)</ref>, psychologists have experi-mented with emotional values activated by words, often through the evaluation of their pleasantness. Osgood and his colleagues proposed a factor analysis based on semantic differential scales measuring three basic attitudes that people display crossculturally: evaluation (along the scale of adjectives "good-bad"), potency (along "strong-weak") and activity ("active-passive"). This line of research continued with studies evaluating Osgood's findings with different population and the pleasantness of words became also a dimension to correlate with other dimensions reported in semantic norms studies, such as familiarity and imagery. We know today that pleasantness is a semantic factor influencing short and long term memory <ref type="bibr" target="#b11">(Monnier et al., 2008)</ref>; similarly, <ref type="bibr" target="#b8">(Hadley and MacKay, 2006)</ref> showed that STM for certain unpleasant emotional words (i.e., taboo words) was better than that for neutral words. Emotional words are better recalled because they are related to longterm representations of autobiographical and selfreference units <ref type="bibr" target="#b13">(Ochsner, 2000)</ref>. Other factors have a role: depressed subjects, for example, recalled more unpleasant words than pleasant words.</p><p>Osgood's studies were revised for the production of the Affective Norms for English Words (ANEW) <ref type="bibr" target="#b2">(Bradley et al, 1999)</ref>, a set of normative emotional ratings for 1034 words in American English. This set of verbal materials have been rated in terms of pleasure, arousal, and dominance in order to create a standard for use in studies of emotion and attention (the same three basic dimensions used by Osgood). Affective valence (or pleasure, ranging from pleasant to unpleasant) and arousal (ranging from calm to excited) were the two primary dimensions. A third, less strongly-related dimension, was called "dominance" or "control".</p><p>Connotative meaning emerges as a complex and stratified concept and only psychological studies can guide in this maze, especially when they are supported by significant experimental outputs such as list of words evaluated by human subjects.</p><p>All these studies are relevant for NLP because connotative meanings of words can help to refine automatic sentiment analysis on social media, where shared contents are often just short reports on pleasant or unpleasant events and activities. For example, <ref type="bibr">(Fenf et al., 2013)</ref> report that connotation lexicon guarantees better performance than other sentiment analysis lexicons that do not encode connotations on Twitter data.</p><p>That said, when psychological experiments ask for judgments about single words they oversimplify: we experience the meanings of single words as arising from compositionality, in expressions and sentences. Even neutral words in specific contexts can acquire a polarity as effect of semantic prosody <ref type="bibr">(Louw 1993)</ref>.</p><p>When subjects are asked for the pleasantness of an event they need to evaluate not just single words but complete sentences; for this reason <ref type="bibr" target="#b9">(Lewinsohn and Amenson, 1978;</ref><ref type="bibr" target="#b10">MacPhillamy and Lewinsohn, 1982)</ref> developed two psychometric instruments, the Pleasant Events Schedule and the Unpleasant Events Schedule, by sampling events that were reported to be source of pleasure or distress by highly diverse samples of people that rated the frequency of event's occurrence during past month plus a complete mood ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLIPEval Annotation</head><p>The CLIPEval exercise provides the NLP community with a newly developed dataset grounded on psychological studies about the pleasantness of events. Dedicated annotation specifications and guidelines for the release of the dataset have been developed.</p><p>The starting point for the development of the annotation guidelines was the PE/UPEs lists, the set of 640 pleasant and unpleasant events (320 pleasant events and 320 unpleasant events, respectively) collected by <ref type="bibr" target="#b9">(Lewinsohn and Amenson, 1978)</ref> and <ref type="bibr" target="#b10">(MacPhillamy and Lewinsohn, 1982)</ref>. The dataset could not be used as it is since it is a list of generic sentences describing either states or actions which are labeled as pleasant or unpleasant events. To clarify this, we report two examples extracted from the original dataset. Example 1.) is a pleasant event while example 2.) is an an unpleasant event. The numbers in brackets at the beginning of the sentence refer to the PE/UPEs number in the original dataset. 1.) (9) Planning trips or vacations.</p><p>2.) (10) Getting separated or divorced from my spouse.</p><p>Furthermore, a closer examination of PE/UPEs has shown that ambiguity occurs, with the same events considered both as a pleasant and an unpleasant one (e.g. Being alone), since this is plausible from a psychological point of view. To overcome these issues and to make the task relevant for sentences from news articles, we have applied the following strategies:</p><p>• all ambiguous PE/UPEs have been removed from the original dataset;</p><p>• PE/UPEs have been grouped into classes whose labels describe and aggregate different PE/UPEs, referring often to a more general event class with respect to the one the single instance of a PE/UPE event describes. This choice has been necessary because the event instances in the original psychological dataset are conceptually similar but using the original descriptions would result either in too generic cases (e.g. Being with children) or too simple (e.g. Washing my hair).</p><p>The grouping of PE/UPEs in classes has been conducted in two phases by two annotators. In the first phase, both annotators have worked independently: for each PE/UPE the annotators had to decide which of them could be clustered in a more generic class and which were to be excluded, either because it describes a too specific (or a too generic) event or because it explicitly express the pleasanteness of the event (e.g. (25) Driving skillfully). As a measure of agreement for this task, we preferred not to use kappa score, because it's not a standard classification task, but we computed the percentage of agreement. The first evaluation shown a relatively low agreement, only 59.06% of the 640 events were considered as belonging to a cluster. An analysis on the cases of disagreement has highlighted some inconsistencies. Thus, a second clusterization task has been performed by asking to the same annotators to go over the same data following new additional rules that were developed during the analysis. The evaluation of this second phase shown a clear improvement with a percentage agreement of 68.25%. As a result of these annotation phases, we had a set of clusters that the annotators were allowed to discuss, finding a joint solution in cases of disagreements and identifying the best labels for the PE/UPEs clusters. The final output of these two phases resulted in 8 classes of PE/UPEs (see Table <ref type="table" target="#tab_0">1</ref> column "Event Class"). It is important to point out that most of these classes contain PE/UPEs both from the 320 pleasant events and the 320 unpleasant events and as a consequence the polarities of their occurrences in the training data are mixed(see Table <ref type="table" target="#tab_0">1</ref>). Due to the novelty of the task, we could not re-use available datasets for SA. For this reason, the second step concerns the identification and manual annotation of real sentences from the Annotated English Gigaword corpus <ref type="bibr" target="#b12">(Napoles et al., 2012)</ref>, an automatically-generated syntactic and discourse structure annotated version of the English Gigaword corpus Fifth Edition, which contains a large English corpus of newspaper articles (four billion words ca.). To facilitate the sentence extraction phase, we manually identified the verbal and the nominal keywords from the event mentions composing the classes. We used WN30 and the Oxford Dictionary to extract all verb and noun synonyms of the PE/UPEs in each class. We then queried the Gigaword corpus with this extended set of keywords to extract sentences which contain self-reported events by means of following patterns:</p><p>• "I|we + [verbal keyword]"</p><p>• "I|we + [nominal keyword]"</p><formula xml:id="formula_0">• "I|we + [verbal keyword] + [nomi- nal keyword]".</formula><p>The sentences thus extracted were manually filtered and annotated with respect to the 8 classes and to their polarity. The annotation has been conducted at sentence level. To provide homogeneous data and annotations, the following guidelines have been developed for the assignment of the class label:</p><p>• the class label and the polarity value must be assigned on the basis of the event that correspond syntactically to the main verb in the sentence;</p><p>• in case of coordinated main clauses, only the first main clause is taken into account to assign the class label and the polarity value;  • subordinated clauses are not annotated with class labels and polarity values.</p><p>Although all event mentions in the selected clusters have either a positive (pleasant events) or negative (unpleasant events) polarity that could be reversed by negation, during the annotation phase a third value, namely neutral, has been introduced to cope with those sentences containing self-reporting events whose occurrence is uncertain</p><p>We are referring here to the notion of event factuality (Sauríand Pustejovsky, 2009), i.e. the degrees of certainty (e.g. possible, probable, certain) associated to an event description along the category of epistemic modality. In the annotation we focused on the syntactic information between target events instances and factuality markers, such as modal auxiliaries and negation cues (including adverbs, adjectives, prepositions, pronouns and determiners). Events which are in the scope of factuality markers signaling uncertainty or improbability have been marked as neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLIPEval Tasks</head><p>The CLIPEval evaluation exercise is composed of two tasks described as follows: The test data has been provided in a single file with only two fields: the sentence id and the sentence extracted from the Gigaword corpus: 6.) 12 After having given a friend a lift home I was stopped by police.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.) 23</head><p>And then we went to a library. The division of the training data for the three polarity values is not balanced due to the event mentions composing the clusters. Only three clusters, namely GOING TO PLACES, PERSONAL CARE and ATTENDING EVENT, present a relatively balanced distribution for the polarity values. This lack of balance reflects real language data: the prevalence of positive or negative values is due to the classes which may have more PEs or UPEs (e.g. OUTDOOR ACTIVITY and COMMUNICA-TION ISSUE, respectively). Including more sentences which reverse the polarity of the PEs or UPEs to balance the occurrences per polarity value would mean to force the data from real language toward an artificial equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Since both Task A and Task B of CLIPEval are essentially classification tasks (classification of the polarity value for Task A and classification of the event instance and the polarity value for Task B), we have used Precision, Recall and F1-measure to evaluate the system results against the test set. Furthermore, since this is a multi-classification task (3 possible values for Task A and 24 possible values for Task B), we have computed micro average Precision, Recall and F1-measure per class. This latter measure has been used for the final ranking of the systems. We have adopted standard definitions for these measures, namely: To better evaluate systems' performances, we have developed three baselines, one per Task A and two per Task B. In particular:</p><p>• Task A baseline has been obtained by assigning to each sentence in the test set the most frequent polarity value on the basis of the data in the training set. This resulted in marking all 371 sentences in the test set with NEGATIVE polarity;</p><p>• Task B baseline 1 has been obtained in two steps: first, for each class in the training data we have selected the most frequent nouns and verbs lemmas. This has provided us with a list of keywords representing each class. We have then compared each sentence in the test set with each group of keywords and assigned as correct the class which scored the higher number of matches. In case of a draw, a random class between the classes with the highest scores is assigned. If no match is found, a random class is assigned. As for the polarity, we have used the absolute most frequent polarity values, like in task A (i.e. all test set entries have been assigned to NEGATIVE value).</p><p>• Task B baseline 2 has been obtained following the approach in Task B baseline 1 for the class assignment and we have assigned the most frequent polarity value per class according to training data (e.g. for items classified as ATTENDING EVENTS the assigned polarity value is POSITIVE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Participant Systems</head><p>Overall 26 different teams registered for the task, only two submitted the output of their system for a total of 3 runs: SHELLFBK (Fondazione Bruno Kessler) and SIGMA2320 (Peking Univeristy).</p><p>Only SHELLFBK submitted results for both tasks. Furthermore, we can provide a short description just for SHELLFBK since the SIGMA2320 team has not submitted a system description paper. SHELLFBK system implements a supervised approach based on information retrieval techniques for representing polarized information. During the training phase, each sentence is analyzed by applying the parser contained in the Stanford NLP Library. From the results of the parsing activity, both the list of the dependency relations and the parsed trees are used for populating an inverted index data structure containing the relationships between each relation extracted from the sentences and the corresponding information about its polarization. The result of the training phase is a set of three indexes containing, respectively, the positive, negative, and neutral information analyzed in the training set. When the polarity of a new sentence has to be computed, the new sentence is given as input to the Stanford NLP Library by obtaining the list of its dependency relations, as well as, the corresponding parsed tree. Such information are built together for composing a query that is afterwards performed on the indexes built during the training phase. For each of the built indexes, a retrieval score value is retrieved by the system and, based on this, the polarity of the new sentence is assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Results</head><p>We report in Table <ref type="table" target="#tab_3">3</ref> the results of both systems for Task A and the Task A baseline. In Table <ref type="table" target="#tab_4">4</ref> we report the results for Task B and both baseline for Task B (baseline 1 and baseline 2, respectively).  SHELLFBK outperforms SIGMA2320 for the Task A; both systems improve the baseline. The results are not as good as in classification tasks concerning the polarities of tweets <ref type="bibr">(Rosental et al., 2014)</ref> or reviews <ref type="bibr" target="#b15">(Pontiki et al., 2014)</ref> but since this is a novel task about implicit polarity we think they are promising.</p><p>For task B SHELLFBK has a better performance both in terms of precision and recall if compared with the two baselines. At the moment we do not know if the results are due to SHELLFBK methodology or if data sparseness in the classes has an influence on the classification task: maybe classes more cohesive from conceptual and lexical point of view could be easier to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>The implicit polarity of words concerns the arising of occasional polarized meanings in specific expressions/linguistic contexts. Labeled as semantic prosody in corpus studies and part of what psychologists call connotative meanings, the implicit polarity is a quite marginal concept in sentiment analysis. It requires a dynamic representation for the polarity of words (i.e. a verb can be neutral in the vast majority of case but can be clearly positive in some contexts) and a compositional approach to sentiment values that goes beyond the oversimplifying assumptions of bag-of-words approaches.</p><p>With the CLIPEval task we asked the NLP community to look at these complexities, considering the detection of a set of events as relevant for SA analyses because they have been judged as pleasant or unpleasant by subjects in psychological experiments conducted by <ref type="bibr" target="#b9">(Lewinsohn and Amenson, 1978;</ref><ref type="bibr" target="#b10">MacPhillamy and Lewinsohn, 1982)</ref>. As future work we plan to extend the dataset, including new classes of events and annotating instances from blogs and tweets. Also, we want to integrate the detection of polarized events with the work on stance and perspectives in news, going toward a theoretical model for SA that takes into account the interplay of linguistic means used by humans to express opinions and feelings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Task A: identification of the polarity value associated to the event instance. Participants are required to associate each sentence with a polarity value (POSITIVE, NEGATIVE or NEU-TRAL); • Task B: identification of the event mentions with respect to one of the 8 event class labels plus identification of the polarity value. The class labels used are: ATTENDING EVENT, COMMUNICA-TION ISSUE, GOING TO PLACES; LEGAL ISSUE, MONEY ISSUE, OUT-DOOR ACTIVITIES, PERSONAL CARE, (FEAR OF) PHYSICAL PAIN. As in Task A the polarity values are (POSITIVE, NEGA-TIVE or NEUTRAL). 5 Dataset Description The CLIPEval evaluation exercise is based on the CLIPEval dataset, which consists of two parts: a training set and a test set. The final size of the dataset is 1,651 sentences, divided in 1,280 sentences for the training and 371 for the test. Each event class in the training data contains 160 sentences. Each class in the training set is available in a separate file composed of four tab separated fields: a sentence id, the sentence extracted from the Gigaword corpus, the polarity value and the class label. Each file is named with the class label. Some exam-ples of the training data are provided in the examples below (examples from 3.) to 5.)): 3.) 8 I had just gone to a concert with my parents and I identified with the conductor a lot Dudamel said in Spanish during a recent interview in Caracas. POSITIVE ATTEND-ING EVENT 4.) 14 "It's too cold and I can't ride my bike" he lamented. NEGATIVE OUT-DOOR ACTIVITY 5.) 4 "I could take the boys to the sports museum' says James. NEUTRAL GOING TO PLACES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Precision: the number of correctly classified positive examples, tp i per class C i , divided by number of examples labeled by the system as positive (tp i plus false positive fp i ): l i=1 tp i tp i +fp i • Recall: the number of correctly classified positive examples tp i per class C i divided by the number of positive examples in the data (tp i plus false negatives fn i ) : l i=1 tp i tp i +fn i • F-measure: the mean of Precision and Recall calculated as follows: (β 2 +1)PrecisionRecall β 2 Precision+Recall</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CLIPEval corpus: Training data.</figDesc><table><row><cell>Event Class</cell><cell>POSITIVE</cell><cell>NEGATIVE</cell><cell>NEUTRAL</cell><cell>Tot. Instances</cell></row><row><cell>(FEAR OF) PHYSICAL PAIN</cell><cell>19</cell><cell>131</cell><cell>10</cell><cell>160</cell></row><row><cell>ATTENDING EVENT</cell><cell>83</cell><cell>35</cell><cell>42</cell><cell>160</cell></row><row><cell>COMMUNICATION ISSUE</cell><cell>21</cell><cell>120</cell><cell>19</cell><cell>160</cell></row><row><cell>GOING TO PLACES</cell><cell>55</cell><cell>72</cell><cell>33</cell><cell>160</cell></row><row><cell>LEGAL ISSUE</cell><cell>24</cell><cell>115</cell><cell>21</cell><cell>160</cell></row><row><cell>MONEY ISSUE</cell><cell>20</cell><cell>109</cell><cell>31</cell><cell>160</cell></row><row><cell>OUTDOOR ACTIVITY</cell><cell>125</cell><cell>18</cell><cell>17</cell><cell>160</cell></row><row><cell>PERSONAL CARE</cell><cell>88</cell><cell>40</cell><cell>32</cell><cell>160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>CLIPEval corpus: Test data.</figDesc><table><row><cell>Event Class</cell><cell>POSITIVE</cell><cell>NEGATIVE</cell><cell>NEUTRAL</cell><cell>Tot. Instances</cell></row><row><cell>(FEAR OF) PHYSICAL PAIN</cell><cell>10</cell><cell>30</cell><cell>5</cell><cell>45</cell></row><row><cell>ATTENDING EVENT</cell><cell>29</cell><cell>5</cell><cell>11</cell><cell>45</cell></row><row><cell>COMMUNICATION ISSUE</cell><cell>8</cell><cell>29</cell><cell>7</cell><cell>44</cell></row><row><cell>GOING TO PLACES</cell><cell>22</cell><cell>23</cell><cell>3</cell><cell>48</cell></row><row><cell>LEGAL ISSUE</cell><cell>5</cell><cell>27</cell><cell>13</cell><cell>45</cell></row><row><cell>MONEY ISSUE</cell><cell>12</cell><cell>27</cell><cell>12</cell><cell>51</cell></row><row><cell>OUTDOOR ACTIVITY</cell><cell>34</cell><cell>4</cell><cell>8</cell><cell>46</cell></row><row><cell>PERSONAL CARE</cell><cell>24</cell><cell>10</cell><cell>13</cell><cell>43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 and</head><label>1</label><figDesc>Table 2 report the figures for polarity values per class in the training and in the test set, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation for Task A : polarity identification.</figDesc><table><row><cell>System</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-measure</cell></row><row><cell>SIGMA2320</cell><cell>0.41</cell><cell>0.42</cell><cell>0.38</cell></row><row><cell>SHELLFBK</cell><cell>0.56</cell><cell>0.56</cell><cell>0.54</cell></row><row><cell>baseline</cell><cell>0.17</cell><cell>0.42</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Evaluation for Task B : event instance and polarity identification.</figDesc><table><row><cell>System</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-measure</cell></row><row><cell>SHELLFBK</cell><cell>0.36</cell><cell>0.27</cell><cell>0.29</cell></row><row><cell>baseline 1</cell><cell>0.02</cell><cell>0.04</cell><cell>0.02</cell></row><row><cell>baseline 2</cell><cell>0.03</cell><cell>0.05</cell><cell>0.04</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>One of the author wants to thanks the NWO Spinoza Prize project Understanding Language by Machines (sub-track 3) for partially supporting this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihalcea</forename><surname>Rada</surname></persName>
		</author>
		<title level="m">Subjectivity Word Sense Disambiguation. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting implicit expressions of sentiment in text based on commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jesús</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</title>
				<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
	<note>Hermida and Andrés Montoyo</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings</title>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep. No. C-1</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SenticNet: A Publicly Available Semantic Resource for Opinion Mining</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI fall symposium: commonsense knowledge</title>
				<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SenticNet 2: A Semantic and Affective Resource for Opinion Mining and Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
				<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Benefactive/Malefactive Event and Writer Attitude Annotation</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connotation lexicon: A dash of sentiment beneath the surface meaning</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun Seok</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">More than words: Syntactic packaging and implicit sentiment</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of human language technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>human language technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="503" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Does emotion help or hinder immediate memory? Arousal versus priority-binding mechanism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">G</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some Relations between Pleasant and Unpleasant Events and Depression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Lewinsohn</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher S Amenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
				<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Pleasant Event Schedule: Studies on Reliability, Validity, and Scale Intercorrelation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Macphillamy</surname></persName>
		</author>
		<author>
			<persName><surname>Lewinsohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Counseling and Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="380" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contribution to verbal short-term memory: Are pleasant words easier to remember than neutral words in serial recall and serial recognition?</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Monnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arielle</forename><surname>Syssaumonnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
				<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are affective events richly recollected or simply familiar? The experience and process of recognizing feelings past</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName><surname>Ochsner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="261" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">FactBank: A corpus annotated with event factuality. Language resources and evaluation</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Osgood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Suci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Tannenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>University of Illinois Press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="268" />
		</imprint>
	</monogr>
	<note>The Measurement of Meaning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SemEval 2014 Task 4: Aspect Based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlopoulos</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (Se-mEval 2014)</title>
				<meeting>the 8th International Workshop on Semantic Evaluation (Se-mEval 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Androutsopoulos Ion and Manandhar Suresh</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Sentiment Analysis in Twitter. Proceedings of the 8th International Workshop on Semantic Evaluation</title>
				<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>SemEval-2014 Task</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">EMO-Cause: An Easy-adaptable Approach to Extract Emotion Cause Contexts</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ester</forename><surname>Boldrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricio</forename><surname>Barco Martínez</surname></persName>
		</author>
		<idno>WASSA 2.011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</title>
				<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="152" to="160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
