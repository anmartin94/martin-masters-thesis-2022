<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IJCNLP-2017 Task 5: Multi-choice Question Answering in Examinations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shangmin</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IJCNLP-2017 Task 5: Multi-choice Question Answering in Examinations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The IJCNLP-2017 Multi-choice Question Answering(MCQA) task aims at exploring the performance of current Question Answering(QA) techniques via the realworld complex questions collected from Chinese Senior High School Entrance Examination papers and CK12 website 1 . The questions are all 4-way multi-choice questions writing in Chinese and English respectively that cover a wide range of subjects, e.g. Biology, History, Life Science and etc. And, all questions are restrained within the elementary and middle school level. During the whole procedure of this task, 7 teams submitted 323 runs in total. This paper describes the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One critical but challenging problem in natural language understanding (NLU) is to develop a question answering(QA) system which could consistently understand and correctly answer general questions about the world. "Multi-choice Question Answering in Exams"(MCQA) is a typical question answering task that aims to test how accurately the participant QA systems could answer the questions in exams. All questions in this competition come from real examinations. We collected multiple choice questions from several curriculums, such as Biology, History, Life-Science, with a restrain that all questions are limited in the elementary and middle school level. For every question, four answer candidates are provided,  where each of them may be a word, a value, a phrase or even a sentence. The participant QA systems are required to select the best one from these four candidates. Fig 1 is an example. To answer these questions, participants could utilize any public toolkits and any resources on the Web, but manually annotation is not permitted.</p><p>As for the knowledge resources, we encourage participants to utilize any resource on Internet, including softwares, toolboxes, and all kinds of corpora. Meanwhile, we also provide a dump of Wikipedia 2 and a collection of related Baidu Baike Corpus 3 under a specific license. These corpora and released questions are all provided in the XML format, which will be explained in section 2.2.</p><p>Main characteristics of our task are as follow:</p><p>• All the questions are from real word examinations.</p><p>• Most of questions require considerable inference ability.</p><p>• Some questions require a deep understanding of context.</p><p>• Questions from different categories have different characteristics, which makes it harder for a model to have a good performance on all kinds of questions.</p><p>• It concentrates only on the textual content, as questions with figures and tables are all filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Data Description</head><p>All questions in MCQA consist of 2 parts, a question and 4 answer candidates, without any figure or table. The participant systems are required to select the only right one from all candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Languages and Subjects</head><p>In order to explore the influence of diversity of questions, we collect questions from seven subjects in two languages, including an English subset and a Chinese subset. The subjects of English subset contain biology, chemistry, physics, earth science and life science. And the subjects of Chinese subset only contain biology and history. The total number of questions is 14,447.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Format</head><p>All questions in our dataset are consisted by the following 7 parts:</p><p>1. ID, i.e. the identical number of a specific question;</p><p>2. Question, i.e. the question to be answered; Take a question in Figure <ref type="figure" target="#fig_1">1</ref> for example. Roles of every part are as follow:</p><p>1. ID: wb415; 2. Question: "Peach trees have sweet-smelling blossoms and produce rich fruit. What is the main purpose of the flowers of a peach tree?"; 3. Option A: "to attract bees for pollination."; 4. Option B: "to create flower arrangements."; 5. Option C: "to protect the tree from disease."; 6. Option D: "to feed migratory birds."; 7. Correct Answer No.: 0.</p><p>It needs to be specified that we exclude the Correct Answer No. in the validation and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Size</head><p>The dataset totally contains 14,447 multiple choice questions. In detail, English subset contains 5,367 questions and Chinese subset contains 9,080 questions. We randomly split the dataset into Train, Validation and Test sets. And more detail statistics is showed in Table <ref type="table" target="#tab_2">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">English Subset</head><p>We collected all the downloadable quiz from CK12 and only reserved 5367 4-way multi-choice questions with their tags which are also the basis of classifying the questions. For every subject, we randomly separate questions into 3 parts, train set, valid set and test set with 50%, 12.5% and 37.5% questions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Chinese Subset</head><p>As questions in Senior High School Entrance Examination(SHSEE) differs among different cities, we collected questions in SHSEE from as many cities as we can. After filtering out the questions containing more information than textual content, the answers of left questions were labeled by human. Finally, we got 4,531 questions in Biology and 4,549 questions in History. For every subject, we randomly separate questions into 3 parts, train  set, valid set and test set with same ratio stated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Evaluation</head><p>This challenge employs the accuracy of a method on answering questions in test set as the metric, the accuracy is calculated as follow.</p><formula xml:id="formula_0">Accuracy = n correct N total × 100%</formula><p>where n correct is the number of correctly answered questions and N total is the total number of all questions.</p><p>To automatically evaluate the performance of QA systems, we built a web-site for participants to submit solutions for valid and test data set and get accuracy immediately on the page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Baseline</head><p>We employ a simple retrieval based method as a baseline, and it is implemented based on Lucene <ref type="bibr">4</ref> which is an open-source information retrieval software library. We employ the method to build reverse-index on the whole Wikipedia dump 5 for English questions and on the Baidu Baike corpus 6 for Chinese questions.</p><p>This method scores pairs of the question and each of its option, the detail steps are shown as follows.</p><p>• concatenate a question with an option as the query;</p><p>• use Lucene to search relevant documents with the query;</p><p>• score relevant documents by the similarity between the query q and the document d, noted as Sim(q, d);</p><p>• choose at most three highest scores to calculate the score of the pair of the question and the option as</p><formula xml:id="formula_1">score(q, a) = 1 n n 1 Sim(q, d)</formula><p>where n 3 and if n = 0, score(q, a) = 0;</p><p>All questions and options are preprocessed by Stanford CoreNLP 7 . The detail result of the baseline on the validation set is shown in Table <ref type="table" target="#tab_4">2</ref>.  The details of participation of different language subsets are listed in the following Table <ref type="table" target="#tab_8">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submission</head><p>In order to avoid the situation that participants submit different permutation of answers to sniff the correct answer labels, we limited the times that a team can submit their solutions. Before the release of test set, a team can submit no more than 5 solutions for valid set in 24 hours. After the release of test set, a team can submit as many as 30 solutions   <ref type="bibr">Aug. 31, 2017)</ref> for valid set per 24 hours, but no more than 5 solutions for test set in 24 hours. Finally, we got 323 runs in total, in which there are 219 runs for valid set and 104 runs for test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In our evaluation system, only the best performance of participants were reserved. The detail results of every subset is listed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">All Questions</head><p>There is only one team, "YNU-HPCC", that took the challenge of both English subset and Chinese subset. And, the performance of their system is listed in Table <ref type="table" target="#tab_10">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English Subset</head><p>Totally, there are 5 teams that only took the challenge of English subset and details of their performance are listed in the Table <ref type="table" target="#tab_11">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chinese Subset</head><p>There are 1 team that only took the challenge of Chinese subset and their performance is listed in the Table <ref type="table" target="#tab_12">7</ref>.</p><p>6 Overview of Participant Systems 6.1 YNU-HPCC, An Attetion-based LSTM YNU-HPCC <ref type="bibr" target="#b6">(Yuan et al., 2017)</ref> proposed an attention-based LSTM(AT-LSTM) model for MCQA. According to them, this model can easily capture long contextual information with the help of an attention mechanism. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, LSTM layer takes the vector representions of question and answers as input and then calculates out the hidden vectors which are the input of attention layer to calculate the weight vector α and weighted hidden representation r.</p><p>Finally, an softmax layer takes r as input to select the right answer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CASIA-NLP, Internet Resources and Localization Method</head><p>Based on the phenomenon that many web pages containing answers of the questions in MCQA, CASIA-NLP <ref type="bibr" target="#b3">(Li and Kong, 2017</ref>) crawled on Internet and analyzed the content in these pages. When analyzing these pages, they use a localization method to locate the positions of sentences that have same meaning of questions in MCQA by merging a score given by edit distance that evaluates the structural similarity and a cosine score given by a CNN network that evaluates the semantic similarity. Finally, the system can analyze answers to find out the right one. The overview of the system is illustrated in Figure <ref type="figure">3</ref> and the CNN network they used is demonstrated in Figure <ref type="figure">4</ref>.</p><p>Figure <ref type="figure">3</ref>: Overview of CAISA-NLP's system <ref type="bibr" target="#b3">(Li and Kong, 2017)</ref>. Communication between modules is indicated by arrows.     <ref type="bibr" target="#b3">(Li and Kong, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cone, Wikipedia and Logistic Regression</head><p>The system of Cone <ref type="bibr" target="#b0">(Dzendzik et al., 2017)</ref>, a team from ADAPT Centre, based on a logistic regression over the string similarities between question, answer, and additional text. Their model is constructed as a four-step pipeline as follow.</p><p>1. Preprocessing cleaning of the input data;</p><p>2. Data selection relative sentences are extracted from Wikipedia based on key words from question;</p><p>3. Feature Vector Concatenation for every question, a feature vector is built as a concatenation of similarities between the answer candidates and sentences obtained in the previous step;</p><p>4. Logistic Regression a logistic regression over the feature vector.</p><p>The features they employed includes term frequencyinverse document frequency (Tf-IDf) metric, character n-grams (with n ranging from 1 to 4), bag of words,and windows slide (a ratio between answer and substrings of extracted data). While their model is trained in two ways, combining training over all domains and separate model training from each domain, the later one got the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">G623, A CNN-LSTM Model with Attention Mechanism</head><p>Figure <ref type="figure">5</ref>: Architecture of the model proposed by G623 <ref type="bibr" target="#b4">(Min et al., 2017)</ref>.</p><p>The system of G623 <ref type="bibr" target="#b4">(Min et al., 2017)</ref> combined CNN with LSTM network and took into account the attention mechanism. Fistly , question and answer pairs are fed into a CNN network and produce joint representations of these pairs which are then fed into a LSTM network. The two separate vector representations of question and answer are then calculated to generate the weight vector by dot multiplication. Finally, a softmax layer is applied to classify the join representations with the help of attention weight. The diagram of their system is illustrated in Figure <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">JU NITM, Complex Decision Tree</head><p>To handle the questions in MCQA, JU NITM <ref type="bibr" target="#b5">(Sarkar et al., 2017)</ref> built a complex decision tree classifier using word embedding features to predict the right answer. The overview of the whole system is demonstrated in Figure <ref type="figure">6</ref>. In distributed semantic similarity module, they trained a word embedding dictionary containing 3 million words in 300-dimensional space on GoogleNews. Then, a complex decision tree is used to select the right answer in step2, classification.</p><p>Figure <ref type="figure">6</ref>: System Framework proposed by JU NITM <ref type="bibr" target="#b5">(Sarkar et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">TALN, MappSent</head><p>Mappsent is proposed in a previous work of TALN, . To adapt to the characteristics of MCQA, they retrofitted MappSent model in two different ways <ref type="bibr" target="#b1">(Hazem, 2017)</ref>. The first approach illustrated in Figure <ref type="figure">7</ref> is to follow the same procedure as the question-to-question similarity task, i.e. using anatated pairs of questions and their corresponding answers to build the mapping matrix. The second approach illustrated in Figure <ref type="figure">8</ref> tends to keep the strong hypothesis of sentence pairs similarity. They built two mapping matrices, one that represent similar question pairs and ther other one to represent similar answers pairs. For a give test question, the system can extracted the most similar quesiont in the training data and select the candidate with highest similarity score as correct answer.</p><p>Figure <ref type="figure">7</ref>: Fist adaptation of MappSent <ref type="bibr" target="#b1">(Hazem, 2017)</ref>.</p><p>Figure <ref type="figure">8</ref>: Second adaptation of MappSent <ref type="bibr" target="#b1">(Hazem, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We described the overview of the Multi-choice Question Answering task. The goal is exploring the performance of current Question Answering(QA) techniques via the real-world complex questions collected from Chinese Senior High School Entrance Examination(SHSEE) papers and CK12 website. We collected 14,447 questions covering 2 language in 7 different subjects. 7 teams submitted 323 runs in total. We describe the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 http://www.ck12.org/browse/ Peach trees have sweet-smelling blossoms and produce rich fruit. What is the main purpose of the flowers of a peach tree? (Answer is A.) (A) to attract bees for pollination. (B) to create flower arrangements. (C) to protect the tree from disease. (D) to feed migratory birds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example question from English Subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of AT-LSTM proposed by team YNU-HPCC(Yuan et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The statistics of dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The detail performance of the baseline method.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Active Participating Teams (as ofAug.  31, 2017)    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Language Selection of Teams (as of</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance of YNU-HPCC (as ofAug. 31, 2017)    </figDesc><table><row><cell cols="3">Team Name Valid Set Test Set</cell></row><row><cell>Cone</cell><cell>48.7%</cell><cell>45.6%</cell></row><row><cell>G623</cell><cell>42.8%</cell><cell>42.2%</cell></row><row><cell>JU NITM</cell><cell>40.7%</cell><cell>40.6%</cell></row><row><cell>TALN</cell><cell>34.7%</cell><cell>30.3%</cell></row><row><cell>QA challenge</cell><cell>21.5%</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">: Performance on English Subset (as of</cell></row><row><cell>Aug. 31, 2017)</cell><cell></cell><cell></cell></row><row><cell cols="3">Team Name Valid Set Test Set</cell></row><row><cell>CASIA-NLP</cell><cell>60.1%</cell><cell>58.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Performance on English Subset (as of</cell></row><row><cell>Aug. 31, 2017)</cell></row><row><cell>Figure 4: Convolutional architecture used in</cell></row><row><cell>CASIA-NLP's system</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.wikipedia.org/ 3 https://baike.baidu.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://lucene.apache.org/ 5 https://dumps.wikimedia.org/ 6 http://www.nlpr.ia.ac.cn/cip/ijcnlp/baidubaike corpus.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://stanfordnlp.github.io/CoreNLP/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Our thanks to participants. This task organization was supported by the Natural Science Foundation of China (No.61533018) and the National Basic Research Program of China (No.  2014CB340503). And this research work was also supported by Google through focused research awards program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A similarity-based logistic regression approach to multi-choice question answering in an examinations shared task</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Dzendzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Poncelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-2017, Shared Task 5</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A textual similarity approach applied to multi-choice question answering in examinations shared task</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hazem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-2017, Shared Task 5</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mappsent: a textual mapping ap-proach for question-toquestion similarity</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hazem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niclas</forename><surname>Boussaha</surname></persName>
		</author>
		<author>
			<persName><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP</title>
				<meeting>the International Conference Recent Advances in Natural Language Processing RANLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Answer localization for multi-choice question answering in exams</title>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunliang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-2017</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Shared Task 5</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A cnn-lstm model with attention for multi-choice question answering in examinations</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-2017, Shared Task 5</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ju nitm: A classification approach for answer selection in multi-choice question answering system</title>
		<author>
			<persName><forename type="first">Sandip</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<idno>IJCNLP-2017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Shared Task 5</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using an attention-based lstm for multichoice question answering in exams</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-2017</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Shared Task 5</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
