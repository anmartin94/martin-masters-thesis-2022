<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemEval-2013 Task 2: Sentiment Analysis in Twitter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
							<email>pnakov@qf.org.qa</email>
						</author>
						<author>
							<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
							<email>kozareva@isi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
							<email>aritter@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SemEval-2013 Task 2: Sentiment Analysis in Twitter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, sentiment analysis in social media has attracted a lot of research interest and has been used for a number of applications. Unfortunately, research has been hindered by the lack of suitable datasets, complicating the comparison between approaches. To address this issue, we have proposed SemEval-2013 Task 2: Sentiment Analysis in Twitter, which included two subtasks: A, an expression-level subtask, and B, a messagelevel subtask. We used crowdsourcing on Amazon Mechanical Turk to label a large Twitter training dataset along with additional test sets of Twitter and SMS messages for both subtasks. All datasets used in the evaluation are released to the research community. The task attracted significant interest and a total of 149 submissions from 44 teams. The bestperforming team achieved an F1 of 88.9% and 69% for subtasks A and B, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decade, new forms of communication, such as microblogging and text messaging have emerged and become ubiquitous. Twitter messages (tweets) and cell phone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.</p><p>Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.</p><p>Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. <ref type="bibr">1</ref> How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research <ref type="bibr" target="#b5">(Jansen et al., 2009;</ref><ref type="bibr" target="#b1">Barbosa and Feng, 2010;</ref><ref type="bibr" target="#b2">Bifet et al., 2011;</ref><ref type="bibr" target="#b4">Davidov et al., 2010;</ref><ref type="bibr">O'Connor et al., 2010;</ref><ref type="bibr" target="#b8">Pak and Paroubek, 2010;</ref><ref type="bibr" target="#b11">Tumasjan et al., 2010;</ref><ref type="bibr" target="#b6">Kouloumpis et al., 2011)</ref>.</p><p>Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties of persuasive language or those associated with influential users.</p><p>Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA corpus <ref type="bibr" target="#b13">(Wiebe et al., 2005)</ref> of newswire text. These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they did not focus on social media.</p><p>Twitter RT @tash jade: That's really sad, Charlie RT "Until tonight I never realised how fucked up I was" -Charlie Sheen #sheenroast SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go? While some Twitter sentiment datasets have already been created, they were either small and proprietary, such as the i-sieve corpus <ref type="bibr" target="#b6">(Kouloumpis et al., 2011)</ref>, or they were created only for Spanish like the TASS corpus 2 <ref type="bibr" target="#b12">(Villena-Rom√°n et al., 2013)</ref>, or they relied on noisy labels obtained from emoticons and hashtags. They further focused on message-level sentiment, and no Twitter or SMS corpus with expression-level sentiment annotations has been made available so far.</p><p>Thus, the primary goal of our SemEval-2013 task 2 has been to promote research that will lead to a better understanding of how sentiment is conveyed in Tweets and SMS messages. Toward that goal, we created the SemEval Tweet corpus, which contains Tweets (for both training and testing) and SMS messages (for testing only) with sentiment expressions annotated with contextual phrase-level polarity as well as an overall message-level polarity. We used this corpus as a testbed for the system evaluation at SemEval-2013 Task 2.</p><p>In the remainder of this paper, we first describe the task, the dataset creation process, and the evaluation methodology. We then summarize the characteristics of the approaches taken by the participating systems and we discuss their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>We had two subtasks: an expression-level subtask and a message-level subtask. Participants could choose to participate in either or both subtasks. Below we provide short descriptions of the objectives of these two subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask A: Contextual Polarity Disambiguation</head><p>Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context. The boundaries for the marked instance were provided: this was a classification task, not an entity recognition task.</p><p>2 http://www.daedalus.es/TASS/corpus.php</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask B: Message Polarity Classification</head><p>Given a message, decide whether it is of positive, negative, or neutral sentiment. For messages conveying both a positive and a negative sentiment, whichever is the stronger one was to be chosen.</p><p>Each participating team was allowed to submit results for two different systems per subtask: one constrained, and one unconstrained. A constrained system could only use the provided data for training, but it could also use other resources such as lexicons obtained elsewhere. An unconstrained system could use any additional data as part of the training process; this could be done in a supervised, semisupervised, or unsupervised fashion.</p><p>Note that constrained/unconstrained refers to the data used to train a classifier. For example, if other data (excluding the test data) was used to develop a sentiment lexicon, and the lexicon was used to generate features, the system would still be constrained. However, if other data (excluding the test data) was used to develop a sentiment lexicon, and this lexicon was used to automatically label additional Tweet/SMS messages and then used with the original data to train the classifier, then such a system would be unconstrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Creation</head><p>In the following sections we describe the collection and annotation of the Twitter and SMS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Twitter is the most common micro-blogging site on the Web, and we used it to gather tweets that express sentiment about popular topics. We first extracted named entities using a Twitter-tuned NER system <ref type="bibr" target="#b9">(Ritter et al., 2011</ref>) from millions of tweets, which we collected over a one-year period spanning from January 2012 to January 2013; we used the public streaming Twitter API to download tweets.</p><p>Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark the position of its start and end in the text boxes below. The number above each word indicates its position. The word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range. Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a sentence is not subjective please select the checkbox indicating that "There are no subjective words/phrases". Please read the examples and invalid responses before beginning if this is your first time answering this hit.</p><p>Figure <ref type="figure">1</ref>: Instructions provided to workers on Mechanical Turk followed by a screenshot.  We then identified popular topics as those named entities that are frequently mentioned in association with a specific date <ref type="bibr" target="#b10">(Ritter et al., 2012)</ref>. Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from training and spanned later periods.</p><p>To identify messages that express sentiment towards these topics, we filtered the tweets using SentiWordNet <ref type="bibr" target="#b0">(Baccianella et al., 2010)</ref>. We removed messages that contained no sentimentbearing words, keeping only those with at least one word with positive or negative sentiment score that is greater than 0.3 in SentiWordNet for at least one sense of the words. Without filtering, we found class imbalance to be too high. <ref type="bibr">3</ref> Twitter messages are rich in social media features, including out-of-vocabulary (OOV) words, emoticons, and acronyms; see Table <ref type="table" target="#tab_0">1</ref>. A large portion of the OOV words are hashtags (e.g., #sheenroast) and mentions (e.g., @tash jade).  We annotated the same Twitter messages with annotations for subtask A and subtask B. However, the final training and testing datasets overlap only partially between the two subtasks since we had to throw away messages with low inter-annotator agreement, and this differed between the subtasks. For testing, we also annotated SMS messages, taken from the NUS SMS corpus 4 <ref type="bibr" target="#b3">(Chen and Kan, 2012)</ref>. Tables <ref type="table" target="#tab_2">2 and 3</ref> show statistics about the corpora we created for subtasks A and B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation Guidelines</head><p>The instructions provided to the annotators, along with an example, are shown in Figure <ref type="figure">1</ref>. We provided several additional examples to the annotators, shown in Table <ref type="table">5</ref>.</p><p>In addition, we filtered spammers by considering the following kinds of annotations invalid:</p><p>‚Ä¢ containing overlapping subjective phrases;</p><p>‚Ä¢ subjective but without a subjective phrase;</p><p>‚Ä¢ marking every single word as subjective;</p><p>‚Ä¢ not having the overall sentiment marked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Process</head><p>Our datasets were annotated for sentiment on Mechanical Turk. Each sentence was annotated by five Mechanical Turk workers (Turkers). In order to qualify for the hits, the Turker had to have an approval rate greater than 95% and have completed 50 approved hits. Each Turker was paid three cents per hit. The Turker had to mark all the subjective words/phrases in the sentence by indicating their start and end positions and say whether each subjective word/phrase was positive, negative, or neutral (subtask A). They also had to indicate the overall polarity of the sentence (subtask B).</p><p>Figure <ref type="figure">1</ref> shows the instructions and an example provided to the Turkers. The first five rows of Table <ref type="table">6</ref> show an example of the subjective words/phrases marked by each of the workers.</p><p>For subtask A, we combined the annotations of each of the workers using intersection as indicated in the last row of Table <ref type="table">6</ref>. A word had to appear in 2/3 of the annotations in order to be considered subjective. Similarly, a word had to be labeled with a particular polarity (positive, negative, or neutral) 2/3 of the time in order to receive that label.</p><p>We also experimented with combining annotations by computing the union of the sentences, and taking the sentence of the worker who annotated the most hits, but we found that these methods were not as accurate. Table <ref type="table">4</ref> shows the lower, average, and upper bounds for all the hits by computing the bounds for each hit and averaging them together. This gives a good indication about how well we can expect the systems to perform. For example, even if we used the best annotator each time, it would still not be possible to get perfect accuracy.</p><p>For subtask B, the polarity of the entire sentence was determined based on the majority of the labels. If there was a tie, the sentence was discarded. In order to reduce the number of sentences lost, we combined the objective and the neutral labels, which Turkers tended to mix up. Table <ref type="table">4</ref> shows the average bound for subtask B by computing the bounds for each hit and averaging them together. Since the polarity is chosen based on the majority, the upper bound is 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scoring</head><p>For both subtasks, the participating systems were required to perform a three-way classification -a particular marked phrase (for subtask A) or an entire message (for subtask B) was to be classified as positive, negative, or objective. For each system, we computed a score for predicting positive/negative phrases/messages vs. the other two classes.</p><p>For instance, to compute positive precision, P pos , we find the number of phrases/messages that a system correctly predicted to be positive, and we divide that number by the total number of messages it predicted to be positive. To compute recall, for the positive class, R pos , we find the number of messages correctly predicted to be positive and we divide that number by the total number of positive messages in the gold standard.</p><p>We then calculate F-score for the positive labels, the harmonic average of precision and recall as follows F pos = 2 PposRpos Ppos+Rpos . We carry out a similar computation to calculate F neg , which is F1 for negative messages.</p><p>The overall score for each system run is then given by the average of the F1-scores for the positive and negative classes: F = (F pos + F neg )/2.</p><p>Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the frontiers. Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas markets and lower costs for material imports, he said. "March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out," according to Chen, also Taiwan's chief WTO negotiator. friday evening plans were great, but saturday's plans didnt go as expected -i went dancing &amp; it was an ok club, but terribly crowded :-( WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE AT&amp;T was okay but whenever they do something nice in the name of customer service it seems like a favor, while T-Mobile makes that a normal everyday thin obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies what we had. #Coward #Traitor My graduation speech: "I'd like to thanks Google, Wikipedia and my computer! :D #iThingteens Table <ref type="table">5</ref>: List of example sentences with annotations that were provided to the annotators. All subjective phrases are italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worker 1</head><p>I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13 Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13 Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13 Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13 Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13 Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination Table <ref type="table">6</ref>: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.</p><p>Note that ignoring F neutral does not reduce the task to predicting positive vs. negative labels only (even though some participants have chosen to do so) since the gold standard still contains neutral labels which are to be predicted: F pos and F neg would suffer if these examples are labeled as positive and/or negative instead of neutral.</p><p>We provided participants with a scorer. In addition to outputting the overall F-score, it produced a confusion matrix for the three prediction classes (positive, negative, and objective), and it also validated the data submission format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants and Results</head><p>The results for subtask A are shown in Tables <ref type="table" target="#tab_7">7 and  8</ref> for Twitter and for SMS messages, respectively; those for subtask B are shown in Table <ref type="table">9</ref> for Twitter and in Table <ref type="table" target="#tab_0">10</ref> for SMS messages. Systems are ranked by their scores for the constrained runs; the ranking based on scores for unconstrained runs is shown as a subindex.</p><p>For both subtasks, there were teams that only submitted results for the Twitter test set. Some teams submitted both a constrained and an unconstrained version (e.g., AVAYA and teragram). As one would expect, the results on the Twitter test set tended to be better than those on the SMS test set since the SMS data was out-of-domain with respect to the training (Twitter) data.</p><p>Moreover, the results for subtask A were significantly better than those for subtask B, which shows that it is a much easier task, probably because there is less ambiguity at the phrase-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Subtask A: Contextual Polarity</head><p>Table <ref type="table" target="#tab_7">7</ref> shows that subtask A, Twitter, attracted 23 teams, who submitted 21 constrained and 7 unconstrained systems. Five teams submitted both a constrained and an unconstrained system, and two other teams submitted constrained systems that are on the boundary between being constrained and unconstrained.  ‚Ä¢ marks a team that includes a task coorganizer, and the indicates a system submitted as constrained but which used additional Tweets or additional sentiment-annotated text to collect statistics that were then used as a feature.</p><p>One system was semi-supervised, and the rest were supervised. The supervised systems used classifiers such as SVM (8 systems), Naive Bayes (7 systems), and Maximum Entropy (3 systems). Other approaches used include an ensemble of classifiers, manual rules, and a linear classifier. Two of the systems chose not to predict neutral as a possible classification label.</p><p>The average F1-measure on the Twitter test set was 74.1% for constrained systems and 60.5% for unconstrained ones; this does not mean that using additional data does not help, it just shows that the best teams only participated with a constrained system. NRC-Canada had the best constrained system with an F1-measure of 88.9%, and AVAYA had the best unconstrained one with F1=87.4%.  <ref type="table">8</ref>: Results for subtask A on the SMS dataset. The indicates a late submission, the ‚Ä¢ marks a team that includes a task co-organizer, and the indicates a system submitted as constrained but which used additional Tweets or additional sentiment-annotated text to collect statistics that were then used as a feature.</p><p>Table <ref type="table">8</ref> shows the results for the SMS test set, where 20 teams submitted 19 constrained and 7 unconstrained systems (again, this included two teams that submitted boundary systems, marked accordingly). The average F-measure on this test set was 70.8% for constrained systems and 65.7% for unconstrained systems. The best constrained system was that of GU-MLT-LT with an F-measure of 88.4%, and AVAYA had the best unconstrained system with an F1 of 85.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subtask B: Message Polarity</head><p>Table <ref type="table">9</ref> shows that subtask B, Twitter, attracted 38 teams, who submitted 36 constrained and 15 unconstrained systems (and two boundary ones).</p><p>The average F1-measure was 53.7% for the constrained and 54.6% for the unconstrained systems.  <ref type="table">9</ref>: Results for subtask B on the Twitter dataset. The indicates a system submitted as constrained but which used additional Tweets or additional sentiment-annotated text to collect statistics that were then used as a feature.</p><p>These averages are much lower than those for subtask A, which indicates that subtask B is harder, probably because a message can contain parts expressing both positive and negative sentiment. Once again, NRC-Canada had the best constrained system with an F1-measure of 69%, followed by teragram, which had the best unconstrained system with an F1-measure of 64.9%.</p><p>As Table <ref type="table" target="#tab_0">10</ref> shows, the average F1-measure on the SMS test set was 50.2% for constrained and 50.3% for unconstrained systems. NRC-Canada had the best constrained system with an F1=68.5%, and AVAYA had the best unconstrained one with F1measure of 59.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall</head><p>Overall, the results achieved by the best teams were very strong, especially for the simpler subtask A:</p><p>‚Ä¢ F1=88.93, NRC-Canada on subtask A, Twitter;</p><p>‚Ä¢ F1=88.37, GU-MLT-LT on subtask A, SMS;</p><p>‚Ä¢ F1=69.02, NRC-Canada on subtask B, Twitter;</p><p>‚Ä¢ F1=68.46, NRC-Canada on subtask B, SMS.</p><p>We can see that the strongest team overall was that of NRC-Canada, which was ranked first on three of the four conditions; and it was second on subtask A, SMS. There were two other teams that were strong across both tasks and on both test sets: GU-MLT-LT and AVAYA. Three other teams, namely teragram, BOUNCE and KLUE, were ranked in the top-3 in at least one subtask and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have seen that most participants restricted themselves to the provided data and submitted constrained systems. Indeed, the best systems for each of the two subtasks and for each of the two testing datasets were constrained systems; of course, this does not mean that additional data would not be useful. Curiously, in some cases where a team submitted a constrained and unconstrained run, the unconstrained run actually performed worse.</p><p>Not surprisingly, most systems were supervised; there were only five semi-supervised systems, and there was only one unsupervised system. One additional team declared their system as unsupervised since it was not making use of the training data; we still classified it as supervised though since it did use supervision -in the form of manual rules.</p><p>Most participants predicted all three labels (positive, negative and neutral), even though some participants opted for not predicting neutral, which made some sense since the final F1-score was averaged over the positive and the negative predictions only.</p><p>The most popular classifiers included SVM, Max-Ent, linear classifier, Naive Bayes; in some cases, manual rules or ensembles of classifiers were used.</p><p>A variety of features were used, including wordrelated (e.g., words, stems, n-grams, word clusters), word-shape (e.g., punctuation, capitalization), syntactic (e.g., POS tags, dependency relations), Twitter-specific (e.g., repeated characters, emoticons, URLs, hashtags, slang, abbreviations), and sentiment-related (e.g., negation); one team also used discourse relations. Almost all participants relied heavily of various sentiment lexicons, the most popular ones being MPQA and SentiWordNet, as well as AFINN and Bing Liu's Opinion Lexicon; some participants used their own lexicons -preexisting or built from the provided data.</p><p>Given that Twitter messages are noisy, most participants did some preprocessing, including tokenization, stemming, lemmatization, stopword removal, normalization/removal of URLs, hashtags, users, slang, emoticons, repeated vowels, punctuation; some even did pronoun resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have described a new task that entered SemEval-2013: task 2 on Sentiment Analysis on Twitter. The task has attracted a very high number of participants: 149 submissions from 44 teams.</p><p>We believe that the datasets that we have created as part of the task and which we have released to the community 5 under a Creative Commons Attribution 3.0 Unported License, 6 will be found useful by researchers beyond SemEval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of sentences from each corpus that contain subjective phrases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics for Subtask A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Statistics for Subtask B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Results for subtask A on the Twitter dataset. The</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Hashtags are a type of tagging for Twitter messages.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Filtering based on an existing lexicon does bias the dataset to some degree; however, note that the text still contains sentiment expressions outside those in the lexicon.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://wing.comp.nus.edu.sg/SMSCorpus/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.cs.york.ac.uk/semeval-2013/task2/ 6 http://creativecommons.org/licenses/by/3.0/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Kathleen McKeown for her insight in creating the Amazon Mechanical Turk annotation task.</p><p>Funding for the Amazon Mechanical Turk annotations was provided by the JHU Human Language Technology Center of Excellence and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation, LREC &apos;10</title>
				<meeting>the Seventh International Conference on Language Resources and Evaluation, LREC &apos;10<address><addrLine>Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias; Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2200" to="2204" />
		</imprint>
	</monogr>
	<note>Nicoletta Calzolari (chair)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust sentiment detection on Twitter from biased and noisy data</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10</title>
				<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting sentiment change in Twitter streaming data</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricard</forename><surname>Gavald√†</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5" to="11" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Creating a live, public short message service corpus: the NUS SMS corpus. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised recognition of sarcastic sentences in Twitter and Amazon</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL &apos;10</title>
				<meeting>the Fourteenth Conference on Computational Natural Language Learning, CoNLL &apos;10<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twitter power: Tweets as electronic word of mouth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimi</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdur</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2169" to="2188" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twitter sentiment analysis: The Good the Bad and the OMG! In</title>
		<author>
			<persName><forename type="first">Efthymios</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM&apos; 11</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
			<persName><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
			<persName><forename type="first">Scott</forename><surname>Baeza-Yates</surname></persName>
			<persName><surname>Counts</surname></persName>
		</editor>
		<meeting>the Fifth International Conference on Weblogs and Social Media, ICWSM&apos; 11<address><addrLine>Barcelona, Spain. Brendan O&apos;Connor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="538" to="541" />
		</imprint>
	</monogr>
	<note>Ramnath Balasubramanyan</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From tweets to polls: Linking text sentiment to public opinion time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM &apos;10</title>
				<editor>
			<persName><forename type="first">W</forename><surname>William</surname></persName>
			<persName><forename type="first">Samuel</forename><surname>Cohen</surname></persName>
			<persName><surname>Gosling</surname></persName>
		</editor>
		<meeting>the Fourth International Conference on Weblogs and Social Media, ICWSM &apos;10<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twitter based system: Using Twitter for disambiguating sentiment ambiguous adjectives</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation, Se-mEval &apos;10</title>
				<meeting>the 5th International Workshop on Semantic Evaluation, Se-mEval &apos;10<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="436" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open domain event extraction from twitter</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;12</title>
				<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;12<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1104" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting elections with Twitter: What 140 characters reveal about political sentiment</title>
		<author>
			<persName><forename type="first">Andranik</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><forename type="middle">G</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabell</forename><forename type="middle">M</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM &apos;10</title>
				<editor>
			<persName><forename type="first">W</forename><surname>William</surname></persName>
			<persName><forename type="first">Samuel</forename><surname>Cohen</surname></persName>
			<persName><surname>Gosling</surname></persName>
		</editor>
		<meeting>the Fourth International Conference on Weblogs and Social Media, ICWSM &apos;10<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Julio</forename><surname>Villena-Rom√°n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Mart√≠nez-C√°mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos√© Carlos Gonz√°lez</forename><surname>Crist√≥bal</surname></persName>
		</author>
		<title level="m">TASS -Workshop on Sentiment Analysis at SEPLN. Procesamiento del Lenguaje Natural</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language</title>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="165" to="210" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
