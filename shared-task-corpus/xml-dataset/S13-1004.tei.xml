<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">*SEM 2013 shared task: Semantic Textual Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<email>e.agirre@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="institution">University of the Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
							<email>danielcer@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
							<email>mtdiab@gwu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Washington University</orgName>
								<address>
									<settlement>George</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of the Basque Country</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
							<email>weiwei@cs.columbia.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">*SEM 2013 shared task: Semantic Textual Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED). CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. Several types of similarity have been defined, including similar author, similar time period or similar location. The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%. The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given two snippets of text, Semantic Textual Similarity (STS) captures the notion that some texts are more similar than others, measuring the degree of semantic equivalence. Textual similarity can range from exact semantic equivalence to complete unrelatedness, corresponding to quantified values between 5 and 0. The graded similarity intuitively captures the notion of intermediate shades of similarity such as pairs of text differ only in some minor nuanced aspects of meaning only, to relatively important differences in meaning, to sharing only some details, or to simply being related to the same topic, as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>One of the goals of the STS task is to create a unified framework for combining several semantic components that otherwise have historically tended to be evaluated independently and without characterization of impact on NLP applications. By providing such a framework, STS will allow for an extrinsic evaluation for these modules. Moreover, this STS framework itself could in turn be evaluated intrinsically and extrinsically as a grey/black box within various NLP applications such as Machine Translation (MT), Summarization, Generation, Question Answering (QA), etc.</p><p>STS is related to both Textual Entailment (TE) and Paraphrasing, but differs in a number of ways and it is more directly applicable to a number of NLP tasks. STS is different from TE inasmuch as it assumes bidirectional graded equivalence between the pair of textual snippets. In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car. STS also differs from both TE and Paraphrasing (in as far as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc.</p><p>• (5) The two sentences are completely equivalent, as they mean the same thing.</p><p>The bird is bathing in the sink. Birdie is washing itself in the water basin. • (4) The two sentences are mostly equivalent, but some unimportant details differ.</p><p>In May 2010, the troops attempted to invade Kabul.</p><p>The US army invaded Kabul on May 7th last year, 2010. • (3) The two sentences are roughly equivalent, but some important information differs/missing.</p><p>John said he is considered a witness but not a suspect. "He is not a suspect anymore." John said. • (2) The two sentences are not equivalent, but share some details.</p><p>They flew out of the nest in groups.</p><p>They flew into the nest together. • (1) The two sentences are not equivalent, but are on the same topic.</p><p>The woman is playing the violin.</p><p>The young lady enjoys listening to the guitar. • (0) The two sentences are on different topics.</p><p>John went horse back riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs <ref type="bibr" target="#b5">(Agirre et al., 2012)</ref>. In addition, we held a DARPA sponsored workshop at Columbia University 1 . In 2013, STS was selected as the official Shared Task of the *SEM 2013 conference. Accordingly, in STS 2013, we set up two tasks: The core task CORE, which is similar to the 2012 task; and a pilot task on typed-similarity TYPED between semi-structured records.</p><p>For CORE, we provided all the STS 2012 data as training data, and the test data was drawn from related but different datasets. This is in contrast to the STS 2012 task where the train/test data were drawn from the same datasets. The 2012 datasets comprised the following: pairs of sentences from paraphrase datasets from news and video elicitation (MSRpar and MSRvid), machine translation evaluation data (SMTeuroparl, SMTnews) and pairs of glosses (OnWN). The current STS 2013 dataset comprises the following: pairs of news headlines, SMT evaluation sentences (SMT) and pairs of glosses (OnWN and FNWN).</p><p>The typed-similarity pilot task TYPED attempts to characterize, for the first time, the reason and/or type of similarity. STS reduces the problem of judging similarity to a single number, but, in some applications, it is important to characterize why and how two items are deemed similar, hence the added nuance. The dataset comprises pairs of Cultural Heritage items from Europeana, 2 a single access point to millions of books, paintings, films, museum objects and archival records that have been digitized throughout Europe. It is an authoritative source of information coming from European cultural and scientific institutions. Typically, the items comprise meta-data describing a cultural heritage item and, sometimes, a thumbnail of the item itself. Participating systems in the TYPED task need to compute the similarity between items, using the textual meta-data. In addition to general similarity, participants need to score specific kinds of similarity, like similar author, similar time period, etc. (cf. Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>The paper is structured as follows. Section 2 reports the sources of the texts used in the two tasks. Section 3 details the annotation procedure. Section 4 presents the evaluation of the systems, followed by the results of CORE and TYPED tasks. Section 6 draws on some conclusions and forward projections.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Source Datasets</head><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the 2012 and 2013 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CORE task</head><p>The CORE dataset comprises pairs of news headlines (HDL), MT evaluation sentences (SMT) and pairs of glosses (OnWN and FNWN).</p><p>For HDL, we used naturally occurring news headlines gathered by the Europe Media Monitor (EMM) engine <ref type="bibr" target="#b8">(Best et al., 2005)</ref> from several different news sources. EMM clusters together related news. Our goal was to generate a balanced data set across the different similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sample another 375 pairs from the different EMM cluster in the same manner.</p><p>The SMT dataset comprises pairs of sentences used in machine translation evaluation. We have two different sets based on the evaluation metric used: an HTER set, and a HYTER set. Both metrics use the TER metric <ref type="bibr" target="#b13">(Snover et al., 2006)</ref> to measure the similarity of pairs. HTER typically relies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference  <ref type="bibr" target="#b9">(Dreyer and Marcu, 2012)</ref>. c. machine translation vs. FST generated translation. The HYTER data set is used in <ref type="bibr" target="#b9">(Dreyer and Marcu, 2012)</ref>.</p><p>The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 <ref type="bibr" target="#b11">(Hovy et al., 2006)</ref> and WordNet 3.0 <ref type="bibr" target="#b10">(Fellbaum, 1998)</ref>. 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 <ref type="bibr" target="#b6">(Baker et al., 1998)</ref> to WordNet 3.1. They are ran-domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Typed-similarity TYPED task</head><p>This task is devised in the context of the PATHS project, 3 which aims to assist users in accessing digital libraries looking for items. The project tests methods that offer suggestions about items that might be useful to recommend, to assist in the interpretation of the items, and to support the user in the discovery and exploration of the collections. Hence the task is about comparing pairs of items. The pairs are generated in the Europeana project.</p><p>A study in the PATHS project suggested that users would be interested in knowing why the system is suggesting related items. The study suggested seven similarity types: similar author or creator, similar people involved, similar time period, similar loca- tion, similar event or action, similar subject and similar description. In addition, we also include general similarity. Figure <ref type="figure" target="#fig_2">3</ref> shows the definition of each similarity type as provided to the annotators.</p><p>The dataset is generated in semi-automatically. First, members of the project manually select 25 pairs of items for each of the 7 similarity types (excluding general similarity), totalling 175 manually selected pairs. After removing duplicates and cleaning the dataset, we got 163 pairs. Second, we use these manually selected pairs as seeds to automatically select new pairs as follows: Starting from those seeds, we use the Europeana API to get similar items, and we repeat this process 5 times in order to diverge from the original items (we stored the vis-ited items to avoid looping). Once removed from the seed set, we select the new pairs following two approaches:</p><p>• Distance 1: Current item and similar item.</p><p>• Distance 2: Current item and an item that is similar to a similar item (twice removed distance wise) This yields 892 pairs for Distance 1 and 445 of Distance 2. We then divide the data into train and test, preserving the ratios. The train data contains 82 manually selected pairs, 446 pairs with similarity distance 1 and 222 pairs with similarity distance 2. The test data follows a similar distribution.</p><p>Europeana items cannot be redistributed, so we provide their urls and a script which uses the official Europeana API to access and extract the corresponding metadata in JSON format and a thumbnail. In addition, the textual fields which are relevant for the task are made accessible in text files, as follows:</p><p>• dcTitle: title of the item • dcSubject: list of subject terms (from some vocabulary) • dcDescription: textual description of the item • dcCreator: creator(s) of the item • dcDate: date(s) of the item • dcSource: source of the item 3 Annotation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CORE task</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the explanations and values for each score between 5 and 0. We use the Crowd-Flower crowd-sourcing service to annotate the CORE dataset. Annotators are presented with the detailed instructions given in Figure <ref type="figure">2</ref> and are asked to label each STS sentence pair on our 6 point scale using a dropdown box. Five sentence pairs at a time are presented to annotators. Annotators are paid 0.20 cents per set of 5 annotations and we collect 5 separate annotations per sentence pair. Annotators are restricted to people from the following countries: Australia, Canada, India, New Zealand, UK, and US.</p><p>To obtain high quality annotations, we create a representative gold dataset of 105 pairs that are manually annotated by the task organizers. During annotation, one gold pair is included in each set of 5 sentence pairs. Crowd annotators are required to rate 4 of the gold pairs correct to qualify to work on the task. Gold pairs are not distinguished in any way from the non-gold pairs. If the gold pairs are annotated incorrectly, annotators are told what the correct annotation is and they are given an explanation of why. CrowdFlower automatically stops low performing annotators -those with too many incorrectly labeled gold pairs -from working on the task.</p><p>The distribution of scores in the headlines HDL dataset is uniform, as in FNWN and OnWN, although the scores are slightly lower in FNWN and slightly higher in OnWN. The scores for SMT are not uniform, with most of the scores uniformly distributed between 3.5 and 5, a few pairs between 2 and 3.5, and nearly no pairs with values below 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TYPED task</head><p>The dataset is annotated using crowdsourcing. The survey contains the 1500 pairs of the dataset (750 for train and 750 for test), plus 20 gold pairs for quality control. Each participant is shown 4 training gold questions at the beginning, and then one gold every 2 or 4 questions depending on the accuracy. If accuracy dropped to less than 66.7% percent the survey is stopped and the answers from that particular annotator are discarded. Each annotator is allowed to rate a maximum of 20 pairs to avoid getting answers from people that are either tired or bored. To ensure a good comprehension of the items, the task is restricted to only accept annotators from some English speaking countries: UK, USA, Australia, Canada and New Zealand.</p><p>Participants are asked to rate the similarity between pairs of cultural heritage items from ranging from 5 to 0, following the instructions shown in Figure <ref type="figure" target="#fig_2">3</ref>. We also add a "Not Applicable" choice for cases in which annotators are not sure or didn't know. For those cases, we calculate the similarity score using the values of the rest of the annotators (if none, we convert it to 0). The instructions given to the annotators are the ones shown in Figure <ref type="figure" target="#fig_2">3</ref>. <ref type="bibr">Figure</ref>  <ref type="figure" target="#fig_3">4</ref> shows a pair from the dataset, as presented to annotators.</p><p>The similarity scores for the pairs follow a similar distribution in all types. Most of the pairs have a score between 4 and 5, which can amount to as much as 50% of all pairs in some types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quality of annotation</head><p>In order to assess the annotation quality, we measure the correlation of each annotator with the average of the rest of the annotators. We then averaged all the correlations. This method to estimate the quality is identical to the method used for evaluation (see Section 4.1) and it can be thus used as the upper bound for the systems. The inter-tagger correlation in the CORE dataset for each of dataset is as follows:</p><p>• HDL: 85.0%</p><p>• FNWN: 69.9%</p><p>• OnWN: 87.2%</p><p>• SMT: 65.8% For the TYPED dataset, the inter-tagger correlation values for each type of similarity is as follows:</p><p>• General: 77.0%</p><p>• Author: 73.1%</p><p>• People Involved: 62.5%</p><p>• Time period: 72.0%</p><p>• Location: 74.3%</p><p>• Event or Action: 63.9%</p><p>• Subject: 74.5%</p><p>• Description: 74.9% In both datasets, the correlation figures are high, confirming that the task is well designed. The weakest correlations in the CORE task are SMT and FNWN. The first might reflect the fact that some automatically produced translations are confusing or difficult to understand, and the second could be caused by the special style used to gloss FrameNet concepts. In the TYPED task the weakest correlations are for the People Involved and Event or Action types, as they might be the most difficult to spot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Systems Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation metrics</head><p>Evaluation of STS is still an open issue. STS experiments have traditionally used Pearson productmoment correlation, or, alternatively, Spearman rank order correlation. In addition, we also need a method to aggregate the results from each dataset into an overall score. The analysis performed in <ref type="bibr">(Agirre and Amigó, In prep)</ref> shows that Pearson and averaging across datasets are the best suited combination in general. In particular, Pearson is more informative than Spearman, in that Spearman only takes the rank differences into account, while Pearson does account for value differences as well. The study also showed that other alternatives need to be considered, depending on the requirements of the target application.</p><p>We leave application-dependent evaluations for future work, and focus on average weighted Pearson correlation. When averaging, we weight each individual correlation by the size of the dataset. In addition, participants in the CORE task are allowed to provide a confidence score between 1 and 100 for each of their scores. The evaluation script down-weights the pairs with low confidence, following weighted Pearson. <ref type="bibr">4</ref> In order to compute statistical significance among system results, we use a one-tailed parametric test based on Fisher's ztransformation <ref type="bibr">(Press et al., 2002, equation 14.5.10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Baseline Systems</head><p>For the CORE dataset, we produce scores using a simple word overlap baseline system. We tokenize the input sentences splitting at white spaces, and then represent each sentence as a vector in the multidimensional token space. Each dimension has 1 if the token is present in the sentence, 0 otherwise. Vector similarity is computed using the cosine similarity metric. We also run two freely available systems, DKPro <ref type="bibr" target="#b7">(Bar et al., 2012)</ref> and <ref type="bibr">TakeLab (Šarić et al., 2012)</ref> from STS 2012, 5 and evaluate them on the CORE dataset. They serve as two strong contenders since they ranked 1st (DKPro) and 2nd (TakeLab) in last year's STS task.</p><p>For the TYPED dataset, we first produce XML files for each of the items, using the fields as provided to participants. Then we run named entity recognition and classification (NERC) and date detection using Stanford CoreNLP. This is followed by calculating the similarity score for each of the types as follows.</p><p>• General: cosine similarity of TF-IDF vectors of tokens from all fields. • Author: cosine similarity of TF-IDF vectors for dc:Creator field. • People involved, time period and location: cosine similarity of TF-IDF vectors of location/date/people recognized by NERC in all fields. • Events: cosine similarity of TF-IDF vectors of verbs in all fields. • Subject and description: cosine similarity of TF-IDF vectors of respective fields. IDF values are calculated from a subset of the Europeana collection (Culture Grid collection). We also run a random baseline several times, yielding close to 0 correlations in all datasets, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participation</head><p>Participants could send a maximum of three system runs. After downloading the test datasets, they had a maximum of 120 hours to upload the results. 34 teams participated in the CORE task, submitting 89 system runs. For the TYPED task, 6 teams participated, submitting 14 system runs. <ref type="bibr">6</ref> Some submissions had minor issues: one team had a confidence score of 0 for all items (we replaced them by 100), and another team had a few Not-a-Number scores for the SMT dataset, which we replaced by 5. One team submitted the results past the 120 hours. This team, and the teams that in-6 Due to lack of space we can't detail the full names of authors and institutions that participated.The interested reader can use the name of the runs in Tables <ref type="table" target="#tab_2">2 and 3</ref> to find the relevant paper in these proceedings. cluded one of the organizers, are explicitly marked. We want to stress that in these teams the organizers did not allow the developers of the system to access any data or information which was not available for the rest of participants. After the submission deadline expired, the organizers published the gold standard in the task website, in order to ensure a transparent evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CORE Task Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the results of the CORE task, with runs listed in alphabetical order. The correlation in each dataset is given, followed by the mean correlation (the official measure), and the rank of the run. The baseline ranks 73. The highest correlations are for OnWN (84%, by deft) and HDL (78%, by UMBC), followed by FNWN (58%, by UMBC) and SMT (40%, by NTNU). This fits nicely with the inter-tagger correlations (respectively 87, 85, 70 and 65, cf. Section 3). It also shows that the systems get close to the human correlations in the OnWN and HDL dataset, with bigger differences for FNWN and SMT.</p><p>The result of the best run (by UMBC) is significantly different (p-value &lt; 0.05) than all runs except the second best. The second best run is only significantly different to the runs ranking 7th and below, and the third best to the 14th run and below. The difference between consecutive runs was not significant. This indicates that many system runs performed very close to each other.</p><p>Only 13 runs included non-uniform confidence scores. In 10 cases the confidence value allowed to improve performance, sometimes as much as .11 absolute points. For instance, SXUCFN-run3 improves from .4773 to .5458. The most notable exception is MayoClinicNLP-r2CDT, which achieves a mean correlation of .5879 instead of .5572 if they provide uniform confidence values.</p><p>The  <ref type="formula">4</ref>) SMT is trained on 2012 SM-Teuroparl data. Note that Takelab-best is an upper bound, as the best combination is selected on the test dataset. TakeLab-sts12, TakeLab-best, DKPro rank as 58th, 27th and 6th in this year's system submissions, respectively. The different results yielded from TakeLab depending on the training data suggests that some STS systems are quite sensitive to the source of the sentence pairs, indicating that domain adaptation techniques could have a role in this task. On the other hand, DKPro performed extremely well when trained on all available training, with no special tweaking for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">TYPED Task Results</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the results of TYPED task. The columns show the correlation for each type of similarity, followed by the mean correlation (the official measure), and the rank of the run. The best system (from Unitor) is best in all types. The baseline ranked 8th, but the performance difference with the best system is quite significant. The best result is significantly different (p-value &lt; 0.02) to all runs. The second and third best runs are only significantly different from the run ranking 5th and below. Note that in this dataset the correlations of the best system are higher than the inter-tagger correlations. This might indicate that the task has been solved, in the sense that the features used by the top systems are enough to characterize the problem and reach human performance, although the correlations of some types could be too low for practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tools and resources used</head><p>The organizers asked participants to submit a description file, making special emphasis on the tools and resources that were used. Tables <ref type="table">4 and 5</ref> show schematically the tools and resources as reported by some of the participants for the CORE and TYPED tasks (respectively). In the last row, the totals show that WordNet and monolingual corpora were the most used resources for both tasks, followed by Wikipedia and the use of acronyms (for CORE and TYPED tasks respectively). Dictionaries, multilingual corpora, opinion and sentiment analysis, and lists and tables of paraphrases are also used.</p><p>For CORE, generic NLP tools such as lemmatization and PoS tagging are widely used, and to a lesser extent, distributional similarity, knowledgebased similarity, syntactic analysis, named entity recognition, lexical substitution and time and date resolution (in this order). Other popular tools are Semantic Role Labeling, Textual Entailment, String Similarity, Tree Kernels and Word Sense Disambiguation. Machine learning is widely used to combine and tune components (and so, it is not mentioned in the tables). Several less used tools are also listed but are used by three or less systems. The top scoring systems use most of the resources and tools listed (UMBC EBIQUITY-ParingWords, MayoClinicNLP-r3wtCD). Other well ranked systems like deft-baseline are only based on distributional similarity. Although not mentioned in the descriptions files, some systems used the publicly available DKPro and Takelab systems.</p><p>For the TYPED task, the most used tools are lemmatizers, Named Entity Recognizers, and PoS taggers. Distributional and Knowledge-base similarity is also used, and at least four systems used syntactic analysis and time and date resolution. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We presented the 2013 *SEM shared task on Semantic Textual Similarity. 8 Two tasks were defined: a</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotation values with explanations and examples for the core STS task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2: Annotation instructions for CORE task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Annotation instructions for TYPED task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: TYPED pair on our survey. Only general and author similarity types are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of STS 2012 and 2013 datasets.    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available systems, see text for details. Note: † signals team involving one of the organizers, ‡ for systems submitting past the 120 hour window.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on TYPED task. The first row corresponds to the baseline. Note: † signals team involving one of the organizers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table also shows the results of TakeLab and DKPro. We train the DKPro and TakeLab-sts12 models on all the training and test STS 2012 data. We additionally train another variant system of TakeLab, TakeLab-best, where we use targeted training where the model yields the best per-</figDesc><table><row><cell>formance for each test subset as follows: (1) HDL</cell></row><row><cell>is trained on MSRpar 2012 data; (2) OnWN is</cell></row><row><cell>trained on all 2012 data; (3) FnWN is trained on</cell></row><row><cell>2012 OnWN data; (</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cs.columbia.edu/˜weiwei/ workshop/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.europeana.eu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.paths-project.eu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://en.wikipedia.org/wiki/Pearson_ product-moment_correlation_coefficient# Calculating_a_weighted_correlation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Code is available at http://www-nlp.stanford. edu/wiki/STS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For a more detailed analysis, the reader is directed to the papers in this volume.8  All annotations, evaluation scripts and system outputs are available in the website for the task 9 . In addition, a collabora-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the OntoNotes team for sharing OntoNotes to WordNet mappings <ref type="bibr" target="#b11">(Hovy et al. 2006</ref>). We thank Language Weaver, INC, DARPA and LDC for providing the SMT data. This work is also partially funded by the Spanish Ministry of Education, Culture and Sport (grant FPU12/06243). This a comprehensive list of evaluation tasks, datasets, software and papers related to STS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6 </p><p>x Total 4 7 3 7 7 4 11 3 11 11 4 4 2 core task CORE similar to the STS 2012 task, and a new pilot on typed-similarity TYPED. We had 34 teams participate in both tasks submitting 89 system runs for CORE and 14 system runs for TYPED, in total amounting to a 103 system evaluations. CORE uses datasets which are related to but different from those used in 2012: news headlines, MT evaluation data, gloss pairs. The best systems attained correlations close to the human inter tagger correlations. The TYPED task characterizes, for the first time, the reasons why two items are deemed similar. The results on TYPED show that the training data provided allowed systems to yield high correlation scores, demonstrating the practical viability of this new task. In the future, we are planning on adding more nuanced evaluation data sets that include modality (belief, negation, permission, etc.) and sentiment. Also given the success rate of the TYPED task, however, the data in this pilot is relatively structured, hence in the future we are interested in investigating identifying reasons why two pairs of unstructured texts as those present in CORE are deemed similar.</p><p>work was partially funded by the DARPA BOLT and DEFT programs.</p><p>We want to thank Nikolaos Aletras, German Rigau and Mark Stevenson for their help designing, annotating and collecting the typed-similarity data. The development of the typed-similarity dataset was supported by the PATHS project (http://paths-project.eu) funded by the European Community's Seventh Framework <ref type="bibr">Program (FP7/2007</ref><ref type="bibr">-2013</ref> under grant agreement no. 270082. The tasks were partially financed by the READERS project under the CHIST-ERA framework (FP7 ERA-Net). We thank Europeana and all contributors to Europeana for sharing their content through the API.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OnWN FNWN SMT Mean # baseline-tokencos</title>
		<author>
			<persName><surname>Head</surname></persName>
		</author>
		<author>
			<persName><surname>Onwn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">#</forename><surname>Smt Mean</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Head</surname></persName>
		</author>
		<idno>5399 .2828 .2146 .2861 .3639</idno>
		<imprint>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mayoclinicnlp</surname></persName>
		</author>
		<idno>r3wtCD .6440 .8295 .3202 .3561 .5671 5 CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Unitor-Svregressor</surname></persName>
		</author>
		<idno>run1 .6353 .5744 .3521 .3285 .4941 37 INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319</idno>
		<imprint>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Unitor-Svregressor</surname></persName>
		</author>
		<idno>run3 .6027 .5489 .3269 .3192 .4716 52 KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65 KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83 UPC-AED T .5119 .6386 -.0464 .1235 .3671 72</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring evaluation measures for semantic textual similarity</title>
	</analytic>
	<monogr>
		<title level="m">prep</title>
				<imprint/>
	</monogr>
	<note>References Eneko Agirre and Enrique Amigó. Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
				<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;98 Proceedings of the 17th international conference on Computational linguistics</title>
				<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ukp: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation, in conjunction with the 1st Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the 6th International Workshop on Semantic Evaluation, in conjunction with the 1st Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europe media monitorsystem description</title>
		<author>
			<persName><forename type="first">Clive</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Blackler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUR Report 22173-En</title>
				<meeting><address><addrLine>Ispra, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Tefilo Garcia, and David Horby</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyter: Meaning-equivalent semantics for translation evaluation</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics</title>
				<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL</title>
				<meeting>the Human Language Technology Conference of the North American Chapter of the ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Numerical Recipes: The Art of Scientific Computing V 2.10 With Linux Or Single-Screen License</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
				<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Takelab: Systems for measuring semantic text similarity</title>
		<author>
			<persName><forename type="first">Goran</forename><surname>Franešarić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janšnajder</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojana Dalbelo</forename><surname>Bašić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation</title>
				<meeting>the Sixth International Workshop on Semantic Evaluation<address><addrLine>Montréal, Canada, 7-8 June</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
