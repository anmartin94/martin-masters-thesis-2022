<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">English Lexical Sample Task Description</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
							<email>adam@itri.bton.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ITRI</orgName>
								<orgName type="institution" key="instit2">University of Brighton</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">English Lexical Sample Task Description</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-08-25T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The English lexical sample task (adjectives and nouns) for SENSEVAL 2 was set up according to the same principles as for SENSEVAL-1, as reported in <ref type="bibr" target="#b0">(Kilgarriff and Rosenzweig, 2000)</ref>. (Adjectives and nouns only, because the data preparation for the verbs lexical sample was undertaken alongside that for the English all-words task, and is reported in Palmer et al (this volume). All discussion below up to the Results section covers only adjectives and nouns.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Lexical sample</head><p>The lexicon was sampled to give a range of low, medium and high frequency words (see Table <ref type="table" target="#tab_1">1</ref>). These were all different words to the ones used in SENSEVAL 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus choic~</head><p>For the most part, the British National Corpus (New edition) was used. (The new edition has the advantage that it is available worldwide, so all participants had the opportunity of obtaining it for system training.) Our goal was to match this source, containing British English, with another, of American English. In the event, only limited quantities of corpus data for American English were available without copyright complications, so the lion's share of the data was from the BNC with a limited quantity from the Wall Street Journal.</p><p>In accordance with standard SENSEVAL procedure, the goal was to have 75 + 15n + 6m instances for each lexical-sample word, where n is the number of senses the word has and m is the number of multiword expressions that the word is part of (both, of course, relative to a specific lexicon). In practice numbers varied slightly, as instances were deleted because they had the wrong part of speech or were otherwise unus-17 able. See Table <ref type="table" target="#tab_1">1</ref> for actual numbers of senses, multiwords expressions and instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexicon choice</head><p>Here lay the biggest contrast with the SENSEVAL-1 task, which had used Oxford University Press's experimental HECTOR lexicon. This time, in response to popular acclaim, WordNet was used. Since SENSEVAL was first mooted, in 1997, WordNet-or-not-WordNet has been a recurring theme. In favour was the argument that it was already very widely used, almost a de facto standard. The argument against concerned its sense distinctions. WordNet, like thesauruses but unlike standard dictionaries, is organised around groups of words of similar meanings ( synsets), not around words (with their various meanings). This means that the priority for the lexicographer is building coherent synsets rather than the coherent analysis of the various meanings of a particular word. The writer of a thesaurus does not need to pay as much attention to the distinction between two senses of a word, as the writer of a dictionary. Word sense disambiguation is a task which needs clear and well-motivated sense distinctions. In English SENSEVAL-1, Word-Net was not used because of concerns that it did not provide clean enough sense distinctions.</p><p>While HECTOR provided good sense distinctions, it was unsatisfactory in that it did not cover the whole lexicon so there was no possibility of scaling up. The case for WordNet -that it was already integrated into so much NLP and WSD work -still stood, so the decision was made to use WordNet. To guard against cases where WordNet made a distinction between two meanings, but it was not clear what the distinction was, all the words in the lexical sample had their entries reviewed by a  lexicographer, with a view particularly to merging insufficiently-distinct senses. It was initially unclear how these revisions would relate to the publicly available version of WordNet (at that time, WordNet 1.6). We are very grateful to the Princeton WordNet team (George Miller, Christiane Fellbaum and Randee Tengi) for their help at this point; they agreed to incorporate our proposed revisions into a new version of Word-Net (1.7) which was then made available in time (despite some very tight deadlines) for the SEN-SEVAL competition. WordNet 1.7 was not available as a complete object at the time of the gold standard production, in Spring 2001, but the entries for the lexical sample words were fixed at that point. For each lexical sample entry, we produced an HTML version for the lexicographers to work from. In addition to all the relevant information in WordNet, this had a mnemonic for each sense, so that taggers could use mnemonics when doing the tagging, rather than easily:forgotten, easily-confused sense numbers. The mnemonics were selected by a lexicographer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gold standard production</head><p>Once the corpus sources and lexical entries were fixed, work could proceed with the Gold-Standard tagging. <ref type="bibr">1</ref> First, a team of three professional lexicographers and fourteen students and others was recruited. Recruitment proceeded as follows: an aptitude test was set up on the web. The test involved sense-tagging some corpus instances (taken from SENSEVAL-1, so the gold-standard answers were known). Email postings were made asking interested people to visit the website and take the test. All applicants scoring sufficiently well on the test were then offered work, on a piecework basis.</p><p>An HTML version of the corpus for a word was prepared. This comprised a series of tensentence stretches of text, with one word in the last of the sentences highlighted; that was the word to be sense-tagged. The files were HTML versions of the XML files used for test and training data.</p><p>A tagger was emailed the lexical entry and corpus for a word. They then tagged it, and returned, by email, a file of answers. These files were checked automatically, and if they contained 'answers' which were not possible answers for the word, the suspect items were automatically emailed back to the tagger for correction.</p><p>The tagger guidelines are available along with other resources for the English-lexical-sample task. They developed in the course of the exercise; when a tagger asked a pertinent questions, I circulated the question and my answer to all taggers and incorporated them into the guidelines.</p><p>As in SENSEVAL-1, "Unassignable" and "Proper-name" tags were always available alongside regular tags, and taggers were told to put down more than one tag, where multiple tags were equally applicable. Taggers were also asked to mark items where the part of speech was wrong; these were then deleted from the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tagger agreement procedures and scores</head><p>As in all exercises where a gold standard corpus is the goal, it was necessary to have all data tagged by more than one person. The question then arises, how many taggings does each item need? The algorithm adopted here was:</p><p>1. send item out to two taggers 2. if they agree completely, stop; return agreed answer 3. else, send out to another tagger 4. is there one or more tag that two agree on?</p><p>5. if yes, stop; return all tags which two people agree on 6. if no, return to step 3 Thus, in simple cases, a minimum of effort was used, but in difficult cases, more opinions were obtained. The number of taggings per items is shown below. Note that the algorithm stops at step 2 if both taggers agree on one tag, or if both taggers agree on two or more tags.  Of the 5032 two-tagger items, in 4688 cases, the taggers agreed on one tag; in 340 cases, on two tags; and in 4 cases, on three tags.</p><p>For the 2446 cases which were tagged three times, 136 were cases where all three taggers agreed perfectly (so, had the algorithm been followed to the letter, the item would not have been tagged a third time; such cases were caused by delays in taggers returning answers.) The common patterns amongst the remainder are shown in Table <ref type="table" target="#tab_3">2</ref>.</p><p>For the 86 cases with four taggers, half the cases were {A, A, B, C} taggings.</p><p>Fine-grained inter-tagger agreement (ITA) figures was calculated using the same scoring algorithm as for the systems. 2 For each pair of taggers tagging an instance, two scores were calculated, one with the one answer as the key, the other with the other. For each instance, scores were normalised so that the maximum score for each corpus instance was one, however many times it had been tagged. The overall ITA was 85.5%. A breakdown by word and by word cla.':;s is given in Table <ref type="table" target="#tab_1">1</ref>. <ref type="bibr">3</ref> As argued in <ref type="bibr" target="#b0">(Kilgarriff and Rosenzweig, 2000)</ref> (also <ref type="bibr" target="#b2">(Kilgarriff, 1999)</ref>) the inter-tagger agreement figure for a gold standard is of less interest than the replicability figure: if a completely different team of taggers used the same methodology to do the same task, what would the agreement level between the two teams' outputs be? It is the replicability figure, rather than ITA, which defines an upper bound for the task. We have not yet had time to conduct such a study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Task organisation</head><p>The organisation followed standard SENSEVAL procedure. The data was prepared in XML using SENSEVAL DTDs, with the data for each word split in a ration of 2:1 between training and test data. Data distribution, results uploads, baselines and scoring were handled at UPenn (see paper by Cotton and Edmonds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Results are presented in the table below. Owing to space constraints, where a team submitted multiple systems with similar results, only the best result is shown. Full results are available at the SENSEVAL website, as are decodings of system names. At the SENSEVAL workshop (5-6 July 2001) it was agreed that there should also be a later deadline (end July 2001) so that 'egregious bugs' could be fixed. In order to honour both standard practice in evaluation exercises ( eg, no extension of deadlines) and also the agreement made at the workshop, both results sets are presented, with later-deadline results marked with (R) as a suffix to the name.</p><p>There has not yet been time for an analysis of the results. The one comment that does seem pertinent is the contrast with the Englishlexical-sample task in SENSEVAL-1. The tasks were organised in similar ways, and some of the systems were improved versions of systems participating in 1998. Yet the performance of the best systems has, apparently, dropped around 14%. We may well ask, why?</p><p>We believe the drop is due to the choice of lexicon. As discussed above, using Word-Net for SENSEVAL has drawbacks. High-  accuracy word sense disambiguation is only possible where the lexicon makes clear and wellmotivated sense distinctions, and provides sufficient information about the distinctions for the disambiguation algorithm to build on. An implication for future WSD research is that it is time to turn our attention from algorithms, to sense distinctions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>I I Word I Ss I Mwe I inst I ITA I</figDesc><table><row><cell cols="4">ADJS: lexical sample size: 15</cell></row><row><cell>blind</cell><cell>3</cell><cell>21</cell><cell>163 89.6</cell></row><row><cell>colorless</cell><cell>2</cell><cell>0</cell><cell>103 94.2</cell></row><row><cell>cool</cell><cell>6</cell><cell>1</cell><cell>158 92.1</cell></row><row><cell>faithful</cell><cell>3</cell><cell>0</cell><cell>70 94.6</cell></row><row><cell>fine</cell><cell>9</cell><cell>6</cell><cell>212 84.0</cell></row><row><cell>fit</cell><cell>3</cell><cell>0</cell><cell>86 85.0</cell></row><row><cell>free</cell><cell>8</cell><cell>36</cell><cell>247 79.2</cell></row><row><cell>graceful</cell><cell>2</cell><cell>0</cell><cell>85 72.6</cell></row><row><cell>green</cell><cell>7</cell><cell>80</cell><cell>284 86.6</cell></row><row><cell>local</cell><cell>3</cell><cell>12</cell><cell>113 89.1</cell></row><row><cell>natural</cell><cell>10</cell><cell>37</cell><cell>309 72.4</cell></row><row><cell>oblique</cell><cell>2</cell><cell>5</cell><cell>86 96.4</cell></row><row><cell>simple</cell><cell>7</cell><cell>19</cell><cell>196 67.8</cell></row><row><cell>solemn</cell><cell>2</cell><cell>0</cell><cell>77 84.1</cell></row><row><cell>vital</cell><cell>4</cell><cell>7</cell><cell>112 93.7</cell></row><row><cell>ALL ADJS</cell><cell></cell><cell></cell><cell>2301 83.4</cell></row><row><cell cols="4">NOUNS: lexical sample size: 29</cell></row><row><cell>art</cell><cell>5</cell><cell>35</cell><cell>294 78.5</cell></row><row><cell>authority</cell><cell>7</cell><cell>6</cell><cell>276 84.3</cell></row><row><cell>bar</cell><cell>13</cell><cell>57</cell><cell>455 87.3</cell></row><row><cell>bum</cell><cell>4</cell><cell>0</cell><cell>137 91.7</cell></row><row><cell>chair</cell><cell>4</cell><cell>35</cell><cell>207 92.8</cell></row><row><cell>channel</cell><cell>7</cell><cell>10</cell><cell>218 84.8</cell></row><row><cell>child</cell><cell>4</cell><cell>16</cell><cell>193 92.3</cell></row><row><cell>church</cell><cell>3</cell><cell>21</cell><cell>192 88.0</cell></row><row><cell>circuit</cell><cell>6</cell><cell>31</cell><cell>255 93.5</cell></row><row><cell>day</cell><cell>9</cell><cell>82</cell><cell>434 76.3</cell></row><row><cell>detention</cell><cell>2</cell><cell>5</cell><cell>95 98.7</cell></row><row><cell>dyke</cell><cell>2</cell><cell>0</cell><cell>86 96.5</cell></row><row><cell>facility</cell><cell>5</cell><cell>9</cell><cell>172 89.5</cell></row><row><cell>fatigue</cell><cell>4</cell><cell>6</cell><cell>128 97.7</cell></row><row><cell>feeling</cell><cell>6</cell><cell>5</cell><cell>153 77.0</cell></row><row><cell>grip</cell><cell>7</cell><cell>3</cell><cell>153 85.2</cell></row><row><cell>hearth</cell><cell>3</cell><cell>1</cell><cell>96 85.0</cell></row><row><cell>holiday</cell><cell>2</cell><cell>9</cell><cell>93 90.5</cell></row><row><cell>lady</cell><cell>3</cell><cell>27</cell><cell>158 74.1</cell></row><row><cell>material</cell><cell>5</cell><cell>39</cell><cell>209 85.1</cell></row><row><cell>mouth</cell><cell>8</cell><cell>10</cell><cell>179 88.7</cell></row><row><cell>nation</cell><cell>3</cell><cell>10</cell><cell>112 90.5</cell></row><row><cell>nature</cell><cell>5</cell><cell>8</cell><cell>138 86.7</cell></row><row><cell>post</cell><cell>8</cell><cell>33</cell><cell>236 87.7</cell></row><row><cell>restraint</cell><cell>6</cell><cell>3</cell><cell>136 80.4</cell></row><row><cell>sense</cell><cell>5</cell><cell>37</cell><cell>160 87.1</cell></row><row><cell>spade</cell><cell>3</cell><cell>7</cell><cell>98 95.1</cell></row><row><cell>stress</cell><cell>5</cell><cell>7</cell><cell>118 74.7</cell></row><row><cell>yew</cell><cell>2</cell><cell>15</cell><cell>85 97.1</cell></row><row><cell>ALL NOUNS</cell><cell></cell><cell></cell><cell>5266 86.3</cell></row><row><cell>II ALL</cell><cell></cell><cell></cell><cell>1 7567 1 85.5 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Lexical sample: rubric for column</figDesc><table /><note>headers: Ss=number of fine-grained senses; Mwe =number of multi-word expressions which the word participates in (as bear participates in WordNet headword polar bear); inst = number of instances tagged; ITA = inter-tagger agreement (fine-grained).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Patterns of (dis)agreement for 3-tagger</cell></row><row><cell>cases. GS = gold standard tagging arising from</cell></row><row><cell>these human taggings. ";" used as separator</cell></row><row><cell>where a tagger (or the gold standard) gave mul-</cell></row><row><cell>tiple tags.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>the figures shown, unlike kappa figures, have the merit of being directly comparable with system performance scores.</figDesc><table><row><cell cols="2">PR ATT System</cell></row><row><cell></cell><cell>Supervised systems</cell></row><row><cell>.82</cell><cell>28 BCU ehu-dlist-best</cell></row><row><cell>.67</cell><cell>25 IRST</cell></row><row><cell>.64</cell><cell>100 JHU (R)</cell></row><row><cell>.64</cell><cell>100 SMUls</cell></row><row><cell>.63</cell><cell>100 KUNLP</cell></row><row><cell>.62</cell><cell>100 Stanford-CS224</cell></row><row><cell>.61</cell><cell>100 Sinequa-LIA SCT</cell></row><row><cell>.59</cell><cell>100 TALP</cell></row><row><cell>.57</cell><cell>98 BCU ehu-dlist-all</cell></row><row><cell>.57</cell><cell>100 Duluth-3</cell></row><row><cell>.57</cell><cell>100 UMD-SST</cell></row><row><cell>.50</cell><cell>100 UNED LS-T</cell></row><row><cell>.42</cell><cell>98 Alicante</cell></row><row><cell></cell><cell>Supervised baselines</cell></row><row><cell>.51</cell><cell>100 Base Lesk</cell></row><row><cell>.48</cell><cell>100 Base Commonest</cell></row><row><cell></cell><cell>Unsupervised systems</cell></row><row><cell>.58</cell><cell>55 ITRI-WASPS</cell></row><row><cell>40</cell><cell>100 UNED-LS-U</cell></row><row><cell>.29</cell><cell>100 CLresearch DIMAP</cell></row><row><cell>.25</cell><cell>99 IIT-2 (R)</cell></row><row><cell></cell><cell>Unsupervised baselines</cell></row><row><cell>.16</cell><cell>100 Base Lesk-defs</cell></row><row><cell>.14</cell><cell>100 Base random</cell></row><row><cell>20</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>PR=system precision; ATT= percentage of cases for which an answer was returned ("attempted").</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The tagging was supported by a grant from EPSRC, the UK funding council, under GR/R02337 /01 (MATS).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">taggers' answers GS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All ITA figures and other results reported in this paper refer to fine-grained sense distinctions. The grouping of senses into coarse-grained categories took place independently of the gold-standard preparation, which was based entirely on fine sense distinctions.3 Kappa was not calculated because there were various ways in which it might have been calculated, so it was unclear which was appropriate, and it would have introduced more complication than clarification. Also</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Framework and results for English SENSEVAL</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Rosenzweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Special Issue on SENSEVAL</title>
		<editor>Adam Kilgarriff and Martha Palmer</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">95% replicability for manual word sense tagging</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
				<meeting>EACL<address><addrLine>Bergen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="277" to="278" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
