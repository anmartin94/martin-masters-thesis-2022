title
SemEval-2020 Task 8: Memotion Analysis-The Visuo-Lingual Metaphor!
abstract
Information on social media comprises of various modalities such as textual, visual and audio. NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media. However, computational processing of Internet memes needs a hybrid approach. The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content anymore. To the best of our knowledge, there is not much attention towards meme emotion analysis. The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes. The task Memotion analysis released approx 10K annotated memes-with human annotated labels namely sentiment(positive, negative, neutral), type of emotion(sarcastic,funny,offensive, motivation) and their corresponding intensity. The challenge consisted of three subtasks: <TASK>sentiment (positive, negative, and neutral) analysis of memes, overall emotion (humor, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion</TASK>. The best performances achieved were F 1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks.

Introduction
In the last few years, the growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter has become a topic of immense interest. Memes are one of the most typed English words Sonnad (2018) in recent times which are often derived from our prior social and cultural experiences such as TV series or a popular cartoon character (think: "One Does Not Simply" -a now immensely popular meme taken from the movie Lord of the Rings). These digital constructs are so deeply ingrained in our Internet culture that to understand the opinion of a community, we need to understand the type of memes it shares. (Gal et al., 2016) aptly describes them as performative acts, which involve a conscious decision to either support or reject an ongoing social discourse.
The prevalence of hate speech in online social media is a nightmare and a great societal responsiblity for many social media companies. However, the latest entrant "Internet memes" (Williams et al., 2016) has doubled the challenge. When malicious users upload something offensive to torment or disturb people, it traditionally has to be seen and flagged by at least one human, either a user or a paid worker. Even today, companies like Facebook and Twitter rely extensively on outside human contractors from different companies. But with the growing volume of multimodal social media it is becoming impossible to scale. The detection of offensive content on online social media is an ongoing struggle. OffenseEval (Zampieri et al., 2019) is a shared task which is being organized since the last two years at SemEval. But, detecting an offensive meme is more complex than detecting an offensive text -as it involves visual cues and language understanding. This is one of the motivating aspects which encouraged us to propose this task.
Analogous to textual content on social media, memes also need to be analysed and processed to extract the conveyed message. A few researchers have tried to automate the meme generation (Peirson et al., 2018;oli, ) process, while a few others tried to extract its inherent sentiment (French, 2017) in the recent past. Nevertheless, a lot more needs to be done to distinguish their finer aspects such as type of humor or offense.
The paper is organised as follows: Related work is described in Section 2. The proposed task is described in Section 3. Data collection and data distribution is explained in Section 4 while Section 5 demonstrates the baseline model. Section 6 shows the reason for considering Macro F1 as evaluation metric. In Section 7, participants and the top performing models are discussed in detail. Section 8 shows the results, analysis and the takeaway points from Memotion 1.0. Finally, we summarise our work by highlighting the insights derived along-with the further scope and open ended pointers in section 9.

Related Work
Identifying the text in the image is as important as the context of the image, so we present the related work in two parts, one involving the models used to extract text from the meme and the other on the analysis of memes. (Jaderberg et al., 2014) proposed one of the first CNN based approach for text recognition to classify words into fixed set of character texts. ( 2019) uses an n-gram model to correct the OCR text extracted. (Memon et al., 2020) performed a comprehensive literature review on handwriting character recognition. A survey analysed in (Islam et al., 2017) shows an overview of different aspects of OCR and discuss different methods at resolving issues related to OCR. Template matching using contours is used in (Olszewska, 2015) to recognize visual characters from real-world scenarios.
While there are not many works involving direct classification of emotions on memes. Early works on the detection of offensive content on online social media is OffenseEval (Zampieri et al., 2019) shared task, organized at SemEval since 2019. The latest entrant "Internet memes" (Williams et al., 2016) has doubled the challenge. Detecting an offensive meme is more complex than detecting an offensive text as it involves visual cues and language understanding. Automate of meme generation process are explored in (Peirson et al., 2018;oli, ) while others have sought to extract the memes' inherent sentiment (French, 2017). Nevertheless, this challenge proves the necessity of more research in the field of multimodal approaches to detect and classify the sentiments of memes.

The Memotion Analysis Task
Memes typically induce humor and strive to be relatable. Many of them aim to express solidarity during certain life phases and thus, to connect with their audience. Some memes are directly humorous whereas others go for sarcastic dig at daily life events. Inspired by the various humorous effects of memes, we propose three task as follows:
• Task A-Sentiment Classification: Given an Internet meme, the first task is to classify it as positive, negative or neutral meme.
• Task B-Humor Classification: Given an Internet meme, the system has to identify the type of emotion expressed. The categories are sarcastic, humorous, motivation and offensive meme. A meme can have more than one category.
• Task C-Scales of Semantic Classes: The third task is to quantify the extent to which a particular effect is being expressed. Details of such quantifications is reported in the Table 1.
We have released 10K human annotated Internet memes labelled with semantic dimensions namely sentiment, and type of humor that is, sarcastic, humorous, or offensive and motivation with their corresponding intensity. The humor types are further quantified on a Likert scale as in  

Dataset
To understand the complexity of memes, as discussed in the prior sections, it is essential to collect memes from different categories, with varying emotion classes. Details of preparing the data-set is presented below:
• Data collection: We identified a total of 52 unique and globally popular categories, for example, Hillary, Trump, Minions, Baby godfather, etc., for downloading the meme data. The meme (images) were downloaded using Google images search service, with the help of a browser extension tool called as fatkun batch downloader 1 . It provided a simple yet effective means to scrape a large number of memes, relevant for our purpose. To avoid any copyright issue in this, we have collected memes which are available in public domain along with their URLs, and added that information as additional meta-data in our data-set as well.
• Filtering: The memes are filtered keeping the following constraints into perspective:
-The meme must contain clear background picture, along-with an embedded textual content.
-Memes with only English language text content are considered for this study.
• Annotation: For getting our data-set of approx 10k samples annotated, we reached out to Amazon Mechanical Turk (AMT) workers, to annotate the emotion class labels as Humorous, Sarcasm, Offensive, Motivation and quantify the intensity to which a particular effect of a class is expressed, along-with the overall sentiments (very negative, negative, neutral, positive, very positive).
Emotion about memes highly depends upon an individual's perception of an array of aspects within society, and could thus vary from one person to another. This phenomenon is called as "Subjective Perception Problem" as noted in (Zhao et al., 2018). To address this challenge, the annotation process is performed multiple times ie. each sample is provided to 5 annotators, and the final annotations are adjudicated based upon majority voting scheme. The filtered and the annotated data-set comprises to the size of 9871 data samples. In addition to these aspects, textual content plays a pivotal role in ascertaining the emotion of a meme.
To understand the textual content from the memes, text has been extracted using Google vision OCR APIs. The extracted text was not completely accurate, therefore AMT workers were asked to provide the rectified text against the given OCR extracted text, for the inaccurately extracted OCR text.
Data Distribution: The statistical summaries are provided in Table 2 and 3. It is clear from the distribution of the data that there are significant overlapping emotions for the memes, which essentially validates the challenges discussed at the beginning. It can also be observed that majority of the memes are sarcastic in nature. Interestingly, most of the funny memes fall under the category of sarcastic class. Simultaneously, another noteworthy observation is that a significant number of memes are both motivational and offensive.
For the challenge, 1K samples were provided as trial data, 6992 samples as training data while 1879 samples as test data.  

Baseline Model
Memes are the types of multi-modal content, wherein the graphical content and textual messages are self-sufficient to convey some meaning. But within the context of their dissemination, it is a specially tailored idea, that is designed to be propagated. Such complex ideas cannot be conveyed as effectively by any of the constituent data modality, as by their combination. In order to fully address the system modeling tasks that use such data, it is imperative to study the efficacy of individual content modality ie. image or text as well as their combination.

Textual cues for Memotion analysis
A meme can be expressed using varying textual contents, so as to convey different emotions. In some cases, different memes can have same images, but due to the different textual messages embedded in each of them, different sentimental reactions can be induced from all. Recognition of the emotion induced in such memes would require accurate modelling of the textual influence. To evaluate automated emotion recognition from the meme textual content, we built text binary classifier as shown in the bottom half of Fig. 3, to understand different classes of emotion. We have used 100-D pre-trained Glove word embeddings (Pennington et al., 2014) to generate word embeddings from text emd(txt). These embeddings Figure 2: A Multi-level system for the task of emotion intensity prediction (1×14 dimensional), using the emotion class multi-label output (1×4 dimensional). are given as input x i to the CNN, having 64 filters of size 1×5 with Relu as activation function to extract the textual features. To reduce the dimension of number of parameters generated by CNN layer we have used 1D maxpooling layer of size 2. Weighted CNN output is given as input to LSTM where we get a feature vector s t . s t is fed to fully-connected layer, and activation function sigmoid is used to classify the text with binary cross-entropy as a loss function.

Visual cues for Memotion analysis
To comprehend the significance of image in deducing the humour of a meme, we have used many pre-trained models like VGG-16 (Simonyan and Zisserman, 2015), ResNet-50 (He et al., 2015), AlexNet (Krizhevsky et al., 2012)to extract the features of an image ,but VGG-16 has given better features in comparison to other networks due to it's capability of extracting both high and low level features. To maintain uniformity in images, we have resized the image into 224×224×3 from the original Image I, the resized image X i is fed into vgg16 as input for feature extraction. Extracted feature Y i from vgg16 for the given meme is flattened by flatten layer. Flatten output Y i is fed to a fully connected network, and sigmoid function is used for classification with a loss function of binary cross entropy. The system performs weighted averaging of image-text outputs to evaluate the class-wise scores.

Visuo-Lingual modeling for Memotion analysis
The information contained within a meme in any mode whether text or image, needs to be analyzed for recognizing the emotion associated with it. Analyses of the results obtained show that features extracted from Section 5.2 or 5.1 alone are not sufficient to recognize the emotion of a meme. So we created the classifier for leveraging the combination of both image and text based model training, by performing weighted averaging as shown in Fig. 3, which resulted in better predictions. The model, shown in Fig. 3, predicts the output for each class as I 1 &amp; T 1 for sarcastic, I 2 &amp; T 2 for humour, I 3 &amp; T 3 for offense and I 4 &amp; T 4 for motivation emotions, where I i is for image based model and T i is for text based model. To combine predicted probabilities of image and text, we have used softmax function X i with a weighted average of the obtained probabilities of image and text classifier. In our work we have used weighted average for scaling the predicted output and find the threshold H to generate final 1×4 output vector to show how a meme is classified into multiple classes, where H is the average of image and text based network outputs, across the data-set size. The final output O, which generates a 1×4 vector for a given meme, is used as decision vector to activate the next level of emotion subclass to understand the intensity of emotion with respect to parent class.

Predicting the Intensity of Memes
To understand the intensity of an individual emotion associated with a meme, we have created a multilabel classifier with two levels where at the first level meme is classified as sarcastic, offensive, motivational and humorous and the second level predicts the intensity of a particular class depending upon the vector generated at the first level, as depicted in Fig. 2. Obtain predicted output in a vector form, of size 1×4 from first level, i.e., emotion classification. In Fig. 2, the decision system takes decision vector and activates the next multi-class classifier depending upon the value of the corresponding class in the vector. As we have a total of 14 classes (4 each of humour, sarcasm and offense while 2 of motivation), the final output that is the predicted intensity of each class will be a vector of size 1×14.
The performance of the system is shown in  

Evaluation Metric
The challenge comprises of classifying the sentiment and emotion associated with a meme. The task A is a multi-class problem involved in identifying the sentiment (positive, negative, neutral) associated with a meme while the other 2 tasks B and C are multi-label classification problem associated with emotion detection. There are various evaluation metrics for multi-class and multi-label classification problem such as hamming loss, exact match ratio, macro/micro F1 score etc. The most used metric for this kind of problem is hamming loss which is evaluated as the fraction of wrongly classified label to the total number of labels. As our problem deals with different emotions associated with a meme, we have used macro F1 score that will help us to evaluate and analyse the individual class performance.

Participation and Top Performing Systems
The challenge was a great success, involving total of 583 participants, with varying submissions in different tasks comprising of 31, 26 and 23 submissions in Task A, Task B and Task C respectively where in evaluation phase, a user is allowed for 5 submissions per day. 27 teams submitted the system description paper. A brief description of the task wise top performing models is shown below.

Top 3 Task A systems @Memotion
• IITK Vkeswani: Employed wide variety of methods, ranging from a simple linear classifier such as FFNN, Naive Bayes to transformers like MMBT (Rahman et al., 2019) and BERT (Devlin et al., 2019). Implemented the model considering only text and the combination of image and text.
• Guoym: Used ensembling Method considering the textual features extracted using Bi-GRU, BERT (Devlin et al., 2019), or ELMo (Peters et al., 2018), image features extracted by Resnet50 (He et al., 2015) network and fusion features of text and images.
• Aihaihara: Implemented the model that is a concatenation of visual and textual features obtained from n-gram language model and VGG-16 (Simonyan and Zisserman, 2015) pretrained model respectively.

Top 3 Task B and Task C systems @Memotion
• UPB George: In order to extract most salient features from text input, they opted to use the AL-BERT (Lan et al., 2019)model while VGG -16 (Simonyan and Zisserman, 2015) is used for extracting the visual features from image input. To determine the humour associated with a meme, they have concatenated the visual and textual features followed by an output layer of softmax.
• Souvik Mishra Kraken: Applied Transfer learning by using hybrid neural Naïve-Bayes Support Vector Machine and logistic regression for solving the task of humour classification and significant score.
• Hitachi: They have proposed simple but effective MODALITY ENSEMBLE that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. They fine-tuned four pre-trained visual models (i.e., Inception-ResNet (Szegedy et al., 2016), Polynet (Zhang et al., 2016), SENet (Hu et al., 2017), and PNAS-Net (Liu et al., 2017)) and four textual models (i.e., BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2018), Transformer-XL , and XLNet ), followed by the fusion of their predictions by ensemble methods to effectively capture cross-modal correlations.  8 Results, Analysis, and Takeaway points from Memotion 1.0  5, 6 and 7 shows the best scores of the all the participants and the comparison with the baseline model whereas Table 8 shows a summary of the models employed by different participants. Some of the noteworthy points regarding various techniques and consideration of different modals is described in the subsequent sections.  

Unimodal vs Multi-modal
• Considering only text: Steve and CSECU KDE MA used only textual features to determine the humour as well as the corresponding intensity. System of steve include a Logistic Regression baseline, a BiLSTM + Attention-based learner and a transfer learning approach with BERT while CSECU KDE MA applied fastext forward embedding followed by convolution layers with multiple kernel sizes and time distribution LSTM with attention mechanism. Results are quite significant but less than the models implemented considering the combination of image and text.
• Combination of image+text: Most participant's approach include fusion of visual and textual features extracted using different models as shown in Table 8. Teams HonoMi Hitachi, Lisa Bonheme and Yingmei Guo proposed ensemble learning where as teams Sunil, DelaPen and many others have used multimodal approaches, few teams even have performed transfer learning on pre-trained models of BERT (Devlin et al., 2019), VGG-16 (Simonyan andZisserman, 2015) , ResNet, etc.

Techniques based on
• Visual approaches: Pretrained models like Inception-ResNet (He et al., 2015),Polynet (Zhang et al., 2016), SENet (Hu et al., 2017) and the popular off-the-shelf systems like VGG-16 (Simonyan and Zisserman, 2015) and ResNET (He et al., 2015) are significantly leveraged as part of the visual feature extractors.
• Textual feature extraction approaches: For modeling the content based on textual format, techniques like BiLSTM, BIGRU, and Attention models are used to perform cross domain suggestion  mining. Besides these, BiLSTM+Attention based learner and a transfer learning approach with BERT (Devlin et al., 2019) is also used for analysing the text.
• Approaches to handle categorical imbalance: Interestingly, to address the inherent skewness within the categorical data distribution at different levels, approaches like GMM and Training Signal Annealing (TSA) are used.

Special Mentions
In addition to the description of top performing models, we have some unique systems implemented by various participants, summarized below:  ous experiments, they have shown that considering either of the text or image performs better than considering the combination of both.
• Pradyumn Gupta: Proposed a system which uses different bimodal fusion techniques like GMU, early and late fusion to leverage the inter-modal dependency for sentiment and emotion classification tasks. To extract visual features, they have used facial expression, face emotions and different pretrained deep learning models like ResNet-50 (He et al., 2015), AlexNet (Krizhevsky et al., 2012).
To understand the textual information associated with a meme, BiLSTM, RoBERTa are used.

Memotion Analysis -the next Horizon!
The submissions that we received also came along with their respective analytical reasoning, towards ascertaining whether an image or a text or their combination contributes towards modeling the associated emotions from memes. Most of the analyses provided present conflicting views, regarding the importance of a particular content modality. This essentially reinstates the requirement of further investigations into better approaches towards modeling the affect related information from the multi-modal content like memes. The complexity of understanding the emotions from a meme arises primarily due to the interaction of both image and embedded text. Although, few results reported are better when evaluating over either image or text, a human always attempts to take cognizance of both image and text to understand the meaning intended. The challenge is highlighted more by memes which are domain specific, i.e. based on a popular Movie or TV Show. While there are several State-of-the-Art deep learning based systems that leverage data intensive training approaches, that perform various tasks at par with humans on both image and especially for text, still there is a lot more space for applications involving multi-modality like memes, to drive the required progress. Recently Facebook proposed a challenge (Kiela et al., 2020) to classify the meme as Hateful and Not Hateful content.
At present, memes have become one of the most prominent ways of expressing an individual's opinion towards societal issues. Further on classifying the emotion of memes, this work can be extended as follows:
• Properly annotated meme data is still scarce. We plan to enrich our data-set with annotations for different language memes (Hinglish, Spanglish etc).
• The success of Memotion 1.0 motivates us to go further and organize similar events in the future.
• The emotion classification could be further extended to develop a meme recommendation system as well as establishing a AI algorithm that could flag the offensive meme from social media platforms automatically.

