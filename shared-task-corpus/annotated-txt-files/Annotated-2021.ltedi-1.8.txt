title
Findings of the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion
abstract
Hope plays a crucial role in the well-being, recuperation and restoration of human life. Hope speech reflects the belief that one can discover pathways to their desired objectives and become motivated to utilise those pathways. We created a hope speech detection dataset to encourage research in Natural Language Processing (NLP) towards reinforcement of positivity rather than minimising negativity (through control of hate speech etc.). In this paper, we report the findings of the shared task of hope speech detection for Tamil, English, and Malayalam languages conducted as a part of the EACL 2021 workshop on Language Technology for Equality, Diversity, and Inclusion (LT-EDI-2021). We also present an overview of the methods and results of the competing systems. The datasets for this challenge are openly available 1 . To the best of our knowledge, this is the first shared task to conduct hope speech detection.

Introduction
In the recent years, there has been an exponential rise in the number of studies focusing on the detection and management of hate speech and offensive language in social media (Kumar et al., 2018;Mandl et al., 2020;Zampieri et al., 2020). However, this has led to controlling user expression instead of improving user experience. (Ghanghor et al., 2021a;Hegde et al., 2021;Yasaswini et al., 2021). This has also resulted in putting barriers on modes of expression of different groups of people resulting in the violation of the principles of Equality, Diversity, and Inclusion (EDI). For instance, some NLP systems have classified the comments made in African American English as offensive language without accommodating the linguistic features peculiar to the dialect. Hence there is a need for a shift in the approaches taken to handle hate speech without compromising on the principles of EDI.
We propose to shift the prevailing research direction in Natural Language Processing from controlling negativity (curbing hate speech etc.) to encouraging positivity (promoting hope speech). Hope speech is any expression that is positive, encouraging, supportive, and / or inspires promise of the future (Chakravarthi, 2020a). For the shared task organised in this connection, participants were provided with development, training and test dataset in English as well as in two under-resourced languages -Tamil and Malayalam. Tamil (ISO 639-3: tam) belongs to the Dravidian language family and is widely spoken in the southern state of Tamil Nadu in India, Sri Lanka, Malaysia and Singapore (Krishnamurti, 2003;Kolipakam et al., 2018;Chakravarthi, 2020b;Mahesan, 2019, 2020a,b). Malayalam (ISO 639-3: mal) also belongs to the Dravidian language family and is spoken in the Indian state of Kerala and the Union Territories of Lakshdweep and Puducherry (Chakravarthi et al., 2020). Both Tamil and Malayalam have their own scripts which are alphasyllabaries like other Indic scripts i.e. they are partially alphabetic and partially syllabic. The Tamil language was written using Tamili, Vattezhuthu, Chola, Pallava and Chola-Pallava scripts at different points in history. The modern Tamil script descended from the Chola-Pallava script that was conceived around the 6th century CE (Srinivasan, 2019). Malayalam was first written with Vattezhuthu script that evolved from Tamili script around 4-5th century CE (Mahadevan, 2003). Modern Malayalam is written using the Vattezhuttu alphabets extended with symbols from the Grantha script to accomodate non-native Sanskrit sounds (Krishnamurti, 2003). Although they have their own scripts, the social media comments in these languages are often written in Latin script as it is easy to input.
Our dataset for the hope speech for EDI shared task was created from user-generated content in Tamil, Malayalam, and English (Chakravarthi and Muralidaran, 2021). The user-generated comments in our dataset for Tamil and Malayalam were codemixed (Chakravarthi, 2020a). We proposed a comment/post level classification task. The goal for the participants was to <TASK>classify a given Youtube comment into either 'Hope speech', 'Non hope speech' or 'Not Tamil / Not Malayalam / Not English'</TASK>. Our CodaLab website will remain open to allow researchers to access the data and build upon this work.

Task Description
We define the hope speech for our problem as "YouTube comments/posts that offer support, reassurance, suggestions, inspiration and insight". A comment/post within the corpus may contain more than one sentence, but the average sentence length of the corpus is one. The annotations in the corpus are made at a comment/post level. The participants were provided with development, training and test dataset in English, Tamil, and Malayalam. A few examples from the dataset are given below along with their translations and hope speech class annotations. 

Datasets
We have used the HopeEDI datasets from Chakravarthi (2020a) for this shared task. Our dataset contained user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as hope speech or not. The inter-annotator agreement of the dataset labels was verified using Krippendorff's alpha. We will now briefly describe the annotation process and the statistics of the collected dataset. For detailed explanation of the datasets, please refer to the works of Chakravarthi (2020a). The data was collected on a wide range of socially relevant topics related to EDI, including LGBTIQ issues, COVID-19, women in STEM, Black Lives Matter, Dravidian languages, and the Indo-China war.
The data for English was collected from Englishspeaking regions across the world. For Tamil and Malayalam, we collected the data from the states of Tamil Nadu and Kerala in India. Full consideration was given to minimize the risk associated with breach of privacy by removing the information related to individual identity from a comment before it was sent for annotation. The annotators were educated about EDI and a minimum of three annotators labelled each comment. Our diverse annotator base was from Australia, the Republic of Ireland, the United Kingdom, the United States of America, Tamil Nadu, Kerala (in India) and Sri Lanka. The inter-annotator agreement in terms of Krippendorff's alpha (α) was 0.63, 0.76, anad 0.85 for English, Tamil and Malayalam.
In total, HopeEDI dataset contains 59,354 comments with 28,451 comments in English,20,198 comments in Tamil,and 10,705 comments in Malayalam. Table  1 and Table 2 show the distribution of HopeEDI dataset by language and annotation label respectively. To calculate corpus statistics nltk tool (Bird et al., 2009) was used to tokenise the sentences in the comments. The vocabulary size of the Tamil and Malayalam dataset was high due to different types of code-mixing in the user-generated content.
Most of the comments fell under the 'Not hope' category because of how we defined hope speech and hence the distribution of classes in the dataset was skewed. It is typical for user-generated content from online social media platforms to be skewed and the systems should be able to handle imbalanced data common in real-life setting. 'not Tamil' and 'not Malayalam' labels are found in considerably large amount due to high code-mixing. The fully annotated dataset was split into training, de-     3.

Training Phase
In this first phase, training, validation and development data were released for the participants to train and develop hope speech detection for one or more of the three languages. The participants could choose to perform cross-validation on the training data or use the validation dataset for preliminary evaluations and use the development set for hyperparameter sharing. The goal of this phase was to ensure that the systems developed by the participants be ready for evaluation before the release of the test data. In total, 137 participants registered for all the three languages and downloaded the data.

Testing Phase
In this phase, the test dataset was released in Co-daLab without the gold labels. Participants were instructed to submit their predictions using Google Forms. They were allowed to submit their results as many times as they wanted to and their best submission was chosen for evaluation and preparation of rank list. The predictions were evaluated against the gold standard labels. The performance of the classification system was measured in terms of weighted averaged Precision, Recall and F-Score across all the classes. Weighted averaged scores calculate the support-weighted mean per label. The metric used for the preparing the rank list was the weighted F1 score. Participants were encouraged to check their system with Sklearn classification report 2 . For Tamil, Malayalam and English languages, the number of participants were 30, 31 and 31 respectively for the final test.

Systems
We first describe the baseline systems from the HopeEDI paper. Next, we briefly describe the general approach taken by each participating team. We encourage the readers to refer to the individual papers of each team for more details.

Baseline Classifiers
We used the prior published work as the baseline for hope speech detection from social media. More details about the baseline system description and results can be seen in Chakravarthi (2020a). In this prior work, we described in detail our dataset and presented our baseline results by employing a wide range of standard classifiers on the unbalanced settings of the dataset. The experiment was applied on the token frequency-inverse document frequency (Tf-Idf) of tokens. We used sklearn 3 library to create baseline classifiers. We used a grid search for the k-nearest neighbours (KNN), support vector machine (SVM), decision tree and logistic regression. For the multinomial Naive Bayes, we set alpha = 0.7. More details about the parameters of the classifier will be published in the code.

System Descriptions
In this section we summarise the systems implemented by the participants for the shared task. For more details, please refer to the shared task papers submitted by the authors.
• Chinnappa (2021) participated in identifying hope speech classes in English, Tamil, and Malayalam datasets. They presented a twophase mechanism to detect hope speech. In the first phase, they built a classifier to identify the language of the text. In the second phase, they created a classifier to identify the class labels. The author used language Models SBERT, FNN and BERT inference. They achieved 3rd, 4th, and 2nd ranks in Tamil, Malayalam and English respectively.
• M K and A P ( 2021  (Puranik et al., 2021) 0.38 0.39 0.37   the Stratified-K-Fold method to address class imbalance. They achieved a weighted average F1-score of 0.59, 0.84, and 0.92 for Tamil, Malayalam, and English languages, which ranked 3rd, 2nd, and 2nd respectively.
• Huang and Bai (2021) used the method and model that combines the XLM-RoBERTa preraining language model and the TF-IDF algorithm. They got 1st, 2nd, and 3rd ranks on the English, Malayalam, and Tamil dataset respectively.
• Chen and Kong (2021) used fine-tuned BERT and K fold cross-validation to accomplish classification on English dataset. They achieved a final F1 score of 0.93, and got 1st rank for English language.
• Ziehe et al. (2021) demonstrated that even very simple baseline algorithms perform reasonably well on this task if provided with enough training data. However, their best performing algorithm is a cross-lingual transfer learning approach in which they fine-tuned XLM-RoBERTa. The model achieved the 1st rank for Malayalam and English and the 4th rank for Tamil.
• Mahajan et al. (2021), in their paper, described their approach of fine-tuning RoBERTa for Hope Speech detection in English and finetuning XLM-RoBERTa for Hope Speech detection in Tamil and Malayalam languages. They ranked 1st in English (F1 = 0.93), 1st in Tamil (F1 = 0.61) and 3rd in Malayalam (F1 = 0.83).
• Gundapu and Mamidi (2021) describe a Transformer-based BERT model for Hope speech detection. Their model achieved a weighted averaged F1-score of 0.93 on the test set for English. They showed that BERT model helped for better contextual representation of words in a comment, and the language identification model-assisted in detecting 'Other language' comments. They also explored with other transformer models like RoBERTa, XLNet, Albert, FLAIR, and ELMo for a superior hope speech detection.
• S et al. (2021b) proposed a BiLSTM with attention based approach in solving hope speech detection and using this approach they achieved an F1 score of 0.73 (9th rank) in Malayalam-English dataset.
• Upadhyay et al. ( 2021) experimented with two approaches. In the first approach, they used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models. The second approach involved using a majority voting ensemble of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, AL-BERT, RoBERTa, IndicBERT) after adding an output layer. They found that the second approach was superior for English, Tamil and Malayalam. They got a weighted F1 score of 0.93, 0.75 and 0.49 for English,Malayalam and Tamil respectively. They ranked 1st in English, 8th in Malayalam and 11th in Tamil.
• Awatramani (2021) achieved an F-score of 0.93, ranking 1st on the leaderboard for English comments. The paper used pre-trained transformers and Paraphrasing Generation for Data Augmentation.
• Hossain et al. (2021) employed various machine learning (SVM, LR, ensemble), deep learning (CNN+BiLSTM) and transformer (m-BERT, Indic-BERT, XLNet, XLM-R) based methods. They showed that XLM-R outperformed all other techniques by gaining a weighted F1-score of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Their team achieved 1st , 2nd and 1st rank in these three tasks respectively.
• Que (2021) used XLM-Roberta model and proposed an excellent multilingual model to achieve the classification task.
• Balouchzahi et al. (2021)  training and then using pre-trained BERT LM as weights in BiLSTM-Conv1d model. Out of the three proposed models, CoHope-ML model (best one among the models proposed) obtained 1st, 2nd, and 3rd ranks with weighted F1-scores of 0.85, 0.92, and 0.59 for Ma-En, English and Ta-En texts respectively.
• Sharma and Arora (2021) extends that of (Arora, 2020a) as they used their strategy to synthetically generate code-mixed data for training a transformer-based model RoBERTa and used it in an ensemble along with their pretrained ULMFiT. They presented RoBERTa language model for code-mixed Tamil which they pre-trained from scratch. Using transfer learning they fine-tune RoBERTa and ULM-FiT language models on down-stream tasks of OLI and HSD. They got Rank 4 in the former task using an ensemble of classifiers trained on RoBERTa and ULMFiT and Rank 1 in the latter task using classifier based on ULMFiT.

Results and Discussion
Overall, we received a total of 31,31 and 30 submissions for English, Malayalam and Tamil tasks. It is interesting to note that the top performing teams in all the three languages predominantly used XLM-Roberta to complete the shared task. One of the top ranking teams for English used context-aware string embeddings for word representations and Recurrent Neural Networks and pooled document embeddings for text representation. Among the other submissions, although Bi-LSTM was popular, there were other machine learning and deep learning models that were used. However, they did not achieve good results compared to the Roberta based models. The top scores were 0.61, 0.85 and 0.93 for Tamil, Malayalam and English respectively. The range of scores was between 0.37 to 0.61, 0.49 to 0.85 and 0.61 to 0.93 for Tamil, Malayalam and English datasets respectively. It can be seen that the F1 scores of all the submissions on the Tamil dataset were considerably lower than those of Malayalam and English. It is not suprising that the English scores were better because many approaches used variations of pretrained transformer based models trained on English data. Due to codemixing at various levels the scores are naturally lower for Malayalam and Tamil datasets. Among these two, the systems submitted performed badly on Tamil data. The identification of the exact reasons for the bad performance in Tamil requires further research. However, one possible explanation for this could be that the distribution of 'Hope speech', 'Non hope speech' and 'not-Tamil' classes is starkly different from those of English and Malayalam. In the English dataset there were just 27 out of 28,451 comments which were labelled as not-English by the annotators. In the remaining two classes, the number of non-hope speech comments were significantly higher than hope speech comments. Similarly in Malayalam there were only 888 out of 10,705 comments that were not Malayalam and overwhelmingly higher proportion of non-hope speech in the dataset. However in Tamil dataset, there were 2,483 comments out of 20,198 comments that were not Tamil. Moreover the proportion of Non-hope speech comments in Tamil (9,816 comments) is close enough to the number of hope speech labels (7,899 comments). This made the classifications in English and Malayalam a binary prediction task rather than a three class classification and naturally the former systems performed better than the latter.
The participants rarely faced challenges during the training, testing and submission phase. However, there were minor inconsistencies in the labelling of classes that was duly rectified on Codalab immediately. The intial round of peer review offered significant suggestions to the participants in terms of rewriting paper in a more presentable way. These suggestions of the reviewers were duly incorporated before the final submissions.

Conclusion
In this paper, we have presented the first shared task results on hope speech detection for equality, diversity, and inclusion. We received a wide range of entries that satisfied the goals of the shared task. We hope that HopeEDI shared task makes a lasting contribution to the NLP field.

