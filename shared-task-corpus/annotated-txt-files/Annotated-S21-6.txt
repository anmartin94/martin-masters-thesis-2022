title
SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images
abstract
We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. <TASK>The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image</TASK>. It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.

Introduction
Internet and social media have amplified the impact of disinformation campaigns. Traditionally a monopoly of states and large organizations, now such campaigns have become within the reach of even small organisations and individuals (Da San Martino et al., 2020b).
Such propaganda campaigns are often carried out using posts spread on social media, with the aim to reach very large audience. While the rhetorical and the psychological devices that constitute the basic building blocks of persuasive messages have been thoroughly studied (Miller, 1939;Weston, 2008;Torok, 2015), only few isolated efforts have been made to devise automatic systems to detect them .
WARNING: This paper contains meme examples and wording that might be offensive to some readers. Thus, in 2020, we proposed SemEval-2020 task 11 on Detection of Persuasion Techniques in News Articles, with the aim to help bridge this gap (Da San Martino et al., 2020a). The task focused on text only. Yet, some of the most influential posts in social media use memes, as shown in Figure 1, 1 where visual cues are being used, along with text, as a persuasive vehicle to spread disinformation (Shu et al., 2017). During the 2016 US Presidential campaign, malicious users in social media (bots, cyborgs, trolls) used such memes to provoke emotional responses (Guo et al., 2020).
In 2021, we introduced a new SemEval shared task, for which we prepared a multimodal corpus of memes annotated with an extended set of techniques, compared to SemEval-2020 task 11. This time, we annotated both the text of the memes, highlighting the spans in which each technique has been used, as well as the techniques appearing in the visual content of the memes.
Based on our annotations, we offered the following three subtasks: Subtask 1 (ST1) Given the textual content of a meme, identify which techniques (out of 20 possible ones) are used in it. This is a multilabel classification problem.
Subtask 2 (ST2) Given the textual content of a meme, identify which techniques (out of 20 possible ones) are used in it together with the span(s) of text covered by each technique. This is a multilabel sequence tagging task.
Subtask 3 (ST3) Given a meme, identify which techniques (out of 22 possible ones) are used in the meme, considering both the text and the image. This is a multilabel classification problem.
A total of 71 teams registered for the task, 22 of them made an official submission on the test set and 15 of the participating teams submitted a system description paper.

Related Work
Propaganda Detection Previous work on propaganda detection has focused on analyzing textual content (Barrón-Cedeno et al., 2019;Rashkin et al., 2017). See  for a recent survey on computational propaganda detection. Rashkin et al. (2017) developed the TSHP-17 corpus, which had document-level annotations with four classes: trusted, satire, hoax, and propaganda. Note that TSHP-17 was labeled using distant supervision, i.e., all articles from a given news outlet were assigned the label of that news outlet. The news articles were collected from the English Gigaword corpus (which covers reliable news sources), as well as from seven unreliable news sources, including two propagandistic ones. They trained a model using word n-grams, and reported that it performed well only on articles from sources that the system was trained on, and that the performance degraded quite substantially when evaluated on articles from unseen news sources. Barrón-Cedeno et al. (2019) developed a corpus QProp with two labels (propaganda vs. non-propaganda), and experimented with two corpora: TSHP-17 and QProp . They binarized the labels of TSHP-17 as follows: propaganda vs. the other three categories.
They performed massive experiments, investigated writing style and readability level, and trained models using logistic regression and SVMs. Their findings confirmed that using distant supervision, in conjunction with rich representations, might encourage the model to predict the source of the article, rather than to discriminate propaganda from non-propaganda. The study by Habernal et al. (2017 also proposed a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring, and irrelevant authority, which directly relate to propaganda techniques.
A more fine-grained propaganda analysis was done by Da San , who developed a corpus of news articles annotated with the spans of use of 18 propaganda techniques, from an invetory they put together. They targeted two tasks: (i) binary classification -given a sentence, predict whether any of the techniques was used in it; and (ii) multi-label multi-class classification and span detection task -given a raw text, identify both the specific text fragments where a propaganda technique is being used as well as the type of technique. They further proposed a multigranular gated deep neural network that captures signals from the sentence-level task to improve the performance of the fragment-level classifier and vice versa. Subsequently, an automatic system, Prta, was developed and made publicly available (Da San Martino et al., 2020c), which performs fine-grained propaganda analysis of text using these 18 fine-grained propaganda techniques.

Multimodal Content
Another line of related research is on analyzing multimodal content, e.g., for predicting misleading information , for detecting deception (Glenski et al., 2019), emotions and propaganda (Abd Kadir et al., 2016), hateful memes (Kiela et al., 2020), and propaganda in images (Seo, 2014).  developed a corpus of 500K Twitter posts consisting of images and labeled with six classes: disinformation, propaganda, hoaxes, conspiracies, clickbait, and satire. Glenski et al. (2019) explored multilingual multimodal content for deception detection. Multimodal hateful memes were the target of the Hateful Memes Challenge, which was addressed by fine-tuning state-of-art methods such as ViLBERT (Lu et al., 2019), Multimodal Bitransformers (Kiela et al., 2019), and VisualBERT (Li et al., 2019) to classify hateful vs. not-hateful memes (Kiela et al., 2020).
Related Shared Tasks The present shared task is closely related to SemEval-2020 task 11 on Detection of Persuasion Techniques in News Articles (Da San Martino et al., 2020a), which focused on news articles, and asked (i) to detect the spans where propaganda techniques are used, as well as (ii) to predict which propaganda technique (from an inventory of 14 techniques) is used in a given text span. Another closely related shared task is the NLP4IF-2019 task on Fine-Grained Propaganda Detection, which asked to detect the spans of use in news articles of each of 18 propaganda techniques . While these tasks focused on the text of news articles, here we target memes and multimodality, and we further use an extended inventory of 22 propaganda techniques.
Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on predicting the veracity of rumors in Twitter (Derczynski et al., 2017;Gorrell et al., 2019), the SemEval-2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), the NLP4IF-2021 shared task on Fighting the COVID-19 Infodemic (Shaar et al., 2021). We should also mention the CLEF 2018-2021 CheckThat! lab Elsayed et al., 2019a,b;, which featured tasks on automatic identification  and verification Hasanain et al., 2019Nakov et al., 2021) of claims in political debates and social media. While these tasks focused on factuality, check-worthiness, and stance detection, here we target propaganda; moreover, we focus on memes and on multimodality rather than on analyzing the text of tweets, political debates, or community question answering forums.

Persuasion Techniques
Scholars have proposed a number of inventories of persuasion techniques of various sizes (Miller, 1939;Torok, 2015;Abd Kadir and Sauffiyan, 2014). Here, we use an inventory of 22 techniques, borrowing from the lists of techniques described in , (Shah, 2005) and (Abd Kadir and Sauffiyan, 2014). Among these 22 techniques, the first 20 are applicable to both text and images, while the last two, Appeal to (Strong) Emotions and Transfer, are reserved for images.
Below, we provide a definition for each of these 22 techniques; more detailed instructions of the annotation process and examples are provided in Appendix A.
1. Loaded Language: Using specific words and phrases with strong emotional implications (either positive or negative) to influence an audience.
2. Name Calling or Labeling: Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable, or loves, praises.
3. Doubt: Questioning the credibility of someone or something.
4. Exaggeration or Minimisation: Either representing something in an excessive manner, e.g., making things larger, better, worse ("the best of the best", "quality guaranteed"), or making something seem less important or smaller than it really is, e.g., saying that an insult was just a joke.
5. Appeal to Fear or Prejudices: Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases, the support is built based on preconceived judgments.
6. Slogans: A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.

Whataboutism:
A technique that attempts to discredit an opponent's position by charging them with hypocrisy without directly disproving their argument.
8. Flag-Waving: Playing on strong national feeling (or positive feelings toward any group, e.g., based on race, gender, political preference) to justify or promote an action or idea.
9. Misrepresentation of Someone's Position (Straw Man): When an opponent's proposition is substituted with a similar one, which is then refuted in place of the original proposition.
10. Causal Oversimplification: Assuming a single cause or reason, when there are actually multiple causes for an issue. It includes transferring blame to one person or group of people without investigating the actual complexities of the issue.
11. Appeal to Authority: Stating that a claim is true because a valid authority or expert on the issue said so, without any other supporting evidence offered. We consider the special case in which the reference is not an authority or an expert as part of this technique, although it is referred to as Testimonial in the literature.
12. Thought-Terminating Cliché: Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract the attention away from other lines of thought.
13. Black-and-White Fallacy or Dictatorship: Presenting two alternative options as the only possibilities, when in fact more possibilities exist. As an extreme case, tell the audience exactly what actions to take, eliminating any other possible choices (Dictatorship).
14. Reductio ad Hitlerum: Persuading an audience to disapprove of an action or an idea by suggesting that the idea is popular with groups that are hated or in contempt by the target audience. It can refer to any person or concept with a negative connotation.
15. Repetition: Repeating the same message over and over again, so that the audience will eventually accept it.
16. Obfuscation, Intentional Vagueness, Confusion: Using words that are deliberately not clear, so that the audience can have their own interpretations.
17. Presenting Irrelevant Data (Red Herring):
Introducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made.
18. Bandwagon Attempting to persuade the target audience to join in and take the course of action because "everyone else is taking the same action."
19. Smears: A smear is an effort to damage or call into question someone's reputation, by propounding negative propaganda. It can be applied to individuals or groups.
20. Glittering Generalities (Virtue): These are words or symbols in the value system of the target audience that produce a positive image when attached to a person or an issue.
21. Appeal to (Strong) Emotions: Using images with strong positive/negative emotional implications to influence an audience.
22. Transfer: Also known as Association, this is a technique that evokes an emotional response by projecting positive or negative qualities (praise or blame) of a person, entity, object, or value onto another one in order to make the latter more acceptable or to discredit it.

Dataset
The annotation process is explained in detail in Appendix A, and in this section, we give a just brief summary. We collected English memes from our personal Facebook accounts over several months in 2020 by following 26 public Facebook groups, which focus on politics, vaccines, COVID-19, and gender equality. We considered a meme to be a "photograph style image with a short text on top of it", and we removed examples that did not fit this definition, e.g., cartoon-style memes, memes whose textual content was strongly dominant or non-existent, memes with a single-color background image, etc. Then, we annotated the memes using our 22 persuasion techniques. For each meme, we first annotated its textual content, and then the entire meme. We performed each of these two annotations in two phases: in the first phase, the annotators independently annotated the memes; afterwards, all annotators met together with a consolidator to discuss and to select the final gold label(s).
The final annotated dataset consists of 950 memes: 687 memes for training, 63 for development, and 200 for testing. While the maximum number of sentences in a meme is 13, the average number of sentences per meme is just 1.68, as most memes contain very little text.
Table 1 shows the number of instances of each technique for each of the tasks. Note that Transfer and Appeal to (Strong) Emotions are not applicable to text, i.e., to Subtasks 1 and 2. For Subtasks 1 and 3, each technique can be present at most once per example, while in Subtask 2, a technique could appear multiple times in the same example. This explains the sizeable differences in the number of instances for some persuasion techniques between Subtasks 1 and 2: some techniques are over-used in memes, with the aim of making the message more persuasive, and thus they contribute higher counts to Subtask 2.  Note that the number of instances for Subtasks 1 and 3 differs, and in some cases by quite a bit, e.g., for Smears, Doubt, and Appeal to Fear/Prejudice. This shows that many techniques cannot be found in the text, and require the visual content, which motivates the need for multimodal approaches for Subtask 3. Note also that different techniques have different span lengths, e.g., Loaded Language and Name Calling are about 2-3 words long, e.g., violence, mass shooter, and coward. However, for techniques such as Whataboutism, the average span length is 22 words.
Figure 2 shows statistics about the distribution of the number of persuasion techniques per meme. Note the difference for memes without persuasion techniques between Figures 2a and 2c: we can see that the number of memes without any persuasion technique drastically drops for Subtask 3. This is because the visual modality introduces additional context that was not available during the text-only annotation, which further supports the need for multimodal analysis. The visual modality also has an impact on memes that already had persuasion techniques in the text-only phase.
We observe that the number of memes with only one persuasion technique in Subtask 3 is considerably lower compared to Subtask 1, while the number of memes with three or more persuasion techniques has greatly increased for Subtask 3. 5 Evaluation Framework

Evaluation Measures
Subtasks 1 and 3 To measure the performance of the systems, for Subtasks 1 and 3, we use Micro and Macro F 1 , as these are multi-class multi-label tasks, where the labels are imbalanced. The official measure for the task is Micro F 1 .
Subtask 2 For Subtask 2, the evaluation requires matching the text spans. Hence, we use an evaluation function that gives credit to partial matches between gold and predicted spans.
Let document d be represented as a sequence of characters. The i-th propagandistic text fragment is then represented as a sequence of contiguous characters t ⊆ d. A document includes a set of (possibly overlapping) fragments T . Similarly, a learning algorithm produces a set S with fragments s ⊆ d, predicted on d. A labeling function l(x) ∈ {1, . . . , 20} associates t ∈ T , s ∈ S with one of the techniques. An example of (gold) annotation is shown in Figure 3, where an annotation t 1 marks the span stupid and petty with the technique Loaded Language. We define the following function to handle partial overlaps of fragments with the same labels:
where h is a normalizing factor and δ(a, b) = 1 if a = b, and 0, otherwise. For example, still with reference to Figure 3
Given Eq. (1), we now define variants of precision and recall that can account for the imbalance in the corpus:
We define (2) to be zero if |S| = 0, and Eq. (3) to be zero if |T | = 0. Following Potthast et al. (2010), in (2) and ( 3) we penalize systems predicting too many or too few instances by dividing by |S| and |T |, respectively. Finally, we combine Eqs. (2) and ( 3) into an F 1 -measure, the harmonic mean of precision and recall.

Task Organization
We ran the shared task in two phases:
Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. A live leaderboard was made available to keep track of all submissions.
Test Phase In the second phase, the test set was released and the participants were given just a few days to submit their final predictions.
In the Development Phase, the participants could make an unlimited number of submissions, and see the outcome in their private space. The best score for each team, regardless of the submission time, was also shown in a public leaderboard. As a result, not only could the participants observe the impact of various modifications in their systems, but they could also compare against the results by other participating teams. In the Test Phase, the participants could again submit multiple runs, but they would not get any feedback on their performance. Only the latest submission of each team was considered as official and was used for the final team ranking. The final leaderboard on the test set was made public after the end of the shared task.
In the Development Phase, a total of 15, 10 and 13 teams made at least one submission for ST1, ST2 and ST3, respectively. In the Test Phase the number of teams who made official submissions was 16, 8, and 15 for ST1, ST2, ST3, respectively.
After the competition was over, we left the submission system open for the development set, and we plan to reopen it on the test set as well.  (Zhu et al., 2021) 13 (Hossain et al., 2021) 15 (Gupta and Sharma, 2021)  

Participants and Results
Below, we give a general description of the systems that participated in the three subtasks and their results, with focus on those ranked among the top-3. Appendix C gives a description of every system.

Subtask 1 (Unimodal: Text)
Table 2 gives an overview of the systems that took part in Subtask 1. We can see that transformers were quite popular, and among them, most commonly used was RoBERTa, followed by BERT. Some participants used learning models such as LSTM, CNN, and CRF in their final systems, while internally, Naïve Bayes and Random Forest were also tried. In terms of representation, embeddings clearly dominated. Moreover, techniques such as ensembles, data augmentation, and post-processing were also used in some systems.
The evaluation results are shown in Table 3, which also includes two baselines: (i) random, and (ii) majority class. The latter always predicts Loaded Language, as it is the most frequent technique for Subtask 1 (see Table 1).
The  The final prediction for MinD averages the probabilities for these models, and further uses postprocessing rules, e.g., each bigram appearing more than three times is flagged as a Repetition.
Team Alpha (Feng et al., 2021) was ranked second. However, they used features from images, which was not allowed (images were only allowed for Subtask 3).
Team Volta  was third. They used a combination of transformers with the [CLS] token as an input to a two-layer feed-forward network. They further used example weighting to address class imbalance.
We should also mention team LeCun, which used additional corpora such as the PTC corpus (Da San Martino et al., 2020a), and augmented the training data using synonyms, random insertion/deletion, random swapping, and backtranslation.

Subtask 2 (Unimodal: Text)
The approaches for this task varied from modeling it as a question answering (QA) task to performing multi-task learning. Table 4 presents a high-level summary. We can see that BERT dominated, while RoBERTa was much less popular. We further see a couple of systems using data augmentation. Unfortunately, there are too few systems with system description papers for this subtask, and thus it is hard to do a very deep analysis.  Table 5 shows the evaluation results. We report our random baseline, which is based on the random selection of spans with random lengths and a random assignment of labels. The best model by team Volta  used various transformer models, such as BERT and RoBERTa, to predict token classes by considering the output of each token embedding. Then, they assigned classes for a given word as the union of the classes predicted for the subwords that make that word (to account for BPEs).
Team HOMADOS (Kaczyński and Przybyła, 2021) was second, and they used a multi-task learning (MTL) and additional datasets such as the PTC corpus from SemEval-2020 task 11 (Da San Martino et al., 2020a), and a fake news corpus (Przybyla, 2020). They used BERT, followed by several output layers that perform auxiliary tasks of propaganda detection and credibility assessment in two distinct scenarios: sequential and parallel MTL. Their final submission used the latter.
Team TeamFPAI (Xiaolong et al., 2021) formulated the task as a question answering problem using machine reading comprehension, thus improving over the ensemble-based approach of Liu et al. (2018). They further explored data augmentation and loss design techniques, in order to alleviate the problem of data sparseness and data imbalance.

Subtask 3 (Multimodal: Memes)
Table 6 presents an overview of the approaches used by the systems that participated in Subtask 3. This is a very rich and very interesting table. We can see that transformers were quite popular for text representation, with BERT dominating, but RoBERTa being quite popular as well. For the visual modality, the most common representations were variants of ResNet, but VGG16 and CNNs were also used. We further see a variety of representations and fusion methods, which is to be expected given the multi-modal nature of this subtask. 11 (Zhu et al., 2021) 13 (Pritzkau, 2021) 15 (Singh and Lefever, 2021)  Table 7 shows the performance on the test set for the participating systems for Subtask 3. The two baselines shown in the table are similar to those for Subtask 1, namely a random baseline and a majority class baseline. However, this time the most frequent class baseline always predicts Smears (for Subtask 1, it was Loaded Language), as this is the most frequent technique for Subtask 3 (as can be seen in Table 1).
Team Alpha (Feng et al., 2021) pre-trained a transformer using text with visual features. They extracted grid features using ResNet50, and salient region features using BUTD. They further used these grid features to capture the high-level semantic information in the images. Moreover, they used salient region features to describe objects and to caption the event present in the memes. Finally, they built an ensemble of fine-tuned De-BERTA+ResNet, DeBERTA+BUTD, and ERNIE-VIL systems.
Team MinD (Tian et al., 2021) combined a system for Subtask 1 with (i) ResNet-34, a face recognition system, (ii) OCR-based positional embeddings for text boxes, and (iii) Faster R-CNN to extract region-based image features. They used late fusion to combine the textual and the visual representations. Other multimodal fusion strategies they tried were concatenation of the representation and mapping using a multi-layer perceptron.
Team 1213Li (Peiguang et al., 2021) used RoBERTa and ResNet-50 as feature extractors for texts and images, respectively, and adopted a label embedding layer with a multi-modal attention mechanism to measure the similarity between labels with multi-modal information, and fused features for label prediction.  

Rank
Conclusion and Future Work
We presented SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. It was a successful task: a total of 71 teams registered to participate, 22 teams eventually made an official submission on the test set, and 15 teams also submitted a task description paper.
In future work, we plan to increase the data size and to add more propaganda techniques. We further plan to cover several different languages.

