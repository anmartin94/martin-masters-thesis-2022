title
Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling
abstract
In this paper we describe the CoNLL-2004 shared task: semantic role labeling. We introduce the specification and goal of the task, describe the data sets and evaluation methods, and present a general overview of the systems that have contributed to the task, providing comparative description.

Introduction
In recent years there has been an increasing interest in semantic parsing of natural language, which is becoming a key issue in Information Extraction, Question Answering, Summarization, and, in general, in all NLP applications requiring some kind of semantic interpretation.
The shared task of CoNLL-2004 1 concerns the recognition of semantic roles, for the English language. We will refer to it as Semantic Role Labeling (SRL). <TASK>Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence. In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be extracted</TASK> (see Figure 1 for a detailed example). Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.
Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments.
Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002;Gildea and Palmer, 2002;Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003;Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003;Pradhan et al., 2003a;Pradhan et al., 2003b).
There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a;Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks.
Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate-argument structures. It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates. A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations.
To date, the best results reported on the PropBank correspond to a F 1 measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information (Pradhan et al., 2003b). This performance drops to 77 when a real parser is used instead. Comparatively, the best SRL system based solely on shallow syntactic information (Pradhan et al., 2003a) performs more than 15 points below. Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task.
The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases. The annotations provided for the development of systems include, apart from the argument boundaries and role labels, the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.
The rest of the paper is organized as follows. Section 2 describes the general setting of the task. Section 3 provides a detailed description of training, development and test data. Participant systems are described and compared in section 4. In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets. Finally, section 5 concludes.

Task Description
The goal of the task is to develop a machine learning system to recognize arguments of verbs in a sentence, and label them with their semantic role. A verb and its set of arguments form a proposition in the sentence, and typically, a sentence will contain a number of propositions.
There are two properties that characterize the structure of the arguments in a proposition. First, arguments do not overlap, and are organized sequentially. Second, an argument may appear split into a number of non-contiguous phrases. For instance, in the sentence "[ A1 The apple], said John, [ C−A1 is on the table]", the utterance argument (labeled with type A1) appears split into two phrases. Thus, there is a set of non-overlapping arguments labeled with semantic roles associated with each proposition. The set of arguments of a proposition can be seen as a chunking of the sentence, in which chunks are parts of the semantic roles of the proposition predicate.
In practice, number of target verbs are marked in a sentence, each governing one proposition. A system has to recognize and label the arguments of each target verb.

Methodological Setting
Training and development data are provided to build the learning system. Apart from the correct output, both data sets contain the correct input, as well as predictions of the input made by state-of-the-art processors. The training set is used for training systems, whereas the development set is used to tune parameters of the learning systems and select the best model. Systems have to be developed strictly with the data provided, which consists of input and output data and the official external resources (described below). Since the correct annotations for the input data are provided, a system is allowed either to be trained to predict the input part, or to make use of an external tool developed strictly within this setting, such as previous CoNLL shared task systems.

Evaluation
Evaluation is performed on a separate test set, which includes only predicted input data. A system is evaluated with respect to precision, recall and the F 1 measure. Precision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Finally, the F 1 measure computes the harmonic mean of precision and recall, and is the final measure to compare the performance of systems. It is formulated as:
For an argument to be correctly recognized, the words spanning the argument as well as its semantic role have to be correct. 2 As an exceptional case, the verb argument of each proposition is excluded from the evaluation. This argument is the lexicalization of the predicate of the proposition. Most of the time, the verb corresponds to the target verb of the proposition, which is provided as input, and only in few cases the verb participant spans more words than the target verb.
Except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition overestimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation.

Data
The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). We first describe annotations related to argument structure. Then, we describe the preprocessing of input data. Finally, we describe the format of the data sets.

PropBank
The Proposition Bank (PropBank) (Palmer et al., 2004) annotates the Penn Treebank with verb argument structure. The semantic roles covered by PropBank are the following:
• Numbered arguments (A0-A5, AA): Arguments defining verb-specific roles. Their semantics depends on the verb and the verb usage in a sentence, or verb sense. In general, A0 stands for the agent and A1 corresponds to the patient or theme of the proposition, and these two are the most frequent roles. However, no consistent generalization can be made across different verbs or different senses of the same verb. PropBank takes the definition of verb senses from VerbNet, and for each verb and each sense defines the set of possible roles for that verb usage, called the roleset. The definition of rolesets is provided in the PropBank Frames files, which is made available for the shared task as an official resource to develop systems.
• • Verbs (V): Participant realizing the verb of the proposition, with exactly one verb for each one.
We used the February 2004 release of PropBank. Most predicative verbs were annotated, although not all of them (for example, most of the occurrences of the verb "to have" and "to be" were not annotated). We applied procedures to check consistency of propositions, looking for overlapping arguments, and incorrect semantic role labels. Also, co-referenced arguments were annotated as a single item in PropBank, and we automatically distinguished between the referent and the reference with simple rules matching pronominal expressions, which were tagged as R arguments. A total number of 68 propositions were not compliant with our procedures, and were filtered out from the CoNLL data sets. The predicateargument annotations, thus, are not necessarily complete in a sentence. Table 1 provides counts of the number of sentences, annotated propositions, distinct verbs and arguments in the three data sets.

Preprocessing
In this section we describe the pipeline of processors to compute the annotations which form the input part of the data: part-of-speech (PoS) tags, chunks, clauses and named entities. The preprocessors correspond to the following state-of-the-art systems for each level of annota- 
Table 1: Counts on the three data sets.
• PoS tagger: (Giménez and Màrquez, 2003), based on Support Vector Machines, and trained on Penn Treebank sections 0-18.
• Chunker and Clause Recognizer: (Carreras and Màrquez, 2003), based on Voted Perceptrons, and following the CoNLL settings of 2000 and 2001 tasks (Tjong Kim Sang and Buchholz, 2000;Tjong Kim Sang and Déjean, 2001). These two processors form a coherent partial syntax of a sentence, that is, chunks and clauses form a tree.
• Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). Such processors were ran in a pipeline, from PoS tags, to chunks, clauses and finally named entities. Table 2 summarizes the performance of the processors on the development and test sections. These figures differ from the original results in the original due to a better quality of the input information in our runs. The figures of the named entity extractor are based on the corpus of the CoNLL-2003 shared task, since gold annotations of named entities were not available for the current corpus.

Format
Figure 1 shows an example of a fully-annotated sentence. Annotations of a sentence are given using a flat representation in columns, separated by spaces. Each column encodes an annotation by associating a tag with every word. For each sentence, the following columns are provided: 1. Words. 2. Part of Speech tags. 3. Chunks in IOB2 format. 4. Clauses in Start-End format. 5. Named Entities in IOB2 format. 6. Target verbs, marking n predicative verbs. This column, provided as input, specifies the governing verbs of the propositions to be analyzed. Each target verb is in the base form. Occasionally this column does not mark any verb (i.e., n may be 0). 7. For each of the n target verbs, a column in Start-End format specifying the arguments of the proposition. These columns are the output of a system, that is, the ones to be predicted, and are not available for the test set. 

Start-End format.
Represents non-overlapping phrases (clauses or arguments) which may be embed-ded 3 inside one another. Each tag indicates whether a clause starts or ends at that word and is of the form START*END. The START part is a concatenation of (k parentheses, each representing that a phrase of type k starts at that word. The END part is a concatenation of k) parentheses, each representing that a phrase of type k ends at that word. For example, the * tag represents a word with no starts and ends; the (A0*A0) tag represents a word constituting an A0 argument; and the (S(S*S) tag represents a word which constitutes a base clause (labeled S) and starts another higher-level clause. Finally, the concatenation of all tags constitutes a well-formed bracketing. For the particular case of split arguments, of type k, the first part appears as a phrase with label k, and the remaining as phrases with label C-k (continuation prefix). See examples of annotations at columns 4th, 7th and 8th of Figure 1.

Participating Systems
Ten systems have participated in the CoNLL-2004 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets.

Learning techniques
Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004;Lim et al., 2004). Two teams used Brill's Transformation-based Error-driven Learning (TBL) (Higgins, 2004;Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004;Kouchnir, 2004) As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL strategies and into the development of features (see sections 4.2 and 4.3). As an exception, van den Bosch et al. ( 2004   4th) and named entities (5th). The 6th column marks target verbs, and their propositions are found in remaining columns. According to the PropBank Frames, for issue (7th), the A0 annotates the issuer, and the A1 the thing issued, which appears split into two parts. For fill (8th), A1 is the the destination, and A2 the theme.
voting strategy to derive the final sequence tagging as a voted combination of three overlapping n-gram output sequences. The same team also applied a meta-learning step, by using iterative classifier stacking, for correcting systematic errors committed by the low-level classifiers. This work is also worth mentioning because of the extensive work done on parameter tuning and feature selection.

SRL approaches
SRL is a complex task which has to be decomposed into a number of simpler decisions and tagging schemes in order to be addressed by learning techniques. One first issue is the annotation of the different propositions of a sentence. Most of the groups treated the annotation of semantic roles for each verb predicate as an independent problem. An exception is the system of Carreras et al. (2004), which performs the annotation of all propositions simultaneously. As a consequence, the former teams treat the problem as the recognition of sequential structures (a.k.a. chunking), while the latter directly derives a hierarchical structure formed by the arguments of all propositions. Table 3 summarizes the main properties of each system regarding the SRL strategy implemented. This property corresponds to the first column.
Regarding the labeling strategy, we can distinguish at least three different strategies. The first one consists of performing role identification directly by a IOB-type sequence tagging. The second approach consists of dividing the problem into two independent phases: recognition, in which the arguments are recognized, and label-ing, in which the already recognized arguments are assigned role labels. The third approach also proceeds in two phases: filtering, in which a set of argument candidates are decided and labeling, in which the set of optimal arguments is derived from the proposed candidates. As a variant of the first two-phase strategy, van den Bosch et al. (2004) first perform a direct classification of chunks into argument labels, and then decide the actual arguments in a post-process by joining previously classified argument fragments. All this information is summarized in the second column of Table 3.
An implication of implementing the two-phase strategy is the ability to work with argument candidates in the second phase, allowing to develop feature patterns for complete arguments. Regarding the first phase, the recognition of candidate arguments is performed by means of a IOB or open-close tagging using classifiers, either argument-independent, or specialized by argument type.
It is also worth noting that all participant systems performed learning of predicate-independent classifiers instead of specializing by the verb predicate. Information about verb predicates is captured through features and some global restrictions.
Another important issue is the granularity at which the sentence elements are processed. It has become very clear that a good election for this problem is phrase-byphrase processing (P-by-P, using the notation introduced by Hacioglu et al. (2004)) instead of word-by-word (Wby-W). The motivation is twofold: (1) phrase boundaries are almost always consistent with argument boundaries;
(2) P-by-P processing is computationally less expensive and allows to explore a relatively larger context. Most of the groups performed a P-by-P processing, but admitting a processing by words within the target verb chunks. The system by Baldewein et al. (2004) works with a bit more general elements called "chunk sequences", extracted in a preprocess using heuristic rules. This information is presented in the third column of Table 3.
Information regarding clauses has proven to be very useful, as can be seen in section 4.3. All systems captured some kind of clause information through feature codification. However, some of the systems restrict the search for arguments only to the immediate clause Williams et al., 2004) and others use the clause hierarchy to guide the exploration of the sentence (Lim et al., 2004;Carreras et al., 2004).
Very relevant to the SRL strategy is the availability of global sentential information when decisions are taken. Almost all of the systems try to capture some global level information by collecting features describing the target predicate and its context, the "syntactic path" from the element under consideration to the predicate, etc. (see section 4.3). But only some of them include a global optimization procedure at sentence level in the labeling strategy. The systems working with Maximum Entropy Models (Baldewein et al., 2004;Lim et al., 2004) use beam search to find taggings that maximize the probability of the output sequence. Carreras et al. (2004) and Punyakanok et al. (2004) also define a global scoring function to maximize. At this point, the system of Punyakanok et al. (2004) deserves special consideration, since it formally implements a set of structural and linguistic constraints directly in the global cost function to maximize. These constraints act as a filter for valid output sequences and ensure coherence of the output. Authors refer to this part of the system as the inference layer and they implement it using integer linear programming. The iterative classifier stacking mechanism used by van den Bosch et al. ( 2004) also tries to alleviate the problem of locality of the low-level classifiers. This information is found in the fourth column of Table 3.
Finally, some systems use some kind of postprocessing to ensure coherence of the final labeling, correct some systematic errors, or to treat some types of adjunctive arguments. In most of the cases, this postprocess is performed on the basis of simple ad-hoc rules. This information is included in the last column of Table 3.

Features
With a very few exceptions all the participant systems have used all levels of linguistic information provided in the training data sets, that is, words, PoS and chunk labels, clauses, and named entities.
It is worth mentioning that the general type of features  derived from the basic information are strongly inspired by previous works on the SRL task (Gildea and Jurafsky, 2002;Surdeanu et al., 2003;Pradhan et al., 2003a). Many systems used the same kind of ideas but implemented in different ways, since the particular learning strategies used (see section 4.2) impose different constraints on the type of information available or the way of expressing it. As a general idea, we can divide the features into four types: (1) basic features, evaluating some kind of local information on the context of the word or constituent being treated; (2) Features characterizing the internal structure of a candidate argument; (3) Features describing properties of the target verb predicate; (4) Features that capture the relations between the verb predicate and the constituent under consideration.
All systems used some kind of basic features. Roughly speaking, they consist of words, PoS tags, chunks, clause labels, and named entities extracted from a windowbased context. These values can be considered with or without the relative position with respect to the element under consideration, and some n-grams of them can also be computed. If the granularity of the system is at phrase level then typically a representative head word of the phrase is used as lexical information. As an exception to the general approach, the system of Williams et al. (2004) does not make use of word forms.
The rest of the features are more interesting since they are task dependent, and deserve special attention. Table 4 summarizes the type of features exploited by systems.
To represent an argument itself, few attributes are of general usage. Some systems count the length of it, with different granularities. Others make use of heuristics to derive its syntactic type. There are systems that extract a structured representation of the argument, either homogeneous (capturing different sequences of head words, PoS tags, chunks or clauses), or heterogeneous (combining all elements, based on the syntactic hierarchy). A few systems have captured the existence of neighboring arguments, previously identified in the process. Interestingly, the system of Lim et al. (2004) represents the context of an argument relative to the syntactic hierarchy by means of relative constituent sequences and syntactic levels. Concerning lexicalization of the argument, most of the techniques rely on head word rules based on Collins', or content word rules as in Surdeanu et al. (2003). Only Carreras et al. (2004) decide to use a bag-of-words model, apart from heuristicbased lexicalization.
Regarding the target verb, the voice feature of the verb is generally used, in addition to basic features capturing the form and PoS tag of the verb. Some systems captured statistics on frequent argument patterns for each predicate. Also, systems represented the elements in the proximity of the target verb, inspired by local subcategorization patterns of a predicate.
As for features related to a constituent-predicate pair, all systems use the simple feature describing the relative position between them, and to a lesser degree, the distance and the difference in clausal levels. Again, there is a general tendency to describe the structured path from the argument to the verb. Its design goes from simple homogeneous sequences of head words or chunks, to more sophisticated paths combining chunks and clauses, and capturing hierarchical properties. The system of  also tracks the number of different syntactic elements found between the pair. Remarkably, the system of Baldewein et al. (2004) uses an EM clustering technique to derive features representing the affinity of an argument and a predicate.
On top of basic feature extraction, all teams working with SVM and VP used polynomial kernels of degree 2. Similar in expressiveness, the system designed by Punyakanok et al. (2004) expanded the feature space with all pairs of basic features.

Evaluation
A baseline rate was computed for the task. It was produced by a system developed by Erik Tjong Kim Sang, from the University of Antwerp, Belgium. The baseline processor finds semantic roles based on the following seven rules:
• Tag target verb and successive particles as V.
• Tag not and n't in target verb chunk as AM-NEG.
• Tag modal verbs in target verb chunk as AM-MOD.
• Tag first NP before target verb as A0.
• Tag first NP after target verb as A1.
• Tag that, which and who before target verb as R-A0. • Switch A0 and A1, and R-A0 and R-A1 if the target verb is part of a passive VP chunk. A VP chunk is considered in passive voice if it contains a form of to be and the verb does not end in ing.
Table 5 presents the overall results obtained by the ten participating systems, on the development and test sets. The best performance was obtained by the SVMbased IOB tagger of (Hacioglu et al., 2004), which almost reached the performance of 70 in F 1 on the test. The seven best systems obtained F 1 scores in the range of 60-70, and only three systems scored below that.
Comparing the results across development and test corpora, most systems experienced a decrease in performance between 1.5 and 3 points. As in previous editions of the shared task, we attribute this behavior to a greater difficulty of the test set instead of an overfitting effect. Interestingly, the three systems performing below 60 in the development set did not experienced this decrease. In fact (Williams et al., 2004) and (Baldewein et al., 2004) even improved the results on the test set.
Table 6 details the performance of systems for the A0-A4 arguments, on the test set. Consistently, the best performing system of the task also outperforms all other systems on these semantic roles.

Conclusion
We have described the CoNLL-2004 shared task on semantic role labeling. The task was based on the Prop-Bank corpus, and the challenge was to come up with machine learning techniques to recognize and label semantic roles on the basis of partial syntactic structure. Ten systems have participated to the task, contributing with a variety of standard or novel learning architectures. The best system, presented by the most experienced group on the task (Hacioglu et al., 2004), achieved a moderate performance of 69.49 at the F 1 measure. It is based on a SVM tagging system, performing IOB decisions on the chunks of the sentence, and exploiting a wide variety of features based on partial syntax.
Most of the systems advance the state-of-the-art on semantic role labeling on the basis of partial syntax. However, state-of-the-art systems working with full syntax still perform substantially better, although far from a desired behavior for real-task application.   4: Main feature types used by the 10 participating systems in the CoNLL-2004 shared task, sorted by performance on the test set. "sy": use of partial syntax (all levels); "ne": use of named entities; "al": argument length; "at": argument type; "as": argument internal structure; "aw": head-word lexicalization of arguments; "an": neighboring arguments; "vv": verb voice; "vs": verb statistics; "vf": verb features derived from PropBank frames; "vc": verb local context; "rp": relative position; "di": distance (horizontal or in the hierarchy); "pa": path; "ex": feature expansion.
most contributed to the performance of systems.  

IOB2 format .

