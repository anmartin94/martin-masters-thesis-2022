title
Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic
abstract
This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F P N 1 respectively.

Introduction
Work on opinion mining and subjective language analysis has been prominent in the natural language processing (NLP) field during the last two decades. One of the main tasks in this area is sentiment analysis (SA). One of the early works on SA is (Pang et al., 2002), where the authors analysed the sentiment in movie reviews. Following that, and embarked with the popularity of social media, SA became one of the popular topics in NLP. Most of the work on SA targeted English, while other languages, including Arabic, lagged behind. In the last decade, researchers on Arabic NLP started targeting SA such as the work of Abdul-Mageed et al. (2011). Since then, there have been numerous works on Arabic SA such as the works of (Abdulla et al., 2013;Alayba et al., 2018;Abdul-Mageed, 2019;Al-Smadi et al., 2019;Abu Farha and Magdy, 2021). Work on Arabic SA has been hindered by many challenges such as the large variation in dialects (Habash, 2010;Darwish et al., 2014) and the complex morphology of the language (Abdul-Mageed et al., 2011). With the advancement of work on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006).
There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016;Barbieri et al., 2014a,b;Filatova, 2012;Ghosh et al., 2015;Joshi et al., 2016;Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015;Joshi et al., 2015;Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants' submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020).
In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researchers to work on Arabic sarcasm detection. The shared task has two subtasks, sarcasm detection (subtask 1) and sentiment analysis (subtask 2). We provided the participant with a new dataset (ArSarcasm-v2), which is publicly available 1 . The dataset is annotated for sarcasm, sentiment and dialect. We received 27 submissions for subtask 1 and 22 submissions for subtask 2. This paper provides an overview of the shared task and the achieved results by the participants along with their approaches.
Most of the approaches used by participants were based on fine-tuning pre-trained language models. A small number of participants utilised other deep learning and conventional machine learning algorithms. The top team in the sarcasm detection task was BhamNLP (Alharbi and Lee, 2021), who achieved an F1-score of 0.6225 over the sarcastic class. While the top team in the sentiment analysis task was CS-UM6P (El Mahdaouy et al., 2021), who achieved F P N 1 of 0.748.

Related Work
Our shared task offers two subtasks on Arabic Sarcasm and sentiment detection. In the following, we discuss the literature in both tasks within the Arabic NLP community.

Arabic Sarcasm Classification
Arabic sarcasm did not receive the same degree of attention as English. The work on Arabic sarcasm detection is limited to a few works. soukhria2017 were the first to work on Arabic sarcasm/irony detection. In their work, they created a corpus of sarcastic Arabic tweets, which they collected using a set of political keywords. They filtered the sarcastic tweets using distant supervision, where they relied on some markers such as the Arabic equivalent of #sarcasm such as # , # , # and # . The final dataset consists of 5,479, 1,733 of which are sarcastic/ironic. They experimented with various classifiers such as SVM, Naive Bayes, Logistic Regression, Linear Regression. Random Forest was the best model, where it achieved an F1-score of 0.73. ghanem2019idat organised a shared task on Arabic sarcasm/irony detection. They prepared their dataset through collecting tweets related to different topics such as the US elections. Then they filtered out tweets that contain sarcastic hashtags, where they used the same hashtags used by soukhria2017. To prepare the final dataset, the authors sampled tweets from both the sarcastic and non-sarcastic portions, then they manually annotated them. The final dataset consists of 5,030 tweets, 2,614 of which are sarcastic. The shared task saw the participation of 18 teams. The first place was obtained by Khalifa and Hussein (2019), where they achieved an F1-score of 0.85. In their work, they relied on a set of features that include word n-grams, topic modelling features, sentiment features, statistical features and word embeddings. They experimented with multiple classifiers such as BiLSTM, Random Forest, XGBoost. Their best model was an ensemble of XGBoost, neural network and Random Forest. In a recent work by Abu Farha and Magdy (2020), the authors proposed ArSarcasm dataset for sarcasm detection, which contains around 10K tweets out of which around 1,600 are sarcastic. They presented a basic baseline that uses BiLSTM and achieved a F-score of 0.46 over the sarcastic class. Another recent study, Abbes et al. (2020) created a corpus of ironic tweets, namely DAICT. To prepare the corpus, the authors followed the same approach used by Ghanem et al. (2019) the corpus consists of 5,358 tweets distributed as follows: 4,809 sarcastic, 435 non-sarcastic and 114 labelled as ambiguous.

Arabic Sentiment Analysis
Unlike Arabic sarcasm detection, Arabic sentiment analysis (SA) has been under the researchers' radar for a while. Early work on Arabic SA such as in Abdul-Mageed et al. (2011); Abbasi et al. (2008), focused on modern standard Arabic (MSA). Since then, researchers started targeting dialectal Arabic (DA) such as the work of Mourad and Darwish (2013), where the authors introduced an expandable Arabic sentiment lexicon along with a corpus of tweets. Other datasets include the works of Kiritchenko et al. (2016); Rosenthal et al. (2017); Elmadany et al. (2018). Other works focused on proposing and comparing various approaches for Arabic SA (El-Beltagy et al., 2017;Al-Smadi et al., 2019;Abdulla et al., 2013;Alayba et al., 2018;Abu Farha and Magdy, 2019).
A recent comprehensive study by Abu Farha and Magdy (2021) provides a thorough comparative analysis of the available approaches on SA. In their work, they compared a large variety of models on three benchmark datasets. Their analysis shows that deep learning models combined with word embeddings achieve much better performance compared to classical machine learning models, such as SVMs. However, their experiments show that the utilisation of transformer-based language model achieves better results the best deep learning model architecture that uses word-embeddings. They show that using a fine-tuned AraBERT(Antoun et al., 2020) outperforms all existing classical and deep learning models on all the three benchmark datasets they examined.   

Dataset
The shared task provides the ArSarcasm-v2, which is a new dataset for Arabic sarcasm detection. The dataset is an extension of the original ArSarcasm dataset (Abu Farha and Magdy, 2020).

Resources
ArSarcasm-v2 uses the whole original ArSarcasm dataset (Abu Farha and Magdy, 2020) as part of its training data. The original ArSarcasm consists of 10,547 tweets, 1,682 of which are sarcastic. Additional sarcastic tweets are added to the ArSarcasm-v2 dataset from the DAICT dataset (Abbes et al., 2020), which represents a corpus of ironic/sarcastic tweets. DAICT contains 5,358 tweets, 4,809 of which are ironic/sarcastic. Since the goal is to extend the larger ArSarcasm, and because DAICT is mostly sarcastic, a new set of random tweets were collected over the period November-December 2020. The tweets where collected using the Twitter streaming API with the language filter set to Arabic ("lang:ar"). Since sarcasm is usually present in percentage, the new tweets were used to balance out DAICT.

Annotation
For the annotation process, we used appen 2 crowdsourcing platform. ArSarcasm represents the majority portion of ArSarcasm-v2. Thus, the goal was to annotate the new portions to have similar labels 2 https://www.appen.com/ to ArSarcasm. Additionally, DAICT was only annotated for sarcasm/irony, thus a new annotation was needed. To ensure consistency with ArSarcasm, we followed the same procedure and used the same guidelines to annotated the new portions. The original ArSarcasm paper defined sarcasm as an utterance that is used to express ridicule, where the intended meaning is different from the apparent one. Appendix A shows the guidelines (in Arabic) that have been shown to annotators.
Since DAICT is only annotated for sarcasm/irony, it was used as a pool of sarcastic examples which were balanced with the set of random Arabic tweets. A new set of 5,000 tweets, 2,500 of which are from DAICT, were annotated. The annotators were asked to provide three labels for each tweet as the following:
• Sarcasm: sarcastic or non-sarcastic.
• Sentiment: positive, negative or neutral.
• Dialect: Egyptian, Gulf, Levantine, Maghrebi or Modern Standard Arabic (MSA).
Only annotators with an Arab origin were allowed to participate. This was verified through their profile (usage of the Arabic language). Each tweet was annotated by at least three different annotators. The quality of annotation was monitored using a set of 100 hidden test questions that appear randomly during the task, each of those questions has the correct label for sentiment, sarcasm and dialect. If the performance of an annotator in these test questions dropped below 80%, this annotator is eliminated and all the labels he/she provided are also ignored. Agreement among annotators was 78.9% for sentiment, 87.3% for sarcasm and 77.0% for dialects.

Dataset Statistics
The new ArSarcasm-v2 dataset consists of 15,548 tweets, 10,547 of them were taken from the original ArSarcasm dataset while the rest (5,001 tweets) from DAICT and the new collection of tweets. These additional 5,001 tweets were split into two parts: 2,001 tweets added to the original ArSarcasm to form the set of training data of 12,548 tweets, and the remaining 3000 were used as the test set, as shown in Table 1. Each of the tweets has three labels for sarcasm, sentiment and dialect. Tables 1 and 2 show the statistics of the new dataset, where we can see that 19.2% of the data is sarcastic (2,989 tweets). Also, the annotation shows that most of the data is either in MSA or Egyptian dialect while the Maghrebi dialect is underrepresented with only 45 tweets.

Shared Task
This section provides an overview of the shared task, the description of the subtasks and the evaluation metrics.

Tasks Description
<TASK>The shared task on sarcasm detection and sentiment analysis in Arabic contains two subtasks as follows:
• Sarcasm Detection (subtask 1): the goal is to identify whether a tweet is sarcastic or not.
• Sentiment Analysis (subtask 2): the goal is to classify the tweet to one of the sentiment classes: positive, negative or neutral</TASK>.
The data for both subtasks was provided as train/test split without a specific development set. Table 1 shows the statistics of the two sets. The training set consists of 12,548 tweets, while the testing set consists of 3,000 tweets. The participants had access to the tweets' text and the dialect label during the testing phase.

Evaluation Metrics
The main evaluation metric for subtask 1 (sarcasm detection) is the F1-score of the sarcastic class only (F1-sarcastic), since it is the main class to be detected. Sarcasm is usually present in small percentages in the data, thus the task is an imbalanced classification task. F1-sarcastic is calculated using the following equation:
Where P sarcastic , R sarcastic are the precision and recall with respect to the sarcastic class.
For the sentiment analysis, the macro F1-score over the positive and negative classes was used (F P N 1 ). It is worth noting that the neutral class is excluded from the metric calculation and not the whole task. Thus miss-classified neutral tweets will lead to the increase of false positives for the positive or negative class, and thus should lead to the reduction of the F P N 1 value. This metric is the main adapted measure in multiple sentiment analysis shared tasks in different languages (Kiritchenko et al., 2016;Rosenthal et al., 2017).
F P N 1 is calculated using the following equation:
Where F P 1 , F N 1 are the F 1 with respect to the positive and negative classes respectively, while the neutral class is ignored.

Participating Teams
The shared task saw the participation of 30 unique teams. The sarcasm detection task (subtask 1) received 27 submissions, while the sentiment analysis task (subtask 2) received 22 submissions. Table 3 shows the list of the participating teams whose papers were accepted 3 .

Shared Task Results
Tables 4 and 5 show the results of both subtask 1 and subtask 2 respectively. The results are sorted in descending order based on the official metric of the corresponding subtask, where F1-sarcastic and F P N 1 are the official metrics for subtask 1 and subtask 2 respectively. For each team, only the last submission was considered for the leaderboard. For subtask 1 (sarcasm detection), BhamNLP Alharbi and Lee (2021) achieved first place with an F1-sarcastic of 0.6225. For subtask 2 (sentiment analysis), CS-UM6P El Mahdaouy et al. ( 2021) team achieved first place with an F P N 1 of 0.748.   3: The list of participating teams who provided their affiliation details along with the citation for those who submitted a system description paper. Runs that did not provide any details on their affiliation are not listed, but their results are listed in Tables 4 and 5.

Approaches by Top Submissions
The participating teams used a variety of approaches for both subtasks.
Most of the teams used pre-trained language models such as AraBERT (Antoun et al., 2020) and MARBERT (Abdul-Mageed et al., 2020). Abu Farha and Magdy (2021), provide an extensive comparison of pre-trained language models on ArSarcasm-v2 dataset. A few of the participants used deep learning and conventional machine learning approaches. All the teams, that are participating in the two subtasks, used the same architecture for both tasks.
For the sarcasm detection task, BhamNLP (Alharbi and Lee, 2021) team was ranked first with an F1-sarcastic of 0.6225. In their approach, they used a multi-task learning architecture that is trained for sarcasm and sentiment classification. The model is based on both MARBERT and a CNN-LSTM model, where the output of each of these models is concatenated and fed to the final output layer. The CNN-LSTM used both word and character embeddings. The second place (SPPU-AASM) (Hengle et al., 2021)  

Other Interesting Approaches
salambert-arsarcasm built their on the hypothesis that tweets with negative sentiment and tweets with sarcasm content are more likely to have offensive content. Thus, they pre-trained AraBERT (Antoun et al., 2020) on offensive language data then finetuned it for the target task. In (Israeli et al., 2021), the authors filtered the data through down sampling the non-sarcastic class. Their hypothesis is that the test set would be similar to the extra portions added to the original ArSarcasm. Thus, for both ArSarcasm and the added tweets, they built a  

Conclusion and Future Directions
This paper provides an overview of the shared task on sarcasm detection and sentiment analysis in Ara-bic. We provide an overview of the current state of research on Arabic sarcasm. The paper provides an overview of the new ArSarcasm-v2 dataset which was used for the shared task. We also provide a high-level description of the top participating teams in the shared task. The aim of this shared task is to encourage researchers to work on Arabic sarcasm, which was reflected by the popularity of the task and having 27 run submissions and 17 system description papers discussing different approaches applied on this task.
We hope that this task would not be the last on Arabic Sarcasm detection. More datasets on Arabic Sarcasm would further help the development of better detection models. In addition, much work is still required for this challenging task, since as it is noticed, the state-of-the-art performance is 0.62 F-score, which shows that there is large room of improvement to be achieved.   

