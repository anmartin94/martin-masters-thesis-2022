title
SemEval-2010 Task 7: Argument Selection and Coercion
abstract
We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to <TASK>identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing</TASK>. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.

Introduction
In recent years, a number of annotation schemes that encode semantic information have been developed and used to produce data sets for training machine learning algorithms. Semantic markup schemes that have focused on annotating entity types and, more generally, word senses, have been extended to include semantic relationships between sentence elements, such as the semantic role (or label) assigned to the argument by the predicate (Palmer et al., 2005;Ruppenhofer et al., 2006;Kipper, 2005;Burchardt et al., 2006;Subirats, 2004).
In this task, we take this one step further and attempt to capture the "compositional history" of the argument selection relative to the predicate. In particular, this task attempts to identify the operations of type adjustment induced by a predicate over its arguments when they do not match its selectional properties. The task is defined as follows: for each argument of a predicate, identify whether the entity in that argument position satisfies the type expected by the predicate. If not, then identify how the entity in that position satisfies the typing expected by the predicate; that is, identify the source and target types in a type-shifting or coercion operation.
Consider the example below, where the verb report normally selects for a human in subject position, as in (1a). Notice, however, that through a metonymic interpretation, this constraint can be violated, as demonstrated in (1b).
(1) a. John reported in late from Washington. b. Washington reported in late.
Neither the surface annotation of entity extents and types nor assigning semantic roles associated with the predicate would reflect in this case a crucial point: namely, that in order for the typing requirements of the predicate to be satisfied, a type coercion or a metonymy (Hobbs et al., 1993;Pustejovsky, 1991;Nunberg, 1979;Egg, 2005) has taken place.
The SemEval Metonymy task (Markert and Nissim, 2007) was a good attempt to annotate such metonymic relations over a larger data set. This task involved two types with their metonymic variants: categories-for-locations (e.g., placefor-people) and categories-for-organizations (e.g., organization-for-members). One of the limitations of this approach, however, is that while appropriate for these specialized metonymy relations, the annotation specification and resulting corpus are not an informative guide for extending the annotation of argument selection more broadly.
In fact, the metonymy example in (1) is an instance of a much more pervasive phenomenon of type shifting and coercion in argument selection. For example, in (2) below, the sense annotation for the verb enjoy should arguably assign similar values to both (2a) and (2b). The consequence of this is that under current sense and role annotation strategies, the mapping to a syntactic realization for a given sense is made more complex, and is in fact perplexing for a clustering or learning algorithm operating over subcategorization types for the verb.

Methodology of Annotation
Before introducing the specifics of the argument selection and coercion task, we will briefly review our assumptions regarding the role of annotation in computational linguistic systems.
We assume that the features we use for encoding a specific linguistic phenomenon are rich enough to capture the desired behavior. These linguistic descriptions are typically distilled from extensive theoretical modeling of the phenomenon. The descriptions in turn form the basis for the annotation values of the specification language, which are themselves the features used in a development cycle for training and testing a labeling algorithm over a text. Finally, based on an analysis and evaluation of the performance of a system, the model of the phenomenon may be revised.
We call this cycle of development the MATTER methodology (Fig. 1):
Model: Structural descriptions provide theoretically informed attributes derived from empirical observations over the data;
Annotate: Annotation scheme assumes a feature set that encodes specific structural descriptions and properties of the input data;
Train: Algorithm is trained over a corpus annotated with the target feature set;
Test: Algorithm is tested against held-out data;
Evaluate: Standardized evaluation of results;
Revise: Revisit the model, annotation specification, or algorithm, in order to make the annotation more robust and reliable.
Some of the current and completed annotation efforts that have undergone such a development cycle include PropBank (Palmer et al., 2005), Nom-Bank (Meyers et al., 2004), andTimeBank (Pustejovsky et al., 2005).

Task Description
The argument selection and coercion (ASC) task involves identifying the selectional mechanism used by the predicate over a particular argument. 1 For the purposes of this task, the possible relations between the predicate and a given argument are restricted to selection and coercion. In selection, the argument NP satisfies the typing requirements of the predicate, as in ( 3):
(3) a. The spokesman denied the statement (PROPOSI-TION).
b. The child threw the stone (PHYSICAL OBJECT).
c. The audience didn't believe the rumor (PROPOSI-TION).
Coercion occurs when a type-shifting operation must be performed on the complement NP in order to satisfy selectional requirements of the predicate, as in ( 4). Note that coercion operations may apply to any argument position in a sentence, including the subject, as seen in ( 4b). Coercion can also be seen as an object of a proposition, as in (4c). In order to determine whether type-shifting has taken place, the classification task must then involve (1) identifying the verb sense and the associated syntactic frame, (2) identifying selectional requirements imposed by that verb sense on the target argument, and (3) identifying the semantic type of the target argument.

Resources and Corpus Development
We prepared the data for this task in two phases: the data set construction phase and the annotation phase (see Fig. 2). The first phase consisted of (1) selecting the target verbs to be annotated and compiling a sense inventory for each target, and (2) data extraction and preprocessing. The prepared data was then loaded into the annotation interface. During the annotation phase, the annotation judgments were entered into the database, and an adjudicator resolved disagreements. The resulting database was then exported in an XML format. 

Data Set Construction Phase: English
For the English data set, the data construction phase was combined with the annotation phase. The data for the task was created using the following steps:
1. The verbs were selected by examining the data from the BNC, using the Sketch Engine (Kilgarriff et al., 2004) as described in (Rumshisky and Batiukova, 2008). Verbs that consistently impose semantic typing on one of their arguments in at least one of their senses (strongly coercive verbs) were included into the final data set: arrive (at), cancel, deny, finish, and hear.
2. Sense inventories were compiled for each verb, with the senses mapped to OntoNotes (Pradhan et al., 2007) whenever possible. For each sense, a set of type templates was compiled using a modification of the CPA technique (Hanks and Pustejovsky, 2005;Pustejovsky et al., 2004): every argument in the syntactic pattern associated with a given sense was assigned a type specification. Although a particular sense is often compatible with more than one semantic type for a given argument, this was never the case in our data set, where no disjoint types were tested. The coercive senses of the chosen verbs were associated with the following type templates: We used a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part of the CPA effort (Hanks, 2009;Pustejovsky et al., 2004;Rumshisky et al., 2006). Types were selected for their prevalence in manually identified selection context patterns developed for several hundred English verbs. That is, they capture common semantic distinctions associated with the selectional properties of many verbs. The types used for annotation were: This set of types is purposefully shallow and non-hierarchical. For example, HUMAN is a subtype of both ANIMATE and PHYSICAL OB-JECT, but annotators and system developers were instructed to choose the most relevant type (e.g., HUMAN) and to ignore inheritance.
3. A set of sentences was randomly extracted for each target verb from the BNC (Burnard, 1995). The extracted sentences were parsed automatically, and the sentences organized according to the grammatical relation the target verb was involved in. Sentences were excluded from the set if the target argument was expressed as anaphor, or was not present in the sentence. The semantic head for the target grammatical relation was identified in each case.
4. Word sense disambiguation of the target predicate was performed manually on each extracted sentence, matching the target against the sense inventory and the corresponding type templates as described above. The appropriate senses were then saved into the database along with the associated type template.
5. The sentences containing coercive senses of the target verbs were loaded into the Brandeis Annotation Tool (Verhagen, 2010). Annotators were presented with a list of sentences and asked to determine whether the argument in the specified grammatical relation to the target belongs to the type associated with that sense in the corresponding template. Disagreements were resolved by adjudication. 

Data Set Construction Phase: Italian
In constructing the Italian data set, we adopted the same methodology used for the English data set, with the following differences:
1. The list of coercive verbs was selected by examining data from the ItWaC (Baroni and Kilgarriff, 2006) using the Sketch Engine (Kilgarriff et al., 2004):
accusare 'accuse', annunciare 'announce', arrivare 'arrive', ascoltare 'listen', avvisare 'inform', chiamare 'call', cominciare 'begin', completare 'complete', concludere 'conclude', contattare 'contact', divorare 'devour', echeggiare 'echo', finire 'finish', informare 'inform', interrompere 'interrupt', leggere 'read', raggiungere 'reach', recar(si) 'go to', rimbombare 'resound', sentire 'hear', udire 'hear', visitare 'visit'.
2. The coercive senses of the chosen verbs were associated with type templates, some of which are listed listed below. Whenever possible, senses and type templates were adapted from the Italian Pattern Dictionary (Hanks and Jezek, 2007) and mapped to their SIMPLE equivalents (Lenci et al., 2000). The annotators were provided with a set of definitions and examples of each type.
3. A set of sentences for each target verb was extracted and parsed from the PAROLE sottoinsieme corpus (Bindi et al., 2000). They were skimmed to ensure that the final data set contained a sufficient number of coercions, with proportionally more selections than coercions. Sentences were preselected to include instances representing one of the chosen senses.
4. In order to exclude instances that may have been wrongly selected, a judge performed word sense disambiguation of the target predicate in the extracted sentences.
5. Annotators were presented with a list of sentences and asked to determine the usual semantic type associated with the argument in the specified grammatical relation. Every sentence was annotated by two annotators and one judge, who resolved disagreements.
6. Some of the coercion types selected for Italian were: 

Data Format
The test and training data were provided in XML.
The relation between the predicate (viewed as a function) and its argument were represented by composition link elements (CompLink), as
shown below. The test data differed from the training data in the omission of CompLink elements. In case of coercion, there is a mismatch between the source and the target types, and both types need to be identified; e.g., The State Department repeatedly denied the attack:
The State Department repeatedly &lt;SELECTOR sid="s1"&gt;denied&lt;/SELECTOR&gt; the &lt;TARGET id="t1"&gt;attack&lt;/TARGET&gt;. &lt;CompLink cid="cid1" compType="COERCION" selector_id="s1" relatedToTarget="t1" sourceType="EVENT" targetType="PROPOSITION"/&gt;
When the compositional operation is selection, the source and target types must match; e.g., The State Department repeatedly denied the statement:
The State Department repeatedly &lt;SELECTOR sid="s2"&gt;denied&lt;/SELECTOR&gt; the &lt;TARGET id="t2"&gt;statement&lt;/TARGET&gt;. &lt;CompLink cid="cid2" compType="SELECTION" selector_id="s2" relatedToTarget="t2" sourceType="PROPOSITION" targetType="PROPOSITION"/&gt;

Results &amp; Analysis
We received only a single submission for the ASC task. The UTDMet system was an SVMbased system with features derived from two main sources: a PageRank-style algorithm over Word-Net hypernyms used to define semantic classes, and statistics from a PropBank-style parse of some 8 million documents from the English Gigaword corpus. The results, shown in Table 2, were computed from confusion matrices constructed for each of four classification tasks for the 1039 link instances in the English test data: determination of argument selection or coercion, identification of the argument source type, identification of the argument target type, and the joint identification of the source/target type pair. Clearly, the UTDMet system did quite well at this task. The one immediately noticeable outlier is the macro-averaged precision for the joint type, which reflects a small number of miscategorizations of rare types. For example, eliminating the single miscategorized ARTIFACT-LOCATION link in the submitted test data bumps this score up to a respectable 94%. This large discrepancy can explained by the lack of any coercions with those types in the gold-standard data.  In the absence of any other submissions, it is difficult to provide a point of comparison for this performance. However, we can provide a baseline by taking each link to be a selection whose source and target types are the most common type (EVENT for the gold-standard English data). This yields micro-averaged precision scores of 69% for selection vs. coercion, 33% for source type identification, 37% for the target type identification, and 22% for the joint type.
The performance of the UTDMet system suggests that most of the type coercions were identifiable based largely on examination of lexical clues associated with selection contexts. This is in fact to be expected for the type coercions that were the focus of the English data set. It will be interesting to see how systems perform on the Italian data set and an expanded corpus for English and Italian, where more subtle and complex type exploitations and manipulations are at play. These will hopefully be explored in future competitions.

Conclusion
In this paper, we have described the Argument Selection and Coercion task for SemEval-2010. This task involves identifying the relation between a predicate and its argument as one that encodes the compositional history of the selection process. This allows us to distinguish surface forms that directly satisfy the selectional (type) requirements of a predicate from those that are coerced in context. We described some details of a specification language for selection, the annotation task using this specification to identify argument selection behavior, and the preparation of the data for the task. Finally, we analyzed the results of the task submissions.


