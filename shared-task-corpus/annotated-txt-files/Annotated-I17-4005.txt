title
IJCNLP-2017 Task 5: Multi-choice Question Answering in Examinations
abstract
The IJCNLP-2017 Multi-choice Question Answering(MCQA) task aims at exploring the performance of current Question Answering(QA) techniques via the realworld complex questions collected from Chinese Senior High School Entrance Examination papers and CK12 website 1 . The questions are all 4-way multi-choice questions writing in Chinese and English respectively that cover a wide range of subjects, e.g. Biology, History, Life Science and etc. And, all questions are restrained within the elementary and middle school level. During the whole procedure of this task, 7 teams submitted 323 runs in total. This paper describes the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods.

Introduction
One critical but challenging problem in natural language understanding (NLU) is to develop a question answering(QA) system which could consistently understand and correctly answer general questions about the world. <TASK>"Multi-choice Question Answering in Exams"(MCQA) is a typical question answering task that aims to test how accurately the participant QA systems could answer the questions in exams</TASK>. All questions in this competition come from real examinations. We collected multiple choice questions from several curriculums, such as Biology, History, Life-Science, with a restrain that all questions are limited in the elementary and middle school level. For every question, four answer candidates are provided,  where each of them may be a word, a value, a phrase or even a sentence. The participant QA systems are required to select the best one from these four candidates. Fig 1 is an example. To answer these questions, participants could utilize any public toolkits and any resources on the Web, but manually annotation is not permitted.
As for the knowledge resources, we encourage participants to utilize any resource on Internet, including softwares, toolboxes, and all kinds of corpora. Meanwhile, we also provide a dump of Wikipedia 2 and a collection of related Baidu Baike Corpus 3 under a specific license. These corpora and released questions are all provided in the XML format, which will be explained in section 2.2.
Main characteristics of our task are as follow:
• All the questions are from real word examinations.
• Most of questions require considerable inference ability.
• Some questions require a deep understanding of context.
• Questions from different categories have different characteristics, which makes it harder for a model to have a good performance on all kinds of questions.
• It concentrates only on the textual content, as questions with figures and tables are all filtered out.

Task and Data Description
All questions in MCQA consist of 2 parts, a question and 4 answer candidates, without any figure or table. The participant systems are required to select the only right one from all candidates.

Languages and Subjects
In order to explore the influence of diversity of questions, we collect questions from seven subjects in two languages, including an English subset and a Chinese subset. The subjects of English subset contain biology, chemistry, physics, earth science and life science. And the subjects of Chinese subset only contain biology and history. The total number of questions is 14,447.

Format
All questions in our dataset are consisted by the following 7 parts:
1. ID, i.e. the identical number of a specific question;
2. Question, i.e. the question to be answered; Take a question in Figure 1 for example. Roles of every part are as follow:
1. ID: wb415; 2. Question: "Peach trees have sweet-smelling blossoms and produce rich fruit. What is the main purpose of the flowers of a peach tree?"; 3. Option A: "to attract bees for pollination."; 4. Option B: "to create flower arrangements."; 5. Option C: "to protect the tree from disease."; 6. Option D: "to feed migratory birds."; 7. Correct Answer No.: 0.
It needs to be specified that we exclude the Correct Answer No. in the validation and test set.

Data Size
The dataset totally contains 14,447 multiple choice questions. In detail, English subset contains 5,367 questions and Chinese subset contains 9,080 questions. We randomly split the dataset into Train, Validation and Test sets. And more detail statistics is showed in Table 1.  

English Subset
We collected all the downloadable quiz from CK12 and only reserved 5367 4-way multi-choice questions with their tags which are also the basis of classifying the questions. For every subject, we randomly separate questions into 3 parts, train set, valid set and test set with 50%, 12.5% and 37.5% questions respectively.

Chinese Subset
As questions in Senior High School Entrance Examination(SHSEE) differs among different cities, we collected questions in SHSEE from as many cities as we can. After filtering out the questions containing more information than textual content, the answers of left questions were labeled by human. Finally, we got 4,531 questions in Biology and 4,549 questions in History. For every subject, we randomly separate questions into 3 parts, train  set, valid set and test set with same ratio stated above.

Evaluation
This challenge employs the accuracy of a method on answering questions in test set as the metric, the accuracy is calculated as follow.
where n correct is the number of correctly answered questions and N total is the total number of all questions.
To automatically evaluate the performance of QA systems, we built a web-site for participants to submit solutions for valid and test data set and get accuracy immediately on the page.

Baseline
We employ a simple retrieval based method as a baseline, and it is implemented based on Lucene 4 which is an open-source information retrieval software library. We employ the method to build reverse-index on the whole Wikipedia dump 5 for English questions and on the Baidu Baike corpus 6 for Chinese questions.
This method scores pairs of the question and each of its option, the detail steps are shown as follows.
• concatenate a question with an option as the query;
• use Lucene to search relevant documents with the query;
• score relevant documents by the similarity between the query q and the document d, noted as Sim(q, d);
• choose at most three highest scores to calculate the score of the pair of the question and the option as
where n 3 and if n = 0, score(q, a) = 0;
All questions and options are preprocessed by Stanford CoreNLP 7 . The detail result of the baseline on the validation set is shown in Table 2.  The details of participation of different language subsets are listed in the following Table 4.

Submission
In order to avoid the situation that participants submit different permutation of answers to sniff the correct answer labels, we limited the times that a team can submit their solutions. Before the release of test set, a team can submit no more than 5 solutions for valid set in 24 hours. After the release of test set, a team can submit as many as 30 solutions   Aug. 31, 2017) for valid set per 24 hours, but no more than 5 solutions for test set in 24 hours. Finally, we got 323 runs in total, in which there are 219 runs for valid set and 104 runs for test set.

Results
In our evaluation system, only the best performance of participants were reserved. The detail results of every subset is listed in the following subsections.

All Questions
There is only one team, "YNU-HPCC", that took the challenge of both English subset and Chinese subset. And, the performance of their system is listed in Table 5.

English Subset
Totally, there are 5 teams that only took the challenge of English subset and details of their performance are listed in the Table 6.

Chinese Subset
There are 1 team that only took the challenge of Chinese subset and their performance is listed in the Table 7.
6 Overview of Participant Systems 6.1 YNU-HPCC, An Attetion-based LSTM YNU-HPCC (Yuan et al., 2017) proposed an attention-based LSTM(AT-LSTM) model for MCQA. According to them, this model can easily capture long contextual information with the help of an attention mechanism. As illustrated in Figure 2, LSTM layer takes the vector representions of question and answers as input and then calculates out the hidden vectors which are the input of attention layer to calculate the weight vector α and weighted hidden representation r.
Finally, an softmax layer takes r as input to select the right answer. 

CASIA-NLP, Internet Resources and Localization Method
Based on the phenomenon that many web pages containing answers of the questions in MCQA, CASIA-NLP (Li and Kong, 2017) crawled on Internet and analyzed the content in these pages. When analyzing these pages, they use a localization method to locate the positions of sentences that have same meaning of questions in MCQA by merging a score given by edit distance that evaluates the structural similarity and a cosine score given by a CNN network that evaluates the semantic similarity. Finally, the system can analyze answers to find out the right one. The overview of the system is illustrated in Figure 3 and the CNN network they used is demonstrated in Figure 4.
Figure 3: Overview of CAISA-NLP's system (Li and Kong, 2017). Communication between modules is indicated by arrows.     (Li and Kong, 2017).

Cone, Wikipedia and Logistic Regression
The system of Cone (Dzendzik et al., 2017), a team from ADAPT Centre, based on a logistic regression over the string similarities between question, answer, and additional text. Their model is constructed as a four-step pipeline as follow.
1. Preprocessing cleaning of the input data;
2. Data selection relative sentences are extracted from Wikipedia based on key words from question;
3. Feature Vector Concatenation for every question, a feature vector is built as a concatenation of similarities between the answer candidates and sentences obtained in the previous step;
4. Logistic Regression a logistic regression over the feature vector.
The features they employed includes term frequencyinverse document frequency (Tf-IDf) metric, character n-grams (with n ranging from 1 to 4), bag of words,and windows slide (a ratio between answer and substrings of extracted data). While their model is trained in two ways, combining training over all domains and separate model training from each domain, the later one got the best performance.

G623, A CNN-LSTM Model with Attention Mechanism
Figure 5: Architecture of the model proposed by G623 (Min et al., 2017).
The system of G623 (Min et al., 2017) combined CNN with LSTM network and took into account the attention mechanism. Fistly , question and answer pairs are fed into a CNN network and produce joint representations of these pairs which are then fed into a LSTM network. The two separate vector representations of question and answer are then calculated to generate the weight vector by dot multiplication. Finally, a softmax layer is applied to classify the join representations with the help of attention weight. The diagram of their system is illustrated in Figure 5.

JU NITM, Complex Decision Tree
To handle the questions in MCQA, JU NITM (Sarkar et al., 2017) built a complex decision tree classifier using word embedding features to predict the right answer. The overview of the whole system is demonstrated in Figure 6. In distributed semantic similarity module, they trained a word embedding dictionary containing 3 million words in 300-dimensional space on GoogleNews. Then, a complex decision tree is used to select the right answer in step2, classification.
Figure 6: System Framework proposed by JU NITM (Sarkar et al., 2017).

TALN, MappSent
Mappsent is proposed in a previous work of TALN, . To adapt to the characteristics of MCQA, they retrofitted MappSent model in two different ways (Hazem, 2017). The first approach illustrated in Figure 7 is to follow the same procedure as the question-to-question similarity task, i.e. using anatated pairs of questions and their corresponding answers to build the mapping matrix. The second approach illustrated in Figure 8 tends to keep the strong hypothesis of sentence pairs similarity. They built two mapping matrices, one that represent similar question pairs and ther other one to represent similar answers pairs. For a give test question, the system can extracted the most similar quesiont in the training data and select the candidate with highest similarity score as correct answer.
Figure 7: Fist adaptation of MappSent (Hazem, 2017).
Figure 8: Second adaptation of MappSent (Hazem, 2017).

Conclusions
We described the overview of the Multi-choice Question Answering task. The goal is exploring the performance of current Question Answering(QA) techniques via the real-world complex questions collected from Chinese Senior High School Entrance Examination(SHSEE) papers and CK12 website. We collected 14,447 questions covering 2 language in 7 different subjects. 7 teams submitted 323 runs in total. We describe the collected data, the format and size of these questions, formal run statistics and results, overview and performance statistics of different methods in this paper.

