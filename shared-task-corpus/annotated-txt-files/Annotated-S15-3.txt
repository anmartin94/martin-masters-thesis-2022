title
SemEval-2015 Task 3: Answer Selection in Community Question Answering
abstract
Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts. In this context, we organized SemEval-2015 Task 3 on Answer Selection in cQA, which included <TASK>two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect to the question, and (b) answering a YES/NO question with yes, no, or unsure, based on the list of all answers</TASK>. We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community. Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive. The best systems achieved an official score (macro-averaged F 1 ) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.

Introduction
Many social activities on the Web, e.g., in forums and social networks, are accomplished by means of the community Question Answering (cQA) paradigm. User interaction in this context is seldom moderated, is rather open, and thus has little restrictions, if any, on who can post and who can answer a question.
On the positive side, this means that one can freely ask a question and expect some good, honest answers. On the negative side, it takes efforts to go through all possible answers and to make sense of them. It is often the case that many answers are only loosely related to the actual question, and some even change the topic. It is also not unusual for a question to have hundreds of answers, the vast majority of which would not satisfy a user's information needs; thus, finding the desired information in a long list of answers might be very time-consuming.
In our SemEval-2015 Task 3, we proposed two subtasks. First, subtask A asks for identifying the posts in the answer thread that answer the question well vs. those that can be potentially useful to the user (e.g., because they can help educate him/her on the subject) vs. those that are just bad or useless. This subtask goes in the direction of automating the answer search problem that we discussed above, and we offered it in two languages: English and Arabic. Second, for the special case of YES/NO questions, we propose an extreme summarization exercise (subtask B), which aims to produce a simple YES/NO overall answer, considering all good answers to the questions (according to subtask A).
For English, the two subtasks are built on a particular application scenario of cQA, based on the Qatar Living forum. 1 However, we decoupled the tasks from the Information Retrieval component in order to facilitate participation, and to focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005;Jeon et al., 2005;Shen and Lapata, 2007;Moschitti et al., 2007;Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. 2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012;Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general.
For Arabic, we also made use of a real cQA portal, the Fatwa website, 3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic.
Overall for the task, we needed manual annotations in two different languages and for two domains. For English, we built the Qatar Living datasets as a joint effort between MIT and the Qatar Computing Research Institute, co-organizers of the task, using Amazon's Mechanical Turk to recruit human annotators. For Arabic, we built the dataset automatically from the data available in the Fatwa website, without the need for any manual annotation. We made all datasets publicly available, i.e., also usable beyond SemEval.
2 aclweb.org/aclwiki/index.php?title= Question_Answering_(State_of_the_art)
3 http://fatwa.islamweb.net/
Our SemEval task attracted 13 teams, who submitted a total of 61 runs. The participants mainly focused on defining new features that go beyond question-answer similarity, e.g., author-and userbased, and spent less time on the design of complex machine learning approaches. Indeed, most systems used multi-class classifiers such as Max-Ent and SVM, but some used regression. Overall, almost all submissions managed to outperform the baselines using the official F 1 -based score. In particular, the best system can detect a correct answer with an accuracy of about 73% in the English task and 83% in the easier Arabic task. For the extreme summarization task, the best accuracy is 72%.
An interesting outcome of this task is that the Qatar Living company, a co-organizer of the challenge, is going to use the experience and the technology developed during the evaluation excercise to improve their products, e.g., the automatic search of comments useful to answer users' questions.
The remainder of the paper is organized as follows: Section 2 gives a detailed description of the task, Section 3 describes the datasets, Section 4 explains the scorer, Section 5 presents the participants and the evaluation results, Section 6 provides an overview of the various features and techniques used by the participating systems, Section 7 offers further discussion, and finally, Section 8 concludes and points to possible directions for future work.

Task Definition
We have two subtasks:
• Subtask A: Given a question (short title + extended description), and several community answers, classify each of the answers as (a) definitely relevant (good), (b) potentially useful (potential), or (c) bad or irrelevant (bad, dialog, non-English, other).
• Subtask B: Given a YES/NO question (short title + extended description), and a list of community answers, decide whether the global answer to the question should be yes, no, or unsure, based on the individual good answers. This subtask is only available for English. 

Datasets
We offer the task in two languages, English and Arabic, with some differences in the type of data provided. For English, there is a question (short title + extended description) and a list of several community answers to that question. For Arabic, there is a question and a set of possible answers, which include (i) a highly accurate answer, (ii) potentially useful answers from other questions, and (iii) answers to random questions. The following subsections provide all the necessary details.

English Data: CQA-QL corpus
The source of the CQA-QL corpus is the Qatar Living forum. A sample of questions and answer threads was selected and then manually filtered and annotated with the categories defined in the task. Each question is followed by a list of comments (or answers). A comment has a subject and a body (text), as well as the following attributes:
• CID: comment identifier; • CUSERID: identifier of the user posting the comment; • CGOLD: human assessment about whether the comment is Good, Bad, Potential, Dialogue, non-English, or Other. • CGOLD YN: human assessment on whether the comment suggests a Yes, a No, or an Unsure answer.
At test time, CGOLD, CGOLD YN, and QGOLD YN are hidden, and systems are asked to predict CGOLD for subtask A, and QGOLD YN for subtask B; CGOLD YN is not to be predicted.
Figure 1 shows a fully annotated English YES/NO question from the CQA-QL corpus. We can see that it is asked and answered in a very informal way and that there are many typos, incorrect capitalization, punctuation, slang, elongations, etc. Four of the comments are good answers to the question, and four are bad. The bad answers are irrelevant with respect to the YES/NO answer to the question as a whole, and thus their CGOLD YN label is Not Applicable. The remaining four good answers predict Yes twice, No once, and Unsure once; as there are more Yes answers than the two alternatives, the overall QGOLD YN is Yes.

Annotating the CQA-QL corpus
The manual annotation was a joint effort between MIT and the Qatar Computing Research Institute, co-organizers of the task. After a first internal labeling of a trial dataset (50+50 questions) by several independent annotators, we defined the annotation procedure and prepared detailed annotation guidelines. We then used Amazon's Mechanical Turk to collect human annotations for a much larger dataset. This involved the setup of three HITs:
• HIT 1: Select appropriate example questions and classify them as GENERAL vs. YES/NO (QCATEGORY);
• HIT 2: For GENERAL questions, annotate each comment as Good, Bad, Potential, Dialogue, non-English, or Other (CGOLD);
• HIT 3: For YES/NO questions, annotate the comments as in HIT 2 (CGOLD), plus a label indicating whether the comment answers the question with a clear Yes, a clear No, or in an undefined way, i.e., as Unsure (CGOLD YN).
For all HITs, we collected annotations from 3-5 annotators for each decision, and we resolved discrepancies using majority voting. Ties led to the elimination of some comments and sometimes even of entire questions.
We assigned the Yes/No/Unsure labels at the question level (QGOLD YN) automatically, using the Yes/No/Unsure labels at the comment level (CGOLD YN). More precisely, we labeled a YES/NO question as Unsure, unless there was a majority of Yes or No labels among the Yes/No/Unsure labels for the comments that are labeled as Good, in which case we assigned the majority label.
Table 1 shows some statistics about the datasets. We can see that the YES/NO questions are about 10% of the questions. This makes subtask B generally harder for machine learning, as there is much less training data. We further see that on average, there are about 6 comments per question, with the number varying widely from 1 to 143. About half of the comments are Good, another 10% are Potential, and the rest are Bad. Note that for the purpose of classification, Bad is in fact a heterogeneous class that includes about 50% Bad, 50% Dialogue, and also a tiny fraction of non-English and Other comments. We released the fine grained labels to the task participants as we thought that having information about the heterogeneous structure of Bad might be helpful for some systems. About 40-50% of the YES/NO annotations at the comment level (CGOLD YN) are Yes, with the rest nearly equally split between No and Unsure, with No slightly more frequent. However, at the question level, the YES/NO annotations (QGOLD YN) have more Unsure than No. Overall, the label distribution in development and testing is similar to that in training for the CGOLD values, but there are somewhat larger differences for QGOLD YN.
We further released the raw text of all questions and of all comments from Qatar Living, including more than 100 million word tokens, which are useful for training word embeddings, topic models, etc. 

Arabic Data: Fatwa corpus
For Arabic, we used data from the Fatwa website, which deals with questions about Islam. This website contains questions by ordinary users and answers by knowledgeable scholars in Islamic studies. The user question can be general, for example "How to pray?", or it can be very personal, e.g., the user has a specific problem in his/her life and wants to find out how to deal with it according to Islam.
Each question (Fatwa) is answered carefully by a knowledgeable scholar. The answer is usually very descriptive: it contains an introduction to the topic of the question, then the general rules in Islam on the topic, and finally an actual answer to the specific question and/or guidance on how to deal with the problem. Typically, links to related questions are also provided to the user to read more about similar situations and to look at related questions.
In the Arabic version of subtask A, a question from the website is provided with a set of exactly five different answers. Each answer of the provided five ones carries one of the following labels:
• direct: direct answer to the question; • related: not directly answering the question, but contains related information;
• irrelevant: answer to another question not related to the topic.
Similarly to the English corpus, a dataset file is a sequence of examples (Questions), where each question has a subject and a body (text), as well as the following attributes:
• QID: internal question identifier; • QCATEGORY: question category;
• QDATE: date of posting.
Each question is followed by a list of possible answers. An answer has a subject and a body (text), as well as the following attributes:
• CID: answer identifier; • CGOLD: label of the answer, which is one of three: direct, related, or irrelevant.
Moreover, the answer body text can contain tags such as the following:
• NE: named entities in the text, usually person names; • Quran: verse from the Quran; • Hadeeth: saying by the Islamic prophet.
Figure 2 shows some fully annotated Arabic question from the Fatwa corpus.  

Annotating the Fatwa corpus
We selected the shortest questions and answers from IslamWeb to create our training, development and testing datasets. We avoided long questions and answers since they are likely to be harder to parse, analyse, and classify. For each question, we labeled its answer as direct, the answers of linked questions as related, and we selected some random answers as irrelevant to make the total number of provided answers per question equal to 5.
Table 2 shows some statistics about the resulting datasets. We can see that the number of direct answers is the same as the number of questions, since each question has only one direct answer.
One issue with selecting random answers as irrelevant is that the task is too easy; thus, we manually annotated a special hard testset of 30 questions (Test30), where we selected the irrelevant answers using information retrieval to guarantee significant term overlap with the questions. For the general testset, we used these 30 questions and 170 more where the irrelevant answers were chosen randomly.

Scoring
The official score for both subtasks is F 1 , macroaveraged over the target categories:
• For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure.
We also report classification accuracy.   The first column shows the rank for the primary submissions according to macro F 1 , and the subindex in the last column shows the rank based on accuracy. Teams marked with a include a task co-organizer.

Participants and Results
The list of all participating teams can be found in Table 3. The results for subtask A, English and Arabic, are shown in Tables 4-5 and 6-7, respectively; those for subtask B are in Table 8. The systems are ranked by their macro-averaged F 1 scores for their primary runs (shown in the first column); a ranking based on accuracy is also shown as a subindex in the last column. We mark explicitly with an asterisk the teams that had a task co-organizer as a team member. This is for information only; these teams competed in the same conditions as everybody else.  Teams marked with a include a task co-organizer.

Subtask A, English
Table 4 shows the results for subtask A, English, which attracted 12 teams, which submitted 30 runs: 12 primary and 18 contrastive. We can see that all submissions outperform, in terms of macro F 1 , the majority class baseline that always predicts Good (shown in the last line of the table ); for the primary submissions, this is so by a large margin. However, in terms of accuracy, one of the primary submissions falls below the baseline; this might be due to them optimizing for macro F 1 rather than for accuracy.
The best system for this subtask is JAIST, which ranks first both in the official macro F 1 score (57.19) and in accuracy (72.52); it used a supervised featurerich approach, which includes topic models and word vector representation, with an SVM classifier.
The second best system is HITSZ-ICRC, which used an ensemble of classifiers. While it ranked second in terms of macro F 1 (56.41), it was only fifth on accuracy (68.67); the second best in accuracy was ECNU, with 70.55.
The third best system, in both macro F 1 (53.74) and accuracy (70.50), is QCRI. In addition to the features they used for Arabic (see the next subsection), they further added cosine similarity based on word embeddings, sentiment polarity lexicons, and metadata features such as the identity of the users asking and answering the questions or the existence of acknowledgments. Interestingly, the top two systems have contrastive runs that scored higher than their primary runs both in terms of macro F 1 and accuracy, even though these differences are small. This is also true for QCRI's contrastive run in terms of macro F 1 but not in terms of accuracy, which indicates that they optimized for macro F 1 for that contrastive run. Note that ECNU was very close behind QCRI in macro F 1 (53.47), and it slightly outperformed it in accuracy.
Note that while most systems trained a four-way classifier to distinguish Good/Bad/Potential/Dialog, where Bad includes Bad, Not English and Other, some systems targetted a three-way distinction Good/Bad/Potential, following the grouping in Table 1, as for the official scoring the scorer was merging Dialog with Bad anyway.
Table 5 shows the results with four classes. The last four systems did not predict Dialog, and thus are severely penalized by macro F 1 . Comparing Tables 4 and 5, we can see that the scores for the 4-way classification are up to 10 points lower than for the 3-way case. Distinguishing Dialog from Bad turns out to be very hard: e.g., HITSZ-ICRC achieved an F 1 of 76.52 for Good, 18.41 for Potential, 40.38 for Bad, 57.21 for Dialog; however, merging Bad and Dialog yielded an F 1 of 74.32 for the Bad+Dialog category. The other systems show a similar trend.
Finally, note that Potential is by far the hardest class (with an F 1 lower than 20 for all teams), and it is also the smallest one, which amplifies its weight with F 1 macro; thus, two teams (CoMiC and FBK-HLT) have chosen never to predict it.

Subtask A, Arabic
Table 6 shows the results for subtask A, Arabic, which attracted four teams, which submitted a total of 11 runs: 4 primary and 7 contrastive. All teams performed well above a majority class baseline that always predicts irrelevant.
QCRI was a clear winner with a macro F 1 of 78.55 and accuracy of 83.02. They used a set of features composed of lexical similarities and word [1, 2]-grams. Most importantly, they exploited the fact that there is at most one good answer for a given question: they rank the answers by means of logistic regression, and label the top answer as direct, the next one as related and the remaining as irrelevant (a similar strategy is used by some other teams too).   Even though QCRI did not consider semantic models for this subtask, and the second best team did, the distance between them is sizeable.
The second place went to VectorSLU (F 1 =70.99, Acc=76.32), whose feature vectors incorporated text-based similarities, embedded word vectors from both the question and answers, and features based on normalized ranking scores. Their word embeddings were generated with word2vec (Mikolov et al., 2013), and trained on the Arabic Gigaword corpus. Their contrastive condition labeled the top scoring response as direct, the second best as related, and the others as irrelevant. Their primary condition did not make use of this constraint.
Then come HITSZ-ICRC and Al-Bayan, which are tied on accuracy (74.53), and are almost tied on macro F 1 : 67.70 vs. 67.65. HITSZ-ICRC translated the Arabic to English and then extracted features from both the Arabic original and from the English translation. Al-Bayan had a knowledge-rich approach that used MADA for morphological analysis, and then combined information retrieval scores with explicit semantic analysis in a decision tree.
For all submitted runs, identifying the irrelevant answers was easiest, with F 1 for this class ranging from 85% to 91%. This was expected, since most of these answers were randomly selected and thus the probability of finding common terms between them and the questions was low. The F 1 for detecting the direct answers ranged from 67% to 77%, while for the related answers, it was lowest: 47% to 67%.
Table 7 presents the results for the 30 manually annotated Arabic questions, for which a search engine was used to find possibly irrelevant answers. We can see that the results are much lower than those reported in Table 6, which shows that detecting direct and related answers is more challenging when the irrelevant answers contain many common terms with the question. The decrease in performance can be also explained by the different class distribution in training and testing, e.g., on the average, there are 1.5 direct answers in Test30 vs. just 1 in training, and the proportion of irrelevant also changed (see Table 2). The team ranking changed too. QCRI remained the best-performing team, but the worst performing group now has one of its contrastive runs doing quite well. VectorSLU, which relies heavily on word overlap and similarity between the question and the answer experienced a relatively higher drop in performance compared to the rest. In future work, we plan to study further the impact of selecting the irrelevant answers in various challenging ways.

Subtask B, English
Table 8 shows the results for subtask B, English, which attracted eight teams, who submitted a total of 20 runs: 8 primary and 12 contrastive. As for subtask A, all submissions outperformed the majority class baseline that always predicts Yes (shown in the last line of the table). However, this is so in terms of macro F 1 only; in terms of accuracy, only half of the systems managed to beat the baseline.  The first column shows the rank for the primary submissions according to macro F 1 , and the subindex in the last column shows the rank based on accuracy. Teams marked with a include a task co-organizer. The submission marked with a was corrected after the deadline.
For most teams, the features used for subtask B were almost the same as for subtask A, with some teams adding extra features, e.g., that look for positive, negative and uncertainty words from small hand-crafted dictionaries.
Most teams designed systems that make Yes/No/Unsure decisions at the comment level, predicting CGOLD YN labels (typically, for the comments that were predicted to be Good by the team's system for subtask A), and were then assigned a question-level label using majority voting. 4 This is a reasonable strategy as it mirrors the human annotation process. Some teams tried to extract features from the whole list of comments and to predict QGOLD YN directly, but this yielded drop in performance.
The top-performing system, in both macro F 1 (63.7) and accuracy (72), is VectorSLU. It is followed by ECNU with F 1 =55.8, Acc=68. The third place is shared by QCRI and HITSZ-ICRC, which have exactly the same scores (F 1 =53.6, Acc=64), but different errors and different confusion matrices. These four systems are much better than the rest; the next system is far behind at F 1 =38.8, Acc=44. Interestingly, once again there is a tie for the third place between the participating teams, as was the case for subtask A, Arabic and English. Note, however, that this time all top systems' primary runs performed better than their corresponding contrastive runs, which was not the case for subtask A.

Features and Techniques
Most systems were supervised, 5 and thus the main efforts were focused on feature engineering. We can group the features participants used into the following four categories:
• question-specific features: e.g., length of the question, words/stems/lemmata/n-grams in the question, etc.
• comment-specific features: e.g., length of the comment, words/stems/lemmata/n-grams in the question, punctuation (e.g., does the comment contain a question mark), proportion of positive/negative sentiment words, rank of the comment in the list of comments, named entities (locations, organizations), formality of the language used, surface features (e.g., phones, URLs), etc.
• features about the question-comment pair: various kinds of similarity between the question and the comment (e.g., lexical based on cosine, or based on WordNet, language modeling, topic models such as LDA or explicit semantic analysis), word/lemma/stem/n-gram/POS overlap between the question and the comment (e.g., greedy string tiling, longest common subsequences, Jaccard coefficient, containment, etc.), information gain from the comment with respect to the question, etc.
• metadata features: ID of the user who asked the question, ID of the one who posted the comment, whether they are the same, known number of Good/Bad/Potential comments (in the training data) written by the user who wrote the comment, timestamp, question category, etc.
Note that the metadata features overlap with the other three groups as a metadata feature is about the question, about the comment, or about the questioncomment pair. Note also that the features above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems.
Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing graph traversal, using word alignments between the question and the comment, time modeling, and sentiment analysis. Finally, for Arabic, some participants translated the Arabic data to English, and then extracted features from both the Arabic and the English version; this is helpful, as there are many more tools and resources for English than for Arabic.
When building their systems, participants used a number of tools and resources for preprocessing, feature extraction, and machine learning, including Deeplearning4J, DKPro, GATE, GloVe, Google translate, HeidelTime, LibLinear, LibSVM, MADA, Mallet, Meteor, Networkx, NLTK, NRC-Canada sentiment lexicons, PPDB, sklearn, Spam filtering corpus, Stanford NLP toolkit, TakeLab, TiMBL, UIMA, Weka, Wikipedia, Wiktionary, word2vec, WordNet, and WTMF.
There was also a rich variety of preprocessing techniques used, including sentence splitting, tokenization, stemming, lemmatization, morphological analysis (esp. for Arabic), dependency parsing, part of speech tagging, temporal tagging, named entity recognition, gazetteer matching, word alignment between the question and the comment, word embedding, spam filtering, removing some content (e.g., all contents enclosed in HTML tags, emoticons, repetitive punctuation, stopwords, the ending signature, URLs, etc.) substituting (e.g., HTML character encodings and some common slang words), etc.

Discussion
The task attracted 13 teams and 61 submissions. Naturally, the English subtasks were more popular (with 12 and 8 teams for subtasks A and B, respectively; compared to just 4 for Arabic): there are more tools and resources for English as well as more general research interest. Moreover, the English data followed the natural discussion threads in a forum, while the Arabic data was somewhat artificial.
We have seen that all submissions managed to outperform, on the official macro F 1 metric, 6 a majority class baseline for both subtasks and for both languages; this improvement is smaller for English and much larger for Arabic. However, if we consider accuracy, many systems fall below the baseline for English in both subtasks.
Overall, the results for Arabic are higher than those for English for subtask A, e.g., there is an absolute difference of over 21 points in macro F 1 (78.55 vs. 57.19) for the top systems. This suggests that the Arabic task was generally easier. Indeed, it uses very formal polished language both for the questions and the answers (as opposed to the noisy English forum data); moreover, it is known a priori that each question can have at most one direct answer, and the teams have exploited this information.
However, looking at accuracy, the difference between the top systems for Arabic and English is just 10 points (82.02 vs. 72.52). This suggests that part of the bigger difference for F 1 macro comes from the measure itself. Indeed, having a closer look at the distribution of the F 1 values for the different classes before the macro averaging, we can see that the results are much more balanced for Arabic (F 1 of 77.31/67.13/91.21 for direct/related/irrelevant; with P and R very close to F 1 ) than for English (F 1 of 78.96/14.36/78.24 for Good/Potential/Bad; with P and R very close to F 1 ). We can see that the Potential class is the hardest. This can hurt the accuracy but only slightly as this class is the smallest. However, it can still have a major impact on macro-F 1 due to the effect of macro-averaging.
Overall, for both Arabic and English, it was much easier to recognize Good/direct and Bad/irrelevant examples (P, R, F 1 about 80-90), and much harder to do so for Potential/related (P, R, F 1 around 67 for Arabic, and 14 for English). This should not be surprising, as this intermediate category is easily confusable with the other two: for Arabic, these are answers to related questions, while for English, this is a category that was quite hard for human annotators.
We should say that even though we had used majority voting to ensure agreement between annotators, we were still worried about the the quality of human annotations collected on Amazon's Mechanical Turk. Thus, we asked eight people to do a manual re-annotation of the QGOLD YN labels for the test data. We found a very high degree of agreement between each of the human annotators and the Turkers. Originally, there were 29 YES/NO questions, but we found that four of them were arguably general rather than YES/NO, and thus we excluded them. For the remaining 25 questions, we had a discussion between our annotators about any potential disagreement, and finally, we arrived with a new annotation that changed the labels of three questions. This corresponds to an agreement of 22/25=0.88 between our consolidated annotation and the Turkers, which is very high. This new annotation was the one we used for the final scoring. Note that using the original Turkers' labels yielded slightly different scores but exactly the same ranking for the systems. The high agreement between our re-annotations and the Turkers and the fact that the ranking did not change makes us optimistic about the quality of the annotations for subtask A too (even though we are aware of some errors and inconsistencies in the annotations).

Conclusion and Future Work
We have described a new task that entered SemEval-2015: task 3 on Answer Selection in Community Question Answering. The task has attracted a reasonably high number of submissions: a total of 61 by 13 teams. The teams experimented with a large number of features, resources and approaches, and we believe that the lessons learned will be useful for the overall development of the field of community question answering. Moreover, the datasets that we have created as part of the task, and which we have released for use to the community, 7 should be useful beyond SemEval.
In our task description, we especially encouraged solutions going beyond simple keyword and bagof-words matching, e.g., using semantic or complex linguistic information in order to reason about the relation between questions and answers. Although participants experimented with a broad variety of features (including semantic word-based representations, syntactic relations, contextual features, meta-information, and external resources), we feel that much more can be done in this direction. Ultimately, the question of whether complex linguistically-based representations and inference can be successfully applied to the very informal and ungrammatical text from cQA forums remains unanswered to a large extent.
Complementary to the research direction presented by this year's task, we plan to run a followup task at SemEval-2016, with a focus on answering new questions, i.e., that were not already answered in Qatar Living. For Arabic, we plan to use a real community question answering dataset, similar to Qatar Living for English.

