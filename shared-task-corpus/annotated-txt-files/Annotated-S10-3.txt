title
SemEval-2010 Task 3: Cross-Lingual Word Sense Disambiguation
abstract
The goal of this task is to evaluate the feasibility of multilingual WSD on a newly developed multilingual lexical sample data set. Participants were asked to <TASK>automatically determine the contextually appropriate translation of a given English noun in five languages, viz. Dutch, German, Italian, Spanish and French</TASK>. This paper reports on the sixteen submissions from the five different participating teams.

Introduction
Word Sense Disambiguation, the task of selecting the correct sense of an ambiguous word in a given context, is a well-researched NLP problem (see for example Agirre and Edmonds (2006) and Navigli (2009)), largely boosted by the various Senseval and SemEval editions. The SemEval-2010 Cross-lingual Word Sense Disambiguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)).
The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993;Ide et al., 2002;Ng et al., 2003;Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted from the multilingual corpus Europarl 1 and (ii) a data set in which the ambiguous words were annotated with the senses from the multilingual sense inventory. The Cross-Lingual WSD task is a lexical sample task for English nouns, in which the word senses are made up of the translations in five languages, viz. Dutch, French, Italian, Spanish and German. Both the sense inventory and the annotated data set were constructed for a sample of 25 nouns. The data set was divided into a trial set of 5 ambiguous nouns and a test set of 20 nouns. The participants had to automatically determine the contextually appropriate translation for a given English noun in each or a subset of the five target languages. Only translations present in Europarl were considered as valid translations.
The remainder of this article is organized as follows. Section 2 focuses on the task description and gives a short overview of the construction of the sense inventory and the annotation of the benchmark data set with the senses from the multilingual sense inventory. Section 3 clarifies the scoring metrics and presents two frequency-based baselines. The participating systems are presented in Section 4, while the results of the task are discussed in Section 5. Section 6 concludes this paper.

Task setup
Data sets
Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages.
Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009;2010).
The gold standard sense inventory was derived from the Europarl parallel corpus 2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the JRC-ACQUIS Multilingual Parallel Corpus 3 and BNC 4 .

Creation of the sense inventory
Two steps were taken to obtain a multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations.
GIZA++ (Och and Ney, 2003) was used to generate the initial word alignments, which were manually verified by certified translators in all six involved languages. The human annotators were asked to assign a "NULL" link to words for which no valid translation could be identified. Furthermore, they were also asked to provide extra information on compound translations (e.g. the Dutch word Investeringsbank as a translation of the English multiword Investment Bank ), fuzzy links, or target words with a different PoS (e.g. the verb to bank ).
The manually verified translations were clustered by meaning by one annotator. In order to do so, the translations were linked across languages on the basis of unique sentence IDs.
After the selection of all unique translation combinations, the translations were grouped into clusters. The clusters were organized in two levels, in which the top level reflects the main sense categories (e.g. for the word coach we have (1) (sports) manager, (2) bus, (3) carriage and (4) part of a train), and the subclusters represent the finer sense distinctions. Translations that correspond to English multiword units were identified and in case of non-apparent compounds, i.e. compounds which are not marked with a "-", the different compound parts were separated by § § in the clustering file (e.g. the German Post § §kutsche). All clustered translations were also manually lemmatized.

Sense annotation of the test data
The resulting sense inventory was used to annotate the sentences in the trial set (20 sentences per ambiguous word) and the test set (50 sentences per ambiguous word). In total, 1100 sentences were annotated. The annotators were asked to (a) pick the contextually appropriate sense cluster and to (b) choose their three preferred translations from this cluster. In case they were not able to find three appropriate translations, they were also allowed to provide fewer. These potentially different translations were used to assign frequency weights (shown in example (2)) to the gold standard translations per sentence. The example (1) below shows the annotation result in both German and Dutch for an English source sentence containing coach.
(1) SENTENCE 12. STRANGELY , the national coach of the Irish teams down the years has had little direct contact with the four provincial coaches . For each instance, the gold standard that results from the manual annotation contains a set of translations that are enriched with frequency information. The format of both the input file and gold standard is similar to the format that will be used for the Sem-Eval Cross-Lingual Lexical Substitution task (Sinha and Mihalcea, 2009). The following example illustrates the six-language gold standard format for the trial sentence in (1). The first field contains the target word, PoS-tag and language code, the second field contains the sentence ID and the third field contains the gold standard translations in the target language, enriched with their frequency weight:
(2) coach.n.nl 12 :: coach 3; speler-trainer 1; trainer 3; voetbaltrainer 1; coach.n.fr 12 :: capitaine 1; entraîneur 3; coach.n.de 12 :: Coach 1; Fußbaltrainer 1; Nationaltrainer 2; Trainer 3; coach.n.it 12 :: allenatore 3; coach.n.es 12 :: entrenador 3;

Evaluation
Scoring
To score the participating systems, we use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). We perform both a best result evaluation and a more relaxed evaluation for the top five results. The evaluation is performed using precision and recall (P rec and Rec in the equations below), and Mode precision (M P ) and Mode recall (M R ), where we calculate precision and recall against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others.
For the precision and recall formula we use the following variables. Let H be the set of annotators, T the set of test items and h i the set of responses for an item i ∈ T for annotator h ∈ H. For each i ∈ T we calculate the mode (m i ) which corresponds to the translation with the highest frequency weight. For a detailed overview of the M P and M R calculations, we refer to McCarthy and Navigli (2007). Let A be the set of items from T (and T M ) where the system provides at least one answer and a i : i ∈ A the set of guesses from the system for item i. For each i, we calculate the multiset union (H i ) for all h i for all h ∈ H and for each unique type (res) in H i that has an associated frequency (f req res ). In order to assign frequency weights to our gold standard translations, we asked our human annotators to indicate their top 3 translations, which enables us to also obtain meaningful associated frequencies (f req res ) viz. "1" in case a translation is picked by 1 annotator, "2" if picked by two annotators and "3" if chosen by all three annotators.
Best result evaluation For the best result evaluation, systems can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output a lot of guesses are not favoured.
Out-of-five (Oof ) evaluation For the more relaxed evaluation, systems can propose up to five guesses. For this evaluation, the resulting score is not divided by the number of guesses.
P rec =

Baselines
We produced two frequency-based baselines:
1. For the Best result evaluation, we select the most frequent lemmatized translation that results from the automated word alignment process (GIZA++).
2. For the Out-of-five or more relaxed evaluation, we select the five most frequent (lemmatized) translations that result from the GIZA++ alignment.
Table 1 shows the baselines for the Best evaluation, while Table 2 gives an overview per language of the baselines for the Out-offive evaluation.  

Systems
We received sixteen submissions from five different participating teams. One group tackled all five target languages, whereas the other groups focused on four (one team), two (one team) or one (two teams) target language(s). For both the best and the Out-of-five evaluation tasks, there were between three and seven participating systems per language.
The OWNS system identifies the nearest neighbors of the test instances from the training data using a pairwise similarity measure (weighted sum of the word overlap and semantic overlap between two sentences). They use WordNet similarity measures as an additional information source, while the other teams merely rely on parallel corpora to extract all lexical information. The UvT-WSD systems use a k-nearest neighbour classifier in the form of one word expert per lemma-Part-of-Speech pair to be disambiguated. The classifier takes as input a variety of local and global context features. Both the FCC-WSD and T3-COLEUR systems use bilingual translation probability tables that are derived from the Europarl corpus. The FCC-WSD system uses a Naive Bayes classifier, while the T3-COLEUR system uses an unsupervised graph-based method. Finally, the UHD systems build for each target word a multilingual co-occurrence graph based on the target word's aligned contexts found in parallel corpora. The cross-lingual nodes are first linked by translation edges, that are labeled with the translations of the target word in the corresponding contexts. The graph is transformed into a minimum spanning tree which is used to select the most relevant words in context to disambiguate a given test instance.

Results
For the system evaluation results, we show precision (P rec), recall (Rec), Mode precision (M P ) and Mode recall (M R ). We ranked all system results according to recall, as was done for the Lexical Substitution task. Table 3 shows the system ranking on the best task, while Table 4 shows the results for the Oof task.

Prec
Rec  Beating the baseline seems to be quite challenging for this WSD task. While the best systems outperform the baseline for the best task, Table 4: Out-of-five System Results this is not always the case for the Out-of-five task. This is not surprising though, as the Oof baseline contains the five most frequent Europarl translations. As a consequence, these translations usually contain the most frequent translations from different sense clusters, and in addition they also contain the most generic translation that often covers multiple senses of the target word.
The best results are achieved by the UvT-WSD (Spanish, Dutch) and ColEur (French, Italian and German) systems. An interesting feature that these systems have in common, is that they extract all lexical information from the parallel corpus at hand, and do not need any additional data sources. As a consequence, the systems can easily be applied to other languages as well. This is clearly illustrated by the ColEur system, that participated for all supported languages, and outperformed the other systems for three of the five languages.
In general, we notice that Spanish and French have the highest scores, followed by Italian, whereas Dutch and German seem to be more challenging. The same observation can be made for both the Oof and Best results, except for Italian that performs worse than Dutch for the latter. However, given the low participation rate for Italian, we do not have sufficient information to explain this different behaviour on the two tasks. The discrepancy between the performance figures for Spanish and French on the one hand, and German and Dutch on the other hand, seems more readily explicable. A likely explanation could be the number of classes (or translations) the systems have to choose from. As both Dutch and German are characterized by a rich compounding system, these compound translations also result in a higher number of different translations. Figure 1 illustrates this by listing the number of different translations (or classes in the context of WSD) for all trial and test words. As a result, the broader set of translations makes the WSD task, that consists in choosing the most appropriate translation from all possible translations for a given instance, more complicated for Dutch and German.

Concluding remarks
We believe that the Cross-lingual Word Sense Disambiguation task is an interesting contribution to the domain, as it attempts to address two WSD problems which have received a lot of attention lately, namely (1) the scarcity of hand-crafted sense inventories and sensetagged corpora and (2) the need to make WSD more suited for practical applications.
The system results lead to the following observations. Firstly, languages which make extensive use of single word compounds seem harder to tackle, which is also reflected in the baseline scores. A possible explanation for this phenomenon could lie in the number of translations the systems have to choose from. Secondly, it is striking that the systems with the highest performance solely rely on parallel corpora as a source of information. This would seem very promising for future multilingual WSD research; by eliminating the need for external information sources, these systems present a more flexible and languageindependent approach to WSD.


