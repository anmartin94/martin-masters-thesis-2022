title
The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion
abstract
In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which <TASK>take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma</TASK>. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.

Introduction
In morphologically rich languages, words inflect: grammatical information like person, number, tense, and case are incorporated into the word itself, rather than expressed via function words. Not all languages mark the same properties: German nouns, for instance, have more inflected forms than their English counterparts.
When acquiring a language, humans usually learn to inflect words without explicit instruction. Thus, most native speakers are capable of generating inflected forms even of artificial lemmas (Berko, 1958). However, models that can generate paradigms without explicit morphological train- * Equal contribution. Figure 1: The task of unsupervised morphological paradigm completion (Jin et al., 2020) consists of generating complete inflectional paradigms for given lemmas, with the only additional available information being a corpus without annotations. ing have not yet been developed. We anticipate that such systems will be extremely useful, as they will open the possibility of rapid development of first-pass inflectional paradigms in a large set of languages. These can be utilized both in se for generation and as a starting point for elicitation , thus aiding the development of low-resource human language technologies (Christianson et al., 2018).
In this paper, we present the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2). We asked participants to produce systems that can learn to inflect in an unsupervised fashion: given a small corpus (the Bible) together with a list of lemmas for each language, systems for the shared task should output all corresponding inflected forms. In their output, systems had to mark which forms expressed the same morphosyntactic features, e.g., demonstrate knowledge of the fact that walks is to walk as listens is to listen, despite not recognizing the morphological features explic-itly. We show a visualization of our shared task setup in Figure 1.
Unsupervised morphological paradigm completion requires solving multiple subproblems either explicitly or implicitly. First, a system needs to figure out which words in the corpus belong to the same paradigm. This can, for instance, be done via string similarity: walks is similar to walk, but less so to listen. Second, it needs to figure out the shape of the paradigm. This requires detecting which forms of different lemmas express the same morphosyntactic features, even if they are not constructed from their respective lemmas in the exact same way. Third, a system needs to generate all forms not attested in the provided corpus. Using the collected inflected forms as training data, this can be reduced to the supervised morphological inflection task (Cotterell et al., 2016). This year's submitted systems can be split into two categories: those that built on the baseline (Retrieval+X) and those that did not (Segment+Conquer). The baseline system is set up as a pipeline which performs the following steps: edit tree retrieval, additional lemma retrieval, paradigm size discovery, and inflection generation (Jin et al., 2020). As it is highly modular, we provided two versions that employ different inflection models. 1 All systems built on the baseline substituted the morphological inflection component.
No system outperformed the baseline overall. However, two Retrieval+X models slightly improved over the baseline on three individual languages. We conclude that the task of unsupervised morphological paradigm completion is still an open challenge, and we hope that this shared task will inspire future research in this area.

Task and Evaluation
Unsupervised Morphological Paradigm Completion
Informal description. The task of unsupervised morphological paradigm completion mimics a setting where the only resources available in a language are a corpus and a short list of dictionary forms, i.e., lemmas. The latter could, for instance, be obtained via basic word-to-word translation.
The goal is to generate all inflected forms of the given lemmas.
For an English example, assume the following lemma list to be given: walk listen With the help of raw text, systems should then produce an output like this: walk walk 1 walk walks 2 walk walked 3 walk walking 4 walk walked 5
(1) listen listens 2 listen listened 5 listen listened 3 listen listening 4 listen listen 1
The numbers serve as unique identifiers for paradigm slots: in above example, "4" corresponds to the present participle. The inflections walking and talking therefore belong to the same paradigm slot. For the task, participants are not provided any knowledge of the grammatical content of the slots.
Formal definition. We denote the paradigm π( ) of a lemma as
with f : Σ * × T → Σ * being a function that maps a lemma and a vector of morphological features t γ ∈ T expressed by paradigm slot γ to the corresponding inflected form. Γ( ) is the set of slots in lemma 's paradigm. We then formally describe the task of unsupervised morphological paradigm completion as follows. Given a corpus D = w 1 , . . . , w |D| together with a list L = { j } of |L| lemmas belonging to the same part of speech, 2 unsupervised morphological paradigm completion consists of generating the paradigms {π( )} of all lemmas ∈ L.
Remarks. It is impossible for unsupervised systems to predict the names of the features expressed by paradigm slots, an arbitrary decision made by human annotators. This is why, for the shared task, we asked systems to mark which forms belong to the same slot by numbering them, e.g., to predict that walked is the form for slot 3, while listens corresponds to slot 2.

Macro-averaged Best-Match Accuracy
The official evaluation metric was macro-averaged best-match accuracy (BMAcc; Jin et al., 2020).
In contrast to supervised morphological inflection (Cotterell et al., 2016), our task cannot be evaluated with word-level accuracy. For the former, one can compare the prediction for each lemma and morphological feature vector to the ground truth. However, for unsupervised paradigm completion, this requires a mapping from predicted slots to the gold standard's paradigm slots.
BMAcc, thus, first computes the word-level accuracy each predicted slot would obtain against each true slot. It then constructs a complete bipartite graph, with those accuracies as edge weights. This enables computing of the maximum-weight full matching with the algorithm of Karp (1980). BMAcc then corresponds to the sum of all accuracies for the best matching, divided by the maximum of the number of gold and predicted slots.
BMAcc penalizes systems for predicting a wrong number of paradigm slots. However, detecting the correct number of identical slots -something we encounter in some languages due to syncretism -is extremely challenging. Thus, we merge slots with identical forms for all lemmas in both the predictions and the ground truth before evaluating.
Example. Assume our gold standard is (1) (the complete, 5-slot English paradigms for the verbs walk and listen) and a system outputs the following, including an error in the fourth row: walk walks 1 walk walking 2 listen listens 1 listen listenen 2 First, we merge slots 3 and 5 in the gold standard, since they are identical for both lemmas. Ignoring slot 5, we then compute the BMAcc as follows. Slot 1 yields an accuracy of 100% as compared to gold slot 2, and 0% otherwise. Similarly, slot 2 reaches an accuracy of 50% for gold slot 4, and 0% otherwise. Additionally, given the best mapping of those two slots, we obtain 0% accuracy for gold slots 1 and 3. Thus, the BMAcc is BMAcc = 1 + 0.5 + 0 + 0 4 = 0.375 (3)
3 Shared Task Data

Provided Resources
We provided data for 5 development and 9 test languages. The development languages were available for system development and hyperparameter tuning, while the test languages were released shortly before the shared task deadline. For the test languages, no ground truth data was available before system submission. This setup emulated a realworld scenario with the goal to create a system for languages about which we have no information.
For the raw text corpora, we leveraged the JHU Bible Corpus . This resource covers 1600 languages, which will enable future work to quickly produce systems for a large set of languages. Additionally, using the Bible allowed for a fair comparison of models across languages without potential confounds such as domain mismatch. 7 of the languages have only the New Testament available (approximately 8k sentences), and 7 have both the New and Old Testaments (approximately 31k sentences).
All morphological information was taken from UniMorph (Sylak-Glassman et al., 2015;Kirov et al., 2018), a resource which contains paradigms for more than 100 languages. However, this information was only accessible to the participants for the development languages. UniMorph paradigms were further used internally for evaluation on the test languages-this data was then released after the conclusion of the shared task.

Languages
During the development phase of the shared task, we released 5 languages to allow participants to investigate various design decisions: Maltese (MLT), Persian (FAS), Portuguese (POR), Russian (RUS), and Swedish (SWE). These languages are typologically and genetically varied, representing a number of verbal inflectional phenomena. Swedish and Portuguese are typical of Western European languages, and mostly exhibit fusional, suffixing verbal inflection. Russian, as an exemplar of Slavic languages, is still mostly suffixing, but does observe regular ablaut, and has considerable phonologicallyconditioned allomorphy. Maltese is a Semitic language with a heavy Romance influence, and verbs   combine templatic and suffixing inflection. Persian is mostly suffixing, but does allow for verbal inflectional prefixation, such as negation and marking subjunctive mood. Since the development languages were used for system tuning, their scores did not count towards the final ranking.
After a suitable period for system development and tuning, we released nine test languages: Basque (EUS), Bulgarian (BUL), English (ENG), Finnish (FIN), German (DEU), Kannada (KAN), Navajo (NAV), Spanish (SPA), and Turkish (TUR). Although these languages observe many features common to the development languages, such as fusional inflection, suffixation, and ablaut, they also cover inflectional categories absent in the development languages. Navajo, unlike any of the development languages, is strongly prefixing. Basque, Finnish, and Turkish are largely agglutinative, with long, complex affix chains that are difficult to identify through longest suffix matching. Furthermore, Finnish and Turkish feature vowel harmony and consonant gradation, which both require a method to identify allomorphs correctly to be able to merge different variants of the same paradigm slot.

Statistics
Statistics of the resources provided for all languages are shown in Table 1 for the development languages and in Table 2 for the test languages.
The token count (line 1) and, thus, the size of the provided Bible corpora, differs between 104,631 (Kannada) and 871,707 (Swedish). This number depends both on the typology of a language and on the completeness of the provided Bible translation. The number of types (line 2) is between 7,144 (English) and 59,458 (Turkish). It is strongly influenced by how morphologically rich a language is, i.e., how large the paradigms are, which is often approximated with the type-token ratio. The verbal paradigm size is listed in line 7: English has with a size of 5 the smallest paradigms, and, correspondingly, the lowest type count. Turkish, which has the highest number of types, in contrast, has large paradigms (120). The last line serves as an indicator of syncretism: subtracting line 8 from line 7 results in the number of paradigm slots that have been merged as a language evolved to use identical forms for different inflectional categories.
Lines 3 and 4 show the number of lemmas in the lemma lists for all languages, as well as the   In line 5, we list the number of total inflections, counting each one in the case of identical forms, i.e., this corresponds to the number of lines in our gold inflection file. English, due to its small verbal paradigm size, has only 500 inflections in our data. Conversely, Finnish has with 14,100 the largest number of inflections. Line 6 describes how many of the forms from line 5 appear in the corpus. As before, all forms are counted, even if they are identical. For all languages, a large majority of forms cannot be found in the corpus. This makes the task of unsupervised morphological paradigm completion with our provided data a challenging one.

Systems
In this section, we first review the baseline before describing the submitted systems. An additional overview of the submissions is shown in Table 3.

Baseline
We compared all submissions to the baseline system of Jin et al. (2020), graphically summarized in Figure 2. It is a pipeline system, which consists of 4 separate modules, which, in turn, can be grouped into two major components: retrieval and generation. The retrieval component discovers and returns inflected forms -and, less importantly, additional lemmas -from the provided Bible corpus. The generation component produces new inflected forms which cannot be found in the raw text.
The retrieval component performs three steps: First, it extracts the most common edit trees (Chrupała, 2008), i.e., it detects regularities with regards to word formation, based on the lemma list. If, for instance, both walk and listen are the lemmas provided and both walked and listened are encountered in the corpus, the system notes that appending -ed is a common transformation, which might correspond to an inflectional strategy.
Second, it retrieves new lemmas, with the goal to gather additional evidence for our collected edit trees. If, for instance, it has already identified the suffix -ed as an inflectional marker, finding both pray and prayed in the Bible is an indication that pray might be a lemma. New lemmas can then, in turn, be used to detect new regularities, e.g., in the case that listen and listens as well as pray and prays are attested in the corpus, but walks is not. Due to their complementary nature, components one and two can, as a unit, be applied iteratively to bootstrap a larger list of lemmas and transformations. For the baseline, we apply each of them only once.
Finally, the baseline's retrieval component predicts the paradigm size by analyzing which edit trees might be representing the same inflection. For instance, the suffixes -d and -ed both represent the past tense in English. The output of the retrieval component is a list of inflected forms with their lemmas, annotated with a paradigm slot number.
The generation component receives this output and prepares the data to train an inflectional generator. First, identified inflections are divided into a training and development split, and missing paradigm slots are identified. The generator is trained on the discovered inflections, and new forms are predicted for each missing slot.
We used two morphological inflection systems for the two variants of our baseline: the non-neural baseline from Cotterell et al. (2017) and the model proposed by Makarov and Clematide (2018). Both are highly suitable for the low-resource setting.

Submitted Systems: Retrieval+X
We now describe the first category of shared task submissions: Retrieval+X. Systems in this category leverage the retrieval component of the baseline, while substituting the morphological inflection component with a custom inflection system.
The IMS-CUBoulder team relied on LSTM (Hochreiter and Schmidhuber, 1997) sequence-tosequence models for inflection. In IMS-CUB-1, the generation component is based on the architecture by Bahdanau et al. (2015), but with fewer parameters, as suggested by Kann and Schütze (2016). This model -as well as all other inflection components used for systems in this category -receives the sequence of the lemma's characters and the paradigm slot number as input and produces a sequence of output characters.
Their second system, IMS-CUB-2, uses an LSTM pointer-generator network (See et al., 2017) instead. This architecture has originally been proposed for low-resource morphological inflection by Sharma et al. (2018).
The NYU-CUBoulder team also substituted the baseline's generation component. Their morphological inflection models are ensembles of dif-ferent combinations of transformer sequence-tosequence models (Vaswani et al., 2017) and pointergenerator transformers, a model they introduced for the task.
NYU-CUB-1 is an ensemble of 6 pointergenerator transformers, while NYU-CUB-2 is an ensemble of 6 vanilla transformers. Their last system, NYU-CUB-3, is an ensemble of all 12 models.

Submitted Systems: Segment+Conquer
The KU-CST team did not modify the baseline directly, but, nevertheless, was heavily inspired by it. Their system first employs a charactersegmentation algorithm to identify stem-suffix splits in both the provided lemma list and the corpus, thus identifying potential suffix-replacement rules. Next, k-means is used to cluster the extracted suffixes into allomorphic groups. These suffixes are then concatenated with the most frequent stems obtained from the lemma list, and scored by a language model, in order to arrive at plausible inflectional candidates. This approach is KU-CST-2.
However, KU-CST-2 often produces very small inflectional paradigms; unsurprisingly, given that the provided corpora are small as well, and, thus, any particular lemma is only inflected in limited ways -if at all. Therefore, KU-CST-1 expands the lemma list with a logistic-regression classifier that identifies novel verbs to be added.

Results and Analysis
Results on Development Languages
To encourage reproducibility, we first report the performance of all systems on the development languages in the upper part of Table 4. Although participants were not evaluated on these languages, the results provide insight and enable future researchers to benchmark their progress, while maintaining the held-out status of the test languages.

Official Shared Task Results
We show the official test results in the lower part of Table 4. Baseline-2 obtained the highest BMAcc on average, followed in order by Baseline-1, IMS-CUB-2, and NU-CUB-2. Overall, systems built on top of the baseline, i.e., systems from Re-trieval+X, performed better than systems from Seg-ment+Conquer: the best Segment+Conquer system only reached 4.66% BMAcc on average. This shows the effectiveness of the baseline. However, it also shows that we still have substantial room  35 (9) 29.00 (9) 0.70 (425) 4.98 (40) 24.60 (9) 28.35 (9) 27.30 (9) 27.35 (9) 27.35 (9  for improvement on unsupervised morphological paradigm completion.
Looking at individual languages, Baseline-2 performed best for all languages except for EUS, where NYU-CUB-3 obtained the highest BMAcc, and BUL and KAN, where IMS-CUB-2 was best.

Analysis: Seen and Unseen Lemmas
We further look separately at the results for lemmas which appear in the corpus and those that do not. While seeing a lemma in context might help some systems, we additionally assume that inflections of attested lemmas are also more likely to appear in the corpus. Thus, we expect the performance for seen lemmas to be higher on average.
Examining the performance with respect to observed inflected forms might give cleaner results. However, we instead perform this analysis on a per-lemma basis, since the lemmas are part of a system's input, while the inflected forms are not.
Table 5 shows the performance of all systems for seen and unseen lemmas. Surprisingly, both versions of the baseline show similar BMAcc for both settings with a maximum difference of 0.12% on average. However, the baseline is the only system that performs equally well for unseen lemmas; IMS-CUB-1 observes the largest difference, with an absolute drop of 7.85% BMAcc when generating the paradigms of unseen lemmas. Investigating the cause for IMS-CUB-1's low BMAcc, we manually inspected the English output files, and found that, for unseen lemmas, many generations are nonsensi-cal (e.g., demoates as an inflected form of demodulate). This does not happen in the case of seen lemmas. A similar effect has been found by Kann and Schütze (2018), who concluded that this might be caused by the LSTM sequence-to-sequence model not having seen similar character sequences during training. The fact that IMS-CUB-2, which uses another inflection model, performs better for unseen lemmas confirms this suspicion. Thus, additional training of the inflection component of IMS-CUB-1 on words from the corpus might improve generation. Conversely, the baseline -which benefits from inflection models specifically catered to low-resource settings -is better suited to inflecting unseen lemmas. Overall, we conclude that there is little evidence that the difficulty of the task increases for unseen lemmas. Rather, inflection systems need to compensate for the low contextual variety in their training data.
6 Where from and Where to?

Previous Work
Prior to this shared task, most research on unsupervised systems for morphology was concerned with developing approaches to segment words into morphemes, i.e., their smallest meaning-bearing units (Goldsmith, 2001;Creutz, 2003;Creutz and Lagus, 2007;Snyder and Barzilay, 2008;Goldwater et al., 2009;Kurimo et al., 2010;Kudo and Richardson, 2018). These methods were built around the observation that inflectional morphemes are very common across word types, and leveraged probabil-  35 (171) 15.61 (172) 6.61 (44) 1.69 (1) 13.99 (172) 16.49 (172) 14.63 (172) 14.68 (172) 14.63 (172  97 (29) 21.60 (29) 4.43 (225) 16.37 (40) 20.40 (29) 21.14 (29) 21.17 (29) 21.09 (29) 21.14 (29)  TUR 14.68 (104) 16.38 (104) 0.23(1772) 1.42 (502) 16.98 (104) 18.02 (104) 18.30 (104) 18.70 (104) 18.50 (104  ity estimates such as maximum likelihood (MLE) or maximum a posteriori (MAP) estimations to determine segmentation points, or minimum description length (MDL)-based approaches. However, they tended to make assumptions regarding how morphemes are combined, and worked best for purely concatenative morphology. Furthermore, these methods had no productive method of handling allomorphy-morphemic variance was simply treated as separate morphemes.
The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots and, finally, generation of unobserved forms.
While Xu et al. (2018) did discover something similar to paradigms, those paradigms were a means to a segmentation end and the shape or size of the paradigms was not a subject of their research. Moon et al. (2009) similarly uses segmentation and clustering of affixes to group words into conflation sets, groups of morphologically related words, in an unsupervised way. Their work assumes prefixing and suffixing morphology. In a more task-driven line of research, Soricut and Och (2015) develop an approach to learn morphological transformation rules from observing how consis-tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words.
Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011;Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001;Täckström et al., 2013).
Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morphology.
The first such task (Cotterell et al., 2016) encouraged participants to create inflectional tools in a typologically diverse group of 10 languages. The task was fully-supervised, requiring systems to learn inflectional morphology from a large annotated database. This task is similar to human learners needing to generate inflections of previously unencountered word forms, after having studied thousands of other types.
The second task (Cotterell et al., 2017) extended the first task from 10 to 52 languages and started to encourage the development of tools for the lowresource setting. While the first shared task approximated an adult learner with experience with thousands of word forms, low-resource inflection was closer to the language learner that has only studied a small number of inflections-however, it was closer to L2 learning than L1, as it still required training sets with lemma-inflection-slot triplets. The 2017 edition of the shared task also introduced a paradigm-completion subtask: participants were given partially observed paradigms and asked to generate missing forms, based on complete paradigms observed during training. This could be described as the supervised version of our unsupervised task, and notably did not require participants to identify inflected forms from raw text-a crucial step in L1 learning.
The third year of the shared task (Cotterell et al., 2018) saw a further extension to more than 100 languages and another step away from supervised learning, in the form of a contextual prediction task. This task stripped away inflectional annotations, requiring participants to generate an inflection solely utilizing a provided lemma and sentential cues. This task further imitated language learners, but extended beyond morphological learning to morphosyntactic incorporation. Furthermore, removing the requirement of an inflectional feature vector more closely approximated the generation step in our task. However, it was still supervised in that participants were provided with lemma-inflection pairs in context during training. We, in contrast, made no assumption of the existence of such pairs. Finally, the fourth iteration of the task (Mc-Carthy et al., 2019) again concentrated on lesssupervised inflection. Cross-lingual training allowed low-resource inflectors to leverage information from high-resource languages, while a contextual analysis task flipped the previous year's contextual task on its head-tagging a sentence with inflectional information. This process is very similar to the retrieval portion of our task. We extended this effort to not only identify the paradigm slot of particular word, but to combine learned information from each class to extend and complete existing paradigms. Furthermore, we lifted the requirement of named inflectional features, more closely approximating the problem as approached by L1 language learners.

Future Shared Tasks
Future editions of the shared task could extend this year's Task 2 to a larger variety of languages or parts of speech. Another possible direction is to focus on derivational morphology instead of or in addition to inflectional morphology. We are also considering merging Task 2 with the traditional morphological inflection task: participants could then choose to work on the overall task or on either of the retrieval or generation subproblem.
Finally, we are looking into extending the shared task to use speech data as input. This is closer to how L1 learners acquire morphological knowledge, and, while this could make the task harder in some aspects, it could make it easier in others.

Conclusion
We presented the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), in which participants were asked to generate paradigms without explicit supervision. Surprisingly, no team was able to outperform the provided baseline, a pipeline system, on average over all test languages. Even though 2 submitted systems were better on 3 individual languages, this highlights that the task is still an open challenge for the NLP community. We argue that it is an important one: systems obtaining high performance will be able to aid the development of human language technologies for low-resource languages.
All teams that participated in the shared task devised modular approaches. Thus, it will be easy to include improved components in the future as, for instance, systems for morphological inflection improve. We released all data, the baseline, the evaluation script, and the system outputs in the official repository, 3 in the hope that this shared task will lay the foundation for future research on unsupervised morphological paradigm completion.

