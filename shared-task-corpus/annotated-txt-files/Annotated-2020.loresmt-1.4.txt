title
Findings of the LoResMT 2020 Shared Task on Zero-Shot for Low Resource languages
abstract
This paper presents the findings of the LoResMT 2020 Shared Task on zero-shot translation for low resource languages. This task was organised as part of the 3 rd Workshop on Technologies for MT of Low Resource Languages (LoResMT) at AACL-IJCNLP 2020. The focus was on the zero-shot approach as a notable development in Neural Machine Translation to build MT systems for language pairs where parallel corpora are small or even nonexistent. The shared task experience suggests that back-translation and domain adaptation methods result in better accuracy for smallsize datasets. We further noted that, although translation between similar languages is no cakewalk, linguistically distinct languages require more data to give better results.

Introduction
Research and development in Statistical and Neural Machine Translation has rapidly emerged over in the last one decade especially after the availability of several open source machine translation (MT) toolkits like: Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017), Nematus (Sennrich et al., 2017), Marian (Junczys-Dowmunt et al., 2018), etc. For the past few years, researchers, developers, users and commercial organizations are widely using Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Luong et al., 2015;Bahdanau et al., 2016;Vaswani et al., 2017) to enhance the performance of their MT systems. NMT has become the preferred paradigm due to its ability to produce better results. Despite the multiple advantages of using NMT and Statistical Machine Translation (SMT) methods, several challenges are also encountered, the main challenge being the lack of quality data for training the systems. Both SMT and NMT require large-sized parallel corpora. However, out of 7,117 languages, 1 most languages are low resourced or even endangered. This particular challenge has long posed a strong impediment for developing NMT systems for low resource languages (Koehn and Knowles, 2017).
Overcoming this obstacle is an extensive and tedious process. The preparation of a parallel corpus in any language can be a very costly procedure that demands the expertise of language professionals at several levels. It is therefore necessary to exploit the existing resources to build MT systems in low resource languages. Methodologies such as data augmentation, exploitation of monolingual data, cross-lingual transfer etc. are preferred approaches under the aforesaid circumstances.
In the preceding year, a shared task (Karakanta et al., 2019) was organised where a monolingual and parallel corpus for the low-resource languages Bhojpuri, Magahi, Sindhi, and Latvian was provided to create NMT/SMT systems 2 . This year's workshop in an extension to the same objective as last year, but this time the team has focused on the zero-shot approach (Firat et al., 2016) for building quality MT systems. In addition to pivot MT, the zero-shot approach is one notable development in NMT to build MT systems for language pairs where parallel corpora are small or even nonexistent. However, the performance of zero-shot NMT is low compared to pivot MT in general. In this paper, we discuss the results of the LoResMT 2020 shared task, organised as part of the 3 rd Workshop on Technologies for MT of Low Resource Languages (LoResMT) 3 at AACL-IJCNLP 2020 4 . In this task, we solicited participants to <TASK>submit novel zero-shot NMT systems for the following language pairs: • Hindi↔Bhojpuri • Hindi↔Magahi • Russian↔Hindi</TASK>.
The remaining paper is organized as follows. Section 2 presents the setup and schedule of the LoResMT 2020 shared task and Section 3 presents the dataset used in the competition. Section 4 describes the approaches used by participants of the competition and Section 5 presents and analyzes the results they obtained. Finally, 6 concludes this paper and presents avenues for future work.

Task Setup and Schedule
Based on a detailed call for participation, researchers were asked to register themselves. The choice of language pair was left to the participants. These registered participants were sent the links to the training (train) dataset including monolingual and development (dev) data, along with a description of the format and statistics of the dataset. They were allowed to use only additional monolingual data to train the system, with the condition that the additional monolingual dataset should be publicly available. Moreover, participants were allowed to use pretrained word embeddings, and publiclyexisting linguistic models. The participants were given 24 days to experiment and develop a system. After this period, the test set was released and the participants had 5 days to test and upload their system using the following abbreviations:
• "-a" -Only provided development and monolingual corpora.
• "-b"-Any provided corpora, plus publicly available different/similar language's monolingual corpora and/or pretrained/linguistics model (e.g. systems used pretrained word2vec, UDPipe, etc. model).
• "-c" -Any provided corpora, plus any publicly external monolingual corpora.
The complete timeline of the shared task is given in Table 1. Each team was allowed to submit any number of systems for evaluation and their best 3 systems were included in the final ranking presented in   (Papineni et al., 2002), Precision, Recall, F-measure and RIBES (Isozaki et al., 2010).

Datasets
The dataset of this shared task comprises three domains: news, subtitling and/or literature. Details of the collected sources are described below:
• Monolingual dataset: Bhojpuri data was extracted from Wikipedia and online newspapers . Magahi data was collected from blogs (Kumar et al., 2018). Russian data was extracted from the Opensubtitles (OPUS) 5 website. Hindi data was compiled from Wikipedia, pmindia (Haddow and Kirefu, 2020) and OPUS.
• Dev and test dataset: Each language pair's dev and test dataset was built on monolingual data which were manually translated and validated by professional translators, native speakers of the target languages.
The participants of the shared task were provided with more than one million words of monolingual data for each language pair, while 500 manually translated and validated parallel sentences were provided for dev and test set. The complete shared task datasets are available at GitHub 6 . The detailed statistics of the dataset in each language is provided in Table 2.    

Participants and Methodology
A total of 5 participants registered for the shared task, with most of the teams registering to participate for Hindi↔Bhojpuri and Hindi↔Magahi language pair except 1 team (see table 3). Out of these, finally a total of 6 systems were submitted by CNLP-NITS and NLPRL teams. All the teams who submitted their system were invited to submit the system description paper, describing the experiments conducted by them. Table 3 lists the participating teams and the language they took part in.
Next, we give a short description of the approach taken by each team for building their system(s). More details about the approaches can be found in the papers submitted by the respective teams.
• CNLP-NITS (Laskar et al., 2020) uses unsupervised masked sequence-to-sequence pretraining for language generation (MASS) (Song et al., 2019) for Hindi-Russian and Russian-Hindi language pair. They used additional Hindi and Russian monolingual data on the same method. This system is submitted as a constrained system.
• NLPRL (Kumar et al., 2020) uses unsupervised domain adaptation and back-translation for Hindi-Bhojpuri, Bhojpuri-Hindi, Hindi-Magahi and Magahi-Hindi using similar Hindi-Nepali data.

Results
As previously mentioned, the participants were allowed to use monolingual datasets, other than that provided. However, due to the lack of a similar substitute monolingual dataset for Bhojpuri and Magahi, participants used only one of the provided data by the shared task organisers. The NLPRL team used orthographically similar Hindi-Nepali data to build their system. On the other hand, the CNLP-NIST team only used additional Hindi and Russian monolingual data for the constrained system submission. As mentioned earlier, for the evaluation of the system, 500 sentences were given to the participants in each language pair for each direction.
The results of the participating teams on Hindi-Bhojpuri, Hindi-Magahi and Hindi-Russian language pairs is presented in Table 4.

Conclusion
In this paper, we have reported the findings of the LoResMT 2020 Shared Task on zero-shot translation for low resource languages, organized as part of the 3 rd LoResMT workshop at AACL-IJCNLP 2020. All the systems submitted used the unsupervised method. We conclude that the use of domain adaptation and back translation methods provides better results for MT system training where the datasets are small-sized. Another concluding point is that the Masked sequence-to-sequence pre-training method provides comparatively low performance on all measures: BLEU, Precision, Recall, F-measure and RIBES. Bhojpuri to Hindi has provided better accuracy scores than vice versa for both the teams who selected Bhojpuri and Hindi as their language pairs. The systems trained for Hindi and Russian did not provide desired results in any language direction, despite them having larger datasets than the other two languages in the shared task. This understanding should be accompanied with the knowledge that Russian and Hindi are completely dissimilar languages belonging to separate language families. We also believe that a human evaluation could provide better insights than automatic evaluation metrics. In the next version of the Shared Task, we are planning to introduce human evaluation of the systems, in order to extend and improve the findings of our Shared Task on low resource languages.

