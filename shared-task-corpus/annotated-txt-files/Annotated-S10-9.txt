title
SemEval-2010 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions
abstract
Previous research has shown that the meaning of many noun-noun compounds N 1 N 2 can be approximated reasonably well by paraphrasing clauses of the form 'N 2 that . . . N 1 ', where '. . . ' stands for a verb with or without a preposition. For example, malaria mosquito is a 'mosquito that carries malaria'. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems.

Introduction
Noun compounds (NCs) are sequences of two or more nouns that act as a single noun, 1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003;Lapata and Lascarides, 2003;Baldwin and Tanaka, 2004;Ó Séaghdha, 2008). As a consequence, Natural Language Processing (NLP) 1 We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or to assume that they can be handled by relying on a dictionary or other static resource.
Trouble with lexical resources for NCs notwithstanding, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing.
Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003;Moldovan et al., 2004;Kim and Baldwin, 2005;Nastase and Szpakowicz, 2006;Girju, 2007;Ó Séaghdha and Copestake, 2007) and paraphrasing (Vanderwende, 1994;Kim and Baldwin, 2006;Butnariu and Veale, 2008;Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as 'oil that is extracted from olive(s)', drug death as 'death that is caused by drug(s)', flu shot as a 'shot that prevents flu'.
The growing popularity -and expected direct utility -of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird's-eye view of the task. Section 2 presents its objective, data, data collection, and evaluation method. Section 3 lists the participating teams. Section 4 shows the results and our analysis. In Section 5, we sum up our experience so far.

Task Description
The Objective
For the purpose of the task, we focused on twoword NCs which are modifier-head pairs of nouns, such as apple pie or malaria mosquito. There are several ways to "attack" the paraphrase-based semantics of such NCs.
We have proposed a rather simple problem: assume that many paraphrases can be found -perhaps via clever Web search -but their relevance is up in the air. Given sufficient training data, we seek to estimate the quality of candidate paraphrases in a test set. Each NC in the training set comes with a long list of verbs in the infinitive (often with a preposition) which may paraphrase the NC adequately. Examples of apt paraphrasing verbs: olive oilbe extracted from, drug death -be caused by, flu shot -prevent. These lists have been constructed from human-proposed paraphrases. For the training data, we also provide the participants with a quality score for each paraphrase, which is a simple count of the number of human subjects who proposed that paraphrase. At test time, <TASK>given a noun compound and a list of paraphrasing verbs, a participating system needs to produce aptness scores that correlate well (in terms of relative ranking) with the held out human judgments</TASK>. There may be a diverse range of paraphrases for a given compound, some of them in fact might be inappropriate, but it can be expected that the distribution over paraphrases estimated from a large number of subjects will indeed be representative of the compound's meaning.

The Datasets
Following Nakov (2008b), we took advantage of the Amazon Mechanical Turk 2 (MTurk) to acquire paraphrasing verbs from human annotators. The service offers inexpensive access to subjects for tasks which require human intelligence. Its API allows a computer program to run tasks easily and collate the subjects' responses. MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al. (2008) for an overview and a further discussion.
Even though we recruited human subjects, whom we required to take a qualification test, 3 data collection was time-consuming since many annotators did not follow the instructions. We had to monitor their progress and to send them timely messages, pointing out mistakes. Although the MTurk service allows task owners to accept or reject individual submissions, rejection was the last resort since it has the triply unpleasant effect of (1) denying the worker her fee, (2) negatively affecting her rating, and (3) lowering our rating as a requester. We thus chose to try and educate our workers "on the fly". Even so, we ended up with many examples which we had to correct manually by labor-intensive post-processing. The flaws were not different from those already described by Nakov (2008b). Post-editing was also necessary to lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we released as trial data the previously collected paraphrase sets (Nakov, 2008b) for the Levi-250 dataset (after further review and cleaning). This dataset consisted of 250 noun-noun compounds form (Levi, 1978), each paraphrased by 25-30 MTurk workers (without a qualification test).
Training Data. The training dataset was an extension of the trial dataset. It consisted of the same 250 noun-noun compounds, but the number of annotators per compound increased significantly. We aimed to recruit at least 30 additional MTurk workers per compound; for some compounds we managed to get many more. For example, when we added the paraphrasing verbs from the trial dataset to the newly collected verbs, we had 131 different workers for neighborhood bars, compared to just 50 for tear gas. On the average, we had 72.7 workers per compound. Each worker was instructed to try to produce at least three paraphrasing verbs, so we ended up with 191.8 paraphrasing verbs per compound, 84.6 of them being unique. See Table 1 for more details.
Test Data. The test dataset consisted of 388 noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003)  

NC paraphrase frequency
where NC is a noun-noun compound (e.g., apple cake, flu virus), paraphrase is a humanproposed paraphrasing verb optionally followed by a preposition, and frequency is the number of annotators who proposed that paraphrase. Here is an illustrative extract from the training dataset: The test file has a similar format, except that the frequency is not included and the paraphrases for each noun compound appear in random order: ... chest pain originate chest pain start in chest pain descend in chest pain be in ...

License.
All datasets are released under the Creative Commons Attribution 3.0 Unported license. 4 4 creativecommons.org/licenses/by/3.0

Evaluation
All evaluation was performed by computing an appropriate measure of similarity/correlation between system predictions and the compiled judgements of the human annotators. We did it on a compound-bycompound basis and averaged over all compounds in the test dataset. Section 4 shows results for three measures: Spearman rank correlation, Pearson correlation, and cosine similarity.
Spearman Rank Correlation (ρ) was adopted as the official evaluation measure for the competition. As a rank correlation statistic, it does not use the numerical values of the predictions or human judgements, only their relative ordering encoded as integer ranks. For a sample of n items ranked by two methods x and y, the rank correlation ρ is calculated as follows:
(1) where x i , y i are the ranks given by x and y to the ith item, respectively. The value of ρ ranges between -1.0 (total negative correlation) and 1.0 (total positive correlation).
Pearson Correlation (r) is a standard measure of correlation strength between real-valued variables. The formula is the same as (1), but with x i , y i taking real values rather than rank values; just like ρ, r's values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to compare numerical vectors:
For non-negative data, the cosine similarity takes values between 0.0 and 1.0. Pearson's r can be viewed as a version of the cosine similarity which performs centering on x and y.
Baseline: To help interpret these evaluation measures, we implemented a simple baseline. A distribution over the paraphrases was estimated by  summing the frequencies for all compounds in the training dataset, and the paraphrases for the test examples were scored according to this distribution.
Note that this baseline entirely ignores the identity of the nouns in the compound.

Participants
The task attracted five teams, one of which (UCD-GOGGLE) submitted three runs. The participants are listed in Table 2 along with brief system descriptions; for more details please see the teams' own description papers.

Results and Discussion
The task results appear in Table 3. In an evaluation by Spearman's ρ (the official ranking measure), the winning system was UVT-MEPHISTO, which scored 0.450. UVT also achieved the top Pearson's r score. UCD-PN is the top-scoring system according to the cosine measure. One participant submitted part of his results after the official deadline, which is marked by an asterisk.
The participants used a variety of information sources and estimation methods. UVT-MEPHISTO is a supervised system that uses frequency information from the Google N-Gram Corpus and features from WordNet (Fellbaum, 1998) to rank candidate paraphrases. On the other hand, UCD-PN uses no external resources and no supervised training, yet came within 0.009 of UVT-MEPHISTO in the official evaluation. The basic idea of UCD-PNthat one can predict the plausibility of a paraphrase simply by knowing which other paraphrases have been given for that compound regardless of their frequency -is clearly a powerful one. Unlike the other systems, UCD-PN used information about the test examples (not their ranks, of course) for model estimation; this has similarities to "transductive" methods for semi-supervised learning. However, post-hoc analysis shows that UCD-PN would have preserved its rank if it had estimated its model on the training data only. On the other hand, if the task had been designed differently -by asking systems to propose paraphrases from the set of all possible verb/preposition combinations -then we would not expect UCD-PN's approach to work as well as models that use corpus information.
The other systems are comparable to UVT-MEPHISTO in that they use corpus frequencies to evaluate paraphrases and apply some kind of semantic smoothing to handle sparsity. However, UCD-GOGGLE-I, UCAM and NC-INTERP are unsupervised systems. UCAM uses the 100million word BNC corpus, while the other systems use Web-scale resources; this has presumably exacerbated sparsity issues and contributed to a relatively poor performance.
The hybrid approach exemplified by UCD-GOGGLE-III combines the predictions of a system that models paraphrase correlations and one that learns from corpus frequencies and thus attains better performance. Given that the two topscoring systems can also be characterized as using these two distinct information sources, it is natural to consider combining these systems. Simply normalizing (to unit sum) and averaging the two sets of prediction values for each compound does indeed give better scores: Spearman ρ = 0.472, r = 0.431, Cosine = 0.685. The baseline from Section 2.3 turns out to be very strong. Evaluating with Spearman's ρ, only three systems outperform it. It is less competitive on the other evaluation measures though. This suggests that global paraphrase frequencies may be useful for telling sensible paraphrases from bad ones, but will not do for quantifying the plausibility of a paraphrase for a given noun compound.

Conclusion
Given that it is a newly-proposed task, this initial experiment in paraphrasing noun compounds has been a moderate success. The participation rate has been sufficient for the purposes of comparing and contrasting different approaches to the role of paraphrases in the interpretation of noun-noun compounds. We have seen a variety of approaches applied to the same dataset, and we have been able to compare the performance of pure approaches to hybrid approaches, and of supervised approaches to unsupervised approaches. The results reported here are also encouraging, though clearly there is considerable room for improvement.
This task has established a high baseline for systems to beat. We can take heart from the fact that the best performance is apparently obtained from a combination of corpus-derived usage features and dictionary-derived linguistic knowledge. Although clever but simple approaches can do quite well on such a task, it is encouraging to note that the best results await those who employ the most robust and the most informed treatments of NCs and their paraphrases. Despite a good start, this is a challenge that remains resolutely open. We expect that the dataset created for the task will be a valuable resource for future research.

