	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
0	0	S01-basque	title	1	1	1.0	0.008130081300813	The Basque task: did systems perform in the upperbound?	0
1	1	S01-basque	abstract	1	2	0.142857142857143	0.016260162601626	In this paper we describe the Senseval 2 Basque lexical-sample task.	0
2	2	S01-basque	abstract	2	3	0.285714285714286	0.024390243902439	The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary.	0
3	3	S01-basque	abstract	3	4	0.428571428571429	0.032520325203252	Most examples were taken from the Egunkaria newspaper.	0
4	4	S01-basque	abstract	4	5	0.571428571428571	0.040650406504065	The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration.	0
5	5	S01-basque	abstract	5	6	0.714285714285714	0.048780487804878	The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage.	0
6	6	S01-basque	abstract	6	7	0.857142857142857	0.056910569105691	The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems.	0
7	7	S01-basque	abstract	7	8	1.0	0.065040650406504	In particular, we argue that inter-tagger agreement is not a real upperbound for the B,asque WSD task.	0
8	8	S01-basque	Introduction	1	9	0.2	0.073170731707317	This paper reviews the design of the lexicalsample task for Basque.	0
9	9	S01-basque	Introduction	2	10	0.4	0.08130081300813	The following steps were taken in order to build the hand-tagged corpus: 1. set the exercise a. choose sense inventory b. choose target corpus c. choose target words d. select examples from the corpus 2. hand-tagging a. define procedure b. tag c. analysis of inter-tagger agreement d. arbitration	0
10	10	S01-basque	Introduction	3	11	0.6	0.089430894308943	The following section presents the setting of the exercise.	0
11	11	S01-basque	Introduction	4	12	0.8	0.097560975609756	Section 3 reviews the hand-tagging, and section 4 the results of the participant systems.	0
12	12	S01-basque	Introduction	5	13	1.0	0.105691056910569	Section 5 discusses the design of the task, as well 9 as the results, and section 6 presents some future work.	0
13	13	S01-basque	2	1	14	0.5	0.113821138211382	Setting of the exercise	0
14	14	S01-basque	2	2	15	1.0	0.121951219512195	In this section we present the setting of the Basque lexical-sample exercise.	0
15	15	S01-basque	Basque	1	16	0.25	0.130081300813008	Basque is an agglutinative language, that is, for the formation of words, the dictionary entry independently takes each of the elements necessary for the different functions (syntactic case included).	0
16	16	S01-basque	Basque	2	17	0.5	0.138211382113821	More specifically, the affixes corresponding to the determinant, number and declension case are taken in this order and independently of each other (deep morphological structure).	0
17	17	S01-basque	Basque	3	18	0.75	0.146341463414634	One of the main characteristics of Basque is its declension system with numerous cases, which differentiates it from the languages spoken in the surrounding countries.	0
18	18	S01-basque	Basque	4	19	1.0	0.154471544715447	An example follows (the order of the lemmas is the reverse): etxekoari emaiozu [Give it] [to the one in the house]	0
19	19	S01-basque	Sense inventory	1	20	0.111111111111111	0.16260162601626	We chose a published dictionary, Euskal Hiztegia (Sarasola, 1996), for the sense inventory.	0
20	20	S01-basque	Sense inventory	2	21	0.222222222222222	0.170731707317073	It is a monolingual dictionary of Basque.	0
21	21	S01-basque	Sense inventory	3	22	0.333333333333333	0.178861788617886	It is normative and repository of standard Basque.	0
22	22	S01-basque	Sense inventory	4	23	0.444444444444444	0.186991869918699	It was produced based mainly on literary tradition.	0
23	23	S01-basque	Sense inventory	5	24	0.555555555555556	0.195121951219512	The dictionary has 30,715 entries and 41,699 main senses (see comment on nuances below).	0
24	24	S01-basque	Sense inventory	6	25	0.666666666666667	0.203252032520325	The TEl version with all the information for each entry was included in the distribution.	0
25	25	S01-basque	Sense inventory	7	26	0.777777777777778	0.211382113821138	As the format was quite complex, another version was also included, which listed a plain list of word senses and multiword terms.	0
26	26	S01-basque	Sense inventory	8	27	0.888888888888889	0.219512195121951	This dictionary has the particularity that word senses can have very specific sub-senses, called nuances which sometimes are illustrated with just an example and other times have a full definition. •	0
27	27	S01-basque	Sense inventory	9	28	1.0	0.227642276422764	These nuances were also included in the set of word senses.	0
28	28	S01-basque	Corpora used	1	29	0.166666666666667	0.235772357723577	At first the EEBS balanced corpus was chosen, comprising one million words.	0
29	29	S01-basque	Corpora used	2	30	0.333333333333333	0.24390243902439	Unfortunately this size is too small to provide the number of occurrences per word that was defined in the Senseval task specification.	0
30	30	S01-basque	Corpora used	3	31	0.5	0.252032520325203	We therefore turned to the biggest corpus at hand, the Egunkaria corpus, comprising texts taken from the newspaper.	0
31	31	S01-basque	Corpora used	4	32	0.666666666666667	0.260162601626016	The size of this corpus allowed us to easily reach the number of examples required.	0
32	32	S01-basque	Corpora used	5	33	0.833333333333333	0.268292682926829	On the negative side, it is a specific corpus, and the distribution of the word senses could be highly biased.	0
33	33	S01-basque	Corpora used	6	34	1.0	0.276422764227642	We used Egunkaria as the main corpus, but we also used the EEBS corpus in some cases, as we will see below.	0
34	34	S01-basque	Words chosen	1	35	0.058823529411765	0.284552845528455	The criterion to choose the 40 words (15 nouns, 15 verbs and 10 adjectives) was that they should cover all possible combinations of frequency, polysemy and skew 1 .	0
35	35	S01-basque	Words chosen	2	36	0.117647058823529	0.292682926829268	The first two can be objectively determined before starting to hand tag, but skew could only be determined by introspection.	0
36	36	S01-basque	Words chosen	3	37	0.176470588235294	0.300813008130081	After choosing a word, the expected skew was sometimes different from the desired skew.	0
37	37	S01-basque	Words chosen	4	38	0.235294117647059	0.308943089430894	A secondary criterion was the overlap with the words in other languages, and the overlap with a number of verbs that are being used for subcategorization and diathesis alternation studies in our group.	0
38	38	S01-basque	Words chosen	5	39	0.294117647058823	0.317073170731707	The English task organizers and the Spanish task organizers provided us with half of the words chosen in their lexical-sample task.	0
39	39	S01-basque	Words chosen	6	40	0.352941176470588	0.32520325203252	This information could be used for cross-language mapping of word senses.	0
40	40	S01-basque	Words chosen	7	41	0.411764705882353	0.333333333333333	Regarding the overlap with verbs, we plan to explore the influence of 1	0
41	41	S01-basque	Words chosen	8	42	0.470588235294118	0.341463414634146	By skew in this context, we mean the dominance of one sense over the others.	0
42	42	S01-basque	Words chosen	9	43	0.529411764705882	0.349593495934959	It is given as the percentage of occurrences of the most frequent sense over all the others.	0
43	43	S01-basque	Words chosen	10	44	0.588235294117647	0.357723577235772	10 word senses in subcategorization and diathesis alternations.	0
44	44	S01-basque	Words chosen	11	45	0.647058823529412	0.365853658536585	We chose the first set of 40 words that covered more or less all combinations of the above phenomena from the set of translations of the words in the other tasks.	0
45	45	S01-basque	Words chosen	12	46	0.705882352941176	0.373983739837398	This was done blindly, without knowing which specific word was chosen.	0
46	46	S01-basque	Words chosen	13	47	0.764705882352941	0.382113821138211	This first set was used to extract the examples (cf. following section), and the hand-taggers started to tag them.	0
47	47	S01-basque	Words chosen	14	48	0.823529411764706	0.390243902439024	Unfortunately, for a number of words, all examples in the corpus referred to a single word sense.	0
48	48	S01-basque	Words chosen	15	49	0.882352941176471	0.398373983739837	We had not foreseen this situation and took two measures: 1) Search for occurrences in the secondary EEBS corpus.	0
49	49	S01-basque	Words chosen	16	50	0.941176470588235	0.40650406504065	2) If occurrences of new senses were not found, then the word was discarded and a replacement word was chosen.	0
50	50	S01-basque	Words chosen	17	51	1.0	0.414634146341463	In order to find the replacement, the hand-tagger that was . doing the arbitration scanned the examples of a word with similar polysemy and frequency and decided whether it had occurrences of more than one sense.	0
51	51	S01-basque	Selection of examples from corpora	1	52	0.142857142857143	0.422764227642276	The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+ 15*senses+6mword where senses does not include the nuances (cf. section 2.2) and mword is the number of multiword terms that included the target word.	0
52	52	S01-basque	Selection of examples from corpora	2	53	0.285714285714286	0.430894308943089	The minimum number of examples per word was extracted at random from the Egunkaria corpus, plus a 10% buffer.	0
53	53	S01-basque	Selection of examples from corpora	3	54	0.428571428571429	0.439024390243902	As explained in the previous section, for some words occurring in a single sense in this corpus, additional examples were taken from the secondary EEBS corpus.	0
54	54	S01-basque	Selection of examples from corpora	4	55	0.571428571428571	0.447154471544715	In this case, all available examples from EEBS were used, plus the examples from Egunkaria to meet the minimum number of examples required.	0
55	55	S01-basque	Selection of examples from corpora	5	56	0.714285714285714	0.455284552845528	The context included 5 sentences, with the sentence with the target word appearing in the middle.	0
56	56	S01-basque	Selection of examples from corpora	6	57	0.857142857142857	0.463414634146341	Links were kept to the source corpus, document, and to the newspaper section when applicable.	0
57	57	S01-basque	Selection of examples from corpora	7	58	1.0	0.471544715447154	The occurrences were split at random in training set (two thirds of all occurrences) and test set.	0
58	58	S01-basque	Hand tagging	1	59	0.016393442622951	0.479674796747968	Three persons, graduate linguistics students, took part in the tagging.	0
59	59	S01-basque	Hand tagging	2	60	0.032786885245902	0.487804878048781	They are familiar with word senses, as they are involved in the development of the Basque WordNet and cleaning the TEl version of the Euskal Hiztegia dictionary.	0
60	60	S01-basque	Hand tagging	3	61	0.049180327868853	0.495934959349593	The following procedure was defined for each word: •	0
61	61	S01-basque	Hand tagging	4	62	0.065573770491803	0.504065040650406	The three of them would meet, read the definitions and examples given in , the dictionary and discuss the meaning of each word sense.	0
62	62	S01-basque	Hand tagging	5	63	0.081967213114754	0.51219512195122	They tried to agree the meaning differences among the word senses. •	0
63	63	S01-basque	Hand tagging	6	64	0.098360655737705	0.520325203252033	Two taggers independently tagged all examples for the word.	0
64	64	S01-basque	Hand tagging	7	65	0.114754098360656	0.528455284552846	No communication was allowed while tagging the word.	0
65	65	S01-basque	Hand tagging	8	66	0.131147540983607	0.536585365853659	• Multiple tags were allowed, as well as the following tags: B new sense or multiword term, U unassignable.:.	0
66	66	S01-basque	Hand tagging	9	67	0.147540983606557	0.544715447154471	Examples with these tags were removed from the final release.	0
67	67	S01-basque	Hand tagging	10	68	0.163934426229508	0.552845528455285	• A program was used to compute agreement rate and output those occurrences where there was disagreement grouped by the senses assigned.	0
68	68	S01-basque	Hand tagging	11	69	0.180327868852459	0.560975609756098	•	0
69	69	S01-basque	Hand tagging	12	70	0.19672131147541	0.569105691056911	The third tagger, the referee, reviewed the disagreements and decided which one was correct.	0
70	70	S01-basque	Hand tagging	13	71	0.213114754098361	0.577235772357724	For the word itzal (shadow), the disagreement was specially high.	0
71	71	S01-basque	Hand tagging	14	72	0.229508196721311	0.585365853658537	The, taggers decided that the definitions and examples were too confusing, and decided to replace it with another word.	0
72	72	S01-basque	Hand tagging	15	73	0.245901639344262	0.59349593495935	Overall, the two taggers agreed 75% of the time.	0
73	73	S01-basque	Hand tagging	16	74	0.262295081967213	0.601626016260163	Some words attained an agreement rate above 95% (e.g. nouns kanal -channelor tentsio -tension -), but others like herritown/people/nation attained only 52% agreement..:.	0
74	74	S01-basque	Hand tagging	17	75	0.278688524590164	0.609756097560976	All in all, 5284 occurrences of the 40 words were released.	0
75	75	S01-basque	Hand tagging	18	76	0.295081967213115	0.617886178861789	On average, one hand-tagger took 0.41 minutes per occurrence and the other 0.55 minutes.	0
76	76	S01-basque	Hand tagging	19	77	0.311475409836066	0.626016260162602	The referee took 0.22 minutes per entry, including selection of replacement words.	0
77	77	S01-basque	Hand tagging	20	78	0.327868852459016	0.634146341463415	Time for arbitration meeting is also included.	0
78	78	S01-basque	Hand tagging	21	79	0.344262295081967	0.642276422764228	These are the main issues that we think are interesting for further discussion.	0
79	79	S01-basque	Hand tagging	22	80	0.360655737704918	0.650406504065041	Dictionary used.	0
80	80	S01-basque	Hand tagging	23	81	0.377049180327869	0.658536585365854	Before designing the task, we had to choose between two possible dictionaries: the Basque WordNet and the Euskal Hiztegia dictionary.	0
81	81	S01-basque	Hand tagging	24	82	0.39344262295082	0.666666666666667	Another alternative was to start the lexicographer's work afresh, defining the word senses as the tagging proceeded.	0
82	82	S01-basque	Hand tagging	25	83	0.40983606557377	0.67479674796748	We thought the printed dictionary would provide clear-cut sense distinctions that would allow the tagging to be easier.	0
83	83	S01-basque	Hand tagging	26	84	0.426229508196721	0.682926829268293	After the tagging, the hand-taggers complained that this was not the case.	0
84	84	S01-basque	Hand tagging	27	85	0.442622950819672	0.691056910569106	They think that the tagging would be much more satisfactory had they defined the word senses directly from the corpus.	0
85	85	S01-basque	Hand tagging	28	86	0.459016393442623	0.699186991869919	In particular, they were not allowed to introduce new senses or multiword terms, and such examples were discarded.	0
86	86	S01-basque	Hand tagging	29	87	0.475409836065574	0.707317073170732	Corpus used.	0
87	87	S01-basque	Hand tagging	30	88	0.491803278688525	0.715447154471545	There was a mismatch between the dictionary and the corpus: the corpus was linked to a specific genre, and this resulted in having some senses which were not included in the dictionary.	0
88	88	S01-basque	Hand tagging	31	89	0.508196721311475	0.723577235772358	Besides, many senses in the dictionary did not appear in our corpus, and some words had to be replaced.	0
89	89	S01-basque	Hand tagging	32	90	0.524590163934426	0.731707317073171	This caused the taggers some overwork, but did not influence the quality of the result.	0
90	90	S01-basque	Hand tagging	33	91	0.540983606557377	0.739837398373984	Hand-tagging is a very unpleasant task.	0
91	91	S01-basque	Hand tagging	34	92	0.557377049180328	0.747967479674797	"When asked about future editions, the hand taggers suggested the following: ""please do get somebody else""."	0
92	92	S01-basque	Hand tagging	35	93	0.573770491803279	0.75609756097561	We have to note that the hand taggers are used to repetitive tasks, such as building the Basque WordNet or cleaning-up the TEl version of Euskal Hiztegia.	0
93	93	S01-basque	Hand tagging	36	94	0.590163934426229	0.764227642276423	Inter-tagger agreement.	0
94	94	S01-basque	Hand tagging	37	95	0.60655737704918	0.772357723577236	Part of the disagreement was caused by typos and mistakes.	0
95	95	S01-basque	Hand tagging	38	96	0.622950819672131	0.780487804878049	Nevertheless, we think that the low inter-tagger agreement (75%) was caused mainly by the procedure used to tag the occurrences.	0
96	96	S01-basque	Hand tagging	39	97	0.639344262295082	0.788617886178862	The taggers met and tried to understand the word senses, but the fact is that it was only after tagging a few occurrences that they started to really conceptualise the word senses and draw specific lines among one sense and the others.	0
97	97	S01-basque	Hand tagging	40	98	0.655737704918033	0.796747967479675	If both taggers had been allowed to meet (at least once) while they were tagging, they could have discussed and agreed on a common conceptuali~ation.	0
98	98	S01-basque	Hand tagging	41	99	0.672131147540984	0.804878048780488	The referee found that most of the times whole sets of examples were systematically tagged differently by each of the taggers, that is, each of the taggers had a different criterion about the word sense applicable to that set of examples.	0
99	99	S01-basque	Hand tagging	42	100	0.688524590163934	0.813008130081301	The referee then had to decide on the tag for those sets of examples.	0
100	100	S01-basque	Hand tagging	43	101	0.704918032786885	0.821138211382114	Systems performing as good as inter-tagger agreement.	0
101	101	S01-basque	Hand tagging	44	102	0.721311475409836	0.829268292682927	Traditionally, inter-tagger agreement has been used as an upperbound for the performance of machines in cognitive tasks.	0
102	102	S01-basque	Hand tagging	45	103	0.737704918032787	0.83739837398374	We think that in this case, a system may perform better on the Basque WSD task than a human, in the sense that if the taggers were evaluated against the gold standard they would score lower that the systems.	0
103	103	S01-basque	Hand tagging	46	104	0.754098360655738	0.845528455284553	In fact, current systems, which are still under development for Basque, reach the same performance as humans.	0
104	104	S01-basque	Hand tagging	47	105	0.770491803278688	0.853658536585366	Are machines performing better than humans?	0
105	105	S01-basque	Hand tagging	48	106	0.786885245901639	0.861788617886179	We think that inter-tagger agreement, at least as derived from the procedure used in this exercise, is not a real upperbound, and that systems can easily perform better.	0
106	106	S01-basque	Hand tagging	49	107	0.80327868852459	0.869918699186992	The gold standard reflects the conceptualization of one human, the referee, which does not have to agree with the conceptualization made by other persons (specially if these are done in isolation).	0
107	107	S01-basque	Hand tagging	50	108	0.819672131147541	0.878048780487805	People disagree whether in a certain occurrence this word sense or the other applies, i.e. they can disagree in 12 the meaning of the word senses as defined in the dictionary.	0
108	108	S01-basque	Hand tagging	51	109	0.836065573770492	0.886178861788618	In fact, trying to achieve a common ground when reading the dictionary definitions sometimes produced heated debate in the meetings.	0
109	109	S01-basque	Hand tagging	52	110	0.852459016393443	0.894308943089431	If the gold standard reflects a systematic conceptualization of a person, machine learning algorithms can learn to replicate these conceptualization (categorizations in this case), and achieve high degrees of agreement with the person behind the gold standard.	0
110	110	S01-basque	Hand tagging	53	111	0.868852459016393	0.902439024390244	This does not mean that the system is smarter than the human taggers, but rather that the system has no opinion on his own, and just imitates one of the persons.	0
111	111	S01-basque	Hand tagging	54	112	0.885245901639344	0.910569105691057	Error reduction similar to English task.	0
112	112	S01-basque	Hand tagging	55	113	0.901639344262295	0.91869918699187	The best recall for Basque was 75% vs. 64% of the MFS baseline.	0
113	113	S01-basque	Hand tagging	56	114	0.918032786885246	0.926829268292683	In English the best system achieved 64% recall vs. 47% of the most frequent sense baseline (called commonest baseline in the official results).	0
114	114	S01-basque	Hand tagging	57	115	0.934426229508197	0.934959349593496	It is clear that the skew of the Basque words allowed for higher results.	0
115	115	S01-basque	Hand tagging	58	116	0.950819672131147	0.943089430894309	On the other hand, the error reduction for Basque was 29%, compared to 32% for English.	0
116	116	S01-basque	Hand tagging	59	117	0.967213114754098	0.951219512195122	This implies that systems could effectively learn from the data in both tasks.	0
117	117	S01-basque	Hand tagging	60	118	0.983606557377049	0.959349593495935	No use of domain tags, full documents.	0
118	118	S01-basque	Hand tagging	61	119	1.0	0.967479674796748	No system used the extra information provided by the full documents or the domain tags.	0
119	119	S01-basque	Future work	1	120	0.25	0.975609756097561	First of all, we plan to explore the use of other procedures for the hand-tagging.	0
120	120	S01-basque	Future work	2	121	0.5	0.983739837398374	We think that the data attained high levels of quality (which has been shown by the error reduction attained by the participating systems over the MFS baseline), but still we are not satisfied with the sense inventory used.	0
121	121	S01-basque	Future work	3	122	0.75	0.991869918699187	Further analysis of the results of the participating systems is also planned, as Kappa statistics and the performance of the combination of the systems.	0
122	122	S01-basque	Future work	4	123	1.0	1.0	Bibliography Sarasola, I., 1996, Euskal Hiztegia, Donostia, Gipuzkoako Kutxa.	0
2093	2093	S07-2	title	1	1	1.0	0.006134969325153	Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems	1
2094	2094	S07-2	abstract	1	2	0.25	0.012269938650307	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems.	0
2095	2095	S07-2	abstract	2	3	0.5	0.01840490797546	In total there were 6 participating systems.	0
2096	2096	S07-2	abstract	3	4	0.75	0.024539877300614	We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).	0
2097	2097	S07-2	abstract	4	5	1.0	0.030674846625767	We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
2098	2098	S07-2	Introduction	1	6	0.052631578947369	0.03680981595092	Word Sense Disambiguation (WSD) is a key enabling-technology.	0
2099	2099	S07-2	Introduction	2	7	0.105263157894737	0.042944785276074	Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.	0
2100	2100	S07-2	Introduction	3	8	0.157894736842105	0.049079754601227	Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).	0
2101	2101	S07-2	Introduction	4	9	0.210526315789474	0.05521472392638	In theory, larger amounts of training data (SemCor has approx.	0
2102	2102	S07-2	Introduction	5	10	0.263157894736842	0.061349693251534	500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.	0
2103	2103	S07-2	Introduction	6	11	0.31578947368421	0.067484662576687	Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Martínez and Agirre, 2000;	0
2104	2104	S07-2	Introduction	7	12	0.368421052631579	0.073619631901841	Koeling et al., 2005).	0
2105	2105	S07-2	Introduction	8	13	0.421052631578947	0.079754601226994	"Supervised WSD is based on the ""fixed-list of senses"" paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon."	0
2106	2106	S07-2	Introduction	9	14	0.473684210526316	0.085889570552147	Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).	0
2107	2107	S07-2	Introduction	10	15	0.526315789473684	0.092024539877301	Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.	1
2108	2108	S07-2	Introduction	11	16	0.578947368421053	0.098159509202454	Typical WSID systems involve clustering techniques, which group together similar examples.	0
2109	2109	S07-2	Introduction	12	17	0.631578947368421	0.104294478527607	Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.	0
2110	2110	S07-2	Introduction	13	18	0.68421052631579	0.110429447852761	One of the problems of unsupervised systems is that of managing to do a fair evaluation.	0
2111	2111	S07-2	Introduction	14	19	0.736842105263158	0.116564417177914	Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.	0
2112	2112	S07-2	Introduction	15	20	0.789473684210526	0.122699386503067	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.	0
2113	2113	S07-2	Introduction	16	21	0.842105263157895	0.128834355828221	The paper is organized as follows.	0
2114	2114	S07-2	Introduction	17	22	0.894736842105263	0.134969325153374	Section 2 presents the evaluation framework used in this task.	0
2115	2115	S07-2	Introduction	18	23	0.947368421052632	0.141104294478528	Section 3 presents the systems that participated in the task, and the official results.	0
2116	2116	S07-2	Introduction	19	24	1.0	0.147239263803681	Finally, Section 5 draws the conclusions.	0
2117	2117	S07-2	Evaluating WSID systems	1	25	0.041666666666667	0.153374233128834	All WSID algorithms need some addition in order to be evaluated.	0
2118	2118	S07-2	Evaluating WSID systems	2	26	0.083333333333333	0.159509202453988	One alternative is to manually decide the correctness of the clusters assigned to each occurrence of the words.	0
2119	2119	S07-2	Evaluating WSID systems	3	27	0.125	0.165644171779141	This approach has two main disadvantages.	0
2120	2120	S07-2	Evaluating WSID systems	4	28	0.166666666666667	0.171779141104294	First, it is expensive to manually verify each occurrence of the word, and different runs of the algorithm need to be evaluated in turn.	0
2121	2121	S07-2	Evaluating WSID systems	5	29	0.208333333333333	0.177914110429448	Second, it is not an easy task to manually decide if an occurrence of a word effectively corresponds with the use of the word the assigned cluster refers to, especially considering that the person is given a short list of words linked to the cluster.	0
2122	2122	S07-2	Evaluating WSID systems	6	30	0.25	0.184049079754601	We also think that instead of judging whether the cluster returned by the algorithm is correct, the person should have independently tagged the occurrence with his own senses, which should have been then compared to the cluster returned by the system.	0
2123	2123	S07-2	Evaluating WSID systems	7	31	0.291666666666667	0.190184049079755	This is paramount to compare a corpus which has been hand-tagged with some reference senses (also known as the gold-standard) with the clustering result.	0
2124	2124	S07-2	Evaluating WSID systems	8	32	0.333333333333333	0.196319018404908	The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes.	0
2125	2125	S07-2	Evaluating WSID systems	9	33	0.375	0.202453987730061	A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon.	0
2126	2126	S07-2	Evaluating WSID systems	10	34	0.416666666666667	0.208588957055215	Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping.	0
2127	2127	S07-2	Evaluating WSID systems	11	35	0.458333333333333	0.214723926380368	More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004;	0
2128	2128	S07-2	Evaluating WSID systems	12	36	0.5	0.220858895705521	Niu et al., 2005).	0
2129	2129	S07-2	Evaluating WSID systems	13	37	0.541666666666667	0.226993865030675	A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Schütze, 1998).	0
2130	2130	S07-2	Evaluating WSID systems	14	38	0.583333333333333	0.233128834355828	This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance.	0
2131	2131	S07-2	Evaluating WSID systems	15	39	0.625	0.239263803680982	In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind.	0
2132	2132	S07-2	Evaluating WSID systems	16	40	0.666666666666667	0.245398773006135	With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solution on it.	0
2133	2133	S07-2	Evaluating WSID systems	17	41	0.708333333333333	0.251533742331288	We evaluate the results according to the following types of evaluation:	0
2134	2134	S07-2	Evaluating WSID systems	18	42	0.75	0.257668711656442	1. Evaluate the induced senses as clusters of examples.	0
2135	2135	S07-2	Evaluating WSID systems	19	43	0.791666666666667	0.263803680981595	The induced clusters are compared to the sets of examples tagged with the given gold standard word senses (classes), and evaluated using the FScore measure for clusters.	0
2136	2136	S07-2	Evaluating WSID systems	20	44	0.833333333333333	0.269938650306748	We will call this evaluation unsupervised.	0
2137	2137	S07-2	Evaluating WSID systems	21	45	0.875	0.276073619631902	2. Map the induced senses to gold standard senses, and use the mapping to tag the test corpus with gold standard tags.	0
2138	2138	S07-2	Evaluating WSID systems	22	46	0.916666666666667	0.282208588957055	The mapping is automatically produced by the organizers, and the resulting results evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.	0
2139	2139	S07-2	Evaluating WSID systems	23	47	0.958333333333333	0.288343558282209	We call this evaluation supervised.	0
2140	2140	S07-2	Evaluating WSID systems	24	48	1.0	0.294478527607362	We will see each of them in turn.	0
2141	2141	S07-2	Unsupervised evaluation	1	49	0.052631578947369	0.300613496932515	In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes.	0
2142	2142	S07-2	Unsupervised evaluation	2	50	0.105263157894737	0.306748466257669	In order to compare the clusters with the classes, hand annotated corpora is needed.	0
2143	2143	S07-2	Unsupervised evaluation	3	51	0.157894736842105	0.312883435582822	The test set is first tagged with the induced senses.	0
2144	2144	S07-2	Unsupervised evaluation	4	52	0.210526315789474	0.319018404907975	A perfect clustering solution will be the one where each cluster has exactly the same examples as one of the classes, and vice versa.	0
2145	2145	S07-2	Unsupervised evaluation	5	53	0.263157894736842	0.325153374233129	Following standard cluster evaluation practice (Zhao and Karypis, 2005), we consider the FScore measure for measuring the performance of the systems.	0
2146	2146	S07-2	Unsupervised evaluation	6	54	0.31578947368421	0.331288343558282	"The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly ""retrieved"" examples for a cluster (divided by total cluster size), and recall as the percentage of correctly ""retrieved"" examples for a cluster (divided by total class size)."	0
2147	2147	S07-2	Unsupervised evaluation	7	55	0.368421052631579	0.337423312883436	Given a particular class s r of size n r and a cluster h i of size n i , suppose n i r examples in the class s r belong to h i .	0
2148	2148	S07-2	Unsupervised evaluation	8	56	0.421052631578947	0.343558282208589	The F value of this class and cluster is defined to be:	0
2149	2149	S07-2	Unsupervised evaluation	9	57	0.473684210526316	0.349693251533742	where P (s r , h i ) = n i r nr is the precision value and R(s r , h i ) = n i r n i is the recall value defined for class s r and cluster h i .	0
2150	2150	S07-2	Unsupervised evaluation	10	58	0.526315789473684	0.355828220858896	The FScore of class s r is the maximum F value attained at any cluster, that is,	0
2151	2151	S07-2	Unsupervised evaluation	11	59	0.578947368421053	0.361963190184049	and the FScore of the entire clustering solution is:	0
2152	2152	S07-2	Unsupervised evaluation	12	60	0.631578947368421	0.368098159509202	where q is the number of classes and n is the size of the clustering solution.	0
2153	2153	S07-2	Unsupervised evaluation	13	61	0.68421052631579	0.374233128834356	If the clustering is the identical to the original classes in the datasets, FScore will be equal to one which means that the higher the FScore, the better the clustering is.	0
2154	2154	S07-2	Unsupervised evaluation	14	62	0.736842105263158	0.380368098159509	For the sake of completeness we also include the standard entropy and purity measures in the unsupervised evaluation.	0
2155	2155	S07-2	Unsupervised evaluation	15	63	0.789473684210526	0.386503067484663	The entropy measure considers how the various classes of objects are distributed within each cluster.	0
2156	2156	S07-2	Unsupervised evaluation	16	64	0.842105263157895	0.392638036809816	In general, the smaller the entropy value, the better the clustering algorithm performs.	0
2157	2157	S07-2	Unsupervised evaluation	17	65	0.894736842105263	0.398773006134969	The purity measure considers the extent to which each cluster contained objects from primarily one class.	0
2158	2158	S07-2	Unsupervised evaluation	18	66	0.947368421052632	0.404907975460123	The larger the values of purity, the better the clustering algorithm performs.	0
2159	2159	S07-2	Unsupervised evaluation	19	67	1.0	0.411042944785276	For a formal definition refer to (Zhao and Karypis, 2005).	0
2160	2160	S07-2	Supervised evaluation	1	68	0.1	0.417177914110429	We have followed the supervised evaluation framework for evaluating WSID systems as described in (Agirre et al., 2006).	0
2161	2161	S07-2	Supervised evaluation	2	69	0.2	0.423312883435583	First, we split the corpus into a train/test part.	0
2162	2162	S07-2	Supervised evaluation	3	70	0.3	0.429447852760736	Using the hand-annotated sense information in the train part, we compute a mapping matrix M that relates clusters and senses in the following way.	0
2163	2163	S07-2	Supervised evaluation	4	71	0.4	0.43558282208589	Suppose there are m clusters and n senses for the target word.	0
2164	2164	S07-2	Supervised evaluation	5	72	0.5	0.441717791411043	Then, M = {m ij } 1 ≤ i ≤ m, 1 ≤ j ≤ n, and each m ij = P (s j |h i ), that is, m ij is the probability of a word having sense j given that it has been assigned cluster i.	0
2165	2165	S07-2	Supervised evaluation	6	73	0.6	0.447852760736196	This probability can be computed counting the times an occurrence with sense s j has been assigned cluster h i in the train corpus.	0
2166	2166	S07-2	Supervised evaluation	7	74	0.7	0.45398773006135	The mapping matrix is used to transform any cluster score vectorh = (h 1 , . . . , h m ) returned by the WSID algorithm into a sense score vectors = (s 1 , . . . , s n ).	0
2167	2167	S07-2	Supervised evaluation	8	75	0.8	0.460122699386503	It suffices to multiply the score vector by M , i.e.,s =hM .	0
2168	2168	S07-2	Supervised evaluation	9	76	0.9	0.466257668711656	We use the M mapping matrix in order to convert the cluster score vector of each test corpus instance into a sense score vector, and assign the sense with  maximum score to that instance.	0
2169	2169	S07-2	Supervised evaluation	10	77	1.0	0.47239263803681	Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.	0
2170	2170	S07-2	Results	1	78	0.5	0.478527607361963	In this section we will introduce the gold standard and corpus used, the description of the systems and the results obtained.	0
2171	2171	S07-2	Results	2	79	1.0	0.484662576687117	Finally we provide some material for discussion.	0
2172	2172	S07-2	Gold Standard	1	80	0.142857142857143	0.49079754601227	"The data used for the actual evaluation was borrowed from the SemEval-2007 ""English lexical sample subtask"" of task 17."	0
2173	2173	S07-2	Gold Standard	2	81	0.285714285714286	0.496932515337423	The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al., 2006).	0
2174	2174	S07-2	Gold Standard	3	82	0.428571428571429	0.503067484662577	Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case.	0
2175	2175	S07-2	Gold Standard	4	83	0.571428571428571	0.50920245398773	Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears.	0
2176	2176	S07-2	Gold Standard	5	84	0.714285714285714	0.515337423312883	After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants.	0
2177	2177	S07-2	Gold Standard	6	85	0.857142857142857	0.521472392638037	Participants had to tag with the induced senses all the examples in this corpus.	0
2178	2178	S07-2	Gold Standard	7	86	1.0	0.52760736196319	Table 1 summarizes the size of the corpus.	0
2179	2179	S07-2	Participant systems	1	87	0.2	0.533742331288344	In total there were 6 participant systems.	0
2180	2180	S07-2	Participant systems	2	88	0.4	0.539877300613497	One of them (UoFL) was not a sense induction system, but rather a knowledge-based WSD system.	0
2181	2181	S07-2	Participant systems	3	89	0.6	0.54601226993865	We include their data in the results section below for coherence with the official results submitted to participants, but we will not mention it here.	0
2182	2182	S07-2	Participant systems	4	90	0.8	0.552147239263804	I2R:	0
2183	2183	S07-2	Participant systems	5	91	1.0	0.558282208588957	This team used a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of this target word into the estimated number of clusters using the sequential Information Bottleneck algorithm.	0
2184	2184	S07-2	UBC-AS:	1	92	0.058823529411765	0.56441717791411	A two stage graph-based clustering where a co-occurrence graph is used to compute similarities against contexts.	0
2185	2185	S07-2	UBC-AS:	2	93	0.117647058823529	0.570552147239264	The context similarity matrix is pruned and the resulting associated graph is clustered by means of a random-walk type algorithm.	0
2186	2186	S07-2	UBC-AS:	3	94	0.176470588235294	0.576687116564417	The parameters of the system are tuned against the Senseval-3 lexical sample dataset, and some manual tuning is performed in order to reduce the overall number of induced senses.	0
2187	2187	S07-2	UBC-AS:	4	95	0.235294117647059	0.582822085889571	Note that this system was submitted by the organizers.	0
2188	2188	S07-2	UBC-AS:	5	96	0.294117647058823	0.588957055214724	The organizers took great care in order to participate under the same conditions as the rest of participants.	0
2189	2189	S07-2	UBC-AS:	6	97	0.352941176470588	0.595092024539877	UMND2: A system which clusters the second order co-occurrence vectors associated with each word in a context.	0
2190	2190	S07-2	UBC-AS:	7	98	0.411764705882353	0.601226993865031	Clustering is done using k-means and the number of clusters was automatically discovered using the Adapted Gap Statistic.	0
2191	2191	S07-2	UBC-AS:	8	99	0.470588235294118	0.607361963190184	No parameter tuning is performed.	0
2192	2192	S07-2	UBC-AS:	9	100	0.529411764705882	0.613496932515337	upv si: A self-term expansion method based on co-ocurrence, where the terms of the corpus are expanded by its best co-ocurrence terms in the same corpus.	0
2193	2193	S07-2	UBC-AS:	10	101	0.588235294117647	0.619631901840491	The clustering is done using one implementation of the KStar method where the stop criterion has been modified.	0
2194	2194	S07-2	UBC-AS:	11	102	0.647058823529412	0.625766871165644	The trial data was used for determining the corpus structure.	0
2195	2195	S07-2	UBC-AS:	12	103	0.705882352941176	0.631901840490798	No further tuning is performed.	0
2196	2196	S07-2	UBC-AS:	13	104	0.764705882352941	0.638036809815951	UOY: A graph based system which creates a cooccurrence hypergraph model.	0
2197	2197	S07-2	UBC-AS:	14	105	0.823529411764706	0.644171779141104	The hypergraph is filtered and weighted according to some association rules.	0
2198	2198	S07-2	UBC-AS:	15	106	0.882352941176471	0.650306748466258	The clustering is performed by selecting the nodes of higher degree until a stop criterion is reached.	0
2199	2199	S07-2	UBC-AS:	16	107	0.941176470588235	0.656441717791411	WSD is performed by assigning to each induced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word.	0
2200	2200	S07-2	UBC-AS:	17	108	1.0	0.662576687116564	The system was tested and tuned on 10 nouns of Senseval-3 lexical-sample.	0
2201	2201	S07-2	Official Results	1	109	0.045454545454546	0.668711656441718	Participants were required to induce the senses of the target words and cluster all target word contexts accordingly 2 . Table 2 summarizes the average number of induced senses as well as the real senses in the gold standard.	0
2202	2202	S07-2	Official Results	2	110	0.090909090909091	0.674846625766871	2	0
2203	2203	S07-2	Official Results	3	111	0.136363636363636	0.680981595092024	They were allowed to label each context with a weighted score vector, assigning a weight to each induced sense.	0
2204	2204	S07-2	Official Results	4	112	0.181818181818182	0.687116564417178	In the unsupervised evaluation only the sense with maximum weight was considered, but for the supervised one the whole score vector was used.	0
2205	2205	S07-2	Official Results	5	113	0.227272727272727	0.693251533742331	However, none of the participating systems labeled any instance with more than one sense.	0
2206	2206	S07-2	Official Results	6	114	0.272727272727273	0.699386503067485	Table 3 shows the unsupervised evaluation of the systems on the test corpus.	0
2207	2207	S07-2	Official Results	7	115	0.318181818181818	0.705521472392638	"We also include three baselines: the ""one cluster per word"" baseline (1c1word), which groups all instances of a word into a single cluster, the ""one cluster per instance"" baseline (1c1inst), where each instance is a distinct cluster, and a random baseline, where the induced word senses and their associated weights have been randomly produced."	0
2208	2208	S07-2	Official Results	8	116	0.363636363636364	0.711656441717791	The random baseline figures in this paper are averages over 10 runs.	0
2209	2209	S07-2	Official Results	9	117	0.409090909090909	0.717791411042945	As shown in Table 3, no system outperforms the 1c1word baseline, which indicates that this baseline is quite strong, perhaps due the relatively small number of classes in the gold standard.	0
2210	2210	S07-2	Official Results	10	118	0.454545454545455	0.723926380368098	However, all systems outperform by far the random and 1c1inst baselines, meaning that the systems are able to induce correct senses.	0
2211	2211	S07-2	Official Results	11	119	0.5	0.730061349693252	Note that the purity and entropy measures are not very indicative in this setting.	0
2212	2212	S07-2	Official Results	12	120	0.545454545454545	0.736196319018405	For completeness, we also computed the FScore using the complete corpus (both train and test).	0
2213	2213	S07-2	Official Results	13	121	0.590909090909091	0.742331288343558	The results are similar and the ranking is the same.	0
2214	2214	S07-2	Official Results	14	122	0.636363636363636	0.748466257668712	We omit them for brevity.	0
2215	2215	S07-2	Official Results	15	123	0.681818181818182	0.754601226993865	The results of the supervised evaluation can be seen in Table 4.	0
2216	2216	S07-2	Official Results	16	124	0.727272727272727	0.760736196319018	The evaluation is also performed over the test corpus.	0
2217	2217	S07-2	Official Results	17	125	0.772727272727273	0.766871165644172	Apart from participants, we also show the most frequent sense (MFS), which tags every test instance with the sense that occurred most often in the training part.	0
2218	2218	S07-2	Official Results	18	126	0.818181818181818	0.773006134969325	Note that the supervised evaluation combines the information in the clustering solution implicitly with the MFS information via the mapping in the training part.	0
2219	2219	S07-2	Official Results	19	127	0.863636363636364	0.779141104294479	Previous Senseval evaluation exercises have shown that the MFS baseline is very hard to beat by unsupervised systems.	0
2220	2220	S07-2	Official Results	20	128	0.909090909090909	0.785276073619632	In fact, only three of the participant systems are above the MFS baseline, which shows that the clustering information carries over the mapping successfully for these systems.	0
2221	2221	S07-2	Official Results	21	129	0.954545454545455	0.791411042944785	Note that the 1c1word baseline is equivalent to MFS in this setting.	0
2222	2222	S07-2	Official Results	22	130	1.0	0.797546012269939	We will review the random baseline in the discussion section below.	0
2223	2223	S07-2	Further Results	1	131	0.076923076923077	0.803680981595092	Table 5 shows the results of the best systems from the lexical sample subtask of task 17.	0
2224	2224	S07-2	Further Results	2	132	0.153846153846154	0.809815950920245	The best sense induction system is only 6.9 percentage points below the best supervised, and 3.5 percentage points below the best (and only) semi-supervised system.	0
2225	2225	S07-2	Further Results	3	133	0.230769230769231	0.815950920245399	If the sense induction system had participated, it would be deemed as semi-supervised, as it uses, albeit in a shallow way, the training data for mapping the clusters into senses.	0
2226	2226	S07-2	Further Results	4	134	0.307692307692308	0.822085889570552	In this sense, our supervised evaluation does not seek to optimize the available training data.	0
2227	2227	S07-2	Further Results	5	135	0.384615384615385	0.828220858895705	After the official evaluation, we realized that contrary to previous lexical sample evaluation exercises task 17 organizers did not follow a random train/test split.	0
2228	2228	S07-2	Further Results	6	136	0.461538461538462	0.834355828220859	We decided to produce a random train/test split following the same 82/18 proportion as the official split, and re-evaluated the systems.	0
2229	2229	S07-2	Further Results	7	137	0.538461538461538	0.840490797546012	The results are presented in   participants are above the MFS baseline, showing that all of them learned useful clustering information.	0
2230	2230	S07-2	Further Results	8	138	0.615384615384615	0.846625766871166	Note that UOY was specially affected by the original split.	0
2231	2231	S07-2	Further Results	9	139	0.692307692307692	0.852760736196319	The distribution of senses in this split did not vary (cf. Table 2).	0
2232	2232	S07-2	Further Results	10	140	0.769230769230769	0.858895705521472	Finally, we also studied the supervised evaluation of several random clustering algorithms, which can attain performances close to MFS, thanks to the mapping information.	0
2233	2233	S07-2	Further Results	11	141	0.846153846153846	0.865030674846626	This is due to the fact that the random clusters would be mapped to the most frequent senses.	0
2234	2234	S07-2	Further Results	12	142	0.923076923076923	0.871165644171779	Table 7 shows the results of random solutions using varying numbers of clusters (e.g. random2 is a random choice between two clusters).	0
2235	2235	S07-2	Further Results	13	143	1.0	0.877300613496933	Random2 is only 0.1 below MFS, but as the number of clusters increases some clusters don't get mapped, and the recall of the random baselines decrease.	0
2236	2236	S07-2	Discussion	1	144	0.090909090909091	0.883435582822086	The evaluation of clustering solutions is not straightforward.	0
2237	2237	S07-2	Discussion	2	145	0.181818181818182	0.889570552147239	All measures have some bias towards certain clustering strategy, and this is one of the reasons of adding the supervised evaluation as a complementary information to the more standard unsupervised evaluation.	0
2238	2238	S07-2	Discussion	3	146	0.272727272727273	0.895705521472393	In our case, we noticed that the FScore penalized the systems with a high number of clusters, and favored those that induce less senses.	0
2239	2239	S07-2	Discussion	4	147	0.363636363636364	0.901840490797546	Given the fact that FScore tries to balance precision (higher for large numbers of clusters) and recall (higher for small numbers of clusters), this was not expected.	0
2240	2240	S07-2	Discussion	5	148	0.454545454545455	0.907975460122699	"We were also surprised to see that no system could System Supervised evaluation random2 78.6 random10 77.6 ramdom100 64.2 random1000 31.8 beat the ""one cluster one word"" baseline."	0
2241	2241	S07-2	Discussion	6	149	0.545454545454545	0.914110429447853	An explanation might lay in that the gold-standard was based on the coarse-grained OntoNotes senses.	0
2242	2242	S07-2	Discussion	7	150	0.636363636363636	0.920245398773006	We also noticed that some words had hundreds of instances and only a single sense.	0
2243	2243	S07-2	Discussion	8	151	0.727272727272727	0.926380368098159	We suspect that the participating systems would have beaten all baselines if a fine-grained sense inventory like WordNet had been used, as was customary in previous WSD evaluation exercises.	0
2244	2244	S07-2	Discussion	9	152	0.818181818181818	0.932515337423313	Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.	0
2245	2245	S07-2	Discussion	10	153	0.909090909090909	0.938650306748466	Each of the induced clusters is mapped into a weighted vector of senses, and thus inducing a number of clusters similar to the number of senses is not a requirement for good results.	0
2246	2246	S07-2	Discussion	11	154	1.0	0.94478527607362	With this measure some of the systems 3 are able to beat all baselines.	0
2247	2247	S07-2	Conclusions	1	155	0.111111111111111	0.950920245398773	We have presented the design and results of the SemEval-2007 task 02 on evaluating word sense induction and discrimination systems.	0
2248	2248	S07-2	Conclusions	2	156	0.222222222222222	0.957055214723926	6 systems participated, but one of them was not a sense induction system.	0
2249	2249	S07-2	Conclusions	3	157	0.333333333333333	0.96319018404908	We reused the data from the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the training part of the dataset for mapping).	0
2250	2250	S07-2	Conclusions	4	158	0.444444444444444	0.969325153374233	We also provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
2251	2251	S07-2	Conclusions	5	159	0.555555555555556	0.975460122699387	Evaluating clustering solutions is not straightforward.	0
2252	2252	S07-2	Conclusions	6	160	0.666666666666667	0.98159509202454	The unsupervised evaluation seems to be sensitive to the number of senses in the gold standard, and the coarse grained sense inventory used in the gold standard had a great impact in the results.	0
2253	2253	S07-2	Conclusions	7	161	0.777777777777778	0.987730061349693	The supervised evaluation introduces a mapping step which interacts with the clustering solution.	0
2254	2254	S07-2	Conclusions	8	162	0.888888888888889	0.993865030674847	In fact, the ranking of the participating systems 3 All systems in the case of a random train/test split varies according to the evaluation method used.	0
2255	2255	S07-2	Conclusions	9	163	1.0	1.0	We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systems needs further developments, perhaps linked to a certain application or purpose.	0
2815	2815	S07-9	title	1	1	1.0	0.007692307692308	SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish	0
2816	2816	S07-9	abstract	1	2	0.333333333333333	0.015384615384615	In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish).	0
2817	2817	S07-9	abstract	2	3	0.666666666666667	0.023076923076923	In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish.	1
2818	2818	S07-9	abstract	3	4	1.0	0.030769230769231	Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.	0
2819	2819	S07-9	Introduction	1	5	0.071428571428572	0.038461538461539	The Multilevel Semantic Annotation of Catalan and Spanish task is split into the following three subtasks:	0
2820	2820	S07-9	Introduction	2	6	0.142857142857143	0.046153846153846	Noun Sense Disambiguation (NSD):	0
2821	2821	S07-9	Introduction	3	7	0.214285714285714	0.053846153846154	"Disambiguation of all frequent nouns (""all words"" style)."	0
2822	2822	S07-9	Introduction	4	8	0.285714285714286	0.061538461538462	Named Entity Recognition (NER):	0
2823	2823	S07-9	Introduction	5	9	0.357142857142857	0.069230769230769	The annotation of (possibly embedding) named entities with basic entity types.	0
2824	2824	S07-9	Introduction	6	10	0.428571428571429	0.076923076923077	Semantic Role Labeling (SRL):	0
2825	2825	S07-9	Introduction	7	11	0.5	0.084615384615385	Including also two subtasks, i.e., the annotation of verbal predicates with semantic roles (SR), and verb tagging with semantic-class labels (SC).	0
2826	2826	S07-9	Introduction	8	12	0.571428571428571	0.092307692307692	All semantic annotation tasks are performed on exactly the same corpora for each language.	0
2827	2827	S07-9	Introduction	9	13	0.642857142857143	0.1	We presented all the annotation levels together as a complex global task, since we were interested in approaches which address these problems jointly, possibly taking into account cross-dependencies among them.	0
2828	2828	S07-9	Introduction	10	14	0.714285714285714	0.107692307692308	However, we were also accepting systems approaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of the languages.	0
2829	2829	S07-9	Introduction	11	15	0.785714285714286	0.115384615384615	In Section 2 we describe the methodology followed to develop the linguistic corpora for the task.	0
2830	2830	S07-9	Introduction	12	16	0.857142857142857	0.123076923076923	Sections 3 and 4 summarize the task setting and the participant systems, respectively.	0
2831	2831	S07-9	Introduction	13	17	0.928571428571429	0.130769230769231	Finally, Section 5 presents a comparative analysis of the results.	0
2832	2832	S07-9	Introduction	14	18	1.0	0.138461538461538	For any additional information on corpora, resources, formats, tagsets, annotation manuals, etc. we refer the reader to the official website of the task 1 .	0
2833	2833	S07-9	Linguistic corpora	1	19	0.2	0.146153846153846	The corpora used in this SemEval task are a subset of CESS-ECE, a multilingual Treebank, composed of a Spanish (CESS-ESP) and a Catalan (CESS-CAT) corpus of 500K words each (Martí et al., 2007b).	0
2834	2834	S07-9	Linguistic corpora	2	20	0.4	0.153846153846154	These corpora were enriched with different kinds of semantic information: argument structure, thematic roles, semantic class, named entities, and WordNet synsets for the 150 most frequent nouns.	0
2835	2835	S07-9	Linguistic corpora	3	21	0.6	0.161538461538462	The annotation process was carried out in a semiautomatic way, with a posterior manual revision of all automatic processes.	0
2836	2836	S07-9	Linguistic corpora	4	22	0.8	0.169230769230769	A sequential approach was adopted for the annotation of the corpus, beginning with the basic levels of analysis, i.e., POS tagging and chunking (automatically performed) and followed by the more complex levels: syntactic constituents and functions (manually tagged) and semantic annotation (manual and semiautomatic processes with manual completion and posterior revision).	0
2837	2837	S07-9	Linguistic corpora	5	23	1.0	0.176923076923077	Furthermore, some experiments concerning inter-annotator agreement were carried out at the syntactic (Civit et al., 2003) and semantic levels (Màrquez et al., 2004) in order to evaluate the quality of the results.	0
2838	2838	S07-9	Syntactic Annotation	1	24	0.25	0.184615384615385	The syntactic annotation consists of the labeling of constituents, including elliptical subjects, and syntactic functions.	0
2839	2839	S07-9	Syntactic Annotation	2	25	0.5	0.192307692307692	The surface order was maintained and only those constituents directly attached to any kind of 'Sentence' root node were considered ('S', 'S.NF', 'S.F', 'S*').	0
2840	2840	S07-9	Syntactic Annotation	3	26	0.75	0.2	The syntactic functions are: subject (SUJ), direct object (OD), indirect object (OI), attribute (ATR), predicative (CPRED), agent complement (CAG), and adjunct (CC).	0
2841	2841	S07-9	Syntactic Annotation	4	27	1.0	0.207692307692308	Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive any thematic role.	0
2842	2842	S07-9	Lexical Semantic Information: WordNet	1	28	0.2	0.215384615384615	We selected the 150 most frequent nouns in the whole corpus and annotated their occurrences with WordNet synsets.	0
2843	2843	S07-9	Lexical Semantic Information: WordNet	2	29	0.4	0.223076923076923	No other word categories were treated (verbs, adjectives and adverbs).	0
2844	2844	S07-9	Lexical Semantic Information: WordNet	3	30	0.6	0.230769230769231	We used a steady version of Catalan and Spanish WordNets, linked to WordNet 1.6.	0
2845	2845	S07-9	Lexical Semantic Information: WordNet	4	31	0.8	0.238461538461538	Each noun either matched a WordNet synset or a special label indicating a specific circumstance (for instance, the tag C2S indicates that the word does not appear in the dictionary).	0
2846	2846	S07-9	Lexical Semantic Information: WordNet	5	32	1.0	0.246153846153846	All this process was carried out manually.	0
2847	2847	S07-9	Named Entities	1	33	0.111111111111111	0.253846153846154	The corpora were annotated with both strong and weak Named Entities.	0
2848	2848	S07-9	Named Entities	2	34	0.222222222222222	0.261538461538462	"Strong NEs correspond to single lexical tokens (e.g., ""[U.S.] LOC ""), while weak NEs include, by definition, some strong entities (e.g., ""The [president of [US] LOC ] P ER "")."	0
2849	2849	S07-9	Named Entities	3	35	0.333333333333333	0.269230769230769	(Arévalo et al., 2004).	0
2850	2850	S07-9	Named Entities	4	36	0.444444444444444	0.276923076923077	Thus, NEs may embed.	0
2851	2851	S07-9	Named Entities	5	37	0.555555555555556	0.284615384615385	Six basic semantic categories were distinguished: Person, Organization, Location, Date, Numerical expression, and Others (Borrega et al., 2007).	0
2852	2852	S07-9	Named Entities	6	38	0.666666666666667	0.292307692307692	Two golden rules underlie the definition of NEs in Spanish and Catalan.	0
2853	2853	S07-9	Named Entities	7	39	0.777777777777778	0.3	On the one hand, only a noun phrase can be a NE.	0
2854	2854	S07-9	Named Entities	8	40	0.888888888888889	0.307692307692308	On the other hand, its referent must be unique and unambiguous.	0
2855	2855	S07-9	Named Entities	9	41	1.0	0.315384615384615	Finally, another hard rule (although not 100% reliable) is that only a definite singular noun phrase might be a NE.	0
2856	2856	S07-9	Thematic Role Labeling / Semantic Class	1	42	0.1	0.323076923076923	Basic syntactic functions were tagged with both arguments and thematic roles, taking into account the semantic class related to the verbal predicate (Taulé et al., 2006b).	0
2857	2857	S07-9	Thematic Role Labeling / Semantic Class	2	43	0.2	0.330769230769231	We characterized predicates by means of a limited number of Semantic Classes based on Event Structure Patterns, according to four basic event classes: states, activities, accomplishments, and achievements.	0
2858	2858	S07-9	Thematic Role Labeling / Semantic Class	3	44	0.3	0.338461538461538	These general classes were split into 17 subclasses, depending on thematic roles and diathesis alternations.	0
2859	2859	S07-9	Thematic Role Labeling / Semantic Class	4	45	0.4	0.346153846153846	Similar to PropBank, the set of arguments selected by the verb are incrementally numbered expressing the degree of proximity of an argument in relation to the verb (Arg0, Arg1, Arg2, Arg3, Arg4).	0
2860	2860	S07-9	Thematic Role Labeling / Semantic Class	5	46	0.5	0.353846153846154	In our proposal, each argument includes the thematic role in its label (e.g., Arg1-PAT).	0
2861	2861	S07-9	Thematic Role Labeling / Semantic Class	6	47	0.6	0.361538461538461	Thus, we have two different levels of semantic description: the argument position and the specific thematic role.	0
2862	2862	S07-9	Thematic Role Labeling / Semantic Class	7	48	0.7	0.369230769230769	This information was previously stored in a verbal lexicon for each language.	0
2863	2863	S07-9	Thematic Role Labeling / Semantic Class	8	49	0.8	0.376923076923077	In these lexicons, a semantic class was established for each verbal sense, and the mapping between their syntactic functions with the corresponding argument structure and thematic roles was declared.	0
2864	2864	S07-9	Thematic Role Labeling / Semantic Class	9	50	0.9	0.384615384615385	These classes resulted from the analysis of 1,555 verbs from the Spanish corpus and 1,077 from the Catalan.	0
2865	2865	S07-9	Thematic Role Labeling / Semantic Class	10	51	1.0	0.392307692307692	The annotation process was performed in two steps: firstly, we annotated automatically the unambiguous correspondences between syntactic functions and thematic roles (Martí et al., 2007a); secondly, we manually checked the outcome of the previous process and completed the rest of thematic role assignments.	0
2866	2866	S07-9	Subset for SemEval-2007	1	52	0.2	0.4	The corpora extracted from CESS-ECE to conform SemEval-2007 datasets are: (a) SemEval-CESS-ESP (Spanish), made of 101,136 words (3,611 sentences), with 29% of the corpus coming from the Spanish EFE News Agency and 71% coming from Lexesp, a Spanish balanced corpus; (b) SemEval-CESS-CAT (Catalan), consisting of 108,207 words (3,202 sentences), with 71% of the corpus consistinf of Catalan news from EFE News Agency and 29% coming from the Catalan News Agency (ACN).	0
2867	2867	S07-9	Subset for SemEval-2007	2	53	0.4	0.407692307692308	These corpora were split into training and test subsets following a a 90%-10% proportion.	0
2868	2868	S07-9	Subset for SemEval-2007	3	54	0.6	0.415384615384615	Each test set was also partitioned into two subsets: 'indomain' and 'out-of-domain' test corpora.	0
2869	2869	S07-9	Subset for SemEval-2007	4	55	0.8	0.423076923076923	The first is intended to be homogeneous with respect to the training corpus and the second was extracted from a part of the CESS-ECE corpus annotated later and not involved in the development of the resources (e.g., verbal dictionaries).	0
2870	2870	S07-9	Subset for SemEval-2007	5	56	1.0	0.430769230769231	2	0
2871	2871	S07-9	Task setting	1	57	0.111111111111111	0.438461538461538	Data formats are similar to those of CoNLL-2004/2005 shared tasks on SRL (column style presentation of levels of annotation), in order to be able to share evaluation tools and already developed scripts for format conversion.	0
2872	2872	S07-9	Task setting	2	58	0.222222222222222	0.446153846153846	In Figure 1 you can find an example of a fully annotated sentence in the column-based format.	0
2873	2873	S07-9	Task setting	3	59	0.333333333333333	0.453846153846154	There is one line for each token, and a blank line after the last token of each sentence.	0
2874	2874	S07-9	Task setting	4	60	0.444444444444444	0.461538461538462	The columns, separated by blank spaces, represent different annotations of the sentence with a tagging along words.	0
2875	2875	S07-9	Task setting	5	61	0.555555555555556	0.469230769230769	For structured annotations (parse trees, named entities, and arguments), we use the Start-End format.	0
2876	2876	S07-9	Task setting	6	62	0.666666666666667	0.476923076923077	Columns 1-6 correspond to the input information; columns 7 and above contain the information to be predicted.	0
2877	2877	S07-9	Task setting	7	63	0.777777777777778	0.484615384615385	We can group annotations in five main categories:	0
2878	2878	S07-9	Task setting	8	64	0.888888888888889	0.492307692307692	All these annotations in column format are extracted automatically from the syntactic-semantic trees from the CESS-ECE corpora, which were distributed with the datasets.	0
2879	2879	S07-9	Task setting	9	65	1.0	0.5	Participants were also provided with the whole Catalan and Spanish Word-Nets (v1.6), the verbal lexicons used in the role labeling annotation, the annotation guidelines as well as the annotated corpora.	0
2880	2880	S07-9	Participant systems	1	66	0.083333333333333	0.507692307692308	About a dozen teams expressed their interest in the task.	0
2881	2881	S07-9	Participant systems	2	67	0.166666666666667	0.515384615384615	From those, only 5 registered and downloaded datasets, and finally, only two teams met the deadline and submitted results.	0
2882	2882	S07-9	Participant systems	3	68	0.25	0.523076923076923	ILK2 (Tilburg University) presented a system addressing Semantic Role Labeling, and UPC* (Technical University of Catalonia) presented a system addressing all subtasks independently 3 .	0
2883	2883	S07-9	Participant systems	4	69	0.333333333333333	0.530769230769231	The ILK2 SRL system is based on memory-based classification of syntactic constituents using a rich feature set.	0
2884	2884	S07-9	Participant systems	5	70	0.416666666666667	0.538461538461538	UPC* used several machine learning algorithms for addressing the different subtasks (AdaBoost, SVM, Perceptron).	0
2885	2885	S07-9	Participant systems	6	71	0.5	0.546153846153846	For SRL, the system implements a re-ranking strategy using global features.	0
2886	2886	S07-9	Participant systems	7	72	0.583333333333333	0.553846153846154	The candidates are generated using a state-of-the-art SRL base system.	0
2887	2887	S07-9	Participant systems	8	73	0.666666666666667	0.561538461538462	Although the task targeted at systems addressing all subtasks jointly none of the participants did it.	0
2888	2888	S07-9	Participant systems	9	74	0.75	0.569230769230769	4	0
2889	2889	S07-9	Participant systems	10	75	0.833333333333333	0.576923076923077	We believe that the high complexity of the whole task together with the short period of time available were the main reasons for this failure.	0
2890	2890	S07-9	Participant systems	11	76	0.916666666666667	0.584615384615385	From this point of view, the conclusions are somehow disappointing.	0
2891	2891	S07-9	Participant systems	12	77	1.0	0.592307692307692	However, we think that we have contributed with a very valuable resource for the future research and, although not complete, the current systems provide also valuable insights about the task and are very good baselines for the systems to come.	0
2892	2892	S07-9	Evaluation	1	78	0.166666666666667	0.6	In the following subsections we present an analysis of the results obtained by participant systems in the INPUT--------------------------------------------------------------&gt; OUTPUT-----------------------------------BASIC_INPUT_INFO-----&gt; EXTRA_INPUT_INFO---------------------------&gt; NE NS-------&gt; SR------------------------	0
2893	2893	S07-9	Evaluation	2	79	0.333333333333333	0.607692307692308	-------------------------------------------------------------------------------------------------------------  three subtasks.	0
2894	2894	S07-9	Evaluation	3	80	0.5	0.615384615384615	Results on the test set are presented along 2 dimensions: (a) language ('ca'=Catalan; 'es'=Spanish); (b) corpus source ('in'=in-domain corpus; 'out'=out-of-domain corpus).	0
2895	2895	S07-9	Evaluation	4	81	0.666666666666667	0.623076923076923	We will use a language.	0
2896	2896	S07-9	Evaluation	5	82	0.833333333333333	0.630769230769231	source pair to denote a particular test set.	0
2897	2897	S07-9	Evaluation	6	83	1.0	0.638461538461538	Finally, '*' will denote the addition of the two subcorpora, either in the language or source dimensions.	0
2898	2898	S07-9	NSD	1	84	0.111111111111111	0.646153846153846	"Results on the NSD subtask are presented in  The left part of the table (""all words"") contains results on the complete test sets, while the right part (""selected words"") contains the results restricted to the set of words with trained SVM classifiers."	0
2899	2899	S07-9	NSD	2	85	0.222222222222222	0.653846153846154	This set covers 31.0% of the word occurrences in the training set and 28.2% in the complete test set.	0
2900	2900	S07-9	NSD	3	86	0.333333333333333	0.661538461538462	The main observation is that training/test corpora contain few sense variations.	0
2901	2901	S07-9	NSD	4	87	0.444444444444444	0.669230769230769	Sense distributions are very skewed and, thus, the simple baseline shows a very high accuracy (almost 85%).	0
2902	2902	S07-9	NSD	5	88	0.555555555555556	0.676923076923077	The UPC* system only improves BSL accuracy by one point.	0
2903	2903	S07-9	NSD	6	89	0.666666666666667	0.684615384615385	This can be partly explained by the small size of the wordbased training corpora.	0
2904	2904	S07-9	NSD	7	90	0.777777777777778	0.692307692307692	Also, this improvement is diminished because UPC* only treated a subset of words.	0
2905	2905	S07-9	NSD	8	91	0.888888888888889	0.7	However, looking at the right-hand side of the table, the improvement over the baseline is still modest (∼3 points) when focusing only on the treated words.	0
2906	2906	S07-9	NSD	9	92	1.0	0.707692307692308	As a final observation, no significant differences are observed across languages and corpora sources.	0
2907	2907	S07-9	NER	1	93	0.05	0.715384615384615	Results on the NER subtask are presented in Table 2.	0
2908	2908	S07-9	NER	2	94	0.1	0.723076923076923	This time, BSL stands for a baseline system consisting of collecting a gazetteer with the strong NEs appearing in the training set and assigning the longest matches of these NEs in the test set.	0
2909	2909	S07-9	NER	3	95	0.15	0.730769230769231	Weak entities are simply ignored by BSL.	0
2910	2910	S07-9	NER	4	96	0.2	0.738461538461539	UPC* presented a system which treats strong and weak NEs in a pipeline of two processors.	0
2911	2911	S07-9	NER	5	97	0.25	0.746153846153846	Classifiers trained with multiclass AdaBoost are used to predict the strong and weak NEs.	0
2912	2912	S07-9	NER	6	98	0.3	0.753846153846154	See authors' paper for details.	0
2913	2913	S07-9	NER	7	99	0.35	0.761538461538462	Table 2: Overall results on the NER subtask UPC* system largely overcomes the baseline, mainly due to the low recall of the latter.	0
2914	2914	S07-9	NER	8	100	0.4	0.769230769230769	By languages, results on Catalan are significantly better than those on Spanish.	0
2915	2915	S07-9	NER	9	101	0.45	0.776923076923077	We think this is attributable mainly to corpora variations across languages.	0
2916	2916	S07-9	NER	10	102	0.5	0.784615384615385	"By corpus source, ""in-domain"" results are slightly better, but the difference is small (1.78 points)."	0
2917	2917	S07-9	NER	11	103	0.55	0.792307692307692	Overall, the results for the NER task are in the mid seventies, a remarkable result given the small training set and the complexity of predicting embedded NEs.	0
2918	2918	S07-9	NER	12	104	0.6	0.8	Detailed results on concrete entity types are presented in Table 3 (sorted by decreasing F 1 ).	0
2919	2919	S07-9	NER	13	105	0.65	0.807692307692308	As expected, DAT and NUM are the easiest entities to recognize since they can be easily detected by simple patterns and POS tags.	0
2920	2920	S07-9	NER	14	106	0.7	0.815384615384615	On the contrary, entity types requiring more semantic information present fairly lower results.	0
2921	2921	S07-9	NER	15	107	0.75	0.823076923076923	ORG PER and LOC are in the seventies, while OTH is by far the most difficult class, showing a very low recall.	0
2922	2922	S07-9	NER	16	108	0.8	0.830769230769231	This is not surprising since OTH agglutinates a wide variety of entity cases which are difficult to characterize as a whole.	0
2923	2923	S07-9	NER	17	109	0.85	0.838461538461539	Another interesting analysis is to study the differences between strong and weak entities (see Table 4) .	0
2924	2924	S07-9	NER	18	110	0.9	0.846153846153846	Contrary to our first expectations, results on weak entities are much better (up to 11 F 1 points higher).	0
2925	2925	S07-9	NER	19	111	0.95	0.853846153846154	Weak NEs are simpler for two reasons: (a) there exist simple patters to characterize them, with-out the need of fully recognizing their internal strong NEs; (b) there is some redundancy in the corpus when tagging many equivalent weak NEs in embedded noun phrases.	0
2926	2926	S07-9	NER	20	112	1.0	0.861538461538462	"It is worth noting that the low results for strong NEs come from classification rather than recognition (recognition is almost 100% given the ""proper noun"" PoS tag), thus the recall for weak entities is not diminished by the errors in strong entity classification."	0
2927	2927	S07-9	SRL	1	113	0.055555555555556	0.869230769230769	SRL is the most complex and interesting problem in the task.	0
2928	2928	S07-9	SRL	2	114	0.111111111111111	0.876923076923077	We had two participants ILK2 and UPC*, which participated in both subproblems, i.e., labeling arguments of verbal predicates with thematic roles (SR), and assigning semantic class labels to target verbs (SC).	0
2929	2929	S07-9	SRL	3	115	0.166666666666667	0.884615384615385	Detailed results of the two systems are presented in Tables 5 and 6.	0
2930	2930	S07-9	SRL	4	116	0.222222222222222	0.892307692307692	The ILK2 system outperforms UPC* in both SR and SC.	0
2931	2931	S07-9	SRL	5	117	0.277777777777778	0.9	For SR, both systems use a traditional architecture of labeling syntactic tree nodes with thematic roles using supervised classifiers.	0
2932	2932	S07-9	SRL	6	118	0.333333333333333	0.907692307692308	We would attribute the overall F 1 difference (2.68 points) to a better feature engineering by ILK2, rather than to differences in the Machine Learning techniques used.	0
2933	2933	S07-9	SRL	7	119	0.388888888888889	0.915384615384615	Overall results in the eighties are remarkably high given the training set size and the granularity of the thematic roles (though we have to take into account that systems work with gold parse trees).	0
2934	2934	S07-9	SRL	8	120	0.444444444444444	0.923076923076923	"Again, the results are comparable across languages and slightly better in the ""in-domain"" test set."	0
2935	2935	S07-9	SRL	9	121	0.5	0.930769230769231	In the SC subproblem, the differences are similar (2.60 points).	0
2936	2936	S07-9	SRL	10	122	0.555555555555556	0.938461538461538	In this case, ILK2 trained specialized classifiers for the task, while UPC* used heuristics based on the SR outcomes.	0
2937	2937	S07-9	SRL	11	123	0.611111111111111	0.946153846153846	As a reference, the baseline consisting of tagging each verb with its most frequent semantic class achieves F 1 values of 64.01, 63.97, 41.00, and 57.42 on ca.in, ca.out, es.in, es.out, respectively.	0
2938	2938	S07-9	SRL	12	124	0.666666666666667	0.953846153846154	Now, the results are significantly better in Catalan, and, surprisingly, the 'out' test corpora makes F 1 to raise.	0
2939	2939	S07-9	SRL	13	125	0.722222222222222	0.961538461538462	The latter is an anomalous situation provoked by the 'es.in' tset.	0
2940	2940	S07-9	SRL	14	126	0.777777777777778	0.969230769230769	5 Table 7 shows the global SR results by numbered arguments and adjuncts Interestingly, tagging adjuncts is far more difficult than tagging core arguments (this result was also observed for English in previous works).	0
2941	2941	S07-9	SRL	15	127	0.833333333333333	0.976923076923077	Moreover, the global difference between ILK2 and UPC* systems is explained by their ability to tag adjuncts (70.22 vs. 58.37).	0
2942	2942	S07-9	SRL	16	128	0.888888888888889	0.984615384615385	In the core arguments both systems are tied.	0
2943	2943	S07-9	SRL	17	129	0.944444444444444	0.992307692307692	Also in the same table we can see the overall results on a simplified SR setting, in which the thematic roles are eliminated from the SR labels keeping only the argument number (like other evaluations on PropBank).	0
2944	2944	S07-9	SRL	18	130	1.0	1.0	The results are only ∼2 points higher in this setting.	0
5272	5272	S10-12	title	1	1	1.0	0.006849315068493	SemEval-2010 Task 12: Parser Evaluation using Textual Entailments	0
5273	5273	S10-12	abstract	1	2	0.333333333333333	0.013698630136986	Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.	0
5274	5274	S10-12	abstract	2	3	0.666666666666667	0.02054794520548	The task involves recognizing textual entailments based on syntactic information alone.	1
5275	5275	S10-12	abstract	3	4	1.0	0.027397260273973	PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.	0
5276	5276	S10-12	Introduction	1	5	0.016393442622951	0.034246575342466	Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.	1
5277	5277	S10-12	Introduction	2	6	0.032786885245902	0.041095890410959	"Given two text fragments called ""text"" and ""hypothesis"", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text."	0
5278	5278	S10-12	Introduction	3	7	0.049180327868853	0.047945205479452	In contrast with general RTE tasks (Dagan et al., 2009) the PETE task focuses on syntactic entailments:	0
5279	5279	S10-12	Introduction	4	8	0.065573770491803	0.054794520547945	Text:	0
5280	5280	S10-12	Introduction	5	9	0.081967213114754	0.061643835616438	The man with the hat was tired.	0
5281	5281	S10-12	Introduction	6	10	0.098360655737705	0.068493150684932	Hypothesis-1: The man was tired.	0
5282	5282	S10-12	Introduction	7	11	0.114754098360656	0.075342465753425	(yes) Hypothesis-2: The hat was tired.	0
5283	5283	S10-12	Introduction	8	12	0.131147540983607	0.082191780821918	(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).	0
5284	5284	S10-12	Introduction	9	13	0.147540983606557	0.089041095890411	We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
5285	5285	S10-12	Introduction	10	14	0.163934426229508	0.095890410958904	The PARSEVAL measures introduced nearly two decades ago (Black et al., 1991) still dominate the field of parser evaluation.	0
5286	5286	S10-12	Introduction	11	15	0.180327868852459	0.102739726027397	"These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or ""treebank""."	0
5287	5287	S10-12	Introduction	12	16	0.19672131147541	0.10958904109589	Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.	0
5288	5288	S10-12	Introduction	13	17	0.213114754098361	0.116438356164384	Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.	0
5289	5289	S10-12	Introduction	14	18	0.229508196721311	0.123287671232877	Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.	0
5290	5290	S10-12	Introduction	15	19	0.245901639344262	0.13013698630137	The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.	0
5291	5291	S10-12	Introduction	16	20	0.262295081967213	0.136986301369863	In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank (Marcus et al., 1994), 5646 (15%) have multiple conflicting annotations.	0
5292	5292	S10-12	Introduction	17	21	0.278688524590164	0.143835616438356	If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90% (Charniak and Johnson, 2005).	0
5293	5293	S10-12	Introduction	18	22	0.295081967213115	0.150684931506849	Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.	0
5294	5294	S10-12	Introduction	19	23	0.311475409836066	0.157534246575342	Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.	0
5295	5295	S10-12	Introduction	20	24	0.327868852459016	0.164383561643836	Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.	0
5296	5296	S10-12	Introduction	21	25	0.344262295081967	0.171232876712329	Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (Bonnema et al., 1997).	0
5297	5297	S10-12	Introduction	22	26	0.360655737704918	0.178082191780822	In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.	0
5298	5298	S10-12	Introduction	23	27	0.377049180327869	0.184931506849315	Framework independence: Entailment recognition is a formalism independent task.	0
5299	5299	S10-12	Introduction	24	28	0.39344262295082	0.191780821917808	A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (Nivre et al., 2007a;	0
5300	5300	S10-12	Introduction	25	29	0.40983606557377	0.198630136986301	Hockenmaier and Steedman, 2007).	0
5301	5301	S10-12	Introduction	26	30	0.426229508196721	0.205479452054794	The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.	0
5302	5302	S10-12	Introduction	27	31	0.442622950819672	0.212328767123288	In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.	0
5303	5303	S10-12	Introduction	28	32	0.459016393442623	0.219178082191781	Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.	0
5304	5304	S10-12	Introduction	29	33	0.475409836065574	0.226027397260274	PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.	0
5305	5305	S10-12	Introduction	30	34	0.491803278688525	0.232876712328767	These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals).	0
5306	5306	S10-12	Introduction	31	35	0.508196721311475	0.23972602739726	Each use a set of binary relations between words in a sentence as the primary unit of representation.	0
5307	5307	S10-12	Introduction	32	36	0.524590163934426	0.246575342465753	They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.	0
5308	5308	S10-12	Introduction	33	37	0.540983606557377	0.253424657534247	Here is an example sentence and its SD representation (De Marneffe and Manning, 2008):	0
5309	5309	S10-12	Introduction	34	38	0.557377049180328	0.26027397260274	Bell, based in Los Angeles, makes and distributes electronic, computer and building products.	0
5310	5310	S10-12	Introduction	35	39	0.573770491803279	0.267123287671233	nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod (products-16, electronic-11) conj-and(electronic-11, computer-13) amod (products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.	0
5311	5311	S10-12	Introduction	36	40	0.590163934426229	0.273972602739726	Bell makes something.	0
5312	5312	S10-12	Introduction	37	41	0.60655737704918	0.280821917808219	Bell distributes something.	0
5313	5313	S10-12	Introduction	38	42	0.622950819672131	0.287671232876712	Someone is based in Los Angeles.	0
5314	5314	S10-12	Introduction	39	43	0.639344262295082	0.294520547945205	Someone makes products.	0
5315	5315	S10-12	Introduction	40	44	0.655737704918033	0.301369863013699	PETE has some advantages over representations based on grammatical relations.	0
5316	5316	S10-12	Introduction	41	45	0.672131147540984	0.308219178082192	For example SD defines 55 relations organized in a hierarchy, and it may be non-trivial for a non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).	0
5317	5317	S10-12	Introduction	42	46	0.688524590163934	0.315068493150685	In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.	0
5318	5318	S10-12	Introduction	43	47	0.704918032786885	0.321917808219178	In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.	0
5319	5319	S10-12	Introduction	44	48	0.721311475409836	0.328767123287671	However there are also significant challenges associated with an evaluation scheme like PETE.	0
5320	5320	S10-12	Introduction	45	49	0.737704918032787	0.335616438356164	It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.	0
5321	5321	S10-12	Introduction	46	50	0.754098360655738	0.342465753424657	Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.	0
5322	5322	S10-12	Introduction	47	51	0.770491803278688	0.349315068493151	Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.	0
5323	5323	S10-12	Introduction	48	52	0.786885245901639	0.356164383561644	"Introducing dummy words like ""someone"" or ""something"" alleviates part of the problem but does not help in the case of clausal complements."	0
5324	5324	S10-12	Introduction	49	53	0.80327868852459	0.363013698630137	In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.	0
5325	5325	S10-12	Introduction	50	54	0.819672131147541	0.36986301369863	PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.	0
5326	5326	S10-12	Introduction	51	55	0.836065573770492	0.376712328767123	This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.	0
5327	5327	S10-12	Introduction	52	56	0.852459016393443	0.383561643835616	However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.	0
5328	5328	S10-12	Introduction	53	57	0.868852459016393	0.39041095890411	The balance of what is being evaluated is also important.	0
5329	5329	S10-12	Introduction	54	58	0.885245901639344	0.397260273972603	A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.	0
5330	5330	S10-12	Introduction	55	59	0.901639344262295	0.404109589041096	In this evaluation, we focused on sentences and relations where state of the art parsers disagree.	0
5331	5331	S10-12	Introduction	56	60	0.918032786885246	0.410958904109589	We hope this methodology will uncover weaknesses that the next generation systems can focus on.	0
5332	5332	S10-12	Introduction	57	61	0.934426229508197	0.417808219178082	The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.	0
5333	5333	S10-12	Introduction	58	62	0.950819672131147	0.424657534246575	Section 2 explains the method followed to create the PETE dataset.	0
5334	5334	S10-12	Introduction	59	63	0.967213114754098	0.431506849315069	Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.	0
5335	5335	S10-12	Introduction	60	64	0.983606557377049	0.438356164383562	Section 4 presents the participating systems, their methods and results.	0
5336	5336	S10-12	Introduction	61	65	1.0	0.445205479452055	Section 5 summarizes our contribution.	0
5337	5337	S10-12	Dataset	1	66	0.25	0.452054794520548	To generate the entailments for the PETE task we followed the following three steps:	0
5338	5338	S10-12	Dataset	2	67	0.5	0.458904109589041	1. Identify syntactic dependencies that are challenging to state of the art parsers.	0
5339	5339	S10-12	Dataset	3	68	0.75	0.465753424657534	2. Construct short entailment sentences that paraphrase those dependencies.	0
5340	5340	S10-12	Dataset	4	69	1.0	0.472602739726027	3. Identify the subset of the entailments with high inter-annotator agreement.	0
5341	5341	S10-12	Identifying Challenging Dependencies	1	70	0.066666666666667	0.479452054794521	To identify syntactic dependencies that are challenging for current state of the art parsers, we used example sentences from the following sources:	0
5342	5342	S10-12	Identifying Challenging Dependencies	2	71	0.133333333333333	0.486301369863014	•	0
5343	5343	S10-12	Identifying Challenging Dependencies	3	72	0.2	0.493150684931507	"The ""Unbounded Dependency Corpus"" (Rimell et al., 2009)."	0
5344	5344	S10-12	Identifying Challenging Dependencies	4	73	0.266666666666667	0.5	"An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting ""gap""."	0
5345	5345	S10-12	Identifying Challenging Dependencies	5	74	0.333333333333333	0.506849315068493	"An unlimited number of clause boundaries may intervene between the moved element and the gap (hence ""unbounded"")."	0
5346	5346	S10-12	Identifying Challenging Dependencies	6	75	0.4	0.513698630136986	•	0
5347	5347	S10-12	Identifying Challenging Dependencies	7	76	0.466666666666667	0.520547945205479	A list of sentences from the Penn Treebank on which the Charniak parser (Charniak and Johnson, 2005) performs poorly 1 .	0
5348	5348	S10-12	Identifying Challenging Dependencies	8	77	0.533333333333333	0.527397260273973	•	0
5349	5349	S10-12	Identifying Challenging Dependencies	9	78	0.6	0.534246575342466	The Brown section of the Penn Treebank.	0
5350	5350	S10-12	Identifying Challenging Dependencies	10	79	0.666666666666667	0.541095890410959	We tested a number of parsers (both phrase structure and dependency) on these sentences and identified the differences in their output.	0
5351	5351	S10-12	Identifying Challenging Dependencies	11	80	0.733333333333333	0.547945205479452	We took sentences where at least one of the parsers gave a different answer than the others or the gold parse.	0
5352	5352	S10-12	Identifying Challenging Dependencies	12	81	0.8	0.554794520547945	Some of these differences reflected linguistic convention rather than semantic disagreement (e.g. representation of coordination) and some did not represent meaningful differences that can be expressed with entailments (e.g. labeling a phrase ADJP vs ADVP).	0
5353	5353	S10-12	Identifying Challenging Dependencies	13	82	0.866666666666667	0.561643835616438	The remaining differences typically reflected genuine semantic disagreements	0
5354	5354	S10-12	Identifying Challenging Dependencies	14	83	0.933333333333333	0.568493150684932	1 http://www.cs.brown.edu/˜ec/papers/badPars.txt.gz that would effect downstream applications.	0
5355	5355	S10-12	Identifying Challenging Dependencies	15	84	1.0	0.575342465753425	These were chosen to turn into entailments in the next step.	0
5356	5356	S10-12	Constructing Entailments	1	85	0.071428571428572	0.582191780821918	We tried to make the entailments as targeted as possible by building them around two content words that are syntactically related.	0
5357	5357	S10-12	Constructing Entailments	2	86	0.142857142857143	0.589041095890411	When the two content words were not sufficient to construct a grammatical sentence we used one of the following techniques:	0
5358	5358	S10-12	Constructing Entailments	3	87	0.214285714285714	0.595890410958904	•	0
5359	5359	S10-12	Constructing Entailments	4	88	0.285714285714286	0.602739726027397	"Complete the mandatory elements using the words ""somebody"" or ""something""."	0
5360	5360	S10-12	Constructing Entailments	5	89	0.357142857142857	0.60958904109589	(e.g.	0
5361	5361	S10-12	Constructing Entailments	6	90	0.428571428571429	0.616438356164384	"To test the subject-verb dependency in ""John kissed Mary."" we construct the entailment ""John kissed somebody."")"	0
5362	5362	S10-12	Constructing Entailments	7	91	0.5	0.623287671232877	•	0
5363	5363	S10-12	Constructing Entailments	8	92	0.571428571428571	0.63013698630137	Make a passive sentence to avoid using a spurious subject.	0
5364	5364	S10-12	Constructing Entailments	9	93	0.642857142857143	0.636986301369863	(e.g.	0
5365	5365	S10-12	Constructing Entailments	10	94	0.714285714285714	0.643835616438356	"To test the verb-object dependency in ""John kissed Mary."" we construct the entailment ""Mary was kissed."")"	0
5366	5366	S10-12	Constructing Entailments	11	95	0.785714285714286	0.650684931506849	•	0
5367	5367	S10-12	Constructing Entailments	12	96	0.857142857142857	0.657534246575342	"Make a copular sentence or use existential ""there"" to express noun modification."	0
5368	5368	S10-12	Constructing Entailments	13	97	0.928571428571429	0.664383561643836	(e.g.	0
5369	5369	S10-12	Constructing Entailments	14	98	1.0	0.671232876712329	"To test the noun-modifier dependency in ""The big red boat sank."" we construct the entailment ""The boat was big."" or ""There was a big boat."")"	0
5370	5370	S10-12	Filtering Entailments	1	99	0.055555555555556	0.678082191780822	To identify the entailments that are clear to human judgement we used the following procedure:	0
5371	5371	S10-12	Filtering Entailments	2	100	0.111111111111111	0.684931506849315	1. Each entailment was tagged by 5 untrained annotators from the Amazon Mechanical Turk crowdsourcing service.	0
5372	5372	S10-12	Filtering Entailments	3	101	0.166666666666667	0.691780821917808	2	0
5373	5373	S10-12	Filtering Entailments	4	102	0.222222222222222	0.698630136986301	The results from the annotators whose agreement with the gold parse fell below 70% were eliminated.	0
5374	5374	S10-12	Filtering Entailments	5	103	0.277777777777778	0.705479452054795	3	0
5375	5375	S10-12	Filtering Entailments	6	104	0.333333333333333	0.712328767123288	The entailments for which there was unanimous agreement of at least 3 annotators were kept.	0
5376	5376	S10-12	Filtering Entailments	7	105	0.388888888888889	0.719178082191781	The instructions for the annotators were brief and targeted people with no linguistic background:	0
5377	5377	S10-12	Filtering Entailments	8	106	0.444444444444444	0.726027397260274	Computers try to understand long sentences by dividing them into a set of short facts.	0
5378	5378	S10-12	Filtering Entailments	9	107	0.5	0.732876712328767	You will help judge whether the computer extracted the right facts from a given set of 25 English sentences.	0
5379	5379	S10-12	Filtering Entailments	10	108	0.555555555555556	0.73972602739726	Each of the following examples consists of a sentence (T), and a short statement (H) derived from this sentence by a computer.	0
5380	5380	S10-12	Filtering Entailments	11	109	0.611111111111111	0.746575342465753	"Please read both of them carefully and choose ""Yes"" if the meaning of (H) can be inferred from the meaning of (T)."	0
5381	5381	S10-12	Filtering Entailments	12	110	0.666666666666667	0.753424657534247	Here is an example: (T) Any lingering suspicion that this was a trick Al Budd had thought up was dispelled.	0
5382	5382	S10-12	Filtering Entailments	13	111	0.722222222222222	0.76027397260274	(H) The suspicion was dispelled.	0
5383	5383	S10-12	Filtering Entailments	14	112	0.777777777777778	0.767123287671233	Answer: YES (H) The suspicion was a trick.	0
5384	5384	S10-12	Filtering Entailments	15	113	0.833333333333333	0.773972602739726	Answer: NO	0
5385	5385	S10-12	Filtering Entailments	16	114	0.888888888888889	0.780821917808219	"You can choose the third option ""Not sure"" when the (H) statement is unrelated, unclear, ungrammatical or confusing in any other manner."	0
5386	5386	S10-12	Filtering Entailments	17	115	0.944444444444444	0.787671232876712	"The ""Not sure"" answers were grouped with the ""No"" answers during evaluation."	0
5387	5387	S10-12	Filtering Entailments	18	116	1.0	0.794520547945205	Approximately 50% of the original entailments were retained after the inter-annotator agreement filtering.	0
5388	5388	S10-12	Dataset statistics	1	117	0.25	0.801369863013699	The final dataset contained 367 entailments which were randomly divided into a 66 sentence development test and a 301 sentence test set.	0
5389	5389	S10-12	Dataset statistics	2	118	0.5	0.808219178082192	52% of the entailments in the test set were positive.	0
5390	5390	S10-12	Dataset statistics	3	119	0.75	0.815068493150685	Approximately half of the final entailments were from the Unbounded Dependency Corpus, a third were from the Brown section of the Penn Treebank, and the remaining were from the Charniak sentences.	0
5391	5391	S10-12	Dataset statistics	4	120	1.0	0.821917808219178	Table 1	0
5392	5392	S10-12	Baselines	1	121	0.083333333333333	0.828767123287671	In order to establish baseline results for this task, we built an entailment decision system for CoNLL format dependency files and tested several publicly available parsers.	0
5393	5393	S10-12	Baselines	2	122	0.166666666666667	0.835616438356164	The parsers used were the Berkeley Parser (Petrov and Klein, 2007), Charniak Parser (Charniak and Johnson, 2005), Collins Parser (Collins, 2003), Malt Parser (Nivre et al., 2007b), MSTParser (McDonald et al., 2005) and Stanford Parser (Klein and Manning, 2003).	0
5394	5394	S10-12	Baselines	3	123	0.25	0.842465753424658	Each parser was trained on sections 02-21 of the WSJ section of Penn Treebank.	0
5395	5395	S10-12	Baselines	4	124	0.333333333333333	0.849315068493151	Outputs of phrase structure parsers were automatically annotated with function tags using Blaheta's function tagger (Blaheta and Charniak, 2000) and converted to the dependency structure with LTH Constituentto-Dependency Conversion Tool (Johansson and Nugues, 2007).	0
5396	5396	S10-12	Baselines	5	125	0.416666666666667	0.856164383561644	To decide the entailments both the test and hypothesis sentences were parsed.	0
5397	5397	S10-12	Baselines	6	126	0.5	0.863013698630137	All the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations.	0
5398	5398	S10-12	Baselines	7	127	0.583333333333333	0.86986301369863	After applying some heuristics such as active-passive conversion, the extracted dependency path between the content words was searched in the dependency graph of the test sentence.	0
5399	5399	S10-12	Baselines	8	128	0.666666666666667	0.876712328767123	"In this search process, same relation types for the direct relations between the content word pairs and isomorphic subgraphs in the test and hypothesis sentences were required for the ""YES"" answer."	0
5400	5400	S10-12	Baselines	9	129	0.75	0.883561643835616	Table 2 lists the baseline results achieved.	0
5401	5401	S10-12	Baselines	10	130	0.833333333333333	0.89041095890411	There are significant differences in the entailment accuracies of systems that have comparable unlabeled attachment scores.	0
5402	5402	S10-12	Baselines	11	131	0.916666666666667	0.897260273972603	One potential reason for this difference is the composition of the PETE dataset which emphasizes challenging syntactic constructions that some parsers may be better at.	0
5403	5403	S10-12	Baselines	12	132	1.0	0.904109589041096	Another reason is the complete indifference of treebank based measures like UAS to the semantic significance of various dependencies and their impact on potential applications.	0
5404	5404	S10-12	System	1	133	0.125	0.910958904109589	Systems	0
5405	5405	S10-12	System	2	134	0.25	0.917808219178082	There were 20 systems from 7 teams participating in the PETE task.	0
5406	5406	S10-12	System	3	135	0.375	0.924657534246575	(Nivre et al., 2007b), MSTParser (McDonald et al., 2005 and Stanford Parser (Klein and Manning, 2003).	0
5407	5407	S10-12	System	4	136	0.5	0.931506849315068	After the parsing step, the decision for the entailment was based on the comparison of relations, predicates, or dependency paths between the text and the hypothesis.	0
5408	5408	S10-12	System	5	137	0.625	0.938356164383562	Most systems relied on heuristic methods of comparison.	0
5409	5409	S10-12	System	6	138	0.75	0.945205479452055	A notable exception is the MARS-3 system which used an SVM-based classifier to decide on the entailment using dependency path features.	0
5410	5410	S10-12	System	7	139	0.875	0.952054794520548	Table 4 lists the frequency of various grammatical relations in the instances where the top system made mistakes.	0
5411	5411	S10-12	System	8	140	1.0	0.958904109589041	A comparison with Table 1 shows the direct objects and reduced relative clauses to be the frequent causes of error.	0
5412	5412	S10-12	Contributions	1	141	0.166666666666667	0.965753424657534	We introduced PETE, a new method for parser evaluation using textual entailments.	0
5413	5413	S10-12	Contributions	2	142	0.333333333333333	0.972602739726027	By basing the entailments on dependencies that current state  of the art parsers disagree on, we hoped to create a dataset that would focus attention on the long tail of parsing problems that do not get sufficient attention using common evaluation metrics.	0
5414	5414	S10-12	Contributions	3	143	0.5	0.979452054794521	By further restricting ourselves to differences that can be expressed by natural language entailments, we hoped to focus on semantically relevant decisions rather than accidents of convention which get mixed up in common evaluation metrics.	0
5415	5415	S10-12	Contributions	4	144	0.666666666666667	0.986301369863014	We chose to rely on untrained annotators on a natural inference task rather than trained annotators on an artificial tagging task because we believe (i) many subfields of computational linguistics are struggling to make progress because of the noise in artificially tagged data, and (ii) systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
5416	5416	S10-12	Contributions	5	145	0.833333333333333	0.993150684931507	Our hope is datasets like PETE will be used not only for evaluation but also for training and fine-tuning of systems in the future.	0
5417	5417	S10-12	Contributions	6	146	1.0	1.0	Further work is needed to automate the entailment generation process and to balance the composition of syntactic phenomena covered in a PETE dataset.	0
5809	5809	S10-18	title	1	1	1.0	0.006289308176101	SemEval-2010 Task 18: Disambiguating Sentiment Ambiguous Adjectives	0
5810	5810	S10-18	abstract	1	2	0.25	0.012578616352201	Sentiment ambiguous adjectives cause major difficulties for existing algorithms of sentiment analysis.	0
5811	5811	S10-18	abstract	2	3	0.5	0.018867924528302	We present an evaluation task designed to provide a framework for comparing different approaches in this problem.	0
5812	5812	S10-18	abstract	3	4	0.75	0.025157232704403	We define the task, describe the data creation, list the participating systems and discuss their results.	0
5813	5813	S10-18	abstract	4	5	1.0	0.031446540880503	There are 8 teams and 16 systems.	0
5814	5814	S10-18	Introduction	1	6	0.04	0.037735849056604	In recent years, sentiment analysis has attracted considerable attention (Pang and Lee, 2008).	0
5815	5815	S10-18	Introduction	2	7	0.08	0.044025157232704	It is the task of mining positive and negative opinions from natural language, which can be applied to many natural language processing tasks, such as document summarization and question answering.	0
5816	5816	S10-18	Introduction	3	8	0.12	0.050314465408805	Previous work on this problem falls into three groups: opinion mining of documents, sentiment classification of sentences and polarity prediction of words.	0
5817	5817	S10-18	Introduction	4	9	0.16	0.056603773584906	Sentiment analysis both at document and sentence level rely heavily on word level.	0
5818	5818	S10-18	Introduction	5	10	0.2	0.062893081761006	The most frequently explored task at word level is to determine the semantic orientation (SO) of words, in which most work centers on assigning a prior polarity to words or word senses in the lexicon out of context.	0
5819	5819	S10-18	Introduction	6	11	0.24	0.069182389937107	However, for some words, the polarity varies strongly with context, making it hard to attach each to a specific sentiment category in the lexicon.	0
5820	5820	S10-18	Introduction	7	12	0.28	0.075471698113208	"For example, consider "" low cost"" versus "" low salary"" ."	0
5821	5821	S10-18	Introduction	8	13	0.32	0.081761006289308	"The word "" low"" has a positive orientation in the first case but a negative orientation in the second case."	0
5822	5822	S10-18	Introduction	9	14	0.36	0.088050314465409	Turney and Littman (2003) claimed that sentiment ambiguous words could not be avoided easily in a real-world application in the future research.	0
5823	5823	S10-18	Introduction	10	15	0.4	0.09433962264151	But unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (Hatzivassiloglou and McKeown, 1997;	0
5824	5824	S10-18	Introduction	11	16	0.44	0.10062893081761	Turney and Littman, 2003;Kim and Hovy, 2004).	0
5825	5825	S10-18	Introduction	12	17	0.48	0.106918238993711	The exception work is Ding et al. (2008).	0
5826	5826	S10-18	Introduction	13	18	0.52	0.113207547169811	They call these words as context dependant opinions and propose a holistic lexicon-based approach to solve this problem.	0
5827	5827	S10-18	Introduction	14	19	0.56	0.119496855345912	The language they deal with is English.	0
5828	5828	S10-18	Introduction	15	20	0.6	0.125786163522013	The disambiguation of sentiment ambiguous words can also be considered as a problem of phrase-level sentiment analysis.	0
5829	5829	S10-18	Introduction	16	21	0.64	0.132075471698113	Wilson et al. (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features.	0
5830	5830	S10-18	Introduction	17	22	0.68	0.138364779874214	Takamura et al. (2006	0
5831	5831	S10-18	Introduction	18	23	0.72	0.144654088050314	"Takamura et al. ( , 2007 propose latent variable model and lexical network to determine SO of phrases, focusing on "" noun+adjective"" pairs."	0
5832	5832	S10-18	Introduction	19	24	0.76	0.150943396226415	Their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.	0
5833	5833	S10-18	Introduction	20	25	0.8	0.157232704402516	The task 18 at SemEval 2010 provides a benchmark data set to encourage studies on this problem.	0
5834	5834	S10-18	Introduction	21	26	0.84	0.163522012578616	This paper is organized as follows.	0
5835	5835	S10-18	Introduction	22	27	0.88	0.169811320754717	Section 2 defines the task.	0
5836	5836	S10-18	Introduction	23	28	0.92	0.176100628930818	Section 3 describes the data annotation.	0
5837	5837	S10-18	Introduction	24	29	0.96	0.182389937106918	Section 4 gives a brief summary of 16 participating systems.	0
5838	5838	S10-18	Introduction	25	30	1.0	0.188679245283019	Finally Section 5 draws conclusions.	0
5839	5839	S10-18	Task Set up 2.1 Task description	1	31	0.125	0.19496855345912	In this task, we focus on 14 frequently used sentiment ambiguous adjectives in Chinese, which all have the meaning of measurement, as shown below.	0
5840	5840	S10-18	Task Set up 2.1 Task description	2	32	0.25	0.20125786163522	"(1) Sentiment ambiguous adjectives(SAAs) ={ 大 da "" large"" , 多 duo "" many"" , 高 gao "" high"" , 厚 hou "" thick"" , 深 shen "" deep"" , 重 zhong "" heavy"" , 巨大 ju-da "" huge"" , 重大 zhong-da "" great"" , 小 xiao "" small"" , 少 shao "" few"" , 低 di "" low"" , 薄 bao "" thin"" , 浅 qian "" shallow"" , 轻 qing "" light"" }"	0
5841	5841	S10-18	Task Set up 2.1 Task description	3	33	0.375	0.207547169811321	These adjectives are neutral out of context, but when they co-occur with some target nouns, positive or negative emotion will be evoked.	0
5842	5842	S10-18	Task Set up 2.1 Task description	4	34	0.5	0.213836477987421	Although the number of such ambiguous adjectives is not large, they are frequently used in real text, especially in the texts expressing opinions and emotions.	0
5843	5843	S10-18	Task Set up 2.1 Task description	5	35	0.625	0.220125786163522	The task is designed to automatically determine the SO of these sentiment ambiguous adjectives within context: positive or negative.	1
5844	5844	S10-18	Task Set up 2.1 Task description	6	36	0.75	0.226415094339623	"For example, 高 gao "" high""should be assigned as positive in 工 资 高 gong-zi -gao "" salary is high""but negative in 价格高 jia-ge-gao "" price is high"" ."	0
5845	5845	S10-18	Task Set up 2.1 Task description	7	37	0.875	0.232704402515723	This task was carried out in an unsupervised setting.	0
5846	5846	S10-18	Task Set up 2.1 Task description	8	38	1.0	0.238993710691824	No training data was provided, but external resources are encouraged to use.	0
5847	5847	S10-18	Data Creation	1	39	0.055555555555556	0.245283018867925	We collected data from two sources.	0
5848	5848	S10-18	Data Creation	2	40	0.111111111111111	0.251572327044025	The main part was extracted from Xinhua News Agency of Chinese Gigaword (Second Edition) released by LDC.	0
5849	5849	S10-18	Data Creation	3	41	0.166666666666667	0.257861635220126	The texts were automatically wordsegmented and POS-tagged using the open software ICTCLAS 1 .	0
5850	5850	S10-18	Data Creation	4	42	0.222222222222222	0.264150943396226	In order to concentrate on the disambiguation of sentiment ambiguous adjectives, and reduce the noise introduced by the parser, we extracted sentences containing strings in pattern of (2), where the target nouns are modified by the adjectives in most cases.	0
5851	5851	S10-18	Data Creation	5	43	0.277777777777778	0.270440251572327	"(2) noun+adverb+adjective (adjective∈SAAs) e.g. 成本/n 较/d 低/a cheng-ben-jiao-di "" the cost is low."""	0
5852	5852	S10-18	Data Creation	6	44	0.333333333333333	0.276729559748428	Another small part of data was extracted from the Web.	0
5853	5853	S10-18	Data Creation	7	45	0.388888888888889	0.283018867924528	Using the search engine Google 2 , we searched the queries as in ( 3):	0
5854	5854	S10-18	Data Creation	8	46	0.444444444444444	0.289308176100629	"(3) 很 hen "" very"" + adjective (adjective∈"	0
5855	5855	S10-18	Data Creation	9	47	0.5	0.29559748427673	SAAs )	0
5856	5856	S10-18	Data Creation	10	48	0.555555555555556	0.30188679245283	From the returned snippets, we manually picked out some sentences that contain the strings of (2).	0
5857	5857	S10-18	Data Creation	11	49	0.611111111111111	0.308176100628931	Also, the sentences were automatically segmented and POS-tagged using ICTCLAS.	0
5858	5858	S10-18	Data Creation	12	50	0.666666666666667	0.314465408805031	Sentiment ambiguous adjectives in the data were assigned as positive, negative or neutral, independently by two annotators.	0
5859	5859	S10-18	Data Creation	13	51	0.722222222222222	0.320754716981132	Since we focus on the distinction between positive and negative categories, the neutral instances were removed.	0
5860	5860	S10-18	Data Creation	14	52	0.777777777777778	0.327044025157233	The inter-annotator agreement is in a high level with a kappa of 0.91.	0
5861	5861	S10-18	Data Creation	15	53	0.833333333333333	0.333333333333333	After cases with disagreement were negotiated between the two annotators, a gold standard annotation was agreed upon.	0
5862	5862	S10-18	Data Creation	16	54	0.888888888888889	0.339622641509434	In total 2917 instances were provided as the test data in the task, and the number of sentences of per target adjective is listed in Table 2.	0
5863	5863	S10-18	Data Creation	17	55	0.944444444444444	0.345911949685535	Evaluation was performed in micro accuracy and macro accuracy:	0
5864	5864	S10-18	Data Creation	18	56	1.0	0.352201257861635	where N is the number of all target words, i n is the number of all test instances for a specific word, and i m is the number of correctly labeled instances.	0
5865	5865	S10-18	Baseline	1	57	0.125	0.358490566037736	We group 14 sentiment ambiguous adjectives into two categories: positive-like adjectives and negative-like adjectives.	0
5866	5866	S10-18	Baseline	2	58	0.25	0.364779874213836	The former has the connotation towards large measurement, whereas the latter towards small measurement.	0
5867	5867	S10-18	Baseline	3	59	0.375	0.371069182389937	"(4) Positive-like adjectives (Pa) ={大 da "" large"" , 多 duo "" many"" , 高 gao "" high"" , 厚 hou "" thick"" , 深 shen "" deep"" , 重 zhong "" heavy"" , 巨大 ju-da "" huge"" , 重大 zhong-da "" great"" }"	0
5868	5868	S10-18	Baseline	4	60	0.5	0.377358490566038	"(5) Negative-like adjectives (Na) ={ 小 xiao "" small"" , 少 shao "" few"" , 低 di "" low"" , 薄 bao "" thin"" , 浅 qian "" shallow"" , 轻 qing "" light"" }"	0
5869	5869	S10-18	Baseline	5	61	0.625	0.383647798742138	We conduct a baseline in the dataset.	0
5870	5870	S10-18	Baseline	6	62	0.75	0.389937106918239	Not considering the context, assign all positive-like adjectives as positive and all negative-like adjectives as negative.	0
5871	5871	S10-18	Baseline	7	63	0.875	0.39622641509434	The micro accuracy of the baseline is 61.20%.	0
5872	5872	S10-18	Baseline	8	64	1.0	0.40251572327044	The inter-annotator agreement of 0.91 can be considered as the upper bound of the dataset.	0
5873	5873	S10-18	Systems and Results	1	65	0.333333333333333	0.408805031446541	We published firstly trial data and then test data.	0
5874	5874	S10-18	Systems and Results	2	66	0.666666666666667	0.415094339622642	In total 11 different teams downloaded both the trial and test data.	0
5875	5875	S10-18	Systems and Results	3	67	1.0	0.421383647798742	Finally 8 teams submitted their experimental results, including 16 systems.	0
5876	5876	S10-18	Results	1	68	0.111111111111111	0.427672955974843	Table 1 lists all systems'scores, ranked from best to worst performance measured by micro accuracy.	0
5877	5877	S10-18	Results	2	69	0.222222222222222	0.433962264150943	To our surprise, the performance of different systems differs greatly.	0
5878	5878	S10-18	Results	3	70	0.333333333333333	0.440251572327044	The micro accuracy of the best system is 94.20% that is 43.12% higher than the worst system.	0
5879	5879	S10-18	Results	4	71	0.444444444444444	0.446540880503145	The accuracy of the best three systems is even higher than inter-annotator agreement.	0
5880	5880	S10-18	Results	5	72	0.555555555555556	0.452830188679245	The performance of the worst system is only a little higher than random baseline, which is 50% when we randomly assign the SO of sentiment ambiguous adjectives.	0
5881	5881	S10-18	Results	6	73	0.666666666666667	0.459119496855346	Table 1: The scores of 16 systems	0
5882	5882	S10-18	Results	7	74	0.777777777777778	0.465408805031447	Table 2 shows that the performance of different systems differs greatly on each of 14 target adjectives.	0
5883	5883	S10-18	Results	8	75	0.888888888888889	0.471698113207547	"For example, the accuracy of 大 da "" large""is 95.53% by one system but only 46.51% by another system."	0
5884	5884	S10-18	Results	9	76	1.0	0.477987421383648	Table 2: The scores of 14 ambiguous adjectives	0
5885	5885	S10-18	Systems	1	77	0.016949152542373	0.484276729559748	In this section, we give a brief description of the systems.	0
5886	5886	S10-18	Systems	2	78	0.033898305084746	0.490566037735849	YSC-DSAA	0
5887	5887	S10-18	Systems	3	79	0.050847457627119	0.49685534591195	This system creates a new word library named SAAOL (SAA-Oriented Library), which is built manually with the help of software.	0
5888	5888	S10-18	Systems	4	80	0.067796610169492	0.50314465408805	SAAOL consists of positive words, negative words, NSSA, PSSA, and inverse words.	0
5889	5889	S10-18	Systems	5	81	0.084745762711864	0.509433962264151	The system divides the sentences into clauses using heuristic rules, and disambiguates SAA by analyzing the relationship between SAA and the keywords.	0
5890	5890	S10-18	Systems	6	82	0.101694915254237	0.515723270440252	HITSZ_CITYU	0
5891	5891	S10-18	Systems	7	83	0.11864406779661	0.522012578616352	This group submitted three systems, including one baseline system and two improved systems.	0
5892	5892	S10-18	Systems	8	84	0.135593220338983	0.528301886792453	HITSZ_CITYU_3: The baseline system is based on collocation of opinion words and their targets.	0
5893	5893	S10-18	Systems	9	85	0.152542372881356	0.534591194968553	For the given adjectives, their collocations are extracted from People' s Daily Corpus.	0
5894	5894	S10-18	Systems	10	86	0.169491525423729	0.540880503144654	With human annotation, the system obtained 412 positive and 191 negative collocations, which are regarded as seed collocations.	0
5895	5895	S10-18	Systems	11	87	0.186440677966102	0.547169811320755	Using the context words of seed collocations as features, the system trains a oneclass SVM classifier.	0
5896	5896	S10-18	Systems	12	88	0.203389830508475	0.553459119496855	HITSZ_CITYU_2 and HITSZ_CITYU_1: Using HowNet-based word similarity as clue, the authors expand the seed collocations on both ambiguous adjectives side and collocated targets side.	0
5897	5897	S10-18	Systems	13	89	0.220338983050847	0.559748427672956	The authors then exploit sentence-level opinion analysis to further improve performance.	0
5898	5898	S10-18	Systems	14	90	0.23728813559322	0.566037735849057	The strategy is that if the neighboring sentences on both sides have the same polarity, the ambiguous adjective is assigned as the same polarity; if the neighboring sentences have conflicted polarity, the SO of ambiguous adjective is determined by its context words and the transitive probability of sentence polarity.	0
5899	5899	S10-18	Systems	15	91	0.254237288135593	0.572327044025157	The two systems use different parameters and combination strategy.	0
5900	5900	S10-18	Systems	16	92	0.271186440677966	0.578616352201258	OpAL	0
5901	5901	S10-18	Systems	17	93	0.288135593220339	0.584905660377358	This system combines supervised methods with unsupervised ones.	0
5902	5902	S10-18	Systems	18	94	0.305084745762712	0.591194968553459	The authors employ Google translator to translate the task dataset from Chinese to English, since their system is working in English.	0
5903	5903	S10-18	Systems	19	95	0.322033898305085	0.59748427672956	The system explores three types of judgments.	0
5904	5904	S10-18	Systems	20	96	0.338983050847458	0.60377358490566	The first one trains a SVM classifier based on NTCIR data and EmotiBlog annotations.	0
5905	5905	S10-18	Systems	21	97	0.35593220338983	0.610062893081761	"The second one uses search engine, issuing queries of "" noun + SAA + AND + non-ambiguous adjective""."	0
5906	5906	S10-18	Systems	22	98	0.372881355932203	0.616352201257862	"The nonambiguous adjectives include positive set ("" positive, beautiful, good"" ) and negative set ("" negative, ugly, bad"" )."	0
5907	5907	S10-18	Systems	23	99	0.389830508474576	0.622641509433962	"An example is "" price high and good"" ."	0
5908	5908	S10-18	Systems	24	100	0.406779661016949	0.628930817610063	"The third one uses "" too, very- rules"" ."	0
5909	5909	S10-18	Systems	25	101	0.423728813559322	0.635220125786163	The final result is determined by the majority vote of the three components.	0
5910	5910	S10-18	Systems	26	102	0.440677966101695	0.641509433962264	CityUHK	0
5911	5911	S10-18	Systems	27	103	0.457627118644068	0.647798742138365	This group submitted four systems.	0
5912	5912	S10-18	Systems	28	104	0.474576271186441	0.654088050314465	Both machine learning method and lexiconbased method are employed in their systems.	0
5913	5913	S10-18	Systems	29	105	0.491525423728814	0.660377358490566	In the machine learning method, maximum entropy model is used to train a classifier based on the Chinese data from NTCIR opinion task.	0
5914	5914	S10-18	Systems	30	106	0.508474576271186	0.666666666666667	Clauselevel and sentence-level classifiers are compared.	0
5915	5915	S10-18	Systems	31	107	0.525423728813559	0.672955974842767	In the lexicon-based method, the authors classify SAAs into two clusters: intensifiers (our positive-like adjectives in ( 4)) and suppressors (our negative-like adjectives in ( 5)), and then use the polarity of context to determine the SO of SAAs.	0
5916	5916	S10-18	Systems	32	108	0.542372881355932	0.679245283018868	City	0
5917	5917	S10-18	Systems	33	109	0.559322033898305	0.685534591194968	UHK4: clause-level machine learning + lexicon.	0
5918	5918	S10-18	Systems	34	110	0.576271186440678	0.691823899371069	City	0
5919	5919	S10-18	Systems	35	111	0.593220338983051	0.69811320754717	UHK3: sentence-level machine learning + lexicon.	0
5920	5920	S10-18	Systems	36	112	0.610169491525424	0.70440251572327	City	0
5921	5921	S10-18	Systems	37	113	0.627118644067797	0.710691823899371	UHK2: clause-level machine learning.	0
5922	5922	S10-18	Systems	38	114	0.644067796610169	0.716981132075472	City	0
5923	5923	S10-18	Systems	39	115	0.661016949152542	0.723270440251572	UHK2: sentence-level machine learning.	0
5924	5924	S10-18	Systems	40	116	0.677966101694915	0.729559748427673	QLK_DSAA	0
5925	5925	S10-18	Systems	41	117	0.694915254237288	0.735849056603773	This group submitted two systems.	0
5926	5926	S10-18	Systems	42	118	0.711864406779661	0.742138364779874	The authors adopt their SELC model (Qiu, et al., 2009), which is proposed to exploit the complementarities between lexicon-based and corpus-based methods to improve the whole performance.	0
5927	5927	S10-18	Systems	43	119	0.728813559322034	0.748427672955975	They determine the sentence polarity by SELC model, and simply regard the sentence polarity as the polarity of SAA in the sentence.	0
5928	5928	S10-18	Systems	44	120	0.745762711864407	0.754716981132076	QLK_DSAA_NR: Based on the result of SELC model, they inverse the SO of SAA when it is modified by negative terms.	0
5929	5929	S10-18	Systems	45	121	0.76271186440678	0.761006289308176	Our task includes only positive and negative categories, so they replace the neutral value obtained by SELC model by the predominant polarity of the adjective.	0
5930	5930	S10-18	Systems	46	122	0.779661016949153	0.767295597484277	"QLK_DSAA_R: Based on the result of QLK_DSAA_NR, they add a rule to cope with two modifiers 偏 pian "" specially"" and 太 tai "" too"" , which always have the negative meaning."	0
5931	5931	S10-18	Systems	47	123	0.796610169491525	0.773584905660377	Twitter sentiment	0
5932	5932	S10-18	Systems	48	124	0.813559322033898	0.779874213836478	This group submitted three systems.	0
5933	5933	S10-18	Systems	49	125	0.830508474576271	0.786163522012579	The authors use a training data collected from microblogging platform.	0
5934	5934	S10-18	Systems	50	126	0.847457627118644	0.792452830188679	By exploiting Twitter, they collected automatically a dataset consisting of negative and positive expressions.	0
5935	5935	S10-18	Systems	51	127	0.864406779661017	0.79874213836478	The sentiment classifier is trained using Naive Bayes with n-grams of words as features.	0
5936	5936	S10-18	Systems	52	128	0.88135593220339	0.805031446540881	Twitter Sentiment: Translating the task dataset from Chinese to English using Google translator, and then based on training data in English texts from Twitter.	0
5937	5937	S10-18	Systems	53	129	0.898305084745763	0.811320754716981	Twitter Sentiment_ext: With Twitter Sentiment as basis, using extended data.	0
5938	5938	S10-18	Systems	54	130	0.915254237288135	0.817610062893082	Twitter Sentiment_zh: Based on training data in Chinese texts from Twitter.	0
5939	5939	S10-18	Systems	55	131	0.932203389830508	0.823899371069182	Biparty	0
5940	5940	S10-18	Systems	56	132	0.949152542372881	0.830188679245283	This system transforms the problem of disambiguating SAAs to predict the polarity of target nouns.	0
5941	5941	S10-18	Systems	57	133	0.966101694915254	0.836477987421384	The system presents a bootstrapping method to automatically build the sentiment lexicon, by building a nouns-verbs biparty graph from a large corpus.	0
5942	5942	S10-18	Systems	58	134	0.983050847457627	0.842767295597484	Firstly they select a few nouns as seed words, and then they use a cross inducing method to expand more nouns and verbs into the lexicon.	0
5943	5943	S10-18	Systems	59	135	1.0	0.849056603773585	The strategy is based on a random walk model.	0
5944	5944	S10-18	Discussion	1	136	0.047619047619048	0.855345911949686	The experimental results of some systems are promising.	0
5945	5945	S10-18	Discussion	2	137	0.095238095238095	0.861635220125786	The micro accuracy of the best three systems is over 93%.	0
5946	5946	S10-18	Discussion	3	138	0.142857142857143	0.867924528301887	Therefore, the interannotator agreement (91%) is not an upper bound on the accuracy that can be achieved.	0
5947	5947	S10-18	Discussion	4	139	0.19047619047619	0.874213836477987	On the contrary, the experimental results of some systems are disappointing, which are below our predefined simple baseline (61.20%), and are only a little higher than random baseline (50%).	0
5948	5948	S10-18	Discussion	5	140	0.238095238095238	0.880503144654088	The accuracy variance of different systems makes this task more interesting.	0
5949	5949	S10-18	Discussion	6	141	0.285714285714286	0.886792452830189	The participating 8 teams exploit totally different methods.	0
5950	5950	S10-18	Discussion	7	142	0.333333333333333	0.893081761006289	Human annotation.	0
5951	5951	S10-18	Discussion	8	143	0.380952380952381	0.89937106918239	In YSC-DSAA system, the word library of SAAOL is verified by human.	0
5952	5952	S10-18	Discussion	9	144	0.428571428571429	0.905660377358491	In HITSZ_CITYU systems, the seed collocations are annotated by human.	0
5953	5953	S10-18	Discussion	10	145	0.476190476190476	0.911949685534591	The three systems rank top 3.	0
5954	5954	S10-18	Discussion	11	146	0.523809523809524	0.918238993710692	Undoubtedly, human labor can help improve the performance in this task.	0
5955	5955	S10-18	Discussion	12	147	0.571428571428571	0.924528301886793	Training data.	0
5956	5956	S10-18	Discussion	13	148	0.619047619047619	0.930817610062893	The OpAL system employs SVM machine learning based on NTCIR data and EmotiBlog annotations.	0
5957	5957	S10-18	Discussion	14	149	0.666666666666667	0.937106918238994	The CityUHK systems trains a maximum entropy classifier based on the annotated Chinese data from NTCIR.	0
5958	5958	S10-18	Discussion	15	150	0.714285714285714	0.943396226415094	The Twitter Sentiment systems use a training data automatically collected from Twitter.	0
5959	5959	S10-18	Discussion	16	151	0.761904761904762	0.949685534591195	The results show that some of these supervised methods based on training data cannot rival unsupervised ones, partly due to the poor quality of the training data.	0
5960	5960	S10-18	Discussion	17	152	0.80952380952381	0.955974842767296	English resources.	0
5961	5961	S10-18	Discussion	18	153	0.857142857142857	0.962264150943396	Our task is in Chinese.	0
5962	5962	S10-18	Discussion	19	154	0.904761904761905	0.968553459119497	Some systems use English resources by translating Chinese into English, as OpAL and Twitter Sentiment.	0
5963	5963	S10-18	Discussion	20	155	0.952380952380952	0.974842767295598	The OpAL system achieves a quite good result, making this method a promising direction.	0
5964	5964	S10-18	Discussion	21	156	1.0	0.981132075471698	This also shows that disambiguating SAAs is a common problem in natural language.	0
5965	5965	S10-18	Conclusion	1	157	0.333333333333333	0.987421383647799	This paper describes task 18 at SemEval-2010, disambiguating sentiment ambiguous adjectives.	0
5966	5966	S10-18	Conclusion	2	158	0.666666666666667	0.993710691823899	The experimental results of the 16 participating systems are promising, and the used approaches are quite novel.	0
5967	5967	S10-18	Conclusion	3	159	1.0	1.0	We encourage further research into this issue, and integration of the disambiguation of sentiment ambiguous adjectives into applications of sentiment analysis.	0
6979	6979	S12-6	title	1	1	1.0	0.005128205128205	SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity	0
6980	6980	S12-6	abstract	1	2	0.125	0.01025641025641	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts.	0
6981	6981	S12-6	abstract	2	3	0.25	0.015384615384615	This paper presents the results of the STS pilot task in Semeval.	0
6982	6982	S12-6	abstract	3	4	0.375	0.020512820512821	The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources.	0
6983	6983	S12-6	abstract	4	5	0.5	0.025641025641026	The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise.	0
6984	6984	S12-6	abstract	5	6	0.625	0.030769230769231	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%.	0
6985	6985	S12-6	abstract	6	7	0.75	0.035897435897436	35 teams participated in the task, submitting 88 runs.	0
6986	6986	S12-6	abstract	7	8	0.875	0.041025641025641	The best results scored a Pearson correlation &gt;80%, well above a simple lexical baseline that only scored a 31% correlation.	0
6987	6987	S12-6	abstract	8	9	1.0	0.046153846153846	This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.	0
6988	6988	S12-6	Introduction	1	10	0.055555555555556	0.051282051282051	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two sentences.	0
6989	6989	S12-6	Introduction	2	11	0.111111111111111	0.056410256410257	STS is related to both Textual Entailment (TE) and Paraphrase (PARA).	0
6990	6990	S12-6	Introduction	3	12	0.166666666666667	0.061538461538462	STS is more directly applicable in a number of NLP tasks than TE and PARA such as Machine Translation and evaluation, Summarization, Machine Reading, Deep Question Answering, etc. STS differs from TE in as much as it assumes symmetric graded equivalence between the pair of textual snippets.	0
6991	6991	S12-6	Introduction	4	13	0.222222222222222	0.066666666666667	In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.	0
6992	6992	S12-6	Introduction	5	14	0.277777777777778	0.071794871794872	Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).	0
6993	6993	S12-6	Introduction	6	15	0.333333333333333	0.076923076923077	STS provides a unified framework that allows for an extrinsic evaluation of multiple semantic components that otherwise have tended to be evaluated independently and without broad characterization of their impact on NLP applications.	0
6994	6994	S12-6	Introduction	7	16	0.388888888888889	0.082051282051282	Such components include word sense disambiguation and induction, lexical substitution, semantic role labeling, multiword expression detection and handling, anaphora and coreference resolution, time and date resolution, named-entity handling, underspecification, hedging, semantic scoping and discourse analysis.	0
6995	6995	S12-6	Introduction	8	17	0.444444444444444	0.087179487179487	Though not in the scope of the current pilot task, we plan to explore building an open source toolkit for integrating and applying diverse linguistic analysis modules to the STS task.	0
6996	6996	S12-6	Introduction	9	18	0.5	0.092307692307692	While the characterization of STS is still preliminary, we observed that there was no comparable existing dataset extensively annotated for pairwise semantic sentence similarity.	0
6997	6997	S12-6	Introduction	10	19	0.555555555555556	0.097435897435898	We approached the construction of the first STS dataset with the following goals: (1)	0
6998	6998	S12-6	Introduction	11	20	0.611111111111111	0.102564102564103	To set a definition of STS as a graded notion which can be easily communicated to non-expert annotators beyond the likert-scale; (2)	0
6999	6999	S12-6	Introduction	12	21	0.666666666666667	0.107692307692308	To gather a substantial amount of sentence pairs from diverse datasets, and to annotate them with high quality; (3)	0
7000	7000	S12-6	Introduction	13	22	0.722222222222222	0.112820512820513	To explore evaluation measures for STS; (4)	0
7001	7001	S12-6	Introduction	14	23	0.777777777777778	0.117948717948718	To explore the relation of STS to PARA and Machine Translation Evaluation exercises.	0
7002	7002	S12-6	Introduction	15	24	0.833333333333333	0.123076923076923	In the next section we present the various sources of the STS data and the annotation procedure used.	0
7003	7003	S12-6	Introduction	16	25	0.888888888888889	0.128205128205128	Section 4 investigates the evaluation of STS systems.	0
7004	7004	S12-6	Introduction	17	26	0.944444444444444	0.133333333333333	Section 5 summarizes the resources and tools used by participant systems.	0
7005	7005	S12-6	Introduction	18	27	1.0	0.138461538461538	Finally, Section 6 draws the conclusions.	0
7006	7006	S12-6	Source Datasets	1	28	0.016129032258065	0.143589743589744	Datasets for STS are scarce.	0
7007	7007	S12-6	Source Datasets	2	29	0.032258064516129	0.148717948717949	Existing datasets include (Li et al., 2006) and (Lee et al., 2005).	0
7008	7008	S12-6	Source Datasets	3	30	0.048387096774194	0.153846153846154	The first dataset includes 65 sentence pairs which correspond to the dictionary definitions for the 65 word pairs in Similarity (Rubenstein and Goodenough, 1965).	0
7009	7009	S12-6	Source Datasets	4	31	0.064516129032258	0.158974358974359	The authors asked human informants to assess the meaning of the sentence pairs on a scale from 0.0 (minimum similarity) to 4.0 (maximum similarity).	0
7010	7010	S12-6	Source Datasets	5	32	0.080645161290323	0.164102564102564	While the dataset is very relevant to STS, it is too small to train, develop and test typical machine learning based systems.	0
7011	7011	S12-6	Source Datasets	6	33	0.096774193548387	0.169230769230769	The second dataset comprises 50 documents on news, ranging from 51 to 126 words.	0
7012	7012	S12-6	Source Datasets	7	34	0.112903225806452	0.174358974358974	"Subjects were asked to judge the similarity of document pairs on a five-point scale (with 1.0 indicating ""highly unrelated"" and 5.0 indicating ""highly related"")."	0
7013	7013	S12-6	Source Datasets	8	35	0.129032258064516	0.179487179487179	This second dataset comprises a larger number of document pairs, but it goes beyond sentence similarity into textual similarity.	0
7014	7014	S12-6	Source Datasets	9	36	0.145161290322581	0.184615384615385	When constructing our datasets, gathering naturally occurring pairs of sentences with different degrees of semantic equivalence was a challenge in itself.	0
7015	7015	S12-6	Source Datasets	10	37	0.161290322580645	0.18974358974359	If we took pairs of sentences at random, the vast majority of them would be totally unrelated, and only a very small fragment would show some sort of semantic equivalence.	0
7016	7016	S12-6	Source Datasets	11	38	0.17741935483871	0.194871794871795	Accordingly, we investigated reusing a collection of existing datasets from tasks that are related to STS.	0
7017	7017	S12-6	Source Datasets	12	39	0.193548387096774	0.2	We first studied the pairs of text from the Recognizing TE challenge.	0
7018	7018	S12-6	Source Datasets	13	40	0.209677419354839	0.205128205128205	The first editions of the challenge included pairs of sentences as the following:	0
7019	7019	S12-6	Source Datasets	14	41	0.225806451612903	0.21025641025641	The first sentence is the text, and the second is the hypothesis.	0
7020	7020	S12-6	Source Datasets	15	42	0.241935483870968	0.215384615384615	The organizers of the challenge annotated several pairs with a binary tag, indicating whether the hypothesis could be entailed from the text.	0
7021	7021	S12-6	Source Datasets	16	43	0.258064516129032	0.22051282051282	Although these pairs of text are interesting we decided to discard them from this pilot because the length of the hypothesis was typically much shorter than the text, and we did not want to bias the STS task in this respect.	0
7022	7022	S12-6	Source Datasets	17	44	0.274193548387097	0.225641025641026	We may, however, explore using TE pairs for STS in the future.	0
7023	7023	S12-6	Source Datasets	18	45	0.290322580645161	0.230769230769231	Microsoft Research (MSR) has pioneered the acquisition of paraphrases with two manually annotated datasets.	0
7024	7024	S12-6	Source Datasets	19	46	0.306451612903226	0.235897435897436	The first, called MSR Paraphrase (MSRpar for short) has been widely used to evaluate text similarity algorithms.	0
7025	7025	S12-6	Source Datasets	20	47	0.32258064516129	0.241025641025641	It contains 5801 pairs of sentences gleaned over a period of 18 months from thousands of news sources on the web (Dolan et al., 2004).	0
7026	7026	S12-6	Source Datasets	21	48	0.338709677419355	0.246153846153846	67% of the pairs were tagged as paraphrases.	0
7027	7027	S12-6	Source Datasets	22	49	0.354838709677419	0.251282051282051	The inter annotator agreement is between 82% and 84%.	0
7028	7028	S12-6	Source Datasets	23	50	0.370967741935484	0.256410256410256	Complete meaning equivalence is not required, and the annotation guidelines allowed for some relaxation.	0
7029	7029	S12-6	Source Datasets	24	51	0.387096774193548	0.261538461538462	The pairs which were annotated as not being paraphrases ranged from completely unrelated semantically, to partially overlapping, to those that were almost-but-not-quite semantically equivalent.	0
7030	7030	S12-6	Source Datasets	25	52	0.403225806451613	0.266666666666667	In this sense our graded annotations enrich the dataset with more nuanced tags, as we will see in the following section.	0
7031	7031	S12-6	Source Datasets	26	53	0.419354838709677	0.271794871794872	We followed the original split of 70% for training and 30% for testing.	0
7032	7032	S12-6	Source Datasets	27	54	0.435483870967742	0.276923076923077	A sample pair from the dataset follows:	0
7033	7033	S12-6	Source Datasets	28	55	0.451612903225806	0.282051282051282	The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq.	0
7034	7034	S12-6	Source Datasets	29	56	0.467741935483871	0.287179487179487	American intelligence leading up to the war on Iraq will be criticized by a powerful US Congressional committee due to report soon, officials said today.	0
7035	7035	S12-6	Source Datasets	30	57	0.483870967741936	0.292307692307692	In order to construct a dataset which would reflect a uniform distribution of similarity ranges, we sampled the MSRpar dataset at certain ranks of string similarity.	0
7036	7036	S12-6	Source Datasets	31	58	0.5	0.297435897435897	We used the implementation readily accessible at CPAN 1 of a well-known metric (Ukkonen, 1985).	0
7037	7037	S12-6	Source Datasets	32	59	0.516129032258065	0.302564102564102	We sampled equal numbers of pairs from five bands of similarity in the [0.4 .. 0.8] range separately from the paraphrase and non-paraphrase pairs.	0
7038	7038	S12-6	Source Datasets	33	60	0.532258064516129	0.307692307692308	We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.	0
7039	7039	S12-6	Source Datasets	34	61	0.548387096774194	0.312820512820513	The second dataset from MSR is the MSR Video Paraphrase Corpus (MSRvid for short).	0
7040	7040	S12-6	Source Datasets	35	62	0.564516129032258	0.317948717948718	The authors showed brief video segments to Annotators from Amazon Mechanical Turk (AMT) and were asked to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011).	0
7041	7041	S12-6	Source Datasets	36	63	0.580645161290323	0.323076923076923	Nearly 120 thousand sentences were collected for 2000 videos.	0
7042	7042	S12-6	Source Datasets	37	64	0.596774193548387	0.328205128205128	The sentences can be taken to be roughly parallel descriptions, and they included sentences for many languages.	0
7043	7043	S12-6	Source Datasets	38	65	0.612903225806452	0.333333333333333	Figure 1 shows a video and corresponding descriptions.	0
7044	7044	S12-6	Source Datasets	39	66	0.629032258064516	0.338461538461538	The sampling procedure from this dataset is similar to that for MSRpar.	0
7045	7045	S12-6	Source Datasets	40	67	0.645161290322581	0.343589743589744	We construct two bags of data to draw samples.	0
7046	7046	S12-6	Source Datasets	41	68	0.661290322580645	0.348717948717949	The first includes all possible pairs for the same video, and the second includes pairs taken from different videos.	0
7047	7047	S12-6	Source Datasets	42	69	0.67741935483871	0.353846153846154	Note that not all sentences from the same video were equivalent, as some descriptions were contradictory or unrelated.	0
7048	7048	S12-6	Source Datasets	43	70	0.693548387096774	0.358974358974359	Conversely, not all sentences coming from different videos were necessarily unrelated, as many videos were on similar topics.	0
7049	7049	S12-6	Source Datasets	44	71	0.709677419354839	0.364102564102564	We took an equal number of samples from each of these two sets, in an attempt to provide a balanced dataset between equivalent and non-equivalent pairs.	0
7050	7050	S12-6	Source Datasets	45	72	0.725806451612903	0.369230769230769	The sampling was also done according to string similarity, but in four bands in the [0.5 .. 0.8] range, as sentences from the same video had a usually higher string similarity than those in the MSRpar dataset.	0
7051	7051	S12-6	Source Datasets	46	73	0.741935483870968	0.374358974358974	We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.	0
7052	7052	S12-6	Source Datasets	47	74	0.758064516129032	0.379487179487179	Given the strong connection between STS systems and Machine Translation evaluation metrics, we also sampled pairs of segments that had been part of human evaluation exercises.	0
7053	7053	S12-6	Source Datasets	48	75	0.774193548387097	0.384615384615385	Those pairs included a reference translation and a automatic Machine Translation system submission, as follows:	0
7054	7054	S12-6	Source Datasets	49	76	0.790322580645161	0.38974358974359	The only instance in which no tax is levied is when the supplier is in a non-EU country and the recipient is in a Member State of the EU.	0
7055	7055	S12-6	Source Datasets	50	77	0.806451612903226	0.394871794871795	"The only case for which no tax is still perceived ""is an example of supply in the European Community from a third country."	0
7056	7056	S12-6	Source Datasets	51	78	0.82258064516129	0.4	We selected pairs from the translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2007;Callison-Burch et al., 2008).	0
7057	7057	S12-6	Source Datasets	52	79	0.838709677419355	0.405128205128205	For consistency, we only used French to English system submissions.	0
7058	7058	S12-6	Source Datasets	53	80	0.854838709677419	0.41025641025641	The training data includes all of the Europarl human ranked fr-en system submissions from WMT 2007, with each machine translation being paired with the correct reference translation.	0
7059	7059	S12-6	Source Datasets	54	81	0.870967741935484	0.415384615384615	This resulted in 729 unique training pairs.	0
7060	7060	S12-6	Source Datasets	55	82	0.887096774193548	0.420512820512821	The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less.	0
7061	7061	S12-6	Source Datasets	56	83	0.903225806451613	0.425641025641026	In addition, we selected two other datasets that were used as out-of-domain testing.	0
7062	7062	S12-6	Source Datasets	57	84	0.919354838709677	0.430769230769231	One of them comprised of all the human ranked fr-en system submissions from the WMT 2007 news conversation test set, resulting in 351 unique system reference pairs.	0
7063	7063	S12-6	Source Datasets	58	85	0.935483870967742	0.435897435897436	2	0
7064	7064	S12-6	Source Datasets	59	86	0.951612903225806	0.441025641025641	The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.1 (Fellbaum, 1998) senses.	0
7065	7065	S12-6	Source Datasets	60	87	0.967741935483871	0.446153846153846	The mapping of the senses of both resources comprised 110K sense pairs.	0
7066	7066	S12-6	Source Datasets	61	88	0.983870967741935	0.451282051282051	The similarity between the sense pairs was generated using simple word overlap.	0
7067	7067	S12-6	Source Datasets	62	89	1.0	0.456410256410256	50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another.	0
7068	7068	S12-6	Annotation	1	90	0.0625	0.461538461538462	In this first dataset we defined a straightforward likert scale ranging from 5 to 0, but we decided to provide definitions for each value in the scale (cf. Figure 2).	0
7069	7069	S12-6	Annotation	2	91	0.125	0.466666666666667	We first did pilot annotations of 200 pairs se-lected at random from the three main datasets in the training set.	0
7070	7070	S12-6	Annotation	3	92	0.1875	0.471794871794872	We did the annotation, and the pairwise Pearson ranged from 84% to 87% among ourselves.	0
7071	7071	S12-6	Annotation	4	93	0.25	0.476923076923077	The agreement of each annotator with the average scores of the other was between 87% and 89%.	0
7072	7072	S12-6	Annotation	5	94	0.3125	0.482051282051282	In the future, we would like to explore whether the definitions improve the consistency of the tagging with respect to a likert scale without definitions.	0
7073	7073	S12-6	Annotation	6	95	0.375	0.487179487179487	Note also that in the assessment of the quality and evaluation of the systems performances, we just took the resulting SS scores and their averages.	0
7074	7074	S12-6	Annotation	7	96	0.4375	0.492307692307692	Using the qualitative descriptions for each score in analysis and evaluation is left for future work.	0
7075	7075	S12-6	Annotation	8	97	0.5	0.497435897435897	Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.	0
7076	7076	S12-6	Annotation	9	98	0.5625	0.502564102564103	The turkers were required to have achieved a 95% of approval rating in their previous HITs, and had to pass a qualification task which included 6 example pairs.	0
7077	7077	S12-6	Annotation	10	99	0.625	0.507692307692308	Each HIT included 5 pairs of sentences, and was paid at 0.20$ each.	0
7078	7078	S12-6	Annotation	11	100	0.6875	0.512820512820513	We collected 5 annotations per HIT.	0
7079	7079	S12-6	Annotation	12	101	0.75	0.517948717948718	In the latest data collection, each HIT required 114.9 second for completion.	0
7080	7080	S12-6	Annotation	13	102	0.8125	0.523076923076923	In order to ensure the quality, we also performed post-hoc validation.	0
7081	7081	S12-6	Annotation	14	103	0.875	0.528205128205128	Each HIT contained one pair from our pilot.	0
7082	7082	S12-6	Annotation	15	104	0.9375	0.533333333333333	After the tagging was completed we checked the correlation of each individual turker with our scores, and removed annotations of turkers which had low correlations (below 50%).	0
7083	7083	S12-6	Annotation	16	105	1.0	0.538461538461538	Given the high quality of the annotations among the turkers, we could alternatively use the correlation between the turkers itself to detect poor quality annotators.	0
7084	7084	S12-6	Systems Evaluation	1	106	0.1	0.543589743589744	Given two sentences, s1 and s2, an STS system would need to return a similarity score.	1
7085	7085	S12-6	Systems Evaluation	2	107	0.2	0.548717948717949	Participants can also provide a confidence score indicating their confidence level for the result returned for each pair, but this confidence is not used for the main results.	1
7086	7086	S12-6	Systems Evaluation	3	108	0.3	0.553846153846154	The output of the systems performance is evaluated using the Pearson product-moment correlation coefficient between the system scores and the human scores, as customary in text similarity (Rubenstein and Goodenough, 1965).	0
7087	7087	S12-6	Systems Evaluation	4	109	0.4	0.558974358974359	We calculated Pearson for each evaluation dataset separately.	0
7088	7088	S12-6	Systems Evaluation	5	110	0.5	0.564102564102564	In order to have a single Pearson measure for each system we concatenated the gold standard (and system outputs) for all 5 datasets into a single gold stan-dard file (and single system output).	0
7089	7089	S12-6	Systems Evaluation	6	111	0.6	0.569230769230769	The first version of the results were published using this method, but the overall score did not correspond well to the individual scores in the datasets, and participants proposed two additional evaluation metrics, both of them based on Pearson correlation.	0
7090	7090	S12-6	Systems Evaluation	7	112	0.7	0.574358974358974	The organizers of the task decided that it was more informative, and on the benefit of the community, to also adopt those evaluation metrics, and the idea of having a single main evaluation metric was dropped.	0
7091	7091	S12-6	Systems Evaluation	8	113	0.8	0.57948717948718	This decision was not without controversy, but the organizers gave more priority to openness and inclusiveness and to the involvement of participants.	0
7092	7092	S12-6	Systems Evaluation	9	114	0.9	0.584615384615385	The final result table thus included three evaluation metrics.	0
7093	7093	S12-6	Systems Evaluation	10	115	1.0	0.58974358974359	For the future we plan to analyze the evaluation metrics, including non-parametric metrics like Spearman.	0
7094	7094	S12-6	Evaluation metrics	1	116	0.076923076923077	0.594871794871795	The first evaluation metric is the Pearson correlation for the concatenation of all five datasets, as described above.	0
7095	7095	S12-6	Evaluation metrics	2	117	0.153846153846154	0.6	We will use overall Pearson or simply ALL to refer to this measure.	0
7096	7096	S12-6	Evaluation metrics	3	118	0.230769230769231	0.605128205128205	The second evaluation metric normalizes the output for each dataset separately, using the linear least squares method.	0
7097	7097	S12-6	Evaluation metrics	4	119	0.307692307692308	0.61025641025641	We concatenated the system results for five datasets and then computed a single Pearson correlation.	0
7098	7098	S12-6	Evaluation metrics	5	120	0.384615384615385	0.615384615384615	Given Y = {y i } and X = {x i } (the gold standard scores and the system scores, respectively), we transform the system scores into X = {x i } in order to minimize the squared error i (y i − x i ) 2 .	0
7099	7099	S12-6	Evaluation metrics	6	121	0.461538461538462	0.620512820512821	The linear transformation is given by	0
7100	7100	S12-6	Evaluation metrics	7	122	0.538461538461538	0.625641025641026	where β 1 and β 2 are found analytically.	0
7101	7101	S12-6	Evaluation metrics	8	123	0.615384615384615	0.630769230769231	We refer to this measure as Normalized Pearson or simply ALLnorm.	0
7102	7102	S12-6	Evaluation metrics	9	124	0.692307692307692	0.635897435897436	This metric was suggested by one of the participants, Sergio Jimenez.	0
7103	7103	S12-6	Evaluation metrics	10	125	0.769230769230769	0.641025641025641	The third evaluation metric is the weighted mean of the Pearson correlations on individual datasets.	0
7104	7104	S12-6	Evaluation metrics	11	126	0.846153846153846	0.646153846153846	The Pearson returned for each dataset is weighted according to the number of sentence pairs in that dataset.	0
7105	7105	S12-6	Evaluation metrics	12	127	0.923076923076923	0.651282051282051	Given r i the five Pearson scores for each dataset, and n i the number of pairs in each dataset, the weighted mean is given as i=1..5 (r i * n i )/ i=1..5 n i	0
7106	7106	S12-6	Evaluation metrics	13	128	1.0	0.656410256410256	We refer to this measure as weighted mean of Pearson or Mean for short.	0
7107	7107	S12-6	Using confidence scores	1	129	0.333333333333333	0.661538461538462	Participants were allowed to include a confidence score between 1 and 100 for each of their scores.	0
7108	7108	S12-6	Using confidence scores	2	130	0.666666666666667	0.666666666666667	We used weighted Pearson to use those confidence scores 3 . Table 2 includes the list of systems which provided a non-uniform confidence.	0
7109	7109	S12-6	Using confidence scores	3	131	1.0	0.671794871794872	The results show that some systems were able to improve their correlation, showing promise for the usefulness of confidence in applications.	0
7110	7110	S12-6	The Baseline System	1	132	0.166666666666667	0.676923076923077	We produced scores using a simple word overlap baseline system.	0
7111	7111	S12-6	The Baseline System	2	133	0.333333333333333	0.682051282051282	We tokenized the input sentences splitting at white spaces, and then represented each sentence as a vector in the multidimensional token space.	0
7112	7112	S12-6	The Baseline System	3	134	0.5	0.687179487179487	Each dimension had 1 if the token was present in the sentence, 0 otherwise.	0
7113	7113	S12-6	The Baseline System	4	135	0.666666666666667	0.692307692307692	Similarity of vectors was computed using cosine similarity.	0
7114	7114	S12-6	The Baseline System	5	136	0.833333333333333	0.697435897435897	We also run a random baseline several times, yielding close to 0 correlations in all datasets, as expected.	0
7115	7115	S12-6	The Baseline System	6	137	1.0	0.702564102564102	We will refer to the random baseline again in Section 4.5.	0
7116	7116	S12-6	Participation	1	138	0.066666666666667	0.707692307692308	Participants could send a maximum of three system runs.	0
7117	7117	S12-6	Participation	2	139	0.133333333333333	0.712820512820513	After downloading the test datasets, they had a maximum of 120 hours to upload the results.	0
7118	7118	S12-6	Participation	3	140	0.2	0.717948717948718	35 teams participated, submitting 88 system runs (cf. first column of Table 1).	0
7119	7119	S12-6	Participation	4	141	0.266666666666667	0.723076923076923	Due to lack of space we can't detail the full names of authors and institutions that participated.	0
7120	7120	S12-6	Participation	5	142	0.333333333333333	0.728205128205128	The interested reader can use the name of the runs to find the relevant paper in these proceedings.	0
7121	7121	S12-6	Participation	6	143	0.4	0.733333333333333	There were several issues in the submissions.	0
7122	7122	S12-6	Participation	7	144	0.466666666666667	0.738461538461539	The submission software did not ensure that the naming conventions were appropriately used, and this caused some submissions to be missed, and in two cases the results were wrongly assigned.	0
7123	7123	S12-6	Participation	8	145	0.533333333333333	0.743589743589744	Some participants returned Not-a-Number as a score, and the organizers had to request whether those where to be taken as a 0 or as a 5.	0
7124	7124	S12-6	Participation	9	146	0.6	0.748717948717949	Finally, one team submitted past the 120 hour deadline and some teams sent missing files after the deadline.	0
7125	7125	S12-6	Participation	10	147	0.666666666666667	0.753846153846154	All those are explicitly marked in Table 1.	0
7126	7126	S12-6	Participation	11	148	0.733333333333333	0.758974358974359	The teams that included one of the organizers are also explicitly marked.	0
7127	7127	S12-6	Participation	12	149	0.8	0.764102564102564	We want to stress that in these teams the organizers did not allow the developers of the system to access any data or information which was not available for the rest of participants.	0
7128	7128	S12-6	Participation	13	150	0.866666666666667	0.769230769230769	One exception is weiwei, as they generated the 110K OntoNotes-Word	0
7129	7129	S12-6	Participation	14	151	0.933333333333333	0.774358974358974	Net dataset from which the other organizers sampled the surprise data set.	0
7130	7130	S12-6	Participation	15	152	1.0	0.77948717948718	After the submission deadline expired, the organizers published the gold standard in the task website, in order to ensure a transparent evaluation process.	0
7131	7131	S12-6	Results	1	153	0.071428571428572	0.784615384615385	Table 1 shows the results for each run in alphabetic order.	0
7132	7132	S12-6	Results	2	154	0.142857142857143	0.78974358974359	Each result is followed by the rank of the system according to the given evaluation measure.	0
7133	7133	S12-6	Results	3	155	0.214285714285714	0.794871794871795	To the right, the Pearson score for each dataset is given.	0
7134	7134	S12-6	Results	4	156	0.285714285714286	0.8	In boldface, the three best results in each column.	0
7135	7135	S12-6	Results	5	157	0.357142857142857	0.805128205128205	First of all we want to stress that the large majority of the systems are well above the simple baseline, although the baseline would rank 70 on the Mean measure, improving over 19 runs.	0
7136	7136	S12-6	Results	6	158	0.428571428571429	0.81025641025641	The correlation for the non-MT datasets were really high: the highest correlation was obtained was for MSRvid (0.88 r), followed by MSRpar (0.73 r) and On-WN (0.73 r).	0
7137	7137	S12-6	Results	7	159	0.5	0.815384615384615	The results for the MT evaluation data are lower, (0.57 r) for SMT-eur and (0.61 r) for SMT-News.	0
7138	7138	S12-6	Results	8	160	0.571428571428571	0.82051282051282	The simple token overlap baseline, on the contrary, obtained the highest results for On-WN (0.59 r), with (0.43 r) on MSRpar and (0.40 r) on MSRvid.	0
7139	7139	S12-6	Results	9	161	0.642857142857143	0.825641025641026	The results for MT evaluation data are also reversed, with (0.40 r) for SMT-eur and (0.45 r) for SMT-News.	0
7140	7140	S12-6	Results	10	162	0.714285714285714	0.830769230769231	The ALLnorm measure yields the highest correlations.	0
7141	7141	S12-6	Results	11	163	0.785714285714286	0.835897435897436	This comes at no surprise, as it involves a normalization which transforms the system outputs using the gold standard.	0
7142	7142	S12-6	Results	12	164	0.857142857142857	0.841025641025641	In fact, a random baseline which gets Pearson correlations close to 0 in all datasets would attain Pearson of 0.5891 4 .	0
7143	7143	S12-6	Results	13	165	0.928571428571429	0.846153846153846	Although not included in the results table for lack of space, we also performed an analysis of confidence intervals.	0
7144	7144	S12-6	Results	14	166	1.0	0.851282051282051	For instance, the best run according to ALL (r = .8239) has a 95% confidence interval of [.8123,.8349] and the second a confidence interval of [.8016,.8254], meaning that the differences are not statistically different.	0
7145	7145	S12-6	Tools and resources used	1	167	0.1	0.856410256410256	The organizers asked participants to submit a description file, special emphasis on the tools and resources that they used.	0
7146	7146	S12-6	Tools and resources used	2	168	0.2	0.861538461538462	fied way the tools and resources used by those participants that did submit a valid description file.	0
7147	7147	S12-6	Tools and resources used	3	169	0.3	0.866666666666667	In the last row, the totals show that WordNet was the most used resource, followed by monolingual corpora and Wikipedia.	0
7148	7148	S12-6	Tools and resources used	4	170	0.4	0.871794871794872	Acronyms, dictionaries, multilingual corpora, stopword lists and tables of paraphrases were also used.	0
7149	7149	S12-6	Tools and resources used	5	171	0.5	0.876923076923077	Generic NLP tools like lemmatization and PoS tagging were widely used, and to a lesser extent, parsing, word sense disambiguation, semantic role labeling and time and date resolution (in this order).	0
7150	7150	S12-6	Tools and resources used	6	172	0.6	0.882051282051282	Knowledge-based and distributional methods got used nearly equally, and to a lesser extent, alignment and/or statistical machine translation software, lexical substitution, string similarity, textual entailment and machine translation evaluation software.	0
7151	7151	S12-6	Tools and resources used	7	173	0.7	0.887179487179487	Machine learning was widely used to combine and tune components.	0
7152	7152	S12-6	Tools and resources used	8	174	0.8	0.892307692307692	Several less used tools were also listed but were used by three or less systems.	0
7153	7153	S12-6	Tools and resources used	9	175	0.9	0.897435897435897	The top scoring systems tended to use most of the resources and tools listed (UKP, Takelab), with some notable exceptions like Sgjimenez which was based on string similarity.	0
7154	7154	S12-6	Tools and resources used	10	176	1.0	0.902564102564103	For a more detailed analysis, the reader is directed to the papers of the participants in this volume.	0
7155	7155	S12-6	Conclusions and Future Work	1	177	0.052631578947369	0.907692307692308	This paper presents the SemEval 2012 pilot evaluation exercise on Semantic Textual Similarity.	0
7156	7156	S12-6	Conclusions and Future Work	2	178	0.105263157894737	0.912820512820513	A simple definition of STS beyond the likert-scale was set up, and a wealth of annotated data was produced.	0
7157	7157	S12-6	Conclusions and Future Work	3	179	0.157894736842105	0.917948717948718	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk.	0
7158	7158	S12-6	Conclusions and Future Work	4	180	0.210526315789474	0.923076923076923	The dataset includes 1500 sentence pairs from MSRpar and MSRvid (each), ca. 1500 pairs from WMT, and 750 sentence pairs from a mapping between OntoNotes and WordNet senses.	0
7159	7159	S12-6	Conclusions and Future Work	5	181	0.263157894736842	0.928205128205128	The correlation be-tween non-expert annotators and annotations from the authors is very high, showing the high quality of the dataset.	0
7160	7160	S12-6	Conclusions and Future Work	6	182	0.31578947368421	0.933333333333333	The dataset was split 50% as train and test, with the exception of the surprise test datasets: a subset of WMT from a different domain and the OntoNotes-WordNet mapping.	0
7161	7161	S12-6	Conclusions and Future Work	7	183	0.368421052631579	0.938461538461538	All datasets are publicly available.	0
7162	7162	S12-6	Conclusions and Future Work	8	184	0.421052631578947	0.943589743589743	5	0
7163	7163	S12-6	Conclusions and Future Work	9	185	0.473684210526316	0.948717948717949	The exercise was very successful in participation and results.	0
7164	7164	S12-6	Conclusions and Future Work	10	186	0.526315789473684	0.953846153846154	35 teams participated, submitting 88 runs.	0
7165	7165	S12-6	Conclusions and Future Work	11	187	0.578947368421053	0.958974358974359	The best results scored a Pearson correlation over 80%, well beyond a simple lexical baseline with 31% of correlation.	0
7166	7166	S12-6	Conclusions and Future Work	12	188	0.631578947368421	0.964102564102564	The metric for evaluation was not completely satisfactory, and three evaluation metrics were finally published.	0
7167	7167	S12-6	Conclusions and Future Work	13	189	0.68421052631579	0.969230769230769	We discuss the shortcomings of those measures.	0
7168	7168	S12-6	Conclusions and Future Work	14	190	0.736842105263158	0.974358974358974	There are several tasks ahead in order to make STS a mature field.	0
7169	7169	S12-6	Conclusions and Future Work	15	191	0.789473684210526	0.979487179487179	The first is to find a satisfactory evaluation metric.	0
7170	7170	S12-6	Conclusions and Future Work	16	192	0.842105263157895	0.984615384615385	The second is to analyze the definition of the task itself, with a thorough analysis of the definitions in the likert scale.	0
7171	7171	S12-6	Conclusions and Future Work	17	193	0.894736842105263	0.98974358974359	We would also like to analyze the relation between the STS scores and the paraphrase judgements in MSR, as well as the human evaluations in WMT.	0
7172	7172	S12-6	Conclusions and Future Work	18	194	0.947368421052632	0.994871794871795	Finally, we would also like to set up an open framework where NLP components and similarity algorithms can be combined by the community.	0
7173	7173	S12-6	Conclusions and Future Work	19	195	1.0	1.0	All in all, we would like this dataset to be the focus of the community working on algorithmic approaches for semantic processing and inference at large.	0
8118	8118	S13-5	title	1	1	1.0	0.004950495049505	SemEval-2013 Task 5: Evaluating Phrasal Semantics	0
8119	8119	S13-5	abstract	1	2	0.2	0.00990099009901	"This paper describes the SemEval-2013 Task 5: ""Evaluating Phrasal Semantics""."	0
8120	8120	S13-5	abstract	2	3	0.4	0.014851485148515	Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length.	0
8121	8121	S13-5	abstract	3	4	0.6	0.01980198019802	The second one addresses deciding the compositionality of phrases in a given context.	0
8122	8122	S13-5	abstract	4	5	0.8	0.024752475247525	The paper discusses the importance and background of these subtasks and their structure.	0
8123	8123	S13-5	abstract	5	6	1.0	0.02970297029703	In succession, it introduces the systems that participated and discusses evaluation results.	0
8124	8124	S13-5	Introduction	1	7	0.043478260869565	0.034653465346535	Numerous past tasks have focused on leveraging the meaning of word types or words in context.	0
8125	8125	S13-5	Introduction	2	8	0.086956521739131	0.03960396039604	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.	0
8126	8126	S13-5	Introduction	3	9	0.130434782608696	0.044554455445545	As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.	0
8127	8127	S13-5	Introduction	4	10	0.173913043478261	0.04950495049505	In this paper, we present two subtasks designed to evaluate such phrasal models: a. Semantic similarity of words and compositional phrases b.	1
8128	8128	S13-5	Introduction	5	11	0.217391304347826	0.054455445544555	Evaluating the compositionality of phrases in context	1
8129	8129	S13-5	Introduction	6	12	0.260869565217391	0.059405940594059	"For example, the first subtask addresses computing how similar the word ""valuation"" is to the compositional sequence ""price assessment"", while the second subtask addresses deciding whether the phrase ""piece of cake"" is used literally or figuratively in the sentence ""Labour was a piece of cake!""."	0
8130	8130	S13-5	Introduction	7	13	0.304347826086957	0.064356435643564	The aim of these subtasks is two-fold.	0
8131	8131	S13-5	Introduction	8	14	0.347826086956522	0.069306930693069	Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.	0
8132	8132	S13-5	Introduction	9	15	0.391304347826087	0.074257425742574	It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.	0
8133	8133	S13-5	Introduction	10	16	0.434782608695652	0.079207920792079	Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.	0
8134	8134	S13-5	Introduction	11	17	0.478260869565217	0.084158415841584	Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.	0
8135	8135	S13-5	Introduction	12	18	0.521739130434783	0.089108910891089	We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.	0
8136	8136	S13-5	Introduction	13	19	0.565217391304348	0.094059405940594	Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.	0
8137	8137	S13-5	Introduction	14	20	0.608695652173913	0.099009900990099	In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.	0
8138	8138	S13-5	Introduction	15	21	0.652173913043478	0.103960396039604	The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.	0
8139	8139	S13-5	Introduction	16	22	0.695652173913043	0.108910891089109	Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.	0
8140	8140	S13-5	Introduction	17	23	0.739130434782609	0.113861386138614	In subsection 3.1 the subtask is described in detail together with some information about its background.	0
8141	8141	S13-5	Introduction	18	24	0.782608695652174	0.118811881188119	Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.	0
8142	8142	S13-5	Introduction	19	25	0.826086956521739	0.123762376237624	Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.	0
8143	8143	S13-5	Introduction	20	26	0.869565217391304	0.128712871287129	Subsection 4.1 explains the data creation process for this subtask.	0
8144	8144	S13-5	Introduction	21	27	0.91304347826087	0.133663366336634	In subsection 4.2 the evaluation statistics of participating systems are presented.	0
8145	8145	S13-5	Introduction	22	28	0.956521739130435	0.138613861386139	Section 5 is a discussion about the conclusions of the entire task.	0
8146	8146	S13-5	Introduction	23	29	1.0	0.143564356435644	Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.	0
8147	8147	S13-5	Data Sources &amp; Methodology	1	30	0.037037037037037	0.148514851485149	Data instances of both subtasks are drawn from the large-scale, freely available WaCky corpora (Baroni et al., 2009).	0
8148	8148	S13-5	Data Sources &amp; Methodology	2	31	0.074074074074074	0.153465346534653	The resource contains corpora in 4 languages: English, French, German and Italian.	0
8149	8149	S13-5	Data Sources &amp; Methodology	3	32	0.111111111111111	0.158415841584158	The English corpus, ukWaC, consists of 2 billion words and was constructed by crawling to the .uk domain of the web and using medium-frequency words from the BNC as seeds.	0
8150	8150	S13-5	Data Sources &amp; Methodology	4	33	0.148148148148148	0.163366336633663	The corpus is part-of-speech (PoS) tagged and lemmatized using the TreeTagger (Schmid, 1994).	0
8151	8151	S13-5	Data Sources &amp; Methodology	5	34	0.185185185185185	0.168316831683168	The French corpus, frWaC, contains 1.6 billion word corpus and was constructed by web-crawling the .fr domain and using mediumfrequency words from the Le Monde Diplomatique corpus and basic French vocabulary lists as seeds.	0
8152	8152	S13-5	Data Sources &amp; Methodology	6	35	0.222222222222222	0.173267326732673	The corpus was PoS tagged and lemmatized with the TreeTagger.	0
8153	8153	S13-5	Data Sources &amp; Methodology	7	36	0.259259259259259	0.178217821782178	The French corpus, deWaC, consists of 1.7 billion word corpus and was constructed by crawling the .de domain and using mediumfrequency words from the SudDeutsche Zeitung cor-pus and basic German vocabulary lists as seeds.	0
8154	8154	S13-5	Data Sources &amp; Methodology	8	37	0.296296296296296	0.183168316831683	The corpus was PoS tagged and lemmatized with the TreeTagger.	0
8155	8155	S13-5	Data Sources &amp; Methodology	9	38	0.333333333333333	0.188118811881188	The Italian corpus, itWaC, is a 2 billion word corpus constructed from the .it domain of the web using medium-frequency words from the Repubblica corpus and basic Italian vocabulary lists as seeds.	0
8156	8156	S13-5	Data Sources &amp; Methodology	10	39	0.37037037037037	0.193069306930693	The corpus was PoS tagged with the Tree-Tagger, and lemmatized using the Morph-it!	0
8157	8157	S13-5	Data Sources &amp; Methodology	11	40	0.407407407407407	0.198019801980198	lexicon (Zanchetta and Baroni, 2005).	0
8158	8158	S13-5	Data Sources &amp; Methodology	12	41	0.444444444444444	0.202970297029703	Several versions of the WaCky corpora, with various extra annotations or modifications are also available 1 .	0
8159	8159	S13-5	Data Sources &amp; Methodology	13	42	0.481481481481481	0.207920792079208	We ensured that data instances occur frequently enough in the WaCky corpora, so that participating systems could gather statistics for building distributional vectors or other uses.	0
8160	8160	S13-5	Data Sources &amp; Methodology	14	43	0.518518518518518	0.212871287128713	As the evaluation data only contains very small annotated samples from freely available web documents, and the original source is provided, we could provide them without violating copyrights.	0
8161	8161	S13-5	Data Sources &amp; Methodology	15	44	0.555555555555556	0.217821782178218	The size of the WaCky corpora is suitable for training reliable distributional models.	0
8162	8162	S13-5	Data Sources &amp; Methodology	16	45	0.592592592592593	0.222772277227723	Sentences are already lemmatized and part-of-speech tagged.	0
8163	8163	S13-5	Data Sources &amp; Methodology	17	46	0.62962962962963	0.227722772277228	Participating approaches making use of distributional methods, part-of-speech tags or lemmas, were strongly encouraged to use these corpora and their shared preprocessing, to ensure the highest possible comparability of results.	0
8164	8164	S13-5	Data Sources &amp; Methodology	18	47	0.666666666666667	0.232673267326733	Additionally, this had the potential to considerably reduce the workload of participants.	0
8165	8165	S13-5	Data Sources &amp; Methodology	19	48	0.703703703703704	0.237623762376238	For the first subtask, data were provided in English, German and Italian and for the second subtask in English and German.	0
8166	8166	S13-5	Data Sources &amp; Methodology	20	49	0.740740740740741	0.242574257425743	The range of methods applicable to both subtasks was deliberately not limited to any specific branch of methods, such as distributional or vector models of semantic compositionality.	0
8167	8167	S13-5	Data Sources &amp; Methodology	21	50	0.777777777777778	0.247524752475248	We believe that the subtasks can be tackled from different directions and we expect a great deal of the scientific benefit to lie in the comparison of very different approaches, as well as how these approaches can be combined.	0
8168	8168	S13-5	Data Sources &amp; Methodology	22	51	0.814814814814815	0.252475247524752	An exception to this rule is the fact that participants in the first subtask were not allowed to use directly definitions extracted from dictionaries or lexicons.	0
8169	8169	S13-5	Data Sources &amp; Methodology	23	52	0.851851851851852	0.257425742574257	Since the subtask is considered fundamental and its data were created from online knowledge resources, systems using the same tools to address it would be of limited use.	0
8170	8170	S13-5	Data Sources &amp; Methodology	24	53	0.888888888888889	0.262376237623762	However, participants were allowed to use other information residing in dictionaries, such as Wordnet synsets or synset relations.	0
8171	8171	S13-5	Data Sources &amp; Methodology	25	54	0.925925925925926	0.267326732673267	Participating systems were allowed to attempt one or both subtasks, in one or all of the languages supported.	0
8172	8172	S13-5	Data Sources &amp; Methodology	26	55	0.962962962962963	0.272277227722772	However, it was expected that systems performing well at the first basic subtask would provide a good starting point for dealing with the second subtask, which is considered harder.	0
8173	8173	S13-5	Data Sources &amp; Methodology	27	56	1.0	0.277227722772277	Moreover, language-independent models were of special interest.	0
8174	8174	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	1	57	0.25	0.282178217821782	The aim of this subtask is to evaluate the component of a semantic model that computes the similarity between word sequences of different length.	0
8175	8175	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	2	58	0.5	0.287128712871287	Participating systems are asked to estimate the semantic similarity of a word and a short sequence of two words.	0
8176	8176	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	3	59	0.75	0.292079207920792	For example, they should be able to figure out that contact and close interaction are similar whereas megalomania and great madness are not.	0
8177	8177	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	4	60	1.0	0.297029702970297	This subtask addresses a core problem, since satisfactory performance in computing the similarity of full sentences depends on similarity computations on shorter sequences.	0
8178	8178	S13-5	Background and Description	1	61	0.111111111111111	0.301980198019802	This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step.	0
8179	8179	S13-5	Background and Description	2	62	0.222222222222222	0.306930693069307	For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008).	0
8180	8180	S13-5	Background and Description	3	63	0.333333333333333	0.311881188118812	Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010;	0
8181	8181	S13-5	Background and Description	4	64	0.444444444444444	0.316831683168317	Baroni and Zamparelli, 2010;Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words.	0
8182	8182	S13-5	Background and Description	5	65	0.555555555555556	0.321782178217822	As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word.	0
8183	8183	S13-5	Background and Description	6	66	0.666666666666667	0.326732673267327	This is important as it is the basic step to analyse models that can compare any word sequences of different length.	0
8184	8184	S13-5	Background and Description	7	67	0.777777777777778	0.331683168316832	The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010).	0
8185	8185	S13-5	Background and Description	8	68	0.888888888888889	0.336633663366337	Dictionaries were used as sources of contact/[kon-takt]	0
8186	8186	S13-5	Background and Description	9	69	1.0	0.341584158415842	1. the act or state of touching; a touching or meeting, as of two things or people.	0
8187	8187	S13-5	close interaction	1	70	0.1	0.346534653465346	3. an acquaintance, colleague, or relative through whom a person can gain access to information, favors, influential people, and the like.	0
8188	8188	S13-5	close interaction	2	71	0.2	0.351485148514851	Figure 1 presents the definition of the word contact, from which the pair (contact, close interaction) can be extracted.	0
8189	8189	S13-5	close interaction	3	72	0.3	0.356435643564356	Such equivalences extracted from dictionaries can be seen as natural and unbiased data instances.	0
8190	8190	S13-5	close interaction	4	73	0.4	0.361386138613861	This idea opens numerous opportunities:	0
8191	8191	S13-5	close interaction	5	74	0.5	0.366336633663366	•	0
8192	8192	S13-5	close interaction	6	75	0.6	0.371287128712871	Since definitions in dictionaries are syntactically rich, we are able to create examples for different syntactic relations.	0
8193	8193	S13-5	close interaction	7	76	0.7	0.376237623762376	•	0
8194	8194	S13-5	close interaction	8	77	0.8	0.381188118811881	We have the opportunity to extract positive examples for languages for which dictionaries with sufficient entries are available.	0
8195	8195	S13-5	close interaction	9	78	0.9	0.386138613861386	Negative examples were generated by matching words under definition with randomly chosen defining sequences.	0
8196	8196	S13-5	close interaction	10	79	1.0	0.391089108910891	In the following subsection, we provide details about the application of this idea to build the development and testing set for subtask 5a.	0
8197	8197	S13-5	Data Creation	1	80	0.041666666666667	0.396039603960396	Data for this subtask were provided in English, German and Italian.	0
8198	8198	S13-5	Data Creation	2	81	0.083333333333333	0.400990099009901	Pairs of words under definitions and defining sequences were extracted from the English, German and Italian part of Wiktionary, respectively.	0
8199	8199	S13-5	Data Creation	3	82	0.125	0.405940594059406	In particular, for each language, all Wiktionary entries were downloaded and part-of-speech tagged using the Genia tagger (Tsuruoka et al., 2005  were kept, only.	0
8200	8200	S13-5	Data Creation	4	83	0.166666666666667	0.410891089108911	For the purpose of extracting word and sequence pairs for this subtask, we consider as noun phrases, sequences that consist of adjectives or noun and end with a noun.	0
8201	8201	S13-5	Data Creation	5	84	0.208333333333333	0.415841584158416	In cases where the extracted noun phrase was longer than two words, the right-most two sequences were kept, since in most cases noun phrases are governed by their rightmost component.	0
8202	8202	S13-5	Data Creation	6	85	0.25	0.420792079207921	Subsequently, we discarded instances whose words occur too infrequently in the WaCky corpora (Baroni et al., 2009) of each language.	0
8203	8203	S13-5	Data Creation	7	86	0.291666666666667	0.425742574257426	WaCky corpora are available freely and are large enough for participating systems to extract distributional statistics.	0
8204	8204	S13-5	Data Creation	8	87	0.333333333333333	0.430693069306931	Taking the numbers of extracted instances into account, we set the frequency thresholds at 10 occurrences for English and 5 for German and Italian.	0
8205	8205	S13-5	Data Creation	9	88	0.375	0.435643564356436	Data instances extracted following this process were then checked by a computational linguist.	0
8206	8206	S13-5	Data Creation	10	89	0.416666666666667	0.440594059405941	Candidate pairs in which the definition sequence was not judged to be a precise and adequate definition of the word under definition were discarded.	0
8207	8207	S13-5	Data Creation	11	90	0.458333333333333	0.445544554455446	These cases were very limited and mostly account for shortcomings of the very simple pattern used for extraction.	0
8208	8208	S13-5	Data Creation	12	91	0.5	0.45049504950495	"For example, the pair (standard, transmission vehicle) coming from the definition of ""standard"" as ""A manual transmission vehicle"" was discarded."	0
8209	8209	S13-5	Data Creation	13	92	0.541666666666667	0.455445544554455	Similarly in German, the pair (Fremde (Eng. stranger), weibliche Person (Eng. female person)) was discarded.	0
8210	8210	S13-5	Data Creation	14	93	0.583333333333333	0.46039603960396	"""Fremde"", which is of female grammatical genre, was defined as ""weibliche Person, die man nicht kennt (Eng. female person, one does not know)""."	0
8211	8211	S13-5	Data Creation	15	94	0.625	0.465346534653465	"In Italian, the pair (paese (Eng. land, country, region), grande estensione (Eng. large tract)) was discarded, since the original definition was ""grande estensione di terreno abitato e generalmente coltivato (Eng. large tract of land inhabited and cultivated in general)""."	0
8212	8212	S13-5	Data Creation	16	95	0.666666666666667	0.47029702970297	The final data sets were divided into training and held-out testing sets, according to a 60% and 40% ratio, respectively.	0
8213	8213	S13-5	Data Creation	17	96	0.708333333333333	0.475247524752475	The first three rows of table 1 present the numbers of the train and test sets for the three languages chosen.	0
8214	8214	S13-5	Data Creation	18	97	0.75	0.48019801980198	It was identified that a fair percentage of the German instances (approximately 27%) refer to the definitions of first names or family names.	0
8215	8215	S13-5	Data Creation	19	98	0.791666666666667	0.485148514851485	This is probably a flaw of the German part of Wiktionary.	0
8216	8216	S13-5	Data Creation	20	99	0.833333333333333	0.49009900990099	In addition, the pattern used for extraction happens to apply to the definitions of names.	0
8217	8217	S13-5	Data Creation	21	100	0.875	0.495049504950495	Name instances were discarded from the German data set to produce the data set described in the last row of table 1.	0
8218	8218	S13-5	Data Creation	22	101	0.916666666666667	0.5	The training set was released approximately 3 months earlier than the test data.	0
8219	8219	S13-5	Data Creation	23	102	0.958333333333333	0.504950495049505	Instances in both set ware annotated as positive or negative.	0
8220	8220	S13-5	Data Creation	24	103	1.0	0.50990099009901	Test set annotations were not released to the participants, but were used for evaluation, only.	0
8221	8221	S13-5	Results	1	104	0.032258064516129	0.514851485148515	Participating systems were evaluated on their ability to predict correctly whether the components of each test instance, i.e. word-sequence pair, are semantically similar or distinct.	0
8222	8222	S13-5	Results	2	105	0.064516129032258	0.51980198019802	Participants were allowed to use or ignore the training data, i.e. the systems could be supervised or unsupervised.	0
8223	8223	S13-5	Results	3	106	0.096774193548387	0.524752475247525	Unsupervised systems were allowed to use the training data for development and parameter tuning.	0
8224	8224	S13-5	Results	4	107	0.129032258064516	0.52970297029703	Since this is a core task, participating systems were not be able to use dictionaries or other prefabricated lists.	0
8225	8225	S13-5	Results	5	108	0.161290322580645	0.534653465346535	Instead, they were allowed to use distributional similarity models, selectional preferences, measures of semantic similarity etc.	0
8226	8226	S13-5	Results	6	109	0.193548387096774	0.53960396039604	Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F 1 score (Radev et al., 2003).	0
8227	8227	S13-5	Results	7	110	0.225806451612903	0.544554455445545	Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted.	0
8228	8228	S13-5	Results	8	111	0.258064516129032	0.54950495049505	Five research teams participated.	0
8229	8229	S13-5	Results	9	112	0.290322580645161	0.554455445544555	Ten system runs were submitted for English, one for German (on data set: German -no names) and one for Italian.	0
8230	8230	S13-5	Results	10	113	0.32258064516129	0.559405940594059	Table 2 illustrates the results of the evaluation process.	0
8231	8231	S13-5	Results	11	114	0.354838709677419	0.564356435643564	The teams of (HsH) (Wartena, 2013)  these approaches performed better than some supervised ones for this experiment.	0
8232	8232	S13-5	Results	12	115	0.387096774193548	0.569306930693069	Below, we summarise the properties of participating systems.	0
8233	8233	S13-5	Results	13	116	0.419354838709677	0.574257425742574	(HsH) (Wartena, 2013) used distributed similarity and especially random indexing to compute similarities between words and possible definitions, under the hypothesis that a word and its definition are distributionally more similar than a word and an arbitrary definition.	0
8234	8234	S13-5	Results	14	117	0.451612903225806	0.579207920792079	Considering all open-class words, context vectors over the entire WaCky corpus were computed for the word under definition, the defining sequence, its component words separately, the addition and multiplication of the vectors of the component words and a general context vector.	0
8235	8235	S13-5	Results	15	118	0.483870967741936	0.584158415841584	Then, various similarity measures were computed on the vectors, including an innovative length-normalised version of Jensen-Shannon divergence.	0
8236	8236	S13-5	Results	16	119	0.516129032258065	0.589108910891089	The similarity values are used to train a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).	0
8237	8237	S13-5	Results	17	120	0.548387096774194	0.594059405940594	The first approach (run 1) of CLaC (Siblini and Kosseim, 2013) is based on a weighted semantic network to measure semantic relatedness between the word and the components of the phrase.	0
8238	8238	S13-5	Results	18	121	0.580645161290323	0.599009900990099	A PART classifier is used to generate a partial decision trained on the semantic relatedness information of the labelled training set.	0
8239	8239	S13-5	Results	19	122	0.612903225806452	0.603960396039604	The second approach uses a supervised distributional method based on words frequently occurring in the Web1TB corpus to calculate relatedness.	0
8240	8240	S13-5	Results	20	123	0.645161290322581	0.608910891089109	A JRip classifier is used to gen-erate rules trained on the semantic relatedness information of the training set.	0
8241	8241	S13-5	Results	21	124	0.67741935483871	0.613861386138614	This approach was used in conjunction with the first one as a backup method (run 2).	0
8242	8242	S13-5	Results	22	125	0.709677419354839	0.618811881188119	In addition, features generated by both approaches were used to train the JRIP classifier collectively (run 3).	0
8243	8243	S13-5	Results	23	126	0.741935483870968	0.623762376237624	The first approach of MELODI (Van de Cruys et al., 2013), called lvw, uses a dependency-based vector space model computed over the ukWaC corpus, in combination with Latent Vector Weighting ( Van de Cruys et al., 2011).	0
8244	8244	S13-5	Results	24	127	0.774193548387097	0.628712871287129	The system computes the similarity between the first noun and the head noun of the second phrase, which was weighted according to the semantics of the modifier.	0
8245	8245	S13-5	Results	25	128	0.806451612903226	0.633663366336634	The second approach, called dm, used a dependency-based vector space model, but, unlike the first approach, disregarded the modifier in the defining sequence.	0
8246	8246	S13-5	Results	26	129	0.838709677419355	0.638613861386139	Since both systems are unsupervised, the training data was used to train a similarity threshold parameter, only.	0
8247	8247	S13-5	Results	27	130	0.870967741935484	0.643564356435644	UMCC DLSI-(EPS) (Dávila et al., 2013) locates the synsets of words in data instances and computes the semantic distances between each synset of the word under definition and each synsets of the defining sequence words.	0
8248	8248	S13-5	Results	28	131	0.903225806451613	0.648514851485149	In succession, a classifier is trained using features based on distance and Word-Net relations.	0
8249	8249	S13-5	Results	29	132	0.935483870967742	0.653465346534653	The first attempt of ITNLP (run 1) consisted of an SVM classifier trained on semantic similarity computations between the word under definition and the defining sequence in each instance.	0
8250	8250	S13-5	Results	30	133	0.967741935483871	0.658415841584158	Their second attempt also uses an SVM, however trained on WordNet-based similarities.	0
8251	8251	S13-5	Results	31	134	1.0	0.663366336633663	The third attempt of ITNLP is a combination of the previous two; it combines their features to train an SVM classifier.	0
8252	8252	S13-5	Subtask 5b: Semantic Compositionality in Context	1	135	0.058823529411765	0.668316831683168	An interesting sub-problem of semantic compositionality is to decide whether a target phrase is used in its literal or figurative meaning in a given context.	0
8253	8253	S13-5	Subtask 5b: Semantic Compositionality in Context	2	136	0.117647058823529	0.673267326732673	"For example ""big picture"" might be used literally as in Click here for a bigger picture or figuratively as in To solve this problem, you have to look at the bigger picture."	0
8254	8254	S13-5	Subtask 5b: Semantic Compositionality in Context	3	137	0.176470588235294	0.678217821782178	"Another example is ""old school"" which can also be used literally or figuratively:"	0
8255	8255	S13-5	Subtask 5b: Semantic Compositionality in Context	4	138	0.235294117647059	0.683168316831683	He will go down in history as one of the old school, a true gentlemen.	0
8256	8256	S13-5	Subtask 5b: Semantic Compositionality in Context	5	139	0.294117647058823	0.688118811881188	vs. During the 1970's the hall of the old school was converted into the library.	0
8257	8257	S13-5	Subtask 5b: Semantic Compositionality in Context	6	140	0.352941176470588	0.693069306930693	Being able to detect whether a phrase is used literally or figuratively is e.g. especially important for information retrieval, where figuratively used words should be treated separately to avoid false positives.	0
8258	8258	S13-5	Subtask 5b: Semantic Compositionality in Context	7	141	0.411764705882353	0.698019801980198	For example, the example sentence	0
8259	8259	S13-5	Subtask 5b: Semantic Compositionality in Context	8	142	0.470588235294118	0.702970297029703	He will go down in history as one of the old school, a true gentlemen.	0
8260	8260	S13-5	Subtask 5b: Semantic Compositionality in Context	9	143	0.529411764705882	0.707920792079208	"should probably not be retrieved for the query ""school""."	0
8261	8261	S13-5	Subtask 5b: Semantic Compositionality in Context	10	144	0.588235294117647	0.712871287128713	"Rather, the insights generated from subtask 5a could be utilized to retrieve sentences using a similar phrase such as ""gentleman-like behavior""."	0
8262	8262	S13-5	Subtask 5b: Semantic Compositionality in Context	11	145	0.647058823529412	0.717821782178218	The task may also be of interest to the related research fields of metaphor detection and idiom identification.	0
8263	8263	S13-5	Subtask 5b: Semantic Compositionality in Context	12	146	0.705882352941176	0.722772277227723	There were no restrictions regarding the array of methods, and the kind of resources that could be employed for this task.	0
8264	8264	S13-5	Subtask 5b: Semantic Compositionality in Context	13	147	0.764705882352941	0.727722772277228	In particular, participants were allowed to make use of pre-fabricated lists of phrases annotated with their probability of being used figuratively from publicly available sources, or to produce these lists from corpora.	0
8265	8265	S13-5	Subtask 5b: Semantic Compositionality in Context	14	148	0.823529411764706	0.732673267326733	Assessing how well the phrase suits its context might be tackled using e.g. measures of semantic relatedness as well as distributional models learned from the underlying corpus.	0
8266	8266	S13-5	Subtask 5b: Semantic Compositionality in Context	15	149	0.882352941176471	0.737623762376238	Participants of this subtask were provided with real usage examples of target phrases.	0
8267	8267	S13-5	Subtask 5b: Semantic Compositionality in Context	16	150	0.941176470588235	0.742574257425743	For each usage example, the task is to make a binary decision whether the target phrase is used literally or figu-ratively in this context.	0
8268	8268	S13-5	Subtask 5b: Semantic Compositionality in Context	17	151	1.0	0.747524752475248	Systems were tested in two different disciplines: a known phrases task where all target phrases in the test set were contained in the training, and an unknown phrases setting, where all target phrases in the test set were unseen.	0
8269	8269	S13-5	Data Creation	1	152	0.076923076923077	0.752475247524752	The first step in creating the corpus was to compile a list of phrases that can be used either literally or metaphorically.	0
8270	8270	S13-5	Data Creation	2	153	0.153846153846154	0.757425742574257	Thus, we created an initial list of several thousand English idioms from Wiktionary by listing all entries under the category ENGLISH ID-IOMS using the JWKTL Wiktionary API (Zesch et al., 2008).	0
8271	8271	S13-5	Data Creation	3	154	0.230769230769231	0.762376237623762	We manually filtered the list removing most idioms that are very unlikely to be ever used literally (anymore), e.g. to knock on heaven's door.	0
8272	8272	S13-5	Data Creation	4	155	0.307692307692308	0.767326732673267	For each of the resulting list of phrases, we extracted usage contexts from the ukWaC corpus (Baroni et al., 2009).	0
8273	8273	S13-5	Data Creation	5	156	0.384615384615385	0.772277227722772	Each usage context contains 5 sentences, where the sentence with the target phrase appears in a randomized position.	0
8274	8274	S13-5	Data Creation	6	157	0.461538461538462	0.777227722772277	Due to segmentation errors, some usage contexts actually might contain less than 5 sentences, but we manually filtered all usage contexts where the remaining context was insufficient.	0
8275	8275	S13-5	Data Creation	7	158	0.538461538461538	0.782178217821782	This was done in the final cleaning step where we also manually removed (near) duplicates, obvious spam, encoding problems etc.	0
8276	8276	S13-5	Data Creation	8	159	0.615384615384615	0.787128712871287	The target phrases in context were annotated for figurative, literal, both or impossible to tell usage, using the CrowdFlower 2 crowdsourcing annotation platform.	0
8277	8277	S13-5	Data Creation	9	160	0.692307692307692	0.792079207920792	"We used about 8% of items as ""gold"" items for quality assurance, and had each example annotated by three crowdworkers."	0
8278	8278	S13-5	Data Creation	10	161	0.769230769230769	0.797029702970297	The task was comparably easy for crowdworkers, who reached 90%-94% pairwise agreement, and 95% success on the gold items.	0
8279	8279	S13-5	Data Creation	11	162	0.846153846153846	0.801980198019802	About 5% of items with low agreement and marked as impossible were removed.	0
8280	8280	S13-5	Data Creation	12	163	0.923076923076923	0.806930693069307	Table 3 summarizes the quantitative characteristics of all datasets resulting from this process.	0
8281	8281	S13-5	Data Creation	13	164	1.0	0.811881188118812	We took care in sampling the data as to keep similar distributions across the training, development and testing parts.	0
8282	8282	S13-5	Results	1	165	0.1	0.816831683168317	Training and development datasets were made available in advance, test data was provided during the evaluation period without labels.	0
8283	8283	S13-5	Results	2	166	0.2	0.821782178217822	System perfor-    mance was measured in accuracy.	0
8284	8284	S13-5	Results	3	167	0.3	0.826732673267327	Since all participants provided classifications for all test items, the accuracy score is equivalent to precision/recall/F1.	0
8285	8285	S13-5	Results	4	168	0.4	0.831683168316832	Participants were allowed to enter up to three different runs for evaluation.	0
8286	8286	S13-5	Results	5	169	0.5	0.836633663366337	We also provide baseline accuracy scores, which are obtained by always assigning the most frequent class (figurative).	0
8287	8287	S13-5	Results	6	170	0.6	0.841584158415841	Table 4 provides the evaluation results for the known phrases task, while Table 5 ranks participants for the unseen phrases task.	0
8288	8288	S13-5	Results	7	171	0.7	0.846534653465347	As expected, the unseen phrases setting is much harder than the known phrases setting, as for unseen phrases it is not possible to learn lexicalised contextual clues.	0
8289	8289	S13-5	Results	8	172	0.8	0.851485148514851	In both settings, the winning entries were able to beat the MFC baseline.	0
8290	8290	S13-5	Results	9	173	0.9	0.856435643564356	While performance in the known phrases setting is close to 80% and thus acceptable, the general task of recognizing the literal or figurative use of unseen phrases remains very challenging, with only a small improvement over the baseline.	0
8291	8291	S13-5	Results	10	174	1.0	0.861386138613861	We refer to the system descriptions for more details on the techniques used for this subtask: UNAL (Jimenez et al., 2013), IIRG (Byrne et al., 2013) and CLaC (Siblini and Kosseim, 2013).	0
8292	8292	S13-5	Task Conclusions	1	175	0.05	0.866336633663366	"In this section, we further discuss the findings and conclusion of the evaluation challenge in the task of ""Phrasal Semantics""."	0
8293	8293	S13-5	Task Conclusions	2	176	0.1	0.871287128712871	Looking at the results of both subtasks, one observes that the maximum performance achieved is higher for the first than the second subtask.	0
8294	8294	S13-5	Task Conclusions	3	177	0.15	0.876237623762376	For this comparison to be fair, trivial baselines should be taken into account.	0
8295	8295	S13-5	Task Conclusions	4	178	0.2	0.881188118811881	A system randomly assigning an output value would be on average 50% correct in the first subtask, since the numbers of positive and negative instances in the testing set are equal.	0
8296	8296	S13-5	Task Conclusions	5	179	0.25	0.886138613861386	Similarly, a system assigning the most frequent class, i.e. the figurative use of any phrase, would be 50.3% and 61.6% accurate in the second subtask for seen and unseen test instances, respectively.	0
8297	8297	S13-5	Task Conclusions	6	180	0.3	0.891089108910891	It should also be noted that the testing instances in the first subtask are unseen in the respective training set.	0
8298	8298	S13-5	Task Conclusions	7	181	0.35	0.896039603960396	As a result, in terms of baselines, the second subtask on unseen data (Table 5) should be considered easier than the first subtask (Table 2).	0
8299	8299	S13-5	Task Conclusions	8	182	0.4	0.900990099009901	However, the best performing systems achieved much higher accuracy in the first than in the second subtask.	0
8300	8300	S13-5	Task Conclusions	9	183	0.45	0.905940594059406	This contradiction confirms our conception that the first subtask is less complex than the second.	0
8301	8301	S13-5	Task Conclusions	10	184	0.5	0.910891089108911	In the first subtask, it is evident that no method performs much better or much worse than the others.	0
8302	8302	S13-5	Task Conclusions	11	185	0.55	0.915841584158416	Although the participating systems have employed a wide variety of approaches and tools, the difference between the best and worst accuracy achieved is relatively limited, in particular approximately 14%.	0
8303	8303	S13-5	Task Conclusions	12	186	0.6	0.920792079207921	Even more interestingly, unsupervised approaches performed better than some supervised ones.	0
8304	8304	S13-5	Task Conclusions	13	187	0.65	0.925742574257426	"This observation suggests that no ""golden recipe"" has been identified so far for this task."	0
8305	8305	S13-5	Task Conclusions	14	188	0.7	0.930693069306931	Thus, probably different processing tools take advantage of different sources of information.	0
8306	8306	S13-5	Task Conclusions	15	189	0.75	0.935643564356436	It is a matter of future research to identify these sources and the corresponding tools, and then develop hybrid methods of improved performance.	0
8307	8307	S13-5	Task Conclusions	16	190	0.8	0.940594059405941	In the second subtask, the results of evaluation on known phrases are much higher than on unseen phrases.	0
8308	8308	S13-5	Task Conclusions	17	191	0.85	0.945544554455445	This was expected, as for unseen phrases it is not possible to learn lexicalised contextual clues.	0
8309	8309	S13-5	Task Conclusions	18	192	0.9	0.95049504950495	Thus, the second subtask has succeeded in identifying the complexity threshold up to which the current state-of-the-art can address the computational problem.	0
8310	8310	S13-5	Task Conclusions	19	193	0.95	0.955445544554455	Further than this threshold, i.e. for unseen phrases, current systems have not yet succeeded in addressing it.	0
8311	8311	S13-5	Task Conclusions	20	194	1.0	0.96039603960396	In conclusion, the difficulty in evaluating the compositionality of previously unseen phrases in context highlights the overall complexity of the second subtask.	0
8312	8312	S13-5	Summary and Future Work	1	195	0.125	0.965346534653465	"In this paper we have presented the 5 th task of Se-mEval 2013, ""Evaluating Phrasal Semantics"", which consists of two subtasks: (1) semantic similarity of words and compositional phrases, and (2) compositionality of phrases in context."	0
8313	8313	S13-5	Summary and Future Work	2	196	0.25	0.97029702970297	The former subtask, which focussed on the first step of composing the meaning of phrases of any length, is less complex than the latter subtask, which considers the effect of context to the semantics of a phrase.	0
8314	8314	S13-5	Summary and Future Work	3	197	0.375	0.975247524752475	The paper presents details about the background and importance of these subtasks, the data creation process, the systems that took part in the evaluation and their results.	0
8315	8315	S13-5	Summary and Future Work	4	198	0.5	0.98019801980198	In the future, we expect evaluation challenges on phrasal semantics to progress towards two directions: (a) the synthesis of semantics of sequences longer than two words, and (b) aiming to improve the performance of systems that determine the compositionality of previously unseen phrases in con-text.	0
8316	8316	S13-5	Summary and Future Work	5	199	0.625	0.985148514851485	The evaluation results of the first task suggest that state-of-the-art systems can compose the semantics of two word sequences with a promising level of success.	0
8317	8317	S13-5	Summary and Future Work	6	200	0.75	0.99009900990099	However, this task should be seen as the first step towards composing the semantics of sentence-long sequences.	0
8318	8318	S13-5	Summary and Future Work	7	201	0.875	0.995049504950495	As far as subtask 5b is concerned, the accuracy achieved by the participating systems on unseen testing data was low, only slightly better than the most frequent class baseline, which assigns the figurative use to all test phrases.	0
8319	8319	S13-5	Summary and Future Work	8	202	1.0	1.0	Thus, the subtask cannot be considered well addressed by the state-of-the-art and further progress should be sought.	0
9049	9049	S13-11	title	1	1	1.0	0.00763358778626	SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application	0
9050	9050	S13-11	abstract	1	2	0.333333333333333	0.015267175572519	In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification.	0
9051	9051	S13-11	abstract	2	3	0.666666666666667	0.022900763358779	Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.	1
9052	9052	S13-11	abstract	3	4	1.0	0.030534351145038	The task enables the end-to-end evaluation and comparison of systems.	0
9053	9053	S13-11	Introduction	1	5	0.05	0.038167938931298	Word ambiguity is a pervasive issue in Natural Language Processing.	0
9054	9054	S13-11	Introduction	2	6	0.1	0.045801526717557	Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009;	0
9055	9055	S13-11	Introduction	3	7	0.15	0.053435114503817	Navigli, 2012) for a survey).	0
9056	9056	S13-11	Introduction	4	8	0.2	0.061068702290076	Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications.	0
9057	9057	S13-11	Introduction	5	9	0.25	0.068702290076336	In fact, the performance of WSD systems depends heavily on which sense inventory is chosen.	0
9058	9058	S13-11	Introduction	6	10	0.3	0.076335877862596	For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002;	0
9059	9059	S13-11	Introduction	7	11	0.35	0.083969465648855	Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008).	0
9060	9060	S13-11	Introduction	8	12	0.4	0.091603053435115	On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses.	0
9061	9061	S13-11	Introduction	9	13	0.45	0.099236641221374	In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output.	0
9062	9062	S13-11	Introduction	10	14	0.5	0.106870229007634	Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.	0
9063	9063	S13-11	Introduction	11	15	0.55	0.114503816793893	Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively.	0
9064	9064	S13-11	Introduction	12	16	0.6	0.122137404580153	A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds.	0
9065	9065	S13-11	Introduction	13	17	0.65	0.129770992366412	Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010;	0
9066	9066	S13-11	Introduction	14	18	0.7	0.137404580152672	Di Marco and Navigli, 2013).	0
9067	9067	S13-11	Introduction	15	19	0.75	0.145038167938931	In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.	0
9068	9068	S13-11	Introduction	16	20	0.8	0.152671755725191	The Semeval-2013 task described in this paper 1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems.	0
9069	9069	S13-11	Introduction	17	21	0.85	0.16030534351145	The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre and Soroa, 2007;Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms.	0
9070	9070	S13-11	Introduction	18	22	0.9	0.16793893129771	Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning).	0
9071	9071	S13-11	Introduction	19	23	0.95	0.175572519083969	For instance, consider the target query apple and the following 3 search result snippets:	0
9072	9072	S13-11	Introduction	20	24	1.0	0.183206106870229	1. Apple Inc., formerly Apple Computer, Inc., is...	0
9073	9073	S13-11	2	1	25	1.0	0.190839694656489	The science of apple growing is called pomology...	0
9074	9074	S13-11	3	1	26	0.5	0.198473282442748	Apple designs and creates iPod and iTunes...	0
9075	9075	S13-11	3	2	27	1.0	0.206106870229008	Participating systems were requested to produce a clustering that groups snippets conveying the same meaning of the input query apple, i.e., ideally {1, 3} and {2} in the above example.	0
9076	9076	S13-11	Task setup	1	28	0.333333333333333	0.213740458015267	For each ambiguous query the task required participating systems to cluster the top ranking snippets returned by a search engine (we used the Google Search API).	0
9077	9077	S13-11	Task setup	2	29	0.666666666666667	0.221374045801527	WSI systems were required to identify the meanings of the input query and cluster the snippets into semantically-related groups according to their meanings.	0
9078	9078	S13-11	Task setup	3	30	1.0	0.229007633587786	Instead, WSD systems were requested to sense-tag the given snippets with the appropriate senses of the input query, thereby implicitly determining a clustering of snippets (i.e., one cluster per sense).	0
9079	9079	S13-11	Dataset	1	31	0.076923076923077	0.236641221374046	We created a dataset of 100 ambiguous queries.	0
9080	9080	S13-11	Dataset	2	32	0.153846153846154	0.244274809160305	The queries were randomly sampled from the AOL search logs so as to ensure that they had been used in real search sessions.	0
9081	9081	S13-11	Dataset	3	33	0.230769230769231	0.251908396946565	Following previous work on the topic Di Marco and Navigli, 2013) we selected those queries for which a sense inventory exists as a disambiguation page in the English Wikipedia 2 .	0
9082	9082	S13-11	Dataset	4	34	0.307692307692308	0.259541984732824	This guaranteed that the selected queries consisted of either a single word or a multiword expression for which we had a collaborativelyedited list of meanings, including lexicographic and encyclopedic ones.	0
9083	9083	S13-11	Dataset	5	35	0.384615384615385	0.267175572519084	We discarded all queries made up of &gt; 4 words, since the length of the great majority of queries lay in the range [1,4].	0
9084	9084	S13-11	Dataset	6	36	0.461538461538462	0.274809160305344	In Table 1 we compare the percentage distribution of 1-to 4-word queries in the AOL query logs against our dataset of queries.	0
9085	9085	S13-11	Dataset	7	37	0.538461538461538	0.282442748091603	Note that we increased the percentage of 3-and 4-word queries in order to have a significant coverage of those lengths.	0
9086	9086	S13-11	Dataset	8	38	0.615384615384615	0.290076335877863	Anyhow, in both cases most queries contained from 1 to 2 words.	0
9087	9087	S13-11	Dataset	9	39	0.692307692307692	0.297709923664122	Note that the reported percentage distributions of query length is different from recent statistics for two reasons: first, over the years users have increased the average number of words per query in order to refine their searches; second, we selected only queries which were either single words (e.g., apple) or multi-word expressions (e.g., mortal kombat), thereby discarding several long queries composed of different words (such as angelina jolie actress).	0
9088	9088	S13-11	Dataset	10	40	0.769230769230769	0.305343511450382	Finally, we submitted each query to Google search and retrieved the 64 top-ranking results returned for each query.	0
9089	9089	S13-11	Dataset	11	41	0.846153846153846	0.312977099236641	Therefore, overall the dataset consists of 100 queries and 6,400 results.	0
9090	9090	S13-11	Dataset	12	42	0.923076923076923	0.320610687022901	Each search result includes the following information: page title, URL of the page and snippet of the page text.	0
9091	9091	S13-11	Dataset	13	43	1.0	0.32824427480916	We show an example of search result for the apple query in Figure 1.	0
9092	9092	S13-11	Dataset Annotation	1	44	0.090909090909091	0.33587786259542	For each query q we used Amazon Mechanical Turk 3 to annotate each query result with the most suitable sense.	0
9093	9093	S13-11	Dataset Annotation	2	45	0.181818181818182	0.343511450381679	The sense inventory for q was obtained by listing the senses available in the Wikipedia disambiguation page of q augmented with additional options from the classes obtained from the section headings of the disambiguation page plus the OTHER catch-all meaning.	0
9094	9094	S13-11	Dataset Annotation	3	46	0.272727272727273	0.351145038167939	For instance, consider the apple query.	0
9095	9095	S13-11	Dataset Annotation	4	47	0.363636363636364	0.358778625954198	We show its disambiguation page in Figure 2.	0
9096	9096	S13-11	Dataset Annotation	5	48	0.454545454545455	0.366412213740458	The sense inventory for apple was made up of the senses listed in that page (e.g., MALUS, APPLE INC., APPLE BANK, etc.) plus the set of generic classes OTHER PLANTS AND PLANT PARTS, OTHER COMPANIES, OTHER FILMS, plus OTHER.	0
9097	9097	S13-11	Dataset Annotation	6	49	0.545454545454545	0.374045801526718	For each query we ensured that three annotators tagged each of the 64 results for that query with the most suitable sense among those in the sense inventory (selecting OTHER if no sense was appropriate).	0
9098	9098	S13-11	Dataset Annotation	7	50	0.636363636363636	0.381679389312977	"Specifically, each Turker was provided with the following instructions: ""The goal is annotating the search result snippets returned by Google for a given query with the appropriate meaning among those available (obtained from the Wikipedia disambiguation page for the query)."	0
9099	9099	S13-11	Dataset Annotation	8	51	0.727272727272727	0.389312977099237	"You have to select the meaning that you consider most appropriate""."	0
9100	9100	S13-11	Dataset Annotation	9	52	0.818181818181818	0.396946564885496	No constraint on the age, gender and citizenship of the annotators was imposed.	0
9101	9101	S13-11	Dataset Annotation	10	53	0.909090909090909	0.404580152671756	However, in order to avoid random tagging of search results, we provided 3 gold-standard result annotations per query, which could be shown to the Turker more than once during the annotation process.	0
9102	9102	S13-11	Dataset Annotation	11	54	1.0	0.412213740458015	In the case (s)he failed to annotate the gold items, the annotator was automatically excluded.	0
9103	9103	S13-11	Inter-Annotator Agreement and Adjudication	1	55	0.142857142857143	0.419847328244275	In order to determine the reliability of the Turkers' annotations, we calculated the individual values of Fleiss' kappa κ (Fleiss, 1971) for each query q and then averaged them:	0
9104	9104	S13-11	Inter-Annotator Agreement and Adjudication	2	56	0.285714285714286	0.427480916030534	where κ q is the Fleiss' kappa agreement of the three annotators who tagged the 64 snippets returned by the Google search engine for the query q ∈ Q, and Q is our set of 100 queries.	0
9105	9105	S13-11	Inter-Annotator Agreement and Adjudication	3	57	0.428571428571429	0.435114503816794	We obtained an average value of κ = 0.66, which according to Landis and Koch (1977) can be seen as substantial agreement, with a standard deviation σ = 0.185.	0
9106	9106	S13-11	Inter-Annotator Agreement and Adjudication	4	58	0.571428571428571	0.442748091603053	In Table 2 we show the agreement distribution of our 6400 snippets, distinguishing between full agreement (3 out of 3), majority agreement (2 out of 3), and no agreement.	0
9107	9107	S13-11	Inter-Annotator Agreement and Adjudication	5	59	0.714285714285714	0.450381679389313	Most of the items were annotated with full or majority agreement, indicating that the manual annotation task was generally doable for the layman.	0
9108	9108	S13-11	Inter-Annotator Agreement and Adjudication	6	60	0.857142857142857	0.458015267175573	We manually checked all the cases of majority agreement, correcting only 7.92% of the majority adjudications, and manually adjudicated all the snippets for which there was no agreement.	0
9109	9109	S13-11	Inter-Annotator Agreement and Adjudication	7	61	1.0	0.465648854961832	We observed during adjudication that in many cases the disagreement was due to the existence of subtle sense distinctions, like between MORTAL KOM-BAT (VIDEO GAME) and MORTAL KOMBAT (2011 VIDEO GAME)  per query on average).	0
9110	9110	S13-11	Scoring	1	62	0.25	0.473282442748092	Following Di Marco and Navigli (2013), we evaluated the systems' outputs in terms of the snippet clustering quality (Section 3.1) and the snippet diversification quality (Section 3.2).	0
9111	9111	S13-11	Scoring	2	63	0.5	0.480916030534351	Given a query q ∈ Q and the corresponding set of 64 snippet results, let C be the clustering output by a given system and let G be the gold-standard clustering for those results.	0
9112	9112	S13-11	Scoring	3	64	0.75	0.488549618320611	Each measure M (C, G) presented below is calculated for the query q using these two clusterings.	0
9113	9113	S13-11	Scoring	4	65	1.0	0.49618320610687	The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of M (C, G) obtained for each single test query q ∈ Q.	0
9114	9114	S13-11	Clustering Quality	1	66	0.037037037037037	0.50381679389313	The first evaluation concerned the quality of the clusters produced by the participating systems.	0
9115	9115	S13-11	Clustering Quality	2	67	0.074074074074074	0.511450381679389	Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely:	0
9116	9116	S13-11	Clustering Quality	3	68	0.111111111111111	0.519083969465649	• Rand Index (Rand, 1971);	0
9117	9117	S13-11	Clustering Quality	4	69	0.148148148148148	0.526717557251908	• Adjusted Rand Index (Hubert and Arabie, 1985);	0
9118	9118	S13-11	Clustering Quality	5	70	0.185185185185185	0.534351145038168	• Jaccard Index (Jaccard, 1901);	0
9119	9119	S13-11	Clustering Quality	6	71	0.222222222222222	0.541984732824427	• F1 measure (van Rijsbergen, 1979).	0
9120	9120	S13-11	Clustering Quality	7	72	0.259259259259259	0.549618320610687	The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows:	0
9121	9121	S13-11	Clustering Quality	8	73	0.296296296296296	0.557251908396947	where TP is the number of true positives, i.e., snippet pairs which are in the same cluster both in C and G, TN is the number of true negatives, i.e., pairs which are in different clusters in both clusterings, and FP and FN are, respectively, the number of false positives and false negatives.	0
9122	9122	S13-11	Clustering Quality	9	74	0.333333333333333	0.564885496183206	RI ranges between 0 and 1, where 1 indicates perfect correspondence.	0
9123	9123	S13-11	Clustering Quality	10	75	0.37037037037037	0.572519083969466	Adjusted Rand Index (ARI) is a development of Rand Index which corrects the RI for chance agreement and makes it vary according to expectaction:	0
9124	9124	S13-11	Clustering Quality	11	76	0.407407407407407	0.580152671755725	.	0
9125	9125	S13-11	Clustering Quality	12	77	0.444444444444444	0.587786259541985	(3) where E(RI(C, G)) is the expected value of the RI.	0
9126	9126	S13-11	Clustering Quality	13	78	0.481481481481481	0.595419847328244	Using the contingency table reported in Table 3 we can quantify the degree of overlap between C and G, where n ij denotes the number of snippets in common between G i and C j (namely, n ij = |G i ∩ C j |), a i and b j represent, respectively, the number of snippets in G i and C j , and N is the total number of snippets, i.e., N = 64.	0
9127	9127	S13-11	Clustering Quality	14	79	0.518518518518518	0.603053435114504	Now, the above equation can be reformulated as:	0
9128	9128	S13-11	Clustering Quality	15	80	0.555555555555556	0.610687022900763	.	0
9129	9129	S13-11	Clustering Quality	16	81	0.592592592592593	0.618320610687023	(4)	0
9130	9130	S13-11	Clustering Quality	17	82	0.62962962962963	0.625954198473282	The ARI ranges between −1 and +1 and is 0 when the index equals its expected value.	0
9131	9131	S13-11	Clustering Quality	18	83	0.666666666666667	0.633587786259542	Jaccard Index (JI) is a measure which takes into account only the snippet pairs which are in the same cluster both in C and G, i.e., the true positives (TP), while neglecting true negatives (TN), which are the vast majority of cases.	0
9132	9132	S13-11	Clustering Quality	19	84	0.703703703703704	0.641221374045801	JI is calculated as follows:	0
9133	9133	S13-11	Clustering Quality	20	85	0.740740740740741	0.648854961832061	Finally, the F1 measure calculates the harmonic mean of precision (P) and recall (R).	0
9134	9134	S13-11	Clustering Quality	21	86	0.777777777777778	0.656488549618321	Precision determines how accurately the clusters of C represent the query meanings in the gold standard G, whereas recall measures how accurately the different meanings in G are covered by the clusters in C. We follow Crabtree et al. (2005) and define the precision of a cluster C j ∈ C as follows:	0
9135	9135	S13-11	Clustering Quality	22	87	0.814814814814815	0.66412213740458	where C s j is the intersection between C j ∈ C and the gold cluster G s ∈ G which maximizes the cardinality of the intersection.	0
9136	9136	S13-11	Clustering Quality	23	88	0.851851851851852	0.67175572519084	The recall of a query sense s is instead calculated as:	0
9137	9137	S13-11	Clustering Quality	24	89	0.888888888888889	0.679389312977099	where C s is the subset of clusters of C whose majority sense is s, and n s is the number of snippets tagged with query sense s in the gold standard.	0
9138	9138	S13-11	Clustering Quality	25	90	0.925925925925926	0.687022900763359	The total precision and recall of the clustering C are then calculated as:	0
9139	9139	S13-11	Clustering Quality	26	91	0.962962962962963	0.694656488549618	where S is the set of senses in the gold standard G for the given query (i.e., |S| = |G|).	0
9140	9140	S13-11	Clustering Quality	27	92	1.0	0.702290076335878	The two values of P and R are then combined into their harmonic mean, namely the F1 measure:	0
9141	9141	S13-11	Clustering Diversity	1	93	0.090909090909091	0.709923664122137	Our second evaluation is aimed at determining the impact of the output clustering on the diversification of the top results shown to a Web user.	0
9142	9142	S13-11	Clustering Diversity	2	94	0.181818181818182	0.717557251908397	To this end, we applied an automatic procedure for flattening the clusterings produced by the participating systems to a list of search results.	0
9143	9143	S13-11	Clustering Diversity	3	95	0.272727272727273	0.725190839694657	Given a clustering C = (C 1 , C 2 , . . . , C m ), we add to the initially empty list the first element of each cluster C j (j = 1, . . . , m); then we iterate the process by selecting the second element of each cluster C j such that |C j | ≥ 2, and so on.	0
9144	9144	S13-11	Clustering Diversity	4	96	0.363636363636364	0.732824427480916	The remaining elements returned by the search engine, but not included in any cluster of C, are appended to the bottom of the list in their original order.	0
9145	9145	S13-11	Clustering Diversity	5	97	0.454545454545455	0.740458015267176	Note that systems were asked to sort snippets within clusters, as well as clusters themselves, by relevance.	0
9146	9146	S13-11	Clustering Diversity	6	98	0.545454545454545	0.748091603053435	Since our goal is to determine how many different meanings are covered by the top-ranking search results according to the output clustering, we used the measures of S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at recall r) (Zhai et al., 2003).	0
9147	9147	S13-11	Clustering Diversity	7	99	0.636363636363636	0.755725190839695	S-recall@	0
9148	9148	S13-11	Clustering Diversity	8	100	0.727272727272727	0.763358778625954	K determines the ratio of different meanings for a given query q in the top-K results returned:	0
9149	9149	S13-11	Clustering Diversity	9	101	0.818181818181818	0.770992366412214	where sense(r i ) is the gold-standard sense associated with the i-th snippet returned by the system, and g is the total number of distinct senses for the query q in our gold standard.	0
9150	9150	S13-11	Clustering Diversity	10	102	0.909090909090909	0.778625954198473	S-precision@r instead determines the ratio of different senses retrieved for query q in the first K r snippets, where K r is the minimum number of top results for which the system achieves recall r.	0
9151	9151	S13-11	Clustering Diversity	11	103	1.0	0.786259541984733	The measure is defined as follows:	0
9152	9152	S13-11	Baselines	1	104	0.071428571428572	0.793893129770992	We compared the participating systems with two simple baselines:	0
9153	9153	S13-11	Baselines	2	105	0.142857142857143	0.801526717557252	• SINGLETONS: each snippet is clustered as a separate singleton cluster (i.e., |C| = 64).	0
9154	9154	S13-11	Baselines	3	106	0.214285714285714	0.809160305343511	• ALL-IN-ONE: all snippets are clustered into a single cluster (i.e., |C| = 1).	0
9155	9155	S13-11	Baselines	4	107	0.285714285714286	0.816793893129771	These baselines are important in that they make explicit the preference of certain quality measures towards clusterings made up with a small or large number of clusters.	0
9156	9156	S13-11	Baselines	5	108	0.357142857142857	0.824427480916031	4 Systems 5 teams submitted 10 systems, out of which 9 were WSI systems, while 1 was a WSD system, i.e., using the Wikipedia sense inventory for performing the disambiguation task.	0
9157	9157	S13-11	Baselines	6	109	0.428571428571429	0.83206106870229	All systems could exploit the information provided for each search result, i.e., URL, page title and result snippet.	0
9158	9158	S13-11	Baselines	7	110	0.5	0.839694656488549	WSI systems were requested to use unannotated corpora only.	0
9159	9159	S13-11	Baselines	8	111	0.571428571428571	0.847328244274809	We asked each team to provide information about their systems.	0
9160	9160	S13-11	Baselines	9	112	0.642857142857143	0.854961832061069	In Table 4 we report the resources used by each system.	0
9161	9161	S13-11	Baselines	10	113	0.714285714285714	0.862595419847328	The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-PK2 both use the snippets for inducing the query senses.	0
9162	9162	S13-11	Baselines	11	114	0.785714285714286	0.870229007633588	Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes.	0
9163	9163	S13-11	Baselines	12	115	0.857142857142857	0.877862595419847	They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection 6 (Biemann et al., 2007).	0
9164	9164	S13-11	Baselines	13	116	0.928571428571429	0.885496183206107	SATTY-APPROACH1 just uses snippets.	0
9165	9165	S13-11	Baselines	14	117	1.0	0.893129770992366	The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009).	0
9166	9166	S13-11	Results	1	118	0.071428571428572	0.900763358778626	We show the results of RI and ARI in Table 5.	0
9167	9167	S13-11	Results	2	119	0.142857142857143	0.908396946564885	The best performing systems are those from the HDP team, with considerably higher RI and ARI.	0
9168	9168	S13-11	Results	3	120	0.214285714285714	0.916030534351145	The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH.	0
9169	9169	S13-11	Results	4	121	0.285714285714286	0.923664122137405	SINGLETONS perform well with RI, but badly when chance agreement is taken into account.	0
9170	9170	S13-11	Results	5	122	0.357142857142857	0.931297709923664	As for F1 and JI, whose values are shown in Table 6, the two HDP systems again perform best in terms of F1, and are on par with UKP-WSI-WACKY-LLR in terms of JI.	0
9171	9171	S13-11	Results	6	123	0.428571428571429	0.938931297709924	The third best approach in terms of F1 is again SATTY-APPROACH1, which however per-	0
9172	9172	S13-11	Results	7	124	0.5	0.946564885496183	To get more insights into the performance of the various systems, we calculated the average number of clusters per clustering produced by each system and compared it with the gold standard average.	0
9173	9173	S13-11	Results	8	125	0.571428571428571	0.954198473282443	We also computed the average cluster size, i.e., the average number of snippets per cluster.	0
9174	9174	S13-11	Results	9	126	0.642857142857143	0.961832061068702	The statistics are shown in Table 7.	0
9175	9175	S13-11	Results	10	127	0.714285714285714	0.969465648854962	Interestingly, the best performing systems are those with the cluster number and average number of clusters closest to the gold standard ones.	0
9176	9176	S13-11	Results	11	128	0.785714285714286	0.977099236641221	This finding is also confirmed by Figure 3, where we draw each system according to its average values regarding cluster number and size: again the distance from the gold standard is meaningful.	0
9177	9177	S13-11	Results	12	129	0.857142857142857	0.984732824427481	We now move to the diversification perfor-    Our annotation experience showed that the Wikipedia sense inventory, augmented with our generic classes, is a good choice for semantically tagging search results, in that it covers most of the meanings a Web user might be interested in.	0
9178	9178	S13-11	Results	13	130	0.928571428571429	0.992366412213741	In fact, only 20% of the snippets was annotated with the OTHER class.	0
9179	9179	S13-11	Results	14	131	1.0	1.0	Future work might consider large-scale multilingual lexical resources, such as BabelNet (Navigli and Ponzetto, 2012), both as sense inventory and for performing the search result clustering and diversification task.	0
9369	9369	S13-13	title	1	1	1.0	0.005235602094241	SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses	0
9370	9370	S13-13	abstract	1	2	0.25	0.010471204188482	Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense.	0
9371	9371	S13-13	abstract	2	3	0.5	0.015706806282723	However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage.	0
9372	9372	S13-13	abstract	3	4	0.75	0.020942408376963	We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability.	0
9373	9373	S13-13	abstract	4	5	1.0	0.026178010471204	Four teams submitted nine systems, which were evaluated in two settings.	0
9374	9374	S13-13	Introduction	1	6	0.0625	0.031413612565445	Word Sense Disambiguation (WSD) attempts to identify which of a word's meanings applies in a given context.	0
9375	9375	S13-13	Introduction	2	7	0.125	0.036649214659686	A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009).	0
9376	9376	S13-13	Introduction	3	8	0.1875	0.041884816753927	Typically, each usage of a word is treated as expressing only a single sense.	0
9377	9377	S13-13	Introduction	4	9	0.25	0.047120418848168	However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations.	0
9378	9378	S13-13	Introduction	5	10	0.3125	0.052356020942408	Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (Véronis, 1998;Murray and Green, 2004;	0
9379	9379	S13-13	Introduction	6	11	0.375	0.057591623036649	Passonneau et al., 2012b;Jurgens, 2013;Navigli et al., 2013).	0
9380	9380	S13-13	Introduction	7	12	0.4375	0.06282722513089	Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled.	0
9381	9381	S13-13	Introduction	8	13	0.5	0.068062827225131	Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability.	0
9382	9382	S13-13	Introduction	9	14	0.5625	0.073298429319372	WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD systems for specific types of text (e.g., social media), or the inventory may lack domain-specific senses.	0
9383	9383	S13-13	Introduction	10	15	0.625	0.078534031413613	Word Sense Induction (WSI) has been proposed as a method for overcoming such limitations by learning the senses automatically from text.	0
9384	9384	S13-13	Introduction	11	16	0.6875	0.083769633507853	In essence, a WSI algorithm acts as a lexicographer by grouping word usages according to their shared meaning.	0
9385	9385	S13-13	Introduction	12	17	0.75	0.089005235602094	The second goal of this task is to assess the performance of WSI algorithms when they are able to model multiple meanings of a usage with graded senses.	0
9386	9386	S13-13	Introduction	13	18	0.8125	0.094240837696335	Task 12 focuses on disambiguating senses for 50 target lemmas: 20 nouns, 20 verbs, and 10 adjectives (Sec. 2).	0
9387	9387	S13-13	Introduction	14	19	0.875	0.099476439790576	Since the Task evaluates only unsupervised systems, no training data was provided; however, to enable more comparison, Unsupervised WSD systems were also allowed to participate.	0
9388	9388	S13-13	Introduction	15	20	0.9375	0.104712041884817	Participating systems were evaluated in two settings (Sec. 3), depending on whether they used induced senses or WordNet 3.1 senses for their annotations.	0
9389	9389	S13-13	Introduction	16	21	1.0	0.109947643979058	The results (Sec. 5) demonstrate a substantial improvement over the competitive most frequent sense baseline.	0
9390	9390	S13-13	Task Description	1	22	0.090909090909091	0.115183246073298	This task required participating systems to annotate instances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected due to its fine-grained senses.	1
9391	9391	S13-13	Task Description	2	23	0.181818181818182	0.120418848167539	Participants could label each instance with one or more senses, weighting	1
9392	9392	S13-13	Task Description	3	24	0.272727272727273	0.12565445026178	We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to encompass the thoughts of every entity we know.	0
9393	9393	S13-13	Task Description	4	25	0.363636363636364	0.130890052356021	dark%3:00:01:: -devoid of or deficient in light or brightness; shadowed or black dark%3:00:00:: -secret I ask because my practice has always been to allow about five minutes grace, then remove it.	0
9394	9394	S13-13	Task Description	5	26	0.454545454545455	0.136125654450262	ask%2:32:02:: -direct or put; seek an answer to ask%2:32:04:: -address a question to and expect an answer from Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual ambiguity (bottom).	0
9395	9395	S13-13	Task Description	6	27	0.545454545454545	0.141361256544503	Senses are specified using their WordNet 3.1 sense keys.	0
9396	9396	S13-13	Task Description	7	28	0.636363636363636	0.146596858638743	each by their applicability.	0
9397	9397	S13-13	Task Description	8	29	0.727272727272727	0.151832460732984	Table 1 highlights two example contexts where multiple senses apply.	0
9398	9398	S13-13	Task Description	9	30	0.818181818181818	0.157068062827225	The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.	0
9399	9399	S13-13	Task Description	10	31	0.909090909090909	0.162303664921466	a as being devoid of light and the causal result of being secret.	0
9400	9400	S13-13	Task Description	11	32	1.0	0.167539267015707	"In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations ""I ask [you] because"" (sense ask%2:32:04::) or ""I ask [the question] because"" (sense ask%2:32:02::)."	0
9401	9401	S13-13	Data	1	33	0.1	0.172774869109948	Three datasets were provided with the task.	0
9402	9402	S13-13	Data	2	34	0.2	0.178010471204188	The trial dataset provided weighted word sense annotations using the data gathered by .	0
9403	9403	S13-13	Data	3	35	0.3	0.183246073298429	The trial dataset consisted of 50 contexts for eight words, where each context was labeled with WordNet 3.0 sense ratings from three untrained lexicographers.	0
9404	9404	S13-13	Data	4	36	0.4	0.18848167539267	Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data.	0
9405	9405	S13-13	Data	5	37	0.5	0.193717277486911	However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses.	0
9406	9406	S13-13	Data	6	38	0.6	0.198952879581152	Previous SemEval WSI tasks had provided participants with corpora specific to the task's target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statistical associations.	0
9407	9407	S13-13	Data	7	39	0.7	0.204188481675393	Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004, OANC) across a variety of genres and from both the spoken and written portions of the corpus, summarized in Table 2.	0
9408	9408	S13-13	Data	8	40	0.8	0.209424083769633	All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense.	0
9409	9409	S13-13	Data	9	41	0.9	0.214659685863874	This filtering also removed instances that were in a collocation, or had an idiomatic meaning.	0
9410	9410	S13-13	Data	10	42	1.0	0.219895287958115	Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word.	0
9411	9411	S13-13	Sense Annotation	1	43	0.076923076923077	0.225130890052356	Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010;	0
9412	9412	S13-13	Sense Annotation	2	44	0.153846153846154	0.230366492146597	Passonneau et al., 2012b;Rumshisky et al., 2012;Jurgens, 2013).	0
9413	9413	S13-13	Sense Annotation	3	45	0.230769230769231	0.235602094240838	Therefore, we initially annotated the Task's data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively.	0
9414	9414	S13-13	Sense Annotation	4	46	0.307692307692308	0.240837696335079	Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating.	0
9415	9415	S13-13	Sense Annotation	5	47	0.384615384615385	0.246073298429319	However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Task's evaluations.	0
9416	9416	S13-13	Sense Annotation	6	48	0.461538461538462	0.25130890052356	Specifically, for some senses and contexts, AMT annotators required more information about sense distinctions than was feasible to integrate into the AMT setting, which led to consistent but incorrect sense assignments.	0
9417	9417	S13-13	Sense Annotation	7	49	0.538461538461538	0.256544502617801	Therefore, the test data was annotated by the two authors, with the first author annotating all instances and the second author annotating a 10% sample of each lemma's instances in order to calculate IAA.	0
9418	9418	S13-13	Sense Annotation	8	50	0.615384615384615	0.261780104712042	IAA was calculated using Krippendorff's α (Krippendorff, 1980;	0
9419	9419	S13-13	Sense Annotation	9	51	0.692307692307692	0.267015706806283	Artstein and Poesio, 2008), which is an agreement measurement that adjusts for chance,   (Passonneau et al., 2006).	0
9420	9420	S13-13	Sense Annotation	10	52	0.769230769230769	0.272251308900524	Table 2 summarizes the annotation statistics for the Task's data.	0
9421	9421	S13-13	Sense Annotation	11	53	0.846153846153846	0.277486910994764	The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators.	0
9422	9422	S13-13	Sense Annotation	12	54	0.923076923076923	0.282722513089005	An analysis across the corpora genres showed that the multiplesense annotation rates were similar.	0
9423	9423	S13-13	Sense Annotation	13	55	1.0	0.287958115183246	Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses.	0
9424	9424	S13-13	Evaluation	1	56	0.25	0.293193717277487	We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007;Manandhar et al., 2010).	0
9425	9425	S13-13	Evaluation	2	57	0.5	0.298429319371728	The first evaluation uses a traditional WSD task that directly compares WordNet sense labels.	0
9426	9426	S13-13	Evaluation	3	58	0.75	0.303664921465969	For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure.	0
9427	9427	S13-13	Evaluation	4	59	1.0	0.308900523560209	The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons.	0
9428	9428	S13-13	WSD Task	1	60	0.2	0.31413612565445	In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability ratings with human annotators.	0
9429	9429	S13-13	WSD Task	2	61	0.4	0.319371727748691	Each objectives uses a specific measurement: (1) the Jaccard Index, (2) positionally-weighted Kendall's τ similarity, and (3) a weighted variant of Normalized Discounted Cumulative Gain, respectively.	0
9430	9430	S13-13	WSD Task	3	62	0.6	0.324607329842932	Each measure is bounded in [0, 1], where 1 indicates complete agreement with the gold standard.	0
9431	9431	S13-13	WSD Task	4	63	0.8	0.329842931937173	We generalize the traditional definition of WSD Recall such that it measures the average score for each measure across all instances, including those not labeled by the system.	0
9432	9432	S13-13	WSD Task	5	64	1.0	0.335078534031414	Systems are ultimately scored using the F1 measure between each objective's measure and Recall.	0
9433	9433	S13-13	Transforming Induced Sense Labels	1	65	0.25	0.340314136125654	In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., Word-Net 3.1) using a sense mapping procedure.	0
9434	9434	S13-13	Transforming Induced Sense Labels	2	66	0.5	0.345549738219895	We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard.	0
9435	9435	S13-13	Transforming Induced Sense Labels	3	67	0.75	0.350785340314136	This process is repeated so that each partition is tested once.	0
9436	9436	S13-13	Transforming Induced Sense Labels	4	68	1.0	0.356020942408377	For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings.	0
9437	9437	S13-13	Jaccard Index	1	69	0.5	0.361256544502618	Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the agreement:	0
9438	9438	S13-13	Jaccard Index	2	70	1.0	0.366492146596859	The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint.	0
9439	9439	S13-13	Positionally-Weighted Kendall's τ	1	71	0.066666666666667	0.371727748691099	Rank correlations have been proposed for evaluating a system's ability to order senses by applicability; in previous work, both  and Jurgens (2012) propose rank correlation coefficients that assume all positions in the ranking are equally important.	0
9440	9440	S13-13	Positionally-Weighted Kendall's τ	2	72	0.133333333333333	0.37696335078534	However, in the case of graded sense evaluation, often only a few senses are applicable, with the applicability ratings of the remaining senses being relatively inconsequential.	0
9441	9441	S13-13	Positionally-Weighted Kendall's τ	3	73	0.2	0.382198952879581	Therefore, we consider an alternate rank scoring based on Kumar and Vassilvitskii (2010), which weights the penalty of reordering the lower positions less than the penalty of reordering the first ranks.	0
9442	9442	S13-13	Positionally-Weighted Kendall's τ	4	74	0.266666666666667	0.387434554973822	Kendall's τ distance, K, is a measure of the number of item position swaps required to make two sequences identical.	0
9443	9443	S13-13	Positionally-Weighted Kendall's τ	5	75	0.333333333333333	0.392670157068063	Kumar and Vassilvitskii (2010) extend this distance definition using a variable penalty function δ for the cost of swapping two positions, which we denote K δ .	0
9444	9444	S13-13	Positionally-Weighted Kendall's τ	6	76	0.4	0.397905759162304	By using an appropriate δ, K δ can be biased towards the correctness of higher ranks by assigning a smaller δ to lower ranks.	0
9445	9445	S13-13	Positionally-Weighted Kendall's τ	7	77	0.466666666666667	0.403141361256544	Because K δ is a distance measure, its value range will be different depending on the number of ranks used.	0
9446	9446	S13-13	Positionally-Weighted Kendall's τ	8	78	0.533333333333333	0.408376963350785	Therefore, to convert the measure to a similarity we normalize the distance to [0, 1] by dividing by the maximum K δ distance and then subtracting the distance from one.	0
9447	9447	S13-13	Positionally-Weighted Kendall's τ	9	79	0.6	0.413612565445026	Given two rankings x and y where x is the reference by which y is to be measured, we may compute the normalized similarity using	0
9448	9448	S13-13	Positionally-Weighted Kendall's τ	10	80	0.666666666666667	0.418848167539267	.	0
9449	9449	S13-13	Positionally-Weighted Kendall's τ	11	81	0.733333333333333	0.424083769633508	(1) Equation 1 has its maximal value of one when ranking y is identical to ranking x, and its minimal value of zero when y is in the reverse order as x.	0
9450	9450	S13-13	Positionally-Weighted Kendall's τ	12	82	0.8	0.429319371727749	We refer to this value as the positionally-weighted Kendall's τ similarity, K sim δ .	0
9451	9451	S13-13	Positionally-Weighted Kendall's τ	13	83	0.866666666666667	0.43455497382199	As defined, K sim δ does not account for ties.	0
9452	9452	S13-13	Positionally-Weighted Kendall's τ	14	84	0.933333333333333	0.43979057591623	Therefore, we arbitrarily break ties in a deterministic fashion for both rankings.	0
9453	9453	S13-13	Positionally-Weighted Kendall's τ	15	85	1.0	0.445026178010471	Second, we define δ to assign higher cost to the first ranks: the cost to move an item into position i, δ i , is defined as n−(i+1) n , where n is the number of senses.	0
9454	9454	S13-13	Weighted NDCG	1	86	0.090909090909091	0.450261780104712	To compare the applicability ratings for sense annotations, we recast the annotation process in an Information Retrieval setting: Given an example context acting as a query over a word's senses, the task is to retrieve all applicable senses, ranking and scoring them by their applicability.	0
9455	9455	S13-13	Weighted NDCG	2	87	0.181818181818182	0.455497382198953	Moffat and Zobel (2008) propose using Discounted Cumulative Gain (DCG) as a method to compare a ranking against a baseline.	0
9456	9456	S13-13	Weighted NDCG	3	88	0.272727272727273	0.460732984293194	Given (1) a gold standard weighting of the k senses applicable to a context, where w i denotes the applicability for sense i in the gold standard, and (2) a ranking of the k senses by some method, the DCG may be calculated as i+1) . DCG is commonly normalized to [0, 1] so that the value is comparable when computed on rankings with different k and weight values.	0
9457	9457	S13-13	Weighted NDCG	4	89	0.363636363636364	0.465968586387435	To normalize, the maximum value is calculated by first computing the DCG on the ranking when the k items are sorted by their weights, referred as the Ideal DCG (IDCG), and then normalizing as N DCG = DCG IDCG .	0
9458	9458	S13-13	Weighted NDCG	5	90	0.454545454545455	0.471204188481675	The DCG only considers the weights assigned in the gold standard, which potentially masks importance differences in the weights assigned to the senses.	0
9459	9459	S13-13	Weighted NDCG	6	91	0.545454545454545	0.476439790575916	Therefore, we propose weighting the DCG by the relative difference in the two weights.	0
9460	9460	S13-13	Weighted NDCG	7	92	0.636363636363636	0.481675392670157	Given an alternate weighting of the k items, denoted asŵ i ,	0
9461	9461	S13-13	Weighted NDCG	8	93	0.727272727272727	0.486910994764398	The key impact in Equation 2 comes from weighting an item's contribution to the score by its relative deviation in absolute weight.	0
9462	9462	S13-13	Weighted NDCG	9	94	0.818181818181818	0.492146596858639	A set of weights that achieves an equivalent ranking may have a low WDCG if the weights are significantly higher or lower than the reference.	0
9463	9463	S13-13	Weighted NDCG	10	95	0.909090909090909	0.49738219895288	Equation 2 may be normalized in the same way as the DCG.	0
9464	9464	S13-13	Weighted NDCG	11	96	1.0	0.50261780104712	We refer to this final normalized measure as the Weighted Normalized Discounted Cumulative Gain (WNDCG).	0
9465	9465	S13-13	Sense Cluster Comparisons	1	97	0.083333333333333	0.507853403141361	Sense induction can be viewed as an unsupervised clustering task where usages of a word are grouped into clusters, each representing uses of the same meaning.	0
9466	9466	S13-13	Sense Cluster Comparisons	2	98	0.166666666666667	0.513089005235602	In previous SemEval tasks on sense induction, instances were labeled with a single sense, which yields a partition over the instances into disjoint sets.	0
9467	9467	S13-13	Sense Cluster Comparisons	3	99	0.25	0.518324607329843	The proposed partition can then be compared with a gold-standard partition using many existing clustering comparison methods, such as the V-Measure (Rosenberg and Hirschberg, 2007) or paired FScore .	0
9468	9468	S13-13	Sense Cluster Comparisons	4	100	0.333333333333333	0.523560209424084	Such cluster comparison methods measure the degree of similarity between the sense boundaries created by lexicographers and those created by WSI methods.	0
9469	9469	S13-13	Sense Cluster Comparisons	5	101	0.416666666666667	0.528795811518325	In the present task, instances are potentially labeled both with multiple senses and with weights reflecting the applicability.	0
9470	9470	S13-13	Sense Cluster Comparisons	6	102	0.5	0.534031413612565	This type of sense labeling produces a fuzzy clustering:	0
9471	9471	S13-13	Sense Cluster Comparisons	7	103	0.583333333333333	0.539267015706806	An instance may belong to one or more sense clusters with its cluster membership relative to its weight for that sense.	0
9472	9472	S13-13	Sense Cluster Comparisons	8	104	0.666666666666667	0.544502617801047	Formally, we refer to (1) a solution where the sets of instances overlap as a cover and (2) a solution where the sets overlap and instances may have partial memberships in a set as fuzzy cover.	0
9473	9473	S13-13	Sense Cluster Comparisons	9	105	0.75	0.549738219895288	We propose two new fuzzy measures for comparing fuzzy sense assignments: Fuzzy B-Cubed and Fuzzy Normalized Mutual Information.	0
9474	9474	S13-13	Sense Cluster Comparisons	10	106	0.833333333333333	0.554973821989529	The two measures provide complementary information.	0
9475	9475	S13-13	Sense Cluster Comparisons	11	107	0.916666666666667	0.56020942408377	B-Cubed summarizes the performance per instance and therefore provides an estimate of how well a system would perform on a new corpus with a similar sense distribution.	0
9476	9476	S13-13	Sense Cluster Comparisons	12	108	1.0	0.565445026178011	In contrast, Fuzzy NMI is measured based on the clusters rather than the instances, thereby providing a performance analysis that is independent of the corpus sense distribution.	0
9477	9477	S13-13	Fuzzy B-Cubed	1	109	0.055555555555556	0.570680628272251	Bagga and Baldwin (1998) proposed a clustering evaluation known as B-Cubed, which compares two partitions on a per-item basis.	0
9478	9478	S13-13	Fuzzy B-Cubed	2	110	0.111111111111111	0.575916230366492	later extended the definition of B-Cubed to compare overlapping clusters (i.e., covers).	0
9479	9479	S13-13	Fuzzy B-Cubed	3	111	0.166666666666667	0.581151832460733	We generalize B-Cubed further to handle the case of fuzzy covers.	0
9480	9480	S13-13	Fuzzy B-Cubed	4	112	0.222222222222222	0.586387434554974	B-Cubed is based on precision and recall, which estimate the fit between two clusterings, X and Y at the item level.	0
9481	9481	S13-13	Fuzzy B-Cubed	5	113	0.277777777777778	0.591623036649215	For an item i, precision reflects how many items sharing a cluster with i in X appear in its cluster in Y ; conversely, recall measures how many items sharing a cluster in Y with i also appear in its cluster in X.	0
9482	9482	S13-13	Fuzzy B-Cubed	6	114	0.333333333333333	0.596858638743455	The final B-Cubed value is the harmonic mean of the two scores.	0
9483	9483	S13-13	Fuzzy B-Cubed	7	115	0.388888888888889	0.602094240837696	To generalize B-Cubed to fuzzy covers, we adopt the formalization of , who define item-based precision and recall functions, P and R, in terms of a correctness function, C → {0, 1}.	0
9484	9484	S13-13	Fuzzy B-Cubed	8	116	0.444444444444444	0.607329842931937	For notational brevity, let avg be a function that returns the mean value of a series, and µ x (i) denote the set of clusters in clustering X of which item i is a member.	0
9485	9485	S13-13	Fuzzy B-Cubed	9	117	0.5	0.612565445026178	B-Cubed precision and recall may therefore calculated over all n items:	0
9486	9486	S13-13	Fuzzy B-Cubed	10	118	0.555555555555556	0.617801047120419	When comparing partitions, P and R are defined as 1 if two items cluster labels are identical.	0
9487	9487	S13-13	Fuzzy B-Cubed	11	119	0.611111111111111	0.62303664921466	To generalize B-Cubed for fuzzy covers, we redefine P and R to account for differences in the partial cluster membership of items.	0
9488	9488	S13-13	Fuzzy B-Cubed	12	120	0.666666666666667	0.628272251308901	Let X (i) denote the set of clusters of which i is a member, and w k (i) denote the membership weight of item i in cluster k in X.	0
9489	9489	S13-13	Fuzzy B-Cubed	13	121	0.722222222222222	0.633507853403141	We therefore define C with respect to X of two items as	0
9490	9490	S13-13	Fuzzy B-Cubed	14	122	0.777777777777778	0.638743455497382	Equation 5 is maximized when i and j have identical membership weights in the clusters of which they are members.	0
9491	9491	S13-13	Fuzzy B-Cubed	15	123	0.833333333333333	0.643979057591623	Importantly, Equation 5generalizes to the correctness operations both when comparing partitions and covers, as defined by .	0
9492	9492	S13-13	Fuzzy B-Cubed	16	124	0.888888888888889	0.649214659685864	Item-based Precision and Recall are then defined using Equation 5as P (i, j, X) = Min(C(i,j,X),C(i,j,Y ))	0
9493	9493	S13-13	Fuzzy B-Cubed	17	125	0.944444444444444	0.654450261780105	, respectively.	0
9494	9494	S13-13	Fuzzy B-Cubed	18	126	1.0	0.659685863874346	These fuzzy generalizations are used in Equations 3 and 4.	0
9495	9495	S13-13	Fuzzy Normalized Mutual Information	1	127	0.043478260869565	0.664921465968586	Mutual information measures the dependence between two random variables.	0
9496	9496	S13-13	Fuzzy Normalized Mutual Information	2	128	0.086956521739131	0.670157068062827	In the context of clustering evaluation, mutual information treats the sense labels as random variables and measures the level of agreement in which instances are labeled with the same senses (Danon et al., 2005).	0
9497	9497	S13-13	Fuzzy Normalized Mutual Information	3	129	0.130434782608696	0.675392670157068	Formally, mutual information is defined as I(X; Y ) = H(X)−(H(X|Y ) where H(X) denotes the entropy of the random variable X that represents a partition, i.e., the sets of instances assigned to each sense.	0
9498	9498	S13-13	Fuzzy Normalized Mutual Information	4	130	0.173913043478261	0.680628272251309	Typically, mutual information is normalized to [0, 1] in order to facilitate comparisons between multiple clustering solutions on the same scale (Luo et al., 2009), with M ax(H(X), H(Y )) being the recommended normalizing factor (Vinh et al., 2010).	0
9499	9499	S13-13	Fuzzy Normalized Mutual Information	5	131	0.217391304347826	0.68586387434555	In its original formulation Mutual information is defined only to compare non-overlapping cluster partitions.	0
9500	9500	S13-13	Fuzzy Normalized Mutual Information	6	132	0.260869565217391	0.69109947643979	Therefore, we propose a new definition of mutual information between fuzzy covers using extension of Lancichinetti et al. (2009) for calculating the normalized mutual information between covers.	0
9501	9501	S13-13	Fuzzy Normalized Mutual Information	7	133	0.304347826086957	0.696335078534031	In the case of partitions, a clustering is represented as a discrete random variable whose states denote the probability of being assigned to each cluster.	0
9502	9502	S13-13	Fuzzy Normalized Mutual Information	8	134	0.347826086956522	0.701570680628272	In the fuzzy cover setting, each item may be assigned to multiple clusters and no longer has a binary assignment to a cluster, but takes on a value in [0, 1].	0
9503	9503	S13-13	Fuzzy Normalized Mutual Information	9	135	0.391304347826087	0.706806282722513	Therefore, each cluster X i can be represented separately as a continuous random variable, with the entire fuzzy cover denoted as the variable X 1...k , where the ith entry of X is the continuous random variable for cluster i.	0
9504	9504	S13-13	Fuzzy Normalized Mutual Information	10	136	0.434782608695652	0.712041884816754	However, by modeling clusters using continuous domain, differential entropy must be used for the continuous variables; importantly, differential entropy does not obey the same properties as discrete entropy and may be negative.	0
9505	9505	S13-13	Fuzzy Normalized Mutual Information	11	137	0.478260869565217	0.717277486910995	To avoid calculating entropy in the continuous domain, we therefore propose an alternative method of computing mutual information based on discretizing the continuous values of X i in the fuzzy setting.	0
9506	9506	S13-13	Fuzzy Normalized Mutual Information	12	138	0.521739130434783	0.722513089005236	For the continuous random variable X i , we discretize the value by dividing up probability mass into discrete bins.	0
9507	9507	S13-13	Fuzzy Normalized Mutual Information	13	139	0.565217391304348	0.727748691099476	That is, the support of X i is partitioned into disjoint ranges, each of which represents a discrete outcome of X i .	0
9508	9508	S13-13	Fuzzy Normalized Mutual Information	14	140	0.608695652173913	0.732984293193717	As a result, X i becomes a categorical distribution over a set of weights ranges {w 1 , . . . , w n } that denote the strength of membership in the fuzzy set.	0
9509	9509	S13-13	Fuzzy Normalized Mutual Information	15	141	0.652173913043478	0.738219895287958	With respect to sense annotation, this discretization process is analogous to having an annotator rate the applicability of a sense for an instance using a Likert scale instead of using a rational number within a fixed bound.	0
9510	9510	S13-13	Fuzzy Normalized Mutual Information	16	142	0.695652173913043	0.743455497382199	Discretizing the continuous cluster membership ratings into bins allows us to avoid the problematic interpretation of entropy in the continuous domain while still expanding the definition of mutual information from a binary cluster membership to one of degrees.	0
9511	9511	S13-13	Fuzzy Normalized Mutual Information	17	143	0.739130434782609	0.74869109947644	Using the definition of X i and Y j as a categorical variables over discrete ratings, we may then estimate the entropy and joint entropy as follows.	0
9512	9512	S13-13	Fuzzy Normalized Mutual Information	18	144	0.782608695652174	0.753926701570681	where p(w i ) is the probability of an instance being labeled with rating w i Similarly, we may define the joint entropy of two fuzzy clusters as	0
9513	9513	S13-13	Fuzzy Normalized Mutual Information	19	145	0.826086956521739	0.759162303664921	where p(w i , w j ) is the probability of an instance being labeled with rating w i in cluster X k and w j in cluster Y l , and m denotes the number of bins for Y l .	0
9514	9514	S13-13	Fuzzy Normalized Mutual Information	20	146	0.869565217391304	0.764397905759162	The conditional entropy between two clusters may then be calculated as	0
9515	9515	S13-13	Fuzzy Normalized Mutual Information	21	147	0.91304347826087	0.769633507853403	Together, Equations 6 and 7 may be used to define I(X, Y ) as in the original definition.	0
9516	9516	S13-13	Fuzzy Normalized Mutual Information	22	148	0.956521739130435	0.774869109947644	We then normalize using the method of McDaid et al. (2011).	0
9517	9517	S13-13	Fuzzy Normalized Mutual Information	23	149	1.0	0.780104712041885	Based on the limited range of fuzzy memberships in [0, 1], we selected uniformly distributed bins in [0, 1] at 0.1 intervals when discretizing the membership weights for sense labelings.	0
9518	9518	S13-13	Baselines	1	150	0.2	0.785340314136126	Task 12 included multiple baselines based on modeling different types of WSI and WSD systems.	0
9519	9519	S13-13	Baselines	2	151	0.4	0.790575916230366	Due to space constraints, we include only the four most descriptive here: (1) Semcor MFS which labels each instance with the most frequent sense of that lemma in SemCor, (2) Semcor Ranked Senses baseline, which labels each instance with all of the target lemma's senses, ranked according to their frequency in SemCor, using weights n−i+1 n , where n is the number of senses and i is the rank, (3) 1c1inst which labels each instance with its own induced sense and (4)	0
9520	9520	S13-13	Baselines	3	152	0.6	0.795811518324607	All-instances,	0
9521	9521	S13-13	Baselines	4	153	0.8	0.801047120418848	One sense which labels all instances with the same induced sense.	0
9522	9522	S13-13	Baselines	5	154	1.0	0.806282722513089	The first two baselines directly use WordNet 3.1 senses, while the last two use induced senses.	0
9523	9523	S13-13	Participating Systems	1	155	0.125	0.81151832460733	Four teams submitted nine systems, seven of which used induced sense inventories.	0
9524	9524	S13-13	Participating Systems	2	156	0.25	0.816753926701571	AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word's contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target.	0
9525	9525	S13-13	Participating Systems	3	157	0.375	0.821989528795812	Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010).	0
9526	9526	S13-13	Participating Systems	4	158	0.5	0.827225130890052	The resulting contextual distributions are then clustered using K-means to identify word senses.	0
9527	9527	S13-13	Participating Systems	5	159	0.625	0.832460732984293	The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012).	0
9528	9528	S13-13	Participating Systems	6	160	0.75	0.837696335078534	Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006)   like other teams, the Unimelb systems were trained on a Wikipedia corpus instead of the ukWaC corpus.	0
9529	9529	S13-13	Participating Systems	7	161	0.875	0.842931937172775	The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013); the resulting fine-grained clusters are then combined based on their degree of separability.	0
9530	9530	S13-13	Participating Systems	8	162	1.0	0.848167539267016	The La Sapienza team submitted two Unsupervised WSD systems based applying Personalized Page Rank (Agirre and Soroa, 2009) over a WordNet-based network to compare the similarity of each sense with the similarity of the context, ranking each sense according to its similarity.	0
9531	9531	S13-13	Results and Discussion	1	163	0.045454545454546	0.853403141361257	Table 3 shows the main results for all instances.	0
9532	9532	S13-13	Results and Discussion	2	164	0.090909090909091	0.858638743455497	Additionally, we report the number of induced clusters used to label each sense as #Cl and the number of resulting WordNet 3.1 senses for each sense with #S.	0
9533	9533	S13-13	Results and Discussion	3	165	0.136363636363636	0.863874345549738	As in previous WSD tasks, the MFS baseline was quite competitive, outperforming all systems on detecting which senses were applicable, measured using the Jaccard Index.	0
9534	9534	S13-13	Results and Discussion	4	166	0.181818181818182	0.869109947643979	However, most systems were able to outperform the MFS baseline on ranking senses and quantifying their applicability.	0
9535	9535	S13-13	Results and Discussion	5	167	0.227272727272727	0.87434554973822	Previous cluster comparison evaluations often faced issues with the measures being biased either towards the 1c1inst baseline or labeling all instances with the same sense.	0
9536	9536	S13-13	Results and Discussion	6	168	0.272727272727273	0.879581151832461	However, Table 3   systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline.	0
9537	9537	S13-13	Results and Discussion	7	169	0.318181818181818	0.884816753926702	An analysis of the systems' results showed that many systems labeled instances with a high number of senses, which could have been influenced by the trial data having significantly more instances labeled with multiple senses than the test data.	0
9538	9538	S13-13	Results and Discussion	8	170	0.363636363636364	0.890052356020942	Therefore, we performed a second analysis that partitioned the test set into two sets: those labeled with a single sense and those with multiple senses.	0
9539	9539	S13-13	Results and Discussion	9	171	0.409090909090909	0.895287958115183	For single-sense set, we modified the test setting to have systems also label instances with a single sense:	0
9540	9540	S13-13	Results and Discussion	10	172	0.454545454545455	0.900523560209424	(1) the sense mapping function for WSI systems (Sec. 3.1.1) was modified so that after the mapping,  only the highest-weighted WordNet 3.1 sense was used, and (2) the La Sapienza system output was modified to retain only the highest weighted sense.	0
9541	9541	S13-13	Results and Discussion	11	173	0.5	0.905759162303665	In this single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall.	0
9542	9542	S13-13	Results and Discussion	12	174	0.545454545454545	0.910994764397906	The remaining subset of instances annotated with multiple senses were evaluated separately.	0
9543	9543	S13-13	Results and Discussion	13	175	0.590909090909091	0.916230366492147	Table 4 shows the systems' performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems.	0
9544	9544	S13-13	Results and Discussion	14	176	0.636363636363636	0.921465968586387	Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002;	0
9545	9545	S13-13	Results and Discussion	15	177	0.681818181818182	0.926701570680628	Mihalcea et al., 2004;Pradhan et al., 2007;	0
9546	9546	S13-13	Results and Discussion	16	178	0.727272727272727	0.931937172774869	Agirre et al., 2010).	0
9547	9547	S13-13	Results and Discussion	17	179	0.772727272727273	0.93717277486911	This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems.	0
9548	9548	S13-13	Results and Discussion	18	180	0.818181818181818	0.942408376963351	Table 5 shows the performance on the subset of instances that were annotated with multiple senses.	0
9549	9549	S13-13	Results and Discussion	19	181	0.863636363636364	0.947643979057592	We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus.	0
9550	9550	S13-13	Results and Discussion	20	182	0.909090909090909	0.952879581151832	Notably, the La Sapienza systems sees a significant performance increase in this setting; their systems label each instance with all of the lemma's senses, which significantly de-grades performance in the most common case where only a single sense applies.	0
9551	9551	S13-13	Results and Discussion	21	183	0.954545454545455	0.958115183246073	However, when multiple senses are known to be present, their method for quantifying sense applicability appears closest to the gold standard judgments.	0
9552	9552	S13-13	Results and Discussion	22	184	1.0	0.963350785340314	Furthermore, the majority of WSI systems are able to surpass all four baselines on identifying which senses are present and quantifying their applicability.	0
9553	9553	S13-13	Conclusion	1	185	0.142857142857143	0.968586387434555	We have introduced a new evaluation setting for WSI and WSD systems where systems are measured by their ability to detect and weight multiple applicable senses for a single context.	0
9554	9554	S13-13	Conclusion	2	186	0.285714285714286	0.973821989528796	Four teams submitted nine systems, annotating a total of 4664 contexts for 50 words from the OANC.	0
9555	9555	S13-13	Conclusion	3	187	0.428571428571429	0.979057591623037	Many systems were able to surpass the competitive MFS baseline.	0
9556	9556	S13-13	Conclusion	4	188	0.571428571428571	0.984293193717278	Furthermore, when WSI systems were trained to produce only a single sense label, the performance of resulting semi-supervised WSD systems surpassed that of many supervised systems in previous WSD evaluations.	0
9557	9557	S13-13	Conclusion	5	189	0.714285714285714	0.989528795811518	Future work may assess the impact of graded sense annotations in a task-based setting.	0
9558	9558	S13-13	Conclusion	6	190	0.857142857142857	0.994764397905759	All materials have been released on the task website.	0
9559	9559	S13-13	Conclusion	7	191	1.0	1.0	1	0
10486	10486	S14-6	title	1	1	1.0	0.005050505050505	SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands	0
10487	10487	S14-6	abstract	1	2	0.166666666666667	0.01010101010101	SemEval-2014	0
10488	10488	S14-6	abstract	2	3	0.333333333333333	0.015151515151515	Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches.	0
10489	10489	S14-6	abstract	3	4	0.5	0.02020202020202	The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm.	1
10490	10490	S14-6	abstract	4	5	0.666666666666667	0.025252525252525	Six teams submitted systems using both rule-based and statistical methods.	0
10491	10491	S14-6	abstract	5	6	0.833333333333333	0.03030303030303	The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context.	0
10492	10492	S14-6	abstract	6	7	1.0	0.035353535353535	However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.	0
10493	10493	S14-6	Introduction	1	8	0.043478260869565	0.04040404040404	Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language.	0
10494	10494	S14-6	Introduction	2	9	0.086956521739131	0.045454545454546	Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013;	0
10495	10495	S14-6	Introduction	3	10	0.130434782608696	0.050505050505051	Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011;	0
10496	10496	S14-6	Introduction	4	11	0.173913043478261	0.055555555555556	Kim and Mooney, 2012).	0
10497	10497	S14-6	Introduction	5	12	0.217391304347826	0.060606060606061	Different parsers can be distinguished by the level of supervision they require during training.	0
10498	10498	S14-6	Introduction	6	13	0.260869565217391	0.065656565656566	Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form.	0
10499	10499	S14-6	Introduction	7	14	0.304347826086957	0.070707070707071	However, because annotated data is often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information.	0
10500	10500	S14-6	Introduction	8	15	0.347826086956522	0.075757575757576	For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system.	0
10501	10501	S14-6	Introduction	9	16	0.391304347826087	0.080808080808081	In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction.	0
10502	10502	S14-6	Introduction	10	17	0.434782608695652	0.085858585858586	In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing.	0
10503	10503	S14-6	Introduction	11	18	0.478260869565217	0.090909090909091	The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board.	0
10504	10504	S14-6	Introduction	12	19	0.521739130434783	0.095959595959596	Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context.	0
10505	10505	S14-6	Introduction	13	20	0.565217391304348	0.101010101010101	For example, the command in Figure 1 may have several plausible interpretations, given different board configurations.	0
10506	10506	S14-6	Introduction	14	21	0.608695652173913	0.106060606060606	'	0
10507	10507	S14-6	Introduction	15	22	0.652173913043478	0.111111111111111	Move the pyramid on the blue cube on the gray one.'	0
10508	10508	S14-6	Introduction	16	23	0.695652173913043	0.116161616161616	The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009;	0
10509	10509	S14-6	Introduction	17	24	0.739130434782609	0.121212121212121	Mitkov, 1999).	0
10510	10510	S14-6	Introduction	18	25	0.782608695652174	0.126262626262626	More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation.	0
10511	10511	S14-6	Introduction	19	26	0.826086956521739	0.131313131313131	Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph.	0
10512	10512	S14-6	Introduction	20	27	0.869565217391304	0.136363636363636	Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands.	0
10513	10513	S14-6	Introduction	21	28	0.91304347826087	0.141414141414141	In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research.	0
10514	10514	S14-6	Introduction	22	29	0.956521739130435	0.146464646464646	In the remainder of this paper, we describe the task, the dataset and the metrics used for evaluation.	0
10515	10515	S14-6	Introduction	23	30	1.0	0.151515151515152	We then compare the approaches used by participant systems and conclude with suggested improvements for future work.	0
10516	10516	S14-6	Task Description	1	31	0.166666666666667	0.156565656565657	The long term research goal encouraged by the task is to develop a system that will robustly execute NL robotic commands.	0
10517	10517	S14-6	Task Description	2	32	0.333333333333333	0.161616161616162	In general, this is a highly complex problem involving computational processing of language, spatial reasoning, contextual awareness and knowledge representation.	0
10518	10518	S14-6	Task Description	3	33	0.5	0.166666666666667	To simplify the problem, participants were provided with additional tools and resources, allowing them to focus on developing a semantic parser for a fixed domain that would fit into an existing component architecture.	0
10519	10519	S14-6	Task Description	4	34	0.666666666666667	0.171717171717172	Figure 2 shows how these components interact.	0
10520	10520	S14-6	Task Description	5	35	0.833333333333333	0.176767676767677	Semantic parser: Systems submitted by participants are semantic parsers that accept an NL command as input, mapping this to a formal Robot Control Language (RCL), described further in section 3.3.	0
10521	10521	S14-6	Task Description	6	36	1.0	0.181818181818182	The Robot Commands Treebank used for the both training and evaluation is an annotated corpus that pairs NL commands with contextual RCL statements.	0
10522	10522	S14-6	Spatial planner:	1	37	0.166666666666667	0.186868686868687	A spatial planner is provided as an open Java API 1 . Commands in the treebank are specified in the context of spatial scenes.	0
10523	10523	S14-6	Spatial planner:	2	38	0.333333333333333	0.191919191919192	By interfacing with the planner, participant systems	0
10524	10524	S14-6	Spatial planner:	3	39	0.5	0.196969696969697	1 https://github.com/kaisdukes/train-robots have access to this additional information.	0
10525	10525	S14-6	Spatial planner:	4	40	0.666666666666667	0.202020202020202	For example, given an RCL fragment for the expression 'the red cube on the blue block', the planner will ground the entity, returning a list of zero or more board coordinates corresponding to possible matches.	0
10526	10526	S14-6	Spatial planner:	5	41	0.833333333333333	0.207070707070707	The planner also validates commands to determine if they are compatible with spatial context.	0
10527	10527	S14-6	Spatial planner:	6	42	1.0	0.212121212121212	It can therefore be used to constrain the search space of possible parses, as well as enabling early resolution of attachment ambiguity during parsing.	0
10528	10528	S14-6	Robotic simulator:	1	43	0.166666666666667	0.217171717171717	The simulated environment consists of an 8 x 8 board that can hold prisms and cubes which occur in eight different colors.	0
10529	10529	S14-6	Robotic simulator:	2	44	0.333333333333333	0.222222222222222	The robot's gripper can move to any discrete position within an 8 x 8 x 8 space above the board.	0
10530	10530	S14-6	Robotic simulator:	3	45	0.5	0.227272727272727	The planner uses the simulator to enforce physical laws within the game.	0
10531	10531	S14-6	Robotic simulator:	4	46	0.666666666666667	0.232323232323232	For example, a block cannot remain unsupported in empty space due to gravity.	0
10532	10532	S14-6	Robotic simulator:	5	47	0.833333333333333	0.237373737373737	Similarly, prisms cannot lie below other block types.	0
10533	10533	S14-6	Robotic simulator:	6	48	1.0	0.242424242424242	In the integrated system, the parser uses the planner for context, then provides the final RCL statement to the simulator which executes the command by moving the robot arm to update the board.	0
10534	10534	S14-6	Data	1	49	0.2	0.247474747474747	Data Collection	0
10535	10535	S14-6	Data	2	50	0.4	0.252525252525252	For the shared task, 3,409 sentences were selected from the treebank.	0
10536	10536	S14-6	Data	3	51	0.6	0.257575757575758	This data size compares with related corpora used for semantic parsing such as the ATIS (Zettlemoyer and Collins, 2007), GeoQuery (Kate et al., 2005), Jobs (Tang and Mooney, 2001) and RoboCup (Kuhlmann et al., 2004) datasets, consisting of 4,978; 880; 640 and 300 sentences respectively.	0
10537	10537	S14-6	Data	4	52	0.8	0.262626262626263	The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations and asked to give a corresponding command to a hypothetical robot arm.	0
10538	10538	S14-6	Data	5	53	1.0	0.267676767676768	To make the game more competitive and to promote data quality, players rated each other's sentences and were rewarded with points for accurate entries (Dukes, 2013b).	0
10539	10539	S14-6	Annotation	1	54	0.166666666666667	0.272727272727273	In total, over 10,000 commands were collected through the game.	0
10540	10540	S14-6	Annotation	2	55	0.333333333333333	0.277777777777778	During an offline annotation phase, sentences were manually mapped to RCL.	0
10541	10541	S14-6	Annotation	3	56	0.5	0.282828282828283	However, due to the nature of the game, players were free to enter arbitrarily complex sentences to describe moves, not all of which could be represented by RCL.	0
10542	10542	S14-6	Annotation	4	57	0.666666666666667	0.287878787878788	In addition, some commands were syntactically well-formed, but not compatible with the corresponding scenes.	0
10543	10543	S14-6	Annotation	5	58	0.833333333333333	0.292929292929293	The 3,409 commands selected for the task had RCL statements that were both understood by the planner and when given to the robotic simulator resulted in the expected move being made between before and after board configurations.	0
10544	10544	S14-6	Annotation	6	59	1.0	0.297979797979798	Due to this extra validation step, all RCL statements provided for the task were contextually well-formed.	0
10545	10545	S14-6	Robot Control Language	1	60	0.0625	0.303030303030303	RCL is a novel linguistically-oriented semantic representation.	0
10546	10546	S14-6	Robot Control Language	2	61	0.125	0.308080808080808	An RCL statement is a semantic tree (Figure 3) where leaf nodes generally align to words in the corresponding sentence, and nonleaves are tagged using a pre-defined set of categories.	0
10547	10547	S14-6	Robot Control Language	3	62	0.1875	0.313131313131313	RCL is designed to annotate rich linguistic structure, including ellipsis (such as 'place [it] on'), anaphoric references ('it' and 'one'), multiword spatial expressions ('on top of') and lexical disambiguation ('one' and 'place').	0
10548	10548	S14-6	Robot Control Language	4	63	0.25	0.318181818181818	Due to ellipsis, unaligned words and multi-word expressions, a leaf node may align to zero, one or more words in a sentence.	0
10549	10549	S14-6	Robot Control Language	5	64	0.3125	0.323232323232323	Figure 4 shows the RCL syntax for the tree in Figure 3, as accepted by the spatial planner and the simulator.	0
10550	10550	S14-6	Robot Control Language	6	65	0.375	0.328282828282828	As these components do not require NL word alignment data, this additional information was made available to task participants for training via a separate Java API.	0
10551	10551	S14-6	Robot Control Language	7	66	0.4375	0.333333333333333	The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively).	0
10552	10552	S14-6	Robot Control Language	8	67	0.5	0.338383838383838	The general elements are typed entities (labelled with semantic features) that are connected using relations and events.	0
10553	10553	S14-6	Robot Control Language	9	68	0.5625	0.343434343434343	This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004;	0
10554	10554	S14-6	Robot Control Language	10	69	0.625	0.348484848484848	UzZaman and Allen, 2010;	0
10555	10555	S14-6	Robot Control Language	11	70	0.6875	0.353535353535353	Coyne et al., 2010;Dukes, 2009).	0
10556	10556	S14-6	Robot Control Language	12	71	0.75	0.358585858585859	In the remainder of this section we summarize aspects of RCL that are relevant to the task; a more detailed description is provided by Dukes (2013a;.	0
10557	10557	S14-6	Robot Control Language	13	72	0.8125	0.363636363636364	In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue).	0
10558	10558	S14-6	Robot Control Language	14	73	0.875	0.368686868686869	Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents.	0
10559	10559	S14-6	Robot Control Language	15	74	0.9375	0.373737373737374	The remaining features model the simulated robotic domain.	0
10560	10560	S14-6	Robot Control Language	16	75	1.0	0.378787878787879	For sequence Used to specify a sequence of events or statements.	0
10561	10561	S14-6	spatial-relation	1	76	0.142857142857143	0.383838383838384	Used to specify a spatial relation between two entities or to describe a location.	0
10562	10562	S14-6	spatial-relation	2	77	0.285714285714286	0.388888888888889	type Used to specify an entity type.	0
10563	10563	S14-6	spatial-relation	3	78	0.428571428571429	0.393939393939394	example, the values of the action feature are the moves used to control the robotic arm, while values of the type and relation features are the entity and relation types understood by the spatial planner (Table 2).	0
10564	10564	S14-6	spatial-relation	4	79	0.571428571428571	0.398989898989899	As well as qualitative relations (such as 'below' or 'above'), the planner also accepts spatial relations that include quantitative measurements, such as in 'two squares left of the red prism' (Figure 5).	0
10565	10565	S14-6	spatial-relation	5	80	0.714285714285714	0.404040404040404	RCL distinguishes between relations which relate entities and indicators, which are attributes of entities (such as 'left' in 'the left cube').	0
10566	10566	S14-6	spatial-relation	6	81	0.857142857142857	0.409090909090909	For the task, participants are asked to map NL sentences to well-formed RCL by identifying spatial relations and indicators, then parsing higher-level entities and events.	0
10567	10567	S14-6	spatial-relation	7	82	1.0	0.414141414141414	Finally, a well-formed RCL tree with an event (or sequence of events) at toplevel is given the simulator for execution.	0
10568	10568	S14-6	Evaluation Metrics	1	83	0.166666666666667	0.419191919191919	Out of the 3,400 sentences annotated for the task, 2,500 sentences were provided to participants for system training.	0
10569	10569	S14-6	Evaluation Metrics	2	84	0.333333333333333	0.424242424242424	During evaluation, trained systems were presented with 909 previously unseen sentences and asked to generate corresponding RCL statements, with access to the spatial planner for additional context.	0
10570	10570	S14-6	Evaluation Metrics	3	85	0.5	0.429292929292929	To keep the evaluation process as simple as possible, each parser's output for a sentence was scored as correct if it exactly matched the expected RCL statement in the treebank.	0
10571	10571	S14-6	Evaluation Metrics	4	86	0.666666666666667	0.434343434343434	Participants were asked to calculate two metrics, P and NP, which are the proportion of exact matches with and without using the spatial planner respectively:  3: System results for supervised semantic parsing of the Robot Commands Treebank (P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning, NP -P = drop in performance without integrated spatial planning, N/A = performance not available).	0
10572	10572	S14-6	Evaluation Metrics	5	87	0.833333333333333	0.439393939393939	These metrics contrast with measures for partially correct parsed structures, such as Parseval (Black et al., 1991) or the leaf-ancestor metric (Sampson and Babarczy, 2003).	0
10573	10573	S14-6	Evaluation Metrics	6	88	1.0	0.444444444444444	The rationale for using a strict match is that in the integrated system, a command will only be executed if it is completely understood, as both the spatial planner and the simulator require well-formed RCL.	0
10574	10574	S14-6	Systems and Results	1	89	0.090909090909091	0.44949494949495	Six teams participated in the shared task using a variety of strategies (Table 3).	0
10575	10575	S14-6	Systems and Results	2	90	0.181818181818182	0.454545454545455	The last measure in the table gives the performance drop without spatial context.	0
10576	10576	S14-6	Systems and Results	3	91	0.272727272727273	0.45959595959596	The value NP -P = -2 for the best performing system suggests this as an upper bound for the task.	0
10577	10577	S14-6	Systems and Results	4	92	0.363636363636364	0.464646464646465	The different values of this measure indicate the sensitivity to (or possibly reliance on) context to guide the parsing process.	0
10578	10578	S14-6	Systems and Results	5	93	0.454545454545455	0.46969696969697	In the remainder of this section we compare the approaches and results of the six systems.	0
10579	10579	S14-6	Systems and Results	6	94	0.545454545454545	0.474747474747475	Packard (2014) achieved the best score for parsing both with and without spatial context, at 92.5% and 90.5%, respectively, using a hybrid system that combines a rule-based grammar with the Berkeley parser (Petrov et al., 2006).	0
10580	10580	S14-6	Systems and Results	7	95	0.636363636363636	0.47979797979798	The rule-based component uses the English Resource Grammar, a broad coverage handwritten HPSG grammar for English.	0
10581	10581	S14-6	Systems and Results	8	96	0.727272727272727	0.484848484848485	The ERG produces a ranked list of Minimal Recursion Semantics (MRS) structures that encode predicate argument relations (Copestake et al., 2005).	0
10582	10582	S14-6	Systems and Results	9	97	0.818181818181818	0.48989898989899	Approximately 80 rules were then used to convert MRS to RCL.	0
10583	10583	S14-6	Systems and Results	10	98	0.909090909090909	0.494949494949495	The highest ranked result that is validated by the spatial planner was selected as the output of the rule-based system.	0
10584	10584	S14-6	Systems and Results	11	99	1.0	0.5	Using this approach, Packard reports scores of P = 82.4% and NP = 80.3% for parsing the evaluation data.	0
10585	10585	S14-6	UW-MRS:	1	100	0.166666666666667	0.505050505050505	To further boost performance, the Berkeley parser was used for back-off.	0
10586	10586	S14-6	UW-MRS:	2	101	0.333333333333333	0.51010101010101	To train the parser, the RCL treebank was converted to phrase struc-ture by removing non-aligned nodes and inserting additional nodes to ensure one-to-one alignment with words in NL sentences.	0
10587	10587	S14-6	UW-MRS:	3	102	0.5	0.515151515151515	Performance of the Berkeley parser alone was NP = 81.5% (no P-measure was available as spatial planning was not integrated).	0
10588	10588	S14-6	UW-MRS:	4	103	0.666666666666667	0.52020202020202	To combine components, the ERG was used initially, with fall back to the Berkeley parser when no contextually compatible RCL statement was produced.	0
10589	10589	S14-6	UW-MRS:	5	104	0.833333333333333	0.525252525252525	The hybrid approach improved accuracy considerably, with P = 92.5% and NP = 90.5%.	0
10590	10590	S14-6	UW-MRS:	6	105	1.0	0.53030303030303	Interestingly, Packard also performs precision and recall analysis, and reports that the rule-based component had higher precision, while the statistical component had higher recall, with the combined system outperforming each separate component in both precision and recall.	0
10591	10591	S14-6	AT&amp;	1	106	0.03448275862069	0.535353535353535	T Labs Research:	0
10592	10592	S14-6	AT&amp;	2	107	0.068965517241379	0.54040404040404	The system by Stoyanchev et al. (2014) scored second best for contextual parsing and third best for parsing without using the spatial planner (P = 87.35% and NP = 60.84%).	0
10593	10593	S14-6	AT&amp;	3	108	0.103448275862069	0.545454545454545	In contrast to Packard's UW-MRS submission, the AT&amp;	0
10594	10594	S14-6	AT&amp;	4	109	0.137931034482759	0.55050505050505	T system is a combination of three statistical models for tagging, parsing and reference resolution.	0
10595	10595	S14-6	AT&amp;	5	110	0.172413793103448	0.555555555555556	During the tagging phase, a two-stage sequence tagger first assigns a part-of-speech tag to each word in a sentence, followed by an RCL feature-value pair such as (type: cube) or (color: blue), with unaligned words tagged as 'O'.	0
10596	10596	S14-6	AT&amp;	6	111	0.206896551724138	0.560606060606061	For parsing, a constituency parser was trained using non-lexical RCL trees.	0
10597	10597	S14-6	AT&amp;	7	112	0.241379310344828	0.565656565656566	Finally, anaphoric references were resolved using a maximum entropy feature model.	0
10598	10598	S14-6	AT&amp;	8	113	0.275862068965517	0.570707070707071	When combined, the three components generate a list of weighted RCL trees, which are filtered by the spatial planner.	0
10599	10599	S14-6	AT&amp;	9	114	0.310344827586207	0.575757575757576	Without integrated planning, the most-probable parse tree is selected.	0
10600	10600	S14-6	AT&amp;	10	115	0.344827586206897	0.580808080808081	In their evaluation, Stoyanchev et al. report accuracy scores for the separate phases as well as for the combined system.	0
10601	10601	S14-6	AT&amp;	11	116	0.379310344827586	0.585858585858586	For the tagger, they report an accuracy score of 95.2%, using the standard split of 2,500 sentences for training and 909 for evaluation.	0
10602	10602	S14-6	AT&amp;	12	117	0.413793103448276	0.590909090909091	To separately measure the joint accuracy of the parser together with reference resolution, gold-standard tags were used resulting in a performance of P = 94.83% and NP = 67.55%.	0
10603	10603	S14-6	AT&amp;	13	118	0.448275862068966	0.595959595959596	However, using predicted tags, the system's final performance dropped to P = 87.35% and NP = 60.84%.	0
10604	10604	S14-6	AT&amp;	14	119	0.482758620689655	0.601010101010101	To measure the effect of less supervision, the models were additionally trained on only 500 sentences.	0
10605	10605	S14-6	AT&amp;	15	120	0.517241379310345	0.606060606060606	In this scenario, the tagging model degraded significantly, while the parsing and reference resolution models performed nearly as well.	0
10606	10606	S14-6	AT&amp;	16	121	0.551724137931034	0.611111111111111	RoBox: Using Combinatory Categorial Grammar (CCG) as a semantic parsing framework has been previously shown to be suitable for translating NL into logical form.	0
10607	10607	S14-6	AT&amp;	17	122	0.586206896551724	0.616161616161616	Inspired by previous work using a CCG parser in combination with a structured perceptron (Zettlemoyer and Collins, 2007), RoBox (Evang and Bos, 2014) was the best performing CCG system in the shared task scoring P = 86.8% and NP = 79.21%.	0
10608	10608	S14-6	AT&amp;	18	123	0.620689655172414	0.621212121212121	Using a similar approach to UW-MRS for its statistical component, RCL trees were interpreted as phrase-structure and converted to CCG derivations for training.	0
10609	10609	S14-6	AT&amp;	19	124	0.655172413793103	0.626262626262626	During decoding, RCL statements were generated directly by the CCG parser.	0
10610	10610	S14-6	AT&amp;	20	125	0.689655172413793	0.631313131313131	However, in contrast to the approach used by the AT&amp;	0
10611	10611	S14-6	AT&amp;	21	126	0.724137931034483	0.636363636363636	T system, RoBox interfaces with the planner during parsing instead of performing spatial validation a post-processing step.	0
10612	10612	S14-6	AT&amp;	22	127	0.758620689655172	0.641414141414141	This enables early resolution of attachment ambiguity and helps constrain the search space.	0
10613	10613	S14-6	AT&amp;	23	128	0.793103448275862	0.646464646464646	However, the planner is only used to validate entity elements, so that event and sequence elements were not validated.	0
10614	10614	S14-6	AT&amp;	24	129	0.827586206896552	0.651515151515152	As a further difference to the AT&amp;	0
10615	10615	S14-6	AT&amp;	25	130	0.862068965517241	0.656565656565657	T system, anaphora resolution was not performed using a statistical model.	0
10616	10616	S14-6	AT&amp;	26	131	0.896551724137931	0.661616161616162	Instead, multiple RCL trees were generated with different candidate anaphoric references, which were filtered out contextually using the spatial planner.	0
10617	10617	S14-6	AT&amp;	27	132	0.931034482758621	0.666666666666667	RoBox suffered only a 7.59% absolute drop in performance without using spatial planning, second only to UW-MRS at 2%.	0
10618	10618	S14-6	AT&amp;	28	133	0.96551724137931	0.671717171717172	Evang and Bos perform error analysis on RoBox and report that most errors relate to ellipsis, the ambiguous word one, anaphora or attachment ambiguity.	0
10619	10619	S14-6	AT&amp;	29	134	1.0	0.676767676767677	They suggest that the system could be improved with better feature selection or by integrating the CCG parser more closely with the spatial planner.	0
10620	10620	S14-6	Shrdlite:	1	135	0.083333333333333	0.681818181818182	The Shrdlite system by Ljunglöf (2014), inspired by the Classic SHRDLU system by Winograd (1972), is a purely rule-based sys-tem that was shown to be effective for the task.	0
10621	10621	S14-6	Shrdlite:	2	136	0.166666666666667	0.686868686868687	Scoring P = 86.1% and NP = 51.5%, Shrdlite ranked fourth for parsing with integrated planning, and fifth without using spatial context.	0
10622	10622	S14-6	Shrdlite:	3	137	0.25	0.691919191919192	However, it suffered the largest absolute drop in performance without planning (34.6 points), indicating that integration with the planner is essential for the system's reported accuracy.	0
10623	10623	S14-6	Shrdlite:	4	138	0.333333333333333	0.696969696969697	Shrdlite uses a hand-written compact unification grammar for the fragment of English appearing in the training data.	0
10624	10624	S14-6	Shrdlite:	5	139	0.416666666666667	0.702020202020202	The grammar is small, consisting of only 25 grammatical rules and 60 lexical rules implemented as a recursive-descent parser in Prolog.	0
10625	10625	S14-6	Shrdlite:	6	140	0.5	0.707070707070707	The lexicon consists of 150 words (and multi-word expressions) divided into 23 lexical categories, based on the RCL preterminal nodes found in the treebank.	0
10626	10626	S14-6	Shrdlite:	7	141	0.583333333333333	0.712121212121212	In a postprocessing phase, the resulting parse trees are normalized to ensure that they are well-formed by using a small set of supplementary rules.	0
10627	10627	S14-6	Shrdlite:	8	142	0.666666666666667	0.717171717171717	However, the grammar is highly ambiguous resulting in multiple parses for a given input sentence.	0
10628	10628	S14-6	Shrdlite:	9	143	0.75	0.722222222222222	These are filtered by the spatial planner.	0
10629	10629	S14-6	Shrdlite:	10	144	0.833333333333333	0.727272727272727	If multiple parse trees were found to be compatible with spatial context (or when not using the planner), the tree with the smallest number of nodes was selected as the parser's final output.	0
10630	10630	S14-6	Shrdlite:	11	145	0.916666666666667	0.732323232323232	Additionally, because both the training and evaluation data were collected via crowdsourcing, sentences occasionally contain spelling errors, which were intentionally included in the task.	0
10631	10631	S14-6	Shrdlite:	12	146	1.0	0.737373737373737	To handle misspelt words, Shrdlite uses Levenshtein edit distance with a penalty to reparse sentences when the parser initially fails to produce any analysis.	0
10632	10632	S14-6	KUL-Eval:	1	147	0.083333333333333	0.742424242424242	The CCG system by Mattelaer et al. (2014) uses a different approach to the RoBox system described previously.	0
10633	10633	S14-6	KUL-Eval:	2	148	0.166666666666667	0.747474747474748	KUL-Eval scored P = 71.29% and NP = 57.76% in comparison to the RoBox scores of P = 86.8% and NP = 79.21%.	0
10634	10634	S14-6	KUL-Eval:	3	149	0.25	0.752525252525252	During training, the RCL treebank was converted to λ-expressions.	0
10635	10635	S14-6	KUL-Eval:	4	150	0.333333333333333	0.757575757575758	This process is fully reversible, so that no information in an RCL tree is lost during conversion.	0
10636	10636	S14-6	KUL-Eval:	5	151	0.416666666666667	0.762626262626263	In contrast to RoBox, but in common with the AT&amp;	0
10637	10637	S14-6	KUL-Eval:	6	152	0.5	0.767676767676768	T parser, KUL-Eval performs spatial validation as a post-processing step and does not integrate the planner directly into the parsing process.	0
10638	10638	S14-6	KUL-Eval:	7	153	0.583333333333333	0.772727272727273	A probabilistic CCG is used for parsing, so that multiple λ-expressions are returned (each with an associated confidence measure) that are translated into RCL.	0
10639	10639	S14-6	KUL-Eval:	8	154	0.666666666666667	0.777777777777778	Finally, in the validation step, the spatial planner is used to discard RCL statements that are incompatible with spatial context and the remaining mostprobable parse is returned as the system's output.	0
10640	10640	S14-6	KUL-Eval:	9	155	0.75	0.782828282828283	Mattelaer et al. note that in several cases the parser produced partially correct statements but that these outputs did not contribute to the final score, given the strictly matching measures used for the P and NP metrics.	0
10641	10641	S14-6	KUL-Eval:	10	156	0.833333333333333	0.787878787878788	However, well-formed RCL statements are required by the spatial planner and robotic simulator for the integrated system to robustly execute the specified NL command.	0
10642	10642	S14-6	KUL-Eval:	11	157	0.916666666666667	0.792929292929293	Partially correct structures included statements which almost matched the expected RCL tree with the exception of incorrect featurevalues, or the addition or deletion of nodes.	0
10643	10643	S14-6	KUL-Eval:	12	158	1.0	0.797979797979798	The most common errors were feature-values with incorrect entity types (such as 'edge' and 'region') and mismatched spatial relations (such as confusing 'above ' and 'within' and confusing 'right', 'left' and 'front').	0
10644	10644	S14-6	UWM:	1	159	0.076923076923077	0.803030303030303	The UWM system submitted by Kate (2014) uses an existing semantic parser, KRISP, for the shared task.	0
10645	10645	S14-6	UWM:	2	160	0.153846153846154	0.808080808080808	KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) as the machine learning method with a string subsequence kernel.	0
10646	10646	S14-6	UWM:	3	161	0.230769230769231	0.813131313131313	As well as training data consisting of RCL paired with NL commands, KRISP required a context-free grammar for RCL, which was hand-written for UWM.	0
10647	10647	S14-6	UWM:	4	162	0.307692307692308	0.818181818181818	During training, id nodes were removed from the RCL trees.	0
10648	10648	S14-6	UWM:	5	163	0.384615384615385	0.823232323232323	These were recovered after parsing in a post-processing phase to resolve anaphora by matching to the nearest preceding antecedent.	0
10649	10649	S14-6	UWM:	6	164	0.461538461538462	0.828282828282828	In contrast to other systems submitted for the task, UWM does not interface with the spatial planner and parses purely non-contextually.	0
10650	10650	S14-6	UWM:	7	165	0.538461538461538	0.833333333333333	Because the planner was not used, the system's accuracy was negatively impacted by simple issues that may have been easily resolved using spatial context.	0
10651	10651	S14-6	UWM:	8	166	0.615384615384615	0.838383838383838	For example, in RCL, the verb 'place' can map to either drop or move actions, depending on whether or not a block is held in the gripper in the corresponding spatial scene.	0
10652	10652	S14-6	UWM:	9	167	0.692307692307692	0.843434343434343	Without using spatial context, it is hard to distinguish between these cases during parsing.	0
10653	10653	S14-6	UWM:	10	168	0.769230769230769	0.848484848484848	The system scored a non-contextual measure of NP = 45.98%, with Kate reporting a 51.18% best F-measure (at 72.67% precision and 39.49% recall).	0
10654	10654	S14-6	UWM:	11	169	0.846153846153846	0.853535353535353	No P-measure was reported as the spatial planner was not used.	0
10655	10655	S14-6	UWM:	12	170	0.923076923076923	0.858585858585859	Due to memory constraints when training the SVM classifiers, only 1,500 out of 2,500 possible sentences were used from the treebank to build the parsing model.	0
10656	10656	S14-6	UWM:	13	171	1.0	0.863636363636364	However, it may be possible to increasing the size of training data in future work through sampling.	0
10657	10657	S14-6	Discussion	1	172	0.071428571428572	0.868686868686869	The six systems evaluated for the task employed a variety of semantic parsing strategies.	0
10658	10658	S14-6	Discussion	2	173	0.142857142857143	0.873737373737374	With the exception of one submission, all systems interfaced with the spatial planner, either in a postprocessing phase, or directly during parsing to enable early disambiguation and to help constrain the search space.	0
10659	10659	S14-6	Discussion	3	174	0.214285714285714	0.878787878787879	An open question that remains following the task is how applicable these methods would be to other domains.	0
10660	10660	S14-6	Discussion	4	175	0.285714285714286	0.883838383838384	Systems that relied heavily on the planner to guide the parsing process could only be adapted to domains for a which a planner could conceivably exist.	0
10661	10661	S14-6	Discussion	5	176	0.357142857142857	0.888888888888889	For example, nearly all robotic tasks such as such as navigation, object manipulation and task execution involve aspects of planning.	0
10662	10662	S14-6	Discussion	6	177	0.428571428571429	0.893939393939394	NL question-answering interfaces to databases or knowledge stores are also good candidates for this approach, since parsing NL questions into a semantic representation within the context of a database schema or an ontology could be guided by a query planner.	0
10663	10663	S14-6	Discussion	7	178	0.5	0.898989898989899	However, approaches with a more attractive NP -P measure (such as UW-MRS and RoBox) are arguably more easily generalized to other domains, as they are less reliant on a planner.	0
10664	10664	S14-6	Discussion	8	179	0.571428571428571	0.904040404040404	Additionally, the usual arguments for rule-based systems verses supervised statistical systems apply to any discussion on domain adaptation: rulebased systems require human manual effort, while supervised statistical systems required annotated data for the new domain.	0
10665	10665	S14-6	Discussion	9	180	0.642857142857143	0.909090909090909	In comparing the best two statistical systems (AT&amp;T and RoBox) it is interesting to note that these performed similarly with integrated planning (P = 87.35% and 86.80%, respectively), but differed considerably without planning (NP = 60.84% and 79.21%).	0
10666	10666	S14-6	Discussion	10	181	0.714285714285714	0.914141414141414	As these two systems employed different parsers (a constituency parser and a CCG parser), it is difficult to perform a direct comparison to understand why the AT&amp;	0
10667	10667	S14-6	Discussion	11	182	0.785714285714286	0.919191919191919	T system is more reliant on spatial context.	0
10668	10668	S14-6	Discussion	12	183	0.857142857142857	0.924242424242424	It would also be interesting to understand, in further work, why the two CCG-based systems differed considerably in their P and NP scores.	0
10669	10669	S14-6	Discussion	13	184	0.928571428571429	0.929292929292929	It is also surprising that the best performing system, UW-MRS, suffered only a 2% drop in performance without using the planner, demonstrating clearly that in the majority of sentences in the evaluation data, spatial context is not actually required to perform semantic parsing.	0
10670	10670	S14-6	Discussion	14	185	1.0	0.934343434343434	Although as shown by the NP -P scores, spatial context can dramatically boost performance of certain approaches for the task when used.	0
10671	10671	S14-6	Conclusion and Future Work	1	186	0.076923076923077	0.939393939393939	This paper described a new task for SemEval: Supervised Semantic Parsing of Robotic Spatial Commands.	0
10672	10672	S14-6	Conclusion and Future Work	2	187	0.153846153846154	0.944444444444444	Despite its novel nature, the task attracted high-quality submissions from six teams, using a variety of semantic parsing strategies.	0
10673	10673	S14-6	Conclusion and Future Work	3	188	0.230769230769231	0.94949494949495	It is hoped that this task will reappear at Se-mEval.	0
10674	10674	S14-6	Conclusion and Future Work	4	189	0.307692307692308	0.954545454545455	Several lessons were learnt from this first version of the shared task which can be used to improve the task in future.	0
10675	10675	S14-6	Conclusion and Future Work	5	190	0.384615384615385	0.95959595959596	One issue which several participants noted was the way in which the treebank was split into training and evaluation datasets.	0
10676	10676	S14-6	Conclusion and Future Work	6	191	0.461538461538462	0.964646464646465	Out of the 3,409 sentences in the treebank, the first 2,500 sequential sentences were chosen for training.	0
10677	10677	S14-6	Conclusion and Future Work	7	192	0.538461538461538	0.96969696969697	Because this data was not randomized, certain syntactic structures were only found during evaluation and were not present in the training data.	0
10678	10678	S14-6	Conclusion and Future Work	8	193	0.615384615384615	0.974747474747475	Although this may have affected results, all participants evaluated their systems against the same datasets.	0
10679	10679	S14-6	Conclusion and Future Work	9	194	0.692307692307692	0.97979797979798	Based on participant feedback, in addition to reporting P and NP-measures, it would also be illuminating to include a metric such as Parseval F1-scores to measure partial accuracy.	0
10680	10680	S14-6	Conclusion and Future Work	10	195	0.769230769230769	0.984848484848485	An improved version of the task could also feature a better dataset by expanding the treebank, not only in terms of size but also in terms of linguistic structure.	0
10681	10681	S14-6	Conclusion and Future Work	11	196	0.846153846153846	0.98989898989899	Many commands captured in the annotation game are not yet represented in RCL due to linguistic phenomena such as negation and conditional statements.	0
10682	10682	S14-6	Conclusion and Future Work	12	197	0.923076923076923	0.994949494949495	Looking forward, a more promising approach to improving the spatial planner could be probabilistic planning, so that semantic parsers could interface with probabilistic facts with confidence measures.	0
10683	10683	S14-6	Conclusion and Future Work	13	198	1.0	1.0	This approach is particularly suitable for robotics, where sensors often supply noisy signals about the robot's environment.	0
11010	11010	S14-9	title	1	1	1.0	0.007042253521127	SemEval-2014 Task 9: Sentiment Analysis in Twitter	0
11011	11011	S14-9	abstract	1	2	0.166666666666667	0.014084507042254	We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014.	0
11012	11012	S14-9	abstract	2	3	0.333333333333333	0.02112676056338	It is a continuation of the last year's task that ran successfully as part of SemEval-2013.	0
11013	11013	S14-9	abstract	3	4	0.5	0.028169014084507	As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams).	0
11014	11014	S14-9	abstract	4	5	0.666666666666667	0.035211267605634	This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii) LiveJournal sentences.	0
11015	11015	S14-9	abstract	5	6	0.833333333333333	0.042253521126761	We further tested on (iv) 2013 tweets, and (v) 2013 SMS messages.	0
11016	11016	S14-9	abstract	6	7	1.0	0.049295774647887	The highest F1score on (i) was achieved by NRC-Canada at 86.63 for subtask A and by TeamX at 70.96 for subtask B.	0
11017	11017	S14-9	Introduction	1	8	0.038461538461539	0.056338028169014	In the past decade, new forms of communication have emerged and have become ubiquitous through social media.	0
11018	11018	S14-9	Introduction	2	9	0.076923076923077	0.063380281690141	Microblogs (e.g., Twitter), Weblogs (e.g., LiveJournal) and cell phone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.	0
11019	11019	S14-9	Introduction	3	10	0.115384615384615	0.070422535211268	Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.	0
11020	11020	S14-9	Introduction	4	11	0.153846153846154	0.077464788732394	The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, e.g., RT for re-tweet and #hashtags 1 .	0
11021	11021	S14-9	Introduction	5	12	0.192307692307692	0.084507042253521	This work is licensed under a Creative Commons Attribution 4.0 International Licence.	0
11022	11022	S14-9	Introduction	6	13	0.230769230769231	0.091549295774648	Page numbers and proceedings footer are added by the organisers.	0
11023	11023	S14-9	Introduction	7	14	0.269230769230769	0.098591549295775	Licence details: http://creativecommons.org/licenses/by/4.0/	0
11024	11024	S14-9	Introduction	8	15	0.307692307692308	0.105633802816901	1 Hashtags are a type of tagging for Twitter messages.	0
11025	11025	S14-9	Introduction	9	16	0.346153846153846	0.112676056338028	Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document.	0
11026	11026	S14-9	Introduction	10	17	0.384615384615385	0.119718309859155	How to handle such challenges so as to automatically mine and understand people's opinions and sentiments has only recently been the subject of research (Jansen et al., 2009;	0
11027	11027	S14-9	Introduction	11	18	0.423076923076923	0.126760563380282	Barbosa and Feng, 2010;	0
11028	11028	S14-9	Introduction	12	19	0.461538461538462	0.133802816901408	Bifet et al., 2011;Davidov et al., 2010;O'Connor et al., 2010;	0
11029	11029	S14-9	Introduction	13	20	0.5	0.140845070422535	Pak and Paroubek, 2010;Tumasjan et al., 2010;Kouloumpis et al., 2011).	0
11030	11030	S14-9	Introduction	14	21	0.538461538461538	0.147887323943662	Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year's SemEval Task 4 (Pontiki et al., 2014).	0
11031	11031	S14-9	Introduction	15	22	0.576923076923077	0.154929577464789	These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets.	0
11032	11032	S14-9	Introduction	16	23	0.615384615384615	0.161971830985916	While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment.	0
11033	11033	S14-9	Introduction	17	24	0.653846153846154	0.169014084507042	Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media.	0
11034	11034	S14-9	Introduction	18	25	0.692307692307692	0.176056338028169	Toward that goal, we created the Se-mEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013).	0
11035	11035	S14-9	Introduction	19	26	0.730769230769231	0.183098591549296	It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity.	0
11036	11036	S14-9	Introduction	20	27	0.769230769230769	0.190140845070423	This year, we extended the corpus by adding new tweets and LiveJournal sentences.	0
11037	11037	S14-9	Introduction	21	28	0.807692307692308	0.197183098591549	Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (González-Ibáñez et al., 2011;	0
11038	11038	S14-9	Introduction	22	29	0.846153846153846	0.204225352112676	Liebrecht et al., 2013).	0
11039	11039	S14-9	Introduction	23	30	0.884615384615385	0.211267605633803	In fact, sarcasm indicates that the message polarity should be flipped.	0
11040	11040	S14-9	Introduction	24	31	0.923076923076923	0.21830985915493	With this in mind, this year, we also evaluate on sarcastic tweets.	0
11041	11041	S14-9	Introduction	25	32	0.961538461538462	0.225352112676056	In the remainder of this paper, we first describe the task, the dataset creation process and the evaluation methodology.	0
11042	11042	S14-9	Introduction	26	33	1.0	0.232394366197183	We then summarize the characteristics of the approaches taken by the participating systems, and we discuss their scores.	0
11043	11043	S14-9	Task Description	1	34	0.25	0.23943661971831	As SemEval-2013	0
11044	11044	S14-9	Task Description	2	35	0.5	0.246478873239437	Task 2, we included two subtasks: an expression-level subtask and a messagelevel subtask.	0
11045	11045	S14-9	Task Description	3	36	0.75	0.253521126760563	Participants could choose to participate in either or both.	0
11046	11046	S14-9	Task Description	4	37	1.0	0.26056338028169	Below we provide short descriptions of the objectives of these two subtasks.	0
11047	11047	S14-9	Subtask A: Contextual Polarity Disambiguation	1	38	0.5	0.267605633802817	Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context.	1
11048	11048	S14-9	Subtask A: Contextual Polarity Disambiguation	2	39	1.0	0.274647887323944	The instance boundaries were provided: this was a classification task, not an entity recognition task.	0
11049	11049	S14-9	Subtask B: Message Polarity Classification	1	40	0.125	0.28169014084507	Given a message, decide whether it is of positive, negative, or neutral sentiment.	0
11050	11050	S14-9	Subtask B: Message Polarity Classification	2	41	0.25	0.288732394366197	For messages conveying both positive and negative sentiment, the stronger one is to be chosen.	0
11051	11051	S14-9	Subtask B: Message Polarity Classification	3	42	0.375	0.295774647887324	Each participating team was allowed to submit results for two different systems per subtask: one constrained, and one unconstrained.	0
11052	11052	S14-9	Subtask B: Message Polarity Classification	4	43	0.5	0.302816901408451	A constrained system could only use the provided data for training, but it could also use other resources such as lexicons obtained elsewhere.	0
11053	11053	S14-9	Subtask B: Message Polarity Classification	5	44	0.625	0.309859154929577	An unconstrained system could use any additional data as part of the training process; this could be done in a supervised, semi-supervised, or unsupervised fashion.	0
11054	11054	S14-9	Subtask B: Message Polarity Classification	6	45	0.75	0.316901408450704	Note that constrained/unconstrained refers to the data used to train a classifier.	0
11055	11055	S14-9	Subtask B: Message Polarity Classification	7	46	0.875	0.323943661971831	For example, if other data (excluding the test data) was used to develop a sentiment lexicon, and the lexicon was used to generate features, the system would still be constrained.	0
11056	11056	S14-9	Subtask B: Message Polarity Classification	8	47	1.0	0.330985915492958	However, if other data (excluding the test data) was used to develop a sentiment lexicon, and this lexicon was used to automatically label additional Tweet/SMS messages and then used with the original data to train the classifier, then such a system would be considered unconstrained.	0
11057	11057	S14-9	Datasets	1	48	1.0	0.338028169014084	In this section, we describe the process of collecting and annotating the 2014 testing tweets, including the sarcastic ones, and LiveJournal sentences.	0
11058	11058	S14-9	Datasets Used	1	49	0.25	0.345070422535211	For training and development, we released the Twitter train/dev/test datasets from SemEval-2013 task 2, as well as the SMS test set, which uses messages from the NUS SMS corpus (Chen and Kan, 2013), which we annotated for sentiment in 2013.	0
11059	11059	S14-9	Datasets Used	2	50	0.5	0.352112676056338	We further added a new 2014 Twitter test set, as well as a small set of tweets that contained the #sarcasm hashtag to determine how sarcasm affects the tweet polarity.	0
11060	11060	S14-9	Datasets Used	3	51	0.75	0.359154929577465	Finally, we included sentences from LiveJournal in order to determine how systems trained on Twitter perform on other sources.	0
11061	11061	S14-9	Datasets Used	4	52	1.0	0.366197183098592	The statistics for each dataset and for each subtask are shown in Tables 1 and 2.	0
11062	11062	S14-9	Annotation	1	53	0.2	0.373239436619718	We annotated the new tweets as in 2013: by identifying tweets from popular topics that contain sentiment-bearing words by using SentiWordNet (Baccianella et al., 2010) as a filter.	0
11063	11063	S14-9	Annotation	2	54	0.4	0.380281690140845	We altered the annotation task for the sarcastic tweets, displaying them to the Mechanical Turk annotators without the #sarcasm hashtag; the Turkers had to determine whether the tweet is sarcastic on their own.	0
11064	11064	S14-9	Annotation	3	55	0.6	0.387323943661972	Moreover, we asked Turkers to indicate the degree of sarcasm as (a) definitely sarcastic, (b) probably sarcastic, and (c) not sarcastic.	0
11065	11065	S14-9	Annotation	4	56	0.8	0.394366197183099	As in 2013, we combined the annotations using intersection, where a word had to appear in 2/3 of the annotations to be accepted.	0
11066	11066	S14-9	Annotation	5	57	1.0	0.401408450704225	An annotated example from each source is shown in Table 3.   [. . .], and are followed by their polarity; the sentence-level polarity is shown in the last column.	0
11067	11067	S14-9	Tweets Delivery	1	58	0.125	0.408450704225352	We did not deliver the annotated tweets to the participants directly; instead, we released annotation indexes, a list of corresponding Twitter IDs, and a download script that extracts the corresponding tweets via the Twitter API.	0
11068	11068	S14-9	Tweets Delivery	2	59	0.25	0.415492957746479	2	0
11069	11069	S14-9	Tweets Delivery	3	60	0.375	0.422535211267606	We provided the tweets in this manner in order to ensure that Twitter's terms of service are not violated.	0
11070	11070	S14-9	Tweets Delivery	4	61	0.5	0.429577464788732	Unfortunately, due to this restriction, the task participants had access to different number of training tweets depending on when they did the downloading.	0
11071	11071	S14-9	Tweets Delivery	5	62	0.625	0.436619718309859	This varied between a minimum of 5,215 tweets and the full set of 10,882 tweets.	0
11072	11072	S14-9	Tweets Delivery	6	63	0.75	0.443661971830986	On average the teams were able to collect close to 9,000 tweets; for teams that did not participate in 2013, this was about 8,500.	0
11073	11073	S14-9	Tweets Delivery	7	64	0.875	0.450704225352113	The difference in training data size did not seem to have had a major impact.	0
11074	11074	S14-9	Tweets Delivery	8	65	1.0	0.457746478873239	In fact, the top two teams in subtask B (coooolll and TeamX) trained on less than 8,500 tweets.	0
11075	11075	S14-9	Scoring	1	66	0.076923076923077	0.464788732394366	The participating systems were required to perform a three-way classification for both subtasks.	0
11076	11076	S14-9	Scoring	2	67	0.153846153846154	0.471830985915493	A particular marked phrase (for subtask A) or an entire message (for subtask B) was to be classified as positive, negative or objective/neutral.	0
11077	11077	S14-9	Scoring	3	68	0.230769230769231	0.47887323943662	We scored the systems by computing a score for predicting positive/negative phrases/messages.	0
11078	11078	S14-9	Scoring	4	69	0.307692307692308	0.485915492957747	For instance, to compute positive precision, p pos , we find the number of phrases/messages that a system correctly predicted to be positive, and we divide that number by the total number it predicted to be positive.	0
11079	11079	S14-9	Scoring	5	70	0.384615384615385	0.492957746478873	To compute positive recall, r pos , we find the number of phrases/messages correctly predicted to be positive and we divide that number by the total number of positives in the gold standard.	0
11080	11080	S14-9	Scoring	6	71	0.461538461538462	0.5	We then calculate F1-score for the positive class as follows F pos = 2(ppos+rpos) ppos * rpos .	0
11081	11081	S14-9	Scoring	7	72	0.538461538461538	0.507042253521127	We carry out a similar computation for F neg , for the negative phrases/messages.	0
11082	11082	S14-9	Scoring	8	73	0.615384615384615	0.514084507042254	The overall score is then F = (F pos + F neg )/2.	0
11083	11083	S14-9	Scoring	9	74	0.692307692307692	0.52112676056338	We used the two test sets from 2013 and the three from 2014, which we combined into one test set and we shuffled to make it hard to guess which set a sentence came from.	0
11084	11084	S14-9	Scoring	10	75	0.769230769230769	0.528169014084507	This guaranteed that participants would submit predictions for all five test sets.	0
11085	11085	S14-9	Scoring	11	76	0.846153846153846	0.535211267605634	It also allowed us to test how well systems trained on standard tweets generalize to sarcastic tweets and to LiveJournal sentences, without the participants putting extra efforts into this.	0
11086	11086	S14-9	Scoring	12	77	0.923076923076923	0.542253521126761	The participants were also not informed about the source the extra test sets come from.	0
11087	11087	S14-9	Scoring	13	78	1.0	0.549295774647887	We provided the participants with a scorer that outputs the overall score F and a confusion matrix for each of the five test sets.	0
11088	11088	S14-9	Participants and Results	1	79	0.076923076923077	0.556338028169014	The results are shown in Tables 4 and 5, and the team affiliations are shown in Table 6.	0
11089	11089	S14-9	Participants and Results	2	80	0.153846153846154	0.563380281690141	Tables 4  and 5 contain results on the two progress test sets (tweets and SMS messages), which are the official test sets from the 2013 edition of the task, and on the three new official 2014 testsets (tweets, tweets with sarcasm, and LiveJournal).	0
11090	11090	S14-9	Participants and Results	3	81	0.230769230769231	0.570422535211268	The tables further show macro-and micro-averaged results over the 2014 datasets.	0
11091	11091	S14-9	Participants and Results	4	82	0.307692307692308	0.577464788732394	There is an index for each result showing the relative rank of that result within the respective column.	0
11092	11092	S14-9	Participants and Results	5	83	0.384615384615385	0.584507042253521	The participating systems are ranked by their score on the Twitter-2014 testset, which is the official ranking for the task; all remaining rankings are secondary.	0
11093	11093	S14-9	Participants and Results	6	84	0.461538461538462	0.591549295774648	As we mentioned above, the participants were not told that the 2013 test sets would be included in the big 2014 test set, so that they do not overtune their systems on them.	0
11094	11094	S14-9	Participants and Results	7	85	0.538461538461538	0.598591549295775	However, the 2013 test sets were made available for development, but it was explicitly forbidden to use them for training.	0
11095	11095	S14-9	Participants and Results	8	86	0.615384615384615	0.605633802816901	Still, some participants did not notice this restriction, which resulted in their unusually high scores on Twitter2013-test; we did our best to identify all such cases, and we asked the authors to submit corrected runs.	0
11096	11096	S14-9	Participants and Results	9	87	0.692307692307692	0.612676056338028	The tables mark such resubmissions accordingly.	0
11097	11097	S14-9	Participants and Results	10	88	0.769230769230769	0.619718309859155	Most of the submissions were constrained, with just a few unconstrained: 7 out of 27 for subtask A, and 8 out of 50 for subtask B.	0
11098	11098	S14-9	Participants and Results	11	89	0.846153846153846	0.626760563380282	In any case, the best systems were constrained.	0
11099	11099	S14-9	Participants and Results	12	90	0.923076923076923	0.633802816901409	Some teams participated with both a constrained and an unconstrained system, but the unconstrained system was not always better than the constrained one: sometimes it was worse, sometimes it performed the same.	0
11100	11100	S14-9	Participants and Results	13	91	1.0	0.640845070422535	Thus, we decided to produce a single ranking, including both constrained and unconstrained systems, where we mark the latter accordingly.	0
11101	11101	S14-9	Subtask A	1	92	0.25	0.647887323943662	Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams.	0
11102	11102	S14-9	Subtask A	2	93	0.5	0.654929577464789	There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
11103	11103	S14-9	Subtask A	3	94	0.75	0.661971830985916	The best systems were constrained.	0
11104	11104	S14-9	Subtask A	4	95	1.0	0.669014084507042	All participating systems outperformed the majority class baseline by a sizable margin.	0
11105	11105	S14-9	Subtask B	1	96	0.2	0.676056338028169	The results for subtask B are shown in Table 5.	0
11106	11106	S14-9	Subtask B	2	97	0.4	0.683098591549296	The subtask attracted 50 submissions from 44 teams.	0
11107	11107	S14-9	Subtask B	3	98	0.6	0.690140845070423	There were eight unconstrained submissions: six teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
11108	11108	S14-9	Subtask B	4	99	0.8	0.697183098591549	As for subtask A, the best systems were constrained.	0
11109	11109	S14-9	Subtask B	5	100	1.0	0.704225352112676	Again, all participating systems outperformed the majority class baseline; however, some systems were very close to it.	0
11110	11110	S14-9	Discussion	1	101	0.03125	0.711267605633803	Overall, we observed similar trends as in SemEval-2013 Task 2.	0
11111	11111	S14-9	Discussion	2	102	0.0625	0.71830985915493	Almost all systems used supervised learning.	0
11112	11112	S14-9	Discussion	3	103	0.09375	0.725352112676056	Most systems were constrained, including the best ones in all categories.	0
11113	11113	S14-9	Discussion	4	104	0.125	0.732394366197183	As in 2013, we observed several cases of a team submitting a constrained and an unconstrained run and the constrained run performing better.	0
11114	11114	S14-9	Discussion	5	105	0.15625	0.73943661971831	It is unclear why unconstrained systems did not outperform constrained ones.	0
11115	11115	S14-9	Discussion	6	106	0.1875	0.746478873239437	It could be because participants did not use enough external data or because the data they used was too different from Twitter or from our annotation method.	0
11116	11116	S14-9	Discussion	7	107	0.21875	0.753521126760563	Or it could be due to our definition of unconstrained, which labels as unconstrained systems that use additional tweets directly, but considers unconstrained those that use additional tweets to build sentiment lexicons and then use these lexicons.	0
11117	11117	S14-9	Discussion	8	108	0.25	0.76056338028169	As in 2013, the most popular classifiers were SVM, MaxEnt, and Naive Bayes.	0
11118	11118	S14-9	Discussion	9	109	0.28125	0.767605633802817	Moreover, two submissions used deep learning, coooolll (Harbin Institute of Technology) and ThinkPositive (IBM Research, Brazil), which were ranked second and tenth on subtask B, respectively.	0
11119	11119	S14-9	Discussion	10	110	0.3125	0.774647887323944	The features used were quite varied, including word-based (e.g., word and character ngrams, word shapes, and lemmata), syntactic, and Twitter-specific such as emoticons and abbreviations.	0
11120	11120	S14-9	Discussion	11	111	0.34375	0.78169014084507	The participants still relied heavily on lexicons of opinion words, the most popular ones being the same as in 2013: MPQA, SentiWord-Net and Bing Liu's opinion lexicon.	0
11121	11121	S14-9	Discussion	12	112	0.375	0.788732394366197	Popular this year was also the NRC lexicon (Mohammad et al., 2013), created by the best-performing team in 2013, which is top-performing this year as well.	0
11122	11122	S14-9	Discussion	13	113	0.40625	0.795774647887324	Preprocessing of tweets was still a popular technique.	0
11123	11123	S14-9	Discussion	14	114	0.4375	0.802816901408451	In addition to standard NLP steps such as tokenization, stemming, lemmatization, stopword removal and POS tagging, most teams applied some kind of Twitter-specific processing such as substitution/removal of URLs, substitution of emoticons, word normalization, abbreviation lookup, and punctuation removal.	0
11124	11124	S14-9	Discussion	15	115	0.46875	0.809859154929577	Finally, several of the teams used Twitter-tuned NLP tools such as part of speech and named entity taggers (Gimpel et al., 2011;	0
11125	11125	S14-9	Discussion	16	116	0.5	0.816901408450704	Ritter et al., 2011).	0
11126	11126	S14-9	Discussion	17	117	0.53125	0.823943661971831	The similarity of preprocessing techniques, NLP tools, classifiers and features used in 2013 and this year is probably partially due to many teams participating in both years.	0
11127	11127	S14-9	Discussion	18	118	0.5625	0.830985915492958	As Table 6 shows, 18 out of the 46 teams are returning teams.	0
11128	11128	S14-9	Discussion	19	119	0.59375	0.838028169014084	Comparing the results on the progress Twitter test in 2013 and 2014, we can see that NRC-Canada, the 2013 winner for subtask A, have now improved their F1 score from 88.93 to 90.14, which is the 2014 best score.	0
11129	11129	S14-9	Discussion	20	120	0.625	0.845070422535211	The best score on the Progress SMS in 2014 of 89.31 belongs to ECNU; this is a big jump compared to their 2013 score of 76.69, but it is less compared to the 2013 best of 88.37 achieved by GU-MLT-LT.	0
11130	11130	S14-9	Discussion	21	121	0.65625	0.852112676056338	For subtask B, on the Twitter progress testset, the 2013 winner NRC-Canada improves their 2013 result from 69.02 to 70.75, which is the second best in 2014; the winner in 2014, TeamX, achieves 72.12.	0
11131	11131	S14-9	Discussion	22	122	0.6875	0.859154929577465	On the SMS progress test, the 2013 winner NRC-Canada improves its F1 score from 68.46 to 70.28.	0
11132	11132	S14-9	Discussion	23	123	0.71875	0.866197183098591	Overall, we see consistent improvements on the progress testset for both subtasks: 0-1 and 2-3 points absolute for subtasks A and B, respectively.	0
11133	11133	S14-9	Discussion	24	124	0.75	0.873239436619718	Table 4: Results for subtask A.	0
11134	11134	S14-9	Discussion	25	125	0.78125	0.880281690140845	The * indicates system resubmissions (because they initially trained on Twitter2013-test), and the indicates a system that includes a task co-organizer as a team member.	0
11135	11135	S14-9	Discussion	26	126	0.8125	0.887323943661972	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
11136	11136	S14-9	Discussion	27	127	0.84375	0.894366197183099	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
11137	11137	S14-9	Discussion	28	128	0.875	0.901408450704225	Finally, note that for both subtasks, the best systems on the Twitter-2014 dataset are those that performed best on the 2013 progress Twitter dataset: NRC-Canada for subtask A, and TeamX (Fuji Xerox Co., Ltd.) for subtask B.	0
11138	11138	S14-9	Discussion	29	129	0.90625	0.908450704225352	It is interesting to note that the best results for Twitter2014-test are lower than those for Twitter2013-test for both subtask A (86.63 vs. 90.14) and subtask B (70.96 vs 72.12).	0
11139	11139	S14-9	Discussion	30	130	0.9375	0.915492957746479	This is so despite the baselines for Twitter2014-test being higher than those for Twitter2013-test: 42.2 vs. 38.1 for subtask A, and 34.6 vs. 29.2 for subtask B. Most likely, having access to Twitter2013-test at development time, teams have overfitted on it.	0
11140	11140	S14-9	Discussion	31	131	0.96875	0.922535211267606	It could be also the case that some of the sentiment dictionaries that were built in 2013 have become somewhat outdated by 2014.	0
11141	11141	S14-9	Discussion	32	132	1.0	0.929577464788732	Finally, note that while some teams such as NRC-Canada performed well across all test sets, other such as TeamX, which used a weighting scheme tuned specifically for class imbalances in tweets, were only strong on Twitter datasets.	0
11142	11142	S14-9	Conclusion	1	133	0.1	0.936619718309859	We have described the data, the experimental setup and the results for SemEval-2014 Task 9.	0
11143	11143	S14-9	Conclusion	2	134	0.2	0.943661971830986	As in 2013, our task was the most popular one at SemEval-2014, attracting 46 participating teams: 21 in subtask A (27 submissions) and 44 in subtask B (50 submissions).	0
11144	11144	S14-9	Conclusion	3	135	0.3	0.950704225352113	We introduced three new test sets for 2014: an in-domain Twitter dataset, an out-of-domain Live-Journal test set, and a dataset of tweets containing sarcastic content.	0
11145	11145	S14-9	Conclusion	4	136	0.4	0.957746478873239	While the performance on the LiveJournal test set was mostly comparable to the in-domain Twitter test set, for most teams there was a sharp drop in performance for sarcastic tweets, highlighting better handling of sarcastic language as one important direction for future work in Twitter sentiment analysis.	0
11146	11146	S14-9	Conclusion	5	137	0.5	0.964788732394366	We plan to run the task again in 2015 with the inclusion of a new sub-evaluation on detecting sarcasm with the goal of stimulating research in this area; we further plan to add one more test domain.	0
11147	11147	S14-9	Conclusion	6	138	0.6	0.971830985915493	-test), and the indicates a system that includes a task co-organizer as a team member.	0
11148	11148	S14-9	Conclusion	7	139	0.7	0.97887323943662	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
11149	11149	S14-9	Conclusion	8	140	0.8	0.985915492957746	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
11150	11150	S14-9	Conclusion	9	141	0.9	0.992957746478873	In the 2015 edition of the task, we might also remove the constrained/unconstrained distinction.	0
11151	11151	S14-9	Conclusion	10	142	1.0	1.0	Finally, as there are multiple opinions about a topic in Twitter, we would like to focus on detecting the sentiment trend towards a topic.	0
12453	12453	S15-6	title	1	1	1.0	0.007246376811594	SemEval-2015 Task 6: Clinical TempEval	0
12454	12454	S15-6	abstract	1	2	0.25	0.014492753623188	Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.	0
12455	12455	S15-6	abstract	2	3	0.5	0.021739130434783	Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.	1
12456	12456	S15-6	abstract	3	4	0.75	0.028985507246377	Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.	0
12457	12457	S15-6	abstract	4	5	1.0	0.036231884057971	Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.	0
12458	12458	S15-6	Introduction	1	6	0.083333333333333	0.043478260869565	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
12459	12459	S15-6	Introduction	2	7	0.166666666666667	0.05072463768116	Verhagen et al., 2010;UzZaman et al., 2013).	0
12460	12460	S15-6	Introduction	3	8	0.25	0.057971014492754	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	0
12461	12461	S15-6	Introduction	4	9	0.333333333333333	0.065217391304348	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
12462	12462	S15-6	Introduction	5	10	0.416666666666667	0.072463768115942	Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic.	0
12463	12463	S15-6	Introduction	6	11	0.5	0.079710144927536	This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understanding of the language of time beyond newswire expressions and structure.	0
12464	12464	S15-6	Introduction	7	12	0.583333333333333	0.086956521739131	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
12465	12465	S15-6	Introduction	8	13	0.666666666666667	0.094202898550725	Participating systems are expected to take as input raw text such as:	0
12466	12466	S15-6	Introduction	9	14	0.75	0.101449275362319	April 23, 2014:	0
12467	12467	S15-6	Introduction	10	15	0.833333333333333	0.108695652173913	The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
12468	12468	S15-6	Introduction	11	16	0.916666666666667	0.115942028985507	And output annotations over the text that capture the following kinds of information:	0
12469	12469	S15-6	Introduction	12	17	1.0	0.123188405797101	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
12470	12470	S15-6	Data	1	18	0.125	0.130434782608696	The Clinical TempEval corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic.	0
12471	12471	S15-6	Data	2	19	0.25	0.13768115942029	These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expressions were not altered.	0
12472	12472	S15-6	Data	3	20	0.375	0.144927536231884	The notes were then manually annotated by the THYME project (thyme.healthnlp.org) using an extension of ISO-TimeML for the annotation of times, events and temporal relations in clinical notes (Styler et al., 2014b).	0
12473	12473	S15-6	Data	4	21	0.5	0.152173913043478	This extension includes additions such as new time expression types (e.g., PREPOSTEXP for expressions like postoperative), new EVENT attributes (e.g., DE-GREE=LITTLE for expressions like slight nausea), and an increased focus on temporal relations of type CONTAINS (a.k.a. INCLUDES).	0
12474	12474	S15-6	Data	5	22	0.625	0.159420289855072	The annotation procedure was as follows:	0
12475	12475	S15-6	Data	6	23	0.75	0.166666666666667	1. Annotators identified time and event expressions, along with their attributes 2.	0
12476	12476	S15-6	Data	7	24	0.875	0.173913043478261	Adjudicators revised and finalized the time and event expressions and their attributes 3.	0
12477	12477	S15-6	Data	8	25	1.0	0.181159420289855	Annotators identified temporal relations between pairs of events and events and times	0
12478	12478	S15-6	Adjudicators revised and finalized the temporal relations	1	26	0.1	0.188405797101449	More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a).	0
12479	12479	S15-6	Adjudicators revised and finalized the temporal relations	2	27	0.2	0.195652173913043	Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.	0
12480	12480	S15-6	Adjudicators revised and finalized the temporal relations	3	28	0.3	0.202898550724638	1	0
12481	12481	S15-6	Adjudicators revised and finalized the temporal relations	4	29	0.4	0.210144927536232	The event, time and temporal relation annotations were distributed separately from the text, in an open source repository 2 using the Anafora standoff format (Chen and Styler, 2013).	0
12482	12482	S15-6	Adjudicators revised and finalized the temporal relations	5	30	0.5	0.217391304347826	1	0
12483	12483	S15-6	Adjudicators revised and finalized the temporal relations	6	31	0.6	0.22463768115942	The details of this process are described at http://thyme.	0
12484	12484	S15-6	Adjudicators revised and finalized the temporal relations	7	32	0.7	0.231884057971014	The corpus was split into three portions: Train (50%), Dev (25%) and Test (25%).	0
12485	12485	S15-6	Adjudicators revised and finalized the temporal relations	8	33	0.8	0.239130434782609	For Clinical TempEval 2015, the Train portion was used for training and the Dev portion was used for testing.	0
12486	12486	S15-6	Adjudicators revised and finalized the temporal relations	9	34	0.9	0.246376811594203	The Test portion was not distributed, and was reserved as a test set for a future iteration of the shared task.	0
12487	12487	S15-6	Adjudicators revised and finalized the temporal relations	10	35	1.0	0.253623188405797	Table 1 shows the number of documents, event expressions (EVENT annotations), time expressions (TIMEX3 annotations) and narrative container relations (TLINK annotations with TYPE=CONTAINS attributes) in the Train and Dev portions of the corpus.	0
12488	12488	S15-6	Tasks	1	36	0.25	0.260869565217391	A total of nine tasks were included, grouped into three categories:  (Pustejovsky and Stubbs, 2011) between events and/or times, represented by TLINK annotations with TYPE=CONTAINS in the THYME corpus	0
12489	12489	S15-6	Tasks	2	37	0.5	0.268115942028985	The evaluation was run in two phases:	0
12490	12490	S15-6	Tasks	3	38	0.75	0.27536231884058	1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2.	0
12491	12491	S15-6	Tasks	4	39	1.0	0.282608695652174	Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations	0
12492	12492	S15-6	Evaluation Metrics	1	40	0.045454545454546	0.289855072463768	All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F 1 :	0
12493	12493	S15-6	Evaluation Metrics	2	41	0.090909090909091	0.297101449275362	where S is the set of items predicted by the system and H is the set of items manually annotated by the humans.	0
12494	12494	S15-6	Evaluation Metrics	3	42	0.136363636363636	0.304347826086957	"Applying these metrics to the tasks only requires a definition of what is considered an ""item"" for each task."	0
12495	12495	S15-6	Evaluation Metrics	4	43	0.181818181818182	0.311594202898551	•	0
12496	12496	S15-6	Evaluation Metrics	5	44	0.227272727272727	0.318840579710145	For evaluating the spans of event expressions or time expressions, items were tuples of (begin, end) character offsets.	0
12497	12497	S15-6	Evaluation Metrics	6	45	0.272727272727273	0.326086956521739	Thus, systems only received credit for identifying events and times with exactly the same character offsets as the manually annotated ones.	0
12498	12498	S15-6	Evaluation Metrics	7	46	0.318181818181818	0.333333333333333	•	0
12499	12499	S15-6	Evaluation Metrics	8	47	0.363636363636364	0.340579710144928	For evaluating the attributes of event expressions or time expressions -Class, Contextual	0
12500	12500	S15-6	Evaluation Metrics	9	48	0.409090909090909	0.347826086956522	Modality, Degree, Polarity and Type -items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute.	0
12501	12501	S15-6	Evaluation Metrics	10	49	0.454545454545455	0.355072463768116	Thus, systems only received credit for an event (or time) attribute if they both found an event (or time) with the correct character offsets and then assigned the correct value for that attribute.	0
12502	12502	S15-6	Evaluation Metrics	11	50	0.5	0.36231884057971	•	0
12503	12503	S15-6	Evaluation Metrics	12	51	0.545454545454545	0.369565217391304	For relations between events and the document creation time, items were tuples of (begin, end, value), just as if it were an event attribute.	0
12504	12504	S15-6	Evaluation Metrics	13	52	0.590909090909091	0.376811594202899	Thus, systems only received credit if they found a correct event and assigned the correct relation (BEFORE, OVERLAP, BEFORE-OVERLAP or AFTER) between that event and the document creation time.	0
12505	12505	S15-6	Evaluation Metrics	14	53	0.636363636363636	0.384057971014493	Note that in the second phase of the evaluation, when manual event annotations were given as input, precision, recall and F 1 are all equivalent to standard accuracy.	0
12506	12506	S15-6	Evaluation Metrics	15	54	0.681818181818182	0.391304347826087	•	0
12507	12507	S15-6	Evaluation Metrics	16	55	0.727272727272727	0.398550724637681	For narrative container relations, items were tuples of ((begin 1 , end 1 ), (begin 2 , end 2 )), where the begins and ends corresponded to the character offsets of the events or times participating in the relation.	0
12508	12508	S15-6	Evaluation Metrics	17	56	0.772727272727273	0.405797101449275	Thus, systems only received credit for a narrative container relation if they found both events/times and correctly assigned a CONTAINS relation between them.	0
12509	12509	S15-6	Evaluation Metrics	18	57	0.818181818181818	0.41304347826087	For attributes, an additional metric measures how accurately a system predicts the attribute values on just those events or times that the system predicted.	0
12510	12510	S15-6	Evaluation Metrics	19	58	0.863636363636364	0.420289855072464	The goal here is to allow a comparison across systems for assigning attribute values, even when different systems produce very different numbers of events and times.	0
12511	12511	S15-6	Evaluation Metrics	20	59	0.909090909090909	0.427536231884058	This is calculated by dividing the F 1 on the attribute by the F 1 on identifying the spans:	0
12512	12512	S15-6	Evaluation Metrics	21	60	0.954545454545455	0.434782608695652	For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A CON-TAINS B and B CONTAINS C, so A CONTAINS C):	0
12513	12513	S15-6	Evaluation Metrics	22	61	1.0	0.442028985507246	These measures take the approach of prior work (Uz-Zaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure).	0
12514	12514	S15-6	Baseline Systems	1	62	0.090909090909091	0.449275362318841	Two rule-based systems were used as baselines to compare the participating systems against.	0
12515	12515	S15-6	Baseline Systems	2	63	0.181818181818182	0.456521739130435	memorize	0
12516	12516	S15-6	Baseline Systems	3	64	0.272727272727273	0.463768115942029	For all tasks but the narrative container task, a memorization-based baseline was used.	0
12517	12517	S15-6	Baseline Systems	4	65	0.363636363636364	0.471014492753623	To train the model, all phrases annotated as either events or times in the training data were collected.	0
12518	12518	S15-6	Baseline Systems	5	66	0.454545454545455	0.478260869565217	All exact character matches for these phrases in the training data were then examined, and only phrases that were annotated as events or times greater than 50% of the time were retained.	0
12519	12519	S15-6	Baseline Systems	6	67	0.545454545454545	0.485507246376812	For each phrase, the most frequently annotated type (event or time) and attribute values for instances of that phrase were determined.	0
12520	12520	S15-6	Baseline Systems	7	68	0.636363636363636	0.492753623188406	To predict with the model, the raw text of the test data was searched for all exact character matches of any of the memorized phrases, preferring longer phrases when multiple matches overlapped.	0
12521	12521	S15-6	Baseline Systems	8	69	0.727272727272727	0.5	Wherever a phrase match was found, an event or time with the memorized (most frequent) attribute values was predicted.	0
12522	12522	S15-6	Baseline Systems	9	70	0.818181818181818	0.507246376811594	closest	0
12523	12523	S15-6	Baseline Systems	10	71	0.909090909090909	0.514492753623188	For the narrative container task, a proximitybased baseline was used.	0
12524	12524	S15-6	Baseline Systems	11	72	1.0	0.521739130434783	Each time expression was predicted to be a narrative container, containing only the closest event expression to it in the text.	0
12525	12525	S15-6	Participating Systems	1	73	1.0	0.528985507246377	Three research teams submitted a total of 13 runs:	0
12526	12526	S15-6	Human Agreement	1	74	0.125	0.536231884057971	We also give two types of human agreement on the task, measured with the same evaluation metrics as the systems:	0
12527	12527	S15-6	Human Agreement	2	75	0.25	0.543478260869565	ann-ann Inter-annotator agreement between the two independent human annotators who annotated each document.	0
12528	12528	S15-6	Human Agreement	3	76	0.375	0.550724637681159	This is the most commonly reported type of agreement, and often considered to be an upper bound on system performance.	0
12529	12529	S15-6	Human Agreement	4	77	0.5	0.557971014492754	adj-ann Inter-annotator agreement between the adjudicator and the two independent annotators.	0
12530	12530	S15-6	Human Agreement	5	78	0.625	0.565217391304348	This is usually a better bound on system performance in adjudicated corpora, since the models are trained on the adjudicated data, not on the individual annotator data.	0
12531	12531	S15-6	Human Agreement	6	79	0.75	0.572463768115942	"Precision and recall are not reported in these scenarios since they depend on the arbitrary choice of one annotator as the ""human"" (H) and the other as the ""system"" (S)."	0
12532	12532	S15-6	Human Agreement	7	80	0.875	0.579710144927536	Note that since temporal relations between events and the document creation time were annotated at the same time as the events themselves, agreement for this task is only reported in phase 1 of the evaluation.	0
12533	12533	S15-6	Human Agreement	8	81	1.0	0.58695652173913	Similarly, since narrative container relations were only annotated after events and times had been adjudicated, agreement for this task is only reported in phase 2 of the evaluation.	0
12534	12534	S15-6	Evaluation Results	1	82	0.071428571428572	0.594202898550725	Time Expressions	0
12535	12535	S15-6	Evaluation Results	2	83	0.142857142857143	0.601449275362319	Table 2 shows results on the time expression tasks.	0
12536	12536	S15-6	Evaluation Results	3	84	0.214285714285714	0.608695652173913	The BluLab system achieved the best F 1 at identifying time expressions, 0.725.	0
12537	12537	S15-6	Evaluation Results	4	85	0.285714285714286	0.615942028985507	The other machine learning systems (KPSCMI run 2-3 and UFPRSheffield-SVM run 1-2) achieved F 1 in the 0.690-0.700 range.	0
12538	12538	S15-6	Evaluation Results	5	86	0.357142857142857	0.623188405797101	The rule-based systems (KPSCMI run 1 and UFPRSheffield-Hynx run 1-5) all achieved higher recall than the machine learning systems, but at substantial costs to precision.	0
12539	12539	S15-6	Evaluation Results	6	87	0.428571428571429	0.630434782608696	All systems outperformed the memorization baseline in terms of recall, and all machine-learning systems outperformed it in terms of F 1 , but only the BluLab system outperformed the baseline in terms of precision.	0
12540	12540	S15-6	Evaluation Results	7	88	0.5	0.63768115942029	The BluLab system also achieved the best F 1 for predicting the classes of time expressions, though this is primarily due to achieving a higher F 1 at identifying time expressions in the first place.	0
12541	12541	S15-6	Evaluation Results	8	89	0.571428571428571	0.644927536231884	UFPRSheffield-Hynx run 5 achieved the best accuracy on predicting classes for the time expressions it found, 0.978, though on this metric it only outperformed the memorization baseline by 0.004.	0
12542	12542	S15-6	Evaluation Results	9	90	0.642857142857143	0.652173913043478	Across the time expression tasks, systems did not quite achieve performance at the level of human agreement.	0
12543	12543	S15-6	Evaluation Results	10	91	0.714285714285714	0.659420289855072	For the spans of time expressions, the top system achieved 0.725 F 1, compared to 0.774 adjudicator-annotator F 1 , though almost half of the systems exceeded the lower annotator-annotator F 1 of 0.690.	0
12544	12544	S15-6	Evaluation Results	11	92	0.785714285714286	0.666666666666667	For the classes of time expressions, the story was similar for F 1 , though several models exceeded the adjudicator-annotator accuracy of 0.965 on just the time expressions predicted by the system.	0
12545	12545	S15-6	Evaluation Results	12	93	0.857142857142857	0.673913043478261	one exception was the semantic type of the event, where the memorization baseline had a better precision and also a better accuracy on the classes of the events that it identified.	0
12546	12546	S15-6	Evaluation Results	13	94	0.928571428571429	0.681159420289855	The BluLab system got close to the level of adjudicator-annotator agreement on identifying the spans of event expressions (0.875 vs. 0.880 F 1 ), identifying the degree of events (0.870 vs. 0.877 F 1 ), and identifying the polarity of events (0.857 vs. 0.869 F 1 ), and it generally met or exceeded the lower annotator-annotator agreement on these tasks.	0
12547	12547	S15-6	Evaluation Results	14	95	1.0	0.688405797101449	There is a larger gap (3+ points of F 1 ) between the system performance and adjudicator-annotator agreement for event modality and event type, though only a small gap (&lt;1 point of F 1 ) for the lower annotatorannotator agreement on these tasks.	0
12548	12548	S15-6	Event Expressions	1	96	0.111111111111111	0.695652173913043	Temporal Relations	0
12549	12549	S15-6	Event Expressions	2	97	0.222222222222222	0.702898550724638	Table 4 shows performance on the temporal relation tasks.	0
12550	12550	S15-6	Event Expressions	3	98	0.333333333333333	0.710144927536232	In detecting the relations between events and the document creation time, the BluLab system substantially outperformed the memorization baseline, achieving F 1 of 0.702 on system-predicted events (phase 1) and F 1 of 0.791 on manually annotated events (phase 2).	0
12551	12551	S15-6	Event Expressions	4	99	0.444444444444444	0.717391304347826	In identifying narrative container relations, the best BluLab system (run 2) outperformed the proximity-based baseline when using systempredicted events (F closure of 0.123 vs. 0.106) but not when using manually annotated events (F closure of 0.181 vs. 0.260).	0
12552	12552	S15-6	Event Expressions	5	100	0.555555555555556	0.72463768115942	Across both phase 1 and phase 2 for narrative container relations, the top BluLab system always had the best recall, while the baseline system always had the best precision.	0
12553	12553	S15-6	Event Expressions	6	101	0.666666666666667	0.731884057971014	Annotator agreement was higher than system performance on all temporal relation tasks.	0
12554	12554	S15-6	Event Expressions	7	102	0.777777777777778	0.739130434782609	For relations between events and the document creation time, adjudicator-annotator agreement was 0.761 F 1 , compared to the best system's 0.702 F 1 , though this system did exceed the lower annotator-annotator agreement of 0.628 F 1 .	0
12555	12555	S15-6	Event Expressions	8	103	0.888888888888889	0.746376811594203	For narrative container relations using manually annotated EVENTs and TIMEX3s, the gap was much greater, with adjudicator-annotator agreement at 0.672 F closure , and the top system (the baseline system) at 0.260 F closure .	0
12556	12556	S15-6	Event Expressions	9	104	1.0	0.753623188405797	Even the lower annotator-annotator agreement of 0.475 F closure was much higher than the system performance.	0
12557	12557	S15-6	Discussion	1	105	0.029411764705882	0.760869565217391	The results of Clinical TempEval 2015 suggest that a small number of temporal information extraction tasks are solved by current state-of-the-art systems, but for the majority of tasks, there is still room for improvement.	0
12558	12558	S15-6	Discussion	2	106	0.058823529411765	0.768115942028985	Identifying events, their degrees and their polarities were the easiest tasks for the participants, with the best systems achieving within about 0.01 of human agreement on the tasks.	0
12559	12559	S15-6	Discussion	3	107	0.088235294117647	0.77536231884058	Systems for identifying event modality and event type were not far behind, achieving within about 0.03 of human agree-  ment.	0
12560	12560	S15-6	Discussion	4	108	0.117647058823529	0.782608695652174	Time expressions and relations to the document creation time were at the next level of difficulty, with a gap of about 0.05 from human agreement.	0
12561	12561	S15-6	Discussion	5	109	0.147058823529412	0.789855072463768	Identifying narrative container relations was by far the most difficult task, with the best systems down by more than 0.40 from human agreement.	0
12562	12562	S15-6	Discussion	6	110	0.176470588235294	0.797101449275362	In absolute terms, performance on narrative container relations was also quite low, with system F closure scores in the 0.10-0.12 range on system-generated events and times, and in the 0.12-0.26 range on manually-annotated events and times.	0
12563	12563	S15-6	Discussion	7	111	0.205882352941176	0.804347826086956	For comparison, in TempEval 2013, which used newswire data, F closure scores were in the 0.24-0.36 range on systemgenerated events and times and in the 0.35-0.56 range on manually-annotated events and times (UzZaman et al., 2013).	0
12564	12564	S15-6	Discussion	8	112	0.235294117647059	0.811594202898551	One major difference between the corpora is that the narrative container relations in the clinical domain often span many sentences, while almost all of the relations in TempEval 2013 were either within the same sentence or across adjacent sentences.	0
12565	12565	S15-6	Discussion	9	113	0.264705882352941	0.818840579710145	Most past research systems have also focused on identifying within-sentence and adjacent-sentence relations.	0
12566	12566	S15-6	Discussion	10	114	0.294117647058823	0.826086956521739	This focus on local relations might explain the poor performance on the more distant relations in the THYME corpus.	0
12567	12567	S15-6	Discussion	11	115	0.323529411764706	0.833333333333333	But further investigation is needed to better understand the challenge here.	0
12568	12568	S15-6	Discussion	12	116	0.352941176470588	0.840579710144927	In almost all tasks, the submitted systems substantially outperformed the baselines.	0
12569	12569	S15-6	Discussion	13	117	0.382352941176471	0.847826086956522	The one exception to this was the narrative containers task.	0
12570	12570	S15-6	Discussion	14	118	0.411764705882353	0.855072463768116	The baseline there -which simply predicted that each time expression contained the nearest event expression to it in the text -achieved 4 times the precision of the best submitted system and consequently achieved the best F 1 by a large margin.	0
12571	12571	S15-6	Discussion	15	119	0.441176470588235	0.86231884057971	This suggests that future systems may want to incorporate better measures of proximity that can capture some of what the baseline is finding.	0
12572	12572	S15-6	Discussion	16	120	0.470588235294118	0.869565217391304	While machine learning methods were overall the most successful, for time expression identification, the submitted rule-based systems achieved the best recall.	0
12573	12573	S15-6	Discussion	17	121	0.5	0.876811594202898	This is counter to the usual assumption that rule-based systems will be more precise, and that machine learning systems will sacrifice precision to increase recall.	0
12574	12574	S15-6	Discussion	18	122	0.529411764705882	0.884057971014493	The difference is likely that the rulebased systems were aiming for good coverage, trying to find all potential time expressions, but had too few constraints to discard such phrases in inappropriate contexts.	0
12575	12575	S15-6	Discussion	19	123	0.558823529411765	0.891304347826087	The baseline system is suggestive of this possibility: it has a constraint to only memorize phrases that corresponded with time expressions more than 50% of the time, and it has high precision (0.743) and low recall (0.372) as is typically expected of a rule-based system, but if the constraint is removed, it has low precision (0.126) and high recall (0.521) like the participant rule-based systems.	0
12576	12576	S15-6	Discussion	20	124	0.588235294117647	0.898550724637681	Clinical TempEval was the first TempEval exercise to use narrative containers, a significant shift from prior exercises.	0
12577	12577	S15-6	Discussion	21	125	0.617647058823529	0.905797101449275	Annotator agreement in the dataset is moderate, but needs to be further improved.	0
12578	12578	S15-6	Discussion	22	126	0.647058823529412	0.91304347826087	Similar agreement scores were found when annotating temporal relations in prior corpora (for TempEval or using TimeML), although these typically involved the application of more complex temporal relation ontologies.	0
12579	12579	S15-6	Discussion	23	127	0.676470588235294	0.920289855072464	The narrative container approach is comparatively simple.	0
12580	12580	S15-6	Discussion	24	128	0.705882352941176	0.927536231884058	The low annotator-adjudicator scores (i.e. below 0.90, a score generally recognized to indicate a production-quality resource) suggests that annotation is difficult independent of the number of potential temporal relation types.	0
12581	12581	S15-6	Discussion	25	129	0.735294117647059	0.934782608695652	Difficulty may lie in the comprehension and reification of the potentially complex temporal structures described in natural language text.	0
12582	12582	S15-6	Discussion	26	130	0.764705882352941	0.942028985507246	Nevertheless, systems did well on the DCT task, achieving high scores -similar to the pattern seen in Task D of TempEval-2, which had a comparable scoring metric.	0
12583	12583	S15-6	Discussion	27	131	0.794117647058823	0.94927536231884	Though the results of Clinical TempEval 2015 are encouraging, they were limited somewhat by the small number of participants in the task.	0
12584	12584	S15-6	Discussion	28	132	0.823529411764706	0.956521739130435	There are two likely reasons for this.	0
12585	12585	S15-6	Discussion	29	133	0.852941176470588	0.963768115942029	First, there were many different sub-tasks for Clinical TempEval, meaning that to compete in all sub-tasks, a large number of sub-systems had to be developed in a limited amount of time (six months or less).	0
12586	12586	S15-6	Discussion	30	134	0.882352941176471	0.971014492753623	This relatively high barrier for entry meant that of the 15 research groups that managed to sign a data use agreement and obtain the data before the competition, only 3 submitted systems to compete.	0
12587	12587	S15-6	Discussion	31	135	0.911764705882353	0.978260869565217	Second, the data use agreement process was time consuming, and more than 10 research groups who began the data use agreement process were unable to complete it before the evaluation.	0
12588	12588	S15-6	Discussion	32	136	0.941176470588235	0.985507246376812	In future iterations of Clinical TempEval, we expect these issues to be reduced.	0
12589	12589	S15-6	Discussion	33	137	0.970588235294118	0.992753623188406	The next Clinical TempEval will use the current Train and Dev data as the training set, and as these data are already available, this leaves research teams with a year or more to develop systems.	0
12590	12590	S15-6	Discussion	34	138	1.0	1.0	Furthermore, arrangements with the Mayo Clinic have been made to further expedite the data use agreement process, which should significantly reduce the wait time for new participants.	0
13717	13717	S15-13	title	1	1	1.0	0.005882352941176	SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking	1
13718	13718	S15-13	abstract	1	2	0.2	0.011764705882353	In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.	1
13719	13719	S15-13	abstract	2	3	0.4	0.01764705882353	Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.	0
13720	13720	S15-13	abstract	3	4	0.6	0.023529411764706	Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.	0
13721	13721	S15-13	abstract	4	5	0.8	0.029411764705882	Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.	0
13722	13722	S15-13	abstract	5	6	1.0	0.035294117647059	Moreover, we investigate this task in a multilingual setting and for some specific domains.	0
13723	13723	S15-13	Introduction	1	7	0.045454545454546	0.041176470588235	The Senseval and SemEval evaluation series represent key moments in the community of computational linguistics and related areas.	0
13724	13724	S15-13	Introduction	2	8	0.090909090909091	0.047058823529412	Their focus has been to provide objective evaluations of methods within the wide spectrum of semantic techniques for tasks mainly related to automatic text understanding.	0
13725	13725	S15-13	Introduction	3	9	0.136363636363636	0.052941176470588	Through SemEval-2015 task 13 we both continue and renew the longstanding tradition of disambiguation tasks, by addressing multilingual WSD and EL in a joint manner.	0
13726	13726	S15-13	Introduction	4	10	0.181818181818182	0.058823529411765	WSD (Navigli, 2009;	0
13727	13727	S15-13	Introduction	5	11	0.227272727272727	0.064705882352941	Navigli, 2012) is a historical task aimed at explicitly assigning meanings to single-word and multi-word occurrences within text, a task which today is more alive than ever in the research community.	0
13728	13728	S15-13	Introduction	6	12	0.272727272727273	0.070588235294118	EL (Erbs et al., 2011;	0
13729	13729	S15-13	Introduction	7	13	0.318181818181818	0.076470588235294	Cornolti et al., 2013;	0
13730	13730	S15-13	Introduction	8	14	0.363636363636364	0.082352941176471	Rao et al., 2013) is a more recent task which aims at discovering mentions of entities within a text and linking them to the most suitable entry in a knowledge base.	0
13731	13731	S15-13	Introduction	9	15	0.409090909090909	0.088235294117647	Both these tasks aim at handling the inherent ambiguity of natural language, however WSD tackles it from a lexicographic perspective, while EL tackles it from an encyclopedic one.	0
13732	13732	S15-13	Introduction	10	16	0.454545454545455	0.094117647058824	Specifically, the main difference between the two tasks lies in the kind of inventory they use.	0
13733	13733	S15-13	Introduction	11	17	0.5	0.1	For instance, WordNet (Miller et al., 1990), a manually curated semantic network for the English language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns.	0
13734	13734	S15-13	Introduction	12	18	0.545454545454545	0.105882352941176	More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become -together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) -the main reference inventory for EL systems.	0
13735	13735	S15-13	Introduction	13	19	0.590909090909091	0.111764705882353	Over the years, the research community has typically focused on each of these tasks separately.	0
13736	13736	S15-13	Introduction	14	20	0.636363636363636	0.117647058823529	Recently, however, joint approaches have been proposed (Moro et al., 2014b).	0
13737	13737	S15-13	Introduction	15	21	0.681818181818182	0.123529411764706	One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013).	0
13738	13738	S15-13	Introduction	16	22	0.727272727272727	0.129411764705882	A case in point here is BabelNet 1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012).	0
13739	13739	S15-13	Introduction	17	23	0.772727272727273	0.135294117647059	Resources like BabelNet provide a common ground for the tasks of WSD and EL.	0
13740	13740	S15-13	Introduction	18	24	0.818181818181818	0.141176470588235	In this task our goal is to promote research in the direction of joint word sense and named entity disambiguation, so as to concentrate research efforts on the aspects that differentiate these two tasks without duplicating research on common problems such as identifying the right meaning in context.	0
13741	13741	S15-13	Introduction	19	25	0.863636363636364	0.147058823529412	However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags.	0
13742	13742	S15-13	Introduction	20	26	0.909090909090909	0.152941176470588	Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers.	0
13743	13743	S15-13	Introduction	21	27	0.954545454545455	0.158823529411765	In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation , our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues).	0
13744	13744	S15-13	Introduction	22	28	1.0	0.164705882352941	Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain.	0
13745	13745	S15-13	Task Setup	1	29	0.2	0.170588235294118	The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided.	0
13746	13746	S15-13	Task Setup	2	30	0.4	0.176470588235294	Differently from previous editions Lefever and Hoste, 2013;Manandhar et al., 2010;	0
13747	13747	S15-13	Task Setup	3	31	0.6	0.182352941176471	Lefever and Hoste, 2010;Pradhan et al., 2007;	0
13748	13748	S15-13	Task Setup	4	32	0.8	0.188235294117647	Navigli et al., 2007;	0
13749	13749	S15-13	Task Setup	5	33	1.0	0.194117647058824	Snyder and Palmer, 2004;Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013 and ERD (Carmel et al., 2014).	0
13750	13750	S15-13	Corpora	1	34	0.2	0.2	"The documents considered in this task are taken from the OPUS project (http://opus.lingfil.uu.se/), more specifically from the EMEA (European Medicines Agency documents), KDEdoc (the KDE manual corpus) and ""The EU bookshop corpus"", which make available parallel and POS-tagged documents."	0
13751	13751	S15-13	Corpora	2	35	0.4	0.205882352941176	We took four documents from these repositories.	0
13752	13752	S15-13	Corpora	3	36	0.6	0.211764705882353	Two documents contain medical information about drugs.	0
13753	13753	S15-13	Corpora	4	37	0.8	0.217647058823529	One document consists of the manual of a mathematical graph calculator (i.e., KAlgebra).	0
13754	13754	S15-13	Corpora	5	38	1.0	0.223529411764706	The remaining document contains a formal discussion about social issues, like supporting elderly workers and, more in general, about issues and solutions to unemployment discussed by the members of the European Commission.	0
13755	13755	S15-13	Sense Inventory	1	39	0.25	0.229411764705882	As our sense inventory we use the BabelNet 2.5.1 (http://babelnet.org) multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012), which is the result of the automatic integration of multiple language resources: Princeton WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata, Open Multi WordNet and automatic translations.	0
13756	13756	S15-13	Sense Inventory	2	40	0.5	0.235294117647059	The meanings contained within this resource are organized in Babel synsets.	0
13757	13757	S15-13	Sense Inventory	3	41	0.75	0.241176470588235	Each of these synsets can contain Wikipedia pages, Word-Net synsets and items from the other integrated resources.	0
13758	13758	S15-13	Sense Inventory	4	42	1.0	0.247058823529412	"For instance, in BabelNet it is possible to find the concept ""medicine"" (bn:00054128n), which is represented by both the second word sense of medicine in WordNet and the Wikipedia page Pharmaceutical drug, among others, together with synonyms such as drug and medication in English and lexicalizations in other languages, such as farmaco in Italian and medicamento in Spanish."	0
13759	13759	S15-13	Dataset Creation	1	43	0.090909090909091	0.252941176470588	The manual annotation of documents was performed in a language-specific manner, i.e., different taggers worked on the various translated versions of the input documents.	0
13760	13760	S15-13	Dataset Creation	2	44	0.181818181818182	0.258823529411765	More precisely, we had two taggers for each language, who annotated each fragment of text recognized as linkable with all the senses deemed appropriate.	0
13761	13761	S15-13	Dataset Creation	3	45	0.272727272727273	0.264705882352941	During the annotation procedure, for all languages, each tagger was shown an HTML page containing the sentence within which the target fragment was boldfaced.	0
13762	13762	S15-13	Dataset Creation	4	46	0.363636363636364	0.270588235294118	Then a table of checkable meanings identified by their glosses (in English or, if not available, in Spanish or Italian), to-  gether with the available synonyms and hypernyms (as found in WordNet and the Wikipedia Bitaxonomy (Flati et al., 2014)).	0
13763	13763	S15-13	Dataset Creation	5	47	0.454545454545455	0.276470588235294	The taggers agreed on at least one meaning for 68% of the instances.	0
13764	13764	S15-13	Dataset Creation	6	48	0.545454545454545	0.282352941176471	A third tagger acted as judge by going through all the items and discarding overly general or irrelevant annotations, especially in the case of disagreement between the two taggers.	0
13765	13765	S15-13	Dataset Creation	7	49	0.636363636363636	0.288235294117647	To enforce coherence and spot missing annotations, we projected the English annotations to the other two languages.	0
13766	13766	S15-13	Dataset Creation	8	50	0.727272727272727	0.294117647058823	Finally, the third tagger determined if the projected English annotations that were missing in one of the other two languages were either correctly not included, or if the taggers had actually missed a correct annotation.	0
13767	13767	S15-13	Dataset Creation	9	51	0.818181818181818	0.3	As a result of this procedure we obtained a dataset with around 1.2k items, but with only around 80 named entity mentions per language.	0
13768	13768	S15-13	Dataset Creation	10	52	0.909090909090909	0.305882352941176	Please refer to Table 1 for general statistics about the dataset: we show the number of annotated instances per language and domain, together with their classification as single-or multi-word expressions and named entities.	0
13769	13769	S15-13	Dataset Creation	11	53	1.0	0.311764705882353	We then show the degree of ambiguity both per POS and per instance and lemma (i.e., multiple instances with the same lemma count as a single instance) and, finally, we show how many of the instances have Wikipedia pages or WordNet keys as annotations 2 .	0
13770	13770	S15-13	Evaluation Measures	1	54	0.5	0.317647058823529	To evaluate the performance of the participating systems we used the classical precision, recall and F1 measures:	0
13771	13771	S15-13	Evaluation Measures	2	55	1.0	0.323529411764706	2 Please note that the sum of Wikipedia pages and WordNet keys does not amount to the number of instances, as BabelNet can have integrated synsets that contain both WordNet keys and Wikipedia pages.	0
13772	13772	S15-13	P recision = T P T P + F P	1	56	0.333333333333333	0.329411764705882	(1)	0
13773	13773	S15-13	P recision = T P T P + F P	2	57	0.666666666666667	0.335294117647059	To handle systems that output multiple answers for a single instance we followed the standard scorer of previous Senseval and SemEval challenges in uniformly weighting the multiple answers when computing the TP counts.	0
13774	13774	S15-13	P recision = T P T P + F P	3	58	1.0	0.341176470588235	Moreover, we decided not to take into account fragments annotated by the systems which were not contained in the gold standard, similarly to the D2KB setting of the GERBIL evaluation framework for EL (Usbeck et al., 2015).	0
13775	13775	S15-13	Baseline	1	59	0.043478260869565	0.347058823529412	As baseline we considered the performance of a simple heuristic (called Babel	0
13776	13776	S15-13	Baseline	2	60	0.086956521739131	0.352941176470588	Net first sense or BFS) that exploits the default comparator integrated within the BabelNet 2.5.1 API (i.e., the Babel-SynsetComparator Java class).	0
13777	13777	S15-13	Baseline	3	61	0.130434782608696	0.358823529411765	Babel synsets in Ba-belNet can be viewed as nodes of a semantic network and each of them can contain Wikipedia pages, WordNet synsets and items from the other integrated resources.	0
13778	13778	S15-13	Baseline	4	62	0.173913043478261	0.364705882352941	The comparator takes as input the lemma of the word for which we are ranking the Babel synsets.	0
13779	13779	S15-13	Baseline	5	63	0.217391304347826	0.370588235294118	There are three main cases managed by the comparator.	0
13780	13780	S15-13	Baseline	6	64	0.260869565217391	0.376470588235294	The first case is when both Babel synsets contain a WordNet synset for the considered word.	0
13781	13781	S15-13	Baseline	7	65	0.304347826086957	0.382352941176471	If this is the case, then the WordNet sense numbers are used to rank the synsets.	0
13782	13782	S15-13	Baseline	8	66	0.347826086956522	0.388235294117647	The second case is when only one of the Babel synsets contains a WordNet synset: in this case the Babel synset that contains the WordNet synset gets ranked first.	0
13783	13783	S15-13	Baseline	9	67	0.391304347826087	0.394117647058823	The last case is when no WordNet synsets are contained within the two Babel synsets.	0
13784	13784	S15-13	Baseline	10	68	0.434782608695652	0.4	In this case a lexicographic ordering of the Wikipedia pages contained within the Babel synsets is taken into account.	0
13785	13785	S15-13	Baseline	11	69	0.478260869565217	0.405882352941176	As is well known, the first sense heuristic based on Word-Net has always proved a really hard to beat baseline, outperforming all the developed systems for the English language over almost all settings and system combinations.	0
13786	13786	S15-13	Baseline	12	70	0.521739130434783	0.411764705882353	In contrast, the BFS heuristic in the other languages shows itself to be weaker, achieving lower performances in almost all settings and system combinations.	0
13787	13787	S15-13	Baseline	13	71	0.565217391304348	0.417647058823529	3 Participating Systems DFKI (Supervised).	0
13788	13788	S15-13	Baseline	14	72	0.608695652173913	0.423529411764706	This system exploits Babel-Net as reference inventory and a CRF-based named entity recognizer.	0
13789	13789	S15-13	Baseline	15	73	0.652173913043478	0.429411764705882	The disambiguation system is divided in two parts: one for nouns and another for verbs.	0
13790	13790	S15-13	Baseline	16	74	0.695652173913043	0.435294117647059	For nouns the approach is based on the idea of maximizing multiple objectives at the same time.	0
13791	13791	S15-13	Baseline	17	75	0.739130434782609	0.441176470588235	Similarly to (Hoffart et al., 2011), the disambiguation objectives consist of a global (coherence, unsupervised) part and a local (supervised) part.	0
13792	13792	S15-13	Baseline	18	76	0.782608695652174	0.447058823529412	The global objective makes sure that disambiguation maximizes coherence of the selected synsets and it is based on the semantic signature graph (Moro et al., 2014b).	0
13793	13793	S15-13	Baseline	19	77	0.826086956521739	0.452941176470588	The local objective ensures that the Word-Net synset type fits the local context of the noun to be disambiguated.	0
13794	13794	S15-13	Baseline	20	78	0.869565217391304	0.458823529411765	One important aspect of this approach is that, unlike previous work (Hoffart et al., 2011;	0
13795	13795	S15-13	Baseline	21	79	0.91304347826087	0.464705882352941	Moro et al., 2014b), it does not apply discrete optimization, but continuous optimization on the normalized sum of all objectives.	0
13796	13796	S15-13	Baseline	22	80	0.956521739130435	0.470588235294118	The disambiguation procedure aims to optimize the objective function by iteratively updating the candidate probabilities for each fragment.	0
13797	13797	S15-13	Baseline	23	81	1.0	0.476470588235294	As far as verbs are concerned, a feed-forward neural network is trained using local features such as arguments of the semantic roles of a verb in a sentence, context words, and the verb and its lemma.	0
13798	13798	S15-13	EBL-Hope (Unsupervised + Sense relevance).	1	82	0.023255813953488	0.482352941176471	This approach uses a modified version of the Lesk algorithm and the Jiang &amp; Conrath similarity measure (Jiang and Conrath, 1997).	0
13799	13799	S15-13	EBL-Hope (Unsupervised + Sense relevance).	2	83	0.046511627906977	0.488235294117647	It validates the output from both techniques for enhanced accuracy and exploits semantic relations and corpus (SemCor) in-formation available in BabelNet and WordNet in an unsupervised manner.	0
13800	13800	S15-13	EBL-Hope (Unsupervised + Sense relevance).	3	84	0.069767441860465	0.494117647058824	el92 (Systems mix).	0
13801	13801	S15-13	EBL-Hope (Unsupervised + Sense relevance).	4	85	0.093023255813954	0.5	This system is a generaldomain system for entity detection and linking.	0
13802	13802	S15-13	EBL-Hope (Unsupervised + Sense relevance).	5	86	0.116279069767442	0.505882352941176	It does not perform WSD.	0
13803	13803	S15-13	EBL-Hope (Unsupervised + Sense relevance).	6	87	0.13953488372093	0.511764705882353	The system combines, via a weighted voting, Entity Linking outputs from four publicly available services: Tagme (Ferragina and Scaiella, 2010), DBpedia Spotlight (Mendes et al., 2011), Wikipedia Miner (Milne and Witten, 2008) and Babelfy (Moro et al., 2014b;	0
13804	13804	S15-13	EBL-Hope (Unsupervised + Sense relevance).	7	88	0.162790697674419	0.517647058823529	Moro et al., 2014a).	0
13805	13805	S15-13	EBL-Hope (Unsupervised + Sense relevance).	8	89	0.186046511627907	0.523529411764706	The different runs correspond to different settings in the weighting formula (De La Clergerie et al., 2008;	0
13806	13806	S15-13	EBL-Hope (Unsupervised + Sense relevance).	9	90	0.209302325581395	0.529411764705882	Fiscus, 1997).	0
13807	13807	S15-13	EBL-Hope (Unsupervised + Sense relevance).	10	91	0.232558139534884	0.535294117647059	LIMSI (Unsupervised + Sense relevance).	0
13808	13808	S15-13	EBL-Hope (Unsupervised + Sense relevance).	11	92	0.255813953488372	0.541176470588235	The system performs WSD by taking advantage of the parallelism of the test data, a feature that was not exploited by the systems that participated in the SemEval-2013 Multilingual Word Sense Disambiguation task 12 .	0
13809	13809	S15-13	EBL-Hope (Unsupervised + Sense relevance).	12	93	0.27906976744186	0.547058823529412	The system needs no training and is applied directly to the test dataset, nor does it use distributional (context) information.	0
13810	13810	S15-13	EBL-Hope (Unsupervised + Sense relevance).	13	94	0.302325581395349	0.552941176470588	The texts are sentence-and wordaligned pairwise, and content words are tagged by their translations in another language.	0
13811	13811	S15-13	EBL-Hope (Unsupervised + Sense relevance).	14	95	0.325581395348837	0.558823529411765	The alignments serve to retrieve the BabelNet synsets that are relevant for each instance of a word in the texts (i.e., synsets that contain both the disambiguation target and its aligned translation).	0
13812	13812	S15-13	EBL-Hope (Unsupervised + Sense relevance).	15	96	0.348837209302326	0.564705882352941	If a Babel synset is retained, this is used to annotate the instance of the word in the test set.	0
13813	13813	S15-13	EBL-Hope (Unsupervised + Sense relevance).	16	97	0.372093023255814	0.570588235294118	If more than one synset is retained, these are ranked using the BabelSynset-Comparator Java class available in the BabelNet API (please refer to Section 2.5 for a detailed explanation).	0
13814	13814	S15-13	EBL-Hope (Unsupervised + Sense relevance).	17	98	0.395348837209302	0.576470588235294	The highest ranked synset among the ones that contain the aligned translation is used to annotate the instance.	0
13815	13815	S15-13	EBL-Hope (Unsupervised + Sense relevance).	18	99	0.418604651162791	0.582352941176471	The system falls back to the BabelNet first sense (BFS) provided by the BabelSynset	0
13816	13816	S15-13	EBL-Hope (Unsupervised + Sense relevance).	19	100	0.441860465116279	0.588235294117647	Comparator for instances with no aligned translation, or in cases where the translation was not found in any of the synsets available for the word in BabelNet.	0
13817	13817	S15-13	EBL-Hope (Unsupervised + Sense relevance).	20	101	0.465116279069767	0.594117647058823	SUDOKU (Unsupervised).	0
13818	13818	S15-13	EBL-Hope (Unsupervised + Sense relevance).	21	102	0.488372093023256	0.6	"This deterministic constraint-based approach relies on a reasonable degree of ""document monosemy"" (percentage of unique monosemous lemmas in a document) and exploits Personalised PageRank (Agirre et al., 2014) to select the best candidate."	0
13819	13819	S15-13	EBL-Hope (Unsupervised + Sense relevance).	22	103	0.511627906976744	0.605882352941176	The PPR is started with a surfing vector biased towards monosemous words (i.e., their respective sense).	0
13820	13820	S15-13	EBL-Hope (Unsupervised + Sense relevance).	23	104	0.534883720930232	0.611764705882353	Each submission differs by its imposed constraints: Run1 is the plain approach (Manion and Sainudiin, 2014) applied at the document level; Run2 is the iterative version of the previous approach applied at the document level and with words disambiguated in order of increasing polysemy; Run3 is like Run2, but it is first applied to nouns and then to verbs, adjectives, and adverbs.	0
13821	13821	S15-13	EBL-Hope (Unsupervised + Sense relevance).	24	105	0.558139534883721	0.617647058823529	TeamUFAL (Unsupervised).	0
13822	13822	S15-13	EBL-Hope (Unsupervised + Sense relevance).	25	106	0.581395348837209	0.623529411764706	This system exploits Apache Lucene search engine to index Wikipedia documents, Wiktionary entries and WordNet senses.	0
13823	13823	S15-13	EBL-Hope (Unsupervised + Sense relevance).	26	107	0.604651162790698	0.629411764705882	Then, to perform disambiguation, the Lucene ranking method is used to query the index with multiple queries (consisting of the text fragment and context words).	0
13824	13824	S15-13	EBL-Hope (Unsupervised + Sense relevance).	27	108	0.627906976744186	0.635294117647059	Finally, all query results are merged and the disambiguated meaning is selected thanks to a simple threshold heuristic.	0
13825	13825	S15-13	EBL-Hope (Unsupervised + Sense relevance).	28	109	0.651162790697674	0.641176470588235	UNIBA (Unsupervised + Sense relevance).	0
13826	13826	S15-13	EBL-Hope (Unsupervised + Sense relevance).	29	110	0.674418604651163	0.647058823529412	This system 3 extends two well-known variations of the Lesk WSD method.	0
13827	13827	S15-13	EBL-Hope (Unsupervised + Sense relevance).	30	111	0.697674418604651	0.652941176470588	The main contribution of the approach relies on the use of a word similarity function defined on a distributional semantic space (Word2vec tool (Mikolov et al., 2013)) to compute the gloss-context overlap.	0
13828	13828	S15-13	EBL-Hope (Unsupervised + Sense relevance).	31	112	0.72093023255814	0.658823529411765	Entities are identified by exploiting a list of possible surface forms extracted from BabelNet synsets.	0
13829	13829	S15-13	EBL-Hope (Unsupervised + Sense relevance).	32	113	0.744186046511628	0.664705882352941	Moreover, each synset has a prior probability computed over an annotated corpus.	0
13830	13830	S15-13	EBL-Hope (Unsupervised + Sense relevance).	33	114	0.767441860465116	0.670588235294118	For Word	0
13831	13831	S15-13	EBL-Hope (Unsupervised + Sense relevance).	34	115	0.790697674418605	0.676470588235294	Net synsets, SemCor is exploited, while for Wikipedia entities the number of citations in Wikipedia internal links is counted.	0
13832	13832	S15-13	EBL-Hope (Unsupervised + Sense relevance).	35	116	0.813953488372093	0.682352941176471	vua-background (Partially supervised).	0
13833	13833	S15-13	EBL-Hope (Unsupervised + Sense relevance).	36	117	0.837209302325581	0.688235294117647	This approach exploits the Named Entities contained in the test data to generate a background corpus.	0
13834	13834	S15-13	EBL-Hope (Unsupervised + Sense relevance).	37	118	0.86046511627907	0.694117647058823	This is done by finding similar DBpedia entities for the entities in the input documents.	0
13835	13835	S15-13	EBL-Hope (Unsupervised + Sense relevance).	38	119	0.883720930232558	0.7	Using this background corpus, the system tries to find the predominant sense of the words in the test data (McCarthy et al., 2004).	0
13836	13836	S15-13	EBL-Hope (Unsupervised + Sense relevance).	39	120	0.906976744186046	0.705882352941176	"If a predominant sense is recognized for a specific lemma, then it is used, otherwise the system falls back to the ""It Makes Sense"" WSD system (Zhong and Ng, 2010)."	0
13837	13837	S15-13	EBL-Hope (Unsupervised + Sense relevance).	40	121	0.930232558139535	0.711764705882353	WSD-games (Unsupervised).	0
13838	13838	S15-13	EBL-Hope (Unsupervised + Sense relevance).	41	122	0.953488372093023	0.717647058823529	This approach is formulated in terms of Evolutionary Game Theory, where each word to be disambiguated is represented as a node in a graph and each sense as a class.	0
13839	13839	S15-13	EBL-Hope (Unsupervised + Sense relevance).	42	123	0.976744186046512	0.723529411764706	The proposed algorithm performs a consistent class assignment of senses according to the similarity information of each word with the others, so that similar words are constrained to similar classes.	0
13840	13840	S15-13	EBL-Hope (Unsupervised + Sense relevance).	43	124	1.0	0.729411764705882	The propagation of the information over the graph is formulated in terms of a non-cooperative multi-player game, where the players are the data points, in order to decide their class memberships, and equilibria correspond to consistent labeling of the data.	0
13841	13841	S15-13	Results and Discussion	1	125	0.25	0.735294117647059	The results obtained by the participating systems are shown in Tables 2-6.	0
13842	13842	S15-13	Results and Discussion	2	126	0.5	0.741176470588235	In Table 2 we show the precision, recall and F1 scores of the participating systems that annotated all classes of items (named entities, nouns, verbs, adverbs, adjectives) over the whole dataset.	0
13843	13843	S15-13	Results and Discussion	3	127	0.75	0.747058823529412	Six out of the nine participating teams annotated the full set of items.	0
13844	13844	S15-13	Results and Discussion	4	128	1.0	0.752941176470588	We also show the F1 performance on each considered domain independently and for different kinds of subsets of the item classes (i.e., we show the F1 score over all items, then only on named entities, all open-class word senses and individually).	0
13845	13845	S15-13	Overall Performance	1	129	0.1	0.758823529411765	From Table 2 we can see that the best system for English (i.e., LIMSI) is able to obtain a performance more than five percentage points higher than the second ranked system.	0
13846	13846	S15-13	Overall Performance	2	130	0.2	0.764705882352941	This is due to the goodquality indirect supervision provided by the alignments combined with the use of the BabelSynset-Comparator.	0
13847	13847	S15-13	Overall Performance	3	131	0.3	0.770588235294118	However, on the other two languages this system obtains lower performance than the other competing systems.	0
13848	13848	S15-13	Overall Performance	4	132	0.4	0.776470588235294	The performance of the SU-DOKU system is of a particular interest, as it obtains the second best scores on the English part of the dataset and the top scores overall on the other two languages.	0
13849	13849	S15-13	Overall Performance	5	133	0.5	0.782352941176471	It exploits monosemous words within the input documents to run Personalized PageRank.	0
13850	13850	S15-13	Overall Performance	6	134	0.6	0.788235294117647	The three runs differ mainly in respect of the order in which the words get disambiguated.	0
13851	13851	S15-13	Overall Performance	7	135	0.7	0.794117647058823	In Table 3 we show the F1 scores of all the systems over the whole dataset for each class of the  manually annotated items and for each language.	0
13852	13852	S15-13	Overall Performance	8	136	0.8	0.8	In the English part of the datasets the DFKI system performs best for verb, noun and named entity disambiguation, thanks to precomputed random walks called semantic signatures, along the lines of Babelfy (Moro et al., 2014b), and supervised techniques.	0
13853	13853	S15-13	Overall Performance	9	137	0.9	0.805882352941176	The UNIBA system on the English dataset obtains the best result on adverbs.	0
13854	13854	S15-13	Overall Performance	10	138	1.0	0.811764705882353	Finally, in the Spanish dataset the EBL-Hope system based on a combination of a Lesk-based measure together with the Jiang &amp; Conrath similarity measure shows the best performance for named entities.	0
13855	13855	S15-13	Domain-based Evaluation	1	139	0.333333333333333	0.817647058823529	In Tables 4-6 we show the detailed performances of all the systems over different classes of items, and on different domains.	0
13856	13856	S15-13	Domain-based Evaluation	2	140	0.666666666666667	0.823529411764706	One of the main goals of this task is to investigate the performance of disambiguation methods over different domains.	0
13857	13857	S15-13	Domain-based Evaluation	3	141	1.0	0.829411764705882	Our documents derive from the biomedical domain, the maths and computer domain, and a broader domain (a document discussing social issues, especially for elderly workers and possible solutions).	0
13858	13858	S15-13	Biomedical domain.	1	142	0.05	0.835294117647059	In Table 4 we show the performance of the systems on the biomedical documents.	0
13859	13859	S15-13	Biomedical domain.	2	143	0.1	0.841176470588235	The first thing to notice is the much higher best score of the first ranked system (i.e., LIMSI), which attains an F1 score of 71.3%.	0
13860	13860	S15-13	Biomedical domain.	3	144	0.15	0.847058823529412	This is due to the lower ambiguity of nouns and named entities (see Table 1) resulting from the greater numbers of domain-specific concepts used within this kind of documents.	0
13861	13861	S15-13	Biomedical domain.	4	145	0.2	0.852941176470588	This can also be seen from the higher scores obtained by the BFS.	0
13862	13862	S15-13	Biomedical domain.	5	146	0.25	0.858823529411765	Overall, all systems obtained a better performance than in the other domains, with a gain of more than four percentage points each.	0
13863	13863	S15-13	Biomedical domain.	6	147	0.3	0.864705882352941	The second ranked system (i.e., SUDOKU) shows its ability to exploit monosemous words obtaining a 0.1 difference from the first ranked system and a 0.9 point distance from the BFS baseline.	0
13864	13864	S15-13	Biomedical domain.	7	148	0.35	0.870588235294118	This is of particular interest as the system does not explicitly exploit any sense relevance information.	0
13865	13865	S15-13	Biomedical domain.	8	149	0.4	0.876470588235294	Moreover, the DFKI system obtains the best scores for nouns and verbs, and is the only system able to obtain a 100% F1 score on NE disambiguation.	0
13866	13866	S15-13	Biomedical domain.	9	150	0.45	0.882352941176471	However, several other systems performed above 90%, showing that in this particular set of documents named entities are easy to disambiguate.	0
13867	13867	S15-13	Biomedical domain.	10	151	0.5	0.888235294117647	On the other two languages the performances are a little bit lower, but the SUDOKU system confirms its ability to exploit monosemous words at a quality comparable to the one obtained in the English dataset.	0
13868	13868	S15-13	Biomedical domain.	11	152	0.55	0.894117647058824	The LIMSI system, instead, obtains a reduction of around 20% due to its exploitation of the BabelSynsetComparator, which performs badly in these languages (see the BFS scores).	0
13869	13869	S15-13	Biomedical domain.	12	153	0.6	0.9	Maths and computer domain.	0
13870	13870	S15-13	Biomedical domain.	13	154	0.65	0.905882352941176	In Table 5 we show the results for the maths and computer domain.	0
13871	13871	S15-13	Biomedical domain.	14	155	0.7	0.911764705882353	As can be seen in Table 1, this is the most ambiguous domain and the best systems obtain much lower performances than in the other domains.	0
13872	13872	S15-13	Biomedical domain.	15	156	0.75	0.917647058823529	Interestingly, the DFKI system is not able to achieve the best performance on any of the considered item classes, while UNIBA and SUDOKU show the best results for nouns and verbs.	0
13873	13873	S15-13	Biomedical domain.	16	157	0.8	0.923529411764706	As regards named en-  tities, the system EBL-Hope obtains the best results in all languages.	0
13874	13874	S15-13	Biomedical domain.	17	158	0.85	0.929411764705882	This system, in addition to exploiting a Lesk-based measure combined with the Jiang &amp; Conrath similarity measure, uses the BabelNet semantic relations, which have already been shown to be useful for attaining state-of-the-art performances in EL (Moro et al., 2014b).	0
13875	13875	S15-13	Biomedical domain.	18	159	0.9	0.935294117647059	Interestingly, in the Italian dataset the system UNIBA (which is based on an extended version of the Lesk measure and a semantic relatedness measure) obtains the same performance for NE as the EBL-Hope system.	0
13876	13876	S15-13	Biomedical domain.	19	160	0.95	0.941176470588235	"the predominant sense algorithm (McCarthy et al., 2004) and, as a fallback routine, on the ""It Makes Sense"" supervised WSD system (Zhong and Ng, 2010)."	0
13877	13877	S15-13	Biomedical domain.	20	161	1.0	0.947058823529412	For the other two languages the SUDOKU system obtains the best scores, with the exception of adverbs in the Italian dataset where the UNIBA system is able to reach an F1 score of 100%.	0
13878	13878	S15-13	Social issues domain.	1	162	0.111111111111111	0.952941176470588	In	0
13879	13879	S15-13	Social issues domain.	2	163	0.222222222222222	0.958823529411765	Conclusion and Future Directions	0
13880	13880	S15-13	Social issues domain.	3	164	0.333333333333333	0.964705882352941	In  disambiguation, and Lesk-based measures for verb, adjective and adverb disambiguation.	0
13881	13881	S15-13	Social issues domain.	4	165	0.444444444444444	0.970588235294118	Another interesting outcome that emerges from this task is that supervised approaches are difficult to generalize in a multilingual setting.	0
13882	13882	S15-13	Social issues domain.	5	166	0.555555555555556	0.976470588235294	In fact, the supervised systems that participated in this task took into account only the English language.	0
13883	13883	S15-13	Social issues domain.	6	167	0.666666666666667	0.982352941176471	Moreover, the task confirms yet again that the WordNet first sense heuristic is a hard baseline to beat.	0
13884	13884	S15-13	Social issues domain.	7	168	0.777777777777778	0.988235294117647	Unfortunately, no domainspecific disambiguation system participated in the task.	0
13885	13885	S15-13	Social issues domain.	8	169	0.888888888888889	0.994117647058824	However, in the biomedical domain, the participating systems show higher quality performances than in the other considered domains.	0
13886	13886	S15-13	Social issues domain.	9	170	1.0	1.0	As future directions, we would like to continue to investigate the nature of this novel joint task, and to concentrate on the differences between named entity  disambiguation and word sense disambiguation with a special focus on non-European languages.	0
23177	23177	S19-2	title	1	1	1.0	0.00374531835206	SemEval-2019 Task 2: Unsupervised Lexical Frame Induction	0
23178	23178	S19-2	abstract	1	2	0.142857142857143	0.00749063670412	This paper presents Unsupervised Lexical	0
23179	23179	S19-2	abstract	2	3	0.285714285714286	0.01123595505618	Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.	0
23180	23180	S19-2	abstract	3	4	0.428571428571429	0.01498127340824	Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.	1
23181	23181	S19-2	abstract	4	5	0.571428571428571	0.0187265917603	Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.	0
23182	23182	S19-2	abstract	5	6	0.714285714285714	0.02247191011236	Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.	0
23183	23183	S19-2	abstract	6	7	0.857142857142857	0.02621722846442	The shared task attracted nine teams, of whom three reported promising results.	0
23184	23184	S19-2	abstract	7	8	1.0	0.029962546816479	This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.	0
23185	23185	S19-2	Introduction	1	9	0.0625	0.033707865168539	SemEval 2019	0
23186	23186	S19-2	Introduction	2	10	0.125	0.037453183520599	Task 2 focused on the unsupervised semantic labeling of a set of prespecified (semantically) unlabeled structures (Figure 1).	0
23187	23187	S19-2	Introduction	3	11	0.1875	0.041198501872659	Unsupervised learning methods analyze these structures (Figure 1a) to augment them with semantic labels (Figure 1b).	0
23188	23188	S19-2	Introduction	4	12	0.25	0.044943820224719	The shape of the manually labeled input frames is constrained to an acyclic connected tree of lexical items (words and multi-word units) of maximum depth 1, where just one root governs several arguments.	0
23189	23189	S19-2	Introduction	5	13	0.3125	0.048689138576779	The task used Berkeley FrameNet (FN) (Ruppenhofer et al., 2016) and Q. Zadeh and Petruck (2019), guidelines for this task, to determine the arguments and label them with semantic information.	0
23190	23190	S19-2	Introduction	6	14	0.375	0.052434456928839	We compared the proposed system results for unsupervised semantic tagging with that of human annotated (or, gold-standard) data in three different subtasks (Figure 2).	0
23191	23191	S19-2	Introduction	7	15	0.4375	0.056179775280899	To evaluate the systems, we computed distributional similarities between  their generated unsupervised labeled data and human annotated reference data.	0
23192	23192	S19-2	Introduction	8	16	0.5	0.059925093632959	For computing similarities we used general purpose numeral methods of text clustering, in particular BCUBED F-SCORE (Bagga and Baldwin, 1998) as the single figure of merit to rank the systems.	0
23193	23193	S19-2	Introduction	9	17	0.5625	0.063670411985019	The most important result of the shared task is the creation of a benchmark for a future complex task.	0
23194	23194	S19-2	Introduction	10	18	0.625	0.067415730337079	This benchmark includes a moderately sized, manually annotated set of frames, where only the verbs of each were included, along with their core frame elements (which uniquely define a frame as Ruppenhofer et al. describe).	0
23195	23195	S19-2	Introduction	11	19	0.6875	0.071161048689139	To complement FN's core frame elements that have highly specific meanings, the benchmark also includes the annotated argument structures of the verbs based on the generic semantic roles proposed for verb classes in VerbNet 3.2 (Kipper et al., 2000;	0
23196	23196	S19-2	Introduction	12	20	0.75	0.074906367041199	Palmer et al., 2017).	0
23197	23197	S19-2	Introduction	13	21	0.8125	0.078651685393259	The benchmark comes with simplified annotation guidelines and a modular annotation sys-tem with browsing and editing capabilities.	0
23198	23198	S19-2	Introduction	14	22	0.875	0.082397003745318	1 Complementing the benchmarking are several state-ofthe-art competing baselines, from the participants, that serve as a point of departure for improvements in the future.	0
23199	23199	S19-2	Introduction	15	23	0.9375	0.086142322097378	2	0
23200	23200	S19-2	Introduction	16	24	1.0	0.089887640449438	The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks.	0
23201	23201	S19-2	Background	1	25	0.055555555555556	0.093632958801498	Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structures have developed in parallel over several decades in theoretical linguistic studies about the syntax-semantics interface, as well as in empirical corpus-driven applications in natural language processing.	0
23202	23202	S19-2	Background	2	26	0.111111111111111	0.097378277153558	Building repositories of (lexical) semantic frames is a core component in all of these efforts.	0
23203	23203	S19-2	Background	3	27	0.166666666666667	0.101123595505618	In formal studies, lexical semantic frame knowledge bases instantiate foundational theories with tangible examples, e.g., to provide supporting evidence for the theory.	0
23204	23204	S19-2	Background	4	28	0.222222222222222	0.104868913857678	Practically, frame semantic repositories play a pivotal role in natural language understanding and semantic parsing, both as inspiration for a representation format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others.	0
23205	23205	S19-2	Background	5	29	0.277777777777778	0.108614232209738	However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks.	0
23206	23206	S19-2	Background	6	30	0.333333333333333	0.112359550561798	The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English.	0
23207	23207	S19-2	Background	7	31	0.388888888888889	0.116104868913858	While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at best.	0
23208	23208	S19-2	Background	8	32	0.444444444444444	0.119850187265918	This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017).	0
23209	23209	S19-2	Background	9	33	0.5	0.123595505617978	Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies.	0
23210	23210	S19-2	Background	10	34	0.555555555555556	0.127340823970037	Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet.	0
23211	23211	S19-2	Background	11	35	0.611111111111111	0.131086142322097	Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information.	0
23212	23212	S19-2	Background	12	36	0.666666666666667	0.134831460674157	Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments.	0
23213	23213	S19-2	Background	13	37	0.722222222222222	0.138576779026217	Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)).	0
23214	23214	S19-2	Background	14	38	0.777777777777778	0.142322097378277	This task aimed to benchmark a class of such unsupervised frame induction methods.	0
23215	23215	S19-2	Background	15	39	0.833333333333333	0.146067415730337	The ambitious goal of this task was the unsupervised induction of frame semantic structures from tokenized and morphosyntacally labeled text corpora.	0
23216	23216	S19-2	Background	16	40	0.888888888888889	0.149812734082397	We sought to achieve this goal by building an evaluation benchmark for three tasks.	0
23217	23217	S19-2	Background	17	41	0.944444444444444	0.153558052434457	Task A dealt with unsupervised labeling of verb lemmas with their frame meaning.	0
23218	23218	S19-2	Background	18	42	1.0	0.157303370786517	Task B involved unsupervised argument role labeling, where B.1 benchmarked unsupervised labeling of frame-specific frame elements (FEs) based on FN, and B.2 benchmarked unsupervised role labeling of arguments in Case Grammar terms (Fillmore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet.	0
23219	23219	S19-2	Task Description	1	43	0.25	0.161048689138577	The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation).	0
23220	23220	S19-2	Task Description	2	44	0.5	0.164794007490637	Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information.	0
23221	23221	S19-2	Task Description	3	45	0.75	0.168539325842697	Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels.	0
23222	23222	S19-2	Task Description	4	46	1.0	0.172284644194757	For development purposes, developers received a small labeled development set.	0
23223	23223	S19-2	Task A: Clustering Verbs	1	47	0.05	0.176029962546816	The goal of this task was to identify verbs that evoke the same frame.	0
23224	23224	S19-2	Task A: Clustering Verbs	2	48	0.1	0.179775280898876	The task involved labeling verb uses in context to resemble their categorization based on Frame Semantics (Figure 2a).	0
23225	23225	S19-2	Task A: Clustering Verbs	3	49	0.15	0.183520599250936	Here, we used FN 1.7 as the reference for frame definitions.	0
23226	23226	S19-2	Task A: Clustering Verbs	4	50	0.2	0.187265917602996	Hence, the task constituted the unsupervised induction of FN's lexical units, where a lexical unit (LU) is a pairing of a lemma and a frame.	0
23227	23227	S19-2	Task A: Clustering Verbs	5	51	0.25	0.191011235955056	For example, we expected that the LUs auction.v, retail.v, sell.v, etc., which evoke the typed situation of COMMERCE SELL, be labeled with the same unsupervised tag.	0
23228	23228	S19-2	Task A: Clustering Verbs	6	52	0.3	0.194756554307116	3	0
23229	23229	S19-2	Task A: Clustering Verbs	7	53	0.35	0.198501872659176	The task resembles word sense induction in that it assigns a class (or sense) label to a verb.	0
23230	23230	S19-2	Task A: Clustering Verbs	8	54	0.4	0.202247191011236	In word sense induction (WSI), labels are determined and evaluated on word forms (lemma + part-ofspeech e.g., sell.v or auction.n).	0
23231	23231	S19-2	Task A: Clustering Verbs	9	55	0.45	0.205992509363296	WSI evaluations assume that the inventory of senses (set S i s) for different word forms f is devised independently.	0
23232	23232	S19-2	Task A: Clustering Verbs	10	56	0.5	0.209737827715356	For instance, assuming f 1 is labeled with the set of senses S 1 and f 2 with S 2 , then S 1 ∩ S 2 = φ only if f 1 = f 2 ; and, if f 1 = f 2 then S 1 ∩ S 2 = φ (as in other SemEval benchmarks, including Agirre and Soroa (2007); Manandhar et al. (2010);	0
23233	23233	S19-2	Task A: Clustering Verbs	11	57	0.55	0.213483146067416	3 Dark red small caps indicate FN frames.	0
23234	23234	S19-2	Task A: Clustering Verbs	12	58	0.6	0.217228464419476	Jurgens and Klapaftis (2013); Navigli and Vannella (2013)).	0
23235	23235	S19-2	Task A: Clustering Verbs	13	59	0.65	0.220973782771536	For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from S sell are assigned to the lemma sell.v, and one label s is assigned to auction.v, knowing that s / ∈ S sell .	0
23236	23236	S19-2	Task A: Clustering Verbs	14	60	0.7	0.224719101123595	Typically, lexical semantic relationships among members of S i s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)).	0
23237	23237	S19-2	Task A: Clustering Verbs	15	61	0.75	0.228464419475655	In contrast, this task assumes that the sense inventory is defined independent of word forms.	0
23238	23238	S19-2	Task A: Clustering Verbs	16	62	0.8	0.232209737827715	This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., f i = f j ) can be mapped to the same meaning (label), and the same meaning (label) can be mapped to several word forms.	0
23239	23239	S19-2	Task A: Clustering Verbs	17	63	0.85	0.235955056179775	We defined S with respect to FrameNet and assumed that its typed-situation frames are units of meaning.	0
23240	23240	S19-2	Task A: Clustering Verbs	18	64	0.9	0.239700374531835	So, COMMERCE SELL captures the meaning associated with both sell.	0
23241	23241	S19-2	Task A: Clustering Verbs	19	65	0.95	0.243445692883895	v and auction.v., as well as other selling-related words.	0
23242	23242	S19-2	Task A: Clustering Verbs	20	66	1.0	0.247191011235955	Hence, in some sense, Task A goes beyond the ordinary WSI task as it also demands identifying (unspecified) lexical semantic relationships between verbs.	0
23243	23243	S19-2	Task B.1: Unsupervised Frame Semantic	1	67	0.142857142857143	0.250936329588015	Argument Labeling	0
23244	23244	S19-2	Task B.1: Unsupervised Frame Semantic	2	68	0.285714285714286	0.254681647940075	Taking the frames as primary and defining roles relative to each frame, the aim of Task B.1 was to cluster prespecified verb-headed argument structures according to the principles of Frame Semantics, where FrameNet served as the reference for evaluation.	0
23245	23245	S19-2	Task B.1: Unsupervised Frame Semantic	3	69	0.428571428571429	0.258426966292135	This task amounted to unsupervised labeling of frames and core FEs (Figure 2b).	0
23246	23246	S19-2	Task B.1: Unsupervised Frame Semantic	4	70	0.571428571428571	0.262172284644195	Because Frame	0
23247	23247	S19-2	Task B.1: Unsupervised Frame Semantic	5	71	0.714285714285714	0.265917602996255	Net defines FEs frame-specifically, Task B.1 entails Task A. Given a set of semantically-unlabelled arguments as input (e.g., Figure 1a), the root nodes (i.e., verbs) are clustered and assigned to a set of unsupervised frame labels π i (1 ≤ i ≤ n, where n is the number of latent frames).	0
23248	23248	S19-2	Task B.1: Unsupervised Frame Semantic	6	72	0.857142857142857	0.269662921348315	Then, the arguments are labeled with semantic role labels (FEs) interpreted locally given the frame.	0
23249	23249	S19-2	Task B.1: Unsupervised Frame Semantic	7	73	1.0	0.273408239700375	That is, for any pair of π x and π y , the set of assigned roles R x to arguments under π x are assumed to be independent from R y labels for π y (R x ∩ R y = φ).	0
23250	23250	S19-2	Task B.2: Unsupervised Case Role Labeling	1	74	0.142857142857143	0.277153558052434	We defined Subtask B.2 in parallel to Subtask B.1 and involved an idea from Case Grammar.	0
23251	23251	S19-2	Task B.2: Unsupervised Case Role Labeling	2	75	0.285714285714286	0.280898876404494	The ar-guments of a verb in a set of prespecified subcategorization frames were clustered according to a common set of generic semantic roles (Figure 2c).	0
23252	23252	S19-2	Task B.2: Unsupervised Case Role Labeling	3	76	0.428571428571429	0.284644194756554	Here, the task assumed that semantic roles are universal and generic (e.g., Agent, Patient).	0
23253	23253	S19-2	Task B.2: Unsupervised Case Role Labeling	4	77	0.571428571428571	0.288389513108614	Their configuration determines the argument structure of verb-headed phrases.	0
23254	23254	S19-2	Task B.2: Unsupervised Case Role Labeling	5	78	0.714285714285714	0.292134831460674	We evaluated this unsupervised labeling of arguments with semantic roles independently of the class, sense, and word form of a verb.	0
23255	23255	S19-2	Task B.2: Unsupervised Case Role Labeling	6	79	0.857142857142857	0.295880149812734	We compared the role labels against a set of semantic roles from VerbNet 3.2 (Kipper et al., 2000).	0
23256	23256	S19-2	Task B.2: Unsupervised Case Role Labeling	7	80	1.0	0.299625468164794	Given a verb instance, no guarantee exists that input argument structures for B.2 and B.1 would be the same.	0
23257	23257	S19-2	Evaluation Dataset	1	81	0.166666666666667	0.303370786516854	The dataset consists of manual annotations for verb-headed frame structures anchored in tokenized sentences.	0
23258	23258	S19-2	Evaluation Dataset	2	82	0.333333333333333	0.307116104868914	These frame structures were manually annotated using the guidelines for this task (Q. Zadeh and Petruck, 2019).	0
23259	23259	S19-2	Evaluation Dataset	3	83	0.5	0.310861423220974	For example, as already illustrated, the verb come from.	0
23260	23260	S19-2	Evaluation Dataset	4	84	0.666666666666667	0.314606741573034	v is annotated in terms of FN's ORIGIN frame and its core FEs, as Example 1 shows.	0
23261	23261	S19-2	Evaluation Dataset	5	85	0.833333333333333	0.318352059925094	(1)	0
23262	23262	S19-2	Evaluation Dataset	6	86	1.0	0.322097378277154	Criticism of futures COMES FROM Wall Street.	0
23263	23263	S19-2	Criticism come from Wall Street	1	87	0.5	0.325842696629213	ORIGIN ENTITY ORIGIN	0
23264	23264	S19-2	Criticism come from Wall Street	2	88	1.0	0.329588014981273	Also, using the set of 32 generic semantic role labels in VerbNet 3.2 and two additional roles, COG-NIZER and CONTENT, we annotated arguments of the verb as the following graphic shows.	0
23265	23265	S19-2	Criticism come from Wall Street THEME SOURCE	1	89	0.25	0.333333333333333	We assumed unique identifiers for sentences, e.g., #s1 for Example 1.	0
23266	23266	S19-2	Criticism come from Wall Street THEME SOURCE	2	90	0.5	0.337078651685393	The evaluation record for come from.v (Task A) appears below, where #s1 4 5 specifies the position of the verb in the sentence (Example 1).	0
23267	23267	S19-2	Criticism come from Wall Street THEME SOURCE	3	91	0.75	0.340823970037453	We stripped off the manually asserted labels from the records and passed them to systems for assigning unsupervised labels.	0
23268	23268	S19-2	Criticism come from Wall Street THEME SOURCE	4	92	1.0	0.344569288389513	Evidently, later a scorer program (Section 5) compared system-generated labels with the manually assigned labels.	0
23269	23269	S19-2	Data Sampling	1	93	0.25	0.348314606741573	We sampled data from the Wall Street Journal (WSJ) corpus of the Penn Treebank.	0
23270	23270	S19-2	Data Sampling	2	94	0.5	0.352059925093633	Kallmeyer et al. (2018) provided frame annotations similar to those in this task for a portion of WSJ sentences, using SemLink  and EngVallex (Cinková et al., 2014) to generate frame semantic annotations semi-automatically.	0
23271	23271	S19-2	Data Sampling	3	95	0.75	0.355805243445693	That work was based on FrameNet and the Prague Dependency Treebank (PSD) (Hajič et al., 2012) from the Broad-coverage Semantic Dependency resource (Oepen et al., 2016).	0
23272	23272	S19-2	Data Sampling	4	96	1.0	0.359550561797753	We started by annotating a portion of the records in Kallmeyer et al. (2018), and later deviated from this subset to create a more representative sample of the overall diversity and distribution of verbs in the WSJ corpus using a stratified random sampling method.	0
23273	23273	S19-2	Guidelines	1	97	0.090909090909091	0.363295880149813	The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks.	0
23274	23274	S19-2	Guidelines	2	98	0.181818181818182	0.367041198501873	In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013).	0
23275	23275	S19-2	Guidelines	3	99	0.272727272727273	0.370786516853933	To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE ENTITY.	0
23276	23276	S19-2	Guidelines	4	100	0.363636363636364	0.374531835205993	We only annotated Criticism, understanding it as the LU that evokes JUDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span.	0
23277	23277	S19-2	Guidelines	5	101	0.454545454545455	0.378277153558052	Thus, we assumed that another frame f a fills an argument of a frame.	0
23278	23278	S19-2	Guidelines	6	102	0.545454545454545	0.382022471910112	We annotated only the main content word(s) that evoke(s) f a ; these main words are the semantic heads.	0
23279	23279	S19-2	Guidelines	7	103	0.636363636363636	0.385767790262172	4 Multi-word unit semantic heads (e.g., named entities, word form combinations) are annotated as if a single word form, such as Wall Street (# 1), excluding modifiers.	0
23280	23280	S19-2	Guidelines	8	104	0.727272727272727	0.389513108614232	In contrast to semantic depen-dency structures (e.g., DELPH-IN MRS-Derived Semantic Dependencies, Enju Predicate	0
23281	23281	S19-2	Guidelines	9	105	0.818181818181818	0.393258426966292	Argument Structures, and Tectogramatical Representation in PSD (Oepen et al., 2016)), we did not commit to the underlying syntactic structure of the sentence since we were not obliged to relabel only syntactic structures.	0
23282	23282	S19-2	Guidelines	10	106	0.909090909090909	0.397003745318352	Rather, we annotated words and MWUs if the frame analysis permitted doing so.	0
23283	23283	S19-2	Guidelines	11	107	1.0	0.400749063670412	5	0
23284	23284	S19-2	Annotation Procedure	1	108	0.033333333333333	0.404494382022472	We annotated the data in a modular manner and in a semi-controlled environment using an annotation system developed for this purpose.	0
23285	23285	S19-2	Annotation Procedure	2	109	0.066666666666667	0.408239700374532	The procedure consisted of four steps: 1) Reading and Comprehension; 2) Choosing a Frame; 3) Annotating Arguments; and 4) Rating, Commenting, or Revising.	0
23286	23286	S19-2	Annotation Procedure	3	110	0.1	0.411985018726592	We tracked and logged all changes in the data as well as annotator interaction with the annotation system upon starting to annotate.	0
23287	23287	S19-2	Annotation Procedure	4	111	0.133333333333333	0.415730337078652	The tool measured the time that annotators spent on each record and each annotation step, as well as how annotators moved between steps.	0
23288	23288	S19-2	Annotation Procedure	5	112	0.166666666666667	0.419475655430712	In Step 1, annotators viewed a sentence with one highlighted verb, as in Example 2.	0
23289	23289	S19-2	Annotation Procedure	6	113	0.2	0.423220973782771	(2) Criticism of futures COMES from Wall Street.	0
23290	23290	S19-2	Annotation Procedure	7	114	0.233333333333333	0.426966292134831	The goal of this step was understanding the meaning of the verb and its semantic function, and identifying semantic heads of arguments and their associated words or MWUs.	0
23291	23291	S19-2	Annotation Procedure	8	115	0.266666666666667	0.430711610486891	To continue, an annotator must confirm the understanding of the verb's meaning of the verb, and can identify its semantic arguments.	0
23292	23292	S19-2	Annotation Procedure	9	116	0.3	0.434456928838951	Without confirmation, an annotator would terminate the annotation process for that input sentence and go to the next one.	0
23293	23293	S19-2	Annotation Procedure	10	117	0.333333333333333	0.438202247191011	If confirmed, Step 2 required the annotator to choose the frame that the verb evoked.	0
23294	23294	S19-2	Annotation Procedure	11	118	0.366666666666667	0.441947565543071	This step may have included annotating multi-word phrasal verbs, e.g., COMES+FROM (Example 2).	0
23295	23295	S19-2	Annotation Procedure	12	119	0.4	0.445692883895131	The annotation system assisted by providing a list of likely frames for the verb, including a LU lookup function (as in FN), an extended set of LUs derived via statistical methods, and previously logged annotations.	0
23296	23296	S19-2	Annotation Procedure	13	120	0.433333333333333	0.449438202247191	After reviewing the definitions of the proposed frames, annotators chose one, or annotated the verb form with a different existing FN frame.	0
23297	23297	S19-2	Annotation Procedure	14	121	0.466666666666667	0.453183520599251	"Otherwise, the annotator terminated the process and the record moved to the list of ""skipped items""."	0
23298	23298	S19-2	Annotation Procedure	15	122	0.5	0.456928838951311	The annotation of arguments, Step 3, required 5 Q. Zadeh and Petruck describe the issues in detail.	0
23299	23299	S19-2	Annotation Procedure	16	123	0.533333333333333	0.460674157303371	that annotators label the core FEs of the chosen frame by first identifying their semantic head, which first may have required marking MWUs, e.g., Wall+Street in Example 3, below.	0
23300	23300	S19-2	Annotation Procedure	17	124	0.566666666666667	0.464419475655431	(3) Criticism of futures comes from Wall Street.	0
23301	23301	S19-2	Annotation Procedure	18	125	0.6	0.468164794007491	The tool lists the core FEs and their definitions, and checks the integrity of record annotations to ensure that each core FE is annotated only once.	0
23302	23302	S19-2	Annotation Procedure	19	126	0.633333333333333	0.471910112359551	In parallel, annotators add the verb's subcategorization frame and its semantic role.	0
23303	23303	S19-2	Annotation Procedure	20	127	0.666666666666667	0.47565543071161	We did not annotate null instantiated FEs (but FN does).	0
23304	23304	S19-2	Annotation Procedure	21	128	0.7	0.47940074906367	During step 3, annotators could go back to the previous step and change their choice of frame type.	0
23305	23305	S19-2	Annotation Procedure	22	129	0.733333333333333	0.48314606741573	For	0
23306	23306	S19-2	Annotation Procedure	23	130	0.766666666666667	0.48689138576779	Step 4, annotators rated their annotation, stating their opinion on how well the annotated instance fit FrameNet's definition and how it compared to other annotated instances.	0
23307	23307	S19-2	Annotation Procedure	24	131	0.8	0.49063670411985	In a sense, annotators measured their confidence in the assigned labels.	0
23308	23308	S19-2	Annotation Procedure	25	132	0.833333333333333	0.49438202247191	They did so by selecting a number on a scale from 1 to 5, with 1 not confident at all and 5 the most confident, i.e., the annotation fit perfectly to the chosen FrameNet frame, its definition, and examples.	0
23309	23309	S19-2	Annotation Procedure	26	133	0.866666666666667	0.49812734082397	Annotators had the option to add free text comments on each record.	0
23310	23310	S19-2	Annotation Procedure	27	134	0.9	0.50187265917603	The annotation procedure was rarely straightforward.	0
23311	23311	S19-2	Annotation Procedure	28	135	0.933333333333333	0.50561797752809	Given the interdependence of Steps 2 and 3, annotators usually moved back and forth between them.	0
23312	23312	S19-2	Annotation Procedure	29	136	0.966666666666667	0.50936329588015	In Step 2 an annotator might believe that a target verb did not belong in any existing FN frame.	0
23313	23313	S19-2	Annotation Procedure	30	137	1.0	0.51310861423221	Likewise, annotators could terminate the annotation process even upon reaching the last step.	0
23314	23314	S19-2	Quality Control	1	138	0.125	0.51685393258427	At least two annotators verified all annotation used in the evaluation.	0
23315	23315	S19-2	Quality Control	2	139	0.25	0.52059925093633	A main annotator annotated all records in the dataset; two other annotators verified or disputed those annotations.	0
23316	23316	S19-2	Quality Control	3	140	0.375	0.524344569288389	If annotators could not reach an agreement, we removed the record from the SemEval dataset.	0
23317	23317	S19-2	Quality Control	4	141	0.5	0.528089887640449	A full analysis of annotator disagreement goes beyond the scope of this work.	0
23318	23318	S19-2	Quality Control	5	142	0.625	0.531835205992509	While the source of annotator disagreement may seem trivial and simple (e.g., only one annotator understood the sentence correctly), we believe that some sentences may have more than one interpretation, all of which are plausible.	0
23319	23319	S19-2	Quality Control	6	143	0.75	0.535580524344569	Like the disagreement resulting from incorrect frame assignment, deciding what frame a verb evokes may be challenging; and resolving the dilemma is not always simple.	0
23320	23320	S19-2	Quality Control	7	144	0.875	0.539325842696629	Choosing between two related frames (e.g., BUILDING vs. INTENTIONALLY CREATE, related via Inheritance in FN), or identifying metaphorical and non-metaphorical uses of a verb requires subtle and sophisticated understanding of the semantics of the language, and of Frame Semantics.	0
23321	23321	S19-2	Quality Control	8	145	1.0	0.543071161048689	At times, disagreements pointed to more complex linguistic issues that remain in debate, e.g., choosing the semantic head of a syntactically complex argument, treating quantifiers, conjunctions, etc.	0
23322	23322	S19-2	Summary statistics	1	146	0.045454545454546	0.546816479400749	Table 1 shows a statistical summary of the annotation task.	0
23323	23323	S19-2	Summary statistics	2	147	0.090909090909091	0.550561797752809	The SemEval column reports the statistics for the final set of records, i.e., gold records with double-agreement between annotators, and which we used to evaluate the systems.	0
23324	23324	S19-2	Summary statistics	3	148	0.136363636363636	0.554307116104869	Total reports the statistics of all analyzed records, from which we chose our SemEval data.	0
23325	23325	S19-2	Summary statistics	4	149	0.181818181818182	0.558052434456929	Skipped and InProg show the statistics for discarded records and records without a final decision, respectively.	0
23326	23326	S19-2	Summary statistics	5	150	0.227272727272727	0.561797752808989	Dev shows the statistics for the development set.	0
23327	23327	S19-2	Summary statistics	6	151	0.272727272727273	0.565543071161049	Each of the rows reports a value of a component of the data or annotator interaction with the data.	0
23328	23328	S19-2	Summary statistics	7	152	0.318181818181818	0.569288389513109	Records indicates the number of annotated verbs and their arguments.	0
23329	23329	S19-2	Summary statistics	8	153	0.363636363636364	0.573033707865169	Sentences and Tokens indicate the size of the sub-corpus of the annotated records.	0
23330	23330	S19-2	Summary statistics	9	154	0.409090909090909	0.576779026217228	VF is the number of distinct verb lemmas (273), mapped to the number of distinct frames that the Frames-Type row shows (149) (Figure 3   Confidence reports the average of annotatorassigned confidence scores for annotations per record.	0
23331	23331	S19-2	Summary statistics	10	155	0.454545454545455	0.580524344569288	Although interpreting this measure demands more work, the averages appear to be as expected.	0
23332	23332	S19-2	Summary statistics	11	156	0.5	0.584269662921348	Specifically, SemEval is higher in value than both InProg and Skipped, facts that we associate with double agreement and the choice reviewing process.	0
23333	23333	S19-2	Summary statistics	12	157	0.545454545454545	0.588014981273408	Still, many records with high confidence scores remained as InProg given the lack of double agreement.	0
23334	23334	S19-2	Summary statistics	13	158	0.590909090909091	0.591760299625468	Table 5 (Appendix A.1) lists the top 10 frames annotated with their respective highest and lowest confidence ratings averaged by their frequency in SemEval.	0
23335	23335	S19-2	Summary statistics	14	159	0.636363636363636	0.595505617977528	The last two rows of Table 1 are meta-data on the annotation process.	0
23336	23336	S19-2	Summary statistics	15	160	0.681818181818182	0.599250936329588	Time reports the total time annotators spent in active annotation, engaged in the steps described above (742 hours), excluding the reviewing process (Section 4.3.1) and including the time to annotate MWUs.	0
23337	23337	S19-2	Summary statistics	16	161	0.727272727272727	0.602996254681648	Total-Move is the total number of logical moves for frame annotation between annotators and the annotation system, i.e., logged changes in the process of frame and core FE annotation.	0
23338	23338	S19-2	Summary statistics	17	162	0.772727272727273	0.606741573033708	This number excludes annotation of verb subcategorization with generic semantic roles.	0
23339	23339	S19-2	Summary statistics	18	163	0.818181818181818	0.610486891385768	6 In SemEval, annotated frames had an average of 2.15 arguments, requiring a minimum of five logical moves to annotate (MWU-less sentences).	0
23340	23340	S19-2	Summary statistics	19	164	0.863636363636364	0.614232209737828	However, on average, each SemEval record required 14.8 moves.	0
23341	23341	S19-2	Summary statistics	20	165	0.909090909090909	0.617977528089888	This number is even higher for InProg (18.2); we believe that it indicates the complexity of the annotation task.	0
23342	23342	S19-2	Summary statistics	21	166	0.954545454545455	0.621722846441948	Table 4 (Appendix A.1) further details annotator activity, with time spent and moves per annotation step.	0
23343	23343	S19-2	Summary statistics	22	167	1.0	0.625468164794007	As expected, frame annotation of verbs (Step 2), was the most time consuming part of the task.	0
23344	23344	S19-2	Development Dataset	1	168	0.5	0.629213483146067	Shared task participants received a development set consisting of 600 records from a total of 4,620 records, where Table 4 shows the statistics.	0
23345	23345	S19-2	Development Dataset	2	169	1.0	0.632958801498127	The development set contained gold annotations for all three subtasks.	0
23346	23346	S19-2	Evaluation Metrics	1	170	0.090909090909091	0.636704119850187	For all subtasks, as figure of merit, here we report the performance of participating systems with measures for evaluating text clustering techniques, including the classic measures of Purity (PU), inverse-Purity (IPU), and their harmonic mean (PIF) (Steinbach et al., 2000), as well as the harmonic mean for BCubed precision and recall (i.e., BCP, BCR, and BCF, respectively) (Bagga and Baldwin, 1998).	0
23347	23347	S19-2	Evaluation Metrics	2	171	0.181818181818182	0.640449438202247	To compute these measures for the pairing of reference-labeled data and unsupervised-labeled data (with each having an exact set of annotated items), we built a contingency table T with rows for gold labels and columns for unsupervised system labels.	0
23348	23348	S19-2	Evaluation Metrics	3	172	0.272727272727273	0.644194756554307	We filled the table with the number of intersecting items, as done in cross-tabulation of results in classification tasks to compute precision and recall.	0
23349	23349	S19-2	Evaluation Metrics	4	173	0.363636363636364	0.647940074906367	For Task A (Section 3), T tracks the unsupervised system labels and the gold reference labels assigned to verbs.	0
23350	23350	S19-2	Evaluation Metrics	5	174	0.454545454545455	0.651685393258427	For Task B.1, we labeled the rows and columns of T with tuples (l v , l a ), where l v labels the frame evoking verb and l a labels the FE filler.	0
23351	23351	S19-2	Evaluation Metrics	6	175	0.545454545454545	0.655430711610487	For Task B.2, the rows and columns in T track the unsupervised system labels and the gold reference labels (generic semantic roles) assigned to arguments.	0
23352	23352	S19-2	Evaluation Metrics	7	176	0.636363636363636	0.659176029962547	These performance measures reflect a notion of similarity between the distribution of unsupervised labels and that of the gold reference labels, given certain criteria.	0
23353	23353	S19-2	Evaluation Metrics	8	177	0.727272727272727	0.662921348314607	Specifically, they define the notions of consistency and completeness of automatically generated clusters based on the evaluation data.	0
23354	23354	S19-2	Evaluation Metrics	9	178	0.818181818181818	0.666666666666667	Each method measures consistency and completeness in its own way, and alone may lack sufficient information for a clear understanding and analysis of system performance (Amigó et al., 2009).	0
23355	23355	S19-2	Evaluation Metrics	10	179	0.909090909090909	0.670411985018727	But, as the single metric for system ranking, we used the BCF measure, given its satisfactory behavior in certain situations.	0
23356	23356	S19-2	Evaluation Metrics	11	180	1.0	0.674157303370787	Note that we modeled the task and its evaluation as hard clustering, where a record receives only one label, without overlap in any generated category of items.	0
23357	23357	S19-2	Baselines	1	181	0.076923076923077	0.677902621722846	Similar to other clustering tasks, we use baselines of random, all-in-one-cluster (AIN1), and one-cluster-per-instance (1CPI).	0
23358	23358	S19-2	Baselines	2	182	0.153846153846154	0.681647940074906	Additionally, we adapted the baseline of the most frequent sense in WSI for these tasks by introducing the one-cluster-per-head (1CPH) baseline in Task A, and one-cluster-per-syntactic-category (1CPG) for verb argument clustering in Task B.2. 7 For Task B.1, we built a baseline, 1CPGH for labeling verbs with their lemmas (as in 1CPH) and FEs with grammatical relation to their heads (as in 1CPG).	0
23359	23359	S19-2	Baselines	3	183	0.230769230769231	0.685393258426966	We included two more labels lcmpx and rcmpx for frame fillers with no direct syntactic relation to the head verb, if occurring left of or right of the verb, respectively.	0
23360	23360	S19-2	Baselines	4	184	0.307692307692308	0.689138576779026	Both 1CPH and 1CPG (and their combination for Task B.1) are hard to beat because of the longtailed distribution of the frequency of our test data.	0
23361	23361	S19-2	Baselines	5	185	0.384615384615385	0.692883895131086	E.g., most verbs frequently instantiate one particular frame and rarely other ones.	0
23362	23362	S19-2	Baselines	6	186	0.461538461538462	0.696629213483146	Similarly, a particular role (FE) frequently is filled by words that have a particular grammatical relation to its governing verb; e.g., most subjects of most verb forms receive the agent label in their subcategorization frame (or, an agent-like element in their Frame Semantics representations).	0
23363	23363	S19-2	Baselines	7	187	0.538461538461538	0.700374531835206	Evidently the chosen labels for grammatical relations influences 1CPG and 1CPHG scores.	0
23364	23364	S19-2	Baselines	8	188	0.615384615384615	0.704119850187266	Values reported later (specifically, Tables 6 and 2) could be improved by employing heuristics, e.g., relabeling enhanced dependencies using a few rules.	0
23365	23365	S19-2	Baselines	9	189	0.692307692307692	0.707865168539326	We also employed one unsupervised and a second supervised system baselines.	0
23366	23366	S19-2	Baselines	10	190	0.769230769230769	0.711610486891386	For the unsupervised one, we trained the system with data from Kallmeyer et al. (2018).	0
23367	23367	S19-2	Baselines	11	191	0.846153846153846	0.715355805243446	For the supervised one, we used OPEN-SESAME, a state-of-the-art supervised FrameNet tagger (Swayamdipta et al., 2018).	0
23368	23368	S19-2	Baselines	12	192	0.923076923076923	0.719101123595506	After converting its output to the format of the present task, we evaluated it similar to other systems.	0
23369	23369	S19-2	Baselines	13	193	1.0	0.722846441947566	Both systems were trained out-of-thebox with no additional tuning.	0
23370	23370	S19-2	System Descriptions	1	194	0.047619047619048	0.726591760299625	We received submissions from nine teams (13 participants).	0
23371	23371	S19-2	System Descriptions	2	195	0.095238095238095	0.730337078651685	Only three chose to submit system description papers.	0
23372	23372	S19-2	System Descriptions	3	196	0.142857142857143	0.734082397003745	provided a solution for Task A and Task B.2, using both sets of these results to address Task B.1.	0
23373	23373	S19-2	System Descriptions	4	197	0.19047619047619	0.737827715355805	Task A used language models and Hearst-like patterns to tune and obtain contextualized vector representations for the verbs in the test set.	0
23374	23374	S19-2	System Descriptions	5	198	0.238095238095238	0.741573033707865	A hierarchical agglomerative clustering method followed, where hyperparameters were set with labeled and unlabeled records from the development and test sets.	0
23375	23375	S19-2	System Descriptions	6	199	0.285714285714286	0.745318352059925	Task B.2 employed a logistic regression trained over the development set to identify only the most frequent labels.	0
23376	23376	S19-2	System Descriptions	7	200	0.333333333333333	0.749063670411985	The classifier was based on features obtained from a language model and hand-crafted rules.	0
23377	23377	S19-2	System Descriptions	8	201	0.380952380952381	0.752808988764045	Using logistic regression and training this algorithm with the development set remains an issue of concern, given the intended unsupervised scenario.	0
23378	23378	S19-2	System Descriptions	9	202	0.428571428571429	0.756554307116105	While we objected to using the development set to train a supervised system for this subtask, we still report its scores.	0
23379	23379	S19-2	System Descriptions	10	203	0.476190476190476	0.760299625468165	The differences between its results and those of the other systems may be informative.	0
23380	23380	S19-2	System Descriptions	11	204	0.523809523809524	0.764044943820225	Still, we considered Arefyev et al.'s results for Task B only complementarily, not to rank the systems.	0
23381	23381	S19-2	System Descriptions	12	205	0.571428571428571	0.767790262172285	Anwar et al. (2019) proposed a method that was similar to that of .	0
23382	23382	S19-2	System Descriptions	13	206	0.619047619047619	0.771535580524345	Arefyev et al. used contextualized word embeddings from the BERT language modeling tool Devlin et al. (2018), whereas Anwar et al. used pre-trained embeddings.	0
23383	23383	S19-2	System Descriptions	14	207	0.666666666666667	0.775280898876405	They merged the outputs of Tasks A and B.2 for Task B.1.	0
23384	23384	S19-2	System Descriptions	15	208	0.714285714285714	0.779026217228464	Task A used agglomerative clustering of vectors with concatenated verb representation vectors and vectors that represent usage context.	0
23385	23385	S19-2	System Descriptions	16	209	0.761904761904762	0.782771535580524	Task B.2 employed hand crafted features, a method to encode syntactic information, and again an agglomerative clustering method.	0
23386	23386	S19-2	System Descriptions	17	210	0.80952380952381	0.786516853932584	Ribeiro et al. (2019) also reported results for all subtasks using similar techniques to those reported in the other two submitted papers.	0
23387	23387	S19-2	System Descriptions	18	211	0.857142857142857	0.790262172284644	Ribeiro et al. (2019) used the bidirectional neural language model BERT, which  also used.	0
23388	23388	S19-2	System Descriptions	19	212	0.904761904761905	0.794007490636704	Task A employed contextualized word representations proposed in (Ustalov et al., 2018), and Biemann's clustering algorithm (Biemann, 2006).	0
23389	23389	S19-2	System Descriptions	20	213	0.952380952380952	0.797752808988764	Compared to the two other systems, Ribeiro et al. (2019) exploited input structures, weighted them, and used them elegantly in its algorithm.	0
23390	23390	S19-2	System Descriptions	21	214	1.0	0.801498127340824	With the same method but different hyper-parameters for B.2 along with combining results from Task A, Ribeiro et al. (2019) offered a solution to B.1.	0
23391	23391	S19-2	Results and Data Analysis	1	215	0.024390243902439	0.805243445692884	Table 2 reports the BCF scores for system submissions along with a baseline for each task.	0
23392	23392	S19-2	Results and Data Analysis	2	216	0.048780487804878	0.808988764044944	8	0
23393	23393	S19-2	Results and Data Analysis	3	217	0.073170731707317	0.812734082397004	As the table shows, each system performs best only in one of the tasks.	0
23394	23394	S19-2	Results and Data Analysis	4	218	0.097560975609756	0.816479400749064	We report Arefyev et al.'s submission for Tasks B.1 and B.2 only to show the benefit of using a small amount of training data and a supervised method together with a clustering algorithm, provided that such training data is available.	0
23395	23395	S19-2	Results and Data Analysis	5	219	0.121951219512195	0.820224719101124	As readers know, finding the optimal (actual) number of clusters is an open research area.	0
23396	23396	S19-2	Results and Data Analysis	6	220	0.146341463414634	0.823970037453183	Participants knew the number of clusters: whereas Arefyev et al. and Anwar et al. used this information, Ribeiro et al. opted for a statistical method tuned with data that we provided.	0
23397	23397	S19-2	Results and Data Analysis	7	221	0.170731707317073	0.827715355805244	The baseline systems, the unsupervised method of Kallmeyer et al. (2018)   of all systems regarding BCF.	0
23398	23398	S19-2	Results and Data Analysis	8	222	0.195121951219512	0.831460674157303	This result is not surprising since that work did not effectively handle MWUs in the test, where only the head of the MWU was kept.	0
23399	23399	S19-2	Results and Data Analysis	9	223	0.219512195121951	0.835205992509363	However, the output of Open-SESAME, and its low BCF was indeed surprising.	0
23400	23400	S19-2	Results and Data Analysis	10	224	0.24390243902439	0.838951310861423	We fed Open-SESAME the sentences from the test set; it identified approximately 5k frames.	0
23401	23401	S19-2	Results and Data Analysis	11	225	0.268292682926829	0.842696629213483	However, the overlap with the test set was only 1,216 records (identification problem in Open-SESAME).	0
23402	23402	S19-2	Results and Data Analysis	12	226	0.292682926829268	0.846441947565543	These 1,216 records exhibit a mismatch between 536 of the arguments and their respective target verbs.	0
23403	23403	S19-2	Results and Data Analysis	13	227	0.317073170731707	0.850187265917603	We ignored the system's extra or incorrectly generated arguments, and replaced the missing items with those of the 1CPHG baseline records.	0
23404	23404	S19-2	Results and Data Analysis	14	228	0.341463414634146	0.853932584269663	We then used the resulting records for evaluation against the task's gold data as did the task's participants.	0
23405	23405	S19-2	Results and Data Analysis	15	229	0.365853658536585	0.857677902621723	As Table 3 shows, the unsupervised method outperforms the supervised system for all tasks by a wide margin, i.e.,the unsupervised label set can carry more information than does the supervised label set.	0
23406	23406	S19-2	Results and Data Analysis	16	230	0.390243902439024	0.861423220973783	We compared results for confidence measure that annotators assigned to records.	0
23407	23407	S19-2	Results and Data Analysis	17	231	0.414634146341463	0.865168539325843	First, we split the evaluation records according to their assigned confidence value into five subsets E i , 1 ≤ i ≤ 5, such that subset E 1 contained only records with confidence value 1, E 2 contained only record with confidence value 2, etc..	0
23408	23408	S19-2	Results and Data Analysis	18	232	0.439024390243902	0.868913857677903	Then we evaluated system outputs on each subset E i and logged that BCF.	0
23409	23409	S19-2	Results and Data Analysis	19	233	0.463414634146341	0.872659176029963	Later, we performed this evaluation cumulatively using subsets E i s by adding records from all E j s to E i where i &lt; j. Interpreting the obtained values requires careful attention (e.g., changes in the prior probabilities of gold clusters and their cardinality must be taken into account), overall, we observed a similar trend for all systems: as expected, namely a positive correlation between the confidence value and BCF.	0
23410	23410	S19-2	Results and Data Analysis	20	234	0.487804878048781	0.876404494382022	Thus, what human annotators usually found hard to annotate, automatic systems also found hard to cluster.	0
23411	23411	S19-2	Results and Data Analysis	21	235	0.51219512195122	0.880149812734082	(The reverse relation does not hold).	0
23412	23412	S19-2	Results and Data Analysis	22	236	0.536585365853659	0.883895131086142	Or, pessimistically, the level of noise in annotation increases as their associated confidence decreases.	0
23413	23413	S19-2	Results and Data Analysis	23	237	0.560975609756098	0.887640449438202	(Table 7 in Appendix A.2 details the results.)	0
23414	23414	S19-2	Results and Data Analysis	24	238	0.585365853658537	0.891385767790262	Finally, we wanted to identify the frames that machines found difficult to cluster.	0
23415	23415	S19-2	Results and Data Analysis	25	239	0.609756097560976	0.895131086142322	To estimate difficulty we used the differences in BCF under the following conditions.	0
23416	23416	S19-2	Results and Data Analysis	26	240	0.634146341463415	0.898876404494382	We repeated the evaluation process 1 ≤ i ≤ n times (where n is the number of gold labels for a task) for each system.	0
23417	23417	S19-2	Results and Data Analysis	27	241	0.658536585365854	0.902621722846442	In each iteration i, we removed all data items of a gold category i.	0
23418	23418	S19-2	Results and Data Analysis	28	242	0.682926829268293	0.906367041198502	We measured and noted the resulting BCF in the given iteration; we deduced the score from the system performance over the entire gold set.	0
23419	23419	S19-2	Results and Data Analysis	29	243	0.707317073170732	0.910112359550562	To cancel frequency effects, we normalized the differences by the number of gold data instances.	0
23420	23420	S19-2	Results and Data Analysis	30	244	0.731707317073171	0.913857677902622	We removed all records annotated as COMMERCE SELL from the evaluation set E to form E .	0
23421	23421	S19-2	Results and Data Analysis	31	245	0.75609756097561	0.917602996254682	We computed the BCF of the systems over E (E ⊂ E), and measured d = E BCF − E BCF .	0
23422	23422	S19-2	Results and Data Analysis	32	246	0.780487804878049	0.921348314606742	We interpreted a positive difference as an easy to cluster gold category i, and a negative difference as a hard to cluster gold category i.	0
23423	23423	S19-2	Results and Data Analysis	33	247	0.804878048780488	0.925093632958802	The heat maps in Table 8 and Table 9 show a summary of the results for Task A and Task B.2, respectively.	0
23424	23424	S19-2	Results and Data Analysis	34	248	0.829268292682927	0.928838951310861	All systems performed similarly for approximately 30% of the gold classes.	0
23425	23425	S19-2	Results and Data Analysis	35	249	0.853658536585366	0.932584269662921	Comparing differences across systems and the baselines of 1CPH and 1CPG reveals (possibly) interesting information.	0
23426	23426	S19-2	Results and Data Analysis	36	250	0.878048780487805	0.936329588014981	Thus, for example, in Task A, most systems found COMMERCE SELL hard and COMMERCE BUY easy to cluster.	0
23427	23427	S19-2	Results and Data Analysis	37	251	0.902439024390244	0.940074906367041	Interestingly, a set of six verbs evokes each frame: buy, purchase, buy back, buy up, buy out, buy into for COMMERCE BUY; and sell, retail, auction, place, deal, resell for COMMERCE SELL.	0
23428	23428	S19-2	Results and Data Analysis	38	252	0.926829268292683	0.943820224719101	From these two sets of verbs, three are polysemous: buy in the former, and place and deal in the latter.	0
23429	23429	S19-2	Results and Data Analysis	39	253	0.951219512195122	0.947565543071161	Does the morphology of the verbs (e.g., buy-back, resell) make one easy to cluster?	0
23430	23430	S19-2	Results and Data Analysis	40	254	0.975609756097561	0.951310861423221	Alternatively, are other factors at play, such as the number of verb instances?	0
23431	23431	S19-2	Results and Data Analysis	41	255	1.0	0.955056179775281	How these factors might influence the proposed naive BCF-difference model is an open question.	0
23432	23432	S19-2	Concluding Remarks	1	256	0.083333333333333	0.958801498127341	We have presented the SemEval 2019 task on unsupervised lexical frame induction.	0
23433	23433	S19-2	Concluding Remarks	2	257	0.166666666666667	0.962546816479401	We described the task in detail, provided a summary of methods that participants developed, and compared the results.	0
23434	23434	S19-2	Concluding Remarks	3	258	0.25	0.966292134831461	Although much room for improvement of the task remains, we consider it a step forward.	0
23435	23435	S19-2	Concluding Remarks	4	259	0.333333333333333	0.970037453183521	It employed a well-motivated typology of lexical frames to distinguish lexical frame induction tasks.	0
23436	23436	S19-2	Concluding Remarks	5	260	0.416666666666667	0.973782771535581	The evaluation data derived from annotations of a well-known resource, namely a portion of WSJ sentences, perhaps the most annotated corpus of English.	0
23437	23437	S19-2	Concluding Remarks	6	261	0.5	0.97752808988764	These features provide opportunities for future investigation, in particular in studies related to reciprocal relations between syntactic and lexical semantic frame structures.	0
23438	23438	S19-2	Concluding Remarks	7	262	0.583333333333333	0.9812734082397	One reason to promote using unsupervised methods is their inherent flexibility to embrace unknown data.	0
23439	23439	S19-2	Concluding Remarks	8	263	0.666666666666667	0.98501872659176	These methods have a high margin of tolerance for noise, and perform better than supervised method with insufficient training data.	0
23440	23440	S19-2	Concluding Remarks	9	264	0.75	0.98876404494382	For unsupervised data, obtaining or generating training data is easier than doing so with supervised methods because they simply do not require annotation.	0
23441	23441	S19-2	Concluding Remarks	10	265	0.833333333333333	0.99250936329588	For example, all participant systems could collect similar unlabeled training data from only syntactically annotated corpora to generate more unlabeled records.	0
23442	23442	S19-2	Concluding Remarks	11	266	0.916666666666667	0.99625468164794	Ultimately, such methods can achieve respectable performance, and produce clusters which are both more informative than the unlabeled input and supervised categories (under certain situations).	0
23443	23443	S19-2	Concluding Remarks	12	267	1.0	1.0	As shown, unsupervised methods can even outperform a state-of-the-art Frame Semantics parser by a wide margin (Section 7), while a very large gap remains for improvements in future work.	0
26004	26004	S20-4	title	1	1	1.0	0.003484320557491	SemEval-2020 Task 4: Commonsense Validation and Explanation	0
26005	26005	S20-4	abstract	1	2	0.111111111111111	0.006968641114983	In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons.	1
26006	26006	S20-4	abstract	2	3	0.222222222222222	0.010452961672474	Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not.	0
26007	26007	S20-4	abstract	3	4	0.333333333333333	0.013937282229965	The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense.	0
26008	26008	S20-4	abstract	4	5	0.444444444444444	0.017421602787457	In the third subtask, a participating system needs to generate the reason.	0
26009	26009	S20-4	abstract	5	6	0.555555555555556	0.020905923344948	We finally attracted 39 teams participating at least one of the three subtasks.	0
26010	26010	S20-4	abstract	6	7	0.666666666666667	0.024390243902439	For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans.	0
26011	26011	S20-4	abstract	7	8	0.777777777777778	0.02787456445993	However, for Subtask C, there is still a relatively large gap between systems and human performance.	0
26012	26012	S20-4	abstract	8	9	0.888888888888889	0.031358885017422	The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation;	0
26013	26013	S20-4	abstract	9	10	1.0	0.034843205574913	The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.	0
26014	26014	S20-4	Introduction	1	11	0.043478260869565	0.038327526132404	In the past decades, computer' ability in processing natural language has significantly improved.	0
26015	26015	S20-4	Introduction	2	12	0.086956521739131	0.041811846689896	However, its intelligence for understanding common sense expressed in language is still limited.	0
26016	26016	S20-4	Introduction	3	13	0.130434782608696	0.045296167247387	"For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: ""John put a turkey into a fridge"" while ""John put an elephant into the fridge"" does not, but it is non-trivial for a computer to tell the difference."	0
26017	26017	S20-4	Introduction	4	14	0.173913043478261	0.048780487804878	Arguably, commonsense reasoning plays a central role in a natural language understanding system (Davis, 2017).	0
26018	26018	S20-4	Introduction	5	15	0.217391304347826	0.052264808362369	It is essential to gauge how well computers can understand whether a given statement makes sense.	0
26019	26019	S20-4	Introduction	6	16	0.260869565217391	0.055749128919861	In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world.	0
26020	26020	S20-4	Introduction	7	17	0.304347826086957	0.059233449477352	1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012;Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016;	0
26021	26021	S20-4	Introduction	8	18	0.347826086956522	0.062717770034843	Ostermann et al., 2018b;	0
26022	26022	S20-4	Introduction	9	19	0.391304347826087	0.066202090592335	Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016;	0
26023	26023	S20-4	Introduction	10	20	0.434782608695652	0.069686411149826	Talmor et al., 2018;Mihaylov et al., 2018).	0
26024	26024	S20-4	Introduction	11	21	0.478260869565217	0.073170731707317	They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge.	0
26025	26025	S20-4	Introduction	12	22	0.521739130434783	0.076655052264808	The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.	0
26026	26026	S20-4	Introduction	13	23	0.565217391304348	0.0801393728223	The SemEval-2020	0
26027	26027	S20-4	Introduction	14	24	0.608695652173913	0.083623693379791	Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons.	0
26028	26028	S20-4	Introduction	15	25	0.652173913043478	0.087108013937282	"In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., ""John put an elephant into the fridge"" and ""John put a turkey into the fridge"", respectively."	0
26029	26029	S20-4	Introduction	16	26	0.695652173913043	0.090592334494774	The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense.	0
26030	26030	S20-4	Introduction	17	27	0.739130434782609	0.094076655052265	"For example, for the nonsensical statement, ""John put an elephant into the fridge"", the three options are ""An elephant is much bigger than a fridge"", ""Elephants are usually white while fridges are usually white"", and ""An elephant cannot eat a fridge."""	0
26031	26031	S20-4	Introduction	18	28	0.782608695652174	0.097560975609756	A system needs to identify the correct reason.	0
26032	26032	S20-4	Introduction	19	29	0.826086956521739	0.101045296167247	In addition, the third task requires the participating systems to generate the reason automatically.	0
26033	26033	S20-4	Introduction	20	30	0.869565217391304	0.104529616724739	We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.	0
26034	26034	S20-4	Introduction	21	31	0.91304347826087	0.10801393728223	There are 39 teams submitting valid systems to at least one subtask.	0
26035	26035	S20-4	Introduction	22	32	0.956521739130435	0.111498257839721	In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects.	0
26036	26036	S20-4	Introduction	23	33	1.0	0.114982578397213	However, for Subtask C, there is still a relatively large between system and human performances.	0
26037	26037	S20-4	Task Setup	1	34	0.03448275862069	0.118466898954704	Task Definition	0
26038	26038	S20-4	Task Setup	2	35	0.068965517241379	0.121951219512195	Formally, each instance in our dataset is composed of eight sentences:	0
26039	26039	S20-4	Task Setup	3	36	0.103448275862069	0.125435540069686	and s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not.	0
26040	26040	S20-4	Task Setup	4	37	0.137931034482759	0.128919860627178	They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense.	0
26041	26041	S20-4	Task Setup	5	38	0.172413793103448	0.132404181184669	For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense.	0
26042	26042	S20-4	Task Setup	6	39	0.206896551724138	0.13588850174216	So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options.	0
26043	26043	S20-4	Task Setup	7	40	0.241379310344828	0.139372822299652	For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense.	0
26044	26044	S20-4	Task Setup	8	41	0.275862068965517	0.142857142857143	Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.	0
26045	26045	S20-4	Task Setup	9	42	0.310344827586207	0.146341463414634	• Subtask A: Validation Task:	0
26046	26046	S20-4	Task Setup	10	43	0.344827586206897	0.149825783972125	Select the statement of the two that does not make sense.	0
26047	26047	S20-4	Task Setup	11	44	0.379310344827586	0.153310104529617	s 1 : John put a turkey into a fridge.	0
26048	26048	S20-4	Task Setup	12	45	0.413793103448276	0.156794425087108	s 2 : John put an elephant into the fridge.	0
26049	26049	S20-4	Task Setup	13	46	0.448275862068966	0.160278745644599	In this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .	0
26050	26050	S20-4	Task Setup	14	47	0.482758620689655	0.163763066202091	• Subtask B: Explanation (Multi-Choice)	0
26051	26051	S20-4	Task Setup	15	48	0.517241379310345	0.167247386759582	Task:	0
26052	26052	S20-4	Task Setup	16	49	0.551724137931034	0.170731707317073	Select the best reason that explains why the given statement does not make sense.	0
26053	26053	S20-4	Task Setup	17	50	0.586206896551724	0.174216027874564	Nonsensical statement (s n ):	0
26054	26054	S20-4	Task Setup	18	51	0.620689655172414	0.177700348432056	John put an elephant into the fridge.	0
26055	26055	S20-4	Task Setup	19	52	0.655172413793103	0.181184668989547	o 1 : An elephant is much bigger than a fridge.	0
26056	26056	S20-4	Task Setup	20	53	0.689655172413793	0.184668989547038	o 2 : Elephants are usually white while fridges are usually white.	0
26057	26057	S20-4	Task Setup	21	54	0.724137931034483	0.18815331010453	o 3 : An elephant cannot eat a fridge.	0
26058	26058	S20-4	Task Setup	22	55	0.758620689655172	0.191637630662021	In this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .	0
26059	26059	S20-4	Task Setup	23	56	0.793103448275862	0.195121951219512	• Subtask C: Explanation (Generation)	0
26060	26060	S20-4	Task Setup	24	57	0.827586206896552	0.198606271777003	Task: Generate the reason why this statement does not make sense.	0
26061	26061	S20-4	Task Setup	25	58	0.862068965517241	0.202090592334495	Nonsensical statement (s n ):	0
26062	26062	S20-4	Task Setup	26	59	0.896551724137931	0.205574912891986	John put an elephant into the fridge.	0
26063	26063	S20-4	Task Setup	27	60	0.931034482758621	0.209059233449477	Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge.	0
26064	26064	S20-4	Task Setup	28	61	0.96551724137931	0.212543554006969	r 2 : A fridge is much smaller than an elephant.	0
26065	26065	S20-4	Task Setup	29	62	1.0	0.21602787456446	r 3 : Most of the fridges aren't large enough to contain an elephant.	0
26066	26066	S20-4	Score Description	1	63	0.5	0.219512195121951	0	0
26067	26067	S20-4	Score Description	2	64	1.0	0.222996515679442	The reason is not grammatically correct, or not comprehensible at all, or not related to the statement at all.	0
26068	26068	S20-4	1	1	65	0.5	0.226480836236934	The reason is just the negation of the statement or a simple paraphrase.	0
26069	26069	S20-4	1	2	66	1.0	0.229965156794425	Obviously, a better explanation can be made.	0
26070	26070	S20-4	2	1	67	0.5	0.233449477351916	The reason is relevant and appropriate, though it may contain a few grammatical errors or unnecessary parts.	0
26071	26071	S20-4	2	2	68	1.0	0.236933797909408	Or like case 1, but it's hard to write a proper reason.	0
26072	26072	S20-4	3	1	69	0.5	0.240418118466899	The reason is appropriate and is a solid explanation of why the statement does not make sense.	0
26073	26073	S20-4	3	2	70	1.0	0.24390243902439	Table 1: Rubrics used in human evaluation in Subtask C.	0
26074	26074	S20-4	Evaluation Metrics	1	71	0.111111111111111	0.247386759581881	The Subtasks	0
26075	26075	S20-4	Evaluation Metrics	2	72	0.222222222222222	0.250871080139373	A and B are evaluated using accuracy.	0
26076	26076	S20-4	Evaluation Metrics	3	73	0.333333333333333	0.254355400696864	Subtask C is evaluated with the BLEU score (Papineni et al., 2002).	0
26077	26077	S20-4	Evaluation Metrics	4	74	0.444444444444444	0.257839721254355	In addition, for Subtask C, we further perform human evaluation.	0
26078	26078	S20-4	Evaluation Metrics	5	75	0.555555555555556	0.261324041811847	We randomly select 100 instances from the test set and evaluate system outputs on Amazon Mechanical Turk.	0
26079	26079	S20-4	Evaluation Metrics	6	76	0.666666666666667	0.264808362369338	We ask three different crowd-sourcing workers to score each generated reason with a scale ranging from 0 to 3, inclusively, according the rubrics listed in Table 1.	0
26080	26080	S20-4	Evaluation Metrics	7	77	0.777777777777778	0.268292682926829	Then we calculate the average score of the three scores as our final human evaluation score.	0
26081	26081	S20-4	Evaluation Metrics	8	78	0.888888888888889	0.271777003484321	Formally, the human evaluation score of system k is	0
26082	26082	S20-4	Evaluation Metrics	9	79	1.0	0.275261324041812	where score ijk means the score from the j th annotator for system k on the i th instance.	0
26083	26083	S20-4	Data Construction	1	80	0.076923076923077	0.278745644599303	Our data construction is mainly performed on Amazon Mechanical Turk, which consists of two steps:	0
26084	26084	S20-4	Data Construction	2	81	0.153846153846154	0.282229965156794	• Step 1: In this step, we construct datasets for Subtask A and Subtask B. Specifically, we ask a crowd-sourcing worker to write a sensical statement s c and a nonsensical statement s n .	0
26085	26085	S20-4	Data Construction	3	82	0.230769230769231	0.285714285714286	For the nonsensical statement s n , the worker further writes three sentences, o 1 , o 2 , o 3 ; one of them, denoted as o c , explains why the nonsensical statement does not make sense; two of them, denoted as o n1 and o n2 , serve as the confusing choices.	0
26086	26086	S20-4	Data Construction	4	83	0.307692307692308	0.289198606271777	(Refer to Section 3.1 for details.)	0
26087	26087	S20-4	Data Construction	5	84	0.384615384615385	0.292682926829268	•	0
26088	26088	S20-4	Data Construction	6	85	0.461538461538462	0.29616724738676	Step 2: We then make three reference reasons, r 1 , r 2 , r 3 for Subtask C.	0
26089	26089	S20-4	Data Construction	7	86	0.538461538461538	0.299651567944251	We use o c as one of three references, and collect two more references in this step.	0
26090	26090	S20-4	Data Construction	8	87	0.615384615384615	0.303135888501742	We ask two different crowd-sourcing workers to write each of them.	0
26091	26091	S20-4	Data Construction	9	88	0.692307692307692	0.306620209059233	Note that instead of letting the same worker in step 1 to write these two references, we asked two more workers.	0
26092	26092	S20-4	Data Construction	10	89	0.769230769230769	0.310104529616725	The reason is to encourage diversity of the reference.	0
26093	26093	S20-4	Data Construction	11	90	0.846153846153846	0.313588850174216	(Refer to Section 3.2 for details.)	0
26094	26094	S20-4	Data Construction	12	91	0.923076923076923	0.317073170731707	Finally, each instance of the dataset have 8 sentences:	0
26095	26095	S20-4	Data Construction	13	92	1.0	0.320557491289199	, but for convenience of description, we denote it differently.	0
26096	26096	S20-4	3.1	1	93	0.041666666666667	0.32404181184669	Step 1: Collecting Data for Subtask A and B Annotation Guidelines.	0
26097	26097	S20-4	3.1	2	94	0.083333333333333	0.327526132404181	When writing instances, workers were asked to follow several principles: (1)	0
26098	26098	S20-4	3.1	3	95	0.125	0.331010452961672	Try to avoid complex knowledge and focus on daily common sense.	0
26099	26099	S20-4	3.1	4	96	0.166666666666667	0.334494773519164	Make the questions as understandable as possible, so that a literate person is able to give the right answers.	0
26100	26100	S20-4	3.1	5	97	0.208333333333333	0.337979094076655	(2)	0
26101	26101	S20-4	3.1	6	98	0.25	0.341463414634146	The confusing reason options, o n1 and o n2 , should better contain more content words or information such as entities and activities in the nonsensical statements s n .	0
26102	26102	S20-4	3.1	7	99	0.291666666666667	0.344947735191638	"For example, the confusing reasons of ""John put an elephant into the fridge"" should better contain both ""elephant"" and ""fridge""."	0
26103	26103	S20-4	3.1	8	100	0.333333333333333	0.348432055749129	(3)	0
26104	26104	S20-4	3.1	9	101	0.375	0.35191637630662	The confusing reasons, o n1 and o n2 , should be related to the statements s n and the correct reason o c and not deviate from the context; otherwise it may be easily captured by pretrained models like BERT (Talmor et al., 2018).	0
26105	26105	S20-4	3.1	10	102	0.416666666666667	0.355400696864111	( 4  and o 3 should only be related to the incorrect statements s n rather than the correct statements s c , because we want further studies to be able to estimate nonsensical statements s n without the correct statement s c . (5)	0
26106	26106	S20-4	3.1	11	103	0.458333333333333	0.358885017421603	The confusing reasons, o n1 and o n2 , should make sense themselves.	0
26107	26107	S20-4	3.1	12	104	0.5	0.362369337979094	Otherwise, the models may simply ignore the incorrect options o n1 , o n2 without considering the casual semantics.	0
26108	26108	S20-4	3.1	13	105	0.541666666666667	0.365853658536585	This concern is raised from and motivated by the fact that models can achieve high performance in the ROC Story Cloze Task, when only looking at the alternative endings and ignoring the story content (Schwartz et al., 2017).	0
26109	26109	S20-4	3.1	14	106	0.583333333333333	0.369337979094077	( 6)	0
26110	26110	S20-4	3.1	15	107	0.625	0.372822299651568	We ask the annotators to make the nonsensical statement s n contain about the same number of words as the sensical statement s c , and the correct reason o c have similar length with other two options.	0
26111	26111	S20-4	3.1	16	108	0.666666666666667	0.376306620209059	We drop the instances which do not meet such requirements.	0
26112	26112	S20-4	3.1	17	109	0.708333333333333	0.37979094076655	Use of Inspirational Materials.	0
26113	26113	S20-4	3.1	18	110	0.75	0.383275261324042	It is not easy for all crowd-sourcing workers to write instances from scratch.	0
26114	26114	S20-4	3.1	19	111	0.791666666666667	0.386759581881533	To address this issue, we also provide them with external reading materials to stimulate inspiration, such as the sentences of the Open Mind Common Sense (OMCS) project (Havasi et al., 2010).	0
26115	26115	S20-4	3.1	20	112	0.833333333333333	0.390243902439024	"For example, ""he was sent to a (restaurant)/(hospital) for treatment after a car crash"" can be inspired by the two sentences ""restaurants provide food"" and ""hospitals provide medical care""."	0
26116	26116	S20-4	3.1	21	113	0.875	0.393728222996516	Quality Control.	0
26117	26117	S20-4	3.1	22	114	0.916666666666667	0.397212543554007	To ensure the quality of the data, we manually check the instances and drop or request a rewriting of the low-quality ones.	0
26118	26118	S20-4	3.1	23	115	0.958333333333333	0.400696864111498	If one worker writes too many low-quality instances, we will remove her or him from our annotator pool.	0
26119	26119	S20-4	3.1	24	116	1.0	0.40418118466899	With such process, we finally accept around 30% submitted instances.	0
26120	26120	S20-4	3.2	1	117	0.055555555555556	0.407665505226481	Step 2: Collecting Data for Subtask C Annotation Guidelines.	0
26121	26121	S20-4	3.2	2	118	0.111111111111111	0.411149825783972	To collect data for Subtask C, each worker is given a nonsensical statement s n and a sensical statement s c and asked to write a reason to explain why the nonsensical statement s n does not make sense.	0
26122	26122	S20-4	3.2	3	119	0.166666666666667	0.414634146341463	They shall follow the following rules: (1)	0
26123	26123	S20-4	3.2	4	120	0.222222222222222	0.418118466898955	Do not explain why the sensical statement s c makes sense.	0
26124	26124	S20-4	3.2	5	121	0.277777777777778	0.421602787456446	(2)	0
26125	26125	S20-4	3.2	6	122	0.333333333333333	0.425087108013937	Avoid mentioning the sensical statement s c . (3)	0
26126	26126	S20-4	3.2	7	123	0.388888888888889	0.428571428571429	"Write the reason, rather than simply add the word ""not"" or ""can't"" to the nonsensical statement s n to form an explanation."	0
26127	26127	S20-4	3.2	8	124	0.444444444444444	0.43205574912892	(4)	0
26128	26128	S20-4	3.2	9	125	0.5	0.435540069686411	"Write the reason, don't use patterns like ""XXX is not for YYY"" to create an explanation."	0
26129	26129	S20-4	3.2	10	126	0.555555555555556	0.439024390243902	(5)	0
26130	26130	S20-4	3.2	11	127	0.611111111111111	0.442508710801394	Do not try to justify why the nonsensical statement s n makes sense. (6)	0
26131	26131	S20-4	3.2	12	128	0.666666666666667	0.445993031358885	Write only one sentence, do not be overly formal.	0
26132	26132	S20-4	3.2	13	129	0.722222222222222	0.449477351916376	(7)	0
26133	26133	S20-4	3.2	14	130	0.777777777777778	0.452961672473868	"Refrain from using ""because"" at the beginning of a sentence. (8)"	0
26134	26134	S20-4	3.2	15	131	0.833333333333333	0.456445993031359	Do not try to correct the statement s n , but just give the reason.	0
26135	26135	S20-4	3.2	16	132	0.888888888888889	0.45993031358885	Quality Control.	0
26136	26136	S20-4	3.2	17	133	0.944444444444444	0.463414634146341	As the same as in Step 1, after the annotators write the reasons in Step 2, the first two authors of the paper perform the check process again.	0
26137	26137	S20-4	3.2	18	134	1.0	0.466898954703833	We reject low-quality reasons (that violate the rules significantly) and low-quality annotators (who write many low-quality reasons with the number above a threshold).	0
26138	26138	S20-4	Data Summary and Analysis	1	135	0.05	0.470383275261324	For SemEval-2020, we created 11,997 instances (i.e., 11,997 8-sentence tuples).	0
26139	26139	S20-4	Data Summary and Analysis	2	136	0.1	0.473867595818815	We further split the instances into three subsets with 10,000 (the training set), 997 (the development set), and 1,000 (the test set) instances, respectively.	0
26140	26140	S20-4	Data Summary and Analysis	3	137	0.15	0.477351916376307	We randomly assign the label of the correct options in subtask A and B to avoid unbalanced correct labels.	0
26141	26141	S20-4	Data Summary and Analysis	4	138	0.2	0.480836236933798	We conduct three more data analysis experiments to evaluate data quality, including sentence length, common words and repetition.	0
26142	26142	S20-4	Data Summary and Analysis	5	139	0.25	0.484320557491289	Average Length.	0
26143	26143	S20-4	Data Summary and Analysis	6	140	0.3	0.487804878048781	In  sensical statements and nonsensical statements almost have the same average lengths in the three sets (the differences are equal or smaller than 1%), which is balanced.	0
26144	26144	S20-4	Data Summary and Analysis	7	141	0.35	0.491289198606272	However, there is an obvious gap between the correct reasons and confusing reasons in terms of the average lengths (roughly 4% in the training set and 10% in the dev/test set).	0
26145	26145	S20-4	Data Summary and Analysis	8	142	0.4	0.494773519163763	Common Word Analysis.	0
26146	26146	S20-4	Data Summary and Analysis	9	143	0.45	0.498257839721254	The most common words are important for showing the differences between sentences.	0
26147	26147	S20-4	Data Summary and Analysis	10	144	0.5	0.501742160278746	We only present those words which have obvious different frequencies between sensical statements and nonsensical statements or between correct/referential reasons and confusing reasons.	0
26148	26148	S20-4	Data Summary and Analysis	11	145	0.55	0.505226480836237	So, we skip most uninformative words, including 'a', 'an', 'the', 'to', 'in', 'on', 'of', 'for', 'and', 'is', 'are' and 'be'.	0
26149	26149	S20-4	Data Summary and Analysis	12	146	0.6	0.508710801393728	After removing those words, we can list the top-5 common words in each type of sentence in the training/dev+test sets.	0
26150	26150	S20-4	Data Summary and Analysis	13	147	0.65	0.51219512195122	For sensical statements s c and nonsensical statements s n , there are no significant differences between the training, dev, and test set.	0
26151	26151	S20-4	Data Summary and Analysis	14	148	0.7	0.515679442508711	"However, there is an obvious gap in the correct reasons o c and confusing reasons o n in negative words such as ""not"", ""no"", and ""cannot""."	0
26152	26152	S20-4	Data Summary and Analysis	15	149	0.75	0.519163763066202	In the training data, negative words are about 3 times more common in the correct option o c than in the confusing options o n .	0
26153	26153	S20-4	Data Summary and Analysis	16	150	0.8	0.522648083623693	In the dev+test data, the gap is about 40%, which indicates that the dev+test data has a higher quality than the training data.	0
26154	26154	S20-4	Data Summary and Analysis	17	151	0.85	0.526132404181185	However, as discussed in (Niven and Kao, 2019), spurious statistical cues can affect BERT's results.	0
26155	26155	S20-4	Data Summary and Analysis	18	152	0.9	0.529616724738676	We conjure that the negative words are also spurious effective clues, which make the Subtask B potentially easier.	0
26156	26156	S20-4	Data Summary and Analysis	19	153	0.95	0.533101045296167	Repetition.	0
26157	26157	S20-4	Data Summary and Analysis	20	154	1.0	0.536585365853659	The dev+test set have 12 instances (0.6%) that repeat the same nonsensical statements in the training data and 36 instances (1.8%) that repeat the same correct reasons with the training data.	0
26158	26158	S20-4	Cautions of using the data	1	155	0.2	0.54006968641115	The following advice is given to all task participants and future users: (1)	0
26159	26159	S20-4	Cautions of using the data	2	156	0.4	0.543554006968641	Feel free to use whatever additional data they deem appropriate for the tasks to train their model. (2)	0
26160	26160	S20-4	Cautions of using the data	3	157	0.6	0.547038327526132	Do not use the input of Subtask B/C to help Subtask A and do not use the option o of Subtask B to help Subtask C. Otherwise the task will be artificially easy.	0
26161	26161	S20-4	Cautions of using the data	4	158	0.8	0.550522648083624	This is because of two reasons: a)	0
26162	26162	S20-4	Cautions of using the data	5	159	1.0	0.554006968641115	The nonsensical statements s n of Subtask B and Subtask C is exactly the nonsensical statements s c of Subtask A and, participants can use the input of the Subtask B/C to directly obtain the answer of Subtask A and the option answers o of Subtask B will also reduce the difficulty of Subtask A; b) the correct reason o c of Subtask B is also one of the reference reason o c in Subtask C.	0
26163	26163	S20-4	Systems and Results	1	160	0.5	0.557491289198606	In this section, we show the evaluation results of all the submitted systems for the three subtasks.	0
26164	26164	S20-4	Systems and Results	2	161	1.0	0.560975609756098	Since most systems share similar model architecture for subtasks A and B, we discuss the two subtasks together.	0
26165	26165	S20-4	Subtask A and Subtask B	1	162	0.038461538461539	0.564459930313589	The formal evaluation results of Subtask A and B are shown in Table 4 and 5.	0
26166	26166	S20-4	Subtask A and Subtask B	2	163	0.076923076923077	0.56794425087108	There are in total 39 valid submissions for Subtask A and 27 valid submissions for Subtask B. Most top-performing submissions Figure 1: The most commonly used model architectures used in the three subtasks.	0
26167	26167	S20-4	Subtask A and Subtask B	3	164	0.115384615384615	0.571428571428571	This figure is mostly based on Team Solomon's system.	0
26168	26168	S20-4	Subtask A and Subtask B	4	165	0.153846153846154	0.574912891986063	"For Subtask B and C, the connector can be simply ""No, "", to help in constraining the model to learn a choice that explains the unreasonability of the statement."	0
26169	26169	S20-4	Subtask A and Subtask B	5	166	0.192307692307692	0.578397212543554	For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input.	0
26170	26170	S20-4	Subtask A and Subtask B	6	167	0.230769230769231	0.581881533101045	For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference.	0
26171	26171	S20-4	Subtask A and Subtask B	7	168	0.269230769230769	0.585365853658537	adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task.	0
26172	26172	S20-4	Subtask A and Subtask B	8	169	0.307692307692308	0.588850174216028	See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge.	0
26173	26173	S20-4	Subtask A and Subtask B	9	170	0.346153846153846	0.592334494773519	Below we introduce in detail several top-performing systems and their main features.	0
26174	26174	S20-4	Subtask A and Subtask B	10	171	0.384615384615385	0.59581881533101	• CN-HIT-IT.NLP  ranks top in Subtask A.	0
26175	26175	S20-4	Subtask A and Subtask B	11	172	0.423076923076923	0.599303135888502	They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs.	0
26176	26176	S20-4	Subtask A and Subtask B	12	173	0.461538461538462	0.602787456445993	K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity.	0
26177	26177	S20-4	Subtask A and Subtask B	13	174	0.5	0.606271777003484	They use ConceptNet as the commonsense repository to extract the triples for the statements.	0
26178	26178	S20-4	Subtask A and Subtask B	14	175	0.538461538461538	0.609756097560976	• ECNU-Sense	0
26179	26179	S20-4	Subtask A and Subtask B	15	176	0.576923076923077	0.613240418118467	Maker (Zhao et al., 2020)    • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa.	0
26180	26180	S20-4	Subtask A and Subtask B	16	177	0.615384615384615	0.616724738675958	• BUT-FIT (Jon et al., 2020), LMVE , Lijunyi  use ALBERT as the encoder.	0
26181	26181	S20-4	Subtask A and Subtask B	17	178	0.653846153846154	0.620209059233449	BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system.	0
26182	26182	S20-4	Subtask A and Subtask B	18	179	0.692307692307692	0.623693379790941	• UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020)	0
26183	26183	S20-4	Subtask A and Subtask B	19	180	0.730769230769231	0.627177700348432	Table 6: Subtask C results of all the submitted systems.	0
26184	26184	S20-4	Subtask A and Subtask B	20	181	0.769230769230769	0.630662020905923	Those marked with * did not submit a system description paper, and those marked with + means they do not include Subtask C in their system description paper.	0
26185	26185	S20-4	Subtask A and Subtask B	21	182	0.807692307692308	0.634146341463415	It can be seen from the results that pretrained language models such as RoBERTa can achieve rather high performance, e.g., the team Solomon achieves 96.0% and 94.0% on Subtask A and Subtask B, respectively, without using further resources.	0
26186	26186	S20-4	Subtask A and Subtask B	22	183	0.846153846153846	0.637630662020906	This shows that large-scale pretrained language models do contain commonsense knowledge to deal with the Subtask A and the Subtask B in this challenge.	0
26187	26187	S20-4	Subtask A and Subtask B	23	184	0.884615384615385	0.641114982578397	Additionally finetuning the pretrained language models on commonsense-related text such as OMCS, which we use as inspirational materials, can push the results even higher, close to human performance.	0
26188	26188	S20-4	Subtask A and Subtask B	24	185	0.923076923076923	0.644599303135889	The best-performing teams on Subtask A and Subtask B both adopt K-BERT, which incorporates the external knowledge base (i.e. ConceptNet) to complement the pretrained language models with knowledge triples.	0
26189	26189	S20-4	Subtask A and Subtask B	25	186	0.961538461538462	0.64808362369338	This shows that knowledge-graph-enhanced approaches, such as K-BERT can effectively incorporate external knowledge.	0
26190	26190	S20-4	Subtask A and Subtask B	26	187	1.0	0.651567944250871	However, the high number may also indicate data leaking to some extent, since in the data creation stage, both ConceptNet and OMCS are used as references for the annotator to write the data instances.	0
26191	26191	S20-4	Subtask C	1	188	0.038461538461539	0.655052264808362	The results for Subtask C are shown in Table 6.	0
26192	26192	S20-4	Subtask C	2	189	0.076923076923077	0.658536585365854	There are in total 17 valid submissions for Subtask C.	0
26193	26193	S20-4	Subtask C	3	190	0.115384615384615	0.662020905923345	There are generally two approaches: (1) sequence-to-sequence approach, where the source side is the non-sensical statement, and the reason is the target sequence. (2) language model generation approach, which uses large-scale pretrained auto-regressive language models such as GPT-2 (Radford et al., 2019) for reason generation, where the non-sensical sentence acts as prompt.	0
26194	26194	S20-4	Subtask C	4	191	0.153846153846154	0.665505226480836	An example of the language model generation approach is shown in Figure 1, which is most commonly used and achieves relatively good results.	0
26195	26195	S20-4	Subtask C	5	192	0.192307692307692	0.668989547038328	Below we describe in detail the systems and their main features.	0
26196	26196	S20-4	Subtask C	6	193	0.230769230769231	0.672473867595819	• BUT-FIT (Jon et al., 2020) experiments with both the sequence-to-sequence approach and the language generation approach.	0
26197	26197	S20-4	Subtask C	7	194	0.269230769230769	0.67595818815331	For the sequence-to-sequence approach, they use BART (Lewis et al., 2019) with beam-search decoding to achieves the highest BLEU among all the teams.	0
26198	26198	S20-4	Subtask C	8	195	0.307692307692308	0.679442508710801	For the language generation approach, the nonsensical statement is used as a prompt.	0
26199	26199	S20-4	Subtask C	9	196	0.346153846153846	0.682926829268293	At the training stage, the statement and the explanation are concatenated together, and a GPT-2 is trained on these sequences with a next token prediction objective.	0
26200	26200	S20-4	Subtask C	10	197	0.384615384615385	0.686411149825784	At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated.	0
26201	26201	S20-4	Subtask C	11	198	0.423076923076923	0.689895470383275	• KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART.	0
26202	26202	S20-4	Subtask C	12	199	0.461538461538462	0.693379790940767	To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary.	0
26203	26203	S20-4	Subtask C	13	200	0.5	0.696864111498258	2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation.	0
26204	26204	S20-4	Subtask C	14	201	0.538461538461538	0.700348432055749	This approach proves effective and makes their system second-best for human evaluations.	0
26205	26205	S20-4	Subtask C	15	202	0.576923076923077	0.70383275261324	• ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework.	0
26206	26206	S20-4	Subtask C	16	203	0.615384615384615	0.707317073170732	Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train the model with two self-attention heads: one for language models and another for classification.	0
26207	26207	S20-4	Subtask C	17	204	0.653846153846154	0.710801393728223	They then use data from both task B and task C to calculate language model loss and classification loss.	0
26208	26208	S20-4	Subtask C	18	205	0.692307692307692	0.714285714285714	Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage.	0
26209	26209	S20-4	Subtask C	19	206	0.730769230769231	0.717770034843206	• Solomon (Srivastava et al., 2020), JUSTers (Fadel et al., 2020), SWAGex (Rim and Okazaki, 2020), UI (Doxolodeo and Mahendra, 2020)	0
26210	26210	S20-4	Subtask C	20	207	0.769230769230769	0.721254355400697	Large-scale pretrained language models such as BART and GPT-2 dominates the submissions.	0
26211	26211	S20-4	Subtask C	21	208	0.807692307692308	0.724738675958188	The two systems with the highest human evaluations, namely ANA and KaLM, use additional resources such as Wiktionary, OMCS, and other commonsense datasets.	0
26212	26212	S20-4	Subtask C	22	209	0.846153846153846	0.728222996515679	This again shows that additional knowledge from structured databases can help with the generation of the reasons.	0
26213	26213	S20-4	Subtask C	23	210	0.884615384615385	0.731707317073171	From Table 6 we can see that BLEU does not correlate well with Human Evaluation, especially for the top-performing systems.	0
26214	26214	S20-4	Subtask C	24	211	0.923076923076923	0.735191637630662	"According to a further experiment of BUT-FIT, the naive baseline of ""copying source sentence as the reason"" can give a BLEU of 17.23, which can rank No. 4 among all the submissions."	0
26215	26215	S20-4	Subtask C	25	212	0.961538461538462	0.738675958188153	This indicates that BLEU, which focuses on the surface token overlap, has difficulty in evaluating the generated text reliably.	0
26216	26216	S20-4	Subtask C	26	213	1.0	0.742160278745645	The top-performed system achieves the human evaluation score of 2.10, showing the power of pretrained language models, but considering the human performance of 2.58, we still have a long way to go to generate human acceptable reasons.	0
26217	26217	S20-4	Related Work	1	214	0.017857142857143	0.745644599303136	Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention.	0
26218	26218	S20-4	Related Work	2	215	0.035714285714286	0.749128919860627	In the Winograd Schema Challenge (WSC) (Levesque et al., 2012;	0
26219	26219	S20-4	Related Work	3	216	0.053571428571429	0.752613240418119	Morgenstern and Ortiz, 2015), a model needs to solve hard co-reference resolution problems based on commonsense knowledge.	0
26220	26220	S20-4	Related Work	4	217	0.071428571428572	0.75609756097561	"For example, ""The trophy would not fit in the brown suitcase because it was too big."	0
26221	26221	S20-4	Related Work	5	218	0.089285714285714	0.759581881533101	"What was too big (trophy or suitcase)?"""	0
26222	26222	S20-4	Related Work	6	219	0.107142857142857	0.763066202090592	The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) emphasizes on events and consequences.	0
26223	26223	S20-4	Related Work	7	220	0.125	0.766550522648084	Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives.	0
26224	26224	S20-4	Related Work	8	221	0.142857142857143	0.770034843205575	All premises and alternatives are simple sentences.	0
26225	26225	S20-4	Related Work	9	222	0.160714285714286	0.773519163763066	"For example, the premise can be ""The man broke his toe."	0
26226	26226	S20-4	Related Work	10	223	0.178571428571429	0.777003484320557	"What was the CAUSE of this?"" and the two candidate answers are ""(1)"	0
26227	26227	S20-4	Related Work	11	224	0.196428571428571	0.780487804878049	"He got a hole in his sock."" and ""(2)"	0
26228	26228	S20-4	Related Work	12	225	0.214285714285714	0.78397212543554	"He dropped a hammer on his foot."""	0
26229	26229	S20-4	Related Work	13	226	0.232142857142857	0.787456445993031	Several subsequent datasets are inspired by COPA.	0
26230	26230	S20-4	Related Work	14	227	0.25	0.790940766550523	The JHU Ordinal Common-sense Inference (JOCI) (Zhang et al., 2017) aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation.	0
26231	26231	S20-4	Related Work	15	228	0.267857142857143	0.794425087108014	Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation.	0
26232	26232	S20-4	Related Work	16	229	0.285714285714286	0.797909407665505	Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused.	0
26233	26233	S20-4	Related Work	17	230	0.303571428571429	0.801393728222996	Besides, our dataset is not limited to events or situations.	0
26234	26234	S20-4	Related Work	18	231	0.321428571428571	0.804878048780488	It concerns a broader commonsense setting, which includes events, descriptions, assertion etc.	0
26235	26235	S20-4	Related Work	19	232	0.339285714285714	0.808362369337979	Some datasets are inspired by reading comprehension.	0
26236	26236	S20-4	Related Work	20	233	0.357142857142857	0.81184668989547	The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016;	0
26237	26237	S20-4	Related Work	21	234	0.375	0.815331010452962	Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story.	0
26238	26238	S20-4	Related Work	22	235	0.392857142857143	0.818815331010453	For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question.	0
26239	26239	S20-4	Related Work	23	236	0.410714285714286	0.822299651567944	Most questions require knowledge beyond the facts mentioned in the text.	0
26240	26240	S20-4	Related Work	24	237	0.428571428571429	0.825783972125435	Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want.	0
26241	26241	S20-4	Related Work	25	238	0.446428571428571	0.829268292682927	Some other datasets evolve from QA problems and care more about factual commonsense knowledge.	0
26242	26242	S20-4	Related Work	26	239	0.464285714285714	0.832752613240418	SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions.	0
26243	26243	S20-4	Related Work	27	240	0.482142857142857	0.836236933797909	Commonsense	0
26244	26244	S20-4	Related Work	28	241	0.5	0.839721254355401	QA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBook	0
26245	26245	S20-4	Related Work	29	242	0.517857142857143	0.843205574912892	QA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions.	0
26246	26246	S20-4	Related Work	30	243	0.535714285714286	0.846689895470383	The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences.	0
26247	26247	S20-4	Related Work	31	244	0.553571428571429	0.850174216027875	MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020).	0
26248	26248	S20-4	Related Work	32	245	0.571428571428571	0.853658536585366	Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.	0
26249	26249	S20-4	Related Work	33	246	0.589285714285714	0.857142857142857	Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018;	0
26250	26250	S20-4	Related Work	34	247	0.607142857142857	0.860627177700348	"Porada et al., 2019), such as ""gorilla-ride-camel""."	0
26251	26251	S20-4	Related Work	35	248	0.625	0.86411149825784	"In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as ""China's territory is larger than Japan's""."	0
26252	26252	S20-4	Related Work	36	249	0.642857142857143	0.867595818815331	And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017).	0
26253	26253	S20-4	Related Work	37	250	0.660714285714286	0.871080139372822	"Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task ""Tom's mom become (happy)/(upset) when Tom gets high grades in the exam"" is about social and emotional common sense."	0
26254	26254	S20-4	Related Work	38	251	0.678571428571429	0.874564459930314	For our first task, those statements that conforms to commonsense can also be phrased as being plausible.	0
26255	26255	S20-4	Related Work	39	252	0.696428571428571	0.878048780487805	Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.	0
26256	26256	S20-4	Related Work	40	253	0.714285714285714	0.881533101045296	More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions.	0
26257	26257	S20-4	Related Work	41	254	0.732142857142857	0.885017421602787	In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks.	0
26258	26258	S20-4	Related Work	42	255	0.75	0.888501742160279	Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on.	0
26259	26259	S20-4	Related Work	43	256	0.767857142857143	0.89198606271777	Event2	0
26260	26260	S20-4	Related Work	44	257	0.785714285714286	0.895470383275261	Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances.	0
26261	26261	S20-4	Related Work	45	258	0.803571428571429	0.898954703832753	These datasets are not directly useful for our benchmark since they focus only on a small domain.	0
26262	26262	S20-4	Related Work	46	259	0.821428571428571	0.902439024390244	Concept	0
26263	26263	S20-4	Related Work	47	260	0.839285714285714	0.905923344947735	Net is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004;	0
26264	26264	S20-4	Related Work	48	261	0.857142857142857	0.909407665505226	Havasi et al., 2007;	0
26265	26265	S20-4	Related Work	49	262	0.875	0.912891986062718	Speer and Havasi, 2013;	0
26266	26266	S20-4	Related Work	50	263	0.892857142857143	0.916376306620209	Speer et al., 2017).	0
26267	26267	S20-4	Related Work	51	264	0.910714285714286	0.9198606271777	Concept	0
26268	26268	S20-4	Related Work	52	265	0.928571428571429	0.923344947735192	Net constructs triples using labeled edges as relations and various words and/or phrases as entities.	0
26269	26269	S20-4	Related Work	53	266	0.946428571428571	0.926829268292683	It also has the sentences describing the corresponding triples.	0
26270	26270	S20-4	Related Work	54	267	0.964285714285714	0.930313588850174	In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.	0
26271	26271	S20-4	Related Work	55	268	0.982142857142857	0.933797909407666	Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences.	0
26272	26272	S20-4	Related Work	56	269	1.0	0.937282229965157	In our task here, we also provide training data with human annotations.	0
26273	26273	S20-4	Summary	1	270	0.055555555555556	0.940766550522648	This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation.	0
26274	26274	S20-4	Summary	2	271	0.111111111111111	0.944250871080139	In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences.	0
26275	26275	S20-4	Summary	3	272	0.166666666666667	0.947735191637631	The task attracted around 40 participating teams, out of which 31 teams submit their system papers.	0
26276	26276	S20-4	Summary	4	273	0.222222222222222	0.951219512195122	The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.	0
26277	26277	S20-4	Summary	5	274	0.277777777777778	0.954703832752613	We attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences.	0
26278	26278	S20-4	Summary	6	275	0.333333333333333	0.958188153310105	2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge (Zhou et al., 2019), which helps achieve considerably better performance.	0
26279	26279	S20-4	Summary	7	276	0.388888888888889	0.961672473867596	3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers.	0
26280	26280	S20-4	Summary	8	277	0.444444444444444	0.965156794425087	The top-3 systems also use OMCS, which potentially help them to attain better performances.	0
26281	26281	S20-4	Summary	9	278	0.5	0.968641114982578	4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty.	0
26282	26282	S20-4	Summary	10	279	0.555555555555556	0.97212543554007	5) Some instances have obvious patterns.	0
26283	26283	S20-4	Summary	11	280	0.611111111111111	0.975609756097561	"For example, there are tens of instances that contain ""put XXX into YYY"", and ""XXX is bigger than YYY"", making the problems simpler."	0
26284	26284	S20-4	Summary	12	281	0.666666666666667	0.979094076655052	6) Hundreds of crowd-sourcing workers write instances.	0
26285	26285	S20-4	Summary	13	282	0.722222222222222	0.982578397212544	"It is likely for workers to think about the shared commonsense knowledge, such as ""XXX is bigger/shorter/quicker/slower than YYY""."	0
26286	26286	S20-4	Summary	14	283	0.777777777777778	0.986062717770035	We consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation.	0
26287	26287	S20-4	Summary	15	284	0.833333333333333	0.989547038327526	2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns.	0
26288	26288	S20-4	Summary	16	285	0.888888888888889	0.993031358885017	3) Subtask A can be turned into a more difficult form.	0
26289	26289	S20-4	Summary	17	286	0.944444444444444	0.996515679442509	Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not.	0
26290	26290	S20-4	Summary	18	287	1.0	1.0	4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.	0
26291	26291	S20-5	title	1	1	1.0	0.004385964912281	SemEval-2020 Task 5: Counterfactual Recognition	0
26292	26292	S20-5	abstract	1	2	0.125	0.008771929824561	We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020.	0
26293	26293	S20-5	abstract	2	3	0.25	0.013157894736842	Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent).	0
26294	26294	S20-5	abstract	3	4	0.375	0.017543859649123	Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequents with causal relations.	0
26295	26295	S20-5	abstract	4	5	0.5	0.021929824561404	Our task provides a benchmark for counterfactual recognition in natural language with two subtasks.	0
26296	26296	S20-5	abstract	5	6	0.625	0.026315789473684	Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not.	0
26297	26297	S20-5	abstract	6	7	0.75	0.030701754385965	Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement.	0
26298	26298	S20-5	abstract	7	8	0.875	0.035087719298246	During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2.	0
26299	26299	S20-5	abstract	8	9	1.0	0.039473684210526	The data, baseline code, and leaderboard can be found	0
26300	26300	S20-5	Introduction	1	10	0.027027027027027	0.043859649122807	"Counterfactual statements describe events that did not happen or cannot happen, and the possible consequences had those events happened, e.g., ""if kangaroos had no tails, they would topple over"" (Lewis, 2013)."	0
26301	26301	S20-5	Introduction	2	11	0.054054054054054	0.048245614035088	"By developing a connection between the antecedent (e.g., ""kangaroos had no tails"") and consequent (e.g., ""they would topple over""), based on the imagination of possible worlds, humans can naturally form some causal judgments; e.g., having tails can prevent kangaroos from toppling over."	0
26302	26302	S20-5	Introduction	3	12	0.081081081081081	0.052631578947369	One can understand counterfactuals using knowledge and explore the relationship between causes and effects.	0
26303	26303	S20-5	Introduction	4	13	0.108108108108108	0.057017543859649	Although we may not be able to rollback the events which have happened or make impossible events occur in the real world, we can still think of potential outcomes of alternatives.	0
26304	26304	S20-5	Introduction	5	14	0.135135135135135	0.06140350877193	Counterfactual thinking is a remarkable ability of human beings and is considered by many researchers, to act as the highest level of causation in the ladder of causal reasoning.	0
26305	26305	S20-5	Introduction	6	15	0.162162162162162	0.065789473684211	Even the most advanced artificial intelligence system may still be far from achieving human-like counterfactual reasoning.	0
26306	26306	S20-5	Introduction	7	16	0.189189189189189	0.070175438596491	Counterfactual reasoning is an important component for AI systems in obtaining stronger capability in generalization (Pearl and Mackenzie, 2018).	0
26307	26307	S20-5	Introduction	8	17	0.216216216216216	0.074561403508772	Modeling counterfactuals has been studied in many different disciplines.	0
26308	26308	S20-5	Introduction	9	18	0.243243243243243	0.078947368421053	For example, research in psychology has shown that counterfactual thinking can affect human cognition and behaviors (Epstude and Roese, 2008;	0
26309	26309	S20-5	Introduction	10	19	0.27027027027027	0.083333333333333	Kray et al., 2010).	0
26310	26310	S20-5	Introduction	11	20	0.297297297297297	0.087719298245614	The landmark paper of (Goodman, 1947) gives a detailed analysis of counterfactual conditionals in philosophy and logistics.	0
26311	26311	S20-5	Introduction	12	21	0.324324324324324	0.092105263157895	As another example, counterfactuals have also been investigated in epidemiology to reveal the relationship between certain diseases and potential risk factors for those diseases (Vandenbroucke et al., 2016;	0
26312	26312	S20-5	Introduction	13	22	0.351351351351351	0.096491228070176	Krieger and Davey Smith, 2016).	0
26313	26313	S20-5	Introduction	14	23	0.378378378378378	0.100877192982456	We present a counterfactual recognition (CR) task, the task of determining whether a given statement conveys counterfactual thinking or not, and further analyzing the causal relations indicated by counterfactual statements.	0
26314	26314	S20-5	Introduction	15	24	0.405405405405405	0.105263157894737	In our counterfactual recognition task, we aim to model counterfactual semantics and reasoning in natural language.	1
26315	26315	S20-5	Introduction	16	25	0.432432432432432	0.109649122807018	Specifically, we provide a benchmark for counterfactual recognition with two subtasks.	0
26316	26316	S20-5	Introduction	17	26	0.45945945945946	0.114035087719298	Subtask-1 requires systems to determine whether a given statement is counterfactual or not.	0
26317	26317	S20-5	Introduction	18	27	0.486486486486487	0.118421052631579	The counterfactual detection task can serve as a foundation for downstream counterfactual analysis.	0
26318	26318	S20-5	Introduction	19	28	0.513513513513513	0.12280701754386	Subtask-2 requires systems to further locate the antecedent and consequent text spans in a given counterfactual statement, as the connection between an antecedent and consequent can reveal core causal inference clues.	0
26319	26319	S20-5	Introduction	20	29	0.540540540540541	0.12719298245614	To build the dataset for counterfactual recognition, we extract over 60,000 candidate counterfactual statements by scanning through news reports in three domains: finance, politics, and healthcare.	0
26320	26320	S20-5	Introduction	21	30	0.567567567567568	0.131578947368421	The first round of annotation focuses on labeling each sample as true or false, where true denotes a sample is counterfactual and false otherwise in Subtask-1.	0
26321	26321	S20-5	Introduction	22	31	0.594594594594595	0.135964912280702	A portion of samples labeled as true will be further used in Subtask-2 to detect the text spans that describe the antecedent and consequent.	0
26322	26322	S20-5	Introduction	23	32	0.621621621621622	0.140350877192982	Specifically, we carefully select 20,000 high-quality samples from the 60,000 statements and use them in Subtask-1, with 13,000 (65%) as the training set and the rest for testing.	0
26323	26323	S20-5	Introduction	24	33	0.648648648648649	0.144736842105263	The dataset for Subtask-2 contains 5,501 samples, among which we use 3,551 (65%) for training and the rest for testing.	0
26324	26324	S20-5	Introduction	25	34	0.675675675675676	0.149122807017544	To achieve a decent performance in our shared task, we expect the systems should have a certain level of language understanding capacity in both semantics and syntax, together with a certain level of commonsense reasoning ability.	0
26325	26325	S20-5	Introduction	26	35	0.702702702702703	0.153508771929825	In Subtask-1, the top-ranked submissions all use pre-trained neural models, which appear to be an effective way to integrate knowledge learned from large corpus.	0
26326	26326	S20-5	Introduction	27	36	0.72972972972973	0.157894736842105	All of these models use neural networks, which further confirms the effectiveness of distributed representation and subsymbolic approaches for this task.	0
26327	26327	S20-5	Introduction	28	37	0.756756756756757	0.162280701754386	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural networks with symbolic approaches.	0
26328	26328	S20-5	Introduction	29	38	0.783783783783784	0.166666666666667	The first-place model also utilizes data augmentation to further improve system performance.	0
26329	26329	S20-5	Introduction	30	39	0.810810810810811	0.171052631578947	In Subtask-2, top systems take two main approaches: sequence labelling or question answering.	0
26330	26330	S20-5	Introduction	31	40	0.837837837837838	0.175438596491228	Same as systems in Subtask-1, all of them benefit from pre-training.	0
26331	26331	S20-5	Introduction	32	41	0.864864864864865	0.179824561403509	We will provide a more detailed analysis in the system and result section.	0
26332	26332	S20-5	Introduction	33	42	0.891891891891892	0.184210526315789	We built a dataset for this shared task from scratch.	0
26333	26333	S20-5	Introduction	34	43	0.918918918918919	0.18859649122807	Our data, baseline code, and leaderboard can be found at https://competitions.codalab.org/competitions/21691.	0
26334	26334	S20-5	Introduction	35	44	0.945945945945946	0.192982456140351	The data and baseline code are also available at https://zenodo.org/record/3932442.	0
26335	26335	S20-5	Introduction	36	45	0.972972972972973	0.197368421052632	In general, our task here is a relatively basic one in counterfactual analysis in natural language.	0
26336	26336	S20-5	Introduction	37	46	1.0	0.201754385964912	We hope it will intrigue and facilitate further research on counterfactual analysis and can benefit other related downstream tasks.	0
26337	26337	S20-5	Task Setup	1	47	0.5	0.206140350877193	In this section, we detail the two counterfactual recognition subtasks and the metrics used to evaluate the performance.	0
26338	26338	S20-5	Task Setup	2	48	1.0	0.210526315789474	During the evaluation, participants can work on both subtasks or any one of them.	0
26339	26339	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	1	49	0.25	0.214912280701754	We formulate the Subtask-1 as a binary classification problem which asks the participating systems to detect whether a particular sentence is counterfactual or not.	0
26340	26340	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	2	50	0.5	0.219298245614035	Below are two examples of counterfactual statements that need to be recognized:	0
26341	26341	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	3	51	0.75	0.223684210526316	• Example-1: Officials say if they had authority to shut non-bank firms, the collapse of Lehman Brothers, which touched off the most virulent phase of the credit crisis, could have been avoided.	0
26342	26342	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	4	52	1.0	0.228070175438596	• Example-2: The delivery numbers would have been high had it not been for the restrictions imposed by the military for security reasons.	0
26343	26343	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	1	53	0.1	0.232456140350877	Indicating causal relationships is an inherent characteristic of counterfactuals.	0
26344	26344	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	2	54	0.2	0.236842105263158	To further detect the causal knowledge conveyed in counterfactual statements, Subtask-2 aims to extract the antecedents and consequents.	0
26345	26345	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	3	55	0.3	0.241228070175439	Specifically, given a counterfactual statement, systems for Subtask-2 need to identify the indices of the characters which indicate the start and end positions for antecedent and consequent, in terms of character indices: antecedent start ind, antecedent end ind, consequent start ind, consequent end ind.	0
26346	26346	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	4	56	0.4	0.245614035087719	For some statements, the consequents may not be expressed in the statements, then the corresponding consequent start ind and consequent end ind will be set as −1.	0
26347	26347	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	5	57	0.5	0.25	• Example-3: The delivery numbers would have been high had it not been for the restrictions imposed by the military for security reasons.	0
26348	26348	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	6	58	0.6	0.254385964912281	Antecedent: had it not been for the restrictions imposed by the military for security reasons Consequent: the delivery numbers would have been high Label: 42, 122, 0, 40 A counterfactual statement can be converted to a contrapositive with a true antecedent and consequent, by assuming the antecedent and consequent in the original counterfactual statement is inalterably false (Goodman, 1947).	0
26349	26349	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	7	59	0.7	0.258771929824561	Consider the Example-3 above.	0
26350	26350	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	8	60	0.8	0.263157894736842	"It can be transposed into ""since the restrictions imposed by the military for security reasons, the delivery numbers were not high""."	0
26351	26351	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	9	61	0.9	0.267543859649123	After extracting the antecedent and the corresponding consequent from a counterfactual statement, we may derive a contrapositive by performing an appropriate transformation, which can naturally reveal a causal relationship between the two parts or even further indicate the properties of each part.	0
26352	26352	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	10	62	1.0	0.271929824561403	In this way, it is possible to extract causal knowledge across corpora.	0
26353	26353	S20-5	Evaluation Metrics	1	63	0.142857142857143	0.276315789473684	Subtask-1 is a binary classification problem evaluated with Precision, Recall, and F1 score 1 .	0
26354	26354	S20-5	Evaluation Metrics	2	64	0.285714285714286	0.280701754385965	In Subtask-2, we utilize two metrics: (i) Exact match is used to evaluate the percentage of predictions that exactly match the ground truth boundaries of the antecedents and consequents. (ii) F1 score is used to measure the overlap between the predictions and ground truth spans.	0
26355	26355	S20-5	Evaluation Metrics	3	65	0.428571428571429	0.285087719298246	For each sample, we calculate the number of tokens in the overlapped intervals by comparing the predictions and ground truth indices of antecedent and consequent boundaries.	0
26356	26356	S20-5	Evaluation Metrics	4	66	0.571428571428571	0.289473684210526	Then we can compute precision, recall, and F1 score for each sample.	0
26357	26357	S20-5	Evaluation Metrics	5	67	0.714285714285714	0.293859649122807	We take the average F1 score across all the samples in the test set.	0
26358	26358	S20-5	Evaluation Metrics	6	68	0.857142857142857	0.298245614035088	Note the F1 score used in both subtasks is calculated as: F 1 = 2 * P recision * Recall P recision+	0
26359	26359	S20-5	Evaluation Metrics	7	69	1.0	0.302631578947368	Recall .	0
26360	26360	S20-5	Data Development	1	70	0.333333333333333	0.307017543859649	We develop our dataset from news articles in the finance, politics, or healthcare domain.	0
26361	26361	S20-5	Data Development	2	71	0.666666666666667	0.31140350877193	The data development consists of data collection and annotation.	0
26362	26362	S20-5	Data Development	3	72	1.0	0.31578947368421	We use different approaches to ensure the quality of the data.	0
26363	26363	S20-5	Data Collection	1	73	0.083333333333333	0.320175438596491	There are two major challenges in our data construction process.	0
26364	26364	S20-5	Data Collection	2	74	0.166666666666667	0.324561403508772	First, due to the relative sparsity of counterfactual statements in the text, manually annotating each sentence in the original text is not of time and financial efficiency.	0
26365	26365	S20-5	Data Collection	3	75	0.25	0.328947368421053	Accordingly, we perform a filtering step to narrow down candidates.	0
26366	26366	S20-5	Data Collection	4	76	0.333333333333333	0.333333333333333	The second challenge is rooted in the flexibility and complexity of counterfactual expressions.	0
26367	26367	S20-5	Data Collection	5	77	0.416666666666667	0.337719298245614	"Not all counterfactual statements follow certain patterns, e.g., the ""if + past perfect"" pattern (although this is a good pattern which can indicate a conditional relationship between the antecedent and the potential consequent)."	0
26368	26368	S20-5	Data Collection	6	78	0.5	0.342105263157895	To solve these problems, we create a set of templates considering the trade-off between the effectiveness of filtering and its diversity in finding candidate counterfactuals, without making the filtering stage too rigorous.	0
26369	26369	S20-5	Data Collection	7	79	0.583333333333333	0.346491228070175	Token-based Filtering	0
26370	26370	S20-5	Data Collection	8	80	0.666666666666667	0.350877192982456	The template set consists of two subsets that jointly work to find candidate potential counterfactual statements when they are used to search through news articles.	0
26371	26371	S20-5	Data Collection	9	81	0.75	0.355263157894737	The first subset focuses on word token patterns and the second subset leverages POS tag-based patterns.	0
26372	26372	S20-5	Data Collection	10	82	0.833333333333333	0.359649122807018	The full list of token-based patterns are listed in Appendix A. Some of the patterns are based on the previous research which revealed common counterfactual constructions (Hobbs, 2005;	0
26373	26373	S20-5	Data Collection	11	83	0.916666666666667	0.364035087719298	Son et al., 2017;	0
26374	26374	S20-5	Data Collection	12	84	1.0	0.368421052631579	Rouvoli et al., 2019).	0
26375	26375	S20-5	POS-based Filtering	1	85	0.166666666666667	0.37280701754386	The second subset of templates utilize patterns based on part-of-speech tags.	0
26376	26376	S20-5	POS-based Filtering	2	86	0.333333333333333	0.37719298245614	We identified five counterfactual forms based on (Janocko et al., 2016) and coverted them into POS-based patterns to increase the chances of identifying true counterfactual statements.	0
26377	26377	S20-5	POS-based Filtering	3	87	0.5	0.381578947368421	The details of the POSbased rules are presented in Appendix B.	0
26378	26378	S20-5	POS-based Filtering	4	88	0.666666666666667	0.385964912280702	To apply the rules, we tokenize each sentence and conduct POS tagging with the NLTK library (Bird et al., 2009).	0
26379	26379	S20-5	POS-based Filtering	5	89	0.833333333333333	0.390350877192982	Then we extract the sentences which match one of the pre-defined patterns.	0
26380	26380	S20-5	POS-based Filtering	6	90	1.0	0.394736842105263	By applying both the token-based and POS-based rules, we obtain the candidate statements for further human annotation.	0
26381	26381	S20-5	Annotation	1	91	0.111111111111111	0.399122807017544	As described above, each sample in Subtask-1 is labeled either as true (counterfactual) or false (noncounterfactual).	0
26382	26382	S20-5	Annotation	2	92	0.222222222222222	0.403508771929825	We employ a two-step annotation strategy.	0
26383	26383	S20-5	Annotation	3	93	0.333333333333333	0.407894736842105	First, each sample in the candidate statement set is annotated by five annotators to determine whether it is a counterfactual statement or not.	0
26384	26384	S20-5	Annotation	4	94	0.444444444444444	0.412280701754386	We include those annotated as true (counterfactuals) by all five annotators, i.e., with an agreement rate of 100%.	0
26385	26385	S20-5	Annotation	5	95	0.555555555555556	0.416666666666667	For negative samples (non-counterfactual statements), we take all of those labeled as false with 100% agreement and some sentences with 80% agreement, which 4 out of the 5 annotators label as false.	0
26386	26386	S20-5	Annotation	6	96	0.666666666666667	0.421052631578947	We use the Amazon Mechanical Turk (AMT) platform for our annotation, by splitting the samples into HITs (Human Intelligent Task, where each HIT contains 20 to 30 samples) and distributing these HITs to qualified annotators along with thorough instructions and examples.	0
26387	26387	S20-5	Annotation	7	97	0.777777777777778	0.425438596491228	In subtask 2, a portion of counterfactual statements (labeled as true in Subtask-1) are further annotated, in which the text spans of antecedents and consequents in counterfactual statements are obtained.	0
26388	26388	S20-5	Annotation	8	98	0.888888888888889	0.429824561403509	In this stage, each sample was annotated by a single annotator on Amazon Mechanical Turk, and the annotators were asked to double-check whether the sample is a counterfactual statement before underlining specific spans.	0
26389	26389	S20-5	Annotation	9	99	1.0	0.43421052631579	All the samples are further manually checked by ourselves to ensure the antecedent and consequent spans are appropriately labelled by a consistent standard.	0
26390	26390	S20-5	Quality Control	1	100	0.111111111111111	0.43859649122807	We further make the following efforts to control the quality of our datasets.	0
26391	26391	S20-5	Quality Control	2	101	0.222222222222222	0.442982456140351	First we set additional requirements when inviting workers to perform annotation.	0
26392	26392	S20-5	Quality Control	3	102	0.333333333333333	0.447368421052632	We only invite workers from English-speaking countries and only if the approval rates of their previous HITs are above 82%.	0
26393	26393	S20-5	Quality Control	4	103	0.444444444444444	0.451754385964912	In addition, workers take a qualification test before starting their annotation work.	0
26394	26394	S20-5	Quality Control	5	104	0.555555555555556	0.456140350877193	The test provides detailed instructions and examples and includes 40 samples for workers to label which of the samples are counterfactuals.	0
26395	26395	S20-5	Quality Control	6	105	0.666666666666667	0.460526315789474	Using this method, we have over 70 qualified workers for our data annotation task.	0
26396	26396	S20-5	Quality Control	7	106	0.777777777777778	0.464912280701754	Having a stable pool of trained workers is beneficial for ensuring the quality of annotation.	0
26397	26397	S20-5	Quality Control	8	107	0.888888888888889	0.469298245614035	In the entire process of annotation, we randomly select some HITs to evaluate the accuracy and the performance of workers to justify whether to accept them or not.	0
26398	26398	S20-5	Quality Control	9	108	1.0	0.473684210526316	For subtask 2, we manually check all of the samples to ensure: (i) the samples are counterfactual statements (any incorrectly labelled statements are further removed from both Subtask-1 and Subtask-2 datasets); (ii) for a very small number of statements, if the antecedent and consequent spans labelled by the Turkers are not full constituent phrases, we manually adjust the span to make them full phrases.	0
26399	26399	S20-5	Data Statistics	1	109	0.142857142857143	0.478070175438597	Table 1 shows the statistics of the data used in Subtask-1.	0
26400	26400	S20-5	Data Statistics	2	110	0.285714285714286	0.482456140350877	We obtain 20,000 statements in total and we randomly split them into the training (65%) and test set (35%    2 shows the size of data used in Subtask-2.	0
26401	26401	S20-5	Data Statistics	3	111	0.428571428571429	0.486842105263158	In total, we have 5,501 samples and randomly split them into the training and test set.	0
26402	26402	S20-5	Data Statistics	4	112	0.571428571428571	0.491228070175439	Specifically, 3,551 samples are for training and the rest for testing.	0
26403	26403	S20-5	Data Statistics	5	113	0.714285714285714	0.495614035087719	Not all counterfactuals have both an antecedent and a consequent, so we also provide statistics for samples that have only antecedents, and those that have both antecedents and consequents.	0
26404	26404	S20-5	Data Statistics	6	114	0.857142857142857	0.5	Figure 1 in Appendix C shows statistics for the frequency of the number of words in both the Subtask-1 and Subtask-2 training and test datasets.	0
26405	26405	S20-5	Data Statistics	7	115	1.0	0.504385964912281	4 Systems and Results	0
26406	26406	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	1	116	0.019230769230769	0.508771929824561	The baseline used for Subtask-1 is a simple SVM classifier with the linear kernel function.	0
26407	26407	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	2	117	0.038461538461539	0.513157894736842	In the baseline model, we first take some basic preprocessing steps starting with lemmatization; we then extract term frequency and inverse document frequency (tf-idf) features for training the SVM model to perform binary classification.	0
26408	26408	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	3	118	0.057692307692308	0.517543859649123	"The motivation behind using this simple SVM as a baseline is to create a simple model that can identify counterfactuals by learning and searching for the presence keywords and phrases like ""had"" or ""should have been"", that tend to mark the presence of a counterfactual in a number of counterfactual grammatical forms."	0
26409	26409	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	4	119	0.076923076923077	0.521929824561403	The baseline has poor performance, signalling that most counterfactuals cannot be determined based on the presence of certain words and that reasoning is necessary.	0
26410	26410	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	5	120	0.096153846153846	0.526315789473684	The baseline is not shown on the official leaderboard but in Table 3.	0
26411	26411	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	6	121	0.115384615384615	0.530701754385965	We received 27 submissions to Subtask-1.	0
26412	26412	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	7	122	0.134615384615385	0.535087719298246	Table 3 shows all the official submission results and nearly all of them exceed the performance of the provided baseline model.	0
26413	26413	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	8	123	0.153846153846154	0.539473684210526	The top-ranked submissions all use pre-trained neural models, which have achieved the state-of-the-art results across many natural language processing (NLP) tasks (Devlin et al., 2018;	0
26414	26414	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	9	124	0.173076923076923	0.543859649122807	Radford et al., 2018;Radford et al., 2019;	0
26415	26415	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	10	125	0.192307692307692	0.548245614035088	Yang et al., 2019;	0
26416	26416	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	11	126	0.211538461538462	0.552631578947368	Liu et al., 2019;	0
26417	26417	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	12	127	0.230769230769231	0.557017543859649	Lan et al., 2019), which we believe is an effective way to integrate additional external knowledge that does not exist in the training data, including common sense.	0
26418	26418	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	13	128	0.25	0.56140350877193	Some participants like shngt experimented with classic machine learning methods like SVM and gradient boosted random forests, and found that model performance plateaued at an F1 score of around 60 percent (Anil Ojha et al., 2020), showing that these methods cannot capture counterfactual reasoning as well as the pre-trained models.	0
26419	26419	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	14	129	0.269230769230769	0.56578947368421	One team, Serena, use a non-transformer approach, basing their system on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and a Pooling operation is done for dimensionality reduction (Ou et al., 2020).	0
26420	26420	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	15	130	0.288461538461538	0.570175438596491	Their system struggles with data imbalance and they conclude that transformer networks can improve their performance.	0
26421	26421	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	16	131	0.307692307692308	0.574561403508772	Among the top 8 competitors, BERT and RoBERTa based systems are most popular, being used as the primary models or as a part of their final ensemble in 5 and 4 of the top 8 participants' systems respectively.	0
26422	26422	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	17	132	0.326923076923077	0.578947368421053	XLNet and ALBERT are less popular choices, but they are also used in the first and second of the top 8 participating systems respectively.	0
26423	26423	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	18	133	0.346153846153846	0.583333333333333	In addition, the top models adopt ensemble strategies, and in most cases, achieve better performance than that of individual classifiers.	0
26424	26424	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	19	134	0.365384615384615	0.587719298245614	One of the teams, shgnt, also found using a convolutional neural network model with GloVe embeddings in their ensemble helped to enhance it (Anil Ojha et al., 2020), showing that non-transformer network based methods can be useful.	0
26425	26425	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	20	135	0.384615384615385	0.592105263157895	Despite the commonality of using pre-trained models, many of the participating systems differ in the structures they add above the pre-trained models, to capture additional information.	0
26426	26426	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	21	136	0.403846153846154	0.596491228070175	Rather than adding a fully connected layer on top, some competitors reconstruct the top structure of the pre-trained models or add a neural network on top.	0
26427	26427	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	22	137	0.423076923076923	0.600877192982456	For example, to capture local patterns in counterfactual statements, Roger (Lu et al., 2020) and shngt (Anil Ojha et al., 2020) add a CNN before classification in some of their systems.	0
26428	26428	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	23	138	0.442307692307692	0.605263157894737	Similarly, Baiyang2581 experiments with this upper structure and use a bidirectional GRU and bidirectional LSTM in some of their systems after the transformer network (Bai and Zhou, 2020).	0
26429	26429	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	24	139	0.461538461538462	0.609649122807018	In contrast to modifying the upper structure of transformer networks, the fifth-place team, lenyabloko, uses rule-based specialist modules by combining fine-tuned pre-trained models with constituency and dependency parsers to compensate for deficiencies in deep learning methods for causal inference in language to great effect (Yabloko, 2020).	0
26430	26430	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	25	140	0.480769230769231	0.614035087719298	The dataset is highly imbalanced in favour of non-counterfactuals, and many participants use techniques to deal with this imbalance.	0
26431	26431	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	26	141	0.5	0.618421052631579	A range of techniques like pseudo-labelling used by haodingkui (Xiao et al., 2020), multi sample dropout used by Ferryman (Chen et al., 2020), and oversampling and undersampling used by some other teams, notably ad6398 who found that undersampling non-counterfactuals optimized the performance of their models . changshivek experimented with 2 novel forms of data augmentation to increase the number of counterfactual samples (Liu and Yu, 2020).	0
26432	26432	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	27	142	0.519230769230769	0.62280701754386	The first was back translation, which involves taking counterfactual samples and translating them into another language and then translating them back in English and adding them to the dataset.	0
26433	26433	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	28	143	0.538461538461538	0.62719298245614	"The second technique was Easy Data Augmentation (EDA), namely synonym replacement of words in the counterfactual samples while making sure to preserve words relating to counterfactuals like ""should""."	0
26434	26434	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	29	144	0.557692307692308	0.631578947368421	Back translation yielded poor performance, but decent improvement was seen when using EDA showing that this could be a viable method.	0
26435	26435	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	30	145	0.576923076923077	0.635964912280702	These methods are all done to combat overfitting.	0
26436	26436	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	31	146	0.596153846153846	0.640350877192982	K-fold cross validation is also a common strategy utilized by many of the participating systems to deal with the relatively small, highly imbalanced dataset to reduce some of the bias.	0
26437	26437	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	32	147	0.615384615384615	0.644736842105263	Lastly, many teams experimented with pre-processing the data.	0
26438	26438	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	33	148	0.634615384615385	0.649122807017544	Baiyang251 notes that minimal preprocessing (e.g. deleting punctuation, making sentences all lower-case) on the data yields the best results as they theorize removing these results in the loss of information useful for prediction (Bai and Zhou, 2020).	0
26439	26439	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	34	149	0.653846153846154	0.653508771929825	Other groups note that extensive pre-processing does not yield notable performance improvements either and can even slightly hurt performance.	0
26440	26440	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	35	150	0.673076923076923	0.657894736842105	The best F1 score in Subtask-1, 90.9%, was achieved by haodingkui (Xiao et al., 2020).	0
26441	26441	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	36	151	0.692307692307692	0.662280701754386	In their approach, a pseudo-labelling strategy is used to generate more data to alleviate overfitting during training.	0
26442	26442	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	37	152	0.711538461538462	0.666666666666667	In this strategy, if all classifiers agree on the labels of certain samples in the test set, then those samples will also be used for training.	0
26443	26443	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	38	153	0.730769230769231	0.671052631578947	For model ensembling, they incorporate BERT, RoBERTa, and XLNet.	0
26444	26444	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	39	154	0.75	0.675438596491228	The second-place system from josefjon concludes that using an ensemble of RoBERTa large models performs better than any other pre-trained model (Fajcik et al., 2020).	0
26445	26445	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	40	155	0.769230769230769	0.679824561403509	The third-place system, proposed by Roger, incorporates convolutional neural networks to capture strong local context information in addition to fine-tuning pre-trained models.	0
26446	26446	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	41	156	0.788461538461538	0.68421052631579	Furthermore, it theorizes about the effectiveness of using knowledge-enriched transformers to improve performance on the task (Lu et al., 2020).	0
26447	26447	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	42	157	0.807692307692308	0.68859649122807	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural nets with symbolic approaches.	0
26448	26448	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	43	158	0.826923076923077	0.692982456140351	Further Analysis and Challenges	0
26449	26449	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	44	159	0.846153846153846	0.697368421052632	In general, one of the main challenges of Subtask-1 is that identifying counterfactuals requires inference and reasoning based on common sense and knowledge.	0
26450	26450	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	45	160	0.865384615384615	0.701754385964912	Particularly, the fact that counterfactuals often do not follow specific grammatical rules makes such an ability important for some statements.	0
26451	26451	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	46	161	0.884615384615385	0.706140350877193	The imbalanced nature of the dataset in Subtask-1 is another challenge; therefore different methods have been proposed to address this issue such as over-sampling and under-sampling.	0
26452	26452	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	47	162	0.903846153846154	0.710526315789474	Some of the top models also try different methods of data augmentation so that they can have more positive examples to tackle the imbalance issue.	0
26453	26453	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	48	163	0.923076923076923	0.714912280701754	By inspecting more details of submitted predictions of top systems, we found most of the wrongly classified samples require systems to understand the statement better while the existing models often lean toward memorizing and overweighting token level features to make predictions.	0
26454	26454	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	49	164	0.942307692307692	0.719298245614035	"Take a counterfactual sentence as an example, ""if I were asked to, I would be happy to talk to anyone""."	0
26455	26455	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	50	165	0.961538461538462	0.723684210526316	"This is misclassified likely because of including ""were...to"" in the antecedent, which is highly correlated to non-counterfactual statements, suggesting a major flaw of existing methods."	0
26456	26456	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	51	166	0.980769230769231	0.728070175438597	Similarly, some non-counterfactual sentences are incorrectly labelled for they include some token-level counterfactual features while not indicating counterfactuals.	0
26457	26457	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	52	167	1.0	0.732456140350877	"For example, the sentence ""under the current alignment, he said, American multinational corporations like Pfizer might invest more money in the United States, not less, if they had their tax domiciles abroad"" is a non-counterfactual sentence for it is not assuming anything counter to the facts, while ""if had"" part along with the modal verb (""might"" in this case) in the same sentence is usually correlated to a counterfactual."	0
26458	26458	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	1	168	0.04	0.736842105263158	We build a conditional random field (CRF) model for sequence labeling as the baseline model for Subtask-2.	0
26459	26459	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	2	169	0.08	0.741228070175439	This model can assign labels to each token in the input sequence by taking advantage of all input tokens and previous predictions.	0
26460	26460	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	3	170	0.12	0.745614035087719	Specifically, same as in many name entity recognition systems, this baseline model annotates the antecedent and consequent using the B/I/O scheme, marking whether a word is at the Beginning, Inside or Outside either the antecedent or consequent.	0
26461	26461	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	4	171	0.16	0.75	A common set of features for each word are extracted and used to train this model, including POS tags, features of nearby words, and whether the word has an uppercase/lowercase/title flag.	0
26462	26462	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	5	172	0.2	0.754385964912281	The performance of the CRF baseline can be found in Table 4.	0
26463	26463	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	6	173	0.24	0.758771929824561	The performance of the baseline model and submitted systems are shown in Table 4.	0
26464	26464	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	7	174	0.28	0.763157894736842	We received 11 official submissions to Subtask-2, and most of the submissions outperform the provided baseline model.	0
26465	26465	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	8	175	0.32	0.767543859649123	In subtask 2, top systems take two main approaches: sequence labelling or question answering, and nearly all of them benefit from pretraining.	0
26466	26466	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	9	176	0.36	0.771929824561403	An exception is the 6th ranked team, Anderson Sung, that use a multi-stack, birdirectional LSTM architecture to some success (Sung et al., 2020), showing that non-transformer approaches are viable for the task.	0
26467	26467	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	10	177	0.4	0.776315789473684	Similarly, habi-akl experiments with a BiLSTM Conditional Random Fields (CRF) model for Subtask-2, but find that a BERT based model with a multilayer perceptron classifier outperforms the LSTM and conclude that the semi-supervised systems show a better level of understanding of challenging counterfactual forms (Abi Akl et al., 2020).	0
26468	26468	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	11	178	0.44	0.780701754385965	One approach among the top models formulates the problem as an extractive question answering (QA) task, with the target being extracting the answer from the given context towards a specific question.	0
26469	26469	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	12	179	0.48	0.785087719298246	The others formulate the task as a sequence labeling task.	0
26470	26470	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	13	180	0.52	0.789473684210526	In the top 4 systems, half of the teams took the QA approach, and the other half took the sequence labelling approach.	0
26471	26471	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	14	181	0.56	0.793859649122807	The choice between BERT and RoBERTa has split almost evenly amongst most of the participants.	0
26472	26472	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	15	182	0.6	0.798245614035088	As is the case for Subtask-1, many teams sought to build on top of the pre-trained models and add additional upper layer structures to handle the task better.	0
26473	26473	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	16	183	0.64	0.802631578947369	The best results are achieved by team Martin, with an F1 score of 88.2 and an exact match score of 57.5 (Fajcik et al., 2020).	0
26474	26474	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	17	184	0.68	0.807017543859649	To predict the start and ending positions of antecedents and consequents, the model utilizes an ensemble of RoBERTa models and extend it in the same manner as how BERT was extended for the SQuAD dataset (Rajpurkar et al., 2016).	0
26475	26475	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	18	185	0.72	0.81140350877193	The second-place system pouria babvey uses a sequence labelling approach: the authors develop the model on top of BERT with a multi-head attention layer and label masking to capture mutual information between nearby labels (Babvey et al., 2020).	0
26476	26476	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	19	186	0.76	0.81578947368421	Label masking, in which only part of the labels is fed during training and the rest have to be predicted, has shown to be particularly effective for improving accuracy, which can be seen as a form of regularization.	0
26477	26477	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	20	187	0.8	0.820175438596491	In addition, a multi-stage algorithm is used to gradually improve certainty in predictions after each step.	0
26478	26478	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	21	188	0.84	0.824561403508772	The third-place system, Roger, formulates the problem as a query-based question answering problem, where antecedents and consequents are extracted after an antecedent and consequent query are supplied along with the original statement into BERT.	0
26479	26479	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	22	189	0.88	0.828947368421053	Pointer networks are further used to predict the start and ending positions (Lu et al., 2020) .	0
26480	26480	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	23	190	0.92	0.833333333333333	A unique approach for Subtask-2 is used by 7th placed team, rajaswa patil, where they use a base architecture for both subtasks.	0
26481	26481	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	24	191	0.96	0.837719298245614	They first train with a binary-classification module for Subtask-1, then replace it with a regression-module and further fine-tune the system for Subtask-2 (Patil and Baths, 2020), leveraging the commonality between the two tasks.	0
26482	26482	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	25	192	1.0	0.842105263157895	We can observe that there is still a gap between the performance of exact match and F1, which is mainly due to the fact that Exact Match is sensitive to non-essential phrases in predictions even the core parts are identified correctly.	0
26483	26483	S20-5	Related Work	1	193	0.0625	0.846491228070175	Modelling counterfactual thinking has started to attract more interest.	0
26484	26484	S20-5	Related Work	2	194	0.125	0.850877192982456	One of the previous works closest to ours is (Son et al., 2017), in which a small-scale counterfactual tweet dataset is collected from social media.	0
26485	26485	S20-5	Related Work	3	195	0.1875	0.855263157894737	There are three main differences between that dataset and ours.	0
26486	26486	S20-5	Related Work	4	196	0.25	0.859649122807017	First, there are only 2,000 samples in the tweet dataset (including the supplement data mentioned in the paper), while our dataset for counterfactual detection in Subtask-1 is ten times larger, which we believe is important for training deep learning based models.	0
26487	26487	S20-5	Related Work	5	197	0.3125	0.864035087719298	Second, our benchmark provides evaluation for antecedents and consequents extraction, which are essential components of counterfactual analysis.	0
26488	26488	S20-5	Related Work	6	198	0.375	0.868421052631579	Third, our dataset includes statements from three different domains (finance, politics, healthcare).	0
26489	26489	S20-5	Related Work	7	199	0.4375	0.87280701754386	In contrast to the statements collected from tweets, which have a very large portion that are open-ended, vague thoughts, the counterfactuals in our dataset are more meaningful domain-related statements.	0
26490	26490	S20-5	Related Work	8	200	0.5	0.87719298245614	There is another dataset TIMETRAVEL proposed in (Qin et al., 2019)  in which given a short story and an alternative counterfactual event context, the story needs to be minimally revised to keep compatible with the intervening counterfactual event.	0
26491	26491	S20-5	Related Work	9	201	0.5625	0.881578947368421	The empirical results show that it is still challenging for current neural language models to perform well on the counterfactual story rewriting task due to the lack of counterfactual reasoning capabilities.	0
26492	26492	S20-5	Related Work	10	202	0.625	0.885964912280702	In a broader viewpoint, counterfatuals are an important form of causal reasoning.	0
26493	26493	S20-5	Related Work	11	203	0.6875	0.890350877192982	Researchers argue that the notion of counterfactuals is essential for causal reasoning, in which causal modeling is proposed to interpret counterfactual conditionals in natural language, and such work has been discussed since the possible worlds semantics developed in the 1970s (Lewis, 2013;	0
26494	26494	S20-5	Related Work	12	204	0.75	0.894736842105263	Lewis, 1986).	0
26495	26495	S20-5	Related Work	13	205	0.8125	0.899122807017544	The more recent work renders useful insights by formulating causal inference as a three-level hierarchy, which are association, intervention, and counterfactual, respectively (Pearl and Mackenzie, 2018;Pearl, 2019).	0
26496	26496	S20-5	Related Work	14	206	0.875	0.903508771929825	"The top of the hierarchy is counterfactual-if a model can correctly answer counterfactual queries like ""what would happen if we had acted differently"", it should also be able to answer association and intervention queries."	0
26497	26497	S20-5	Related Work	15	207	0.9375	0.907894736842105	The research in (Pearl, 1995;	0
26498	26498	S20-5	Related Work	16	208	1.0	0.912280701754386	Pearl, 2010) also made contributions to a general theory of causal inference, which is based on the Structural Causal Model (SCM), and counterfactual analysis is provided with a formal mathematical formalism.	0
26499	26499	S20-5	Summary and Future Work	1	209	0.125	0.916666666666667	We present a counterfactual recognition task that includes two basic subtasks.	0
26500	26500	S20-5	Summary and Future Work	2	210	0.25	0.921052631578947	Subtask-1 evaluates whether a given statement is counterfactual or not with 20,000 training and test statements.	0
26501	26501	S20-5	Summary and Future Work	3	211	0.375	0.925438596491228	Subtask-2 aims at recognizing antecedents and consequents in counterfactual statements.	0
26502	26502	S20-5	Summary and Future Work	4	212	0.5	0.929824561403509	The official task received 27 submissions to Subtask-1 and 11 submissions for Subtask-2.	0
26503	26503	S20-5	Summary and Future Work	5	213	0.625	0.934210526315789	The state-of-the-art performances achieved a 90% F1 score in Subtask 1, as well as an 88.2% F1 and 57.5% Exact Match score in Subtask-2.	0
26504	26504	S20-5	Summary and Future Work	6	214	0.75	0.93859649122807	We hope this task and dataset will intrigue and facilitate further research on counterfactual analysis in natural language.	0
26505	26505	S20-5	Summary and Future Work	7	215	0.875	0.942982456140351	4. Wish/Should Implied: The Wish/Should Implied counterfactual form only explicitly contains an antecedent in the sentence, with the consequent being implied, and it must contain an independent clause following a wish or should (Janocko et al., 2016).	0
26506	26506	S20-5	Summary and Future Work	8	216	1.0	0.947368421052632	To capture this form, sentences that contain the token 'wish' and that have a word after this with a past tense verb or a past participle verb tag.	0
26507	26507	S20-5	Verb Inversion:	1	217	0.083333333333333	0.951754385964912	The category has two specific forms that differ in if the antecedent presents before or after the consequent.	0
26508	26508	S20-5	Verb Inversion:	2	218	0.166666666666667	0.956140350877193	In either case, according to (Janocko et al., 2016), the antecedent contains a had or were inversion along with a past tense verb, and the consequent has a modal verb and a past or present tense verb.	0
26509	26509	S20-5	Verb Inversion:	3	219	0.25	0.960526315789474	(a)	0
26510	26510	S20-5	Verb Inversion:	4	220	0.333333333333333	0.964912280701754	The antecedent presents first in this case.	0
26511	26511	S20-5	Verb Inversion:	5	221	0.416666666666667	0.969298245614035	Thus, for this form the sentence first has to contain had or were as the first token.	0
26512	26512	S20-5	Verb Inversion:	6	222	0.5	0.973684210526316	After this, a token with a past tense or past participle verb token must be present, but only if the first token is had.	0
26513	26513	S20-5	Verb Inversion:	7	223	0.583333333333333	0.978070175438596	In either case, a word with a modal verb tag has to follow, and then further followed by a base verb, present non third person singular verb, present third person singular verb, or a past tense verb tag.	0
26514	26514	S20-5	Verb Inversion:	8	224	0.666666666666667	0.982456140350877	(b)	0
26515	26515	S20-5	Verb Inversion:	9	225	0.75	0.986842105263158	In this form a consequent presents first.	0
26516	26516	S20-5	Verb Inversion:	10	226	0.833333333333333	0.991228070175439	As a result, a word with a modal verb tag must follow, and is in turn followed by a word with base verb, present non third person singular verb, present third person singular verb, or a past tense verb tag.	0
26517	26517	S20-5	Verb Inversion:	11	227	0.916666666666667	0.995614035087719	After this the sentence had to contain a had or were token.	0
26518	26518	S20-5	Verb Inversion:	12	228	1.0	1.0	If it does contain a had token, an additional past tense or past participle verb token word has to also follow it.	0
27902	27902	S20-12	title	1	1	0.5	0.004444444444444	SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offens	0
27903	27903	S20-12	title	2	2	1.0	0.008888888888889	Eval 2020)	0
27904	27904	S20-12	abstract	1	3	0.25	0.013333333333333	We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020).	0
27905	27905	S20-12	abstract	2	4	0.5	0.017777777777778	The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish.	0
27906	27906	S20-12	abstract	3	5	0.75	0.022222222222222	Offens	0
27907	27907	S20-12	abstract	4	6	1.0	0.026666666666667	Eval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.	0
27908	27908	S20-12	Introduction	1	7	0.027777777777778	0.031111111111111	Offensive language is ubiquitous in social media platforms such as Facebook, Twitter, and Reddit, and it comes in many forms.	0
27909	27909	S20-12	Introduction	2	8	0.055555555555556	0.035555555555556	Given the multitude of terms and definitions related to offensive language used in the literature, several recent studies have investigated the common aspects of different abusive language detection tasks (Waseem et al., 2017;	0
27910	27910	S20-12	Introduction	3	9	0.083333333333333	0.04	Wiegand et al., 2018).	0
27911	27911	S20-12	Introduction	4	10	0.111111111111111	0.044444444444445	One such example is SemEval-2019 Task 6: Offens	0
27912	27912	S20-12	Introduction	5	11	0.138888888888889	0.048888888888889	Eval 1 (Zampieri et al., 2019b), which is the precursor to the present shared task.	0
27913	27913	S20-12	Introduction	6	12	0.166666666666667	0.053333333333333	Offens	0
27914	27914	S20-12	Introduction	7	13	0.194444444444444	0.057777777777778	Eval-2019 used the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets annotated using a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account (Zampieri et al., 2019a).	0
27915	27915	S20-12	Introduction	8	14	0.222222222222222	0.062222222222222	The assumption behind this annotation schema is that the target of offensive messages is an important variable that allows us to discriminate between, e.g., hate speech, which often consists of insults targeted toward a group, and cyberbullying, which typically targets individuals.	0
27916	27916	S20-12	Introduction	9	15	0.25	0.066666666666667	A number of recently organized related shared tasks followed similar hierarchical models.	0
27917	27917	S20-12	Introduction	10	16	0.277777777777778	0.071111111111111	Examples include HASOC-2019 (Mandl et al., 2019) for English, German, and Hindi, HatEval-2019 (Basile et al., 2019) for English and Spanish, GermEval-2019 for German (Struß et al., 2019), and TRAC-2020 (Kumar et al., 2020) for English, Bengali, and Hindi.	0
27918	27918	S20-12	Introduction	11	17	0.305555555555556	0.075555555555556	Offens	0
27919	27919	S20-12	Introduction	12	18	0.333333333333333	0.08	Eval-2019 attracted nearly 800 team registrations and received 115 official submissions, which demonstrates the interest of the research community in this topic.	0
27920	27920	S20-12	Introduction	13	19	0.361111111111111	0.084444444444445	Therefore, we organized a follow-up, OffensEval-2020 2 (SemEval-2020 Task 12), which is described in this report, building on the success of OffensEval-2019 with several improvements.	0
27921	27921	S20-12	Introduction	14	20	0.388888888888889	0.088888888888889	In particular, we used the same three-level taxonomy to annotate new datasets in five languages, where each level in this taxonomy corresponds to a subtask in the competition:	0
27922	27922	S20-12	Introduction	15	21	0.416666666666667	0.093333333333333	• Subtask A: Offensive language identification;	0
27923	27923	S20-12	Introduction	16	22	0.444444444444444	0.097777777777778	• Subtask B: Automatic categorization of offense types;	0
27924	27924	S20-12	Introduction	17	23	0.472222222222222	0.102222222222222	• Subtask C: Offense target identification.	0
27925	27925	S20-12	Introduction	18	24	0.5	0.106666666666667	This work is licensed under a Creative Commons Attribution 4.0 International License.	0
27926	27926	S20-12	Introduction	19	25	0.527777777777778	0.111111111111111	License details: http: //creativecommons.org/licenses/by/4.0/.	0
27927	27927	S20-12	Introduction	20	26	0.555555555555556	0.115555555555556	1 http://sites.google.com/site/offensevalsharedtask/offenseval2019 2 http://sites.google.com/site/offensevalsharedtask/home	0
27928	27928	S20-12	Introduction	21	27	0.583333333333333	0.12	The contributions of OffensEval-2020 can be summarized as follows:	0
27929	27929	S20-12	Introduction	22	28	0.611111111111111	0.124444444444444	•	0
27930	27930	S20-12	Introduction	23	29	0.638888888888889	0.128888888888889	We provided the participants with a new, large-scale semi-supervised training dataset containing over nine million English tweets (Rosenthal et al., 2020).	0
27931	27931	S20-12	Introduction	24	30	0.666666666666667	0.133333333333333	•	0
27932	27932	S20-12	Introduction	25	31	0.694444444444444	0.137777777777778	We introduced multilingual datasets, and we expanded the task to four new languages: Arabic (Mubarak et al., 2020b), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Turkish (Çöltekin, 2020).	0
27933	27933	S20-12	Introduction	26	32	0.722222222222222	0.142222222222222	This opens the possibility for cross-lingual training and analysis, which several participants indeed explored.	0
27934	27934	S20-12	Introduction	27	33	0.75	0.146666666666667	• Compared to OffensEval-2019, we used larger test datasets for all subtasks.	0
27935	27935	S20-12	Introduction	28	34	0.777777777777778	0.151111111111111	Overall, Offens	0
27936	27936	S20-12	Introduction	29	35	0.805555555555556	0.155555555555556	Eval-2020 was a very successful task.	0
27937	27937	S20-12	Introduction	30	36	0.833333333333333	0.16	The huge interest demonstrated last year continued this year, with 528 teams signing up to participate in the task, and 145 of them submitting official runs on the test dataset.	0
27938	27938	S20-12	Introduction	31	37	0.861111111111111	0.164444444444444	Furthermore, Offens	0
27939	27939	S20-12	Introduction	32	38	0.888888888888889	0.168888888888889	Eval-2020 received 70 system description papers, which is an all-time record for a SemEval task.	0
27940	27940	S20-12	Introduction	33	39	0.916666666666667	0.173333333333333	The remainder of this paper is organized as follows: Section 2 describes the annotation schema.	0
27941	27941	S20-12	Introduction	34	40	0.944444444444444	0.177777777777778	Section 3 presents the five datasets that we used in the competition.	0
27942	27942	S20-12	Introduction	35	41	0.972222222222222	0.182222222222222	Sections 4-9 present the results and discuss the approaches taken by the participating systems for each of the five languages.	0
27943	27943	S20-12	Introduction	36	42	1.0	0.186666666666667	Finally, Section 10 concludes and suggests some possible directions for future work.	0
27944	27944	S20-12	Annotation Schema	1	43	0.5	0.191111111111111	OLID's annotation schema proposes a hierarchical modeling of offensive language.	0
27945	27945	S20-12	Annotation Schema	2	44	1.0	0.195555555555556	It classifies each example using the following three-level hierarchy:	0
27946	27946	S20-12	Level A -Offensive Language Detection	1	45	0.333333333333333	0.2	Is the text offensive (OFF) or not offesive (NOT)?	0
27947	27947	S20-12	Level A -Offensive Language Detection	2	46	0.666666666666667	0.204444444444444	NOT: text that is neither offensive, nor profane;	0
27948	27948	S20-12	Level A -Offensive Language Detection	3	47	1.0	0.208888888888889	OFF: text containing inappropriate language, insults, or threats.	0
27949	27949	S20-12	Level B -Categorization of Offensive Language	1	48	0.5	0.213333333333333	Is the offensive text targeted (TIN) or untargeted (UNT)?	0
27950	27950	S20-12	Level B -Categorization of Offensive Language	2	49	1.0	0.217777777777778	TIN: targeted insults or threats towards a group or an individual; UNT: untargeted profanity or swearing.	0
27951	27951	S20-12	Level C -Offensive Language Target Identification	1	50	0.333333333333333	0.222222222222222	Who or what is the target of the offensive content?	0
27952	27952	S20-12	Level C -Offensive Language Target Identification	2	51	0.666666666666667	0.226666666666667	IND: the target is an individual, which can be explicitly mentioned or it can be implicit; GRP: the target is a group of people based on ethnicity, gender, sexual orientation, religious belief, or other common characteristic;	0
27953	27953	S20-12	Level C -Offensive Language Target Identification	3	52	1.0	0.231111111111111	OTH: the target does not fall into any of the previous categories, e.g., organizations, events, and issues.	0
27954	27954	S20-12	Data	1	53	0.023255813953488	0.235555555555556	In this section, we describe the datasets for all five languages: Arabic, Danish, English, Greek, and Turkish.	0
27955	27955	S20-12	Data	2	54	0.046511627906977	0.24	All of the languages follow the OLID annotation schema and all datasets were pre-processed in the same way, e.g., all user mentions were substituted by @USER for anonymization.	0
27956	27956	S20-12	Data	3	55	0.069767441860465	0.244444444444444	The introduction of new languages using a standardized schema with the purpose of detecting offensive and targeted speech should improve dataset consistency.	0
27957	27957	S20-12	Data	4	56	0.093023255813954	0.248888888888889	This strategy is in line with current best practices in abusive language data collection (Vidgen and Derczynski, 2020).	0
27958	27958	S20-12	Data	5	57	0.116279069767442	0.253333333333333	All languages contain data for subtask A, and only English contains data for subtasks B and C.	0
27959	27959	S20-12	Data	6	58	0.13953488372093	0.257777777777778	The distribution of the data across categories for all languages for subtask A is shown in     English For English, we provided two datasets: OLID from OffensEval-2019 (Zampieri et al., 2019a), and SOLID, which is a new dataset we created for the task (Rosenthal et al., 2020).	0
27960	27960	S20-12	Data	7	59	0.162790697674419	0.262222222222222	SOLID is an abbreviation for Semi-Supervised Offensive Language Identification Dataset, and it contains 9,089,140 English tweets, which makes it the largest dataset of its kind.	0
27961	27961	S20-12	Data	8	60	0.186046511627907	0.266666666666667	For SOLID, we collected random tweets using the 20 most common English stopwords such as the, of, and, to, etc.	0
27962	27962	S20-12	Data	9	61	0.209302325581395	0.271111111111111	Then, we labeled the collected tweets in a semi-supervised manner using democratic co-training, with OLID as a seed dataset.	0
27963	27963	S20-12	Data	10	62	0.232558139534884	0.275555555555556	For the co-training, we used four models with different inductive biases: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019).	0
27964	27964	S20-12	Data	11	63	0.255813953488372	0.28	We selected the OFF tweets for the test set using this semi-supervised process and we then annotated them manually for all subtasks.	0
27965	27965	S20-12	Data	12	64	0.27906976744186	0.284444444444444	We further added 2,500 NOT tweets using this process without further annotation.	0
27966	27966	S20-12	Data	13	65	0.302325581395349	0.288888888888889	We computed a Fleiss' κ Inter-Annotator Agreement (IAA) on a small subset of instances that were predicted to be OFF, and obtained 0.988 for Level A (almost perfect agreement), 0.818 for Level B (substantial agreement), and 0.630 for Level C (moderate agreement).	0
27967	27967	S20-12	Data	14	66	0.325581395348837	0.293333333333333	The annotation for Level C was more challenging as it is 3-way and also as sometimes there could be different types of targets mentioned in the offensive tweet, but the annotators were forced to choose only one label.	0
27968	27968	S20-12	Data	15	67	0.348837209302326	0.297777777777778	Arabic	0
27969	27969	S20-12	Data	16	68	0.372093023255814	0.302222222222222	The Arabic dataset consists of 10,000 tweets collected in April-May 2019 using the Twitter API with the language filter set to Arabic: lang:ar.	0
27970	27970	S20-12	Data	17	69	0.395348837209302	0.306666666666667	In order to increase the chance of having offensive content, only tweets with two or more vocative particles (yA in Arabic) were considered for annotation; the vocative particle is used mainly to direct the speech to a person or to a group, and it is widely observed in offensive communications in almost all Arabic dialects.	0
27971	27971	S20-12	Data	18	70	0.418604651162791	0.311111111111111	This yielded 20% offensive tweets in the final dataset.	0
27972	27972	S20-12	Data	19	71	0.441860465116279	0.315555555555555	The tweets were manually annotated (for Level A only) by a native speaker familiar with several Arabic dialects.	0
27973	27973	S20-12	Data	20	72	0.465116279069767	0.32	A random subsample of offensive and non-offensive tweets were doubly annotated and the Fleiss κ IAA was found to be 0.92.	0
27974	27974	S20-12	Data	21	73	0.488372093023256	0.324444444444444	More details can be found in (Mubarak et al., 2020b).	0
27975	27975	S20-12	Data	22	74	0.511627906976744	0.328888888888889	Danish	0
27976	27976	S20-12	Data	23	75	0.534883720930232	0.333333333333333	The Danish dataset consists of 3,600 comments drawn from Facebook, Reddit, and a local newspaper, Ekstra Bladet 3 .	0
27977	27977	S20-12	Data	24	76	0.558139534883721	0.337777777777778	The selection of the comments was partially seeded using abusive terms gathered during a crowd-sourced lexicon compilation; in order to ensure sufficient data diversity, this seeding was limited to half the data only.	0
27978	27978	S20-12	Data	25	77	0.581395348837209	0.342222222222222	The training data was not divided into distinct training/development splits, and participants were encouraged to perform cross-validation, as we wanted to avoid issues that fixed splits can cause (Gorman and Bedrick, 2019).	0
27979	27979	S20-12	Data	26	78	0.604651162790698	0.346666666666667	The annotation (for Level A only) was performed at the individual comment level by males aged 25-40.	0
27980	27980	S20-12	Data	27	79	0.627906976744186	0.351111111111111	A full description of the dataset and an accompanying data statement (Bender and Friedman, 2018) can be found in (Sigurbergsson and Derczynski, 2020).	0
27981	27981	S20-12	Data	28	80	0.651162790697674	0.355555555555556	Greek The Offensive Greek Twitter Dataset (OGTD) used in this task is a compilation of 10,287 tweets.	0
27982	27982	S20-12	Data	29	81	0.674418604651163	0.36	These tweets were sampled using popular and trending hashtags, including television programs such as series, reality and entertainment shows, along with some politically related tweets.	0
27983	27983	S20-12	Data	30	82	0.697674418604651	0.364444444444444	"Another portion of the dataset was fetched using pejorative terms and ""you are"" as keywords."	0
27984	27984	S20-12	Data	31	83	0.72093023255814	0.368888888888889	This particular strategy was adopted with the hypothesis that TV and politics would gather a handful of offensive posts, along with tweets containing vulgar language for further investigation.	0
27985	27985	S20-12	Data	32	84	0.744186046511628	0.373333333333333	A team of volunteer annotators participated in the annotation process (for Level A only), with each tweet being judged by three annotators.	0
27986	27986	S20-12	Data	33	85	0.767441860465116	0.377777777777778	In cases of disagreement, labels with majority agreement above 66% were selected as the actual tweet labels.	0
27987	27987	S20-12	Data	34	86	0.790697674418605	0.382222222222222	The IAA was 0.78 (using Fleiss' κ coefficient).	0
27988	27988	S20-12	Data	35	87	0.813953488372093	0.386666666666667	A full description of the dataset collection and annotation is detailed in (Pitenis et al., 2020).	0
27989	27989	S20-12	Data	36	88	0.837209302325581	0.391111111111111	Turkish	0
27990	27990	S20-12	Data	37	89	0.86046511627907	0.395555555555556	The Turkish dataset consists of over 35,000 tweets sampled uniformly from the Twitter stream and filtered using a list of the most frequent words in Turkish, as identified by Twitter.	0
27991	27991	S20-12	Data	38	90	0.883720930232558	0.4	The tweets were annotated by volunteers (for Level A only).	0
27992	27992	S20-12	Data	39	91	0.906976744186046	0.404444444444444	Most tweets were annotated by a single annotator.	0
27993	27993	S20-12	Data	40	92	0.930232558139535	0.408888888888889	The Cohen's κ IAA calculated on 5,000 doubly-annotated tweets was 0.761.	0
27994	27994	S20-12	Data	41	93	0.953488372093023	0.413333333333333	Note that we did not include any specific method for spotting offensive language, e.g., filtering by offensive words, or following usual targets of offensive language.	0
27995	27995	S20-12	Data	42	94	0.976744186046512	0.417777777777778	As a result, the distribution closely resembles the actual offensive language use on Twitter, with more non-offensive tweets than offensive tweets.	0
27996	27996	S20-12	Data	43	95	1.0	0.422222222222222	More details about the sampling and the annotation process can be found in (Çöltekin, 2020).	0
27997	27997	S20-12	Task Participation	1	96	0.071428571428572	0.426666666666667	A total of 528 teams signed up to participate in the task, and 145 of them submitted results: 6 teams made submissions for all five languages, 19 did so for four languages, 11 worked on three languages, 13 on two languages, and 96 focused on just one language.	0
27998	27998	S20-12	Task Participation	2	97	0.142857142857143	0.431111111111111	Tables 13, 14, and 15 show a summary of which team participated in which task.	0
27999	27999	S20-12	Task Participation	3	98	0.214285714285714	0.435555555555556	A total of 70 teams submitted system description papers, which are listed in Table 12.	0
28000	28000	S20-12	Task Participation	4	99	0.285714285714286	0.44	Below, we analyze the representation and the models used for all language tracks.	0
28001	28001	S20-12	Task Participation	5	100	0.357142857142857	0.444444444444444	Representation	0
28002	28002	S20-12	Task Participation	6	101	0.428571428571429	0.448888888888889	The vast majority of teams used some kind of pre-trained embeddings such as contextualized Transformers (Vaswani et al., 2017) and ELMo (Peters et al., 2018) embeddings.	0
28003	28003	S20-12	Task Participation	7	102	0.5	0.453333333333333	The most popular Transformers were BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and the multi-lingual mBERT (Devlin et al., 2019).	0
28004	28004	S20-12	Task Participation	8	103	0.571428571428571	0.457777777777778	4 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic.	0
28005	28005	S20-12	Task Participation	9	104	0.642857142857143	0.462222222222222	Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words.	0
28006	28006	S20-12	Task Participation	10	105	0.714285714285714	0.466666666666667	Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018).	0
28007	28007	S20-12	Task Participation	11	106	0.785714285714286	0.471111111111111	Machine learning models	0
28008	28008	S20-12	Task Participation	12	107	0.857142857142857	0.475555555555556	In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), AL-BERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular.	0
28009	28009	S20-12	Task Participation	13	108	0.928571428571429	0.48	Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014).	0
28010	28010	S20-12	Task Participation	14	109	1.0	0.484444444444444	Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles.	0
28011	28011	S20-12	English Track	1	110	0.090909090909091	0.488888888888889	A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of the task): 27 teams participated in all three English subtasks, 18 teams participated in two English subtasks, and 42 focused on one English subtask only.	0
28012	28012	S20-12	English Track	2	111	0.181818181818182	0.493333333333333	Pre-processing and normalization	0
28013	28013	S20-12	English Track	3	112	0.272727272727273	0.497777777777778	Most teams performed some kind of pre-processing (67 teams) or text normalization (26 teams), which are typical steps when working with tweets.	0
28014	28014	S20-12	English Track	4	113	0.363636363636364	0.502222222222222	Text normalization included various text transformations such as converting emojis to plain text, 5 segmenting hashtags, 6 general tweet text normalization (Satapathy et al., 2019), abbreviation expansion, bad word replacement, error correction, lowercasing, stemming, and/or lemmatization.	0
28015	28015	S20-12	English Track	5	114	0.454545454545455	0.506666666666667	Other techniques included the removal of @user mentions, URLs, hashtags, emojis, emails, dates, numbers, punctuation, consecutive character repetitions, offensive words, and/or stop words.	0
28016	28016	S20-12	English Track	6	115	0.545454545454545	0.511111111111111	Additional data	0
28017	28017	S20-12	English Track	7	116	0.636363636363636	0.515555555555556	Most teams found the weakly supervised SOLID dataset useful, and 58 teams ended up using it in their systems.	0
28018	28018	S20-12	English Track	8	117	0.727272727272727	0.52	Another six teams gave it a try, but could not benefit from it, and the remaining teams only used the manually annotated training data.	0
28019	28019	S20-12	English Track	9	118	0.818181818181818	0.524444444444444	Some teams used additional datasets from HASOC-2019 (Mandl et al., 2019), the Kaggle competitions on Detecting Insults in Social Commentary 7 and Toxic Comment Classification 8 , the TRAC-2018 shared task on Aggression Identification (Kumar et al., 2018a;	0
28020	28020	S20-12	English Track	10	119	0.909090909090909	0.528888888888889	Kumar et al., 2018b), the Wikipedia Detox dataset (Wulczyn et al., 2017), and the datasets from  and (Wulczyn et al., 2017), as well as some lexicons such as HurtLex (Bassignana et al., 2018) and Hatebase.	0
28021	28021	S20-12	English Track	11	120	1.0	0.533333333333333	9 Finally, one team created their own dataset.	0
28022	28022	S20-12	Subtask A	1	121	0.125	0.537777777777778	A total of 82 teams made submissions for subtask A, and the results can be seen in Table 5.	0
28023	28023	S20-12	Subtask A	2	122	0.25	0.542222222222222	This was the most popular subtask among all subtasks and across all languages.	0
28024	28024	S20-12	Subtask A	3	123	0.375	0.546666666666667	The best team UHH-LT achieved an F1 score of 0.9204 using an ensemble of ALBERT models of different sizes.	0
28025	28025	S20-12	Subtask A	4	124	0.5	0.551111111111111	The team ranked second was UHH-LT with an F1 score of 0.9204, and it used RoBERTa-large that was fine-tuned on the SOLID dataset in an unsupervised way, i.e., using the MLM objective.	0
28026	28026	S20-12	Subtask A	5	125	0.625	0.555555555555556	The third team, Galileo, achieved an F1 score of 0.9198, using an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large trained on the subtask A data for all languages.	0
28027	28027	S20-12	Subtask A	6	126	0.75	0.56	The top-10 teams used BERT, RoBERTa or XLM-RoBERTa, sometimes as part of ensembles that also included CNNs and LSTMs (Hochreiter and Schmidhuber, 1997).	0
28028	28028	S20-12	Subtask A	7	127	0.875	0.564444444444444	Overall, the competition for this subtask was very strong, and the scores are very close: the teams ranked 2-16 are within one point in the third decimal place, and those ranked 2-59 are within two absolute points in the second decimal place from the best team.	0
28029	28029	S20-12	Subtask A	8	128	1.0	0.568888888888889	All but one team beat the majority class baseline (we suspect that team might have accidentally flipped their predicted labels).	0
28030	28030	S20-12	Subtask B	1	129	0.125	0.573333333333333	A total of 41 teams made submissions for subtask B, and the results can be seen in Table 6.	0
28031	28031	S20-12	Subtask B	2	130	0.25	0.577777777777778	The best team is Galileo (which were third on subtask A), whose ensemble model achieved an F1 score of 0.7462.	0
28032	28032	S20-12	Subtask B	3	131	0.375	0.582222222222222	The second-place team, PGSG, used a complex teacher-student architecture built on top of a BERT-LSTM model, which was fine-tuned on the SOLID dataset in an unsupervised way, i.e., optimizing for the MLM objective.	0
28033	28033	S20-12	Subtask B	4	132	0.5	0.586666666666667	NTU NLP was ranked third with an F1 score of 0.6906.	0
28034	28034	S20-12	Subtask B	5	133	0.625	0.591111111111111	They tackled subtasks A, B, and C as part of a multi-task BERT-based model.	0
28035	28035	S20-12	Subtask B	6	134	0.75	0.595555555555556	Overall, the differences in the scores for subtask B are much larger than for subtask A.	0
28036	28036	S20-12	Subtask B	7	135	0.875	0.6	For example, the 4th team is two points behind the third one and seven points behind the first one.	0
28037	28037	S20-12	Subtask B	8	136	1.0	0.604444444444444	The top-ranking teams used BERT-based Transformer models, and all but four teams could improve over the majority class baseline.	0
28038	28038	S20-12	Subtask C	1	137	0.111111111111111	0.608888888888889	A total of 37 teams made submissions for subtask C and the results are shown in Table 7.	0
28039	28039	S20-12	Subtask C	2	138	0.222222222222222	0.613333333333333	The best team was once again Galileo, with an F1 score of 0.7145.	0
28040	28040	S20-12	Subtask C	3	139	0.333333333333333	0.617777777777778	LT@Helsinki was ranked second with an F1 score of 0.6700.	0
28041	28041	S20-12	Subtask C	4	140	0.444444444444444	0.622222222222222	They used fine-tuned BERT with oversampling to improve class imbalance.	0
28042	28042	S20-12	Subtask C	5	141	0.555555555555556	0.626666666666667	The third best system was PRHLT-UPV with an F1 score of 0.6692, which combines BERT with hand-crafted features; it is followed very closely by UHH-LT at rank 4, which achieved an F1 score of 0.6683.	0
28043	28043	S20-12	Subtask C	6	142	0.666666666666667	0.631111111111111	This subtask is also dominated by BERT-based models, and all teams outperformed the majority class baseline.	0
28044	28044	S20-12	Subtask C	7	143	0.777777777777778	0.635555555555556	Note that the absolute F1-scores obtained by the best teams in the English subtasks A and C are substantially higher than the scores obtained by the best teams in OffensEval-2019: 0.9223 vs. 0.8290 for subtask A and 0.7145 vs. 0.6600 for subtask C. This suggests that the much larger SOLID dataset made available in OffensEval-2020 helped the models make more accurate predictions.	0
28045	28045	S20-12	Subtask C	8	144	0.888888888888889	0.64	Furthermore, it suggests that the weakly supervised method used to compile and annotate SOLID is a viable alternative to popular purely manual annotation approaches.	0
28046	28046	S20-12	Subtask C	9	145	1.0	0.644444444444444	A more detailed analysis of the systems' performances will be carried out in order to determine the contribution of the SOLID dataset for the results.	0
28047	28047	S20-12	Best Systems	1	146	0.066666666666667	0.648888888888889	We provide some more details about the approaches used by the top teams for each subtask.	0
28048	28048	S20-12	Best Systems	2	147	0.133333333333333	0.653333333333333	We use subindices to show their rank for each subtask.	0
28049	28049	S20-12	Best Systems	3	148	0.2	0.657777777777778	Additional summaries for some of the best teams can be found in Appendix A.	0
28050	28050	S20-12	Best Systems	4	149	0.266666666666667	0.662222222222222	Galileo (A:3,B:1,C:1)	0
28051	28051	S20-12	Best Systems	5	150	0.333333333333333	0.666666666666667	This team was ranked 3rd, 1st, and 1st on the English subtasks A, B, and C, respectively.	0
28052	28052	S20-12	Best Systems	6	151	0.4	0.671111111111111	This is also the only team ranked among the top-3 across all languages.	0
28053	28053	S20-12	Best Systems	7	152	0.466666666666667	0.675555555555556	For subtask A, they used multi-lingual pre-trained Transformers based on XLM-RoBERTa, followed by multi-lingual fine-tuning using the OffensEval data.	0
28054	28054	S20-12	Best Systems	8	153	0.533333333333333	0.68	Ultimately, they submitted an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large, achieving an F1 score of 0.9198.	0
28055	28055	S20-12	Best Systems	9	154	0.6	0.684444444444444	For subtasks B and C, they used knowledge distillation in a teacher-student framework, using Transformers such as ALBERT and ERNIE 2.0  as teacher models, achieving an F1 score of 0.7462 and 0.7145, for subtasks B and C respectively.	0
28056	28056	S20-12	Best Systems	10	155	0.666666666666667	0.688888888888889	UHH-LT (A:1)	0
28057	28057	S20-12	Best Systems	11	156	0.733333333333333	0.693333333333333	This team was ranked 1st on subtask A with an F1 score of 0.9223.	0
28058	28058	S20-12	Best Systems	12	157	0.8	0.697777777777778	They fine-tuned different Transformer models on the OLID training data, and then combined them into an ensemble.	0
28059	28059	S20-12	Best Systems	13	158	0.866666666666667	0.702222222222222	They experimented with BERT-base and BERT-large (uncased), RoBERTa-base and RoBERTa-large, XLM-RoBERTa, and four different ALBERT models (large-v1, large-v2, xxlarge-v1, and xxlarge-v2).	0
28060	28060	S20-12	Best Systems	14	159	0.933333333333333	0.706666666666667	In their official submission, they used an ensemble combining different ALBERT models.	0
28061	28061	S20-12	Best Systems	15	160	1.0	0.711111111111111	They did not use the labels of the SOLID dataset, but found the tweets it contained nevertheless useful for unsupervised fine-tuning (i.e., using the MLM objective) of the pre-trained Transformers.	0
28062	28062	S20-12	Arabic Track	1	161	0.2	0.715555555555556	A total of 108 teams registered to participate in the Arabic track, and ultimately 53 teams entered the competition with at least one valid submission.	0
28063	28063	S20-12	Arabic Track	2	162	0.4	0.72	Among them, ten teams participated in the Arabic track only, while the rest participated in other languages in addition to Arabic.	0
28064	28064	S20-12	Arabic Track	3	163	0.6	0.724444444444444	This was the second shared task for Arabic after the one at the 4th workshop on Open-Source Arabic Corpora and Processing Tools (Mubarak et al., 2020a), which had different settings and less participating teams.	0
28065	28065	S20-12	Arabic Track	4	164	0.8	0.728888888888889	Pre-processing and normalization	0
28066	28066	S20-12	Arabic Track	5	165	1.0	0.733333333333333	Most teams performed some kind of pre-processing or text normalization, e.g., Hamza shapes, Alif Maqsoura, Taa Marbouta, diacritics, non-Arabic characters, etc., and only one team replaced emojis with their textual counter-parts.	0
28067	28067	S20-12	Results	1	166	0.111111111111111	0.737777777777778	Table 8 shows the teams and the F1 scores they achieved for the Arabic subtask A.	0
28068	28068	S20-12	Results	2	167	0.222222222222222	0.742222222222222	The majority class baseline had an F1 score of 0.4441, and several teams achieved results that doubled that baseline score.	0
28069	28069	S20-12	Results	3	168	0.333333333333333	0.746666666666667	The best-performing team was ALAMIHamza with an F1 score of 0.9017.	0
28070	28070	S20-12	Results	4	169	0.444444444444444	0.751111111111111	The second-best team, ALT, was almost tied with the winner, with an F1 score of 0.9016.	0
28071	28071	S20-12	Results	5	170	0.555555555555556	0.755555555555556	The Galileo team was third with an F1 score of 0.8989.	0
28072	28072	S20-12	Results	6	171	0.666666666666667	0.76	A summary of the approaches taken by the top-performing teams can be found in Appendix A; here we briefly describe the winning system:	0
28073	28073	S20-12	Results	7	172	0.777777777777778	0.764444444444445	ALAMIHamza(A:1)	0
28074	28074	S20-12	Results	8	173	0.888888888888889	0.768888888888889	The winning team achieved the highest F1-score using BERT to encode Arabic tweets, followed by a sigmoid classifier.	0
28075	28075	S20-12	Results	9	174	1.0	0.773333333333333	They further performed translation of the meaning of emojis.	0
28076	28076	S20-12	Danish Track	1	175	0.2	0.777777777777778	A total of 72 teams registered to participate in the Danish track, and 39 of them actually made official submissions on the test dataset.	0
28077	28077	S20-12	Danish Track	2	176	0.4	0.782222222222222	This is the first shared task on offensive language identification to include Danish, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Sigurbergsson and Derczynski, 2020).	0
28078	28078	S20-12	Danish Track	3	177	0.6	0.786666666666667	Pre-processing and normalization	0
28079	28079	S20-12	Danish Track	4	178	0.8	0.791111111111111	Many teams used the pre-processing included in the relevant embedding model, e.g., BPE (Heinzerling and Strube, 2018) and WordPiece.	0
28080	28080	S20-12	Danish Track	5	179	1.0	0.795555555555556	Other pre-processing techniques included emoji normalization, spelling correction, sentiment tagging, lexical and regex-based term and phrase flagging, and hashtag segmentation.	0
28081	28081	S20-12	Results	1	180	0.125	0.8	The results are shown in Table 9.	0
28082	28082	S20-12	Results	2	181	0.25	0.804444444444444	We can see that all teams managed to outperform the majority class baseline.	0
28083	28083	S20-12	Results	3	182	0.375	0.808888888888889	Moreover, all but one team improved over a FastText baseline (F1 = 0.5148), and most teams achieved an F1 score of 0.7 or higher.	0
28084	28084	S20-12	Results	4	183	0.5	0.813333333333333	Interestingly, one of the top-ranked teams, JCT, was entirely non-neural.	0
28085	28085	S20-12	Results	5	184	0.625	0.817777777777778	LT@Helsinki (A:1)	0
28086	28086	S20-12	Results	6	185	0.75	0.822222222222222	The winning team LT@Helsinki used NordicBERT for representation, as provided by BotXO. 10 NordicBERT is customized to Danish, and avoids some of the pre-processing noise and ambiguity introduced by other popular BERT implementations.	0
28087	28087	S20-12	Results	7	186	0.875	0.826666666666667	The team further reduced orthographic lengthening to maximum two repeated characters, converted emojis to sentiment scores, and used cooccurrences of hashtags and references to usernames.	0
28088	28088	S20-12	Results	8	187	1.0	0.831111111111111	They tuned the hyper-parameters of their model using 10-fold cross validation.	0
28089	28089	S20-12	Greek Track	1	188	0.2	0.835555555555556	A total of 71 teams registered to participate in the Greek track, and ultimately 37 of them made an official submission on the test dataset.	0
28090	28090	S20-12	Greek Track	2	189	0.4	0.84	This is the first shared task on offensive language identification to include Greek, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Pitenis et al., 2020).	0
28091	28091	S20-12	Greek Track	3	190	0.6	0.844444444444444	Pre-processing and normalization	0
28092	28092	S20-12	Greek Track	4	191	0.8	0.848888888888889	The participants experimented with various pre-processing and text normalization techniques, similarly to what was done for the other languages above.	0
28093	28093	S20-12	Greek Track	5	192	1.0	0.853333333333333	One team further reported replacement of emojis with their textual equivalent.	0
28094	28094	S20-12	Results	1	193	0.125	0.857777777777778	The evaluation results are shown in Table 10.	0
28095	28095	S20-12	Results	2	194	0.25	0.862222222222222	The top team, NLPDove, achieved an F1 score of 0.852, with Galileo coming close at the second place with an F1 score of 0.851.	0
28096	28096	S20-12	Results	3	195	0.375	0.866666666666667	The KS@LTH team was ranked third with an F1 score of 0.848.	0
28097	28097	S20-12	Results	4	196	0.5	0.871111111111111	It is no surprise that the majority of the high-ranking submissions and participants used large-scale pre-trained Transformers, with BERT being the most prominent among them, along with wordwvec-style non-contextualized pre-trained word embeddings.	0
28098	28098	S20-12	Results	5	197	0.625	0.875555555555555	NLP	0
28099	28099	S20-12	Results	6	198	0.75	0.88	Dove (A:1)	0
28100	28100	S20-12	Results	7	199	0.875	0.884444444444445	The winning team NLPDove used pre-trained word embeddings from mBERT, which they fine-tuned using the training data.	0
28101	28101	S20-12	Results	8	200	1.0	0.888888888888889	A domain-specific vocabulary was generated by running the WordPiece algorithm (Schuster and Nakajima, 2012) and using embeddings for extended vocabulary to pre-train and fine-tune the model.	0
28102	28102	S20-12	Turkish Track	1	201	0.333333333333333	0.893333333333333	A total of 86 teams registered to participate in the Turkish track, and ultimately 46 of them made an official submission on the test dataset.	0
28103	28103	S20-12	Turkish Track	2	202	0.666666666666667	0.897777777777778	All teams except for one participated in at least one other track.	0
28104	28104	S20-12	Turkish Track	3	203	1.0	0.902222222222222	This is the first shared task on offensive language identification to include Turkish, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Çöltekin, 2020).	0
28105	28105	S20-12	Results	1	204	0.142857142857143	0.906666666666667	The results are shown in Table 11.	0
28106	28106	S20-12	Results	2	205	0.285714285714286	0.911111111111111	We can see that team Galileo achieved the highest macro-averaged F1 score of 0.8258, followed by SU-NLP and KUI-SAIL with F1 scores of 0.8167 and 0.8141, respectively.	0
28107	28107	S20-12	Results	3	206	0.428571428571429	0.915555555555556	Note that the latter two teams are from Turkey, and they used some language-specific resources and tuning.	0
28108	28108	S20-12	Results	4	207	0.571428571428571	0.92	Most results were in the interval 0.7-0.8, and almost all teams managed to outperform the majority class baseline, which had an F1 score of 0.4435.	0
28109	28109	S20-12	Results	5	208	0.714285714285714	0.924444444444444	Galileo (A:1)	0
28110	28110	S20-12	Results	6	209	0.857142857142857	0.928888888888889	The best team in the Turkish subtask A was Galileo, which achieved top results in several other tracks.	0
28111	28111	S20-12	Results	7	210	1.0	0.933333333333333	Unlike the systems ranked second and third, Galileo's system is language-agnostic, and it used data for all five languages in a multi-lingual training setup.	0
28112	28112	S20-12	Conclusion and Future Work	1	211	0.066666666666667	0.937777777777778	We presented the results of OffensEval-2020, which featured datasets in five languages: Arabic, Danish, English, Greek, and Turkish.	0
28113	28113	S20-12	Conclusion and Future Work	2	212	0.133333333333333	0.942222222222222	For English, we had three subtasks, representing the three levels of the OLID hierarchy.	0
28114	28114	S20-12	Conclusion and Future Work	3	213	0.2	0.946666666666667	For the other four languages, we had a subtask for the top-level of the OLID hierarchy only.	0
28115	28115	S20-12	Conclusion and Future Work	4	214	0.266666666666667	0.951111111111111	A total of 528 teams signed up to participate in OffensEval-2020, and 145 of them actually submitted results across all languages and subtasks.	0
28116	28116	S20-12	Conclusion and Future Work	5	215	0.333333333333333	0.955555555555556	Out of the 145 participating teams, 96 teams participated in one language only, 13 teams participated in two languages, 11 in three languages, 19 in four languages, and 6 teams submitted systems for all five languages.	0
28117	28117	S20-12	Conclusion and Future Work	6	216	0.4	0.96	The official submissions per language ranged from 37 (for Greek) to 81 (for English).	0
28118	28118	S20-12	Conclusion and Future Work	7	217	0.466666666666667	0.964444444444444	Finally, 70 of the 145 participating teams submitted system description papers, which is an all-time record.	0
28119	28119	S20-12	Conclusion and Future Work	8	218	0.533333333333333	0.968888888888889	The wide participation in the task allowed us to compare a number of approaches across different languages and datasets.	0
28120	28120	S20-12	Conclusion and Future Work	9	219	0.6	0.973333333333333	Similarly to OffensEval-2019, we observed that the best systems for all languages and subtasks used large-scale BERT-style pre-trained Transformers such as BERT, RoBERTa, and mBERT.	0
28121	28121	S20-12	Conclusion and Future Work	10	220	0.666666666666667	0.977777777777778	Unlike 2019, however, the multi-lingual nature of this year's data enabled cross-language approaches, which proved quite effective and were used by some of the top-ranked systems.	0
28122	28122	S20-12	Conclusion and Future Work	11	221	0.733333333333333	0.982222222222222	In future work, we plan to extend the task in several ways.	0
28123	28123	S20-12	Conclusion and Future Work	12	222	0.8	0.986666666666667	First, we want to offer subtasks B and C for all five languages from OffensEval-2020.	0
28124	28124	S20-12	Conclusion and Future Work	13	223	0.866666666666667	0.991111111111111	We further plan to add some additional languages, especially under-represented ones.	0
28125	28125	S20-12	Conclusion and Future Work	14	224	0.933333333333333	0.995555555555555	Other interesting aspects to explore are code-mixing, e.g., mixing Arabic script and Latin alphabet in the same Arabic message, and code-switching, e.g., mixing Arabic and English words and phrases in the same message.	0
28126	28126	S20-12	Conclusion and Future Work	15	225	1.0	1.0	Last but not least, we plan to cover a wider variety of social media platforms.	0
30743	30743	2020.nlptea-1.4	title	1	1	1.0	0.008264462809917	Overview of NLPTEA-2020 Shared Task for Chinese Grammatical Error Diagnosis	0
30744	30744	2020.nlptea-1.4	abstract	1	2	0.2	0.016528925619835	This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language.	1
30745	30745	2020.nlptea-1.4	abstract	2	3	0.4	0.024793388429752	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
30746	30746	2020.nlptea-1.4	abstract	3	4	0.6	0.03305785123967	Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs.	0
30747	30747	2020.nlptea-1.4	abstract	4	5	0.8	0.041322314049587	System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level.	0
30748	30748	2020.nlptea-1.4	abstract	5	6	1.0	0.049586776859504	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
30749	30749	2020.nlptea-1.4	Introduction	1	7	0.045454545454546	0.057851239669422	Automated grammar checking for learners of English as a foreign language has achieved obvious progress.	0
30750	30750	2020.nlptea-1.4	Introduction	2	8	0.090909090909091	0.066115702479339	Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011;	0
30751	30751	2020.nlptea-1.4	Introduction	3	9	0.136363636363636	0.074380165289256	Dale et al., 2012).	0
30752	30752	2020.nlptea-1.4	Introduction	4	10	0.181818181818182	0.082644628099174	The shared tasks at CoNLL 2013 and 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013;.	0
30753	30753	2020.nlptea-1.4	Introduction	5	11	0.227272727272727	0.090909090909091	Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language (CFL) learners.	0
30754	30754	2020.nlptea-1.4	Introduction	6	12	0.272727272727273	0.099173553719008	Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012;	0
30755	30755	2020.nlptea-1.4	Introduction	7	13	0.318181818181818	0.107438016528926	Wu et al, 2010;	0
30756	30756	2020.nlptea-1.4	Introduction	8	14	0.363636363636364	0.115702479338843	Yu and Chen, 2012), rule-based analysis (Lee et al., 2013), neuro network modelling (Zheng et al., 2016;	0
30757	30757	2020.nlptea-1.4	Introduction	9	15	0.409090909090909	0.12396694214876	Fu et al., 2018) and hybrid methods Zhou et al., 2017).	0
30758	30758	2020.nlptea-1.4	Introduction	10	16	0.454545454545455	0.132231404958678	In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL .	0
30759	30759	2020.nlptea-1.4	Introduction	11	17	0.5	0.140495867768595	A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015), COLING-2016.	0
30760	30760	2020.nlptea-1.4	Introduction	12	18	0.545454545454545	0.148760330578512	Its name was fixed from then on: Chinese Grammatical Error Diagnosis (CGED).	0
30761	30761	2020.nlptea-1.4	Introduction	13	19	0.590909090909091	0.15702479338843	As a part of IJCNLP 2017, the shared task was organized (Rao et al., 2017).	0
30762	30762	2020.nlptea-1.4	Introduction	14	20	0.636363636363636	0.165289256198347	In conjunction with NLP-TEA workshop in ACL 2018, CGED was organized again (Rao et al., 2018).	0
30763	30763	2020.nlptea-1.4	Introduction	15	21	0.681818181818182	0.173553719008264	The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results.	0
30764	30764	2020.nlptea-1.4	Introduction	16	22	0.727272727272727	0.181818181818182	Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task.	0
30765	30765	2020.nlptea-1.4	Introduction	17	23	0.772727272727273	0.190082644628099	The rest of this paper is organized as follows.	0
30766	30766	2020.nlptea-1.4	Introduction	18	24	0.818181818181818	0.198347107438017	Section 2 describes the task in detail.	0
30767	30767	2020.nlptea-1.4	Introduction	19	25	0.863636363636364	0.206611570247934	Section 3 introduces the constructed data sets.	0
30768	30768	2020.nlptea-1.4	Introduction	20	26	0.909090909090909	0.214876033057851	Section 4 proposes evaluation metrics.	0
30769	30769	2020.nlptea-1.4	Introduction	21	27	0.954545454545455	0.223140495867769	Section 5 reports the results of the participants' approaches.	0
30770	30770	2020.nlptea-1.4	Introduction	22	28	1.0	0.231404958677686	Conclusions are finally drawn in Section 6.	0
30771	30771	2020.nlptea-1.4	Task Description	1	29	0.1	0.239669421487603	The goal of this shared task is to develop NLP techniques to automatically diagnose (and furtherly correct) grammatical errors in Chinese sentences written by CFL learners.	0
30772	30772	2020.nlptea-1.4	Task Description	2	30	0.2	0.247933884297521	"Such errors are defined as PADS: redundant words (denoted as a capital ""R""), missing words (""M""), word selection errors (""S""), and word ordering errors (""W"")."	0
30773	30773	2020.nlptea-1.4	Task Description	3	31	0.3	0.256198347107438	The input sentence may contain one or more such errors.	0
30774	30774	2020.nlptea-1.4	Task Description	4	32	0.4	0.264462809917355	The developed system should indicate which error types are embedded in the given unit (containing 1 to 5 sentences) and the position at which they occur.	0
30775	30775	2020.nlptea-1.4	Task Description	5	33	0.5	0.272727272727273	"Each input unit is given a unique number ""sid""."	0
30776	30776	2020.nlptea-1.4	Task Description	6	34	0.6	0.28099173553719	"If the inputs contain no grammatical errors, the system should return: ""sid, correct""."	0
30777	30777	2020.nlptea-1.4	Task Description	7	35	0.7	0.289256198347107	"If an input unit contains the grammatical errors, the output format should include four items ""sid, start_off, end_off, error_type"", where start_off and end_off respectively denote the positions of starting and ending character at which the grammatical error occurs, and error_type should be one of the defined errors: ""R"", ""M"", ""S"", and ""W""."	0
30778	30778	2020.nlptea-1.4	Task Description	8	36	0.8	0.297520661157025	Each character or punctuation mark occupies 1 space for counting positions.	0
30779	30779	2020.nlptea-1.4	Task Description	9	37	0.9	0.305785123966942	Example sentences and corresponding notes are shown as Table 1 shows.	0
30780	30780	2020.nlptea-1.4	Task Description	10	38	1.0	0.314049586776859	This year, we only have one track of HSK.	0
30781	30781	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	1	39	0.166666666666667	0.322314049586777	"Example 1 Input: (sid=00038800481) 我根本不能了解这妇女辞职回家的现象。在这个时代，为什么放弃自己的工作，就 回家当家庭主妇？ Output: 00038800481, 6, 7, S 00038800481, 8, 8, R (Notes: ""了解""should be ""理解""."	0
30782	30782	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	2	40	0.333333333333333	0.330578512396694	"In addition, ""这"" is a redundant word.)"	0
30783	30783	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	3	41	0.5	0.338842975206612	"Example 2 Input: (sid=00038800464)我真不明白。她们可能是追求一些前代的浪漫。 Output: 00038800464, correct Example 3 Input: (sid=00038801261)人战胜了饥饿，才努力为了下一代作更好的、更健康的东西。 Output: 00038801261, 9, 9, M 00038801261, 16, 16, S (Notes: ""能"" is missing."	0
30784	30784	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	4	42	0.666666666666667	0.347107438016529	"The word ""作""should be ""做""."	0
30785	30785	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	5	43	0.833333333333333	0.355371900826446	"The correct sentence is ""才能努力为了下一代做更好的"")"	0
30786	30786	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	6	44	1.0	0.363636363636364	"Example 4 Input: (sid=00038801320)饥饿的问题也是应该解决的。世界上每天由于饥饿很多人死亡。 Output: 00038801320, 19, 25, W (Notes: ""由于饥饿很多人"" should be ""很多人由于饥饿"")"	0
30787	30787	2020.nlptea-1.4	Data Sets	1	45	0.058823529411765	0.371900826446281	The learner corpora used in our shared task were taken from the writing section of the HSK (Pinyin of Hanyu Shuiping Kaoshi, Test of Chinese Level) (Cui et al, 2011;	0
30788	30788	2020.nlptea-1.4	Data Sets	2	46	0.117647058823529	0.380165289256198	Zhang et al, 2013).	0
30789	30789	2020.nlptea-1.4	Data Sets	3	47	0.176470588235294	0.388429752066116	Native Chinese speakers were trained to manually annotate grammatical errors and provide corrections corresponding to each error.	0
30790	30790	2020.nlptea-1.4	Data Sets	4	48	0.235294117647059	0.396694214876033	The data were then split into two mutually exclusive sets as follows.	0
30791	30791	2020.nlptea-1.4	Data Sets	5	49	0.294117647058823	0.40495867768595	(1) Training Set: All units in this set were used to train the grammatical error diagnostic systems.	0
30792	30792	2020.nlptea-1.4	Data Sets	6	50	0.352941176470588	0.413223140495868	Each unit contains 1 to 5 sentences with annotated grammatical errors and their corresponding corrections.	0
30793	30793	2020.nlptea-1.4	Data Sets	7	51	0.411764705882353	0.421487603305785	All units are represented in SGML format, as shown in Table 2.	0
30794	30794	2020.nlptea-1.4	Data Sets	8	52	0.470588235294118	0.429752066115703	We provide 1129 training units with a total of 2,909 grammatical errors, categorized as redundant (678 instances), missing (801), word selection (1228) and word ordering (201).	0
30795	30795	2020.nlptea-1.4	Data Sets	9	53	0.529411764705882	0.43801652892562	In addition to the data sets provided, participating research teams were allowed to use other public data for system development and implementation.	0
30796	30796	2020.nlptea-1.4	Data Sets	10	54	0.588235294117647	0.446280991735537	Use of other data should be specified in the final system report.	0
30797	30797	2020.nlptea-1.4	Data Sets	11	55	0.647058823529412	0.454545454545455	Test Set:	0
30798	30798	2020.nlptea-1.4	Data Sets	12	56	0.705882352941176	0.462809917355372	This set consists of testing units used for evaluating system performance.	0
30799	30799	2020.nlptea-1.4	Data Sets	13	57	0.764705882352941	0.471074380165289	Table 3 shows statistics for the testing set for this year.	0
30800	30800	2020.nlptea-1.4	Data Sets	14	58	0.823529411764706	0.479338842975207	According to the sampling in the writing sessions in HSK, over 40% of the sentences contain no error.	0
30801	30801	2020.nlptea-1.4	Data Sets	15	59	0.882352941176471	0.487603305785124	This was simulated in the test set, in order to test the performance of the systems in false positive identification.	0
30802	30802	2020.nlptea-1.4	Data Sets	16	60	0.941176470588235	0.495867768595041	The distributions of error types (Table 4) are similar with that of the training set.	0
30803	30803	2020.nlptea-1.4	Data Sets	17	61	1.0	0.504132231404959	The proportion of the correct sentences is sampled from data of the online Dynamic Corpus of HSK 1 .	0
30804	30804	2020.nlptea-1.4	Error Type	1	62	0.04	0.512396694214876	Performance Metrics	0
30805	30805	2020.nlptea-1.4	Error Type	2	63	0.08	0.520661157024793	Table 5 shows the confusion matrix used for evaluating system performance.	0
30806	30806	2020.nlptea-1.4	Error Type	3	64	0.12	0.528925619834711	In this matrix, TP (True Positive) is the number of sentences with grammatical errors are correctly identified by the developed system; FP (False Positive) is the number of sentences in which non-existent grammatical errors are identified as errors; TN (True Negative) is the number of sentences without grammatical errors that are correctly identified as such; FN (False Negative) is the number of sentences with grammatical errors which the system incorrectly identifies as being correct.	0
30807	30807	2020.nlptea-1.4	Error Type	4	65	0.16	0.537190082644628	The criteria for judging correctness are determined at three levels as follows.	0
30808	30808	2020.nlptea-1.4	Error Type	5	66	0.2	0.545454545454545	(1) Detection-level: Binary classification of a given sentence, that is, correct or incorrect, should be completely identical with the gold standard.	0
30809	30809	2020.nlptea-1.4	Error Type	6	67	0.24	0.553719008264463	All error types will be regarded as incorrect.	0
30810	30810	2020.nlptea-1.4	Error Type	7	68	0.28	0.56198347107438	(2) Identification-level:	0
30811	30811	2020.nlptea-1.4	Error Type	8	69	0.32	0.570247933884297	This level could be considered as a multi-class categorization problem.	0
30812	30812	2020.nlptea-1.4	Error Type	9	70	0.36	0.578512396694215	All error types should be clearly identified.	0
30813	30813	2020.nlptea-1.4	Error Type	10	71	0.4	0.586776859504132	A 1 http://bcc.blcu.edu.cn/hsk correct case should be completely identical with the gold standard of the given error type.	0
30814	30814	2020.nlptea-1.4	Error Type	11	72	0.44	0.59504132231405	(3) Position-level:	0
30815	30815	2020.nlptea-1.4	Error Type	12	73	0.48	0.603305785123967	In addition to identifying the error types, this level also judges the occurrence range of the grammatical error.	0
30816	30816	2020.nlptea-1.4	Error Type	13	74	0.52	0.611570247933884	That is to say, the system results should be perfectly identical with the quadruples of the gold standard.	0
30817	30817	2020.nlptea-1.4	Error Type	14	75	0.56	0.619834710743802	Besides the traditional criteria in the past share tasks, Correction-level was introduced to CGED since 2018.	0
30818	30818	2020.nlptea-1.4	Error Type	15	76	0.6	0.628099173553719	(4) Correction-level:	0
30819	30819	2020.nlptea-1.4	Error Type	16	77	0.64	0.636363636363636	For the error types of Selection and Missing, recommended corrections are required.	0
30820	30820	2020.nlptea-1.4	Error Type	17	78	0.68	0.644628099173554	At most 3 recommended corrections are allowed for each S and M type error.	0
30821	30821	2020.nlptea-1.4	Error Type	18	79	0.72	0.652892561983471	In this level the amount of the corrections recommended would influence the precision and F1 in this level.	0
30822	30822	2020.nlptea-1.4	Error Type	19	80	0.76	0.661157024793388	The trust of the recommendation would be test.	0
30823	30823	2020.nlptea-1.4	Error Type	20	81	0.8	0.669421487603306	The sub-track TOP1 count only one recommended correction, while TOP3 count one hit, if one correction in three hits the golden standard, ignoring its ranking.	0
30824	30824	2020.nlptea-1.4	Error Type	21	82	0.84	0.677685950413223	The following metrics are measured at all levels with the help of the confusion matrix.	0
30825	30825	2020.nlptea-1.4	Error Type	22	83	0.88	0.685950413223141	 False Positive Rate = FP / (FP+TN)  Accuracy = (TP+TN) / (TP+FP+TN+FN)  Precision = TP / (TP+FP)  Recall = TP / (TP+FN)  F1 =2*Precision*Recall / (Precision + Recall)	0
30826	30826	2020.nlptea-1.4	Error Type	23	84	0.92	0.694214876033058	"For example, for 4 testing inputs with gold standards shown as ""00038800481, 6, 7, S"", ""00038800481, 8, 8, R"", ""00038800464, correct"", ""00038801261, 9, 9, M"", ""00038801261, 16, 16, S"" and ""00038801320, 19, 25, W"", the system may output the result as ""00038800481, 2, 3, S"", ""00038800481, 4, 5, S"", ""00038800481, 8, 8, R"", ""00038800464, correct"", ""00038801261, 9, 9, M"", ""00038801261, 16, 19, S"" and ""00038801320, 19, 25, M""."	0
30827	30827	2020.nlptea-1.4	Error Type	24	85	0.96	0.702479338842975	The scoring script will yield the following performance.	0
30828	30828	2020.nlptea-1.4	Error Type	25	86	1.0	0.710743801652893	False Positive Rate (FPR) = 0 (=0/1) Detection-level:	0
30829	30829	2020.nlptea-1.4	Evaluation Results	1	87	0.052631578947369	0.71900826446281	Table 6 summarizes the submission statistics for the 17 participating teams.	0
30830	30830	2020.nlptea-1.4	Evaluation Results	2	88	0.105263157894737	0.727272727272727	In the official testing phase, each participating team was allowed to submit at most three runs.	0
30831	30831	2020.nlptea-1.4	Evaluation Results	3	89	0.157894736842105	0.735537190082645	Of the 17 teams, 11 teams submitted their testing results in Correction-level, for a total of 43 runs.	0
30832	30832	2020.nlptea-1.4	Evaluation Results	4	90	0.210526315789474	0.743801652892562	Table 7 to 11 show the testing results of the CGED2020 in 6 tracks: false positive rate (FPR), detection level, identification level, position level and correction level (in two settings: top1 and top3).	0
30833	30833	2020.nlptea-1.4	Evaluation Results	5	91	0.263157894736842	0.752066115702479	All runs of top F1 score are highlighted in the tables.	0
30834	30834	2020.nlptea-1.4	Evaluation Results	6	92	0.31578947368421	0.760330578512397	The CYUT achieved the lowest FPR of 0.0163, about one third of the lowest FPR in the CGED 2018.	0
30835	30835	2020.nlptea-1.4	Evaluation Results	7	93	0.368421052631579	0.768595041322314	Detection-level evaluations are designed to detect whether a sentence contains grammatical errors or not.	0
30836	30836	2020.nlptea-1.4	Evaluation Results	8	94	0.421052631578947	0.776859504132231	A neutral baseline can be easily achieved by reporting all testing sentences containing errors.	0
30837	30837	2020.nlptea-1.4	Evaluation Results	9	95	0.473684210526316	0.785123966942149	According to the test data distribution, the baseline system can achieve an accuracy of 0.7893.	0
30838	30838	2020.nlptea-1.4	Evaluation Results	10	96	0.526315789473684	0.793388429752066	However, not all systems performed above the baseline.	0
30839	30839	2020.nlptea-1.4	Evaluation Results	11	97	0.578947368421053	0.801652892561983	The system result submitted by NJU-NLP achieved the best detection F1 of 0.9122, beating the 0.9 mark for the first time.	0
30840	30840	2020.nlptea-1.4	Evaluation Results	12	98	0.631578947368421	0.809917355371901	For identification-level evaluations, the systems need to identify the error types in a given unit.	0
30841	30841	2020.nlptea-1.4	Evaluation Results	13	99	0.68421052631579	0.818181818181818	The system developed by Flying and Orange	0
30842	30842	2020.nlptea-1.4	Evaluation Results	14	100	0.736842105263158	0.826446280991735	Plus provided the highest F1 score of 0.6736 and 0.6726 for grammatical error identification.	0
30843	30843	2020.nlptea-1.4	Evaluation Results	15	101	0.789473684210526	0.834710743801653	For position-level, Flying achieved the best F1 score of 0.4041, crossing the 0.4 mark for the first time.	0
30844	30844	2020.nlptea-1.4	Evaluation Results	16	102	0.842105263157895	0.84297520661157	Orange	0
30845	30845	2020.nlptea-1.4	Evaluation Results	17	103	0.894736842105263	0.851239669421488	Plus reached 0.394.	0
30846	30846	2020.nlptea-1.4	Evaluation Results	18	104	0.947368421052632	0.859504132231405	Perfectly identifying the error types and their corresponding positions is difficult because the error propagation is serious.	0
30847	30847	2020.nlptea-1.4	Evaluation Results	19	105	1.0	0.867768595041322	In correction-level, UNIPUS-Flaubert achieved best F1 of 0.1891 in top1 setting and YD_NLP of 0.1885 top3 setting.	0
30848	30848	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	1	106	0.083333333333333	0.87603305785124	In CGED 2020, the implementation of pre-trained model like BERT achieved significant improvement in many tracks.	0
30849	30849	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	2	107	0.166666666666667	0.884297520661157	"The ""standard pipe-line"" biLSTM+CRF in CGED2017 and 2018 is replaced."	0
30850	30850	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	3	108	0.25	0.892561983471074	Hybrid methods based on pre-trained model were proposed by most of the teams.	0
30851	30851	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	4	109	0.333333333333333	0.900826446280992	ResNet, graph convolution network and data argumentation appeared for the first time in the solutions.	0
30852	30852	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	5	110	0.416666666666667	0.909090909090909	The rethinking the data construction (including pseudo data generation) and feature selection did not attract the attention of the participants.	0
30853	30853	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	6	111	0.5	0.917355371900826	However, the balance of the FPR and other track did not progress a lot.	0
30854	30854	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	7	112	0.583333333333333	0.925619834710744	The rough merging strategies implemented in hybrid methods and the over generation of generation models may lead the drop in FPR.	0
30855	30855	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	8	113	0.666666666666667	0.933884297520661	From organizers' perspectives, a good system should have a high F1 score and a low false positive rate.	0
30856	30856	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	9	114	0.75	0.942148760330579	In summary, none of the submitted systems provided a comprehensive superior performance using different metrics, indicating the difficulty of developing systems for effective grammatical error diagnosis, especially in CFL contexts.	0
30857	30857	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	10	115	0.833333333333333	0.950413223140496	It is worth noting that in the track of detection, the performance over 0.9 is close to the application of actual scene.	0
30858	30858	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	11	116	0.916666666666667	0.958677685950413	In the highly focused track of position and correction, variant teams lead the ranks, unlike the past CGEDs.	0
30859	30859	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	12	117	1.0	0.966942148760331	It's a very exciting phenomena indicating the attraction the task increased quickly.	0
30860	30860	2020.nlptea-1.4	Conclusion	1	118	0.25	0.975206611570248	This study describes the NLP-TEA 2020 shared task for Chinese grammatical error diagnosis, including task design, data preparation, performance metrics, and evaluation results.	0
30861	30861	2020.nlptea-1.4	Conclusion	2	119	0.5	0.983471074380165	Regardless of actual performance, all submissions contribute to the common effort to develop Chinese grammatical error diagnosis system, and the individual reports in the proceedings provide useful insights into computer-assisted language learning for CFL learners.	0
30862	30862	2020.nlptea-1.4	Conclusion	3	120	0.75	0.991735537190083	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
30863	30863	2020.nlptea-1.4	Conclusion	4	121	1.0	1.0	Therefore, all data sets with gold standards and scoring scripts are publicly available online at http://www.cged.science.	0
31208	31208	W18-3601	title	1	1	1.0	0.005291005291005	The First Multilingual Surface Realisation Shared Task (SR&apos;18): Overview and Evaluation Results	0
31209	31209	W18-3601	abstract	1	2	0.166666666666667	0.010582010582011	We report results from the SR'18 Shared Task, a new multilingual surface realisation task organised as part of the ACL'18 Workshop on Multilingual Surface Realisation.	0
31210	31210	W18-3601	abstract	2	3	0.333333333333333	0.015873015873016	As in its English-only predecessor task SR'11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed.	0
31211	31211	W18-3601	abstract	3	4	0.5	0.021164021164021	The shallow track was offered in ten, and the deep track in three languages.	0
31212	31212	W18-3601	abstract	4	5	0.666666666666667	0.026455026455027	Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity.	0
31213	31213	W18-3601	abstract	5	6	0.833333333333333	0.031746031746032	This report presents the evaluation results, along with descriptions of the SR'18 tracks, data and evaluation methods.	0
31214	31214	W18-3601	abstract	6	7	1.0	0.037037037037037	For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.	0
31215	31215	W18-3601	Introduction and Task Overview	1	8	0.045454545454546	0.042328042328042	Natural Language Generation (NLG) is attracting growing interest both in the form of end-toend tasks (e.g. data-to-text and text-to-text generation), and as embedded component tasks (e.g. in abstractive summarisation, dialogue-based interaction and question answering).	0
31216	31216	W18-3601	Introduction and Task Overview	2	9	0.090909090909091	0.047619047619048	NLG research has been given a boost by two recent developments: the rapid spread of neural language generation techniques, and the growing availability of multilingual treebanks annotated with Universal Dependencies 1 (UD), to the point 1 http://universaldependencies.org/ where as many as 70 treebanks covering about 50 languages can now be downloaded freely.	0
31217	31217	W18-3601	Introduction and Task Overview	3	10	0.136363636363636	0.052910052910053	2 UD treebanks facilitate the development of applications that work potentially across all languages for which UD treebanks are available in a uniform fashion, which is a big advantage for system developers.	0
31218	31218	W18-3601	Introduction and Task Overview	4	11	0.181818181818182	0.058201058201058	As has already been seen in parsing, UD treebanks are also a good basis for multilingual shared tasks: a method that works for some languages may also work for others.	0
31219	31219	W18-3601	Introduction and Task Overview	5	12	0.227272727272727	0.063492063492064	The SR'18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation.	1
31220	31220	W18-3601	Introduction and Task Overview	6	13	0.272727272727273	0.068783068783069	SR'18 also addresses questions about just how suitable and useful the notion of universal dependencies-which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular-is for NLG.	0
31221	31221	W18-3601	Introduction and Task Overview	7	14	0.318181818181818	0.074074074074074	SR'18 follows the SR'11 pilot surface realisation task for English  which was part of Generation Challenges 2011 (GenChal'11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks.	0
31222	31222	W18-3601	Introduction and Task Overview	8	15	0.363636363636364	0.079365079365079	Outside of the SR tasks, just three 'deep' NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG 3 (Gardent et al., 2017), Se-mEval Task 9 4 (May and Priyadarshi, 2017), and E2E 5 (Novikova et al., 2017).	0
31223	31223	W18-3601	Introduction and Task Overview	9	16	0.409090909090909	0.084656084656085	What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016):	0
31224	31224	W18-3601	Introduction and Task Overview	10	17	0.454545454545455	0.08994708994709	http:// universaldependencies.org/conll17/.	0
31225	31225	W18-3601	Introduction and Task Overview	11	18	0.5	0.095238095238095	3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ Interaction	0
31226	31226	W18-3601	Introduction and Task Overview	12	19	0.545454545454545	0.100529100529101	Lab/E2E/ tasks have only been offered for English.	0
31227	31227	W18-3601	Introduction and Task Overview	13	20	0.590909090909091	0.105820105820106	As in SR'11, the Multilingual Surface Realisation shared task (SR'18) comprises two tracks with different levels of difficulty:	0
31228	31228	W18-3601	Introduction and Task Overview	14	21	0.636363636363636	0.111111111111111	Shallow Track:	0
31229	31229	W18-3601	Introduction and Task Overview	15	22	0.681818181818182	0.116402116402116	This track starts from genuine UD structures in which word order information has been removed and tokens have been lemmatised.	0
31230	31230	W18-3601	Introduction and Task Overview	16	23	0.727272727272727	0.121693121693122	In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations.	0
31231	31231	W18-3601	Introduction and Task Overview	17	24	0.772727272727273	0.126984126984127	The task amounts to determining the word order and inflecting words.	0
31232	31232	W18-3601	Introduction and Task Overview	18	25	0.818181818181818	0.132275132275132	Deep Track:	0
31233	31233	W18-3601	Introduction and Task Overview	19	26	0.863636363636364	0.137566137566138	This track starts from UD structures from which functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological and syntactic information have been removed.	0
31234	31234	W18-3601	Introduction and Task Overview	20	27	0.909090909090909	0.142857142857143	In addition to what is required for the Shallow Track, the task in the Deep Track thus also requires reintroduction of the removed functional words and morphological features.	0
31235	31235	W18-3601	Introduction and Task Overview	21	28	0.954545454545455	0.148148148148148	In the remainder of this paper, we describe the data we used in the two tracks (Section 2), and the evaluation methods we used to evaluate submitted systems (Sections 3.1 and 3.2).	0
31236	31236	W18-3601	Introduction and Task Overview	22	29	1.0	0.153439153439153	We then briefly introduce the participating systems (Section 4), report and discuss evaluation results (Section 5), and conclude with some discussion and a look to the future (Section 6).	0
31237	31237	W18-3601	Data	1	30	0.0625	0.158730158730159	To create the SR'18 training and testing data, we used as data sources ten UD treebanks for which annotations of reasonable quality were available, providing PoS tags and morphologically relevant markup (number, tense, verbal finiteness, etc.):	0
31238	31238	W18-3601	Data	2	31	0.125	0.164021164021164	UD Arabic, UD Czech, UD Dutch, UD English, UD Finnish, UD French, UD Italian, UD Portuguese, UD Russian-SynTag	0
31239	31239	W18-3601	Data	3	32	0.1875	0.169312169312169	Rus and UD Spanish-AnCora.	0
31240	31240	W18-3601	Data	4	33	0.25	0.174603174603175	6	0
31241	31241	W18-3601	Data	5	34	0.3125	0.17989417989418	We created training and test data for all ten languages for the Shallow Track, and for three of the languages, namely English, French and Spanish, for the Deep Track.	0
31242	31242	W18-3601	Data	6	35	0.375	0.185185185185185	Inputs in both Shallow and Deep Tracks are trees, and are released in CoNLL-U format, with no meta-information.	0
31243	31243	W18-3601	Data	7	36	0.4375	0.19047619047619	7 Figures 1, 2 and 3 show a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it.	0
31244	31244	W18-3601	Data	8	37	0.5	0.195767195767196	To create inputs to the Shallow Track, the UD structures were processed as follows:	0
31245	31245	W18-3601	Data	9	38	0.5625	0.201058201058201	1	0
31246	31246	W18-3601	Data	10	39	0.625	0.206349206349206	Word order information was removed by randomised scrambling;	0
31247	31247	W18-3601	Data	11	40	0.6875	0.211640211640212	2. Words were replaced by their lemmas.	0
31248	31248	W18-3601	Data	12	41	0.75	0.216931216931217	For the Deep Track, the following steps were additionally carried out:	0
31249	31249	W18-3601	Data	13	42	0.8125	0.222222222222222	3	0
31250	31250	W18-3601	Data	14	43	0.875	0.227513227513227	Edge labels were generalised into predicate/argument labels, in the Prop-Bank/NomBank (Palmer et al., 2005;	0
31251	31251	W18-3601	Data	15	44	0.9375	0.232804232804233	Meyers et al., 2004) fashion.	0
31252	31252	W18-3601	Data	16	45	1.0	0.238095238095238	That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1.	0
31253	31253	W18-3601	Functional prepositions and conjunctions in	1	46	0.071428571428572	0.243386243386243	argument position (i.e. prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure) are removed (e.g. by and of in Figure 2    node for the subject is added if an originally finite verb has no first argument and no available argument to build a passive; for a prodrop language such as Spanish, a dummy pronoun is added if the first argument is missing.	0
31254	31254	W18-3601	Functional prepositions and conjunctions in	2	47	0.142857142857143	0.248677248677249	7. Surface-level morphologically relevant information as prescribed by syntactic structure or agreement (such as verbal finiteness or verbal number) is removed, whereas semantic-level information such as nominal number and verbal tense is retained.	0
31255	31255	W18-3601	Functional prepositions and conjunctions in	3	48	0.214285714285714	0.253968253968254	8. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) are removed, and only coarse-grained ones are retained (column 4 in Figures 2 and 3).	0
31256	31256	W18-3601	Functional prepositions and conjunctions in	4	49	0.285714285714286	0.259259259259259	Shallow Track inputs were generated with the aid of a simple Python script from the original UD structures.	0
31257	31257	W18-3601	Functional prepositions and conjunctions in	5	50	0.357142857142857	0.264550264550265	During the conversion, we filtered out sentences that contained dependencies that only make sense in an analysis context (e.g. reparandum, or orphan).	0
31258	31258	W18-3601	Functional prepositions and conjunctions in	6	51	0.428571428571429	0.26984126984127	This amounted to around 1.5% of sentences for the different languages on average; see Table 2 for an overview of the final sizes of the datasets.	0
31259	31259	W18-3601	Functional prepositions and conjunctions in	7	52	0.5	0.275132275132275	Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of graphtransduction grammars that cover steps 3-8 above (in a similar fashion as Mille et al. (2017)).	0
31260	31260	W18-3601	Functional prepositions and conjunctions in	8	53	0.571428571428571	0.28042328042328	There is a node-to-node correspondence between the deep and shallow input structures.	0
31261	31261	W18-3601	Functional prepositions and conjunctions in	9	54	0.642857142857143	0.285714285714286	The Deep Track inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information (see, e.g. the inputs in the SemEval, WebNLG, E2E shared tasks).	0
31262	31262	W18-3601	Functional prepositions and conjunctions in	10	55	0.714285714285714	0.291005291005291	At the same time, we used only information found in the UD syntactic structures to create the deep inputs, and tried to keep their structure simple.	0
31263	31263	W18-3601	Functional prepositions and conjunctions in	11	56	0.785714285714286	0.296296296296296	It can be argued that not all the information necessary to reconstruct the original sentences is available in the Deep Track inputs.	0
31264	31264	W18-3601	Functional prepositions and conjunctions in	12	57	0.857142857142857	0.301587301587302	Task definitions specifically designed for NLG, as used e.g. in Se-mEval Task 9, tend to use abstract meaning representations (AMRs) as inputs that contain additional information such as OntoNotes labelling or typed circumstantials, which make the generation task easier.	0
31265	31265	W18-3601	Functional prepositions and conjunctions in	13	58	0.928571428571429	0.306878306878307	In the SR'18 Deep Track inputs, words are not disambiguated, full prepositions may be missing, and some argument relations may be underspecified or missing.	0
31266	31266	W18-3601	Functional prepositions and conjunctions in	14	59	1.0	0.312169312169312	train 6,016 66,485 12,375 14,289 12,030 14,529 12,796 12,318 8,325 48  3 Evaluation Methods	0
31267	31267	W18-3601	Automatic methods	1	60	0.083333333333333	0.317460317460317	We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems.	0
31268	31268	W18-3601	Automatic methods	2	61	0.166666666666667	0.322751322751323	BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences.	0
31269	31269	W18-3601	Automatic methods	3	62	0.25	0.328042328042328	We use the smoothed version and report results for n = 4.	0
31270	31270	W18-3601	Automatic methods	4	63	0.333333333333333	0.333333333333333	NIST 9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative.	0
31271	31271	W18-3601	Automatic methods	5	64	0.416666666666667	0.338624338624339	Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text.	0
31272	31272	W18-3601	Automatic methods	6	65	0.5	0.343915343915344	The resulting number is then divided by the number of characters in the reference text, and finally subtracted from 1, in order to align with the other metrics.	0
31273	31273	W18-3601	Automatic methods	7	66	0.583333333333333	0.349206349206349	Spaces and punctuation marks count as characters; output texts were otherwise normalised as for all metrics (see below).	0
31274	31274	W18-3601	Automatic methods	8	67	0.666666666666667	0.354497354497354	The figures in the tables below are the systemlevel scores for BLEU and NIST, and the mean sentence-level scores for DIST.	0
31275	31275	W18-3601	Automatic methods	9	68	0.75	0.35978835978836	Text normalisation:	0
31276	31276	W18-3601	Automatic methods	10	69	0.833333333333333	0.365079365079365	Output texts were normalised prior to computing metrics by lowercasing all tokens, removing any extraneous whitespace characters.	0
31277	31277	W18-3601	Automatic methods	11	70	0.916666666666667	0.37037037037037	Missing outputs: Missing outputs were scored 0.	0
31278	31278	W18-3601	Automatic methods	12	71	1.0	0.375661375661376	Since coverage was 100% for all systems except one, we only report results for all sentences (incorporating the missing-output penalty), rather than also separately reporting scores for just the in-coverage items.	0
31279	31279	W18-3601	Human-assessed methods	1	72	0.045454545454546	0.380952380952381	We assessed two quality criteria in the human evaluations, in separate evaluation experiments: Readability and Meaning Similarity.	0
31280	31280	W18-3601	Human-assessed methods	2	73	0.090909090909091	0.386243386243386	As in SR'11 , we used continuous sliders as rating tools, because raters tend to prefer them .	0
31281	31281	W18-3601	Human-assessed methods	3	74	0.136363636363636	0.391534391534391	Slider positions were mapped to values from 0 to 100 (best).	0
31282	31282	W18-3601	Human-assessed methods	4	75	0.181818181818182	0.396825396825397	Raters were first given brief instructions, including instructions to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation.	0
31283	31283	W18-3601	Human-assessed methods	5	76	0.227272727272727	0.402116402116402	"The part of the instructions used only in the Readability assessment experiments was: ""The quality criterion you need to assess is Readability."	0
31284	31284	W18-3601	Human-assessed methods	6	77	0.272727272727273	0.407407407407407	This is sometimes called fluency, and your task is to decide how well the given text reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc.	0
31285	31285	W18-3601	Human-assessed methods	7	78	0.318181818181818	0.412698412698413	"Please rate the text by moving the slider to the position that corresponds to your rating, where 0 is the worst, and 100 is the best rating."""	0
31286	31286	W18-3601	Human-assessed methods	8	79	0.363636363636364	0.417989417989418	The corresponding instructions for Meaning Similarity assessment, in which system outputs were compared to reference sentences, were as follows:	0
31287	31287	W18-3601	Human-assessed methods	9	80	0.409090909090909	0.423280423280423	"""The quality criterion you need to assess is Meaning Similarity."	0
31288	31288	W18-3601	Human-assessed methods	10	81	0.454545454545455	0.428571428571429	You need to read both texts, and then decide how close in meaning the second text (in black) is to the first (in grey).	0
31289	31289	W18-3601	Human-assessed methods	11	82	0.5	0.433862433862434	Please use the slider at the bottom of the page to express your rating.	0
31290	31290	W18-3601	Human-assessed methods	12	83	0.545454545454545	0.439153439153439	The closer in meaning the second text clipping is to the first, the further to the right (towards 100) you need to place the slider.	0
31291	31291	W18-3601	Human-assessed methods	13	84	0.590909090909091	0.444444444444444	"In other words, a rating of 100% would mean that the meaning of the two text clippings is exactly identical."""	0
31292	31292	W18-3601	Human-assessed methods	14	85	0.636363636363636	0.44973544973545	Slider design:	0
31293	31293	W18-3601	Human-assessed methods	15	86	0.681818181818182	0.455026455026455	In SR'11, a slider design was used, which had a smiley face at the 100 end and a frowning face at the 0 end, with the pointer starting out at 50.	0
31294	31294	W18-3601	Human-assessed methods	16	87	0.727272727272727	0.46031746031746	For conformity with what has emerged as a new affordable human evaluation standard over the past two years in the main machine translation shared tasks held at WMT (Bojar et al., 2017a), we changed this design to look as follows, with the pointer starting at 0: Test data sets for human evaluations:	0
31295	31295	W18-3601	Human-assessed methods	17	88	0.772727272727273	0.465608465608466	Test set sizes out of the box varied considerably for the different languages.	0
31296	31296	W18-3601	Human-assessed methods	18	89	0.818181818181818	0.470899470899471	For the human test sets we selected either the entire set or a subset of 1,000, whichever was the smaller number, for a given language.	0
31297	31297	W18-3601	Human-assessed methods	19	90	0.863636363636364	0.476190476190476	For subsets, test set items were selected randomly but ensuring a similar sentence length distribution as in the whole set.	0
31298	31298	W18-3601	Human-assessed methods	20	91	0.909090909090909	0.481481481481481	Reported scores: Again in keeping with the WMT approach, we report both average raw scores and average standardised scores per system.	0
31299	31299	W18-3601	Human-assessed methods	21	92	0.954545454545455	0.486772486772487	In order to produce standardised scores we simply map each individual evaluator's scores to their standard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator's mean and standard deviation.	0
31300	31300	W18-3601	Human-assessed methods	22	93	1.0	0.492063492063492	For both raw and standard scores, we compute the mean of sentence-level scores.	0
31301	31301	W18-3601	Mechanical	1	94	0.111111111111111	0.497354497354497	Turk evaluations	0
31302	31302	W18-3601	Mechanical	2	95	0.222222222222222	0.502645502645503	For three of the languages in the shallow track (English, Spanish and French), we replicated the human evaluation method from WMT'17, known as Direct Assessment (DA) (Graham et al., 2016), exactly, except that we also ran (separate) experiments to assess the Readability criterion, using the same method.	0
31303	31303	W18-3601	Mechanical	3	96	0.333333333333333	0.507936507936508	Quality assurance: System outputs are randomly assigned to HITs (following Mechanical Turk terminology) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a 'damaged' version and (iii) some are replaced by their corresponding reference texts.	0
31304	31304	W18-3601	Mechanical	4	97	0.444444444444444	0.513227513227513	In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high.	0
31305	31305	W18-3601	Mechanical	5	98	0.555555555555556	0.518518518518518	For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a).	0
31306	31306	W18-3601	Mechanical	6	99	0.666666666666667	0.523809523809524	Below we report QA figures for the MTurk evaluations (Section 3.2.1).	0
31307	31307	W18-3601	Mechanical	7	100	0.777777777777778	0.529100529100529	Code:	0
31308	31308	W18-3601	Mechanical	8	101	0.888888888888889	0.534391534391534	We were able to reuse, with minor adaptations, the code produced for the WMT'17 evaluations.	0
31309	31309	W18-3601	Mechanical	9	102	1.0	0.53968253968254	10	0
31310	31310	W18-3601	Google Data Compute Evaluation	1	103	0.125	0.544973544973545	In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google's internal 'Data Compute' system evaluation service, where experienced evaluators carefully assess each system output.	0
31311	31311	W18-3601	Google Data Compute Evaluation	2	104	0.25	0.55026455026455	We used an interface that matches the WMT'17 interface above, as closely as was possible within the constraints of the Data Compute platform.	0
31312	31312	W18-3601	Google Data Compute Evaluation	3	105	0.375	0.555555555555556	Everything stated at the beginning of Section 3.2 also holds for the expert annotator evaluations with Google Data Compute.	0
31313	31313	W18-3601	Google Data Compute Evaluation	4	106	0.5	0.560846560846561	Quality assurance: Because in the Google Data Compute version of the evaluation experiment we were using expert evaluators from a pool of workers routinely employed to perform such tasks, we did not replicate the WMT'17 QA techniques precisely, opting for a simpler test of self-consistency, or intra-evaluator agreement (IEA) instead.	0
31314	31314	W18-3601	Google Data Compute Evaluation	5	107	0.625	0.566137566137566	Test set items were randomly grouped into sets of 100 (which we are also calling HITs here for uniformity) and order was again randomised before presentation to evaluators.	0
31315	31315	W18-3601	Google Data Compute Evaluation	6	108	0.75	0.571428571428571	Each evaluator did at least one HIT.	0
31316	31316	W18-3601	Google Data Compute Evaluation	7	109	0.875	0.576719576719577	Each HIT contained 5 items which were duplicated to test for IEA which we computed as the average Pearson correlation coefficient per HIT.	0
31317	31317	W18-3601	Google Data Compute Evaluation	8	110	1.0	0.582010582010582	The average IEA for English was 0.75 on the raw scores for Meaning Similarity, and 0.66 for Readability.	0
31318	31318	W18-3601	Overview of Submitted Systems	1	111	0.071428571428572	0.587301587301587	Eight different teams (out of twenty-one registered) submitted outputs to SR'18: the ADAPT Centre (ADAPT, Ireland), AX Semantics (AX, Germany), IIT-BHU Varanasi (IIT-BHU, India), Ohio State University (OSU, USA), University of São Paulo (NILC, Brazil), Tilburg University (Tilburg, The Netherlands), Università degli Studi di Torino (DipInfo-UniTo, Italy), and Technische Universität Darmstadt (BinLin, Germany).	0
31319	31319	W18-3601	Overview of Submitted Systems	2	112	0.142857142857143	0.592592592592593	All teams submitted outputs for at least the English Shallow Track; one team participated in the Deep Track (ADAPT, English), and three teams submitted outputs for all ten languages of the Shallow Track (AX, OSU, and BinLin).	0
31320	31320	W18-3601	Overview of Submitted Systems	3	113	0.214285714285714	0.597883597883598	Most submitted systems are based on neural components, and break down the surface realisation task into two subtasks: linearisation, and word inflection.	0
31321	31321	W18-3601	Overview of Submitted Systems	4	114	0.285714285714286	0.603174603174603	Details of each approach are provided in the teams' reports elsewhere in this volume; here, we briefly summarise each approach:	0
31322	31322	W18-3601	Overview of Submitted Systems	5	115	0.357142857142857	0.608465608465608	ADAPT uses linearised parse tree inputs to train a sequence-to-sequence LSTM model with copy attention, augmenting the training set with additional synthetic data.	0
31323	31323	W18-3601	Overview of Submitted Systems	6	116	0.428571428571429	0.613756613756614	AX is trained on word pairs for ordering and is combined with a rule-based morphology component.	0
31324	31324	W18-3601	Overview of Submitted Systems	7	117	0.5	0.619047619047619	IIT-BHU uses an LSTM-based encoderdecoder model for word re-inflection, and a Language Model-based approach for word reordering.	0
31325	31325	W18-3601	Overview of Submitted Systems	8	118	0.571428571428571	0.624338624338624	OSU first generates inflected wordforms with a neural sequence-to-sequence model, and then incrementally linearises them using a global linear model over features that take into account the dependency structure and dependency location.	0
31326	31326	W18-3601	Overview of Submitted Systems	9	119	0.642857142857143	0.62962962962963	11 NILC is a neural-based system that uses a bottom-up approach to build the sentence using the dependency relations together with a language model, and language-specific lexicons to produce the word forms of each lemma in the sentence.	0
31327	31327	W18-3601	Overview of Submitted Systems	10	120	0.714285714285714	0.634920634920635	Tilburg works by first preprocessing an input dependency tree into an ordered linearised string, 11 Some of OSU's outputs were submitted after the start of the human evaluations and are not included in this report; outputs submitted late, but before the human evaluation started, are included and marked with asterisks in the results tables.	0
31328	31328	W18-3601	Overview of Submitted Systems	11	121	0.785714285714286	0.64021164021164	which is then realised using a statistical machine translation model.	0
31329	31329	W18-3601	Overview of Submitted Systems	12	122	0.857142857142857	0.645502645502645	DipInfo-Uni	0
31330	31330	W18-3601	Overview of Submitted Systems	13	123	0.928571428571429	0.650793650793651	To employs two separate neural networks with different architectures to predict the word ordering and the morphological inflection independently; outputs are combined to produce the final sentence.	0
31331	31331	W18-3601	Overview of Submitted Systems	14	124	1.0	0.656084656084656	BinLin uses one neural module as a binary classifier in a sequential process of ordering token lemmas, and another for character-level morphology generation where the words are inflected to finish the surface realisation.	0
31332	31332	W18-3601	Evaluation results	1	125	0.125	0.661375661375661	Results from metric evaluations	0
31333	31333	W18-3601	Evaluation results	2	126	0.25	0.666666666666667	Tables 3-5 show BLEU-4, NIST, and DIST results for both the Shallow and Deep tracks, for all submitted systems; results are listed in order of number of languages submitted for.	0
31334	31334	W18-3601	Evaluation results	3	127	0.375	0.671957671957672	Best results for each language are shown in boldface.	0
31335	31335	W18-3601	Evaluation results	4	128	0.5	0.677248677248677	In terms of BLEU-4, in the Shallow Track, Tilburg obtained the best scores for four languages (French, Italian, Dutch, Portuguese), OSU for three (Arabic, Spanish, Finnish), BinLin for two (Czech, Russian), and ADAPT for one (English).	0
31336	31336	W18-3601	Evaluation results	5	129	0.625	0.682539682539683	The highest BLEU-4 scores across languages were obtained on the English and Spanish datasets, with BLEU-4 scores of 69.14 (ADAPT) and 65.31 (OSU) respectively.	0
31337	31337	W18-3601	Evaluation results	6	130	0.75	0.687830687830688	Results are identical for DIST, except that AX, rather than BinLin, has the highest score for Czech.	0
31338	31338	W18-3601	Evaluation results	7	131	0.875	0.693121693121693	The picture for NIST is also very similar to that for BLEU-4, except that ADAPT and OSU are tied for best NIST score for English, and Bin-Lin (rather than Tilburg) has the best NIST score for Dutch.	0
31339	31339	W18-3601	Evaluation results	8	132	1.0	0.698412698412698	In the Deep Track, only ADAPT submitted system outputs (English), and as expected, the scores are much lower than for the Shallow Track, across all metrics.	0
31340	31340	W18-3601	Results from human evaluations	1	133	0.25	0.703703703703704	Given the small number of submissions in the Deep Track, we conducted human evaluations for the Shallow Track only.	0
31341	31341	W18-3601	Results from human evaluations	2	134	0.5	0.708994708994709	We used Mechanical Turk for the three languages for which this is feasible (English, Spanish and French), and our aim was to also conduct evaluations via Google's Data Compute service for three additional languages which had the next highest numbers of submissions, as    well as for English in order to enable us to compare results obtained with the two different methods.	0
31342	31342	W18-3601	Results from human evaluations	3	135	0.75	0.714285714285714	However, most of the latter evaluations are still ongoing and will be reported separately in a future paper.	0
31343	31343	W18-3601	Results from human evaluations	4	136	1.0	0.71957671957672	Below, we report Google Data Compute results and comparisons with Mechanical Turk results, for English only.	0
31344	31344	W18-3601	Mechanical	1	137	0.026315789473684	0.724867724867725	Turk results	0
31345	31345	W18-3601	Mechanical	2	138	0.052631578947369	0.73015873015873	Tables 6, 7 and 8 show the results of the human evaluation carried out via Mechanical Turk with Direct Assessment (MTurk DA), for English, French and Spanish, respectively.	0
31346	31346	W18-3601	Mechanical	3	139	0.078947368421053	0.735449735449735	See Section 3.2 for details of the evaluation method.	0
31347	31347	W18-3601	Mechanical	4	140	0.105263157894737	0.740740740740741	'DA' refers to the specific way in which scores are collected in the WMT approach which differs from what we did for SR'11, and here in the Google Data Compute experiments.	0
31348	31348	W18-3601	Mechanical	5	141	0.131578947368421	0.746031746031746	English: Average Meaning Similarity DA scores for English systems range from 86.9% to 67% with OSU achieving the highest overall score in terms of both average raw DA scores and corresponding z-scores.	0
31349	31349	W18-3601	Mechanical	6	142	0.157894736842105	0.751322751322751	Readability scores for the same set of systems range from 78.7% to 41.3%, revealing that MTurk workers rate the Meaning Similarity between generated texts and corresponding reference sentences higher in general than Readability.	0
31350	31350	W18-3601	Mechanical	7	143	0.184210526315789	0.756613756613757	In order to investigate how Readability of system outputs compare to human-produced text, we included the original test sentences as a system in the Readability evaluation (for Meaning Similarity the notional score is 100%).	0
31351	31351	W18-3601	Mechanical	8	144	0.210526315789474	0.761904761904762	Unsurprisingly, human text achieves the highest score in terms of Readability (78.7%) but is quite closely followed by the best performing system in terms of Readability, ADAPT (73.9%).	0
31352	31352	W18-3601	Mechanical	9	145	0.236842105263158	0.767195767195767	Overall in the English Shallow Track, average DA scores for systems are close.	0
31353	31353	W18-3601	Mechanical	10	146	0.263157894736842	0.772486772486772	We tested for statistical significance of differences between average DA scores using a Wilcoxon rank sum test.	0
31354	31354	W18-3601	Mechanical	11	147	0.289473684210526	0.777777777777778	Figure 4 shows significance test results for each pair of systems participating in the English evaluation in the form of heatmaps where a green cell denotes a significantly higher average score for the system in that row over the system in that column, with a darker shade of green denoting a conclusion drawn with more certainty.	0
31355	31355	W18-3601	Mechanical	12	148	0.31578947368421	0.783068783068783	Results show that two entries are tied for first place in terms of Meaning Similarity, OSU and ADAPT, with the small difference in average scores proving not statistically significant.	0
31356	31356	W18-3601	Mechanical	13	149	0.342105263157895	0.788359788359788	In terms of Readability, however, the ADAPT sentences achieve a significantly higher readability score compared to OSU.	0
31357	31357	W18-3601	Mechanical	14	150	0.368421052631579	0.793650793650794	French: Table 7 shows average DA scores for systems participating in the French Shallow Track.	0
31358	31358	W18-3601	Mechanical	15	151	0.394736842105263	0.798941798941799	Meaning Similarity scores for French systems range from 72.9% to 48.6% with the Tilburg system achieving the highest overall score.	0
31359	31359	W18-3601	Mechanical	16	152	0.421052631578947	0.804232804232804	In terms of Readability, again Tilburg achieves the highest average score of 65.4%, with a considerable gap to the next best entry, OSU.	0
31360	31360	W18-3601	Mechanical	17	153	0.447368421052632	0.80952380952381	Compared to the human results, there is a larger gap than we saw for English outputs.	0
31361	31361	W18-3601	Mechanical	18	154	0.473684210526316	0.814814814814815	Figure 5 shows results of tests for statistical significance between average DA scores for systems in the French Shallow Track.	0
31362	31362	W18-3601	Mechanical	19	155	0.5	0.82010582010582	Tilburg achieves a significantly higher average DA score compared to all other systems in terms of both Meaning Similarity and Readability.	0
31363	31363	W18-3601	Mechanical	20	156	0.526315789473684	0.825396825396825	All systems are significantly worse in terms of Readability than the human authored texts.	0
31364	31364	W18-3601	Mechanical	21	157	0.552631578947368	0.830687830687831	Spanish: Table 8 shows average DA scores for systems participating in the Shallow Track for Spanish.	0
31365	31365	W18-3601	Mechanical	22	158	0.578947368421053	0.835978835978836	Meaning Similarity scores range from 77.3% to 43.9%, with OSU achieving the highest score.	0
31366	31366	W18-3601	Mechanical	23	159	0.605263157894737	0.841269841269841	In terms of Readability, the text produced by the systems ranges from 77.0% to 33.0%, and again OSU achieves the highest score.	0
31367	31367	W18-3601	Mechanical	24	160	0.631578947368421	0.846560846560847	Figure 6 shows results of the corresponding significance tests: OSU significantly outperforms all other participating systems with respect to both evaluation criteria.	0
31368	31368	W18-3601	Mechanical	25	161	0.657894736842105	0.851851851851852	Human-generated texts are significantly more readable than all system outputs.	0
31369	31369	W18-3601	Mechanical	26	162	0.68421052631579	0.857142857142857	MTurk DA quality control: Only 31% of workers passed quality control (being able to replicate scores for same sentences and scoring damaged sentences lower, for full details see Bojar et al., 2017a), highlighting the danger of crowdsourcing without good quality control measures.	0
31370	31370	W18-3601	Mechanical	27	163	0.710526315789474	0.862433862433862	The remaining 69%, who did not meet this criterion, were omitted from computation of the of-ficial DA results above.	0
31371	31371	W18-3601	Mechanical	28	164	0.736842105263158	0.867724867724868	Of those 31% included in the evaluation, a very high proportion, 97%, showed no significant difference in scores collected in repeated assessment of the same sentences; these high levels of agreement are consistent with what we have seen in DA used for Machine Translation (Graham et al., 2016) and Video Captioning evaluation (Graham et al., 2017).	0
31372	31372	W18-3601	Mechanical	29	165	0.763157894736842	0.873015873015873	Agreement with automatic metrics: Table 9 shows Pearson correlations between MTurk DA scores and automatic metric scores in the English, French and Spanish shallow tracks.	0
31373	31373	W18-3601	Mechanical	30	166	0.789473684210526	0.878306878306878	Overall, BLEU agrees most consistently across the different tasks, achieving a correlation above 0.95 in all settings, whereas the correlation of NIST scores with human Meaning Similarity scores is just 0.854 for French, while DIST scores correlate with human Readability scores at just 0.831 for English.	0
31374	31374	W18-3601	Mechanical	31	167	0.81578947368421	0.883597883597884	Conclusions from metric correlations should be drawn with a degree of caution, since in all cases the sample size from which we compute correlations is small, 8 systems for English, 5 for French, and 6 for Spanish.	0
31375	31375	W18-3601	Mechanical	32	168	0.842105263157895	0.888888888888889	We carried out significance tests to investigate to what degree differences in correlations are likely to occur by chance.	0
31376	31376	W18-3601	Mechanical	33	169	0.868421052631579	0.894179894179894	In order to take into account the fact that we are comparing correlations between human assessment and competing pairs of metrics (where metric scores themselves correlate with each other), we apply a Williams test for significance of differences in dependent correlations, as done in evaluation of Machine Translation metrics (Graham and Baldwin, 2014;	0
31377	31377	W18-3601	Mechanical	34	170	0.894736842105263	0.899470899470899	Bojar et al., 2017b).	0
31378	31378	W18-3601	Mechanical	35	171	0.921052631578947	0.904761904761905	Results are shown in Table 9.	0
31379	31379	W18-3601	Mechanical	36	172	0.947368421052632	0.91005291005291	Correlations between metrics and human assessment in bold are not significantly lower than any other metric.	0
31380	31380	W18-3601	Mechanical	37	173	0.973684210526316	0.915343915343915	As can be seen from Table 9, there is no significant difference between any of the three metrics in terms of correlation with human assessment in both the French and Spanish tracks.	0
31381	31381	W18-3601	Mechanical	38	174	1.0	0.920634920634921	In the English track, however, the correlation of BLEU and NIST scores with human assessment are significantly higher than that of DIST.	0
31382	31382	W18-3601	Google Data Compute results	1	175	0.125	0.925925925925926	Table 10 shows the results for the English assessment conducted via the Google Data Compute (GDC) evaluation service with expert evaluators.	0
31383	31383	W18-3601	Google Data Compute results	2	176	0.25	0.931216931216931	One difference between the MTurk and the Google results is the range of scores, which for     Meaning Similarity range from 67 to 86.9 for MTurk, compared to 52 to 86.1 for GDC.	0
31384	31384	W18-3601	Google Data Compute results	3	177	0.375	0.936507936507936	The latter is a wider range of scores, and expert evaluators' scores distinguish between systems more clearly than the crowdsourced scores which place the top four systems very close together.	0
31385	31385	W18-3601	Google Data Compute results	4	178	0.5	0.941798941798942	Readability scores range from 41.3 to 78.7 for MTurk, and from 60.2 to 88.2 for GDC.	0
31386	31386	W18-3601	Google Data Compute results	5	179	0.625	0.947089947089947	The expert evaluators tended to assign higher scores overall, but their range and the way they distinguish between systems is similar.	0
31387	31387	W18-3601	Google Data Compute results	6	180	0.75	0.952380952380952	For example, neither evaluation found much difference for the bottom two systems.	0
31388	31388	W18-3601	Google Data Compute results	7	181	0.875	0.957671957671958	The rank order of systems in the two separate evaluations is identical.	0
31389	31389	W18-3601	Google Data Compute results	8	182	1.0	0.962962962962963	Table 11 shows the Pearson correlation of scores for systems in the evaluations, where meaning similarity scores correlate almost perfectly at 0.997 (raw %) and 0.993 (z) and readability at 0.986 (raw %) and 0.985 (z).	0
31390	31390	W18-3601	Conclusion	1	183	0.142857142857143	0.968253968253968	SR'18 was the second surface realisation shared task, and followed an earlier pilot task for English, SR'11.	0
31391	31391	W18-3601	Conclusion	2	184	0.285714285714286	0.973544973544973	Participation was high for a first instance   of a shared task, at least in the Shallow Track, indicating that interest is high enough to continue running it again next year to enable more teams to participate.	0
31392	31392	W18-3601	Conclusion	3	185	0.428571428571429	0.978835978835979	One important question that needs to be addressed is to what extent UDs are suitable inputs for NLG systems.	0
31393	31393	W18-3601	Conclusion	4	186	0.571428571428571	0.984126984126984	More specifically, can they reasonably be expected to be generated by other, content-determining, modules in an NLG system, do they provide all the information necessary to generate surface realisations, and if not, how can they be augmented to provide it.	0
31394	31394	W18-3601	Conclusion	5	187	0.714285714285714	0.989417989417989	We hope to discuss these and related issues with the research community as we prepare the next instance of the SR Task.	0
31395	31395	W18-3601	Conclusion	6	188	0.857142857142857	0.994708994708995	A goal to aim for may be to make it possible for different NLG components to be connected via standard interface representations, to increase re-usability for NLG components.	0
31396	31396	W18-3601	Conclusion	7	189	1.0	1.0	However, what may constitute a good interface representation for surface realisation remains far from clear.	0
31515	31515	W19-3203	title	1	1	0.5	0.007692307692308	Overview of the Fourth Social Media Mining for Health (#SMM4H)	0
31516	31516	W19-3203	title	2	2	1.0	0.015384615384615	Shared Task at ACL 2019	0
31517	31517	W19-3203	abstract	1	3	0.111111111111111	0.023076923076923	The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking on a regular basis 1 . Advances in automated data processing and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media.	0
31518	31518	W19-3203	abstract	2	4	0.222222222222222	0.030769230769231	We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users.	0
31519	31519	W19-3203	abstract	3	5	0.333333333333333	0.038461538461539	For the fourth execution of this challenge, we proposed four different tasks.	0
31520	31520	W19-3203	abstract	4	6	0.444444444444444	0.046153846153846	Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not.	1
31521	31521	W19-3203	abstract	5	7	0.555555555555556	0.053846153846154	Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs.	1
31522	31522	W19-3203	abstract	6	8	0.666666666666667	0.061538461538462	Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary.	1
31523	31523	W19-3203	abstract	7	9	0.777777777777778	0.069230769230769	Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one's health, a more general discussion of the health issue, or is an unrelated mention.	1
31524	31524	W19-3203	abstract	8	10	0.888888888888889	0.076923076923077	A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run.	0
31525	31525	W19-3203	abstract	9	11	1.0	0.084615384615385	We summarize here the corpora for this challenge which are freely available at https://competitions.codalab. org/competitions/22521, and present an overview of the methods and the results of the competing systems.	0
31526	31526	W19-3203	Introduction	1	12	0.037037037037037	0.092307692307692	The intent of the #SMM4H shared tasks series is to challenge the community with Natural Language Processing tasks for mining relevant data for health monitoring and surveillance in social media.	0
31527	31527	W19-3203	Introduction	2	13	0.074074074074074	0.1	Such challenges require processing imbalanced, noisy, real-world, and substantially creative language expressions from social media.	0
31528	31528	W19-3203	Introduction	3	14	0.111111111111111	0.107692307692308	The competing systems should be able to deal with many linguistic variations and semantic complexities in the various ways people express medication-related concepts and outcomes.	0
31529	31529	W19-3203	Introduction	4	15	0.148148148148148	0.115384615384615	It has been shown in past research (Liu et al., 2011;	0
31530	31530	W19-3203	Introduction	5	16	0.185185185185185	0.123076923076923	Giuseppe et al., 2017) that automated systems frequently under-perform when exposed to social media text because of the presence of novel/creative phrases, misspellings and frequent use of idiomatic, ambiguous and sarcastic expressions.	0
31531	31531	W19-3203	Introduction	6	17	0.222222222222222	0.130769230769231	The tasks act as a discovery and verification process of what approaches work best for social media data.	0
31532	31532	W19-3203	Introduction	7	18	0.259259259259259	0.138461538461538	As in previous years, our tasks focused on mining health information from Twitter.	0
31533	31533	W19-3203	Introduction	8	19	0.296296296296296	0.146153846153846	This year we challenged the community with two different problems.	0
31534	31534	W19-3203	Introduction	9	20	0.333333333333333	0.153846153846154	The first problem focuses on performing pharmacovigilance from social media data.	0
31535	31535	W19-3203	Introduction	10	21	0.37037037037037	0.161538461538462	It is now well understood that social media data may contain reports of adverse drug reactions (ADRs) and these reports may complement traditional adverse event reporting systems, such as the FDA adverse event reporting system (FAERS).	0
31536	31536	W19-3203	Introduction	11	22	0.407407407407407	0.169230769230769	However, automatically curating reports from adverse reactions from Twitter requires the application of a series of NLP methods in an end-to-end pipeline .	0
31537	31537	W19-3203	Introduction	12	23	0.444444444444444	0.176923076923077	The first three tasks of this year's challenge represent three key NLP problems in a social media based pharmacovigilance pipeline -(i) automatic classification of ADRs, (ii) extraction of spans of ADRs and (iii) normal-ization of the extracted ADRs to standardized IDs.	0
31538	31538	W19-3203	Introduction	13	24	0.481481481481481	0.184615384615385	The second problem explores the generalizability of predictive models.	0
31539	31539	W19-3203	Introduction	14	25	0.518518518518518	0.192307692307692	In health research using social media, it is often necessary for researchers to build individual classifiers to identify health mentions of a particular disease in a particular context.	0
31540	31540	W19-3203	Introduction	15	26	0.555555555555556	0.2	Classification models that can generalize to different health contexts would be greatly beneficial to researchers in these fields (e.g., (Payam and Eugene, 2018)), as this would allow researchers to more easily apply existing tools and resources to new problems.	0
31541	31541	W19-3203	Introduction	16	27	0.592592592592593	0.207692307692308	Motivated by these ideas, Task 4 was testing tweet classification methods across diverse health contexts, so the test data included a very different health context than the training data.	0
31542	31542	W19-3203	Introduction	17	28	0.62962962962963	0.215384615384615	This setting measures the ability of tweet classifiers to generalize across health contexts.	0
31543	31543	W19-3203	Introduction	18	29	0.666666666666667	0.223076923076923	The fourth iteration of our series follows the same organization as previous iterations.	0
31544	31544	W19-3203	Introduction	19	30	0.703703703703704	0.230769230769231	We collected posts from Twitter, annotated the data for the four tasks proposed and released the posts to the registered teams.	0
31545	31545	W19-3203	Introduction	20	31	0.740740740740741	0.238461538461538	This year, we conducted the evaluation of all participating systems using Codalab, an open source platform facilitating data science competitions.	0
31546	31546	W19-3203	Introduction	21	32	0.777777777777778	0.246153846153846	The performances of the systems were compared on a blind evaluations sets for each task.	0
31547	31547	W19-3203	Introduction	22	33	0.814814814814815	0.253846153846154	All teams registered were allowed to participate to one or multiple tasks.	0
31548	31548	W19-3203	Introduction	23	34	0.851851851851852	0.261538461538462	We provided the participants with two sets of data for each task, a training and a test set.	0
31549	31549	W19-3203	Introduction	24	35	0.888888888888889	0.269230769230769	Participants had a period of six weeks, from March 5 th to April 15 th , for training their systems on our training sets, and 4 days, from the 16 th to 20 th of April, for calibrating their systems on our test sets and submitting their predictions.	0
31550	31550	W19-3203	Introduction	25	36	0.925925925925926	0.276923076923077	In total 34 teams registered and 19 teams submitted at least one run (each team was allowed to submit, at most, three runs per task).	0
31551	31551	W19-3203	Introduction	26	37	0.962962962962963	0.284615384615385	In detail, we received 43 runs for task 1, 24 for task 2, 10 for task 3 and 15 for task 4.	0
31552	31552	W19-3203	Introduction	27	38	1.0	0.292307692307692	We briefly describe each task and their data in section 2, before discussing the results obtained in section 3.	0
31553	31553	W19-3203	Task Descriptions	1	39	0.055555555555556	0.3	Tasks	0
31554	31554	W19-3203	Task Descriptions	2	40	0.111111111111111	0.307692307692308	Task 1: Automatic classification of tweets mentioning an ADR.	0
31555	31555	W19-3203	Task Descriptions	3	41	0.166666666666667	0.315384615384615	This is a binary classification task for which systems are required to predict if a tweet mentions an ADR or not.	0
31556	31556	W19-3203	Task Descriptions	4	42	0.222222222222222	0.323076923076923	In an end-to-end social media based pharmacovigilance pipeline, such a system is needed after data collection to filter out the large volume of medication-related chatter that is not a mention of an ADR.	0
31557	31557	W19-3203	Task Descriptions	5	43	0.277777777777778	0.330769230769231	This task is a rerun of the popular classification task organized in past years.	0
31558	31558	W19-3203	Task Descriptions	6	44	0.333333333333333	0.338461538461538	Task 2: Automatic extraction of ADR mentions from tweets.	0
31559	31559	W19-3203	Task Descriptions	7	45	0.388888888888889	0.346153846153846	This is a named entity recognition (NER) task that typically follows the ADR classification step (Task 1) in an ADR extraction pipeline.	0
31560	31560	W19-3203	Task Descriptions	8	46	0.444444444444444	0.353846153846154	Given a set of tweets containing drug mentions and potentially containing ADRs, the objective was to determine the span of the ADR mention, if any.	0
31561	31561	W19-3203	Task Descriptions	9	47	0.5	0.361538461538461	ADRs are rare events making ADR classification a challenging task with an F1score in the vicinity of 0.5 (based on previous shared task results (Weissenbacher et al., 2018)) for the ADR class.	0
31562	31562	W19-3203	Task Descriptions	10	48	0.555555555555556	0.369230769230769	The dataset for the ADR extraction task contains tweets that are both positive and negative for the presence of ADRs.	0
31563	31563	W19-3203	Task Descriptions	11	49	0.611111111111111	0.376923076923077	This allowed participants to choose to train their systems on either the set of tweets containing ADRs or include tweets that were negative for the presence of ADRs.	0
31564	31564	W19-3203	Task Descriptions	12	50	0.666666666666667	0.384615384615385	Task 3: Automatic extraction of ADR mentions and normalization of extracted ADRs to Med-DRA preferred term identifiers.	0
31565	31565	W19-3203	Task Descriptions	13	51	0.722222222222222	0.392307692307692	This is an extension of Task 2 consisting of the combination of NER and entity normalization tasks: a named entity resolution task.	0
31566	31566	W19-3203	Task Descriptions	14	52	0.777777777777778	0.4	In this task, given the same set of tweets as in Task 2, the objective was to extract the span of an ADR mention and to normalize it to MedDRA identifiers 2 . MedDRA (Medical Dictionary for Regulatory Activities), which is the standard nomenclature for monitoring medical products, and includes diseases, disorders, signs, symptoms, adverse events or adverse drug reactions.	0
31567	31567	W19-3203	Task Descriptions	15	53	0.833333333333333	0.407692307692308	For the normalization task, MedDRA version 21.1 was used, containing 79,507 lower level terms (LLTs) and 23,389 respective preferred terms (PTs).	0
31568	31568	W19-3203	Task Descriptions	16	54	0.888888888888889	0.415384615384615	Task 4: Automatic classification of personal mentions of health.	0
31569	31569	W19-3203	Task Descriptions	17	55	0.944444444444444	0.423076923076923	In this binary classification task, the systems were required to distinguish tweets of personal health status or opinions across different health domains.	0
31570	31570	W19-3203	Task Descriptions	18	56	1.0	0.430769230769231	The proposed task was intended to provide a baseline understanding of the ability to identify personal health mentions in a generalized context.	0
31571	31571	W19-3203	Data	1	57	0.035714285714286	0.438461538461538	All corpora were composed of public tweets downloaded using the official streaming API provided by Twitter and made available to the participants in accordance with Twitter's data use policy.	0
31572	31572	W19-3203	Data	2	58	0.071428571428572	0.446153846153846	This study received an exempt determination by the Institutional Review Board of the University of Pennsylvania.	0
31573	31573	W19-3203	Data	3	59	0.107142857142857	0.453846153846154	Task 1.	0
31574	31574	W19-3203	Data	4	60	0.142857142857143	0.461538461538462	For training, participants were provided with all the tweets from the #SMM4H 2017 shared tasks , which are publicly available at: https://data.mendeley. com/datasets/rxwfb3tysd/2. A total of 25,678 tweets were made available for training.	0
31575	31575	W19-3203	Data	5	61	0.178571428571429	0.469230769230769	The test set consisted of 4575 tweets with 626 (13.7%) tweets representing ADRs.	0
31576	31576	W19-3203	Data	6	62	0.214285714285714	0.476923076923077	The evaluation metric for this task was micro-averaged F1score for the ADR class.	0
31577	31577	W19-3203	Data	7	63	0.25	0.484615384615385	Task 2. Participants of Task 2 were provided with a training set containing 2276 tweets which mentioned at least one drug name.	0
31578	31578	W19-3203	Data	8	64	0.285714285714286	0.492307692307692	The dataset contained 1300 tweets that were positive for the presence of ADRs and 976 tweets that were negative.	0
31579	31579	W19-3203	Data	9	65	0.321428571428571	0.5	Participants were allowed to include additional negative instances from Task 1 for training purposes.	0
31580	31580	W19-3203	Data	10	66	0.357142857142857	0.507692307692308	Positive tweets were annotated with the start and end indices of the ADRs and the corresponding span text in the tweets.	0
31581	31581	W19-3203	Data	11	67	0.392857142857143	0.515384615384615	The evaluation set contained 1573 tweets, 785 and 788 tweets were positive and negative for the presence of ADRs respectively.	0
31582	31582	W19-3203	Data	12	68	0.428571428571429	0.523076923076923	The participants were asked to submit outputs from their systems that contained the predicted start and end indices of ADRs.	0
31583	31583	W19-3203	Data	13	69	0.464285714285714	0.530769230769231	The participants' submissions were evaluated using standard strict and overlapping F1-scores for extracted ADRs.	0
31584	31584	W19-3203	Data	14	70	0.5	0.538461538461538	Under strict mode of evaluation, ADR spans were considered correct only if both start and end indices matched with the indices in our gold standard annotations.	0
31585	31585	W19-3203	Data	15	71	0.535714285714286	0.546153846153846	Under overlapping mode of evaluation, ADR spans were considered correct only if spans in predicted annotations overlapped with our gold standard annotations.	0
31586	31586	W19-3203	Data	16	72	0.571428571428571	0.553846153846154	Task 3.	0
31587	31587	W19-3203	Data	17	73	0.607142857142857	0.561538461538462	Participants were provided with the same training and evaluation datasets as in Task 2.	0
31588	31588	W19-3203	Data	18	74	0.642857142857143	0.569230769230769	However, the datasets contained additional columns for the MedDRA annotated LLT and PT identifiers for each ADR mention.	0
31589	31589	W19-3203	Data	19	75	0.678571428571429	0.576923076923077	In total, of the 79,507 LLT and 23,389 PT identifiers available in MedDRA, the training set of 2276 tweets and 1832 annotated ADRs contained 490 unique LLT iden-tifiers and 327 unique PT identifiers.	0
31590	31590	W19-3203	Data	20	76	0.714285714285714	0.584615384615385	The evaluation set contained 112 PT identifiers that were not present as part of the training set.	0
31591	31591	W19-3203	Data	21	77	0.75	0.592307692307692	The participants were asked to submit outputs containing the predicted start and end indices of ADRs and respective PT identifiers.	0
31592	31592	W19-3203	Data	22	78	0.785714285714286	0.6	Although the training dataset contained annotations at the LLT level, the performance was only evaluated at the higher PT level.	0
31593	31593	W19-3203	Data	23	79	0.821428571428571	0.607692307692308	The participants' submissions were evaluated using standard strict and overlapping F-scores for extracted ADRs and respective MedDRA identifiers.	0
31594	31594	W19-3203	Data	24	80	0.857142857142857	0.615384615384615	Under strict mode of evaluation, ADR spans were considered correct only if both start and end indices matched along with matching MedDRA PT identifiers.	0
31595	31595	W19-3203	Data	25	81	0.892857142857143	0.623076923076923	Under overlapping mode of evaluation, ADR spans were considered correct only if spans in predicted ADRs overlapped with gold standard ADR spans in addition to matching MedDRA PT identifiers.	0
31596	31596	W19-3203	Data	26	82	0.928571428571429	0.630769230769231	Task 4 Data.	0
31597	31597	W19-3203	Data	27	83	0.964285714285714	0.638461538461538	Participants were provided training data from one disease domain, influenza, across two contexts, being sick and getting vaccinated, both annotated for personal mentions: the user is personally sick or the user has been personally vaccinated.	0
31598	31598	W19-3203	Data	28	84	1.0	0.646153846153846	Test data included new tweets of personal health mentions about influenza and tweets from an additional disease domain, Zika virus, with two different contexts, the user is changing their travel plans in response to Zika concerns, or the user is minimizing potential mosquito exposure due to Zika concerns.	0
31599	31599	W19-3203	Annotation and Inter-Annotator Agreements	1	85	0.052631578947369	0.653846153846154	Two annotators with biomedical education and both experienced in Social Media research tasks manually annotated the corpora for tasks 1, 2 and 3.	0
31600	31600	W19-3203	Annotation and Inter-Annotator Agreements	2	86	0.105263157894737	0.661538461538462	Our annotators independently dual-annotated each test sets to insure the quality of our annotations.	0
31601	31601	W19-3203	Annotation and Inter-Annotator Agreements	3	87	0.157894736842105	0.669230769230769	Disagreement were resolved after an adjudication phase between our two annotators.	0
31602	31602	W19-3203	Annotation and Inter-Annotator Agreements	4	88	0.210526315789474	0.676923076923077	On task 1, the classification task, the inter annotatoragreement (IAA) was high with a Cohens Kappa = 0.82.	0
31603	31603	W19-3203	Annotation and Inter-Annotator Agreements	5	89	0.263157894736842	0.684615384615385	On task 2, the information extraction task, IAAs were good with and an F1-score of 0.73 for strict agreement, and 0.85 for overlapping agreement 3 .	0
31604	31604	W19-3203	Annotation and Inter-Annotator Agreements	6	90	0.31578947368421	0.692307692307692	On task 3, our annotators double annotated 535 of the extracted ADR terms and normalized them to MedDRA lower lever terms (LLT).	0
31605	31605	W19-3203	Annotation and Inter-Annotator Agreements	7	91	0.368421052631579	0.7	They achieved an agreement accuracy of 82.6%.	0
31606	31606	W19-3203	Annotation and Inter-Annotator Agreements	8	92	0.421052631578947	0.707692307692308	After converting the LLT to their corresponding preferred term (PT) in MedDRA, which is the coding the task was scored against, accuracy improved to 87.7% 4 . The annotation process followed for task 4 was slightly different due to the nature of the task.	0
31607	31607	W19-3203	Annotation and Inter-Annotator Agreements	9	93	0.473684210526316	0.715384615384615	We obtained the two datasets of our training set, focusing on flu vaccination and flu infection, from (Huang et al., 2017) and (Lamb et al., 2013) respectively.	0
31608	31608	W19-3203	Annotation and Inter-Annotator Agreements	10	94	0.526315789473684	0.723076923076923	Huang et al.	0
31609	31609	W19-3203	Annotation and Inter-Annotator Agreements	11	95	0.578947368421053	0.730769230769231	(Huang et al., 2017) used mechanical turk to crowdsource labels (Fleiss' kappa = 0.793).	0
31610	31610	W19-3203	Annotation and Inter-Annotator Agreements	12	96	0.631578947368421	0.738461538461539	Lamb et al.	0
31611	31611	W19-3203	Annotation and Inter-Annotator Agreements	13	97	0.68421052631579	0.746153846153846	(Lamb et al., 2013) did not report their labeling procedure or annotator agreement metrics, but do report annotation guidelines 5 . A few of the tweets released by Lamb et al. appeared to be mislabeled and were corrected in accordance with the annotation guidelines defined by the authors.	0
31612	31612	W19-3203	Annotation and Inter-Annotator Agreements	14	98	0.736842105263158	0.753846153846154	We obtained the test data for task 4 by compiling three datasets.	0
31613	31613	W19-3203	Annotation and Inter-Annotator Agreements	15	99	0.789473684210526	0.761538461538462	For the dataset related to travel changes due to Zika concerns, we selected a subset of data already available from (Daughton and Paul, 2019).	0
31614	31614	W19-3203	Annotation and Inter-Annotator Agreements	16	100	0.842105263157895	0.769230769230769	Initial labeling of these tweets was performed by two annotators with a public health background (Cohen's kappa = 0.66).	0
31615	31615	W19-3203	Annotation and Inter-Annotator Agreements	17	101	0.894736842105263	0.776923076923077	We reuse the original annotations for this dataset without changes.	0
31616	31616	W19-3203	Annotation and Inter-Annotator Agreements	18	102	0.947368421052632	0.784615384615385	For the mosquito exposure dataset, tweets were labeled by one annotator with public health knowledge and experienced with social media, and then verified by a second annotator with similar experience.	0
31617	31617	W19-3203	Annotation and Inter-Annotator Agreements	19	103	1.0	0.792307692307692	The additional set of data on personal exposure to Influenza were obtained from a separate group, who used an independent labeling procedure.	0
31618	31618	W19-3203	Results	1	104	0.047619047619048	0.8	The challenge received a solid response with 19 teams from 12 countries (7 from North America, 1 from South America, 6 from Asia and 5 from Europe) submitting 92 runs in total in one or more tasks.	0
31619	31619	W19-3203	Results	2	105	0.095238095238095	0.807692307692308	We present an overview of all architectures competing in the different tasks in Table 1, 2, 3, 4.	0
31620	31620	W19-3203	Results	3	106	0.142857142857143	0.815384615384615	We also list in these tables the external resources competitors integrated for improving the pre-training of their systems or for embedding high-level features to help decision-making.	0
31621	31621	W19-3203	Results	4	107	0.19047619047619	0.823076923076923	The overview of all architectures is interesting in two ways.	0
31622	31622	W19-3203	Results	5	108	0.238095238095238	0.830769230769231	First, this challenge confirms the tendency of the community to abandon traditional Machine Learning systems based on handcrafted features for deep learning architectures capable of discovering the features relevant for the task at hand from pre-trained embeddings.	0
31623	31623	W19-3203	Results	6	109	0.285714285714286	0.838461538461539	During the challenge, when participants implemented traditional systems, such as SVM or CRF, they used such systems as baselines and, observing significant differences of performances with systems based on deep learning on their validation sets, most of them did not submit their predictions as official runs.	0
31624	31624	W19-3203	Results	7	110	0.333333333333333	0.846153846153846	"Second, while last year convolutional or recurrent neural networks ""fed"" with pretrained word embeddings learned on local windows of words (e.g. word2vec, GloVe) were the most popular architectures, this year we can see a clear dominance of neural architectures using word embeddings pre-trained with the Bidirectional Encoder Representations from Transformers (BERT) proposed by (Devlin et al., 2018), or fine-tuning these words embeddings on our training corpora."	0
31625	31625	W19-3203	Results	8	111	0.380952380952381	0.853846153846154	BERT allows to compute words embeddings based on the full context of sentences and not only on local windows.	0
31626	31626	W19-3203	Results	9	112	0.428571428571429	0.861538461538462	A notable result from task 1-3 is that, despite an improvement in performances for the detection of ADRs, their resolution remains challenging and will require further research.	0
31627	31627	W19-3203	Results	10	113	0.476190476190476	0.869230769230769	The participants largely adopted contextual word-embeddings during this challenge, a choice rewarded by new records in performances during the task 1, the only task reran from last years.	0
31628	31628	W19-3203	Results	11	114	0.523809523809524	0.876923076923077	The performances increased from .522 F1-score (.442 P, .636 R) (Weissenbacher et al., 2018) to .646 F1-score (0.608 P, 0.689 R) for the best systems of each years.	0
31629	31629	W19-3203	Results	12	115	0.571428571428571	0.884615384615385	However, with a strict matching F1-score of .432 (.362 P, .535 R) for the best system, the performances obtained in task 3 for ADRs resolution are still low and human inspection is still required to make use of the data extracted automatically.	0
31630	31630	W19-3203	Results	13	116	0.619047619047619	0.892307692307692	As shown by the best score of .887 Accuracy obtained on the ADR normalization in task 3 ran during #SMM4H in 2017 (Sarker et al., 2018) 6 , once ADRs are extracted, the normalization of the ADRs can be per-formed with a good reliability.	0
31631	31631	W19-3203	Results	14	117	0.666666666666667	0.9	However errors are made during all steps of the resolution -detection, extraction, normalization -and their overall accumulation render current automatic systems inefficient.	0
31632	31632	W19-3203	Results	15	118	0.714285714285714	0.907692307692308	Note that bulk of the errors are made during the extraction of the ADRs, as shown by the low strict F1-score of the best system in task 2, .464 F1-score (.389P, .576 R).	0
31633	31633	W19-3203	Results	16	119	0.761904761904762	0.915384615384615	For task 4, we were especially interested in the generalizability of first person health classifiers to a domain separate from that of the training data.	0
31634	31634	W19-3203	Results	17	120	0.80952380952381	0.923076923076923	We find that, on average, teams do reasonably well across the full test dataset (average F1-score: 0.70, range: 0.41-0.87).	0
31635	31635	W19-3203	Results	18	121	0.857142857142857	0.930769230769231	Unsurprisingly, classifiers tended to do better on a test set in the same domain as the training dataset (context 1, average F1-score: 0.82) and more modestly on the Zika travel and mosquito datasets (average F1-score: 0.40 and 0.52, respectively).	0
31636	31636	W19-3203	Results	19	122	0.904761904761905	0.938461538461538	Interestingly, in all contexts, precision was higher than recall.	0
31637	31637	W19-3203	Results	20	123	0.952380952380952	0.946153846153846	We note that both the training and the testing data were limited in quantity, and that classifiers would likely improve with more data.	0
31638	31638	W19-3203	Results	21	124	1.0	0.953846153846154	However, in general, it is encouraging that classifiers trained in one health domain can be applied to separate health domains.	0
31639	31639	W19-3203	Conclusion	1	125	0.166666666666667	0.961538461538462	In this paper we presented an overview of the results of #SMM4H 2019 which focuses on a) the resolution of adverse drug reaction (ADR) mentioned in Twitter and b) the distinction between tweets reporting personal health status form opinions across different health domains.	0
31640	31640	W19-3203	Conclusion	2	126	0.333333333333333	0.969230769230769	With a total of 92 runs submitted by 19 teams, the challenge was well attended.	0
31641	31641	W19-3203	Conclusion	3	127	0.5	0.976923076923077	The participants, in large part, opted for neural architectures and integrated pretrained word-embedding sensitive to their contexts based on the recent Bidirectional Encoder Representations from Transformers.	0
31642	31642	W19-3203	Conclusion	4	128	0.666666666666667	0.984615384615385	Such architectures were the most efficient on our four tasks.	0
31643	31643	W19-3203	Conclusion	5	129	0.833333333333333	0.992307692307692	Results on tasks 1-3 show that, despite a continuous improvement of performances in the detection of tweets mentioning ADRs over the past years, their end-to-end resolution still remain a major challenge for the community and an opportunity for further research.	0
31644	31644	W19-3203	Conclusion	6	130	1.0	1.0	Results of task 4 were more encouraging, with systems able to generalized their predictions over domains not present in their training data.	0
32732	32732	2020.sigmorphon-1.1	title	1	1	1.0	0.002724795640327	SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection	0
32733	32733	2020.sigmorphon-1.1	abstract	1	2	0.111111111111111	0.005449591280654	A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language.	0
32734	32734	2020.sigmorphon-1.1	abstract	2	3	0.222222222222222	0.008174386920981	Most systems, however, are developed using data from just one language such as English.	0
32735	32735	2020.sigmorphon-1.1	abstract	3	4	0.333333333333333	0.010899182561308	The SIG-MORPHON 2020 shared task on morphological reinflection aims to investigate systems' ability to generalize across typologically distinct languages, many of which are low resource.	0
32736	32736	2020.sigmorphon-1.1	abstract	4	5	0.444444444444444	0.013623978201635	Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages.	0
32737	32737	2020.sigmorphon-1.1	abstract	5	6	0.555555555555556	0.016348773841962	A total of 22 systems (19 neural) from 10 teams were submitted to the task.	0
32738	32738	2020.sigmorphon-1.1	abstract	6	7	0.666666666666667	0.019073569482289	All four winning systems were neural (two monolingual transformers and two massively multilingual RNNbased models with gated attention).	0
32739	32739	2020.sigmorphon-1.1	abstract	7	8	0.777777777777778	0.021798365122616	Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages.	0
32740	32740	2020.sigmorphon-1.1	abstract	8	9	0.888888888888889	0.024523160762943	Nonneural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data.	0
32741	32741	2020.sigmorphon-1.1	abstract	9	10	1.0	0.02724795640327	Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.	0
32742	32742	2020.sigmorphon-1.1	Introduction	1	11	0.03030303030303	0.029972752043597	Human language is marked by considerable diversity around the world.	0
32743	32743	2020.sigmorphon-1.1	Introduction	2	12	0.060606060606061	0.032697547683924	Though the world's languages share many basic attributes (e.g., Swadesh, 1950 and more recently, List et al., 2016), grammatical features, and even abstract implications (proposed in Greenberg, 1963), each language nevertheless has a unique evolutionary trajectory that is affected by geographic, social, cultural, and other factors.	0
32744	32744	2020.sigmorphon-1.1	Introduction	3	13	0.090909090909091	0.035422343324251	As a result, the surface form of languages varies substantially.	0
32745	32745	2020.sigmorphon-1.1	Introduction	4	14	0.121212121212121	0.038147138964578	The morphology of languages can differ in many ways:	0
32746	32746	2020.sigmorphon-1.1	Introduction	5	15	0.151515151515152	0.040871934604905	"Some exhibit rich grammatical case systems (e.g., 12 in Erzya and 24 in Veps) and mark possessiveness, others might have complex verbal morphology (e.g., Oto-Manguean languages; Palancar and Léonard, 2016) or even ""decline"" nouns for tense (e.g., Tupi-Guarani languages)."	0
32747	32747	2020.sigmorphon-1.1	Introduction	6	16	0.181818181818182	0.043596730245232	Linguistic typology is the discipline that studies these variations by means of a systematic comparison of languages (Croft, 2002;	0
32748	32748	2020.sigmorphon-1.1	Introduction	7	17	0.212121212121212	0.046321525885559	Comrie, 1989).	0
32749	32749	2020.sigmorphon-1.1	Introduction	8	18	0.242424242424242	0.049046321525886	Typologists have defined several dimensions of morphological variation to classify and quantify the degree of crosslinguistic variation.	0
32750	32750	2020.sigmorphon-1.1	Introduction	9	19	0.272727272727273	0.051771117166213	This comparison can be challenging as the categories are based on studies of known languages and are progressively refined with documentation of new languages (Haspelmath, 2007).	0
32751	32751	2020.sigmorphon-1.1	Introduction	10	20	0.303030303030303	0.05449591280654	Nevertheless, to understand the potential range of morphological variation, we take a closer look at three dimensions here: fusion, inflectional synthesis, and position of case affixes (Dryer and Haspelmath, 2013).	0
32752	32752	2020.sigmorphon-1.1	Introduction	11	21	0.333333333333333	0.057220708446867	Fusion, our first dimension of variation, refers to the degree to which morphemes bind to one another in a phonological word (Bickel and Nichols, 2013b).	0
32753	32753	2020.sigmorphon-1.1	Introduction	12	22	0.363636363636364	0.059945504087194	Languages range from strictly isolating (i.e., each morpheme is its own phonological word) to concatenative (i.e., morphemes bind together within a phonological word); nonlinearities such as ablaut or tonal morphology can also be present.	0
32754	32754	2020.sigmorphon-1.1	Introduction	13	23	0.393939393939394	0.062670299727521	From a geographic perspective, isolating languages are found in the Sahel Belt in West Africa, Southeast Asia and the Pacific.	0
32755	32755	2020.sigmorphon-1.1	Introduction	14	24	0.424242424242424	0.065395095367847	Ablaut-concatenative morphology and tonal morphology can be found in African languages.	0
32756	32756	2020.sigmorphon-1.1	Introduction	15	25	0.454545454545455	0.068119891008174	Tonal-concatenative morphology can be found in Mesoamerican languages (e.g., Oto-Manguean).	0
32757	32757	2020.sigmorphon-1.1	Introduction	16	26	0.484848484848485	0.070844686648501	Concatenative morphology is the most common system and can be found around the world.	0
32758	32758	2020.sigmorphon-1.1	Introduction	17	27	0.515151515151515	0.073569482288828	Inflectional synthesis, the second dimension considered, refers to whether grammatical categories like tense, voice or agreement are expressed as affixes (synthetic) or individual words (analytic) (Bickel and Nichols, 2013c).	0
32759	32759	2020.sigmorphon-1.1	Introduction	18	28	0.545454545454545	0.076294277929155	Analytic expressions are common in Eurasia (except the Pacific Rim, and the Himalaya and Caucasus mountain ranges), whereas synthetic expressions are used to a high degree in the Americas.	0
32760	32760	2020.sigmorphon-1.1	Introduction	19	29	0.575757575757576	0.079019073569482	Finally, affixes can variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013).	0
32761	32761	2020.sigmorphon-1.1	Introduction	20	30	0.606060606060606	0.081743869209809	Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013).	0
32762	32762	2020.sigmorphon-1.1	Introduction	21	31	0.636363636363636	0.084468664850136	In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead.	0
32763	32763	2020.sigmorphon-1.1	Introduction	22	32	0.666666666666667	0.087193460490463	These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable.	0
32764	32764	2020.sigmorphon-1.1	Introduction	23	33	0.696969696969697	0.08991825613079	Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging.	0
32765	32765	2020.sigmorphon-1.1	Introduction	24	34	0.727272727272727	0.092643051771117	As Bender (2009	0
32766	32766	2020.sigmorphon-1.1	Introduction	25	35	0.757575757575758	0.095367847411444	Bender ( , 2016 notes, many current architectures and training and tuning algorithms still present language-specific biases.	0
32767	32767	2020.sigmorphon-1.1	Introduction	26	36	0.787878787878788	0.098092643051771	The most commonly used language for developing NLP applications is English.	0
32768	32768	2020.sigmorphon-1.1	Introduction	27	37	0.818181818181818	0.100817438692098	Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology.	0
32769	32769	2020.sigmorphon-1.1	Introduction	28	38	0.848484848484848	0.103542234332425	With respect to languages that exhibit inflectional morphology, English is relatively impoverished.	0
32770	32770	2020.sigmorphon-1.1	Introduction	29	39	0.878787878787879	0.106267029972752	1 Importantly, English is just one morphological system among many.	0
32771	32771	2020.sigmorphon-1.1	Introduction	30	40	0.909090909090909	0.108991825613079	A larger goal of natural language processing is that the system work for any presented language.	0
32772	32772	2020.sigmorphon-1.1	Introduction	31	41	0.939393939393939	0.111716621253406	If an NLP system is trained on just one language, it could be missing important flexibility in its ability to account for cross-linguistic morphological variation.	0
32773	32773	2020.sigmorphon-1.1	Introduction	32	42	0.96969696969697	0.114441416893733	In this year's iteration of the SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems' ability to generalize across typologically distinct languages many of which are low-resource.	0
32774	32774	2020.sigmorphon-1.1	Introduction	33	43	1.0	0.11716621253406	"For example, if a neural network architecture works well for a sample of Indo-European languages, should the same architecture also work well for Tupi-Guarani languages (where nouns are ""declined"" for tense) or Austronesian languages (where verbal morphology is frequently prefixing)?"	0
32775	32775	2020.sigmorphon-1.1	Task Description	1	44	0.066666666666667	0.119891008174387	The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form.	1
32776	32776	2020.sigmorphon-1.1	Task Description	2	45	0.133333333333333	0.122615803814714	For each language we provide a separate training, development, and test set.	0
32777	32777	2020.sigmorphon-1.1	Task Description	3	46	0.2	0.125340599455041	"More historically, all of these tasks resemble the classic ""wug""-test that Berko (1958) developed to test child and human knowledge of English nominal morphology."	0
32778	32778	2020.sigmorphon-1.1	Task Description	4	47	0.266666666666667	0.128065395095368	Unlike the task from earlier years, this year's task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces previously unseen data.	0
32779	32779	2020.sigmorphon-1.1	Task Description	5	48	0.333333333333333	0.130790190735695	The task starts with the Development Phase, which was an elongated period of time (about two months), during which participants develop a model of morphological inflection.	0
32780	32780	2020.sigmorphon-1.1	Task Description	6	49	0.4	0.133514986376022	In this phase, we provide training and development splits for 45 languages representing the Austronesian, Niger-Congo, Oto-Manguean, Uralic and Indo-European language families.	0
32781	32781	2020.sigmorphon-1.1	Task Description	7	50	0.466666666666667	0.136239782016349	Table 1 provides details on the languages.	0
32782	32782	2020.sigmorphon-1.1	Task Description	8	51	0.533333333333333	0.138964577656676	The Generalization Phase is a short period of time (it started about a week before the Evaluation Phase) during which participants fine-tune their models on new data.	0
32783	32783	2020.sigmorphon-1.1	Task Description	9	52	0.6	0.141689373297003	At the start of the phase, we provide training and development splits for 45 new languages where approximately half are genetically related (belong to the same family) and half are genetically unrelated (are isolates or belong to a different family) to the languages presented in the Development Phase.	0
32784	32784	2020.sigmorphon-1.1	Task Description	10	53	0.666666666666667	0.14441416893733	More specifically, we introduce (surprise) languages from Afro-Asiatic, Algic, Dravidian, Indo-European, Niger-Congo, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan families.	0
32785	32785	2020.sigmorphon-1.1	Task Description	11	54	0.733333333333333	0.147138964577657	See Table 2 for more details.	0
32786	32786	2020.sigmorphon-1.1	Task Description	12	55	0.8	0.149863760217984	Finally, test splits for all 90 languages are released in the Evaluation Phase.	0
32787	32787	2020.sigmorphon-1.1	Task Description	13	56	0.866666666666667	0.152588555858311	During this phase, the models are evaluated on held-out forms.	0
32788	32788	2020.sigmorphon-1.1	Task Description	14	57	0.933333333333333	0.155313351498638	Importantly, the languages from both previous phases are evaluated simultaneously.	0
32789	32789	2020.sigmorphon-1.1	Task Description	15	58	1.0	0.158038147138965	This way, we evaluate the extent to which models (especially those with shared parameters) overfit to the development data: a model based on the morphological patterning of the Indo-European languages may end up with a bias towards suffixing and will struggle to learn prefixing or infixation.	0
32790	32790	2020.sigmorphon-1.1	Meet our Languages	1	59	0.25	0.160762942779292	In the 2020 shared task we cover 15 language families: Afro-Asiatic, Algic, Austronesian, Dravidian, Indo-European, Niger-Congo, Oto-Manguean, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan.	0
32791	32791	2020.sigmorphon-1.1	Meet our Languages	2	60	0.5	0.163487738419619	2 Five language families were used for the Development phase while ten were held out for the Generalization phase.	0
32792	32792	2020.sigmorphon-1.1	Meet our Languages	3	61	0.75	0.166212534059946	Tab. 1 and Tab. 2 provide information on the languages, their families, and sources of data.	0
32793	32793	2020.sigmorphon-1.1	Meet our Languages	4	62	1.0	0.168937329700272	In the following section, we provide an overview of each language family's morphological system.	0
32794	32794	2020.sigmorphon-1.1	Afro-Asiatic	1	63	0.111111111111111	0.171662125340599	The Afro-Asiatic language family, consisting of six branches and over 300 languages, is among the largest language families in the world.	0
32795	32795	2020.sigmorphon-1.1	Afro-Asiatic	2	64	0.222222222222222	0.174386920980926	It is mainly spoken in Northern, Western and Central Africa as well as West Asia and spans large modern languages such as Arabic, in addition to ancient languages like Biblical Hebrew.	0
32796	32796	2020.sigmorphon-1.1	Afro-Asiatic	3	65	0.333333333333333	0.177111716621253	Similarly, some of its languages have a long tradition of written form, while others have yet to incorporate a writing system.	0
32797	32797	2020.sigmorphon-1.1	Afro-Asiatic	4	66	0.444444444444444	0.17983651226158	The six branches differ most notably in typology and syntax, with the Chadic language being the main source of differences, which has sparked discussion of the division of the family (Frajzyngier, 2018).	0
32798	32798	2020.sigmorphon-1.1	Afro-Asiatic	5	67	0.555555555555556	0.182561307901907	For example, in the Egyptian and Semitic branches, the root of a verb may not contain vowels, while this is allowed in Chadic.	0
32799	32799	2020.sigmorphon-1.1	Afro-Asiatic	6	68	0.666666666666667	0.185286103542234	Although only four of the six branches, excluding Chadic and Omotic, use a prefix and suffix in conjugation when adding a subject to a verb, it is con-sidered an important characteristic of the family.	0
32800	32800	2020.sigmorphon-1.1	Afro-Asiatic	7	69	0.777777777777778	0.188010899182561	In addition, some of the families in the phylum use tone to encode tense, modality and number among others.	0
32801	32801	2020.sigmorphon-1.1	Afro-Asiatic	8	70	0.888888888888889	0.190735694822888	However, all branches use objective and passive suffixes.	0
32802	32802	2020.sigmorphon-1.1	Afro-Asiatic	9	71	1.0	0.193460490463215	Markers of tense are generally simple, whereas aspect is typically distinguished with more elaborate systems.	0
32803	32803	2020.sigmorphon-1.1	Algic	1	72	0.2	0.196185286103542	The Algic family embraces languages native to North America-more specifically the United States and Canada-and contain three branches.	0
32804	32804	2020.sigmorphon-1.1	Algic	2	73	0.4	0.198910081743869	Of these, our sample contains Cree, the language from the largest genus, Algonquian, from which most languages are now extinct.	0
32805	32805	2020.sigmorphon-1.1	Algic	3	74	0.6	0.201634877384196	The Algonquian genus is characterized by its concatenative morphology.	0
32806	32806	2020.sigmorphon-1.1	Algic	4	75	0.8	0.204359673024523	Cree morphology is also concatenative and suffixing.	0
32807	32807	2020.sigmorphon-1.1	Algic	5	76	1.0	0.20708446866485	It distinguishes between impersonal and non-impersonal verbs and presents four apparent declension classes among non-impersonal verbs.	0
32808	32808	2020.sigmorphon-1.1	Austronesian	1	77	0.142857142857143	0.209809264305177	The Austronesian family of languages is largely comprised of languages from the Greater Central Philippine and Oceanic regions.	0
32809	32809	2020.sigmorphon-1.1	Austronesian	2	78	0.285714285714286	0.212534059945504	They are characterized by limited morphology, mostly prefixing in nature.	0
32810	32810	2020.sigmorphon-1.1	Austronesian	3	79	0.428571428571429	0.215258855585831	Additionally, tense-aspect affixes are predominantly seen as prefixes, though some suffixes are used.	0
32811	32811	2020.sigmorphon-1.1	Austronesian	4	80	0.571428571428571	0.217983651226158	In the general case, verbs do not mark number, person, or gender.	0
32812	32812	2020.sigmorphon-1.1	Austronesian	5	81	0.714285714285714	0.220708446866485	In Māori, verbs may be suffixed with a marker indicating the passive voice.	0
32813	32813	2020.sigmorphon-1.1	Austronesian	6	82	0.857142857142857	0.223433242506812	This marker takes the form of one of twelve endings.	0
32814	32814	2020.sigmorphon-1.1	Austronesian	7	83	1.0	0.226158038147139	These endings are difficult to predict as the language has undergone a loss of word-final consonants and there is no clear link between a stem and the passive suffix that it employs (Harlow, 2007).	0
32815	32815	2020.sigmorphon-1.1	Dravidian	1	84	0.333333333333333	0.228882833787466	Indo-European	0
32816	32816	2020.sigmorphon-1.1	Dravidian	2	85	0.666666666666667	0.231607629427793	Languages in the Indo-European family are native to most of Europe and a large part of Asia-with our sample including languages from the genera: Germanic, Indic, Iranian, and Romance.	0
32817	32817	2020.sigmorphon-1.1	Dravidian	3	86	1.0	0.23433242506812	This is (arguably) the most well studied language family, containing a few of the highest-resource languages in the world.	0
32818	32818	2020.sigmorphon-1.1	Romance	1	87	0.2	0.237057220708447	The Romance genus comprises of a set of fusional languages evolved from Latin.	0
32819	32819	2020.sigmorphon-1.1	Romance	2	88	0.4	0.239782016348774	They traditionally originated in Southern and Southeastern Europe, though they are presently spoken in other continents such Africa and the Americas.	0
32820	32820	2020.sigmorphon-1.1	Romance	3	89	0.6	0.242506811989101	Romance languages mark tense, person, number and mood in verbs, and gender and number in nouns.	0
32821	32821	2020.sigmorphon-1.1	Romance	4	90	0.8	0.245231607629428	Inflection is primarily achieved through suffixes, with some verbal person syncretism and suppletion for high-frequency verbs.	0
32822	32822	2020.sigmorphon-1.1	Romance	5	91	1.0	0.247956403269755	There is some morphological variation within the genus, such as French, which exhibits comparatively less inflection, and Romanian has comparatively more-it still marks case.	0
32823	32823	2020.sigmorphon-1.1	Germanic	1	92	0.166666666666667	0.250681198910082	The Germanic genus comprises several languages which originated in Northern and Northwestern Europe, and today are spoken in many parts of the world.	0
32824	32824	2020.sigmorphon-1.1	Germanic	2	93	0.333333333333333	0.253405994550409	Verbs in Germanic languages mark tense and mood, in many languages person and number are also marked, predominantly through suffixation.	0
32825	32825	2020.sigmorphon-1.1	Germanic	3	94	0.5	0.256130790190736	Some Germanic languages exhibit widespread Indo-European ablaut.	0
32826	32826	2020.sigmorphon-1.1	Germanic	4	95	0.666666666666667	0.258855585831063	The gendering of nouns differs between Germanic languages: German nouns can be masculine, feminine or neuter, while English nouns are not marked for gender.	0
32827	32827	2020.sigmorphon-1.1	Germanic	5	96	0.833333333333333	0.26158038147139	In Danish and Swedish, historically masculine and feminine nouns have merged to form one common gender, so nouns are either common or neuter.	0
32828	32828	2020.sigmorphon-1.1	Germanic	6	97	1.0	0.264305177111717	Marking of case also differs between the languages: German nouns have one of four cases and this case is marked in articles and adjectives as well as nouns and pronouns, while English does not mark noun case (although Old English, which also appears in our language sample, does).   no grammatical evidentials.	0
32829	32829	2020.sigmorphon-1.1	Indo-Iranian	1	98	0.090909090909091	0.267029972752044	Oto-Manguean	0
32830	32830	2020.sigmorphon-1.1	Indo-Iranian	2	99	0.181818181818182	0.269754768392371	The Oto-Manguean languages are a diverse family of tonal languages spoken in central and southern Mexico.	0
32831	32831	2020.sigmorphon-1.1	Indo-Iranian	3	100	0.272727272727273	0.272479564032698	Even though all of these languages are tonal, the tonal system within each language varies widely.	0
32832	32832	2020.sigmorphon-1.1	Indo-Iranian	4	101	0.363636363636364	0.275204359673025	Some have an inventory of two tones (e.g., Chichimec and Pame) others have ten tones (e.g., the Eastern Chatino languages of the Zapotecan branch, Palancar and Léonard (2016)).	0
32833	32833	2020.sigmorphon-1.1	Indo-Iranian	5	102	0.454545454545455	0.277929155313351	Oto-Manguean languages are also rich in tonal morphology.	0
32834	32834	2020.sigmorphon-1.1	Indo-Iranian	6	103	0.545454545454545	0.280653950953678	The inflectional system marks person-number and aspect in verbs and personnumber in adjectives and noun possessions, relying heavily on tonal contrasts.	0
32835	32835	2020.sigmorphon-1.1	Indo-Iranian	7	104	0.636363636363636	0.283378746594005	Other interesting as-pects of Oto-Manguean languages include the fact that pronominal inflections use a system of enclitics, and first and second person plural has a distinction between exclusive and inclusive (Campbell, 2016).	0
32836	32836	2020.sigmorphon-1.1	Indo-Iranian	8	105	0.727272727272727	0.286103542234332	Tone marking schemes in the writing systems also vary greatly.	0
32837	32837	2020.sigmorphon-1.1	Indo-Iranian	9	106	0.818181818181818	0.288828337874659	Some writing systems do not represent tone, others use diacritics, and others represent tones with numbers.	0
32838	32838	2020.sigmorphon-1.1	Indo-Iranian	10	107	0.909090909090909	0.291553133514986	In languages that use numbers, single digits represent level tones and double digits represent contour tones.	0
32839	32839	2020.sigmorphon-1.1	Indo-Iranian	11	108	1.0	0.294277929155313	For example, in San Juan Quiahije of Eastern Chatino number 1 represents high tone, number 4 represents low tone, and numbers 14 represent a descending tone contour and numbers 42 represent an ascending tone contour Cruz (2014).	0
32840	32840	2020.sigmorphon-1.1	Sino-Tibetan	1	109	0.2	0.29700272479564	The Sino-Tibetan family is represented by the Tibetan language.	0
32841	32841	2020.sigmorphon-1.1	Sino-Tibetan	2	110	0.4	0.299727520435967	Tibetan uses an abugida script and contains complex syllabic components in which vowel marks can be added above and below the base consonant.	0
32842	32842	2020.sigmorphon-1.1	Sino-Tibetan	3	111	0.6	0.302452316076294	Tibetan verbs are inflected for tense and mood.	0
32843	32843	2020.sigmorphon-1.1	Sino-Tibetan	4	112	0.8	0.305177111716621	Previous studies on Tibetan morphology (Di et al., 2019) indicate that the majority of mispredictions produced by neural models are due to allomorphy.	0
32844	32844	2020.sigmorphon-1.1	Sino-Tibetan	5	113	1.0	0.307901907356948	This is followed by generation of nonce words (impossible combinations of vowel and consonant components).	0
32845	32845	2020.sigmorphon-1.1	Siouan	1	114	0.166666666666667	0.310626702997275	The Siouan languages are located in North America, predominantly along the Mississippi and Missouri Rivers and in the Ohio Valley.	0
32846	32846	2020.sigmorphon-1.1	Siouan	2	115	0.333333333333333	0.313351498637602	The family is represented in our task by Dakota, a critically endangered language spoken in North and South Dakota, Minnesota, and Saskatchewan.	0
32847	32847	2020.sigmorphon-1.1	Siouan	3	116	0.5	0.316076294277929	The Dakota language is largely agglutinating in its derivational morphology and fusional in its inflectional morphology with a mixed affixation system (Rankin et al., 2003).	0
32848	32848	2020.sigmorphon-1.1	Siouan	4	117	0.666666666666667	0.318801089918256	The present task includes verbs, which are marked for first and second person, number, and duality.	0
32849	32849	2020.sigmorphon-1.1	Siouan	5	118	0.833333333333333	0.321525885558583	All three affixation types are found: person was generally marked by an infix, but could also appear as a prefix, and plurality was marked by a suffix.	0
32850	32850	2020.sigmorphon-1.1	Siouan	6	119	1.0	0.32425068119891	Morphophonological processes of fortition and vowel lowering are also present.	0
32851	32851	2020.sigmorphon-1.1	Songhay	1	120	0.333333333333333	0.326975476839237	The Songhay family consists of around eleven or twelve languages spoken in Mali, Niger, Benin, Burkina Faso and Nigeria.	0
32852	32852	2020.sigmorphon-1.1	Songhay	2	121	0.666666666666667	0.329700272479564	In the shared task we use Zarma, the most widely spoken Songhay language.	0
32853	32853	2020.sigmorphon-1.1	Songhay	3	122	1.0	0.332425068119891	Most of the Songhay languages are predominantly SOV with medium-sized consonant inventories (with implosives), five phonemic vowels, vowel length distinctions, and word level tones, which also are used to distinguish nouns, verbs, and adjectives (Heath, 2014).	0
32854	32854	2020.sigmorphon-1.1	Southern Daly	1	123	0.2	0.335149863760218	The Southern Daly is a small language family of the Northern Territory in Australia that consists of two distantly related languages.	0
32855	32855	2020.sigmorphon-1.1	Southern Daly	2	124	0.4	0.337874659400545	In the current task we only have one of the languages, Murrinh-patha (which was initially thought to be a language isolate).	0
32856	32856	2020.sigmorphon-1.1	Southern Daly	3	125	0.6	0.340599455040872	Murrinh-patha is classified as polysynthetic with highly complex verbal morphology.	0
32857	32857	2020.sigmorphon-1.1	Southern Daly	4	126	0.8	0.343324250681199	Verbal roots are surrounded by prefixes and suffixes that indicate tense, mood, object, subject.	0
32858	32858	2020.sigmorphon-1.1	Southern Daly	5	127	1.0	0.346049046321526	As Mansfield ( 2019) notes, Murrinh-patha verbs have 39 conjugation classes.	0
32859	32859	2020.sigmorphon-1.1	Tungusic	1	128	0.166666666666667	0.348773841961853	Tungusic languages are spoken principally in Russia, China and Mongolia.	0
32860	32860	2020.sigmorphon-1.1	Tungusic	2	129	0.333333333333333	0.35149863760218	In Russia they are concentrated in north and eastern Siberia and in China in the east, in Manchuria.	0
32861	32861	2020.sigmorphon-1.1	Tungusic	3	130	0.5	0.354223433242507	The largest languages in the family are Xibe, Evenki and Even; we use Evenki in the shared task.	0
32862	32862	2020.sigmorphon-1.1	Tungusic	4	131	0.666666666666667	0.356948228882834	The languages are of the agglutinating morphological type with a moderate number of cases, 7 for Xibe and 13 for Evenki.	0
32863	32863	2020.sigmorphon-1.1	Tungusic	5	132	0.833333333333333	0.359673024523161	In addition to case markers, Evenki marks possession in nominals (including reflexive possession) and distinguishes between alienable and inalienable possession.	0
32864	32864	2020.sigmorphon-1.1	Tungusic	6	133	1.0	0.362397820163488	In terms of morphophonological processes, the languages exhibit vowel harmony, consonant alternations and phonological vowel length.	0
32865	32865	2020.sigmorphon-1.1	Turkic	1	134	0.142857142857143	0.365122615803815	Languages of the Turkic family are primarily spoken in Central Asia.	0
32866	32866	2020.sigmorphon-1.1	Turkic	2	135	0.285714285714286	0.367847411444142	The family is morphologically concatenative, fusional, and suffixing.	0
32867	32867	2020.sigmorphon-1.1	Turkic	3	136	0.428571428571429	0.370572207084469	Turkic languages generally exhibit back vowel harmony, with the notable exception of Uzbek.	0
32868	32868	2020.sigmorphon-1.1	Turkic	4	137	0.571428571428571	0.373297002724796	In addition to harmony in backness, several languages also have labial vowel harmony (e.g., Kyrgyz, Turkmen, among others).	0
32869	32869	2020.sigmorphon-1.1	Turkic	5	138	0.714285714285714	0.376021798365123	In addition, most of the languages have dorsal consonant allophony that accompanies back vowel harmony.	0
32870	32870	2020.sigmorphon-1.1	Turkic	6	139	0.857142857142857	0.37874659400545	Additional morphophonological processes include vowel epenthesis and voicing assimilation.	0
32871	32871	2020.sigmorphon-1.1	Turkic	7	140	1.0	0.381471389645777	Selection of the inflectional allomorph can frequently be determined from the infinitive morpheme (which frequently reveals vowel backness and roundedness) and also the final segment of the stem.	0
32872	32872	2020.sigmorphon-1.1	Uralic	1	141	0.1	0.384196185286104	The Uralic languages are spoken in Russia from the north of Siberia to Scandinavia and Hungary in Europe.	0
32873	32873	2020.sigmorphon-1.1	Uralic	2	142	0.2	0.38692098092643	They are agglutinating with some subgroups displaying fusional characteristics (e.g., the Sámi languages).	0
32874	32874	2020.sigmorphon-1.1	Uralic	3	143	0.3	0.389645776566757	Many of the languages have vowel harmony.	0
32875	32875	2020.sigmorphon-1.1	Uralic	4	144	0.4	0.392370572207084	The languages have almost complete suffixal morphology and a medium-sized case inventory, ranging from 5-6 cases to numbers in the high teens.	0
32876	32876	2020.sigmorphon-1.1	Uralic	5	145	0.5	0.395095367847411	Many of the larger case paradigms are made up of spatial cases, sometimes with distinctions for direction and position.	0
32877	32877	2020.sigmorphon-1.1	Uralic	6	146	0.6	0.397820163487738	Most of the languages have possessive suffixes, which can express possession, or agreement in non-finite clauses.	0
32878	32878	2020.sigmorphon-1.1	Uralic	7	147	0.7	0.400544959128065	The paradigms are largely regular, with few, if any, irregular forms.	0
32879	32879	2020.sigmorphon-1.1	Uralic	8	148	0.8	0.403269754768392	Many exhibit complex patterns of consonant gradation-consonant mutations that occur in specific morphological forms in some stems.	0
32880	32880	2020.sigmorphon-1.1	Uralic	9	149	0.9	0.405994550408719	Which gradation category a stem belongs to in often unpredictable.	0
32881	32881	2020.sigmorphon-1.1	Uralic	10	150	1.0	0.408719346049046	The languages spoken in Russia are typically SOV, while those in Europe have SVO order.	0
32882	32882	2020.sigmorphon-1.1	Uto-Aztecan	1	151	0.2	0.411444141689373	The Uto-Aztecan family is represented by the Tohono O'odham (Papago-Pima) language spoken along the US-Mexico border in southern Arizona and northern Sonora.	0
32883	32883	2020.sigmorphon-1.1	Uto-Aztecan	2	152	0.4	0.4141689373297	O'odham is agglutinative with a mixed prefixing and suffixing system.	0
32884	32884	2020.sigmorphon-1.1	Uto-Aztecan	3	153	0.6	0.416893732970027	Nominal and verbal pluralization is frequently realized by partial reduplication of the initial consonant and/or vowel, and occasionally by final consonant deletion or null affixation.	0
32885	32885	2020.sigmorphon-1.1	Uto-Aztecan	4	154	0.8	0.419618528610354	Processes targeting vowel length (shortening or lengthening) are also present.	0
32886	32886	2020.sigmorphon-1.1	Uto-Aztecan	5	155	1.0	0.422343324250681	A small number of verbs exhibit suppletion in the past tense.	0
32887	32887	2020.sigmorphon-1.1	Data Preparation	1	156	0.25	0.425068119891008	Data Format	0
32888	32888	2020.sigmorphon-1.1	Data Preparation	2	157	0.5	0.427792915531335	Similar to previous years, training and development sets contain triples consisting of a lemma, a target form, and morphosyntactic descriptions (MSDs, or morphological tags).	0
32889	32889	2020.sigmorphon-1.1	Data Preparation	3	158	0.75	0.430517711171662	3 Test sets only contain two fields, i.e., target forms are omitted.	0
32890	32890	2020.sigmorphon-1.1	Data Preparation	4	159	1.0	0.433242506811989	All data follows UTF-8 encoding.	0
32891	32891	2020.sigmorphon-1.1	Conversion and Canonicalization	1	160	0.25	0.435967302452316	A significant amount of data for this task was extracted from corresponding (language-specific) grammars.	0
32892	32892	2020.sigmorphon-1.1	Conversion and Canonicalization	2	161	0.5	0.438692098092643	In order to allow cross-lingual comparison, we manually converted their features (tags) into the UniMorph format (Sylak-Glassman, 2016).	0
32893	32893	2020.sigmorphon-1.1	Conversion and Canonicalization	3	162	0.75	0.44141689373297	"We then canonicalized the converted language data 4 to make sure all tags are consistently ordered and no category (e.g., ""Number"") is assigned two tags (e.g., singular and plural)."	0
32894	32894	2020.sigmorphon-1.1	Conversion and Canonicalization	4	163	1.0	0.444141689373297	5	0
32895	32895	2020.sigmorphon-1.1	Splitting	1	164	0.166666666666667	0.446866485013624	We use only noun, verb, and adjective forms to construct training, development, and evaluation sets.	0
32896	32896	2020.sigmorphon-1.1	Splitting	2	165	0.333333333333333	0.449591280653951	We de-duplicate annotations such that there are no multiple examples of exact lemma-formtag matches.	0
32897	32897	2020.sigmorphon-1.1	Splitting	3	166	0.5	0.452316076294278	To create splits, we randomly sample 70%, 10%, and 20% for train, development, and test, respectively.	0
32898	32898	2020.sigmorphon-1.1	Splitting	4	167	0.666666666666667	0.455040871934605	We cap the training set size to 100k examples for each language; where languages exceed this (e.g., Finnish), we subsample to this point, balancing lemmas such that all forms for a given lemma are either included or discarded.	0
32899	32899	2020.sigmorphon-1.1	Splitting	5	168	0.833333333333333	0.457765667574932	Some languages such as Zarma (dje), Tajik (tgk), Lingala (lin), Ludian* (lud), Māori (mao), Sotho (sot), Võro (vro), Anglo-Norman (xno), and Zulu (zul) contain less than 400 training samples and are extremely low-resource.	0
32900	32900	2020.sigmorphon-1.1	Splitting	6	169	1.0	0.460490463215259	6 Tab. 6 and Tab. 7 in the Appendix provide the number of samples for every language in each split, the number of samples per lemma, and statistics on inconsistencies in the data.	0
32901	32901	2020.sigmorphon-1.1	Baseline Systems	1	170	0.5	0.463215258855586	The organizers provided two types of pre-trained baselines.	0
32902	32902	2020.sigmorphon-1.1	Baseline Systems	2	171	1.0	0.465940054495913	Their use was optional.	0
32903	32903	2020.sigmorphon-1.1	Non-neural	1	172	0.166666666666667	0.46866485013624	The first baseline was a non-neural system that had been used as a baseline in earlier shared tasks on morphological reinflection (Cotterell et al., 2017(Cotterell et al., , 2018.	0
32904	32904	2020.sigmorphon-1.1	Non-neural	2	173	0.333333333333333	0.471389645776567	The system first heuristically extracts lemma-to-form transformations; it assumes that these transformations are suffix-or prefix-based.	0
32905	32905	2020.sigmorphon-1.1	Non-neural	3	174	0.5	0.474114441416894	4 Using the UniMorph schema canonicalization script https://github.com/unimorph/umcanonicalize	0
32906	32906	2020.sigmorphon-1.1	Non-neural	4	175	0.666666666666667	0.476839237057221	5 Conversion schemes and canonicalization scripts are available at https://github.com/ sigmorphon2020/task0-data	0
32907	32907	2020.sigmorphon-1.1	Non-neural	5	176	0.833333333333333	0.479564032697548	A simple majority classifier is used to apply the most frequent suitable transformation to an input lemma, given the morphological tag, yielding the output form.	0
32908	32908	2020.sigmorphon-1.1	Non-neural	6	177	1.0	0.482288828337875	See Cotterell et al. (2017) for further details.	0
32909	32909	2020.sigmorphon-1.1	Neural	1	178	0.166666666666667	0.485013623978202	Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*).	0
32910	32910	2020.sigmorphon-1.1	Neural	2	179	0.333333333333333	0.487738419618529	The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIG-MORPHON shared task data (Wu et al., 2020, trm-*).	0
32911	32911	2020.sigmorphon-1.1	Neural	3	180	0.5	0.490463215258856	Both models take the lemma and morphological tags as input and output the target inflection.	0
32912	32912	2020.sigmorphon-1.1	Neural	4	181	0.666666666666667	0.493188010899183	The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. ( 2017)).	0
32913	32913	2020.sigmorphon-1.1	Neural	5	182	0.833333333333333	0.49591280653951	Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language's alphabet, producing hallucinated lemma-tag-form triples.	0
32914	32914	2020.sigmorphon-1.1	Neural	6	183	1.0	0.498637602179837	Both neural baselines were trained in mono-(*-single) and multilingual (shared parameters among the same family, *-shared) settings.	0
32915	32915	2020.sigmorphon-1.1	Competing Systems	1	184	0.041666666666667	0.501362397820163	As Tab. 3 shows, 10 teams submitted 22 systems in total, out of which 19 were neural.	0
32916	32916	2020.sigmorphon-1.1	Competing Systems	2	185	0.083333333333333	0.504087193460491	Some teams such as ETH Zurich and UIUC built their models on top of the proposed baselines.	0
32917	32917	2020.sigmorphon-1.1	Competing Systems	3	186	0.125	0.506811989100817	In particular, ETH Zurich enriched each of the (multilingual) neural baseline models with exact decoding strategy that uses Dijkstra's search algorithm.	0
32918	32918	2020.sigmorphon-1.1	Competing Systems	4	187	0.166666666666667	0.509536784741144	UIUC enriched the transformer model with synchronous bidirectional decoding technique (Zhou et al., 2019) in order to condition the prediction of an affix character on its environment from both sides.	0
32919	32919	2020.sigmorphon-1.1	Competing Systems	5	188	0.208333333333333	0.512261580381471	(The authors demonstrate positive effects in Oto-Manguean, Turkic, and some Austronesian languages.)	0
32920	32920	2020.sigmorphon-1.1	Competing Systems	6	189	0.25	0.514986376021798	A few teams further improved models that were among top performers in previous shared tasks.	0
32921	32921	2020.sigmorphon-1.1	Competing Systems	7	190	0.291666666666667	0.517711171662125	IMS and Flexica re-used the hard monotonic attention model from (Aharoni and Goldberg, 2017).	0
32922	32922	2020.sigmorphon-1.1	Competing Systems	8	191	0.333333333333333	0.520435967302452	IMS developed an ensemble of two models (with left-to-right and right-to-left generation or-der) with a genetic algorithm for ensemble search (Haque et al., 2016) and iteratively provided hallucinated data.	0
32923	32923	2020.sigmorphon-1.1	Competing Systems	9	192	0.375	0.523160762942779	Flexica submitted two neural systems.	0
32924	32924	2020.sigmorphon-1.1	Competing Systems	10	193	0.416666666666667	0.525885558583106	The first model (flexica-02-1) was multilingual (family-wise) hard monotonic attention model with improved alignment strategy.	0
32925	32925	2020.sigmorphon-1.1	Competing Systems	11	194	0.458333333333333	0.528610354223433	This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016).	0
32926	32926	2020.sigmorphon-1.1	Competing Systems	12	195	0.5	0.53133514986376	LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder-decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages.	0
32927	32927	2020.sigmorphon-1.1	Competing Systems	13	196	0.541666666666667	0.534059945504087	DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins ( 2019) and trained it on all languages at once (massively multilingual).	0
32928	32928	2020.sigmorphon-1.1	Competing Systems	14	197	0.583333333333333	0.536784741144414	The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax , deepspin-01-1).	0
32929	32929	2020.sigmorphon-1.1	Competing Systems	15	198	0.625	0.539509536784741	Many teams based their models on the transformer architecture.	0
32930	32930	2020.sigmorphon-1.1	Competing Systems	16	199	0.666666666666667	0.542234332425068	NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and ensembles of three (NYU-CUBoulder-01-0) and five (NYU-CUBoulder-03-0) pointer-generator transformers.	0
32931	32931	2020.sigmorphon-1.1	Competing Systems	17	200	0.708333333333333	0.544959128065395	For languages with less than 1,000 training samples, they also generate hallucinated data.	0
32932	32932	2020.sigmorphon-1.1	Competing Systems	18	201	0.75	0.547683923705722	CULing developed an ensemble of three (monolingual) transformers with identical architecture but different input data format.	0
32933	32933	2020.sigmorphon-1.1	Competing Systems	19	202	0.791666666666667	0.550408719346049	The first model was trained on the initial data format (lemma, target tags, target form).	0
32934	32934	2020.sigmorphon-1.1	Competing Systems	20	203	0.833333333333333	0.553133514986376	For the other two models the team used the idea of lexeme's principal parts (Finkel and Stump, 2007)    were neural, some teams experimented with nonneural approaches showing that in certain scenarios they might surpass neural systems.	0
32935	32935	2020.sigmorphon-1.1	Competing Systems	21	204	0.875	0.555858310626703	A large group of researchers from CU7565 manually developed finite-state grammars for 25 languages (CU7565-01-0).	0
32936	32936	2020.sigmorphon-1.1	Competing Systems	22	205	0.916666666666667	0.55858310626703	They additionally developed a non-neural learner for all languages (CU7565-02-0) that uses hierarchical paradigm clustering (based on similarity of string transformation rules between inflectional slots).	0
32937	32937	2020.sigmorphon-1.1	Competing Systems	23	206	0.958333333333333	0.561307901907357	Another team, Flexica, proposed a model (flexica-01-0) conceptually similar to Hulden et al. (2014), although they did not attempt to reconstruct the paradigm itself and treated transformation rules independently assigning each of them a score based on its frequency and specificity as well as diversity of the characters surrounding the pattern.	0
32938	32938	2020.sigmorphon-1.1	Competing Systems	24	207	1.0	0.564032697547684	7	0
32939	32939	2020.sigmorphon-1.1	Evaluation	1	208	0.125	0.566757493188011	This year, we instituted a slightly different evaluation regimen than in previous years, which takes into account the statistical significance of differences between systems and allows for an informed comparison across languages and families better than a simple macro-average.	0
32940	32940	2020.sigmorphon-1.1	Evaluation	2	209	0.25	0.569482288828338	The process works as follows:	0
32941	32941	2020.sigmorphon-1.1	Evaluation	3	210	0.375	0.572207084468665	1	0
32942	32942	2020.sigmorphon-1.1	Evaluation	4	211	0.5	0.574931880108992	For each language, we rank the systems according to their accuracy (or Levenshtein distance).	0
32943	32943	2020.sigmorphon-1.1	Evaluation	5	212	0.625	0.577656675749319	To do so, we use paired bootstrap resampling (Koehn, 2004) 8 to only take statistically significant differences into account.	0
32944	32944	2020.sigmorphon-1.1	Evaluation	6	213	0.75	0.580381471389646	That way, any system which is the same (as assessed via statistical significance) as the best performing one is also ranked 1 st for that language.	0
32945	32945	2020.sigmorphon-1.1	Evaluation	7	214	0.875	0.583106267029973	re-rank them based on the amount of times they ranked 1 st , 2 nd , 3 rd , etc.	0
32946	32946	2020.sigmorphon-1.1	Evaluation	8	215	1.0	0.5858310626703	Table 4 illustrates an example of this process using four Zapotecan languages and six systems.	0
32947	32947	2020.sigmorphon-1.1	Results	1	216	0.083333333333333	0.588555858310627	This year we had four winning systems (i.e., ones that outperform the best baseline): CULing-01-0, deepspin-02-1, uiuc-01-0, and deepspin-01-1, all neural.	0
32948	32948	2020.sigmorphon-1.1	Results	2	217	0.166666666666667	0.591280653950954	As Tab. 5 shows, they achieve over 90% accuracy.	0
32949	32949	2020.sigmorphon-1.1	Results	3	218	0.25	0.594005449591281	Although CULing-01-0 and uiuc-01-0 are both monolingual transformers that do not use any hallucinated data, they follow different strategies to improve performance.	0
32950	32950	2020.sigmorphon-1.1	Results	4	219	0.333333333333333	0.596730245231608	The strategy proposed by CULing-01-0 of enriching the input data with extra entries that included non-lemma forms and their tags as a source form, enabled their system to be among top performers on all language families; uiuc-01-0, on the other hand, did not modify the data but rather changed the decoder to be bidirectional and made family-wise fine-tuning of each (monolingual) model.	0
32951	32951	2020.sigmorphon-1.1	Results	5	220	0.416666666666667	0.599455040871935	The system is also among the top performers on all language families except Iranian.	0
32952	32952	2020.sigmorphon-1.1	Results	6	221	0.5	0.602179836512262	The third team, DeepSpin, trained and fine-tuned their models on all language data.	0
32953	32953	2020.sigmorphon-1.1	Results	7	222	0.583333333333333	0.604904632152589	Both models are ranked high (although the sparsemax model, deepspin-02-1, performs better overall) on most language groups with exception of Algic.	0
32954	32954	2020.sigmorphon-1.1	Results	8	223	0.666666666666667	0.607629427792916	Sparsemax was also found useful by CMU-Tartan.	0
32955	32955	2020.sigmorphon-1.1	Results	9	224	0.75	0.610354223433242	The neural ensemble model with data augmentation from IMS team shows superior performance on languages with smaller data sizes (under 10,000 samples).	0
32956	32956	2020.sigmorphon-1.1	Results	10	225	0.833333333333333	0.613079019073569	LTI and Flexica teams also observed positive effects of multilingual training and data hallucination on low-resource languages.	0
32957	32957	2020.sigmorphon-1.1	Results	11	226	0.916666666666667	0.615803814713896	The latter was also found useful in the ablation study made by NYU-CUBoulder team.	0
32958	32958	2020.sigmorphon-1.1	Results	12	227	1.0	0.618528610354223	Several teams aimed to address particular research questions; we will further summarize their results.	0
32959	32959	2020.sigmorphon-1.1	System	1	228	0.1	0.62125340599455	Rank Acc	0
32960	32960	2020.sigmorphon-1.1	System	2	229	0.2	0.623978201634877	Has morphological inflection become a solved problem in certain scenarios?	0
32961	32961	2020.sigmorphon-1.1	System	3	230	0.3	0.626702997275204	The results shown in Fig. 2 suggest that for some of the development language families, such as Austronesian and Niger-Congo, the task was relatively easy, with most systems achieving high accuracy, whereas the task was more difficult for Uralic and Oto-Manguean languages, which showed greater variability in level of performance across submitted systems.	0
32962	32962	2020.sigmorphon-1.1	System	4	231	0.4	0.629427792915531	Languages such as Ludic (lud), Norwegian Nynorsk (nno), Middle Low German 1 3 6 7 1 5 0 9 2 3 3 9 1 0 8 1 3 4 6 4 5 9 0 1 5 8 5 7 3 9 9 8 8 7 0 1 6 5 1 1 7 9 9 1 9 6 2 4 8 2 3 8 9 5 3 9 7 4 5 4 4 4 2 9 3 3 3 1 3 5 9 6 2 1 1 5 2 0 2 2 6 0 2 7 0 1 7 5 8 4 6 6 3 1 1 3 4 9 1 2 3 5 0 9 8 0 2 4 8 4 4 7 1 6 7 0 4 1 4 9 1 6 3 7 2 1 3 3 9 7 0.00  (gml), Evenki (evn), and O'odham (ood) seem to be the most challenging languages based on simple accuracy.	0
32963	32963	2020.sigmorphon-1.1	System	5	232	0.5	0.632152588555858	"For a more fine-grained study, we have classified test examples into four categories: ""very easy"", ""easy"", ""hard"", and ""very hard""."	0
32964	32964	2020.sigmorphon-1.1	System	6	233	0.6	0.634877384196185	"""Very easy"" examples are ones that all submitted systems got correct, while ""very hard"" examples are ones that no submitted system got correct."	0
32965	32965	2020.sigmorphon-1.1	System	7	234	0.7	0.637602179836512	"""Easy"" examples were predicted correctly for 80% of systems, and ""hard"" were only correct in 20% of systems."	0
32966	32966	2020.sigmorphon-1.1	System	8	235	0.8	0.640326975476839	Fig. 3, Fig. 4, and Fig. 5 represent percentage of noun, verb, and adjective samples that fall into each category and illustrate that most language samples are correctly predicted by majority of the systems.	0
32967	32967	2020.sigmorphon-1.1	System	9	236	0.9	0.643051771117166	For noun declension, Old English (ang), Middle Low German (gml), Evenki (evn), O'odham (ood), Võro (vro) are the most difficult (some of this difficulty comes from language data inconsistency, as described in the following section).	0
32968	32968	2020.sigmorphon-1.1	System	10	237	1.0	0.645776566757493	For adjective declension, Classic Syriac presents the highest difficulty (likely due to its limited data).	0
32969	32969	2020.sigmorphon-1.1	Error Analysis	1	238	0.0625	0.64850136239782	In our error analysis we follow the error type taxonomy proposed in Gorman et al. (2019).	0
32970	32970	2020.sigmorphon-1.1	Error Analysis	2	239	0.125	0.651226158038147	First, we evaluate systematic errors due to inconsistencies in the data, followed by an analysis of whether having seen the language or its family improved accuracy.	0
32971	32971	2020.sigmorphon-1.1	Error Analysis	3	240	0.1875	0.653950953678474	We then proceed with an overview of accuracy for each of the language families.	0
32972	32972	2020.sigmorphon-1.1	Error Analysis	4	241	0.25	0.656675749318801	For a select number of families, we provide a more detailed analysis of the error patterns.	0
32973	32973	2020.sigmorphon-1.1	Error Analysis	5	242	0.3125	0.659400544959128	Tab. 6 and Tab. 7 provide the number of samples in the training, development, and test sets, percentage of inconsistent entries (the same lemma-tag pair has multiple infected forms) in them, percentage of contradicting entries (same lemma-tag pair occurring in train and development or test sets but assigned to different inflected forms), and percentage of entries in the development or test sets containing a lemma observed in the training set.	0
32974	32974	2020.sigmorphon-1.1	Error Analysis	6	243	0.375	0.662125340599455	The train, development and test sets contain 2%, 0.3%, and 0.6% inconsistent entries, respectively.	0
32975	32975	2020.sigmorphon-1.1	Error Analysis	7	244	0.4375	0.664850136239782	Azerbaijani (aze), Old English (ang), Cree (cre), Danish (dan), Middle Low German (gml), Kannada (kan), Norwegian Bokmål (nob), Chichimec (pei), and Veps (vep) had the highest rates of inconsistency.	0
32976	32976	2020.sigmorphon-1.1	Error Analysis	8	245	0.5	0.667574931880109	These languages also exhibit the highest percentage of contradicting entries.	0
32977	32977	2020.sigmorphon-1.1	Error Analysis	9	246	0.5625	0.670299727520436	The inconsistencies in some Finno-Ugric languages (such as Veps and Ludic) are due to dialectal variations.	0
32978	32978	2020.sigmorphon-1.1	Error Analysis	10	247	0.625	0.673024523160763	The overall accuracy of system and language pairings appeared to improve with an increase in the size of the dataset (Fig. 6; see also Fig. 7 for accuracy trends by language family and Fig. 8 for accuracy trends by system).	0
32979	32979	2020.sigmorphon-1.1	Error Analysis	11	248	0.6875	0.67574931880109	Overall, the variance was considerable regardless of whether the language family or even the language itself had been observed during the Development Phase.	0
32980	32980	2020.sigmorphon-1.1	Error Analysis	12	249	0.75	0.678474114441417	A linear mixed-effects regression was used to assess variation in accuracy using fixed effects of language category, the size of the training dataset (log count), and their interactions, as well as random intercepts for system and language family accuracy.	0
32981	32981	2020.sigmorphon-1.1	Error Analysis	13	250	0.8125	0.681198910081744	10 Language category was sum-coded with three levels: development language-development family, surprise language-development family, or surprise language-surprise family.	0
32982	32982	2020.sigmorphon-1.1	Error Analysis	14	251	0.875	0.683923705722071	A significant effect of dataset size was observed, such that a one unit increase in log count corresponded to a 2% increase in accuracy (β = 0.019, p &lt; 0.001).	0
32983	32983	2020.sigmorphon-1.1	Error Analysis	15	252	0.9375	0.686648501362398	Language category type also significantly influenced accuracy: both development languages and surprise languages from development families were less accurate on average (β dev−dev = -0.145, β sur−dev = -0.167, each p &lt; 0.001).	0
32984	32984	2020.sigmorphon-1.1	Error Analysis	16	253	1.0	0.689373297002725	These main effects were, however, significantly modulated by interactions with dataset size: on top of the main effect of dataset size, accuracy for development languages increased an additional ≈ 1.7% (β dev−dev×size = 0.017, p &lt; 0.001) and accuracy for surprise languages from development families increased an additional ≈ 2.9% (β sur−dev×size = 0.029, p &lt; 0.001).	0
32985	32985	2020.sigmorphon-1.1	Afro-Asiatic:	1	254	0.04	0.692098092643052	This family was represented by three languages.	0
32986	32986	2020.sigmorphon-1.1	Afro-Asiatic:	2	255	0.08	0.694822888283379	Mean accuracy across systems was above average at 91.7%.	0
32987	32987	2020.sigmorphon-1.1	Afro-Asiatic:	3	256	0.12	0.697547683923706	Relative to other families, variance in accuracy was low, but nevertheless ranged from 41.1% to 99.0%.	0
32988	32988	2020.sigmorphon-1.1	Afro-Asiatic:	4	257	0.16	0.700272479564033	Algic:	0
32989	32989	2020.sigmorphon-1.1	Afro-Asiatic:	5	258	0.2	0.70299727520436	This family was represented by one language, Cree.	0
32990	32990	2020.sigmorphon-1.1	Afro-Asiatic:	6	259	0.24	0.705722070844687	Mean accuracy across systems was below average at 65.1%.	0
32991	32991	2020.sigmorphon-1.1	Afro-Asiatic:	7	260	0.28	0.708446866485014	Relative to other families, variance in accuracy was low, ranging from 41.5% to 73%.	0
32992	32992	2020.sigmorphon-1.1	Afro-Asiatic:	8	261	0.32	0.711171662125341	All systems appeared to struggle with the choice of preverbal auxiliary.	0
32993	32993	2020.sigmorphon-1.1	Afro-Asiatic:	9	262	0.36	0.713896457765668	Some auxiliaries were overloaded: 'kitta' could refer to future, imperfective, or imperative.	0
32994	32994	2020.sigmorphon-1.1	Afro-Asiatic:	10	263	0.4	0.716621253405995	The morphological features for mood and tense were also frequently combined, such as SBJV+OPT (subjunctive plus optative mood).	0
32995	32995	2020.sigmorphon-1.1	Afro-Asiatic:	11	264	0.44	0.719346049046322	While the paradigms were very large, there were very few lemmas (28 impersonal verbs and 14 transitive verbs), which may have contributed to the lower accuracy.	0
32996	32996	2020.sigmorphon-1.1	Afro-Asiatic:	12	265	0.48	0.722070844686648	Interestingly, the inflections could largely be generated by rules.	0
32997	32997	2020.sigmorphon-1.1	Afro-Asiatic:	13	266	0.52	0.724795640326976	11 Austronesian:	0
32998	32998	2020.sigmorphon-1.1	Afro-Asiatic:	14	267	0.56	0.727520435967302	This family was represented by five languages.	0
32999	32999	2020.sigmorphon-1.1	Afro-Asiatic:	15	268	0.6	0.730245231607629	Mean accuracy across systems was around average at 80.5%.	0
33000	33000	2020.sigmorphon-1.1	Afro-Asiatic:	16	269	0.64	0.732970027247956	Relative to other families, variance in accuracy was high, with accuracy ranging from 39.5% to 100%.	0
33001	33001	2020.sigmorphon-1.1	Afro-Asiatic:	17	270	0.68	0.735694822888283	One may notice a discrepancy among the difficulty in processing different Austronesian languages.	0
33002	33002	2020.sigmorphon-1.1	Afro-Asiatic:	18	271	0.72	0.73841961852861	For instance, we see a difference of over 10% in the baseline performance of Cebuano (84%) and Hiligaynon (96%).	0
33003	33003	2020.sigmorphon-1.1	Afro-Asiatic:	19	272	0.76	0.741144414168937	12 This could come from the fact that Cebuano only has partial reduplication while Hiligaynon has full reduplication.	0
33004	33004	2020.sigmorphon-1.1	Afro-Asiatic:	20	273	0.8	0.743869209809264	Furthermore, the prefix choice for Cebuano is more irregular, making it more difficult to predict the correct conjugation of the verb.	0
33005	33005	2020.sigmorphon-1.1	Afro-Asiatic:	21	274	0.84	0.746594005449591	Dravidian:	0
33006	33006	2020.sigmorphon-1.1	Afro-Asiatic:	22	275	0.88	0.749318801089918	This family was represented by two languages: Kannada and Telugu.	0
33007	33007	2020.sigmorphon-1.1	Afro-Asiatic:	23	276	0.92	0.752043596730245	Mean accuracy across systems was around average at 82.2%.	0
33008	33008	2020.sigmorphon-1.1	Afro-Asiatic:	24	277	0.96	0.754768392370572	Relative to other families, variance in accuracy was high: system accuracy ranged from 44.6% to 96.0%.	0
33009	33009	2020.sigmorphon-1.1	Afro-Asiatic:	25	278	1.0	0.757493188010899	Accuracy for Telugu was systematically higher than accuracy for Kannada.	0
33010	33010	2020.sigmorphon-1.1	Indo-European:	1	279	0.012820512820513	0.760217983651226	This family was represented by 29 languages and four main branches.	0
33011	33011	2020.sigmorphon-1.1	Indo-European:	2	280	0.025641025641026	0.762942779291553	Mean accuracy across systems was slightly above average at 86.9%.	0
33012	33012	2020.sigmorphon-1.1	Indo-European:	3	281	0.038461538461539	0.76566757493188	Relative to other families, variance in accuracy was very high: system accuracy ranged from 0.02% to 100%.	0
33013	33013	2020.sigmorphon-1.1	Indo-European:	4	282	0.051282051282051	0.768392370572207	For Indo-Aryan, mean accuracy was high (96.0%) with low variance; for Germanic, mean accuracy was slightly below average (79.0%) but with very high variance (ranging from 0.02% to 99.5%), for Romance, mean accuracy was high (93.4%) but also had a high variance (ranging from 23.5% to 99.8%), and for Iranian, mean accuracy was high (89.2%), but again with a high variance (ranging from 25.0% to 100%).	0
33014	33014	2020.sigmorphon-1.1	Indo-European:	5	283	0.064102564102564	0.771117166212534	Languages from the Germanic branch of the Indo-European family were included in the Development Phase.	0
33015	33015	2020.sigmorphon-1.1	Indo-European:	6	284	0.076923076923077	0.773841961852861	Niger-Congo:	0
33016	33016	2020.sigmorphon-1.1	Indo-European:	7	285	0.08974358974359	0.776566757493188	This family was represented by ten languages.	0
33017	33017	2020.sigmorphon-1.1	Indo-European:	8	286	0.102564102564103	0.779291553133515	Mean accuracy across systems was very good at 96.4%.	0
33018	33018	2020.sigmorphon-1.1	Indo-European:	9	287	0.115384615384615	0.782016348773842	Relative to other families, variance in accuracy was low, with accuracy ranging from 62.8% to 100%.	0
33019	33019	2020.sigmorphon-1.1	Indo-European:	10	288	0.128205128205128	0.784741144414169	"Most languages in this family are considered low resource, and the resources used for data gathering may have been biased towards the languages' regular forms, as such this high accuracy may not be representative of the ""easiness"" of the task in this family."	0
33020	33020	2020.sigmorphon-1.1	Indo-European:	11	289	0.141025641025641	0.787465940054496	Languages from the Niger-Congo family was included in the Development Phase.	0
33021	33021	2020.sigmorphon-1.1	Indo-European:	12	290	0.153846153846154	0.790190735694823	Oto-Manguean:	0
33022	33022	2020.sigmorphon-1.1	Indo-European:	13	291	0.166666666666667	0.79291553133515	This family was represented by nine languages.	0
33023	33023	2020.sigmorphon-1.1	Indo-European:	14	292	0.179487179487179	0.795640326975477	Mean accuracy across systems was slightly below average at 78.5%.	0
33024	33024	2020.sigmorphon-1.1	Indo-European:	15	293	0.192307692307692	0.798365122615804	Relative to other families, variance in accuracy was high, with accuracy ranging from 18.7% to 99.1%.	0
33025	33025	2020.sigmorphon-1.1	Indo-European:	16	294	0.205128205128205	0.801089918256131	Languages from the Oto-Manguean family were included in the Development Phase.	0
33026	33026	2020.sigmorphon-1.1	Indo-European:	17	295	0.217948717948718	0.803814713896458	Sino-Tibetan:	0
33027	33027	2020.sigmorphon-1.1	Indo-European:	18	296	0.230769230769231	0.806539509536785	This family was represented by one language, Bodic.	0
33028	33028	2020.sigmorphon-1.1	Indo-European:	19	297	0.243589743589744	0.809264305177112	Mean accuracy across systems was average at 82.1%, and variance across systems was also very low.	0
33029	33029	2020.sigmorphon-1.1	Indo-European:	20	298	0.256410256410256	0.811989100817439	Accuracy ranged from 67.9% to 85.1%.	0
33030	33030	2020.sigmorphon-1.1	Indo-European:	21	299	0.269230769230769	0.814713896457766	The results are similar to those in Di et al. (2019) where majority of errors relate to allomorphy and impossible combinations of Tibetan unit components.	0
33031	33031	2020.sigmorphon-1.1	Indo-European:	22	300	0.282051282051282	0.817438692098093	above average at 89.4%, and variance across systems was also low, despite the range from 0% to 95.7%.	0
33032	33032	2020.sigmorphon-1.1	Indo-European:	23	301	0.294871794871795	0.82016348773842	Dakota presented variable prefixing and infixing of person morphemes, along some complexities related to fortition processes.	0
33033	33033	2020.sigmorphon-1.1	Indo-European:	24	302	0.307692307692308	0.822888283378747	Determining the factor(s) that governed variation in affix position was difficult from a linguist's perspective, though many systems were largely successful.	0
33034	33034	2020.sigmorphon-1.1	Indo-European:	25	303	0.320512820512821	0.825613079019074	Success varied in the choice of the first or second person singular allomorphs which had increasing degrees of consonant strengthening (e.g., /wa/, /ma/, /mi/ /bde/, /bdu/ for the first person singular and /ya/, /na/, /ni/, /de/, or /du/ for the second person singular).	0
33035	33035	2020.sigmorphon-1.1	Indo-European:	26	304	0.333333333333333	0.828337874659401	In some cases, these fortition processes were overapplied, and in some cases, entirely missed.	0
33036	33036	2020.sigmorphon-1.1	Indo-European:	27	305	0.346153846153846	0.831062670299727	Songhay:	0
33037	33037	2020.sigmorphon-1.1	Indo-European:	28	306	0.358974358974359	0.833787465940054	This family was represented by one language, Zarma.	0
33038	33038	2020.sigmorphon-1.1	Indo-European:	29	307	0.371794871794872	0.836512261580381	Mean accuracy across systems was above average at 88.6%, and variance across systems was relatively high.	0
33039	33039	2020.sigmorphon-1.1	Indo-European:	30	308	0.384615384615385	0.839237057220708	Accuracy ranged from 0% to 100%.	0
33040	33040	2020.sigmorphon-1.1	Indo-European:	31	309	0.397435897435897	0.841961852861035	Southern Daly:	0
33041	33041	2020.sigmorphon-1.1	Indo-European:	32	310	0.41025641025641	0.844686648501362	This family was represented by one language, Murrinh-Patha.	0
33042	33042	2020.sigmorphon-1.1	Indo-European:	33	311	0.423076923076923	0.847411444141689	Mean accuracy across systems was below average at 73.2%, and variance across systems was relatively high.	0
33043	33043	2020.sigmorphon-1.1	Indo-European:	34	312	0.435897435897436	0.850136239782016	Accuracy ranged from 21.2% to 91.9%.	0
33044	33044	2020.sigmorphon-1.1	Indo-European:	35	313	0.448717948717949	0.852861035422343	Tungusic:	0
33045	33045	2020.sigmorphon-1.1	Indo-European:	36	314	0.461538461538462	0.85558583106267	This family was represented by one language, Evenki.	0
33046	33046	2020.sigmorphon-1.1	Indo-European:	37	315	0.474358974358974	0.858310626702997	The overall accuracy was the lowest across families.	0
33047	33047	2020.sigmorphon-1.1	Indo-European:	38	316	0.487179487179487	0.861035422343324	Mean accuracy was 53.8% with very low variance across systems.	0
33048	33048	2020.sigmorphon-1.1	Indo-European:	39	317	0.5	0.863760217983651	Accuracy ranged from 43.5% to 59.0%.	0
33049	33049	2020.sigmorphon-1.1	Indo-European:	40	318	0.512820512820513	0.866485013623978	The low accuracy is due to several factors.	0
33050	33050	2020.sigmorphon-1.1	Indo-European:	41	319	0.525641025641026	0.869209809264305	Firstly and primarily, the dataset was created from oral speech samples in various dialects of the language.	0
33051	33051	2020.sigmorphon-1.1	Indo-European:	42	320	0.538461538461538	0.871934604904632	The Evenki language is known to have rich dialectal variation.	0
33052	33052	2020.sigmorphon-1.1	Indo-European:	43	321	0.551282051282051	0.874659400544959	Moreover, there was little attempt at any standardization in the oral speech transcription.	0
33053	33053	2020.sigmorphon-1.1	Indo-European:	44	322	0.564102564102564	0.877384196185286	These peculiarities led to a high number of errors.	0
33054	33054	2020.sigmorphon-1.1	Indo-European:	45	323	0.576923076923077	0.880108991825613	For instance, some of the systems synthesized a wrong plural form for a noun ending in /-n/.	0
33055	33055	2020.sigmorphon-1.1	Indo-European:	46	324	0.58974358974359	0.88283378746594	Depending on the dialect, it can be /-r/ or /-l/, and there is a trend to have /-hVl/ for borrowed nouns.	0
33056	33056	2020.sigmorphon-1.1	Indo-European:	47	325	0.602564102564102	0.885558583106267	Deducing such a rule as well as the fact that the noun is a loanword is a hard task.	0
33057	33057	2020.sigmorphon-1.1	Indo-European:	48	326	0.615384615384615	0.888283378746594	Other suffixes may also have variable forms (such as /-k	0
33058	33058	2020.sigmorphon-1.1	Indo-European:	49	327	0.628205128205128	0.891008174386921	Vllu/ vs /-k	0
33059	33059	2020.sigmorphon-1.1	Indo-European:	50	328	0.641025641025641	0.893732970027248	Vldu/ depending on the dialect for the 2PL imperative.	0
33060	33060	2020.sigmorphon-1.1	Indo-European:	51	329	0.653846153846154	0.896457765667575	Some verbs have irregular past tense forms depending on the dialect and the meaning of the verb (e. g. /o:-/ 'to make' and 'to become').	0
33061	33061	2020.sigmorphon-1.1	Indo-European:	52	330	0.666666666666667	0.899182561307902	Next, various dialects exhibit various vowel and consonant changes in suffixes.	0
33062	33062	2020.sigmorphon-1.1	Indo-European:	53	331	0.67948717948718	0.901907356948229	For example, some dialects (but not all of them) change /w/ to /b/ after /l/, and the systems sometimes synthesized a wrong form.	0
33063	33063	2020.sigmorphon-1.1	Indo-European:	54	332	0.692307692307692	0.904632152588556	The vowel harmony is complex: not all suffixes obey it, and it is also dialect-dependent.	0
33064	33064	2020.sigmorphon-1.1	Indo-European:	55	333	0.705128205128205	0.907356948228883	Some suffixes have variants (e. g., /-sin/ and /-s/ for SEMEL (semelfactive)), and the choice between them might be hard to understand.	0
33065	33065	2020.sigmorphon-1.1	Indo-European:	56	334	0.717948717948718	0.91008174386921	Finally, some of the mistakes are due to the markup scheme scarcity.	0
33066	33066	2020.sigmorphon-1.1	Indo-European:	57	335	0.730769230769231	0.912806539509537	For example, various past tense forms are all annotated as PST, or there are several comitative suffixes all annotated as COM.	0
33067	33067	2020.sigmorphon-1.1	Indo-European:	58	336	0.743589743589744	0.915531335149864	Moreover, some features are present in the word form but they receive no annotation at all.	0
33068	33068	2020.sigmorphon-1.1	Indo-European:	59	337	0.756410256410256	0.918256130790191	It is worth mentioning that some of the predictions could theoretically be possible.	0
33069	33069	2020.sigmorphon-1.1	Indo-European:	60	338	0.769230769230769	0.920980926430518	To sum up, the Evenki case presents the chal-lenges of oral non-standardized speech.	0
33070	33070	2020.sigmorphon-1.1	Indo-European:	61	339	0.782051282051282	0.923705722070845	Turkic:	0
33071	33071	2020.sigmorphon-1.1	Indo-European:	62	340	0.794871794871795	0.926430517711172	This family was represented by nine languages.	0
33072	33072	2020.sigmorphon-1.1	Indo-European:	63	341	0.807692307692308	0.929155313351499	Mean accuracy across systems was relatively high at 93%, and relative to other families, variance across systems was low.	0
33073	33073	2020.sigmorphon-1.1	Indo-European:	64	342	0.82051282051282	0.931880108991826	Accuracy ranged from 51.5% to 100%.	0
33074	33074	2020.sigmorphon-1.1	Indo-European:	65	343	0.833333333333333	0.934604904632153	Accuracy was lower for Azerbaijani and Turkmen, which after closer inspection revealed some slight contamination in the 'gold' files.	0
33075	33075	2020.sigmorphon-1.1	Indo-European:	66	344	0.846153846153846	0.93732970027248	There was very marginal variation in the accuracy for these languages across systems.	0
33076	33076	2020.sigmorphon-1.1	Indo-European:	67	345	0.858974358974359	0.940054495912806	Besides these two, accuracies were predominantly above 98%.	0
33077	33077	2020.sigmorphon-1.1	Indo-European:	68	346	0.871794871794872	0.942779291553133	A few systems struggled with the choice and inflection of the postverbal auxiliary in various languages (e.g., Kyrgyz, Kazakh, and Uzbek).	0
33078	33078	2020.sigmorphon-1.1	Indo-European:	69	347	0.884615384615385	0.945504087193461	Uralic:	0
33079	33079	2020.sigmorphon-1.1	Indo-European:	70	348	0.897435897435897	0.948228882833787	This family was represented by 16 languages.	0
33080	33080	2020.sigmorphon-1.1	Indo-European:	71	349	0.91025641025641	0.950953678474114	Mean accuracy across systems was average at 81.5%, but the variance across systems and languages was very high.	0
33081	33081	2020.sigmorphon-1.1	Indo-European:	72	350	0.923076923076923	0.953678474114441	Accuracy ranged from 0% to 99.8%.	0
33082	33082	2020.sigmorphon-1.1	Indo-European:	73	351	0.935897435897436	0.956403269754768	Languages from the Uralic family were included in the Development Phase.	0
33083	33083	2020.sigmorphon-1.1	Indo-European:	74	352	0.948717948717949	0.959128065395095	Uto-Aztecan:	0
33084	33084	2020.sigmorphon-1.1	Indo-European:	75	353	0.961538461538462	0.961852861035422	This family was represented by one language, O'odham.	0
33085	33085	2020.sigmorphon-1.1	Indo-European:	76	354	0.974358974358974	0.964577656675749	Mean accuracy across systems was slightly below average at 76.4%, but the variance across systems and languages was fairly low.	0
33086	33086	2020.sigmorphon-1.1	Indo-European:	77	355	0.987179487179487	0.967302452316076	Accuracy ranged from 54.8% to 82.5%.	0
33087	33087	2020.sigmorphon-1.1	Indo-European:	78	356	1.0	0.970027247956403	The systems with higher accuracy may have benefited from better recall of suppletive forms relative to lower accuracy systems.	0
33088	33088	2020.sigmorphon-1.1	Conclusion	1	357	0.090909090909091	0.97275204359673	This years's shared task on morphological reinflection focused on building models that could generalize across an extremely typologically diverse set of languages, many from understudied language families and with limited available text resources.	0
33089	33089	2020.sigmorphon-1.1	Conclusion	2	358	0.181818181818182	0.975476839237057	As in previous years, neural models performed well, even in relatively low-resource cases.	0
33090	33090	2020.sigmorphon-1.1	Conclusion	3	359	0.272727272727273	0.978201634877384	Submissions were able to make productive use of multilingual training to take advantage of commonalities across languages in the dataset.	0
33091	33091	2020.sigmorphon-1.1	Conclusion	4	360	0.363636363636364	0.980926430517711	Data augmentation techniques such as hallucination helped fill in the gaps and allowed networks to generalize to unseen inputs.	0
33092	33092	2020.sigmorphon-1.1	Conclusion	5	361	0.454545454545455	0.983651226158038	These techniques, combined with architecture tweaks like sparsemax, resulted in excellent overall performance on many languages (over 90% accuracy on average).	0
33093	33093	2020.sigmorphon-1.1	Conclusion	6	362	0.545454545454545	0.986376021798365	However, the task's focus on typological diversity revealed that some morphology types and language families (Tungusic, Oto-Manguean, South-ern Daly) remain a challenge for even the best systems.	0
33094	33094	2020.sigmorphon-1.1	Conclusion	7	363	0.636363636363636	0.989100817438692	These families are extremely low-resource, represented in this dataset by few or a single language.	0
33095	33095	2020.sigmorphon-1.1	Conclusion	8	364	0.727272727272727	0.991825613079019	This makes cross-linguistic transfer of similarities by multilanguage training less viable.	0
33096	33096	2020.sigmorphon-1.1	Conclusion	9	365	0.818181818181818	0.994550408719346	They may also have morphological properties and rules (e.g., Evenki is agglutinating with many possible forms for each lemma) that are particularly difficult for machine learners to induce automatically from sparse data.	0
33097	33097	2020.sigmorphon-1.1	Conclusion	10	366	0.909090909090909	0.997275204359673	For some languages (Ingrian, Tajik, Tagalog, Zarma, and Lingala), optimal performance was only achieved in this shared task by hand-encoding linguist knowledge in finite state grammars.	0
33098	33098	2020.sigmorphon-1.1	Conclusion	11	367	1.0	1.0	It is up to future research to imbue models with the right kinds of linguistic inductive biases to overcome these challenges.	0
38129	38129	K15-2001	title	1	1	1.0	0.003322259136213	The CoNLL-2015 Shared Task on Shallow Discourse Parsing	0
38130	38130	K15-2001	abstract	1	2	0.125	0.006644518272425	The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text.	0
38131	38131	K15-2001	abstract	2	3	0.25	0.009966777408638	A discourse relation can be expressed explicitly or implicitly, and takes two arguments realized as sentences, clauses, or in some rare cases, phrases.	0
38132	38132	K15-2001	abstract	3	4	0.375	0.013289036544851	Sixteen teams from three continents participated in this task.	0
38133	38133	K15-2001	abstract	4	5	0.5	0.016611295681063	For the first time in the history of the CoNLL shared tasks, participating teams, instead of running their systems on the test set and submitting the output, were asked to deploy their systems on a remote virtual machine and use a web-based evaluation platform to run their systems on the test set.	0
38134	38134	K15-2001	abstract	5	6	0.625	0.019933554817276	This meant they were unable to actually see the data set, thus preserving its integrity and ensuring its replicability.	0
38135	38135	K15-2001	abstract	6	7	0.75	0.023255813953488	In this paper, we present the task definition, the training and test sets, and the evaluation protocol and metric used during this shared task.	0
38136	38136	K15-2001	abstract	7	8	0.875	0.026578073089701	We also summarize the different approaches adopted by the participating teams, and present the evaluation results.	0
38137	38137	K15-2001	abstract	8	9	1.0	0.029900332225914	The evaluation data sets and the scorer will serve as a benchmark for future research on shallow discourse parsing.	0
38138	38138	K15-2001	Introduction	1	10	0.027027027027027	0.033222591362126	The shared task for the Nineteenth Conference on Computational Natural Language Learning (CoNLL-2015) is on Shallow Discourse Parsing (SDP).	0
38139	38139	K15-2001	Introduction	2	11	0.054054054054054	0.036544850498339	In the course of the sixteen CoNLL shared tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011;Pradhan et al., 2012).	0
38140	38140	K15-2001	Introduction	3	12	0.081081081081081	0.039867109634552	The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012).	0
38141	38141	K15-2001	Introduction	4	13	0.108108108108108	0.043189368770764	Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text.	0
38142	38142	K15-2001	Introduction	5	14	0.135135135135135	0.046511627906977	Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012;	0
38143	38143	K15-2001	Introduction	6	15	0.162162162162162	0.049833887043189	Webber et al., 2012;Prasad and Bunt, 2015).	0
38144	38144	K15-2001	Introduction	7	16	0.189189189189189	0.053156146179402	For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008;	0
38145	38145	K15-2001	Introduction	8	17	0.216216216216216	0.056478405315615	Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them.	0
38146	38146	K15-2001	Introduction	9	18	0.243243243243243	0.059800664451827	For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.	0
38147	38147	K15-2001	Introduction	10	19	0.27027027027027	0.06312292358804	1	0
38148	38148	K15-2001	Introduction	11	20	0.297297297297297	0.066445182724253	The necessary conditions are also in place for such a task.	0
38149	38149	K15-2001	Introduction	12	21	0.324324324324324	0.069767441860465	The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008;	0
38150	38150	K15-2001	Introduction	13	22	0.351351351351351	0.073089700996678	Duverle and Prendinger, 2009;	0
38151	38151	K15-2001	Introduction	14	23	0.378378378378378	0.07641196013289	Lin et al., 2009;	0
38152	38152	K15-2001	Introduction	15	24	0.405405405405405	0.079734219269103	Pitler et al., 2009;	0
38153	38153	K15-2001	Introduction	16	25	0.432432432432432	0.083056478405316	Subba and Di Eugenio, 2009;	0
38154	38154	K15-2001	Introduction	17	26	0.45945945945946	0.086378737541528	Zhou et al., 2010;	0
38155	38155	K15-2001	Introduction	18	27	0.486486486486487	0.089700996677741	Feng and Hirst, 2012;	0
38156	38156	K15-2001	Introduction	19	28	0.513513513513513	0.093023255813954	Ghosh et al., 2012;	0
38157	38157	K15-2001	Introduction	20	29	0.540540540540541	0.096345514950166	Park and Cardie, 2012;	0
38158	38158	K15-2001	Introduction	21	30	0.567567567567568	0.099667774086379	Wang et al., 2012;Biran and McKeown, 2013;	0
38159	38159	K15-2001	Introduction	22	31	0.594594594594595	0.102990033222591	Lan et al., 2013;	0
38160	38160	K15-2001	Introduction	23	32	0.621621621621622	0.106312292358804	Feng and Hirst, 2014;	0
38161	38161	K15-2001	Introduction	24	33	0.648648648648649	0.109634551495017	Ji and Eisenstein, 2014;	0
38162	38162	K15-2001	Introduction	25	34	0.675675675675676	0.112956810631229	Li and Nenkova, 2014;	0
38163	38163	K15-2001	Introduction	26	35	0.702702702702703	0.116279069767442	Lin et al., 2014;	0
38164	38164	K15-2001	Introduction	27	36	0.72972972972973	0.119601328903655	Rutherford and Xue, 2014), and the momentum is building.	0
38165	38165	K15-2001	Introduction	28	37	0.756756756756757	0.122923588039867	Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference.	0
38166	38166	K15-2001	Introduction	29	38	0.783783783783784	0.12624584717608	The resurgence of deep learning techniques opens the door for innovative approaches to this problem.	0
38167	38167	K15-2001	Introduction	30	39	0.810810810810811	0.129568106312292	"A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of ""standard"" feature-based learning techniques and ""deep"" representation learning techniques."	0
38168	38168	K15-2001	Introduction	31	40	0.837837837837838	0.132890365448505	The rest of this overview paper is structured as follows.	0
38169	38169	K15-2001	Introduction	32	41	0.864864864864865	0.136212624584718	In Section 2, we provide a concise definition of the shared task.	0
38170	38170	K15-2001	Introduction	33	42	0.891891891891892	0.13953488372093	We describe how the training and test data are prepared in Section 3.	0
38171	38171	K15-2001	Introduction	34	43	0.918918918918919	0.142857142857143	In Section 4, we present the evaluation protocol, metric and scorer.	0
38172	38172	K15-2001	Introduction	35	44	0.945945945945946	0.146179401993355	The different approaches that participants took in the shared task are summarized in Section 5.	0
38173	38173	K15-2001	Introduction	36	45	0.972972972972973	0.149501661129568	In Section 6, we present the ranking of participating systems and analyze the evaluation results.	0
38174	38174	K15-2001	Introduction	37	46	1.0	0.152823920265781	We present our conclusions in Section 7.	0
38175	38175	K15-2001	Task Definition	1	47	0.1	0.156146179401993	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
38176	38176	K15-2001	Task Definition	2	48	0.2	0.159468438538206	Specifically, given a newswire article as input, a participating system is asked to return a set of discourse relations contained in the text.	1
38177	38177	K15-2001	Task Definition	3	49	0.3	0.162790697674419	A discourse relation, as defined in the PDTB, from which the training data for the shared task is drawn, is a relation taking two abstract objects (events, states, facts, or propositions) as arguments.	1
38178	38178	K15-2001	Task Definition	4	50	0.4	0.166112956810631	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between abstract object units.	0
38179	38179	K15-2001	Task Definition	5	51	0.5	0.169435215946844	In the current version of the PDTB, non-explicit relations are inferred only between adjacent units.	0
38180	38180	K15-2001	Task Definition	6	52	0.6	0.172757475083056	Each discourse relation is labeled with a sense selected from a sense hierarchy, and its arguments are generally in the form of sentences, clauses, or in some rare cases, noun phrases.	0
38181	38181	K15-2001	Task Definition	7	53	0.7	0.176079734219269	To detect a discourse relation, a participating system needs to:	0
38182	38182	K15-2001	Task Definition	8	54	0.8	0.179401993355482	1. Identify the text span of an explicit discourse connective, if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments;	0
38183	38183	K15-2001	Task Definition	9	55	0.9	0.182724252491694	"4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
38184	38184	K15-2001	Task Definition	10	56	1.0	0.186046511627907	3 Data	0
38185	38185	K15-2001	Training and Development	1	57	0.041666666666667	0.18936877076412	The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse Tree-Bank 2.0. (PDTB-2.0.)	0
38186	38186	K15-2001	Training and Development	2	58	0.083333333333333	0.192691029900332	(Prasad et al., 2008;	0
38187	38187	K15-2001	Training and Development	3	59	0.125	0.196013289036545	Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005).	0
38188	38188	K15-2001	Training and Development	4	60	0.166666666666667	0.199335548172757	The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text.	0
38189	38189	K15-2001	Training and Development	5	61	0.208333333333333	0.20265780730897	Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two.	0
38190	38190	K15-2001	Training and Development	6	62	0.25	0.205980066445183	A limited but well-defined set of implicit relations are also annotated within sentences.	0
38191	38191	K15-2001	Training and Development	7	63	0.291666666666667	0.209302325581395	Arguments of relations are annotated in each case, following the minimality principle for selecting all and only the material needed to interpret the relation.	0
38192	38192	K15-2001	Training and Development	8	64	0.333333333333333	0.212624584717608	For explicit connectives, Arg2, which is defined as the argument with which the connective is syntactically associated, is in the same sentence as the connective (though not necessarily string adjacent), but Arg1, defined simply as the other argument, is unconstrained in terms of its distance from the connective and can be found anywhere in the text (Exs. 1-3).	0
38193	38193	K15-2001	Training and Development	9	65	0.375	0.215946843853821	(All the following PDTB examples shown highlight Arg1 (in italics), Arg2 (in boldface), expressions realizing the relation (underlined), sense (in parentheses), and the WSJ file number for the text with the example (in square brackets)).	0
38194	38194	K15-2001	Training and Development	10	66	0.416666666666667	0.219269102990033	( Between adjacent sentences unrelated by any explicit connective, four scenarios hold: (a) the sentences may be related by a discourse relation that has no lexical realization, in which case a connective (called an Implicit connective) is inserted to express the inferred relation (Ex. 4), (b) the sentences may be related by a discourse relation that is realized by some alternative non-connective expression (called AltLex), in which case these alternative lexicalizations are annotated as the carriers of the relation (Ex. 5), (c) the sentences may be related not by a discourse relation realizable by a connective or AltLex, but by an entity-based coherence relation, in which case the presence of such a relation is labeled EntRel (Ex 6), and (d) the sentences may not be related at all, in which case they are labeled NoRel.	0
38195	38195	K15-2001	Training and Development	11	67	0.458333333333333	0.222591362126246	Relations annotated in these four scenarios are collectively referred to as Non-Explicit relations in this paper.	0
38196	38196	K15-2001	Training and Development	12	68	0.5	0.225913621262458	(5)	0
38197	38197	K15-2001	Training and Development	13	69	0.541666666666667	0.229235880398671	Now, GM appears to be stepping up the pace of its factory consolidation to get in shape for the 1990s.	0
38198	38198	K15-2001	Training and Development	14	70	0.583333333333333	0.232558139534884	In addition to the argument structure of relations, the PDTB provides sense annotation for each discourse relation, capturing the polysemy of connectives.	0
38199	38199	K15-2001	Training and Development	15	71	0.625	0.235880398671096	Senses are organized in a three-level hierarchy, with 4 top-level semantic classes.	0
38200	38200	K15-2001	Training and Development	16	72	0.666666666666667	0.239202657807309	For each class, a second level of types is defined, and there are 16 such types.	0
38201	38201	K15-2001	Training and Development	17	73	0.708333333333333	0.242524916943522	There is a third level of subtype which provides further refinement to the second level types.	0
38202	38202	K15-2001	Training and Development	18	74	0.75	0.245847176079734	In the PDTB annotation, annotators are allowed back off to a higher level in the sense hierarchy if they are not certain about a lower level sense.	0
38203	38203	K15-2001	Training and Development	19	75	0.791666666666667	0.249169435215947	That is, if they cannot distinguish between the subtypes under a type sense, they can just annotate the type level sense, and if there is further uncertainty in choosing among the types under a class sense, they can just annotate the class level sense.	0
38204	38204	K15-2001	Training and Development	20	76	0.833333333333333	0.252491694352159	Most of the discourse relation instances in the PDTB are annotated with at least a type level sense, but there are also a small number annotated with only a class level sense.	0
38205	38205	K15-2001	Training and Development	21	77	0.875	0.255813953488372	The PDTB also provides annotations of attribution over all discourse relations and each of their arguments, as well as of text spans considered as supplementary to arguments of relations.	0
38206	38206	K15-2001	Training and Development	22	78	0.916666666666667	0.259136212624585	However, both of these annotation types are excluded from the shared task.	0
38207	38207	K15-2001	Training and Development	23	79	0.958333333333333	0.262458471760797	PDTB-2.0. contains annotations of 40,600 discourse relations, distributed into the following five types: 18,459 Explicit relations, 16,053 Implicit relations, 624 AltLex relations, 5,210 EntRel relations, and 254 NoRel relations.	0
38208	38208	K15-2001	Training and Development	24	80	1.0	0.26578073089701	We provide Sections 2-21 of the PDTB 2.0 release as the training set, and Section 22 as the development set.	0
38209	38209	K15-2001	Test Data	1	81	0.333333333333333	0.269102990033223	We provide two test sets for the shared task: Section 23 of the PDTB, and a blind test set we prepared especially for the shared task.	0
38210	38210	K15-2001	Test Data	2	82	0.666666666666667	0.272425249169435	The official ranking of the systems is based on their performance on the blind test set.	0
38211	38211	K15-2001	Test Data	3	83	1.0	0.275747508305648	In this section, we provide a detailed description of how the blind test set was prepared.	0
38212	38212	K15-2001	Data Selection and Post-processing	1	84	0.0625	0.27906976744186	For the blind test data, 30,158 words of untokenized English newswire texts were selected from a dump of English Wikinews 2 , accessed 22nd October 2014, and annotated in accordance with PDTB 2.0 guidelines.	0
38213	38213	K15-2001	Data Selection and Post-processing	2	85	0.125	0.282392026578073	The raw Wikinews data was pre-processed as follows:	0
38214	38214	K15-2001	Data Selection and Post-processing	3	86	0.1875	0.285714285714286	•	0
38215	38215	K15-2001	Data Selection and Post-processing	4	87	0.25	0.289036544850498	News articles were extracted from the Wikinews XML dump 3 using the publicly available WikiExtractor.	0
38216	38216	K15-2001	Data Selection and Post-processing	5	88	0.3125	0.292358803986711	py script.	0
38217	38217	K15-2001	Data Selection and Post-processing	6	89	0.375	0.295681063122924	4	0
38218	38218	K15-2001	Data Selection and Post-processing	7	90	0.4375	0.299003322259136	• Additional processing was done to remove any remaining XML information and produce a raw text version of each article (including its title).	0
38219	38219	K15-2001	Data Selection and Post-processing	8	91	0.5	0.302325581395349	•	0
38220	38220	K15-2001	Data Selection and Post-processing	9	92	0.5625	0.305647840531561	All paragraphs were double spaced to ease paragraph boundary identification.	0
38221	38221	K15-2001	Data Selection and Post-processing	10	93	0.625	0.308970099667774	•	0
38222	38222	K15-2001	Data Selection and Post-processing	11	94	0.6875	0.312292358803987	Each article was named according to its unique Wikinews ID such that it is accessible online at http://en.wikinews.org/ wiki?curid=ID.	0
38223	38223	K15-2001	Data Selection and Post-processing	12	95	0.75	0.315614617940199	Initially, 30k words of text were selected from this processed data at random.	0
38224	38224	K15-2001	Data Selection and Post-processing	13	96	0.8125	0.318936877076412	However, it soon became apparent that some texts were too short for PDTB-style annotation or otherwise still contained remnant XML errors.	0
38225	38225	K15-2001	Data Selection and Post-processing	14	97	0.875	0.322259136212625	Another issue was that since Wikinews texts are written by members of the public, rather than professionally trained journalists, some articles were considered as not up to the same standards of spelling and grammar as the WSJ texts in the PDTB.	0
38226	38226	K15-2001	Data Selection and Post-processing	15	98	0.9375	0.325581395348837	For these reasons, despite making the decision to allow the correction of extremely minor errors (such as obvious typos and occasional article or preposition errors), just under half of the original 30k word random selection was ultimately deemed unsuitable for annotation.	0
38227	38227	K15-2001	Data Selection and Post-processing	16	99	1.0	0.32890365448505	Consequently, the remaining texts were selected manually from Wikinews, with a slight preference for longer articles with many multi-sentence paragraphs that are more consistent with WSJ-style texts.	0
38228	38228	K15-2001	Annotations	1	100	0.066666666666667	0.332225913621262	Annotation of the blind test set was carried out by two of the shared task organizers, one of whom (fifth author) was the main annotator (MA) while the other (fourth author), a lead developer of the PDTB, acted as the reviewing annotator (RA), reviewing each relation annotated by the MA and recording agreement or disagreement.	0
38229	38229	K15-2001	Annotations	2	101	0.133333333333333	0.335548172757475	Annotation involved marking the relation type (Explicit, Implicit, AltLex, EntRel, NoRel), relation realization (explicit connective, implicit connective, Al-tLex expression), arguments (Arg1 and Arg2), and sense of a discourse relation, using the PDTB annotation tool.	0
38230	38230	K15-2001	Annotations	3	102	0.2	0.338870431893688	5 Unlike the PDTB guidelines, we did not allow back-off to the top class level during annotation.	0
38231	38231	K15-2001	Annotations	4	103	0.266666666666667	0.3421926910299	Every relation was annotated with a sense chosen from at least the second type level.	0
38232	38232	K15-2001	Annotations	5	104	0.333333333333333	0.345514950166113	5 https://www.seas.upenn.edu/˜pdtb/tools.shtml# annotator	0
38233	38233	K15-2001	Annotations	6	105	0.4	0.348837209302326	Also different from the PDTB, attribution spans or attribution features were not annotated.	0
38234	38234	K15-2001	Annotations	7	106	0.466666666666667	0.352159468438538	Before commencing official annotation, MA was trained in PDTB-2.0. style annotation by RA.	0
38235	38235	K15-2001	Annotations	8	107	0.533333333333333	0.355481727574751	A review of the guidelines was followed by double blind annotation (by MA and RA) of a small number of WSJ texts not previously annotated in the PDTB, and differences were then compared and discussed.	0
38236	38236	K15-2001	Annotations	9	108	0.6	0.358803986710963	MA then also underwent self-training by first annotating some WSJ texts that were already annotated in the PDTB, and then comparing these annotations, to further strengthen knowledge of the guidelines.	0
38237	38237	K15-2001	Annotations	10	109	0.666666666666667	0.362126245847176	After the training period, the entire blind test data was annotated by MA over a period of a few weeks, and then reviewed by RA.	0
38238	38238	K15-2001	Annotations	11	110	0.733333333333333	0.365448504983389	Disagreements during the review were manually recorded using a formal scheme addressing all aspects of the annotation, including relation type, explicit connective identification, senses, and each of the arguments.	0
38239	38239	K15-2001	Annotations	12	111	0.8	0.368770764119601	This was done to verify the integrity of the blind test data and keep a record of any confusion or difficulty encountered during annotation.	0
38240	38240	K15-2001	Annotations	13	112	0.866666666666667	0.372093023255814	Manual entry of disagreements was done within the tool interface, through its commenting feature.	0
38241	38241	K15-2001	Annotations	14	113	0.933333333333333	0.375415282392027	A recorded comment in the tool is unique to a relation token and is recorded in a stand-off style.	0
38242	38242	K15-2001	Annotations	15	114	1.0	0.378737541528239	Disagreements were later resolved by consensus between MA and RA.	0
38243	38243	K15-2001	Inter-annotator Agreement	1	115	0.05	0.382059800664452	The record of disagreements was utilized to compute inter-annotator agreement between MA and RA.	0
38244	38244	K15-2001	Inter-annotator Agreement	2	116	0.1	0.385382059800664	The overall agreement was 76.5%, which represents the percentage of relations on which there was complete agreement.	0
38245	38245	K15-2001	Inter-annotator Agreement	3	117	0.15	0.388704318936877	Agreement on explicit connective identification was 96.0%, representing the percentage of explicit connectives that both MA and RA identified as discourse connectives.	0
38246	38246	K15-2001	Inter-annotator Agreement	4	118	0.2	0.39202657807309	"We note here that if a connective was identified in the blind test data, but was not annotated in the PDTB despite its occurrence in the WSJ (e.g.,""after which time"", ""despite""), we did not consider it a potential connective and hence did not include it in the agreement calculation."	0
38247	38247	K15-2001	Inter-annotator Agreement	5	119	0.25	0.395348837209302	When the textual context allowed it, such expressions were instead marked as AltLex.	0
38248	38248	K15-2001	Inter-annotator Agreement	6	120	0.3	0.398671096345515	We also did a more fine-grained assessment to determine agreement on Arg1, Arg2, Arg1+Arg2 (i.e., the number of relations on which the annotators agreed on both Arg1 and Arg2), and senses.	0
38249	38249	K15-2001	Inter-annotator Agreement	7	121	0.35	0.401993355481728	This was done for all the relation types considered together, as well as for Explicit and Non-Explicit relation types separately.	0
38250	38250	K15-2001	Inter-annotator Agreement	8	122	0.4	0.40531561461794	Sense disagreement was computed using the CoNLL sense classification scheme (see Section 3.3), even though the annotation was done using the full PDTB sense classification scheme (see Table 2).	0
38251	38251	K15-2001	Inter-annotator Agreement	9	123	0.45	0.408637873754153	The agreement percentages are shown in Table 1.	0
38252	38252	K15-2001	Inter-annotator Agreement	10	124	0.5	0.411960132890365	When multiple senses were provided for a relation, a disagreement on any of the senses was counted as disagreement for the relation; disagreement on more than one of the senses was counted only once.	0
38253	38253	K15-2001	Inter-annotator Agreement	11	125	0.55	0.415282392026578	Absence of a second sense by one annotator when the other did provide one was also counted as disagreement.	0
38254	38254	K15-2001	Inter-annotator Agreement	12	126	0.6	0.418604651162791	As the table shows, agreement on senses was reasonably high overall (85.5%), with agreement for Explicit relations expectedly higher (91.0%) than for Non-Explicit relations (80.9%).	0
38255	38255	K15-2001	Inter-annotator Agreement	13	127	0.65	0.421926910299003	Overall agreement on arguments was also high, but in contrast to the senses, agreement was generally higher for the Non-Explicit than for Explicit relations.	0
38256	38256	K15-2001	Inter-annotator Agreement	14	128	0.7	0.425249169435216	Agreement on the Arg1 of Explicit relations (89.6%) is, not surprisingly, lower than for Arg2 (98.7%), because the Arg1 of Explicit relations can be non-adjacent to the connective's sentence or clause, and thus, harder to identify.	0
38257	38257	K15-2001	Inter-annotator Agreement	15	129	0.75	0.428571428571429	For the Non-Explicit relations, in contrast, but again to be expected, because of the argument adjacency constraint for such relations, agreement on Arg1 (95.0%) and Arg2 (96.4%) shows minimal difference.	0
38258	38258	K15-2001	Inter-annotator Agreement	16	130	0.8	0.431893687707641	Table 1 also provides the percentage of relations with agreement on both Arg1 and Arg2, showing this to be higher for Non-Explicit relations (92.4%) than for Explicit relations (88.7%).	0
38259	38259	K15-2001	Inter-annotator Agreement	17	131	0.85	0.435215946843854	Compared to the agreement reported for the PDTB (Prasad et al., 2008;	0
38260	38260	K15-2001	Inter-annotator Agreement	18	132	0.9	0.438538205980066	Miltsakaki et al., 2004), the results obtained here (See Table 1) are slightly better.	0
38261	38261	K15-2001	Inter-annotator Agreement	19	133	0.95	0.441860465116279	PDTB agreement on Arg1 and Arg2 of Explicit relations is reported to be 86.3% and 94.1%, respectively, whereas overall agreement on arguments of Non-Explicit relations is 85.1%.	0
38262	38262	K15-2001	Inter-annotator Agreement	20	134	1.0	0.445182724252492	For the senses, although the CoNLL senses do not exactly align with the PDTB senses, a rough correspondence can be assumed between the CoNLL classification as a whole and the type and subtype levels of the PDTB classification, for which PDTB reports 84% and 80%, respectively.	0
38263	38263	K15-2001	Adapting the PDTB Annotation for the shared task	1	135	0.020408163265306	0.448504983388704	The discourse relations annotated in the PDTB have many different elements, and it is impracti-cal to predict all of them in the context of a shared task where participants have a relatively short time frame in which to complete the task.	0
38264	38264	K15-2001	Adapting the PDTB Annotation for the shared task	2	136	0.040816326530612	0.451827242524917	As a result, we had to make a number of exclusions and simplifications, which we describe below.	0
38265	38265	K15-2001	Adapting the PDTB Annotation for the shared task	3	137	0.061224489795918	0.45514950166113	The core elements of a discourse relation are the two abstract objects as its arguments.	0
38266	38266	K15-2001	Adapting the PDTB Annotation for the shared task	4	138	0.081632653061225	0.458471760797342	In addition to this, some discourse relations include supplementary information that is relevant but not necessary (as per the minimality principle) to the interpretation of a discourse relation.	0
38267	38267	K15-2001	Adapting the PDTB Annotation for the shared task	5	139	0.102040816326531	0.461794019933555	"Supplementary information is associated with arguments, and optionally marked with the labels ""Sup1"", for material supplementary to Arg1, and ""Sup2"", for material supplementary to Arg2."	0
38268	38268	K15-2001	Adapting the PDTB Annotation for the shared task	6	140	0.122448979591837	0.465116279069767	An example of a Sup1 annotation is shown in (7).	0
38269	38269	K15-2001	Adapting the PDTB Annotation for the shared task	7	141	0.142857142857143	0.46843853820598	In the shared task, supplementary information is excluded from evaluation when computing argument spans.	0
38270	38270	K15-2001	Adapting the PDTB Annotation for the shared task	8	142	0.163265306122449	0.471760797342193	Also excluded from evaluation, to make the shared task manageable, are attribution relations annotated in PDTB.	0
38271	38271	K15-2001	Adapting the PDTB Annotation for the shared task	9	143	0.183673469387755	0.475083056478405	"An example of an explicit attribution is ""he says"" in (8), marked over Arg1."	0
38272	38272	K15-2001	Adapting the PDTB Annotation for the shared task	10	144	0.204081632653061	0.478405315614618	The PDTB senses form a hierarchical system of three levels, consisting of 4 classes, 16 types, and 23 subtypes.	0
38273	38273	K15-2001	Adapting the PDTB Annotation for the shared task	11	145	0.224489795918367	0.481727574750831	While all classes are divided into multiple types, some types do not have subtypes.	0
38274	38274	K15-2001	Adapting the PDTB Annotation for the shared task	12	146	0.244897959183673	0.485049833887043	Previous work on PDTB sense classification has mostly focused on classes (Pitler et al., 2009;	0
38275	38275	K15-2001	Adapting the PDTB Annotation for the shared task	13	147	0.26530612244898	0.488372093023256	Zhou et al., 2010;	0
38276	38276	K15-2001	Adapting the PDTB Annotation for the shared task	14	148	0.285714285714286	0.491694352159468	Park and Cardie, 2012;	0
38277	38277	K15-2001	Adapting the PDTB Annotation for the shared task	15	149	0.306122448979592	0.495016611295681	Biran and McKeown, 2013;	0
38278	38278	K15-2001	Adapting the PDTB Annotation for the shared task	16	150	0.326530612244898	0.498338870431894	Li and Nenkova, 2014;	0
38279	38279	K15-2001	Adapting the PDTB Annotation for the shared task	17	151	0.346938775510204	0.501661129568106	Rutherford and Xue, 2014).	0
38280	38280	K15-2001	Adapting the PDTB Annotation for the shared task	18	152	0.36734693877551	0.504983388704319	The senses that are the target of prediction in the CoNLL-2015 shared task are primarily based on the second-level types and a selected number of third-level subtypes.	0
38281	38281	K15-2001	Adapting the PDTB Annotation for the shared task	19	153	0.387755102040816	0.508305647840532	We made a few modifications to make the distinctions clearer and their distributions more balanced, and these changes are presented in Table 2.	0
38282	38282	K15-2001	Adapting the PDTB Annotation for the shared task	20	154	0.408163265306122	0.511627906976744	First, senses in the PDTB that have distinctions that are too subtle and thus too difficult to predict are collapsed.	0
38283	38283	K15-2001	Adapting the PDTB Annotation for the shared task	21	155	0.428571428571429	0.514950166112957	Senses that involve a change from the PDTB senses are marked * .	0
38284	38284	K15-2001	Adapting the PDTB Annotation for the shared task	22	156	0.448979591836735	0.518272425249169	"For example, ""Contingency."	0
38285	38285	K15-2001	Adapting the PDTB Annotation for the shared task	23	157	0.469387755102041	0.521594684385382	"Pragmatic cause"" is merged into ""Contingency."	0
38286	38286	K15-2001	Adapting the PDTB Annotation for the shared task	24	158	0.489795918367347	0.524916943521595	Cause.	0
38287	38287	K15-2001	Adapting the PDTB Annotation for the shared task	25	159	0.510204081632653	0.528239202657807	"Reason"", and ""Contingency."	0
38288	38288	K15-2001	Adapting the PDTB Annotation for the shared task	26	160	0.530612244897959	0.53156146179402	"Pragmatic condition"" is merged into ""Contingency."	0
38289	38289	K15-2001	Adapting the PDTB Annotation for the shared task	27	161	0.551020408163265	0.534883720930232	"Condition""."	0
38290	38290	K15-2001	Adapting the PDTB Annotation for the shared task	28	162	0.571428571428571	0.538205980066445	"Second, the distinction between ""Expansion."	0
38291	38291	K15-2001	Adapting the PDTB Annotation for the shared task	29	163	0.591836734693878	0.541528239202658	"Conjunction"" and ""Expansion."	0
38292	38292	K15-2001	Adapting the PDTB Annotation for the shared task	30	164	0.612244897959184	0.54485049833887	"List"" is not clear in the PDTB and in fact, they seem very similar for the most part, so the latter is merged into the former."	0
38293	38293	K15-2001	Adapting the PDTB Annotation for the shared task	31	165	0.63265306122449	0.548172757475083	"Third, while ""Expansion."	0
38294	38294	K15-2001	Adapting the PDTB Annotation for the shared task	32	166	0.653061224489796	0.551495016611296	Alternative.	0
38295	38295	K15-2001	Adapting the PDTB Annotation for the shared task	33	167	0.673469387755102	0.554817275747508	"Conjunctive"" and ""Expansion."	0
38296	38296	K15-2001	Adapting the PDTB Annotation for the shared task	34	168	0.693877551020408	0.558139534883721	Alternative.	0
38297	38297	K15-2001	Adapting the PDTB Annotation for the shared task	35	169	0.714285714285714	0.561461794019934	"Disjunctive"" are merged into ""Expansion."	0
38298	38298	K15-2001	Adapting the PDTB Annotation for the shared task	36	170	0.73469387755102	0.564784053156146	"Alternative"", a third subtype of ""Expansion."	0
38299	38299	K15-2001	Adapting the PDTB Annotation for the shared task	37	171	0.755102040816326	0.568106312292359	"Alternative"", ""Expansion."	0
38300	38300	K15-2001	Adapting the PDTB Annotation for the shared task	38	172	0.775510204081633	0.571428571428571	Alternative.	0
38301	38301	K15-2001	Adapting the PDTB Annotation for the shared task	39	173	0.795918367346939	0.574750830564784	"Chosen Alternative"" is kept as a separate category as its meaning involves more than presentation of alternatives."	0
38302	38302	K15-2001	Adapting the PDTB Annotation for the shared task	40	174	0.816326530612245	0.578073089700997	"Finally, while ""EntRel"" relations are not treated as discourse relations in the PDTB, we have included this category as a sense for sense classification since they are a kind of coherence relation and we require systems to label these relations in the shared task."	0
38303	38303	K15-2001	Adapting the PDTB Annotation for the shared task	41	175	0.836734693877551	0.581395348837209	"In contrast, instances annotated with ""NoRel"" are not treated as discourse relations and are excluded from the training, development and test data sets."	0
38304	38304	K15-2001	Adapting the PDTB Annotation for the shared task	42	176	0.857142857142857	0.584717607973422	This means that a system needs to treat them as negative samples and not identify them as discourse relations.	0
38305	38305	K15-2001	Adapting the PDTB Annotation for the shared task	43	177	0.877551020408163	0.588039867109635	These changes have resulted in a flat list of 15 sense categories that need to be predicted in the shared task.	0
38306	38306	K15-2001	Adapting the PDTB Annotation for the shared task	44	178	0.897959183673469	0.591362126245847	A comparison of the PDTB senses and the senses used in the CoNLL shared task is presented in Table 2.	0
38307	38307	K15-2001	Adapting the PDTB Annotation for the shared task	45	179	0.918367346938775	0.59468438538206	Table 3: Distribution of senses across the four relation types in the WSJ PDTB data used for the shared task.	0
38308	38308	K15-2001	Adapting the PDTB Annotation for the shared task	46	180	0.938775510204082	0.598006644518272	The total numbers of the relations here are less than in the complete PDTB release because some sections (00, 01, and 24) are excluded for the shared task, following standard split of WSJ data in the evaluation community.	0
38309	38309	K15-2001	Adapting the PDTB Annotation for the shared task	47	181	0.959183673469388	0.601328903654485	We are intentionally withholding distribution over the blind test set in case there is a repeat of the SDP shared task using the same test set.	0
38310	38310	K15-2001	Adapting the PDTB Annotation for the shared task	48	182	0.979591836734694	0.604651162790698	Table 3 shows the distribution of the senses across the four discourse relations within the WSJ PDTB data 6 .	0
38311	38311	K15-2001	Adapting the PDTB Annotation for the shared task	49	183	1.0	0.60797342192691	We are intentionally withholding the sense distribution across the blind test set in case there is a repeat of the SDP shared task using the same test set.	0
38312	38312	K15-2001	Evaluation	1	184	0.055555555555556	0.611295681063123	Closed and open tracks	0
38313	38313	K15-2001	Evaluation	2	185	0.111111111111111	0.614617940199336	In keeping with the CoNLL shared task tradition, participating systems were evaluated in two tracks, a closed track and an open track.	0
38314	38314	K15-2001	Evaluation	3	186	0.166666666666667	0.617940199335548	A participating system in the closed track could only use the provided PDTB training set but was allowed to process the data using any publicly available (i.e., non-proprietary) natural language processing tools such as syntactic parsers and semantic role labelers.	0
38315	38315	K15-2001	Evaluation	4	187	0.222222222222222	0.621262458471761	In contrast, in the open track, a participating system could not only use any publicly available NLP tools to process the data, but also any publicly available (i.e., non-proprietary) data for training.	0
38316	38316	K15-2001	Evaluation	5	188	0.277777777777778	0.624584717607973	A participating team could choose to participate in the closed track or the open track, or both.	0
38317	38317	K15-2001	Evaluation	6	189	0.333333333333333	0.627906976744186	The motivation for having two tracks in CoNLL shared tasks was to isolate the contribution of algorithms and resources to a particular task.	0
38318	38318	K15-2001	Evaluation	7	190	0.388888888888889	0.631229235880399	In the closed track, the resources are held constant so that the advantages of different algorithms and models can be more meaningfully compared.	0
38319	38319	K15-2001	Evaluation	8	191	0.444444444444444	0.634551495016611	In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task.	0
38320	38320	K15-2001	Evaluation	9	192	0.5	0.637873754152824	This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set.	0
38321	38321	K15-2001	Evaluation	10	193	0.555555555555556	0.641196013289037	However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014).	0
38322	38322	K15-2001	Evaluation	11	194	0.611111111111111	0.644518272425249	In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set.	0
38323	38323	K15-2001	Evaluation	12	195	0.666666666666667	0.647840531561462	As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the train- 6 There is a small number of instances in the PDTB training set that are only annotated with the class level sense.	0
38324	38324	K15-2001	Evaluation	13	196	0.722222222222222	0.651162790697674	We did not take them out of the training set for the sake of completeness.	0
38325	38325	K15-2001	Evaluation	14	197	0.777777777777778	0.654485049833887	ing set:	0
38326	38326	K15-2001	Evaluation	15	198	0.833333333333333	0.6578073089701	To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation processed with state-of-the-art NLP tools:	0
38327	38327	K15-2001	Evaluation	16	199	0.888888888888889	0.661129568106312	•	0
38328	38328	K15-2001	Evaluation	17	200	0.944444444444444	0.664451827242525	Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014))	0
38329	38329	K15-2001	Evaluation	18	201	1.0	0.667774086378738	As it turned out, all of the teams this year chose to participate in the closed track.	0
38330	38330	K15-2001	Evaluation Platform: TIRA	1	202	0.066666666666667	0.67109634551495	We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012;	0
38331	38331	K15-2001	Evaluation Platform: TIRA	2	203	0.133333333333333	0.674418604651163	Potthast et al., 2014).	0
38332	38332	K15-2001	Evaluation Platform: TIRA	3	204	0.2	0.677740863787375	Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation.	0
38333	38333	K15-2001	Evaluation Platform: TIRA	4	205	0.266666666666667	0.681063122923588	This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their systems on the test sets without actually seeing the test sets.	0
38334	38334	K15-2001	Evaluation Platform: TIRA	5	206	0.333333333333333	0.684385382059801	The organizers would then inspect the evaluation results, and verify that participating systems yielded acceptable output.	0
38335	38335	K15-2001	Evaluation Platform: TIRA	6	207	0.4	0.687707641196013	This evaluation protocol allowed us to maintain the integrity of the blind test set and reduce the organizational overhead.	0
38336	38336	K15-2001	Evaluation Platform: TIRA	7	208	0.466666666666667	0.691029900332226	On TIRA, the blind test set can only be accessed in the evaluation environment, and the evaluation results are automatically collected.	0
38337	38337	K15-2001	Evaluation Platform: TIRA	8	209	0.533333333333333	0.694352159468439	Participants cannot see any part of the test sets and hence cannot do iterative development based on the test set performance, which preserves the integrity of the evaluation.	0
38338	38338	K15-2001	Evaluation Platform: TIRA	9	210	0.6	0.697674418604651	Most importantly, this evaluation platform promotes replicability, which is very crucial for proper evaluation of scientific progress.	0
38339	38339	K15-2001	Evaluation Platform: TIRA	10	211	0.666666666666667	0.700996677740864	Reproducing all of the results is just a matter of a button click on TIRA.	0
38340	38340	K15-2001	Evaluation Platform: TIRA	11	212	0.733333333333333	0.704318936877076	All of the results presented in this paper, along with the trained models and the software, are archived and available for distribution upon request to the organizers and upon the permission of the participating team, who holds the copyrights to the software.	0
38341	38341	K15-2001	Evaluation Platform: TIRA	12	213	0.8	0.707641196013289	Replicability also helps speed up the research and development in discourse parsing.	0
38342	38342	K15-2001	Evaluation Platform: TIRA	13	214	0.866666666666667	0.710963455149502	Anyone wanting to extend or apply any of the approaches proposed by a shared task participant does not have to re-implement the model from scratch.	0
38343	38343	K15-2001	Evaluation Platform: TIRA	14	215	0.933333333333333	0.714285714285714	They can request a clone of the virtual machine where the participating system is deployed, and then implement their extension based off the original source code.	0
38344	38344	K15-2001	Evaluation Platform: TIRA	15	216	1.0	0.717607973421927	Any extension effort also benefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation.	0
38345	38345	K15-2001	Evaluation metrics and scorer	1	217	0.037037037037037	0.72093023255814	A shallow discourse parser is evaluated based on the end-to-end F 1 score on a per-discourse relation basis.	0
38346	38346	K15-2001	Evaluation metrics and scorer	2	218	0.074074074074074	0.724252491694352	The input to the system consists of documents with gold-standard word tokens along with their automatic parses.	0
38347	38347	K15-2001	Evaluation metrics and scorer	3	219	0.111111111111111	0.727574750830565	We do not pre-identify the discourse connectives or any other elements of the discourse annotation.	0
38348	38348	K15-2001	Evaluation metrics and scorer	4	220	0.148148148148148	0.730897009966777	The shallow discourse parser must output a list of discourse relations that consist of the argument spans and their labels, explicit discourse connectives where applicable, and the senses.	0
38349	38349	K15-2001	Evaluation metrics and scorer	5	221	0.185185185185185	0.73421926910299	The F 1 score is computed based on the number of predicted relations that match a gold standard relation exactly.	0
38350	38350	K15-2001	Evaluation metrics and scorer	6	222	0.222222222222222	0.737541528239203	A relation is correctly predicted if (a) the discourse connective is correctly detected (for Explicit discourse relations), (b) the sense of the discourse connective is correctly predicted, and (c) the text spans of its two arguments are correctly predicted (Arg1 and Arg2).	0
38351	38351	K15-2001	Evaluation metrics and scorer	7	223	0.259259259259259	0.740863787375415	Although the submissions are ranked based on the relation F 1 score, the scorer also provides component-wise evaluation with error propagation.	0
38352	38352	K15-2001	Evaluation metrics and scorer	8	224	0.296296296296296	0.744186046511628	The scorer computes the precision, recall, and F 1 for the following 7 :	0
38353	38353	K15-2001	Evaluation metrics and scorer	9	225	0.333333333333333	0.747508305647841	• Arg1 and Arg2 identification.	0
38354	38354	K15-2001	Evaluation metrics and scorer	10	226	0.37037037037037	0.750830564784053	•	0
38355	38355	K15-2001	Evaluation metrics and scorer	11	227	0.407407407407407	0.754152823920266	Sense classification with error propagation from discourse connective and argument identification.	0
38356	38356	K15-2001	Evaluation metrics and scorer	12	228	0.444444444444444	0.757475083056478	For purposes of evaluation, an explicit discourse connective predicted by the parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the predicted connective to be a subset of the tokens in the gold raw connective.	0
38357	38357	K15-2001	Evaluation metrics and scorer	13	229	0.481481481481481	0.760797342192691	We provide a function that maps discourse connectives to their corresponding heads.	0
38358	38358	K15-2001	Evaluation metrics and scorer	14	230	0.518518518518518	0.764119601328904	The notion of discourse connective head is not the same as its syntactic head.	0
38359	38359	K15-2001	Evaluation metrics and scorer	15	231	0.555555555555556	0.767441860465116	Rather, it is thought of as the part of the connective conveying its core meaning.	0
38360	38360	K15-2001	Evaluation metrics and scorer	16	232	0.592592592592593	0.770764119601329	"For example, the head of the discourse connective ""At least not when"" is ""when"", and the head of ""five minutes before"" is ""before""."	0
38361	38361	K15-2001	Evaluation metrics and scorer	17	233	0.62962962962963	0.774086378737542	The non-head part of the connective serves to semantically restrict the interpretation of the connective.	0
38362	38362	K15-2001	Evaluation metrics and scorer	18	234	0.666666666666667	0.777408637873754	Although Implicit discourse relations are annotated with an implicit connective inserted between adjacent sentences, participants are not required to provide the inserted connective.	0
38363	38363	K15-2001	Evaluation metrics and scorer	19	235	0.703703703703704	0.780730897009967	They only need to output the sense of the discourse relation.	0
38364	38364	K15-2001	Evaluation metrics and scorer	20	236	0.740740740740741	0.784053156146179	Similarly, for AltLex relations, which are also annotated between adjacent sentences, participants are not required to output the text span of the AltLex expression, but only the sense.	0
38365	38365	K15-2001	Evaluation metrics and scorer	21	237	0.777777777777778	0.787375415282392	The EntRel relation is included as a sense in the shared task, and here, systems are required to correctly label the EntRel relation between adjacent sentence pairs.	0
38366	38366	K15-2001	Evaluation metrics and scorer	22	238	0.814814814814815	0.790697674418605	An argument is considered correctly identified if and only if it matches the corresponding gold standard argument span exactly, and is also correctly labeled (Arg1 or Arg2).	0
38367	38367	K15-2001	Evaluation metrics and scorer	23	239	0.851851851851852	0.794019933554817	Systems are not given any credit for partial match on argument spans.	0
38368	38368	K15-2001	Evaluation metrics and scorer	24	240	0.888888888888889	0.79734219269103	Sense classification evaluation is less straightforward, since senses are sometimes annotated partially or annotated with two senses.	0
38369	38369	K15-2001	Evaluation metrics and scorer	25	241	0.925925925925926	0.800664451827243	To be considered correct, the predicted sense for a relation must match one of the two senses if there is more than one sense.	0
38370	38370	K15-2001	Evaluation metrics and scorer	26	242	0.962962962962963	0.803986710963455	If the gold standard is partially annotated, the sense must match with the partially annotated sense.	0
38371	38371	K15-2001	Evaluation metrics and scorer	27	243	1.0	0.807308970099668	Additionally, the scorer provides a breakdown of the discourse parser performance for Explicit and Non-Explicit discourse relations.	0
38372	38372	K15-2001	Approaches	1	244	0.037037037037037	0.81063122923588	The Shallow Discourse Parsing (SDP) task this year requires the development of an end-to-end system that potentially involves many components.	0
38373	38373	K15-2001	Approaches	2	245	0.074074074074074	0.813953488372093	All participating systems adopt some variation of the pipeline architecture proposed by Lin et al (2014)  ing discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations.	0
38374	38374	K15-2001	Approaches	3	246	0.111111111111111	0.817275747508306	Most participating systems cast discourse connective identification and argument extraction as token-level sequence labeling tasks, while a few systems use rule-based approaches to extract the arguments.	0
38375	38375	K15-2001	Approaches	4	247	0.148148148148148	0.820598006644518	Sense determination is cast as a straightforward multi-category classification task.	0
38376	38376	K15-2001	Approaches	5	248	0.185185185185185	0.823920265780731	Most systems use machine learning techniques to determine the senses, but there are also systems that, due to lack of time, adopt a simple baseline approach that detects the most frequent sense based on the training data.	0
38377	38377	K15-2001	Approaches	6	249	0.222222222222222	0.827242524916943	"In terms of learning techniques, all participating systems except the two systems submitted by the Dublin team use standard ""shallow"" learning models that take binary features as input."	0
38378	38378	K15-2001	Approaches	7	250	0.259259259259259	0.830564784053156	For sequence labeling subtasks such as discourse connective identification and argument extraction, the preferred learning method is Conditional Random Fields (CRF).	0
38379	38379	K15-2001	Approaches	8	251	0.296296296296296	0.833887043189369	For sense determination, a variety of learning methods have been used, including Maximum Entropy, Support Vector Machines, and decision trees.	0
38380	38380	K15-2001	Approaches	9	252	0.333333333333333	0.837209302325581	In the last couple of years, neural networks have experienced a resurgence and have been shown to be effective in many natural language processing tasks.	0
38381	38381	K15-2001	Approaches	10	253	0.37037037037037	0.840531561461794	Neural network based models on discourse parsing have also started to appear (Ji and Eisenstein, 2014).	0
38382	38382	K15-2001	Approaches	11	254	0.407407407407407	0.843853820598007	"The use of neural networks for the SDP task this year represents a minority, presumably because researchers are still less familiar with neural network based techniques, compared with standard ""shallow"" learning techniques, and it is difficult to use a new learning technique to good effect within a short time window."	0
38383	38383	K15-2001	Approaches	12	255	0.444444444444444	0.847176079734219	In this shared task, only the Dublin University team attempted to use neural networks as a learning approach in their system components.	0
38384	38384	K15-2001	Approaches	13	256	0.481481481481481	0.850498338870432	In their first submission (Dublin I), Recurrent Neural Networks (RNN) are used for token level sequence labeling in the argument extraction task.	0
38385	38385	K15-2001	Approaches	14	257	0.518518518518518	0.853820598006645	In their second submission, paragraph embeddings are used in a neural network model to determine the senses of discourse relations.	0
38386	38386	K15-2001	Approaches	15	258	0.555555555555556	0.857142857142857	The discussion of learning techniques cannot be entirely separated from the use of features and the linguistic resources that are used to extract them.	0
38387	38387	K15-2001	Approaches	16	259	0.592592592592593	0.86046511627907	"Standard ""shallow"" architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings."	0
38388	38388	K15-2001	Approaches	17	260	0.62962962962963	0.863787375415282	For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses.	0
38389	38389	K15-2001	Approaches	18	261	0.666666666666667	0.867109634551495	Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015;	0
38390	38390	K15-2001	Approaches	19	262	0.703703703703704	0.870431893687708	Devi et al., 2015;	0
38391	38391	K15-2001	Approaches	20	263	0.740740740740741	0.87375415282392	Kong et al., 2015;	0
38392	38392	K15-2001	Approaches	21	264	0.777777777777778	0.877076411960133	Song et al., 2015;	0
38393	38393	K15-2001	Approaches	22	265	0.814814814814815	0.880398671096346	Stepanov et al., 2015;Wang and Lan, 2015;	0
38394	38394	K15-2001	Approaches	23	266	0.851851851851852	0.883720930232558	Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015).	0
38395	38395	K15-2001	Approaches	24	267	0.888888888888889	0.887043189368771	Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015;, Verb-Net classes (Devi et al., 2015;	0
38396	38396	K15-2001	Approaches	25	268	0.925925925925926	0.890365448504983	Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015;	0
38397	38397	K15-2001	Approaches	26	269	0.962962962962963	0.893687707641196	Kong et al., 2015;Wang and Lan, 2015).	0
38398	38398	K15-2001	Approaches	27	270	1.0	0.897009966777409	Table 4 provides a summary of the different approaches.	0
38399	38399	K15-2001	Results	1	271	0.05	0.900332225913621	Table 5 shows the performance of all participating systems across the three test evaluation sets: i) (Official)	0
38400	38400	K15-2001	Results	2	272	0.1	0.903654485049834	Blind test set; ii) Standard WSJ test set; iii) Standard WSJ development set.	0
38401	38401	K15-2001	Results	3	273	0.15	0.906976744186046	The official rankings are based on the blind test set annotated specifically for this shared task.	0
38402	38402	K15-2001	Results	4	274	0.2	0.910299003322259	The top-ranked system is the submission by East China Normal University (Wang and Lan, 2015).	0
38403	38403	K15-2001	Results	5	275	0.25	0.913621262458472	As discussed in Section 4, the evaluation metric is very strict, and is based on exact match for the extraction of argument spans.	0
38404	38404	K15-2001	Results	6	276	0.3	0.916943521594684	For the detection of discourse connectives, only the head of a discourse connective has to be correctly detected.	0
38405	38405	K15-2001	Results	7	277	0.35	0.920265780730897	Errors in the begin-ning of the pipeline will propagate to the end, and other than word tokenization, all input to the participating systems is automatically generated, so the overall accuracy reflects results in realistic situations.	0
38406	38406	K15-2001	Results	8	278	0.4	0.92358803986711	The scores are very low, with the top system achieving an overall parsing score of 24.00% (F1) on the blind test set and 29.69% (F1) on the Wall Street Journal (WSJ) test set.	0
38407	38407	K15-2001	Results	9	279	0.45	0.926910299003322	For comparison purposes, the National University of Singapore team re-implemented the state-of-the-art endto-end parser described in (Lin et al., 2014), and this system achieves an F1 of 19.98% on the WSJ test set.	0
38408	38408	K15-2001	Results	10	280	0.5	0.930232558139535	This shows that a fair amount of progress has been made against the Lin et al baseline.	0
38409	38409	K15-2001	Results	11	281	0.55	0.933554817275748	The rankings are generally consistent across the two test sets, with the largest change in ranking from the NTT team and the Goethe University team.	0
38410	38410	K15-2001	Results	12	282	0.6	0.93687707641196	This is perhaps not a coincidence: both teams used rule-based approaches to extract arguments.	0
38411	38411	K15-2001	Results	13	283	0.65	0.940199335548173	The rules worked well on the WSJ test set which draws from the same source as the development set, but might not adapt well to the blind test set, which is drawn from a different source.	0
38412	38412	K15-2001	Results	14	284	0.7	0.943521594684385	Machine-learning based approaches generally can better adapt to new data sets.	0
38413	38413	K15-2001	Results	15	285	0.75	0.946843853820598	Due to the short time frame participants had to complete an end-to-end task, teams chose to focus on either argument extraction components or the sense classification components, or in the case of sense classification, either focus on the classification of senses for Explicit relations or senses for Non-Explicit relations.	0
38414	38414	K15-2001	Results	16	286	0.8	0.950166112956811	A detailed breakdown of the performance for Explicit versus Non-Explicit discourse relations is presented in Table 6.	0
38415	38415	K15-2001	Results	17	287	0.85	0.953488372093023	In general, parser performance for Explicit discourse relations is much higher than that of Non-Explicit discourse relations.	0
38416	38416	K15-2001	Results	18	288	0.9	0.956810631229236	The difficulty for Non-Explicit discourse relations mostly stems from Non-Explicit sense classification.	0
38417	38417	K15-2001	Results	19	289	0.95	0.960132890365448	This is evidenced by the fact that even for systems that achieve higher argument extraction accuracy for Non-Explicit discourse relations than Explicit discourse relations, the overall parser accuracy is still lower for Non-Explicit relations.	0
38418	38418	K15-2001	Results	20	290	1.0	0.963455149501661	The lower accuracy in sense classification thus drags down the overall parser accuracy for Non-Explicit discourse relations.	0
38419	38419	K15-2001	Conclusions	1	291	0.090909090909091	0.966777408637874	Sixteen teams from three continents participated in the CoNLL-2015 Shared Task on shallow dis-	0
38420	38420	K15-2001	Conclusions	2	292	0.181818181818182	0.970099667774086	The rows are sorted by the parser performance of the participating systems on the Explicit task.	0
38421	38421	K15-2001	Conclusions	3	293	0.272727272727273	0.973421926910299	The Column O, E, I refer to official, Explicit and Non-Explicit task ranks respectively.	0
38422	38422	K15-2001	Conclusions	4	294	0.363636363636364	0.976744186046512	The blue highlighted rows indicate participants that did not attempt the Non-Explicit relation subtask.	0
38423	38423	K15-2001	Conclusions	5	295	0.454545454545455	0.980066445182724	The green highlighted row shows a team that probably overfitted the development set.	0
38424	38424	K15-2001	Conclusions	6	296	0.545454545454545	0.983388704318937	Finally, the red highlighted row indicates a team that possibly focused on the Explicit relations task and even though their overall rank was lower, they did very well on the Explicit relations subtask.	0
38425	38425	K15-2001	Conclusions	7	297	0.636363636363636	0.98671096345515	This is also the system that did not submit a paper, so we do not know more details.	0
38426	38426	K15-2001	Conclusions	8	298	0.727272727272727	0.990033222591362	course parsing.	0
38427	38427	K15-2001	Conclusions	9	299	0.818181818181818	0.993355481727575	The shared task required the development of an end-to-end system, and the best system achieved an F1 score of 24.0% on the blind test set, reflecting the serious error propagation problem in such a system.	0
38428	38428	K15-2001	Conclusions	10	300	0.909090909090909	0.996677740863787	The shared task exposed the most challenging aspect of shallow discourse parsing as a research problem, helping future research better calibrate their efforts.	0
38429	38429	K15-2001	Conclusions	11	301	1.0	1.0	The evaluation data sets and the scorer we prepared for the shared task will be a useful benchmark for future research on shallow discourse parsing.	0
42557	42557	D19-5719	title	1	1	1.0	0.005025125628141	Bacteria Biotope at BioNLP Open Shared Tasks 2019	0
42558	42558	D19-5719	abstract	1	2	0.2	0.010050251256281	This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019.	0
42559	42559	D19-5719	abstract	2	3	0.4	0.015075376884422	The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology).	1
42560	42560	D19-5719	abstract	3	4	0.6	0.020100502512563	The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology.	0
42561	42561	D19-5719	abstract	4	5	0.8	0.025125628140704	The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization.	0
42562	42562	D19-5719	abstract	5	6	1.0	0.030150753768844	We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.	0
42563	42563	D19-5719	Introduction	1	7	0.0625	0.035175879396985	In this paper, we present the fourth edition 1 of the Bacteria Biotope (BB) task.	0
42564	42564	D19-5719	Introduction	2	8	0.125	0.040201005025126	The task was introduced in 2011.	0
42565	42565	D19-5719	Introduction	3	9	0.1875	0.045226130653266	It has the ambition of promoting large-scale information extraction (IE) from scientific documents in order to automatically fill knowledge bases in the microbial diversity field (Bossy et al., 2012).	0
42566	42566	D19-5719	Introduction	4	10	0.25	0.050251256281407	BB 2019 is part of BioNLP Open Shared Tasks 2019 2 . BioNLP-OST is a community-wide effort for the comparison and evaluation of biomedical text mining technologies on manually curated benchmarks.	0
42567	42567	D19-5719	Introduction	5	11	0.3125	0.055276381909548	A large amount of information about microbes and their properties that is critical for microbiology research and development is scattered among millions of publications and databases .	0
42568	42568	D19-5719	Introduction	6	12	0.375	0.060301507537689	Information extraction as framed by the Bacteria Biotope task identifies relevant entities and interrelationships in the text and map them to reference categories from existing knowledge resources.	0
42569	42569	D19-5719	Introduction	7	13	0.4375	0.065326633165829	This information can thus be combined with information from other sources referring to the same knowledge resources.	0
42570	42570	D19-5719	Introduction	8	14	0.5	0.07035175879397	The knowledge resources used in the BB task are the NCBI taxonomy 3  (Federhen, 2011) for microbial taxa and the OntoBiotope ontology 4 (Nédellec et al., 2018) for microbial habitats and phenotypes.	0
42571	42571	D19-5719	Introduction	9	15	0.5625	0.075376884422111	The large size of these resources relative to the small number of training examples reflects the real conditions of IE application development, whilst it challenges current IE methods.	0
42572	42572	D19-5719	Introduction	10	16	0.625	0.080402010050251	The lexical richness of the two resources partially offsets the difficulty.	0
42573	42573	D19-5719	Introduction	11	17	0.6875	0.085427135678392	Compared to the 2016 corpus that contained only scientific paper abstracts from the PubMed database (Deléger et al., 2016), the 2019 corpus is enriched with extracts from full-text articles.	0
42574	42574	D19-5719	Introduction	12	18	0.75	0.090452261306533	We introduced a new entity type (phenotype) and a new relation type (linking microorganisms and phenotypes).	0
42575	42575	D19-5719	Introduction	13	19	0.8125	0.095477386934673	Phenotypes are observable characteristics such as morphology, or environment requirement (e.g. acidity, oxygen).	0
42576	42576	D19-5719	Introduction	14	20	0.875	0.100502512562814	It is very valuable information for studying the ability of a given microbe to adapt to an environment (Brbić et al., 2016).	0
42577	42577	D19-5719	Introduction	15	21	0.9375	0.105527638190955	The definition of microorganism phenotype in the OntoBiotope ontology includes host interaction characteristics (e.g. symbiont) and community behavior and growth habit (e.g. epilithic).	0
42578	42578	D19-5719	Introduction	16	22	1.0	0.110552763819095	The task organization and the evaluation metrics remain unchanged.	0
42579	42579	D19-5719	Task Description	1	23	0.047619047619048	0.115577889447236	The representation scheme of the Bacteria Biotope task contains four entity types:	0
42580	42580	D19-5719	Task Description	2	24	0.095238095238095	0.120603015075377	• Microorganism: names denoting microorganism taxa.	0
42581	42581	D19-5719	Task Description	3	25	0.142857142857143	0.125628140703518	These taxa correspond to microorganism branches of the NCBI taxon-omy.	0
42582	42582	D19-5719	Task Description	4	26	0.19047619047619	0.130653266331658	The set of relevant taxa is given on the BB task website.	0
42583	42583	D19-5719	Task Description	5	27	0.238095238095238	0.135678391959799	• Habitat: phrases denoting physical places where microorganisms may be observed;	0
42584	42584	D19-5719	Task Description	6	28	0.285714285714286	0.14070351758794	• Geographical: names of geographical places;	0
42585	42585	D19-5719	Task Description	7	29	0.333333333333333	0.14572864321608	• Phenotype: expressions describing microbial characteristics.	0
42586	42586	D19-5719	Task Description	8	30	0.380952380952381	0.150753768844221	The scheme defines two relation types:	0
42587	42587	D19-5719	Task Description	9	31	0.428571428571429	0.155778894472362	•	0
42588	42588	D19-5719	Task Description	10	32	0.476190476190476	0.160804020100503	Lives in relations which link a microorganism entity to its location (either a habitat or a geographical entity, or in few rare cases a microorganism entity);	0
42589	42589	D19-5719	Task Description	11	33	0.523809523809524	0.165829145728643	•	0
42590	42590	D19-5719	Task Description	12	34	0.571428571428571	0.170854271356784	Exhibits relations which link a microorganism entity to a phenotype entity.	0
42591	42591	D19-5719	Task Description	13	35	0.619047619047619	0.175879396984925	Arguments of relations may occur in different sentences.	0
42592	42592	D19-5719	Task Description	14	36	0.666666666666667	0.180904522613065	In addition, microorganisms are normalized to taxa from the NCBI taxonomy.	0
42593	42593	D19-5719	Task Description	15	37	0.714285714285714	0.185929648241206	Habitat and phenotype entities are normalized to concepts from the OntoBiotope ontology.	0
42594	42594	D19-5719	Task Description	16	38	0.761904761904762	0.190954773869347	We used the BioNLP-OST-2019 version of OntoBiotope available on AgroPortal 5 .	0
42595	42595	D19-5719	Task Description	17	39	0.80952380952381	0.195979899497487	We used the NCBI Taxonomy version as available on February 2, 2019 from NCBI website 6 . Copies of both resources can be downloaded from the task website.	0
42596	42596	D19-5719	Task Description	18	40	0.857142857142857	0.201005025125628	The microorganism part of the taxonomy contains 903,191 taxa plus synonyms, while the OntoBiotope ontology includes 3,601 concepts plus synonyms (3,172 for the Habitat branch and 429 for the Phenotype branch of the ontology).	0
42597	42597	D19-5719	Task Description	19	41	0.904761904761905	0.206030150753769	Geographical entities are not normalized.	0
42598	42598	D19-5719	Task Description	20	42	0.952380952380952	0.21105527638191	Figure 1 shows an example of a sentence annotated with normalized entities and relations.	0
42599	42599	D19-5719	Task Description	21	43	1.0	0.21608040201005	As in the 2016 edition, we designed three tasks, each including two modalities, one where entity annotations are provided and one where they are not and have to be predicted.	0
42600	42600	D19-5719	Entity Normalization	1	44	0.333333333333333	0.221105527638191	The first task focused on entity normalization.	0
42601	42601	D19-5719	Entity Normalization	2	45	0.666666666666667	0.226130653266332	In the BB-norm modality of this task, participant systems had to normalize textual entity mentions according to the NCBI taxonomy for microorganisms and to the OntoBiotope ontology for habitats and phenotypes.	0
42602	42602	D19-5719	Entity Normalization	3	46	1.0	0.231155778894472	In the BB-norm+ner modality, systems had to recognize the mentions before normalizing them.	0
42603	42603	D19-5719	Relation Extraction	1	47	0.5	0.236180904522613	The second task focused on the extraction of the two types of relations-Lives in relations among microorganism, habitat and geographical entities, and Exhibits relations between microorganism and phenotype entities.	0
42604	42604	D19-5719	Relation Extraction	2	48	1.0	0.241206030150754	In the BB-rel modality, participant systems only had to extract the relations, while in the BB-rel+ner modality they had to perform entity recognition in addition to relation extraction.	0
42605	42605	D19-5719	Knowledge Base Extraction	1	49	0.166666666666667	0.246231155778894	The goal of the third task is to build a knowledge base using the entities and relations extracted from the corpus.	0
42606	42606	D19-5719	Knowledge Base Extraction	2	50	0.333333333333333	0.251256281407035	It can be viewed as the combination of the previous tasks, followed by a merging step.	0
42607	42607	D19-5719	Knowledge Base Extraction	3	51	0.5	0.256281407035176	Participant systems must normalize entities and extract relations.	0
42608	42608	D19-5719	Knowledge Base Extraction	4	52	0.666666666666667	0.261306532663317	In the BB-kb modality, participant systems had to perform normalization and relation extraction with entity mentions being provided.	0
42609	42609	D19-5719	Knowledge Base Extraction	5	53	0.833333333333333	0.266331658291457	In the BB-kb+ner modality, they had to perform entity recognition as well.	0
42610	42610	D19-5719	Knowledge Base Extraction	6	54	1.0	0.271356783919598	3 Corpus Description	0
42611	42611	D19-5719	Document Selection	1	55	0.111111111111111	0.276381909547739	The BB task corpus consists of two types of documents: PubMed references (titles and abstracts) related to microorganisms, and extracts from fulltext articles related to beneficial microorganisms living in food products.	0
42612	42612	D19-5719	Document Selection	2	56	0.222222222222222	0.281407035175879	The PubMed references are the same as the 215 references of the Bacteria Biotope 2016 corpus.	0
42613	42613	D19-5719	Document Selection	3	57	0.333333333333333	0.28643216080402	They were sampled from all PubMed entries indexed with a term from the Organisms/Bacteria subtree of the MeSH thesaurus.	0
42614	42614	D19-5719	Document Selection	4	58	0.444444444444444	0.291457286432161	The full selection process is described in Deléger et al. (2016).	0
42615	42615	D19-5719	Document Selection	5	59	0.555555555555556	0.296482412060301	Full-text extracts were selected from scientific articles about microorganisms of food interest and annotated by microbiologist experts in the context of the Florilege project (Falentin et al., 2017).	0
42616	42616	D19-5719	Document Selection	6	60	0.666666666666667	0.301507537688442	We reused and complemented this corpus for the BB task.	0
42617	42617	D19-5719	Document Selection	7	61	0.777777777777778	0.306532663316583	Because manual annotation is time-consuming and experts have limited time to dedicate to this task, they did not annotate the full articles.	0
42618	42618	D19-5719	Document Selection	8	62	0.888888888888889	0.311557788944724	Instead, they chose the paragraphs and sentences they found the most informative in the articles.	0
42619	42619	D19-5719	Document Selection	9	63	1.0	0.316582914572864	Thus, this part of the BB corpus is composed of 177 extracts of variable lengths (from one single	0
42620	42620	D19-5719	Annotation	1	64	0.1	0.321608040201005	The PubMed references were already annotated as part of the 2016 edition.	0
42621	42621	D19-5719	Annotation	2	65	0.2	0.326633165829146	We revised these annotations to add phenotype entities with their concept normalization and Exhibits relations.	0
42622	42622	D19-5719	Annotation	3	66	0.3	0.331658291457286	Habitat annotations were also revised to take into account the new and enriched version of the OntoBiotope ontology (compared to the 2016 version 7 ).	0
42623	42623	D19-5719	Annotation	4	67	0.4	0.336683417085427	We also extended the existing annotations of the full-text extracts of the Florilege project by assigning normalized concepts to the entities.	0
42624	42624	D19-5719	Annotation	5	68	0.5	0.341708542713568	Annotation revision was performed by six annotators with backgrounds in biology, computer science and natural language processing.	0
42625	42625	D19-5719	Annotation	6	69	0.6	0.346733668341709	All documents were annotated independently by two annotators and disagreements were resolved through an adjudication phase.	0
42626	42626	D19-5719	Annotation	7	70	0.7	0.351758793969849	Detailed annotation guidelines  were provided to the annotators and were regularly updated following issues raised during the annotation or adjudication phases.	0
42627	42627	D19-5719	Annotation	8	71	0.8	0.35678391959799	The inter-annotator agreement was computed by evaluating one of the two annotations before adjudication against the other.	0
42628	42628	D19-5719	Annotation	9	72	0.9	0.361809045226131	Table 1 summarizes the inter-annotator agreement for named entities, normalization and relations.	0
42629	42629	D19-5719	Annotation	10	73	1.0	0.366834170854271	The metrics used for inter-agreement are the same as for the evaluation of predictions and thus are described below (5.1).	0
42630	42630	D19-5719	Descriptive Statistics	1	74	0.5	0.371859296482412	Table 2 gives the size of the corpus, in terms of documents, words, sentences and annotated ele-	0
42631	42631	D19-5719	Descriptive Statistics	2	75	1.0	0.376884422110553	In the following, we present more detailed statistics and highlight corpus characteristics that may be challenging for the participants.	0
42632	42632	D19-5719	Entities and Concepts	1	76	0.05	0.381909547738693	Table 3 shows the number of mentions, unique (lemmatized) mentions, concepts and average number of mentions per concept for each entity type.	0
42633	42633	D19-5719	Entities and Concepts	2	77	0.1	0.386934673366834	Habitat entities are the most frequent, followed by Microorganism entities.	0
42634	42634	D19-5719	Entities and Concepts	3	78	0.15	0.391959798994975	Geographical entities are very scarce.	0
42635	42635	D19-5719	Entities and Concepts	4	79	0.2	0.396984924623116	There is much more variation in the expression of habitats and phenotypes than in that of microorganisms.	0
42636	42636	D19-5719	Entities and Concepts	5	80	0.25	0.402010050251256	There is an average of respectively 4 and 3.5 unique mentions per habitat and phenotype concept while microorganisms only have 1.9.	0
42637	42637	D19-5719	Entities and Concepts	6	81	0.3	0.407035175879397	Their proportion of unique entities out of all mentions is also higher (respectively 50.6% and 45.2% vs. 38.2% for microorganisms).	0
42638	42638	D19-5719	Entities and Concepts	7	82	0.35	0.412060301507538	The proportion of direct mappings (i.e., exact string matches, taking into account lemmatization) between entity mentions and labels of concepts (from the NCBI taxonomy or the Onto-Biotope ontology) is displayed on Figure 2.	0
42639	42639	D19-5719	Entities and Concepts	8	83	0.4	0.417085427135678	It emphasizes once more the variability of Habitat and Phenotype entity expressions, with respectively 72.5% and 91.2% mentions that do not exactly match a concept label or synonym.	0
42640	42640	D19-5719	Entities and Concepts	9	84	0.45	0.422110552763819	Among exact matches, a small proportion of mentions are not actually normalized with the concept whose label they match.	0
42641	42641	D19-5719	Entities and Concepts	10	85	0.5	0.42713567839196	"These are ""contextual normalization"" cases, i.e. entities are normalized with a more specific concept which can be inferred from the context."	0
42642	42642	D19-5719	Entities and Concepts	11	86	0.55	0.4321608040201	These often correspond to lexical coreference cases.	0
42643	42643	D19-5719	Entities and Concepts	12	87	0.6	0.437185929648241	A distinctive feature of the BB task is that multiple concepts may be assigned to a given entity mention.	0
42644	42644	D19-5719	Entities and Concepts	13	88	0.65	0.442211055276382	Multiple normalization happens when two (or more) concepts can describe an entity and are all deemed necessary because each concept corresponds to a different aspect of the entity.	0
42645	42645	D19-5719	Entities and Concepts	14	89	0.7	0.447236180904523	"An example of such a case is the Habitat entity ""diseased cow"" which is normalized by both the &lt;cow&gt; and &lt;animal with disease&gt; concepts."	0
42646	42646	D19-5719	Entities and Concepts	15	90	0.75	0.452261306532663	This is the case mainly for Habitat entities (8.7%), and rarely happens for Phenotype entities (0.6%) and Microorganism entities (only one occurrence).	0
42647	42647	D19-5719	Entities and Concepts	16	91	0.8	0.457286432160804	Another characteristic of the corpus is the presence of nested entities (entities embedded in another larger entity) and discontinuous entities (entities split in several fragments).	0
42648	42648	D19-5719	Entities and Concepts	17	92	0.85	0.462311557788945	Both phenomena can be challenging for machine-learning methods and are often ignored.	0
42649	42649	D19-5719	Entities and Concepts	18	93	0.9	0.467336683417085	The proportion of discontinuous entities in the corpus is limited, with a total of 3.7%.	0
42650	42650	D19-5719	Entities and Concepts	19	94	0.95	0.472361809045226	Nested entities are more frequent (17.8% in total), especially for habitats.	0
42651	42651	D19-5719	Entities and Concepts	20	95	1.0	0.477386934673367	"For instance, the Habitat entity ""cheese making factory"" also contains the smaller Habitat entity ""cheese""."	0
42652	42652	D19-5719	Relations	1	96	0.125	0.482412060301508	Table 4 shows the number of relations for both Lives in and Exhibits types, including intrasentence and inter-sentence relations.	0
42653	42653	D19-5719	Relations	2	97	0.25	0.487437185929648	Intrasentence relations involve entities occurring in the same sentence while inter-sentence relations involve entities occurring in different sentences, not necessarily contiguous.	0
42654	42654	D19-5719	Relations	3	98	0.375	0.492462311557789	Inter-sentence relations are known to be challenging for automatic methods.	0
42655	42655	D19-5719	Relations	4	99	0.5	0.49748743718593	Their proportion in the corpus is not negligible (17.5% in total).	0
42656	42656	D19-5719	Relations	5	100	0.625	0.50251256281407	An example can be seen in the following extract: Vibrios [. . . ] are ubiquitous to oceans, coastal waters, and estuaries.	0
42657	42657	D19-5719	Relations	6	101	0.75	0.507537688442211	[. . . ]	0
42658	42658	D19-5719	Relations	7	102	0.875	0.512562814070352	The bacterial pathogen is a growing concern in North America.	0
42659	42659	D19-5719	Relations	8	103	1.0	0.517587939698492	There is an inter-sentence relation between the two underlined entities.	0
42660	42660	D19-5719	Training, Development and Test Sets	1	104	0.125	0.522613065326633	The BB corpus is split into training, development and test sets.	0
42661	42661	D19-5719	Training, Development and Test Sets	2	105	0.25	0.527638190954774	"In practice, there are two test sets, one for the modalities involving entity recognition (the ""+ner"" sub-tasks) and one for the modalities where entity annotations are given."	0
42662	42662	D19-5719	Training, Development and Test Sets	3	106	0.375	0.532663316582915	We kept the corpus division of the 2016 edition for the PubMed references.	0
42663	42663	D19-5719	Training, Development and Test Sets	4	107	0.5	0.537688442211055	This was possible because the gold annotations of the test set were never released to the public.	0
42664	42664	D19-5719	Training, Development and Test Sets	5	108	0.625	0.542713567839196	Then we split the Florilege full-text extracts using the same proportions as for   The proportion of concepts seen in the training set out of all concepts present in the knowledge resources is low for all entity types, which means that there is a large number of unseen examples (0.02% for microorganisms, 7.3% for habitats, and 15.6% for phenotypes).	0
42665	42665	D19-5719	Training, Development and Test Sets	6	109	0.75	0.547738693467337	It emphasizes the need for methods that handle few-shot and zeroshot learning.	0
42666	42666	D19-5719	Training, Development and Test Sets	7	110	0.875	0.552763819095477	Microorganisms have the lowest proportion, due to the large size of the microorganism taxonomies.	0
42667	42667	D19-5719	Training, Development and Test Sets	8	111	1.0	0.557788944723618	However, the names of the	0
42668	42668	D19-5719	Supporting Resources	1	112	0.2	0.562814070351759	Supporting resources were made available to participants.	0
42669	42669	D19-5719	Supporting Resources	2	113	0.4	0.567839195979899	They consist of outputs from state-ofthe-art tools applied to the BB data sets (e.g., POS tagging, syntactic parsing, NER, word embeddings).	0
42670	42670	D19-5719	Supporting Resources	3	114	0.6	0.57286432160804	We proposed in-house embeddings trained on selected relevant PubMed abstracts, and links to external embeddings (Pyysalo et al., 2013;	0
42671	42671	D19-5719	Supporting Resources	4	115	0.8	0.577889447236181	Li et al., 2017) trained on PubMed and Wikipedia.	0
42672	42672	D19-5719	Supporting Resources	5	116	1.0	0.582914572864322	The full list of tools and resources is available on the website.	0
42673	42673	D19-5719	Evaluation	1	117	0.076923076923077	0.587939698492462	Metrics	0
42674	42674	D19-5719	Evaluation	2	118	0.153846153846154	0.592964824120603	We used the same evaluation metrics as in the 2016 edition.	0
42675	42675	D19-5719	Evaluation	3	119	0.230769230769231	0.597989949748744	The underlying rationale and formula of each score is detailed in Deléger et al. (2016);	0
42676	42676	D19-5719	Evaluation	4	120	0.307692307692308	0.603015075376884	Bossy et al. (2013).	0
42677	42677	D19-5719	Evaluation	5	121	0.384615384615385	0.608040201005025	Additionally we compute a variety of alternate scorings in order to distinguish the strengths of each submission.	0
42678	42678	D19-5719	Evaluation	6	122	0.461538461538462	0.613065326633166	The evaluation tool was provided to participants 9 .	0
42679	42679	D19-5719	Evaluation	7	123	0.538461538461538	0.618090452261307	Normalization accuracy is measured through a semantic similarity metric, and micro-averaging across entities.	0
42680	42680	D19-5719	Evaluation	8	124	0.615384615384615	0.623115577889447	Relation extraction is measured with Recall, Precision, and F 1 .	0
42681	42681	D19-5719	Evaluation	9	125	0.692307692307692	0.628140703517588	However for tasks where systems must recognize entities, we used the Slot Error Rate (SER) instead of F 1 in order to avoid sanctioning twice the inaccuracy of boundaries.	0
42682	42682	D19-5719	Evaluation	10	126	0.769230769230769	0.633165829145729	The SER measures the amount of errors according to three types: insertions (false positives), deletions (false negatives), and substitutions (partial matches).	0
42683	42683	D19-5719	Evaluation	11	127	0.846153846153846	0.638190954773869	The SER is normalized by the number of reference items.	0
42684	42684	D19-5719	Evaluation	12	128	0.923076923076923	0.64321608040201	The higher the value the worse is the prediction, and there is no upper bound since insertions can exceed the number of items in the reference.	0
42685	42685	D19-5719	Evaluation	13	129	1.0	0.648241206030151	Confidence intervals were computed for each metric with the bootstrap resampling method (90%, n=100).	0
42686	42686	D19-5719	Baseline	1	130	0.083333333333333	0.653266331658292	We designed simple baselines for each sub-task in order to provide a comparison reference.	0
42687	42687	D19-5719	Baseline	2	131	0.166666666666667	0.658291457286432	We preprocessed the corpus with the AlvisNLP 10 engine, that performs tokenization, sentence splitting, and lemmatization using the GENIA tagger (Tsuruoka et al., 2005).	0
42688	42688	D19-5719	Baseline	3	132	0.25	0.663316582914573	• BB-norm: we performed exact matching between lemmatized entities and the knowledge resources.	0
42689	42689	D19-5719	Baseline	4	133	0.333333333333333	0.668341708542714	When no match was found, we normalized habitats and phenotypes with the top-level concept of the Habitat and Phenotype ontology branches, and microorganisms with the high-level &lt;Bacteria&gt; taxon.	0
42690	42690	D19-5719	Baseline	5	134	0.416666666666667	0.673366834170854	• BB-norm+ner: we used our exact matching approach on the lemmatized text of the documents instead of on given entity mentions.	0
42691	42691	D19-5719	Baseline	6	135	0.5	0.678391959798995	• BB-rel: we used a simple co-occurrence approach, linking pairs of entities occurring in the same sentences.	0
42692	42692	D19-5719	Baseline	7	136	0.583333333333333	0.683417085427136	• BB-rel+ner: we first detected entities using our exact matching strategy for microorganisms, habitats and phenotypes.	0
42693	42693	D19-5719	Baseline	8	137	0.666666666666667	0.688442211055276	For geographical entities, we used the Stanford Named Entity Recognition tool (Finkel et al., 2005).	0
42694	42694	D19-5719	Baseline	9	138	0.75	0.693467336683417	Then we linked entities occurring in the same sentences, as for the BB-rel task.	0
42695	42695	D19-5719	Baseline	10	139	0.833333333333333	0.698492462311558	• BB-kb: we combined the BB-norm and BBrel approaches.	0
42696	42696	D19-5719	Baseline	11	140	0.916666666666667	0.703517587939699	• BB-kb+ner: we combined our BB-norm+ner method with our co-occurrence approach.	0
42697	42697	D19-5719	Baseline	12	141	1.0	0.708542713567839	6 Outcome	0
42698	42698	D19-5719	Participation	1	142	0.333333333333333	0.71356783919598	The blind test data was released on the 22 nd of July 2019 and participants were given until the 31 st of July to submit their predictions.	0
42699	42699	D19-5719	Participation	2	143	0.666666666666667	0.718592964824121	Each team was allowed two submissions to each sub-task.	0
42700	42700	D19-5719	Participation	3	144	1.0	0.723618090452261	Ten teams participated to all six sub-tasks and submitted a total of 31 runs.	0
42701	42701	D19-5719	Participants' Methods and Resources	1	145	0.071428571428572	0.728643216080402	As in 2016, most methods are based on Machine Learning algorithms.	0
42702	42702	D19-5719	Participants' Methods and Resources	2	146	0.142857142857143	0.733668341708543	For named entity recognition, the CRF algorithm is still the most used (BLAIR GMU), though sometimes combined with a neural network (MIC-CIS).	0
42703	42703	D19-5719	Participants' Methods and Resources	3	147	0.214285714285714	0.738693467336683	In 2016, the majority of participants used SVMs for relation extraction.	0
42704	42704	D19-5719	Participants' Methods and Resources	4	148	0.285714285714286	0.743718592964824	In this edition nearly all participants used neural networks in a diversity of architectures: multi-layer perceptron (Yuhang Wu), bi-LSTM (whunlp), AGCNN (whunlp).	0
42705	42705	D19-5719	Participants' Methods and Resources	5	149	0.357142857142857	0.748743718592965	One participant predicted relations through filtered co-occurrences (BOUN-ISIK), and another by bagging SVM and Logistic Regression (BLAIR GMU).	0
42706	42706	D19-5719	Participants' Methods and Resources	6	150	0.428571428571429	0.753768844221106	Note that AliAI employed a multi-task architecture similar to BERT (Devlin et al., 2019) to perform both named-entity recognition and relation extraction.	0
42707	42707	D19-5719	Participants' Methods and Resources	7	151	0.5	0.758793969849246	The normalization task was addressed in a more diverse manner.	0
42708	42708	D19-5719	Participants' Methods and Resources	8	152	0.571428571428571	0.763819095477387	On one hand several distinct ML algorithms were used to discriminate entity categories: ensemble CNNs (PADIA BacReader), kNN with reranking (BOUN-ISIK), or Linear Regression (BLAIR GMU).	0
42709	42709	D19-5719	Participants' Methods and Resources	9	153	0.642857142857143	0.768844221105528	On the other hand MIC-CIS employed an exact and an approximate matching algorithm.	0
42710	42710	D19-5719	Participants' Methods and Resources	10	154	0.714285714285714	0.773869346733668	Word embeddings trained with Word2Vec (Mikolov et al., 2013) on a domain-specific corpus (PubMed abstract, PMC articles) seem to be an universal resource since all but one submissions for any task used them.	0
42711	42711	D19-5719	Participants' Methods and Resources	11	155	0.785714285714286	0.778894472361809	BLAIR GMU used contextual embeddings based on BERT and XLNet (Yang et al., 2019).	0
42712	42712	D19-5719	Participants' Methods and Resources	12	156	0.857142857142857	0.78391959798995	Dependency parsing was used in every relation extraction submission, and also for normalization (BOUN-ISIK).	0
42713	42713	D19-5719	Participants' Methods and Resources	13	157	0.928571428571429	0.78894472361809	The most popular NLP tool libraries are Stanford CoreNLP (Manning et al., 2014) and NLTK (Bird et al., 2009).	0
42714	42714	D19-5719	Participants' Methods and Resources	14	158	1.0	0.793969849246231	We also note that the Word-Piece segmentation is used even in systems that do not use BERT.	0
42715	42715	D19-5719	Results	1	159	0.2	0.798994974874372	In this section we report the results for all subtasks, and highlight notable results as well as a comparison with results obtained in 2016 in the third edition of the Bacteria Biotope task in BioNLP-ST 2016.	0
42716	42716	D19-5719	Results	2	160	0.4	0.804020100502513	The task site presents detailed results, including main and alternate metrics, as well as confidence intervals.	0
42717	42717	D19-5719	Results	3	161	0.6	0.809045226130653	However comparison with 2016 is limited by the evolution of the task.	0
42718	42718	D19-5719	Results	4	162	0.8	0.814070351758794	On one hand the data set has increased approximately by 50%, and the annotations were revised and their quality improved.	0
42719	42719	D19-5719	Results	5	163	1.0	0.819095477386935	On the other hand the tasks were made harder because the schema was enriched with an entity type and a relation type, and the target taxa have been extended from Bacteria only to all microorganisms.	0
42720	42720	D19-5719	BB-norm and BB-norm+ner	1	164	0.142857142857143	0.824120603015075	The main results as well as the results for each entity type are shown in Tables 6 and 7. BOUN-ISIK and BLAIR GMU obtained the best overall results for BB-norm, and MIC-CIS for BB-norm+ner.	0
42721	42721	D19-5719	BB-norm and BB-norm+ner	2	165	0.285714285714286	0.829145728643216	The results for each entity type highlight different profiles.	0
42722	42722	D19-5719	BB-norm and BB-norm+ner	3	166	0.428571428571429	0.834170854271357	While BOUN-ISIK predicts accurate normalizations for habitat entities for BB-norm, BLAIR GMU predicts better normalizations for microorganism entities.	0
42723	42723	D19-5719	BB-norm and BB-norm+ner	4	167	0.571428571428571	0.839195979899497	PADIA BacReader's predictions for habitats is on par with BOUN-ISIK, and their normalization of phenotype entities is outstanding.	0
42724	42724	D19-5719	BB-norm and BB-norm+ner	5	168	0.714285714285714	0.844221105527638	As for BB-norm+ner, MIC-CIS consistently predicts the best entity boundaries and normalizations for all types.	0
42725	42725	D19-5719	BB-norm and BB-norm+ner	6	169	0.857142857142857	0.849246231155779	In comparison to 2016, the state of the art for multi-word entity recognition and normalization, like habitats and phenotypes, has improved.	0
42726	42726	D19-5719	BB-norm and BB-norm+ner	7	170	1.0	0.85427135678392	We note that with the introduction of new taxa the recognition and normalization of taxa may have been rendered more difficult than anticipated since the results are lower than obtained in 2016.	0
42727	42727	D19-5719	BB-rel and BB-rel+ner	1	171	0.111111111111111	0.85929648241206	The results of BB-rel and BB-rel+ner are given in Tables 8 and 9 respectively.	0
42728	42728	D19-5719	BB-rel and BB-rel+ner	2	172	0.222222222222222	0.864321608040201	The table includes the scores obtained for each relation type, as well as the best results obtained in 2016.	0
42729	42729	D19-5719	BB-rel and BB-rel+ner	3	173	0.333333333333333	0.869346733668342	The highest F-score for BB-rel was obtained by the whunlp submission, with AliAI as a very close contender.	0
42730	42730	D19-5719	BB-rel and BB-rel+ner	4	174	0.444444444444444	0.874371859296482	UTU, and very closely behind AliAI, obtained the highest Precision, whereas BOUN-ISIK the highest Recall.	0
42731	42731	D19-5719	BB-rel and BB-rel+ner	5	175	0.555555555555556	0.879396984924623	The Recall of the baseline prediction indicates the highest recall possible for relations contained in a single sentence.	0
42732	42732	D19-5719	BB-rel and BB-rel+ner	6	176	0.666666666666667	0.884422110552764	No participating system addresses cross-sentence relations, which appears to be the most productive lead to increase performance.	0
42733	42733	D19-5719	BB-rel and BB-rel+ner	7	177	0.777777777777778	0.889447236180904	Most submissions outperform the best predictions of 2016 in at least one score, and five of the eleven submissions obtain a significantly higher Fscore.	0
42734	42734	D19-5719	BB-rel and BB-rel+ner	8	178	0.888888888888889	0.894472361809045	For BB-rel+ner, AliAI obtains the highest recall and precision, consistently for Lives In and Exhibits relations.	0
42735	42735	D19-5719	BB-rel and BB-rel+ner	9	179	1.0	0.899497487437186	This submission also outperforms significantly the state of the art set in 2016.	0
42736	42736	D19-5719	BB-kb and BB-kb+ner	1	180	0.166666666666667	0.904522613065327	BLAIR GMU is the only team to submit to the BB-kb and BB-kb+ner tasks, their results are shown in Table 10.	0
42737	42737	D19-5719	BB-kb and BB-kb+ner	2	181	0.333333333333333	0.909547738693467	The knowledge-base task and evaluation necessarily require end-to-end prediction systems that must perform named-entity recognition, entity normalization, relation extraction, as well as contributory tasks like POStagging, or coreference resolution.	0
42738	42738	D19-5719	BB-kb and BB-kb+ner	3	182	0.5	0.914572864321608	The limited scores obtained might be explained by the accumulation of errors by successive prediction steps.	0
42739	42739	D19-5719	BB-kb and BB-kb+ner	4	183	0.666666666666667	0.919597989949749	Since the data of all sub-tasks comes from the       10: Results for the BB-kb and BB-kb+ner subtasks.	0
42740	42740	D19-5719	BB-kb and BB-kb+ner	5	184	0.833333333333333	0.924623115577889	The metric is the average of the semantic similarity between the reference and the predicted normalizations for all relation arguments after removing duplicates at the corpus level.	0
42741	42741	D19-5719	BB-kb and BB-kb+ner	6	185	1.0	0.92964824120603	Best scores are in bold font, several scores are in bold if their difference is not significant.	0
42742	42742	D19-5719	Conclusion	1	186	0.071428571428572	0.934673366834171	The Bacteria Biotope	0
42743	42743	D19-5719	Conclusion	2	187	0.142857142857143	0.939698492462311	Task arouses sustained interest with a total of 10 teams participating in the fourth edition.	0
42744	42744	D19-5719	Conclusion	3	188	0.214285714285714	0.944723618090452	As usual, the relation extraction sub-tasks (BB-rel and BB-rel+ner) were the most popular, demonstrating that this task is still a scientific and technical challenge.	0
42745	42745	D19-5719	Conclusion	4	189	0.285714285714286	0.949748743718593	The most notable evolution of participating systems since the last edition is the pervasiveness of methods based on neural networks and word embeddings.	0
42746	42746	D19-5719	Conclusion	5	190	0.357142857142857	0.954773869346734	These systems yielded superior predictions compared to those in 2016.	0
42747	42747	D19-5719	Conclusion	6	191	0.428571428571429	0.959798994974874	As mentioned previously, there is still much room for improvement in addressing cross-sentence relation extraction.	0
42748	42748	D19-5719	Conclusion	7	192	0.5	0.964824120603015	We also note a growing interest in the normalization sub-tasks (BB-norm and BB-norm+ner).	0
42749	42749	D19-5719	Conclusion	8	193	0.571428571428571	0.969849246231156	The predictions improved for habitat entities, and are very promising for phenotype entities.	0
42750	42750	D19-5719	Conclusion	9	194	0.642857142857143	0.974874371859296	However the generalization from bacteria-only taxa in 2016 to all microorganisms in this edition proved to pose an unexpected challenge.	0
42751	42751	D19-5719	Conclusion	10	195	0.714285714285714	0.979899497487437	Knowledge base population (BB-kb and BB-kb+ner) is the most challenging task, since it requires a wider set of capabilities.	0
42752	42752	D19-5719	Conclusion	11	196	0.785714285714286	0.984924623115578	Nevertheless we demonstrated that the combination of other subtask predictions allows to produce better quality knowledge bases.	0
42753	42753	D19-5719	Conclusion	12	197	0.857142857142857	0.989949748743718	To help participants, supporting resources were provided.	0
42754	42754	D19-5719	Conclusion	13	198	0.928571428571429	0.994974874371859	The most used resources were pretrained word embeddings, and general-domain named entities.	0
42755	42755	D19-5719	Conclusion	14	199	1.0	1.0	The evaluation on the test set will be maintained online 11 in order for future experiments to compare with the current state of the art.	0
42958	42958	D19-6007	title	1	1	1.0	0.005555555555556	Commonsense Inference in Natural Language Processing (COIN) -Shared Task Report	0
42959	42959	D19-6007	abstract	1	2	0.25	0.011111111111111	This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019.	0
42960	42960	D19-6007	abstract	2	3	0.5	0.016666666666667	The tasks consisted of two machine comprehension evaluations, each of which tested a system's ability to answer questions/queries about a text.	0
42961	42961	D19-6007	abstract	3	4	0.75	0.022222222222222	Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text.	0
42962	42962	D19-6007	abstract	4	5	1.0	0.027777777777778	A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
42963	42963	D19-6007	Introduction	1	6	0.047619047619048	0.033333333333333	Due to the rise of powerful pre-trained word and sentence representations, automated text processing has come a long way in recent years, with systems that perform even better than humans on some datasets (Rajpurkar et al., 2016a).	0
42964	42964	D19-6007	Introduction	2	7	0.095238095238095	0.038888888888889	However, natural language understanding also involves complex challenges.	0
42965	42965	D19-6007	Introduction	3	8	0.142857142857143	0.044444444444445	One important difference between human and machine text understanding lies in the fact that humans can access commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that are assumed to be common ground.	0
42966	42966	D19-6007	Introduction	4	9	0.19047619047619	0.05	"(1) Max: ""It's 1 pm already, I think we should get lunch."""	0
42967	42967	D19-6007	Introduction	5	10	0.238095238095238	0.055555555555556	"Dustin: ""Let me get my wallet."""	0
42968	42968	D19-6007	Introduction	6	11	0.285714285714286	0.061111111111111	Consider the conversation in Example 1: Max will not be surprised that Dustin needs to get his wallet, since she knows that paying is a part of getting lunch.	0
42969	42969	D19-6007	Introduction	7	12	0.333333333333333	0.066666666666667	Also, she knows that a wallet is needed for paying, so Dustin needs to get a wallet for lunch.	0
42970	42970	D19-6007	Introduction	8	13	0.380952380952381	0.072222222222222	This is part of the commonsense knowledge about getting lunch and should be known by both persons.	0
42971	42971	D19-6007	Introduction	9	14	0.428571428571429	0.077777777777778	For a computer system, inferring such unmentioned facts is a non-trivial challenge.	0
42972	42972	D19-6007	Introduction	10	15	0.476190476190476	0.083333333333333	The workshop on Commonsense Inference in NLP (COIN) is focused on such phenomena, looking at models, data, and evaluation methods for commonsense inference.	0
42973	42973	D19-6007	Introduction	11	16	0.523809523809524	0.088888888888889	This report summarizes the results of the COIN shared tasks, an unofficial extension of the Sem-Eval 2018 shared task 11, Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).	0
42974	42974	D19-6007	Introduction	12	17	0.571428571428571	0.094444444444445	The tasks aim to evaluate the commonsense inference capabilities of text understanding systems in two settings: Commonsense inference in everyday narrations (task 1) and commonsense inference in news texts (task 2).	1
42975	42975	D19-6007	Introduction	13	18	0.619047619047619	0.1	Framed as machine comprehension evaluations, the datasets used for both tasks contain challenging reading comprehension questions asking for facts that are not explicitly mentioned in the given reading texts.	0
42976	42976	D19-6007	Introduction	14	19	0.666666666666667	0.105555555555556	Several teams participated in the shared tasks and submitted system description papers.	0
42977	42977	D19-6007	Introduction	15	20	0.714285714285714	0.111111111111111	All systems are based on Transformer architectures (Vaswani et al., 2017), some of them explicitly incorporating commonsense knowledge resources, whereas others only use pretraining on other machine comprehension data sets.	0
42978	42978	D19-6007	Introduction	16	21	0.761904761904762	0.116666666666667	The best submitted system achieves 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
42979	42979	D19-6007	Introduction	17	22	0.80952380952381	0.122222222222222	Still, there are cases that remain elusive: Humans outperform this system by a margin of 7% (task 1) and 8% (task 2).	0
42980	42980	D19-6007	Introduction	18	23	0.857142857142857	0.127777777777778	Our results indicate that while Transformer models are able to perform extremely well on the data used in our shared task, there are still some remaining cases demonstrating that human level is not achieved yet.	0
42981	42981	D19-6007	Introduction	19	24	0.904761904761905	0.133333333333333	Still, we believe that our results also imply the need for more challenging data sets.	0
42982	42982	D19-6007	Introduction	20	25	0.952380952380952	0.138888888888889	In particular, we need data sets that make it harder to benefit from redundancy in the training data or large-scale pretraining on similar domains.	0
42983	42983	D19-6007	Introduction	21	26	1.0	0.144444444444444	In the following, we briefly describe the data sets ( §2), baselines and evaluation metrics of the shared tasks ( §3) and we present a summary of the participating systems ( §4), their results ( §5) as well as a discussion thereof ( §6).	0
42984	42984	D19-6007	Data and Tasks	1	27	0.058823529411765	0.15	Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC).	0
42985	42985	D19-6007	Data and Tasks	2	28	0.117647058823529	0.155555555555556	The central idea is that a system has to process a text and then find a correct answer to a question that is asked on the text.	0
42986	42986	D19-6007	Data and Tasks	3	29	0.176470588235294	0.161111111111111	Our shared tasks follow this paradigm and use machine comprehension settings to evaluate a model's capability to perform commonsense inferences.	0
42987	42987	D19-6007	Data and Tasks	4	30	0.235294117647059	0.166666666666667	In contrast to most existing MC datasets, the two datasets that are used for our shared tasks, MCScript2.0 (Ostermann et al., 2019) and ReCoRD (Zhang et al., 2018), are focused on questions that cannot be answered from the text alone, but that require a model to draw inference over unmentioned facts.	0
42988	42988	D19-6007	Data and Tasks	5	31	0.294117647058823	0.172222222222222	(2) Text: Camping is one of my favorite summer vacations. (...)	0
42989	42989	D19-6007	Data and Tasks	6	32	0.352941176470588	0.177777777777778	Once I have all my gear and clothing I'll pack it into my car, making sure to leave room for myself, my dog and anything my friends want to bring.	0
42990	42990	D19-6007	Data and Tasks	7	33	0.411764705882353	0.183333333333333	And then we are ready for our camping vacation.	0
42991	42991	D19-6007	Data and Tasks	8	34	0.470588235294118	0.188888888888889	Question:	0
42992	42992	D19-6007	Data and Tasks	9	35	0.529411764705882	0.194444444444444	What do they put the drinks in?	0
42993	42993	D19-6007	Data and Tasks	10	36	0.588235294117647	0.2	a. Cooler b. Sleeping bag Example 2 illustrates the main idea of the shared tasks.	0
42994	42994	D19-6007	Data and Tasks	11	37	0.647058823529412	0.205555555555556	It shows a reading text from MC-Script2.0, together with a question and two candidate answers.	0
42995	42995	D19-6007	Data and Tasks	12	38	0.705882352941176	0.211111111111111	For a human, it is trivial to find that the drinks are put into a cooler rather than the sleeping bag.	0
42996	42996	D19-6007	Data and Tasks	13	39	0.764705882352941	0.216666666666667	This information is however not mentioned in the text, so a machine needs to have the capability to infer this fact from commonsense knowledge.	0
42997	42997	D19-6007	Data and Tasks	14	40	0.823529411764706	0.222222222222222	The reading texts of MCScript2.0 are narrations about everyday activities (task 1).	0
42998	42998	D19-6007	Data and Tasks	15	41	0.882352941176471	0.227777777777778	Due to its domain, MCScript2.0 has a focus on evaluating script knowledge, i.e. knowledge about the events and participants of such everyday activities (Schank and Abelson, 1975).	0
42999	42999	D19-6007	Data and Tasks	16	42	0.941176470588235	0.233333333333333	Task 2 utilizes the ReCoRD corpus (Zhang et al., 2018), which contains news texts, a more open domain.	0
43000	43000	D19-6007	Data and Tasks	17	43	1.0	0.238888888888889	The inferences that are required for finding answers to the questions in ReCoRD are thus of a more general type.	0
43001	43001	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	1	44	0.076923076923077	0.244444444444444	MCScript2.0 is a reading comprehension data set comprising 19,821 questions on 3,487 texts.	0
43002	43002	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	2	45	0.153846153846154	0.25	Each of the questions has two answer candidates, one of which is correct.	0
43003	43003	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	3	46	0.230769230769231	0.255555555555556	Questions in the data were annotated for reasoning types, i.e. according to whether the answer to a question can be found in the text or needs to be inferred from commonsense knowledge.	0
43004	43004	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	4	47	0.307692307692308	0.261111111111111	Roughly half of the questions do require inferences over commonsense knowledge.	0
43005	43005	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	5	48	0.384615384615385	0.266666666666667	The texts in MCScript2.0 are short narrations (164.4 tokens on average) on a total of 200 different everyday activities.	0
43006	43006	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	6	49	0.461538461538462	0.272222222222222	All texts were crowdsourced on Amazon Mechanical Turk 1 , by asking crowd workers to tell a story about one of the 200 scenarios as if talking to a child (Modi et al., 2016;	0
43007	43007	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	7	50	0.538461538461538	0.277777777777778	Ostermann et al., 2018a), resulting in simple texts which explicitly mention many details of a scenario.	0
43008	43008	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	8	51	0.615384615384615	0.283333333333333	In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts.	0
43009	43009	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	9	52	0.692307692307692	0.288888888888889	After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts.	0
43010	43010	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	10	53	0.769230769230769	0.294444444444444	During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge.	0
43011	43011	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	11	54	0.846153846153846	0.3	Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018).	0
43012	43012	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	12	55	0.923076923076923	0.305555555555556	For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set.	0
43013	43013	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	13	56	1.0	0.311111111111111	All texts for five scenarios were reserved for the test set only to increase difficulty.	0
43014	43014	D19-6007	Task 2: Commonsense Inference in News Articles	1	57	0.2	0.316666666666667	ReCoRD is a large-scale dataset for reading comprehension, which consists of over 120,000 ex-  amples, most of which require commonsense reasoning.	0
43015	43015	D19-6007	Task 2: Commonsense Inference in News Articles	2	58	0.4	0.322222222222222	ReCoRD was collected in a fourstage process (Figure 1): (1) curating CNN/Daily	0
43016	43016	D19-6007	Task 2: Commonsense Inference in News Articles	3	59	0.6	0.327777777777778	Mail news articles, (2) generating passage-queryanswers triples based on the news articles, (3) filtering out the queries that can be easily answered by state-of-the-art machine comprehension (MC) models, and (4) filtering out the queries ambiguous to human readers.	0
43017	43017	D19-6007	Task 2: Commonsense Inference in News Articles	4	60	0.8	0.333333333333333	All named entities in the passages are possible answers to the queries.	0
43018	43018	D19-6007	Task 2: Commonsense Inference in News Articles	5	61	1.0	0.338888888888889	Table 1 summarizes the data statistics.	0
43019	43019	D19-6007	Shared Task Setup	1	62	1.0	0.344444444444444	The baselines for our shared tasks were adapted from Ostermann et al. (2019) and Zhang et al. (2018), respectively.	0
43020	43020	D19-6007	Task 1 Baselines	1	63	0.071428571428572	0.35	Following Ostermann et al. (2019), we present results of three baseline models.	0
43021	43021	D19-6007	Task 1 Baselines	2	64	0.142857142857143	0.355555555555556	Logistic Regression Model.	0
43022	43022	D19-6007	Task 1 Baselines	3	65	0.214285714285714	0.361111111111111	Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MC-Script, a predecessor of the dataset used for this task.	0
43023	43023	D19-6007	Task 1 Baselines	4	66	0.285714285714286	0.366666666666667	Their model outperformed many neural networks in spite of its simplicity.	0
43024	43024	D19-6007	Task 1 Baselines	5	67	0.357142857142857	0.372222222222222	Attentive Reader.	0
43025	43025	D19-6007	Task 1 Baselines	6	68	0.428571428571429	0.377777777777778	The second baseline model is an attentive reader network (Hermann et al., 2015).	0
43026	43026	D19-6007	Task 1 Baselines	7	69	0.5	0.383333333333333	GRU units (Cho et al., 2014) are used to process text, question and answer.	0
43027	43027	D19-6007	Task 1 Baselines	8	70	0.571428571428571	0.388888888888889	A questionaware text representation is computed based on a bilinear attention function, which is then combined with a GRU-based answer representation for prediction.	0
43028	43028	D19-6007	Task 1 Baselines	9	71	0.642857142857143	0.394444444444444	For details, we refer to Ostermann et al. (2019), Ostermann et al. (2018a) and Chen et al. (2016) TriAN.	0
43029	43029	D19-6007	Task 1 Baselines	10	72	0.714285714285714	0.4	As last model, we use the three-way attentive network (TriAN) (Wang et al., 2018), a recurrent neural network that scored the first place in the SemEval 2018 task.	0
43030	43030	D19-6007	Task 1 Baselines	11	73	0.785714285714286	0.405555555555556	They use LSTM units (Hochreiter and Schmidhuber, 1997), several attention functions, and self attention to compute representations for text, question and answer.	0
43031	43031	D19-6007	Task 1 Baselines	12	74	0.857142857142857	0.411111111111111	Concept	0
43032	43032	D19-6007	Task 1 Baselines	13	75	0.928571428571429	0.416666666666667	Net (Speer et al., 2017), a large commonsense knowledge base containing thousands of entities and commonsense relations between them, is used to enhance text representations with commonsense information, by computing relation embeddings and appending them to the text representations.	0
43033	43033	D19-6007	Task 1 Baselines	14	76	1.0	0.422222222222222	For more information we refer to Wang et al. (2018).	0
43034	43034	D19-6007	Task 2 Baselines	1	77	0.090909090909091	0.427777777777778	We present five baselines for ReCoRD:	0
43035	43035	D19-6007	Task 2 Baselines	2	78	0.181818181818182	0.433333333333333	BERT	0
43036	43036	D19-6007	Task 2 Baselines	3	79	0.272727272727273	0.438888888888889	(Devlin et al., 2019) is a new language representation model.	0
43037	43037	D19-6007	Task 2 Baselines	4	80	0.363636363636364	0.444444444444444	Recently fine-tuning the pre-trained BERT with an additional output layer has created state-of-the-art models on a wide range of NLP tasks.	0
43038	43038	D19-6007	Task 2 Baselines	5	81	0.454545454545455	0.45	We formalized ReCoRD as an extractive QA task like SQuAD, and then reused the fine-tuning script for SQuAD to fine-tune BERT for ReCoRD.	0
43039	43039	D19-6007	Task 2 Baselines	6	82	0.545454545454545	0.455555555555556	KT-NET	0
43040	43040	D19-6007	Task 2 Baselines	7	83	0.636363636363636	0.461111111111111	(Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension.	0
43041	43041	D19-6007	Task 2 Baselines	8	84	0.727272727272727	0.466666666666667	(Seo et al., 2016) and self-attention, both of which are widely used in MC models.	0
43042	43042	D19-6007	Task 2 Baselines	9	85	0.818181818181818	0.472222222222222	We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task.	0
43043	43043	D19-6007	Task 2 Baselines	10	86	0.909090909090909	0.477777777777778	Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer.	0
43044	43044	D19-6007	Task 2 Baselines	11	87	1.0	0.483333333333333	The reported results are averaged over 5 runs.	0
43045	43045	D19-6007	Evaluation	1	88	0.111111111111111	0.488888888888889	Task 1.	0
43046	43046	D19-6007	Evaluation	2	89	0.222222222222222	0.494444444444444	The evaluation measure for task 1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions.	0
43047	43047	D19-6007	Evaluation	3	90	0.333333333333333	0.5	We also report accuracy values on questions that crowd workers explicitly annotated as requiring commonsense as well as performance on the five held-out scenarios.	0
43048	43048	D19-6007	Evaluation	4	91	0.444444444444444	0.505555555555556	Task 2.	0
43049	43049	D19-6007	Evaluation	5	92	0.555555555555556	0.511111111111111	We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b).	0
43050	43050	D19-6007	Evaluation	6	93	0.666666666666667	0.516666666666667	Exact Match (EM) measures the percentage of predictions that match a reference answer exactly.	0
43051	43051	D19-6007	Evaluation	7	94	0.777777777777778	0.522222222222222	(Macro-averaged) F 1 measures the average overlap between model predictions and reference answers.	0
43052	43052	D19-6007	Evaluation	8	95	0.888888888888889	0.527777777777778	For computing F 1 , we treat prediction and reference answers as bags of tokens.	0
43053	43053	D19-6007	Evaluation	9	96	1.0	0.533333333333333	We take the maximum F 1 over all reference answers for a given query, and then average over all queries.	0
43054	43054	D19-6007	Participants	1	97	0.0625	0.538888888888889	In total, five teams submitted systems in task 1, and one team participated in task 2.	0
43055	43055	D19-6007	Participants	2	98	0.125	0.544444444444444	All submitted models were neural networks, and all made use of pretrained Transformer language models such as BERT (Devlin et al., 2019).	0
43056	43056	D19-6007	Participants	3	99	0.1875	0.55	The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019).	0
43057	43057	D19-6007	Participants	4	100	0.25	0.555555555555556	Table 2 gives a summary of the participating systems.	0
43058	43058	D19-6007	Participants	5	101	0.3125	0.561111111111111	• PSH-SJTU  participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b).	0
43059	43059	D19-6007	Participants	6	102	0.375	0.566666666666667	For task 1, they pretrain the model in several steps, first on the RACE data (Lai et al., 2017) and then on SWAG (Zellers et al., 2018).	0
43060	43060	D19-6007	Participants	7	103	0.4375	0.572222222222222	For task 2, they do not conduct specific pretraining steps, but implement a range of simple rulebased answer verification strategies to verify the output of the model.	0
43061	43061	D19-6007	Participants	8	104	0.5	0.577777777777778	• IIT-KGP (Sharma and Roychowdhury, 2019) present an ensemble of different pretrained language models, namely BERT and XLNet.	0
43062	43062	D19-6007	Participants	9	105	0.5625	0.583333333333333	Both models are pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction.	0
43063	43063	D19-6007	Participants	10	106	0.625	0.588888888888889	• BLCU-NLP	0
43064	43064	D19-6007	Participants	11	107	0.6875	0.594444444444444	(Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model for the actual machine comprehension task.	0
43065	43065	D19-6007	Participants	12	108	0.75	0.6	• JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and Web-Child (Tandon et al., 2017).	0
43066	43066	D19-6007	Participants	13	109	0.8125	0.605555555555555	They extract relevant edges from the knowledge bases and compute relation embeddings, which are combined with BERT-based word representations with a diadic multiplication operation.	0
43067	43067	D19-6007	Participants	14	110	0.875	0.611111111111111	• KARNA (Jain and Singh, 2019) use a BERT model, but they enhance the text representation with edges that are extracted from Con-ceptNet.	0
43068	43068	D19-6007	Participants	15	111	0.9375	0.616666666666667	Following Wang et al. (2018), they extract relations between words in the text and the question/answer, and append them to the text representation.	0
43069	43069	D19-6007	Participants	16	112	1.0	0.622222222222222	Instead of computing relational embeddings, they append a specific string that describes the relation.	0
43070	43070	D19-6007	Results	1	113	0.0625	0.627777777777778	Table 3 shows the performance of the participating systems and the baselines on the task 1 data.	0
43071	43071	D19-6007	Results	2	114	0.125	0.633333333333333	We tested for significance using a pairwise approximate randomization test (Yeh, 2000) over questions.	0
43072	43072	D19-6007	Results	3	115	0.1875	0.638888888888889	Except for the two top scoring systems, each system performs significantly better than the next in rank.	0
43073	43073	D19-6007	Results	4	116	0.25	0.644444444444444	All systems significantly outperform the baselines.	0
43074	43074	D19-6007	Results	5	117	0.3125	0.65	All systems show a lower performance on commonsense-based questions as compared to the average on all questions, with the difference for the two top-scoring systems being smallest.	0
43075	43075	D19-6007	Results	6	118	0.375	0.655555555555556	Surprisingly, all models are able to perform better on the questions from held-out scenarios as compared to their performance on all questions.	0
43076	43076	D19-6007	Results	7	119	0.4375	0.661111111111111	This indicates that all models are able to generalize well from the training material.	0
43077	43077	D19-6007	Results	8	120	0.5	0.666666666666667	Table 5 shows the systems' performance on single question types for task 1.	0
43078	43078	D19-6007	Results	9	121	0.5625	0.672222222222222	Question types are determined automatically, as described in (Ostermann et al., 2019).	0
43079	43079	D19-6007	Results	10	122	0.625	0.677777777777778	As can be seen, both topscoring systems perform well over all different question types, indicating that both systems are able to model a wide range of phenomena.	0
43080	43080	D19-6007	Results	11	123	0.6875	0.683333333333333	Interestingly, when questions seem to be the most challenging question type for all systems, indicating difficulties when it comes to model event ordering information.	0
43081	43081	D19-6007	Results	12	124	0.75	0.688888888888889	Also, where questions seem to be challenging, at least for some systems.	0
43082	43082	D19-6007	Results	13	125	0.8125	0.694444444444444	Table 4 shows EM (%) and F 1 (%) of human performance, the PSH-SJTU system as well as baselines on the development and test sets of task 2.	0
43083	43083	D19-6007	Results	14	126	0.875	0.7	Compared with the best baseline, KT-NET (Yang et al., 2019a), PSH-SJTU achieves significantly better scores.	0
43084	43084	D19-6007	Results	15	127	0.9375	0.705555555555556	On the hidden test set, they improve EM by 10.08%, and F 1 by 8.98%.	0
43085	43085	D19-6007	Results	16	128	1.0	0.711111111111111	Consequently, PSH-SJTU has reduced the gap between human and machine performance, with human performance being only 8% higher than PSH-SJTU.	0
43086	43086	D19-6007	Discussion	1	129	0.05	0.716666666666667	Pretrained Transformer language models.	0
43087	43087	D19-6007	Discussion	2	130	0.1	0.722222222222222	A main finding of our shared tasks is that large pretrained Transformer language models such as BERT or XLNet perform well even on challenging commonsense inference data.	0
43088	43088	D19-6007	Discussion	3	131	0.15	0.727777777777778	Strikingly, all models generalize well, as can be seen from the good performance on held-out scenarios.	0
43089	43089	D19-6007	Discussion	4	132	0.2	0.733333333333333	On task 1, XLNet-based systems perform best.	0
43090	43090	D19-6007	Discussion	5	133	0.25	0.738888888888889	The difference to the models purely based on BERT  can mostly be attributed to the performance on commonsense-based questions:	0
43091	43091	D19-6007	Discussion	6	134	0.3	0.744444444444444	While the performance of XLNet-based models on such questions is almost on par with their average performance, models based on BERT underperform on commonsense questions.	0
43092	43092	D19-6007	Discussion	7	135	0.35	0.75	An interesting observation was made by , who found that including WordNet into a BERT model boosts performance, while there is no such boost for an XL-Net model.	0
43093	43093	D19-6007	Discussion	8	136	0.4	0.755555555555556	This seems to indicate that XLNet is able to cover (at least partially) some form of lexical background knowledge, as encoded in Word-Net, without explicitly requiring access to such a resource.	0
43094	43094	D19-6007	Discussion	9	137	0.45	0.761111111111111	Still, when inspecting questions that were not answered correctly by the best scoring model, we found a large number of commonsense-based when questions that ask for the typical order of events.	0
43095	43095	D19-6007	Discussion	10	138	0.5	0.766666666666667	This indicates that XLNet-based models are only to a certain extent able to model complex phenomena such as temporal order.	0
43096	43096	D19-6007	Discussion	11	139	0.55	0.772222222222222	Commonsense knowledge databases.	0
43097	43097	D19-6007	Discussion	12	140	0.6	0.777777777777778	Only two participants made use of commonsense knowledge, in the form of knowledge graphs such as ConceptNet.	0
43098	43098	D19-6007	Discussion	13	141	0.65	0.783333333333333	Both participants conducted ablation tests indicating the importance of including commonsense knowledge.	0
43099	43099	D19-6007	Discussion	14	142	0.7	0.788888888888889	In comparison to ATOMIC and WebChild, Da (2019) report that ConceptNet is most beneficial for performance on the task 1 data, which can be explained with its domain:	0
43100	43100	D19-6007	Discussion	15	143	0.75	0.794444444444444	The OMCS (Singh et al., 2002) data are part of the ConceptNet database, and OMCS scenarios were also used to collect texts for the task 1 data.	0
43101	43101	D19-6007	Discussion	16	144	0.8	0.8	All in all, powerful pretrained models such as XLNet still outperform approaches that make use of structured knowledge bases, which indicates that they are (at least to some extent) capable of performing commonsense inference without explicit representations of commonsense knowledge.	0
43102	43102	D19-6007	Discussion	17	145	0.85	0.805555555555556	Pretraining and finetuning on other data.	0
43103	43103	D19-6007	Discussion	18	146	0.9	0.811111111111111	Several participants reported effects of pretraining/finetuning their models on related tasks.	0
43104	43104	D19-6007	Discussion	19	147	0.95	0.816666666666667	For instance, Liu et al. (2019) experimented with different pretraining corpora and found results to be best when pretraining the encoder of their BERT model on RACE and ReCoRD.	0
43105	43105	D19-6007	Discussion	20	148	1.0	0.822222222222222	Similarly,  report improved results when using larger data sets from other reading comprehension (RACE) and commonsense inference tasks (SWAG) for training before fine-tuning the model with the actual training data from the shared task.	0
43106	43106	D19-6007	Related Work	1	149	0.045454545454546	0.827777777777778	Evaluating commonsense inference via machine comprehension has recently moved into the focus of interest.	0
43107	43107	D19-6007	Related Work	2	150	0.090909090909091	0.833333333333333	Existing datasets cover various domains:	0
43108	43108	D19-6007	Related Work	3	151	0.136363636363636	0.838888888888889	Web texts.	0
43109	43109	D19-6007	Related Work	4	152	0.181818181818182	0.844444444444444	Trivia	0
43110	43110	D19-6007	Related Work	5	153	0.227272727272727	0.85	QA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web.	0
43111	43111	D19-6007	Related Work	6	154	0.272727272727273	0.855555555555556	A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer.	0
43112	43112	D19-6007	Related Work	7	155	0.318181818181818	0.861111111111111	Commonsense	0
43113	43113	D19-6007	Related Work	8	156	0.363636363636364	0.866666666666667	QA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017).	0
43114	43114	D19-6007	Related Work	9	157	0.409090909090909	0.872222222222222	Evidence documents were webcrawled based on the question and added after the crowdsourcing step.	0
43115	43115	D19-6007	Related Work	10	158	0.454545454545455	0.877777777777778	Fictive texts.	0
43116	43116	D19-6007	Related Work	11	159	0.5	0.883333333333333	Narrative	0
43117	43117	D19-6007	Related Work	12	160	0.545454545454545	0.888888888888889	QA (Kočiský et al., 2018) provides full novels and other long texts as evidence documents and contains approx.	0
43118	43118	D19-6007	Related Work	13	161	0.590909090909091	0.894444444444444	30 crowdsourced questions per text.	0
43119	43119	D19-6007	Related Work	14	162	0.636363636363636	0.9	The questions require a system to understand the whole plot of the text and to conduct many successive complicated inference steps, under the use of various types of background knowledge.	0
43120	43120	D19-6007	Related Work	15	163	0.681818181818182	0.905555555555556	News texts.	0
43121	43121	D19-6007	Related Work	16	164	0.727272727272727	0.911111111111111	News	0
43122	43122	D19-6007	Related Work	17	165	0.772727272727273	0.916666666666667	QA (Trischler et al., 2017) provides news texts with crowdsourced questions and answers, which are spans of the evidence documents.	0
43123	43123	D19-6007	Related Work	18	166	0.818181818181818	0.922222222222222	The question collection procedure for NewsQA resulted in a large number of questions that require factual commonsense knowledge for finding an answer.	0
43124	43124	D19-6007	Related Work	19	167	0.863636363636364	0.927777777777778	Other tasks.	0
43125	43125	D19-6007	Related Work	20	168	0.909090909090909	0.933333333333333	There have been other attempts at evaluating commonsense inference apart from machine comprehension.	0
43126	43126	D19-6007	Related Work	21	169	0.954545454545455	0.938888888888889	One example is the Story cloze test and the ROC dataset (Mostafazadeh et al., 2016), where systems have to find the correct ending to a 5-sentence story, using different types of commonsense knowledge.	0
43127	43127	D19-6007	Related Work	22	170	1.0	0.944444444444444	SWAG (Zellers et al., 2018) is a natural language inference dataset with a focus on difficult commonsense inferences.	0
43128	43128	D19-6007	Conclusion	1	171	0.1	0.95	This report presented the results of the shared tasks at the Workshop for Commonsense Inference in NLP (COIN).	0
43129	43129	D19-6007	Conclusion	2	172	0.2	0.955555555555556	The tasks aimed at evaluating the capability of systems to make use of commonsense knowledge for challenging inference questions in a machine comprehension setting, on everyday narrations (task 1) and news texts (task 2).	0
43130	43130	D19-6007	Conclusion	3	173	0.3	0.961111111111111	In total, 5 systems participated in task 1, and one system participated in task 2.	0
43131	43131	D19-6007	Conclusion	4	174	0.4	0.966666666666667	All submitted models were Transformer models, pretrained with a language modeling objective on large amounts of textual data.	0
43132	43132	D19-6007	Conclusion	5	175	0.5	0.972222222222222	The best system achieved 90.6% accuracy and 83.7% F1-score on task 1 and 2, respectively, leaving a gap of 7% and 8% to human performance.	0
43133	43133	D19-6007	Conclusion	6	176	0.6	0.977777777777778	The results of our shared tasks suggest that existing models cover a large part of the commonsense knowledge required for our data sets in the domains of narrations and news texts.	0
43134	43134	D19-6007	Conclusion	7	177	0.7	0.983333333333333	This does however not mean that commonsense inference is solved:	0
43135	43135	D19-6007	Conclusion	8	178	0.8	0.988888888888889	We found a range of examples in our data that are not successfully covered.	0
43136	43136	D19-6007	Conclusion	9	179	0.9	0.994444444444444	Furthermore, data sets such as HellaSWAG (Zellers et al., 2019) show that commonsense inference tasks can be specifically tailored to be hard for Transformer models.	0
43137	43137	D19-6007	Conclusion	10	180	1.0	1.0	We believe that modeling true language understanding requires a shift towards text types and tasks that test commonsense knowledge go-ing beyond information that can be obtained by exploiting the redundancy of large-scale corpora and/or pretraining on related tasks.	0
45218	45218	I17-4002	title	1	1	1.0	0.013157894736842	IJCNLP-2017 Task 2: Dimensional Sentiment Analysis for Chinese Phrases	0
45219	45219	I17-4002	abstract	1	2	0.2	0.026315789473684	This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions.	0
45220	45220	I17-4002	abstract	2	3	0.4	0.039473684210526	Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm.	0
45221	45221	I17-4002	abstract	3	4	0.6	0.052631578947369	Of the 19 teams registered for this shared task for twodimensional sentiment analysis, 13 submitted results.	0
45222	45222	I17-4002	abstract	4	5	0.8	0.065789473684211	We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing.	0
45223	45223	I17-4002	abstract	5	6	1.0	0.078947368421053	All data sets with gold standards and scoring script are made publicly available to researchers.	0
45224	45224	I17-4002	Introduction	1	7	0.055555555555556	0.092105263157895	Sentiment analysis has emerged as a leading technique to automatically identify affective information within texts.	0
45225	45225	I17-4002	Introduction	2	8	0.111111111111111	0.105263157894737	In sentiment analysis, affective states are generally represented using either categorical or dimensional approaches (Calvo and Kim, 2013).	0
45226	45226	I17-4002	Introduction	3	9	0.166666666666667	0.118421052631579	The categorical approach represents affective states as several discrete classes (e.g., positive, negative, neutral), while the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1.	0
45227	45227	I17-4002	Introduction	4	10	0.222222222222222	0.131578947368421	The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm.	0
45228	45228	I17-4002	Introduction	5	11	0.277777777777778	0.144736842105263	Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011;	0
45229	45229	I17-4002	Introduction	6	12	0.333333333333333	0.157894736842105	Malandrakis et al., 2011; or texts (Kim et al., 2010;	0
45230	45230	I17-4002	Introduction	7	13	0.388888888888889	0.171052631578947	Paltoglou et al, 2013;	0
45231	45231	I17-4002	Introduction	8	14	0.444444444444444	0.184210526315789	Wang et al., 2016b).	0
45232	45232	I17-4002	Introduction	9	15	0.5	0.197368421052632	Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014)	0
45233	45233	I17-4002	Introduction	10	16	0.555555555555556	0.210526315789474	The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing.	0
45234	45234	I17-4002	Introduction	11	17	0.611111111111111	0.223684210526316	Sentiment lexicons with valence-arousal ratings are useful resources for the development of dimensional sentiment applications.	0
45235	45235	I17-4002	Introduction	12	18	0.666666666666667	0.236842105263158	Due to the limited availability of such VA lexicons, especially for Chinese, the objective of the task is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases.	0
45236	45236	I17-4002	Introduction	13	19	0.722222222222222	0.25	The rest of this paper is organized as follows.	0
45237	45237	I17-4002	Introduction	14	20	0.777777777777778	0.263157894736842	Section II describes the task in detail.	0
45238	45238	I17-4002	Introduction	15	21	0.833333333333333	0.276315789473684	Section III introduces the constructed datasets.	0
45239	45239	I17-4002	Introduction	16	22	0.888888888888889	0.289473684210526	Section IV proposes evaluation metrics.	0
45240	45240	I17-4002	Introduction	17	23	0.944444444444444	0.302631578947368	Section V reports the results of the participants' approaches.	0
45241	45241	I17-4002	Introduction	18	24	1.0	0.31578947368421	Conclusions are finally drawn in Section VI.	0
45242	45242	I17-4002	Task Description	1	25	0.052631578947369	0.328947368421053	This task seeks to evaluate the capability of systems for predicting dimensional sentiments of Chinese words and phrases.	0
45243	45243	I17-4002	Task Description	2	26	0.105263157894737	0.342105263157895	For a given word or phrase, participants were asked to provide a realvalued score from 1 to 9 for both the valence and arousal dimensions, respectively indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal.	1
45244	45244	I17-4002	Task Description	3	27	0.157894736842105	0.355263157894737	"The input format is ""term_id, term"", and the output format is ""term_id, valence_rating, arousal_rating""."	0
45245	45245	I17-4002	Task Description	4	28	0.210526315789474	0.368421052631579	"Below are the input/output formats of the example words ""好"" (good), ""非常好"" (very good), ""滿意"" (satisfy) and ""不滿意"" (not satisfy)."	0
45246	45246	I17-4002	Task Description	5	29	0.263157894736842	0.381578947368421	with valence-arousal ratings.	0
45247	45247	I17-4002	Task Description	6	30	0.31578947368421	0.394736842105263	For multi-word phrases, we first selected a set of modifiers such as negators (e.g., not), degree adverbs (e.g., very) and modals (e.g., would).	0
45248	45248	I17-4002	Task Description	7	31	0.368421052631579	0.407894736842105	These modifiers were combined with the affective words in CVAW to form multi-word phrases.	0
45249	45249	I17-4002	Task Description	8	32	0.421052631578947	0.421052631578947	The frequency of each phrase was then retrieved from a large web-based corpus.	0
45250	45250	I17-4002	Task Description	9	33	0.473684210526316	0.43421052631579	Only phrases with a frequency greater than or equal to 3 were retained as candidates.	0
45251	45251	I17-4002	Task Description	10	34	0.526315789473684	0.447368421052632	To avoid several modifiers dominating the whole dataset, each modifier (or modifier combination) can have at most 50 phrases.	0
45252	45252	I17-4002	Task Description	11	35	0.578947368421053	0.460526315789474	In addition, the phrases were selected to maximize the balance between positive and negative words.	0
45253	45253	I17-4002	Task Description	12	36	0.631578947368421	0.473684210526316	Finally, a total of 3,000 phrases were collected by excluding unusual and semantically incomplete candidate phrases, of which 2,250 phrases were randomly selected as the training set according to the proportions of each modifier (or modifier combination) in the original set, and the remaining 750 phrases were used as the test set.	0
45254	45254	I17-4002	Task Description	13	37	0.68421052631579	0.486842105263158	Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words .	0
45255	45255	I17-4002	Task Description	14	38	0.736842105263158	0.5	Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth.	0
45256	45256	I17-4002	Task Description	15	39	0.789473684210526	0.513157894736842	Each multi-word phrase was rated by at least 10 different annotators.	0
45257	45257	I17-4002	Task Description	16	40	0.842105263157895	0.526315789473684	Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations.	0
45258	45258	I17-4002	Task Description	17	41	0.894736842105263	0.539473684210526	They were then excluded from the calculation of the average ratings for each phrase.	0
45259	45259	I17-4002	Task Description	18	42	0.947368421052632	0.552631578947368	The policy of this shared task was implemented as is an open test.	0
45260	45260	I17-4002	Task Description	19	43	1.0	0.56578947368421	That is, in addition to the above official datasets, participating teams were allowed to use other publicly available data for system development, but such sources should be specified in the final technical report.	0
45261	45261	I17-4002	Evaluation Metrics	1	44	0.166666666666667	0.578947368421053	Prediction performance is evaluated by examining the difference between machine-predicted ratings and human-annotated ratings, in which valence and arousal are treated independently.	0
45262	45262	I17-4002	Evaluation Metrics	2	45	0.333333333333333	0.592105263157895	The evaluation metrics include Mean Absolute Error (MAE)	0
45263	45263	I17-4002	Evaluation Metrics	3	46	0.5	0.605263157894737	where	0
45264	45264	I17-4002	Evaluation Metrics	4	47	0.666666666666667	0.618421052631579	Ai is the actual value, Pi is the predicted value, n is the number of test samples, A and P respectively denote the arithmetic mean of A and P, and σ is the standard deviation.	0
45265	45265	I17-4002	Evaluation Metrics	5	48	0.833333333333333	0.631578947368421	The MAE measures the error rate and the PCC measures the linear correlation between the actual values and the predicted values.	0
45266	45266	I17-4002	Evaluation Metrics	6	49	1.0	0.644736842105263	A lower MAE and a higher PCC indicate more accurate prediction performance.	0
45267	45267	I17-4002	Evaluation Results	1	50	0.090909090909091	0.657894736842105	Participants	0
45268	45268	I17-4002	Evaluation Results	2	51	0.181818181818182	0.671052631578947	Baseline	0
45269	45269	I17-4002	Evaluation Results	3	52	0.272727272727273	0.68421052631579	We implemented a baseline by training a linear regression model using word vectors as the only features.	0
45270	45270	I17-4002	Evaluation Results	4	53	0.363636363636364	0.697368421052632	For single words, the regression was implemented by directly training word vectors to determine VA scores.	0
45271	45271	I17-4002	Evaluation Results	5	54	0.454545454545455	0.710526315789474	Given a word wi, the baseline regression model is defined as ( ) ( )	0
45272	45272	I17-4002	Evaluation Results	6	55	0.545454545454545	0.723684210526316	where Valwi and Arowi respectively denote the valence and arousal ratings of wi.	0
45273	45273	I17-4002	Evaluation Results	7	56	0.636363636363636	0.736842105263158	W and b respec-tively denote the weights and bias.	0
45274	45274	I17-4002	Evaluation Results	8	57	0.727272727272727	0.75	For phrases, we first calculate the mean vector of the constituent words in the phrase, considering each modifier word can also obtain its word vector.	0
45275	45275	I17-4002	Evaluation Results	9	58	0.818181818181818	0.763157894736842	Give a phrase pj, its representation can be obtained by, where wi∈pj is the word in phrase pj.	0
45276	45276	I17-4002	Evaluation Results	10	59	0.909090909090909	0.776315789473684	The regression was then trained using vec(pj) as a feature, defined as ( ) ( )	0
45277	45277	I17-4002	Evaluation Results	11	60	1.0	0.789473684210526	The word vectors were trained on the Chinese Wiki Corpus 2 using the CBOW model of word2vec 3 (Mikolov et al., 2013a;2013b) (di-mensionality=300 and window size=5).	0
45278	45278	I17-4002	Results	1	61	0.083333333333333	0.802631578947369	Tables 2 shows the results of valence-arousal prediction for single words.	0
45279	45279	I17-4002	Results	2	62	0.166666666666667	0.81578947368421	The three best performing systems are summarized as follows.	0
45280	45280	I17-4002	Results	3	63	0.25	0.828947368421053	Tables 3 shows the results of valence-arousal prediction for multi-word phrases.	0
45281	45281	I17-4002	Results	4	64	0.333333333333333	0.842105263157895	The three best performing systems are summarized as follows.	0
45282	45282	I17-4002	Results	5	65	0.416666666666667	0.855263157894737	Table 4 shows the overall results for both single words and multi-word phrases.	0
45283	45283	I17-4002	Results	6	66	0.5	0.868421052631579	We rank the MAE and PCC independently and calculate the mean rank (average of MAE rank and PCC rank) for ordering system performance.	0
45284	45284	I17-4002	Results	7	67	0.583333333333333	0.881578947368421	The three best performing systems are THU_NGN, AL_I_NLP and CKIP.	0
45285	45285	I17-4002	Results	8	68	0.666666666666667	0.894736842105263	Table 5 summarizes the approaches for each participating system.	0
45286	45286	I17-4002	Results	9	69	0.75	0.907894736842105	CASIA, SAM and XMUT did not submit reports on their developed methods.	0
45287	45287	I17-4002	Results	10	70	0.833333333333333	0.921052631578947	Nearly all teams used word embeddings.	0
45288	45288	I17-4002	Results	11	71	0.916666666666667	0.934210526315789	The most commonly used word embeddings were word2vec (Mikolov et al., 2013a;2013b) and GloVe (Pennington et al., 2014).	0
45289	45289	I17-4002	Results	12	72	1.0	0.947368421052632	Others included FastText 4 (Bojanowski et al., 2017), characterenhanced word embedding (Chen et al., 2015) and Cw2vec (Cao et al., 2017	0
45290	45290	I17-4002	Conclusions	1	73	0.25	0.960526315789474	This study describes an overview of the IJCNLP 2017 shared task on dimensional sentiment analysis for Chinese phrases, including task design, data preparation, performance metrics, and evaluation results.	0
45291	45291	I17-4002	Conclusions	2	74	0.5	0.973684210526316	Regardless of actual performance, all submissions contribute to the common effort to develop dimensional approaches for affective computing, and the individual report in the proceedings provide useful insights into Chinese sentiment analysis.	0
45292	45292	I17-4002	Conclusions	3	75	0.75	0.986842105263158	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
45293	45293	I17-4002	Conclusions	4	76	1.0	1.0	Therefore, all data sets with gold standard and scoring script are publicly available 5 .	0
46115	46115	W11-1802	title	1	1	0.25	0.00990099009901	Overview of Genia Event Task in BioNLP Shared Task 2011	0
46116	46116	W11-1802	abstract	1	2	0.5	0.01980198019802	The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.	1
46117	46117	W11-1802	abstract	2	3	0.75	0.02970297029703	As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.	0
46118	46118	W11-1802	abstract	3	4	1.0	0.03960396039604	After a 3-month system development period, 15 teams submitted their performance results on test cases.	0
46119	46119	W11-1802	abstract	4	5	0.052631578947369	0.04950495049505	The results show the community has made a significant advancement in terms of both performance improvement and generalization.	0
46120	46120	W11-1802	Introduction	1	6	0.105263157894737	0.059405940594059	The BioNLP Shared Task (BioNLP-ST, hereafter) is a series of efforts to promote a communitywide collaboration towards fine-grained information extraction (IE) in biomedical domain.	0
46121	46121	W11-1802	Introduction	2	7	0.157894736842105	0.069306930693069	The first event, BioNLP-ST 2009, introducing a biomolecular event (bio-event) extraction task to the community, attracted a wide attention, with 42 teams being registered for participation and 24 teams submitting final results (Kim et al., 2009).	0
46122	46122	W11-1802	Introduction	3	8	0.210526315789474	0.079207920792079	To establish a community effort, the organizers provided the task definition, benchmark data, and evaluations, and the participants competed in developing systems to perform the task.	0
46123	46123	W11-1802	Introduction	4	9	0.263157894736842	0.089108910891089	Meanwhile, participants and organizers communicated to develop a better setup of evaluation, and some provided their tools and resources for other participants, making it a collaborative competition.	0
46124	46124	W11-1802	Introduction	5	10	0.31578947368421	0.099009900990099	The final results enabled to observe the state-ofthe-art performance of the community on the bioevent extraction task, which showed that the automatic extraction of simple events -those with unary arguments, e.g. gene expression, localization, phosphorylation -could be achieved at the performance level of 70% in F-score, but the extraction of complex events, e.g. binding and regulation, was a lot more challenging, having achieved 40% of performance level.	0
46125	46125	W11-1802	Introduction	6	11	0.368421052631579	0.108910891089109	After BioNLP-ST 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.	0
46126	46126	W11-1802	Introduction	7	12	0.421052631578947	0.118811881188119	Since then, several improvements have been reported (Miwa et al., 2010b;	0
46127	46127	W11-1802	Introduction	8	13	0.473684210526316	0.128712871287129	Poon and Vanderwende, 2010;Vlachos, 2010;Miwa et al., 2010a;	0
46128	46128	W11-1802	Introduction	9	14	0.526315789473684	0.138613861386139	Björne et al., 2010).	0
46129	46129	W11-1802	Introduction	10	15	0.578947368421053	0.148514851485149	For example, Miwa et al.	0
46130	46130	W11-1802	Introduction	11	16	0.631578947368421	0.158415841584158	(Miwa et al., 2010b) reported a significant improvement with binding events, achieving 50% of performance level.	0
46131	46131	W11-1802	Introduction	12	17	0.68421052631579	0.168316831683168	The task introduced in BioNLP-ST 2009 was renamed to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al., 2011).	0
46132	46132	W11-1802	Introduction	13	18	0.736842105263158	0.178217821782178	As the sole task that was repeated in the two events, the GE task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of 2011.	0
46133	46133	W11-1802	Introduction	14	19	0.789473684210526	0.188118811881188	The GE task in 2011 received final submissions from 15 teams.	0
46134	46134	W11-1802	Introduction	15	20	0.842105263157895	0.198019801980198	The results show the community made a significant progress with the task, and also show the technology can be generalized to full papers at moderate cost of performance.	0
46135	46135	W11-1802	Introduction	16	21	0.894736842105263	0.207920792079208	This paper presents the task setup, preparation, and discusses the results.	0
46136	46136	W11-1802	Introduction	17	22	0.947368421052632	0.217821782178218	1: Event types and their arguments for Genia event task.	0
46137	46137	W11-1802	Introduction	18	23	1.0	0.227722772277228	The type of each filler entity is specified in parenthesis.	0
46138	46138	W11-1802	Introduction	19	24	0.043478260869565	0.237623762376238	"Arguments that may be filled more than once per event are marked with ""+""."	0
46139	46139	W11-1802	Task Definition	1	25	0.086956521739131	0.247524752475248	The GE task follows the task definition of BioNLP-ST 2009, which is briefly described in this section.	0
46140	46140	W11-1802	Task Definition	2	26	0.130434782608696	0.257425742574257	For more detail, please refer to (Kim et al., 2009).	0
46141	46141	W11-1802	Task Definition	3	27	0.173913043478261	0.267326732673267	Table 1 shows the event types to be addressed in the task.	0
46142	46142	W11-1802	Task Definition	4	28	0.217391304347826	0.277227722772277	For each event type, the primary and secondary arguments to be extracted with an event are defined.	0
46143	46143	W11-1802	Task Definition	5	29	0.260869565217391	0.287128712871287	For example, a Phosphorylation event is primarily extracted with the protein to be phosphorylated.	0
46144	46144	W11-1802	Task Definition	6	30	0.304347826086957	0.297029702970297	As secondary information, the specific site to be phosphorylated may be extracted.	0
46145	46145	W11-1802	Task Definition	7	31	0.347826086956522	0.306930693069307	From a computational point of view, the event types represent different levels of complexity.	0
46146	46146	W11-1802	Task Definition	8	32	0.391304347826087	0.316831683168317	When only primary arguments are considered, the first five event types in Table 1 are classified as simple event types, requiring only unary arguments.	0
46147	46147	W11-1802	Task Definition	9	33	0.434782608695652	0.326732673267327	The Binding and Regulation types are more complex: Binding requires detection of an arbitrary number of arguments, and Regulation requires detection of recursive event structure.	0
46148	46148	W11-1802	Task Definition	10	34	0.478260869565217	0.336633663366337	Based on the definition of event types, the entire task is divided to three sub-tasks addressing event extraction at different levels of specificity: Task 1.	0
46149	46149	W11-1802	Task Definition	11	35	0.521739130434783	0.346534653465346	Core event extraction addresses the extraction of typed events together with their primary arguments.	0
46150	46150	W11-1802	Task Definition	12	36	0.565217391304348	0.356435643564356	Task 2. Event enrichment addresses the extraction of secondary arguments that further specify the events extracted in Task 1.	0
46151	46151	W11-1802	Task Definition	13	37	0.608695652173913	0.366336633663366	Task 3. Negation/Speculation detection addresses the detection of negations and speculations over the extracted events.	0
46152	46152	W11-1802	Task Definition	14	38	0.652173913043478	0.376237623762376	Task 1 serves as the backbone of the GE task and is mandatory for all participants, while the other two are optional.	0
46153	46153	W11-1802	Task Definition	15	39	0.695652173913043	0.386138613861386	The annotation T1 identifies the entity referred to by the string (p65) between the character offsets, 15 and 18 to be a Protein.	0
46154	46154	W11-1802	Task Definition	16	40	0.739130434782609	0.396039603960396	T2 identifies the string, translocation, to refer to a Localization event.	0
46155	46155	W11-1802	Task Definition	17	41	0.782608695652174	0.405940594059406	Entities other than proteins or event type references are classified into a default class Entity, as in T3. E1 then represents the event defined by the three entities, as defined in Table 1.	0
46156	46156	W11-1802	Task Definition	18	42	0.826086956521739	0.415841584158416	Note that for Task 1, the entity, T3, does not need to be identified, and the event, E1, may be identified without specification of the secondary argument, ToLoc:T1: E1' Localization:	0
46157	46157	W11-1802	Task Definition	19	43	0.869565217391304	0.425742574257426	T2 Theme:	0
46158	46158	W11-1802	Task Definition	20	44	0.91304347826087	0.435643564356436	T1	0
46159	46159	W11-1802	Task Definition	21	45	0.956521739130435	0.445544554455446	Finding the full representation of E1 is the goal of Task 2.	0
46160	46160	W11-1802	Task Definition	22	46	1.0	0.455445544554455	In the example, the localization event, E1, is negated as expressed in the failure of .	0
46161	46161	W11-1802	Task Definition	23	47	0.125	0.465346534653465	Finding the negation, M1 is the goal of Task 3.	0
46162	46162	W11-1802	Data preparation	1	48	0.25	0.475247524752475	The data sets are prepared in two collections: the abstract and the full text collections.	0
46163	46163	W11-1802	Data preparation	2	49	0.375	0.485148514851485	The abstract collection includes the same data used for BioNLP-ST 2009, and is meant to be used to measure the progress of the community.	0
46164	46164	W11-1802	Data preparation	3	50	0.5	0.495049504950495	The full text collection includes full papers which are newly annotated, and is meant to be used to measure the generalization of the technology to full papers.	0
46165	46165	W11-1802	Data preparation	4	51	0.625	0.504950495049505	Table 2 shows the statistics of the annotations in the GE task data sets.	0
46166	46166	W11-1802	Data preparation	5	52	0.75	0.514851485148515	Since the training data from the full text collection is relatively small despite of the expected rich variety of expressions in full text, it is expected that 'generalization' of a model from the abstract collection to full papers would be a key technique to get a reasonable performance.	0
46167	46167	W11-1802	Data preparation	6	53	0.875	0.524752475247525	A full paper consists of several sections including the title, abstract, introduction, results, conclusion, methods, and so on.	0
46168	46168	W11-1802	Data preparation	7	54	1.0	0.534653465346535	Different sections would be written with different purposes, which may affect the type of information that are found in the sections.	0
46169	46169	W11-1802	Data preparation	8	55	0.083333333333333	0.544554455445545	Table 3	0
46170	46170	W11-1802	Participation	1	56	0.166666666666667	0.554455445544555	In total, 15 teams submitted final results.	0
46171	46171	W11-1802	Participation	2	57	0.25	0.564356435643564	All 15 teams participated in the mandatory Task 1, four teams in Task 2, and two teams in Task 3.	0
46172	46172	W11-1802	Participation	3	58	0.333333333333333	0.574257425742574	Only one team, UTurku, completed all the three tasks.	0
46173	46173	W11-1802	Participation	4	59	0.416666666666667	0.584158415841584	Table 4 shows the profile of the teams, excepting three who chose to remain anonymous.	0
46174	46174	W11-1802	Participation	5	60	0.5	0.594059405940594	A brief examination on the team organization (the People column) suggests the importance of a computer science background, C and BI, to perform the GE task, which agrees with the same observation made in 2009.	0
46175	46175	W11-1802	Participation	6	61	0.583333333333333	0.603960396039604	It is interpreted as follows: the role of computer scientists may be emphasized in part due to the fact that the task requires complex computational modeling, demanding particular efforts in framework design and implementation and computational resources.	0
46176	46176	W11-1802	Participation	7	62	0.666666666666667	0.613861386138614	The '09 column suggests that previous experience in the task may have affected to the performance of the teams, especially in a complex task like the GE task.	0
46177	46177	W11-1802	Participation	8	63	0.75	0.623762376237624	Table 5 shows the profile of the systems.	0
46178	46178	W11-1802	Participation	9	64	0.833333333333333	0.633663366336634	A notable observation is that four teams developed their systems based on the model of UTurku09 (Björne et al., 2009) which was the winning sys-tem of BioNLP-ST 2009.	0
46179	46179	W11-1802	Participation	10	65	0.916666666666667	0.643564356435644	It may show an influence of the BioNLP-ST series in the task.	0
46180	46180	W11-1802	Participation	11	66	1.0	0.653465346534653	For syntactic analyses, the prevailing use of Charniak Johnson re-ranking parser (Charniak and Johnson, 2005) using the self-trained biomedical model from Mc-Closky (2008) (McCCJ) which is converted to Stanford Dependency (de Marneffe et al., 2006) is notable, which may also be an influence from the results of BioNLP-ST 2009.	0
46181	46181	W11-1802	Participation	12	67	0.052631578947369	0.663366336633663	The last two teams, XABioNLP and HCMUS, who did not use syntactic analyses could not get a performance comparable to the others, which may suggest the importance of using syntactic analyses for a complex IE task like GE task.	0
46182	46182	W11-1802	Results	1	68	0.105263157894737	0.673267326732673	Task 1	0
46183	46183	W11-1802	Results	2	69	0.157894736842105	0.683168316831683	Table 6 shows the final evaluation results of Task 1.	0
46184	46184	W11-1802	Results	3	70	0.210526315789474	0.693069306930693	For reference, the reported performance of the two systems, UTurku09 and Miwa10 is listed in the top.	0
46185	46185	W11-1802	Results	4	71	0.263157894736842	0.702970297029703	UTurku09 was the winning system of Task 1 in 2009 (Björne et al., 2009), and Miwa10 was the best system reported after BioNLP-ST 2009 (Miwa et al., 2010b	0
46186	46186	W11-1802	Results	5	72	0.31578947368421	0.712871287128713	The best performance in Task 1 this time is achieved by the FAUST system, which adopts a combination model of UMass and Stanford.	0
46187	46187	W11-1802	Results	6	73	0.368421052631579	0.722772277227723	Its performance on the abstract collection, 56.04%, demonstrates a significant improvement of the community in the repeated GE task, when compared to both UTurku09, 51.95% and Miwa10, 53.29%.	0
46188	46188	W11-1802	Results	7	74	0.421052631578947	0.732673267326733	The biggest improvement is made to the Regulation events (40.11%→46.97%) which requires a complex modeling for recursive event structure -an event may become an argument of another event.	0
46189	46189	W11-1802	Results	8	75	0.473684210526316	0.742574257425743	The second ranked system, UMass, shows the best performance on the full paper collection.	0
46190	46190	W11-1802	Results	9	76	0.526315789473684	0.752475247524752	It suggests that what FAUST obtained from the model combi-nation might be a better optimization to abstracts.	0
46191	46191	W11-1802	Results	10	77	0.578947368421053	0.762376237623762	The Concord	0
46192	46192	W11-1802	Results	11	78	0.631578947368421	0.772277227722772	U system is notable as it is the sole rule-based system that is ranked above the average.	0
46193	46193	W11-1802	Results	12	79	0.68421052631579	0.782178217821782	It shows a performance optimized for precision with relatively low recall.	0
46194	46194	W11-1802	Results	13	80	0.736842105263158	0.792079207920792	The same tendency is roughly replicated by other rule-based systems, CCP-BTMG, TM-SCS, XABioNLP, and HCMUS.	0
46195	46195	W11-1802	Results	14	81	0.789473684210526	0.801980198019802	It suggests that a rule-based system might not be a good choice if a high coverage is desired.	0
46196	46196	W11-1802	Results	15	82	0.842105263157895	0.811881188118812	However, the performance of Concord	0
46197	46197	W11-1802	Results	16	83	0.894736842105263	0.821782178217822	U for simple events suggests that a high precision can be achieved by a rule based system with a modest loss of recall.	0
46198	46198	W11-1802	Results	17	84	0.947368421052632	0.831683168316832	It might be more true when the task is less complex.	0
46199	46199	W11-1802	Results	18	85	1.0	0.841584158415841	This time, three teams achieved better results than Miwa10, which indicates some role of focused efforts like BioNLP-ST.	0
46200	46200	W11-1802	Results	19	86	0.125	0.851485148514851	The comparison between the performance on abstract and full paper collections shows that generalization to full papers is feasible with very modest loss in performance.	0
46201	46201	W11-1802	Task 2	1	87	0.25	0.861386138613861	Tables 7 shows final evaluation results of Task 2.	0
46202	46202	W11-1802	Task 2	2	88	0.375	0.871287128712871	For reference, the reported performance of the taskwinning system in 2009, UT+DBCLS09 (Riedel et al., 2009), is shown in the top.	0
46203	46203	W11-1802	Task 2	3	89	0.5	0.881188118811881	The first and second ranked system, FAUST and UMass, which share a same author with Riedel09, made a significant improvement over Riedel09 in the abstract collection.	0
46204	46204	W11-1802	Task 2	4	90	0.625	0.891089108910891	UTurku achieved the best performance in finding sites arguments but did not produce location arguments.	0
46205	46205	W11-1802	Task 2	5	91	0.75	0.900990099009901	In table 7, the performance of all the systems in full text collection suggests that finding secondary arguments in full text is much more challenging.	0
46206	46206	W11-1802	Task 2	6	92	0.875	0.910891089108911	In detail, a significant improvement was made for Location arguments (36.59%→50.00%).	0
46207	46207	W11-1802	Task 2	7	93	1.0	0.920792079207921	A further breakdown of the results of site extraction, shown in table 8, shows that finding site arguments for Phosphorylation, Binding and Regulation events are all significantly improved, but in different ways.	0
46208	46208	W11-1802	Task 2	8	94	0.25	0.930693069306931	The extraction of protein sites to be phosphorylated is approaching a practical level of performance (84.21%), while protein sites to be bound or to be regulated remains challenging to be extracted.	0
46209	46209	W11-1802	Task 3	1	95	0.5	0.940594059405941	Table 9 shows final evaluation results of Task 3.	0
46210	46210	W11-1802	Task 3	2	96	0.75	0.95049504950495	For reference, the reported performance of the taskwinning system in 2009, Kilicoglu09 (Kilicoglu and Bergler, 2009), is shown in the top.	0
46211	46211	W11-1802	Task 3	3	97	1.0	0.96039603960396	Among the two teams participated in the task, UTurku showed a better performance in extracting negated events, while Concord	0
46212	46212	W11-1802	Task 3	4	98	0.333333333333333	0.97029702970297	U showed a better performance in extracting speculated events.	0
46213	46213	W11-1802	Conclusions	1	99	0.666666666666667	0.98019801980198	The Genia event task which was repeated for BioNLP-ST 2009 and 2011 took a role of measuring the progress of the community and generalization IE technology to full papers.	0
46214	46214	W11-1802	Conclusions	2	100	1.0	0.99009900990099	The results from 15 teams who made their final submissions to the task show that a clear advance of the community in terms of the performance on a focused domain and also generalization to full papers.	0
46215	46215	W11-1802	Conclusions	3	101	1.0	1.0	To our disappointment, however, an effective use of supporting task results was not observed, which thus remains as future work for further improvement.	0
