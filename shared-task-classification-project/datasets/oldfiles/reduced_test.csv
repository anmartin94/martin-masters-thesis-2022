	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
2093	2093	S07-2	title	1	1	1.0	0.006134969325153	Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems	1
2094	2094	S07-2	abstract	1	2	0.25	0.012269938650307	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems.	0
2095	2095	S07-2	abstract	2	3	0.5	0.01840490797546	In total there were 6 participating systems.	0
2096	2096	S07-2	abstract	3	4	0.75	0.024539877300614	We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).	0
2097	2097	S07-2	abstract	4	5	1.0	0.030674846625767	We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
2098	2098	S07-2	Introduction	1	6	0.052631578947369	0.03680981595092	Word Sense Disambiguation (WSD) is a key enabling-technology.	0
2099	2099	S07-2	Introduction	2	7	0.105263157894737	0.042944785276074	Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.	0
2100	2100	S07-2	Introduction	3	8	0.157894736842105	0.049079754601227	Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).	0
2101	2101	S07-2	Introduction	4	9	0.210526315789474	0.05521472392638	In theory, larger amounts of training data (SemCor has approx.	0
2102	2102	S07-2	Introduction	5	10	0.263157894736842	0.061349693251534	500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.	0
2103	2103	S07-2	Introduction	6	11	0.31578947368421	0.067484662576687	Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart√≠nez and Agirre, 2000;	0
2104	2104	S07-2	Introduction	7	12	0.368421052631579	0.073619631901841	Koeling et al., 2005).	0
2105	2105	S07-2	Introduction	8	13	0.421052631578947	0.079754601226994	"Supervised WSD is based on the ""fixed-list of senses"" paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon."	0
2106	2106	S07-2	Introduction	9	14	0.473684210526316	0.085889570552147	Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).	0
2107	2107	S07-2	Introduction	10	15	0.526315789473684	0.092024539877301	Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.	1
2108	2108	S07-2	Introduction	11	16	0.578947368421053	0.098159509202454	Typical WSID systems involve clustering techniques, which group together similar examples.	0
2109	2109	S07-2	Introduction	12	17	0.631578947368421	0.104294478527607	Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.	0
2110	2110	S07-2	Introduction	13	18	0.68421052631579	0.110429447852761	One of the problems of unsupervised systems is that of managing to do a fair evaluation.	0
2111	2111	S07-2	Introduction	14	19	0.736842105263158	0.116564417177914	Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.	0
2112	2112	S07-2	Introduction	15	20	0.789473684210526	0.122699386503067	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.	0
2113	2113	S07-2	Introduction	16	21	0.842105263157895	0.128834355828221	The paper is organized as follows.	0
2114	2114	S07-2	Introduction	17	22	0.894736842105263	0.134969325153374	Section 2 presents the evaluation framework used in this task.	0
2115	2115	S07-2	Introduction	18	23	0.947368421052632	0.141104294478528	Section 3 presents the systems that participated in the task, and the official results.	0
2116	2116	S07-2	Introduction	19	24	1.0	0.147239263803681	Finally, Section 5 draws the conclusions.	0
2816	2816	S07-9	abstract	1	2	0.333333333333333	0.015384615384615	In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish).	0
2817	2817	S07-9	abstract	2	3	0.666666666666667	0.023076923076923	In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish.	1
2818	2818	S07-9	abstract	3	4	1.0	0.030769230769231	Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.	0
5273	5273	S10-12	abstract	1	2	0.333333333333333	0.013698630136986	Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.	0
5274	5274	S10-12	abstract	2	3	0.666666666666667	0.02054794520548	The task involves recognizing textual entailments based on syntactic information alone.	1
5275	5275	S10-12	abstract	3	4	1.0	0.027397260273973	PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.	0
5276	5276	S10-12	Introduction	1	5	0.016393442622951	0.034246575342466	Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.	1
5277	5277	S10-12	Introduction	2	6	0.032786885245902	0.041095890410959	"Given two text fragments called ""text"" and ""hypothesis"", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text."	0
5278	5278	S10-12	Introduction	3	7	0.049180327868853	0.047945205479452	In contrast with general RTE tasks (Dagan et al., 2009) the PETE task focuses on syntactic entailments:	0
5279	5279	S10-12	Introduction	4	8	0.065573770491803	0.054794520547945	Text:	0
5280	5280	S10-12	Introduction	5	9	0.081967213114754	0.061643835616438	The man with the hat was tired.	0
5281	5281	S10-12	Introduction	6	10	0.098360655737705	0.068493150684932	Hypothesis-1: The man was tired.	0
5282	5282	S10-12	Introduction	7	11	0.114754098360656	0.075342465753425	(yes) Hypothesis-2: The hat was tired.	0
5283	5283	S10-12	Introduction	8	12	0.131147540983607	0.082191780821918	(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).	0
5284	5284	S10-12	Introduction	9	13	0.147540983606557	0.089041095890411	We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
5285	5285	S10-12	Introduction	10	14	0.163934426229508	0.095890410958904	The PARSEVAL measures introduced nearly two decades ago (Black et al., 1991) still dominate the field of parser evaluation.	0
5286	5286	S10-12	Introduction	11	15	0.180327868852459	0.102739726027397	"These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or ""treebank""."	0
5287	5287	S10-12	Introduction	12	16	0.19672131147541	0.10958904109589	Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.	0
5288	5288	S10-12	Introduction	13	17	0.213114754098361	0.116438356164384	Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.	0
5289	5289	S10-12	Introduction	14	18	0.229508196721311	0.123287671232877	Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.	0
5290	5290	S10-12	Introduction	15	19	0.245901639344262	0.13013698630137	The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.	0
5291	5291	S10-12	Introduction	16	20	0.262295081967213	0.136986301369863	In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank (Marcus et al., 1994), 5646 (15%) have multiple conflicting annotations.	0
5292	5292	S10-12	Introduction	17	21	0.278688524590164	0.143835616438356	If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90% (Charniak and Johnson, 2005).	0
5293	5293	S10-12	Introduction	18	22	0.295081967213115	0.150684931506849	Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.	0
5294	5294	S10-12	Introduction	19	23	0.311475409836066	0.157534246575342	Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.	0
5295	5295	S10-12	Introduction	20	24	0.327868852459016	0.164383561643836	Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.	0
5296	5296	S10-12	Introduction	21	25	0.344262295081967	0.171232876712329	Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (Bonnema et al., 1997).	0
5297	5297	S10-12	Introduction	22	26	0.360655737704918	0.178082191780822	In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.	0
5298	5298	S10-12	Introduction	23	27	0.377049180327869	0.184931506849315	Framework independence: Entailment recognition is a formalism independent task.	0
5299	5299	S10-12	Introduction	24	28	0.39344262295082	0.191780821917808	A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (Nivre et al., 2007a;	0
5300	5300	S10-12	Introduction	25	29	0.40983606557377	0.198630136986301	Hockenmaier and Steedman, 2007).	0
5301	5301	S10-12	Introduction	26	30	0.426229508196721	0.205479452054794	The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.	0
5302	5302	S10-12	Introduction	27	31	0.442622950819672	0.212328767123288	In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.	0
5303	5303	S10-12	Introduction	28	32	0.459016393442623	0.219178082191781	Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.	0
5304	5304	S10-12	Introduction	29	33	0.475409836065574	0.226027397260274	PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.	0
5305	5305	S10-12	Introduction	30	34	0.491803278688525	0.232876712328767	These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals).	0
5306	5306	S10-12	Introduction	31	35	0.508196721311475	0.23972602739726	Each use a set of binary relations between words in a sentence as the primary unit of representation.	0
5307	5307	S10-12	Introduction	32	36	0.524590163934426	0.246575342465753	They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.	0
5308	5308	S10-12	Introduction	33	37	0.540983606557377	0.253424657534247	Here is an example sentence and its SD representation (De Marneffe and Manning, 2008):	0
5309	5309	S10-12	Introduction	34	38	0.557377049180328	0.26027397260274	Bell, based in Los Angeles, makes and distributes electronic, computer and building products.	0
5310	5310	S10-12	Introduction	35	39	0.573770491803279	0.267123287671233	nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod (products-16, electronic-11) conj-and(electronic-11, computer-13) amod (products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.	0
5311	5311	S10-12	Introduction	36	40	0.590163934426229	0.273972602739726	Bell makes something.	0
5312	5312	S10-12	Introduction	37	41	0.60655737704918	0.280821917808219	Bell distributes something.	0
5313	5313	S10-12	Introduction	38	42	0.622950819672131	0.287671232876712	Someone is based in Los Angeles.	0
5314	5314	S10-12	Introduction	39	43	0.639344262295082	0.294520547945205	Someone makes products.	0
5315	5315	S10-12	Introduction	40	44	0.655737704918033	0.301369863013699	PETE has some advantages over representations based on grammatical relations.	0
5316	5316	S10-12	Introduction	41	45	0.672131147540984	0.308219178082192	For example SD defines 55 relations organized in a hierarchy, and it may be non-trivial for a non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).	0
5317	5317	S10-12	Introduction	42	46	0.688524590163934	0.315068493150685	In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.	0
5318	5318	S10-12	Introduction	43	47	0.704918032786885	0.321917808219178	In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.	0
5319	5319	S10-12	Introduction	44	48	0.721311475409836	0.328767123287671	However there are also significant challenges associated with an evaluation scheme like PETE.	0
5320	5320	S10-12	Introduction	45	49	0.737704918032787	0.335616438356164	It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.	0
5321	5321	S10-12	Introduction	46	50	0.754098360655738	0.342465753424657	Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.	0
5322	5322	S10-12	Introduction	47	51	0.770491803278688	0.349315068493151	Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.	0
5323	5323	S10-12	Introduction	48	52	0.786885245901639	0.356164383561644	"Introducing dummy words like ""someone"" or ""something"" alleviates part of the problem but does not help in the case of clausal complements."	0
5324	5324	S10-12	Introduction	49	53	0.80327868852459	0.363013698630137	In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.	0
5325	5325	S10-12	Introduction	50	54	0.819672131147541	0.36986301369863	PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.	0
5326	5326	S10-12	Introduction	51	55	0.836065573770492	0.376712328767123	This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.	0
5327	5327	S10-12	Introduction	52	56	0.852459016393443	0.383561643835616	However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.	0
5328	5328	S10-12	Introduction	53	57	0.868852459016393	0.39041095890411	The balance of what is being evaluated is also important.	0
5329	5329	S10-12	Introduction	54	58	0.885245901639344	0.397260273972603	A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.	0
5330	5330	S10-12	Introduction	55	59	0.901639344262295	0.404109589041096	In this evaluation, we focused on sentences and relations where state of the art parsers disagree.	0
5331	5331	S10-12	Introduction	56	60	0.918032786885246	0.410958904109589	We hope this methodology will uncover weaknesses that the next generation systems can focus on.	0
5332	5332	S10-12	Introduction	57	61	0.934426229508197	0.417808219178082	The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.	0
5333	5333	S10-12	Introduction	58	62	0.950819672131147	0.424657534246575	Section 2 explains the method followed to create the PETE dataset.	0
5334	5334	S10-12	Introduction	59	63	0.967213114754098	0.431506849315069	Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.	0
5335	5335	S10-12	Introduction	60	64	0.983606557377049	0.438356164383562	Section 4 presents the participating systems, their methods and results.	0
5336	5336	S10-12	Introduction	61	65	1.0	0.445205479452055	Section 5 summarizes our contribution.	0
5839	5839	S10-18	Task Set up 2.1 Task description	1	31	0.125	0.19496855345912	In this task, we focus on 14 frequently used sentiment ambiguous adjectives in Chinese, which all have the meaning of measurement, as shown below.	0
5840	5840	S10-18	Task Set up 2.1 Task description	2	32	0.25	0.20125786163522	"(1) Sentiment ambiguous adjectives(SAAs) ={ Â§ß da "" large"" , Â§ö duo "" many"" , È´ò gao "" high"" , Âéö hou "" thick"" , Ê∑± shen "" deep"" , Èáç zhong "" heavy"" , Â∑®Â§ß ju-da "" huge"" , ÈáçÂ§ß zhong-da "" great"" , Â∞è xiao "" small"" , Â∞ë shao "" few"" , ‰Ωé di "" low"" , ËñÑ bao "" thin"" , ÊµÖ qian "" shallow"" , ËΩª qing "" light"" }"	0
5841	5841	S10-18	Task Set up 2.1 Task description	3	33	0.375	0.207547169811321	These adjectives are neutral out of context, but when they co-occur with some target nouns, positive or negative emotion will be evoked.	0
5842	5842	S10-18	Task Set up 2.1 Task description	4	34	0.5	0.213836477987421	Although the number of such ambiguous adjectives is not large, they are frequently used in real text, especially in the texts expressing opinions and emotions.	0
5843	5843	S10-18	Task Set up 2.1 Task description	5	35	0.625	0.220125786163522	The task is designed to automatically determine the SO of these sentiment ambiguous adjectives within context: positive or negative.	1
5844	5844	S10-18	Task Set up 2.1 Task description	6	36	0.75	0.226415094339623	"For example, È´ò gao "" high""should be assigned as positive in Â∑• ËµÑ È´ò gong-zi -gao "" salary is high""but negative in ‰ª∑Ê†ºÈ´ò jia-ge-gao "" price is high"" ."	0
5845	5845	S10-18	Task Set up 2.1 Task description	7	37	0.875	0.232704402515723	This task was carried out in an unsupervised setting.	0
5846	5846	S10-18	Task Set up 2.1 Task description	8	38	1.0	0.238993710691824	No training data was provided, but external resources are encouraged to use.	0
7084	7084	S12-6	Systems Evaluation	1	106	0.1	0.543589743589744	Given two sentences, s1 and s2, an STS system would need to return a similarity score.	1
7085	7085	S12-6	Systems Evaluation	2	107	0.2	0.548717948717949	Participants can also provide a confidence score indicating their confidence level for the result returned for each pair, but this confidence is not used for the main results.	1
7086	7086	S12-6	Systems Evaluation	3	108	0.3	0.553846153846154	The output of the systems performance is evaluated using the Pearson product-moment correlation coefficient between the system scores and the human scores, as customary in text similarity (Rubenstein and Goodenough, 1965).	0
7087	7087	S12-6	Systems Evaluation	4	109	0.4	0.558974358974359	We calculated Pearson for each evaluation dataset separately.	0
7088	7088	S12-6	Systems Evaluation	5	110	0.5	0.564102564102564	In order to have a single Pearson measure for each system we concatenated the gold standard (and system outputs) for all 5 datasets into a single gold stan-dard file (and single system output).	0
7089	7089	S12-6	Systems Evaluation	6	111	0.6	0.569230769230769	The first version of the results were published using this method, but the overall score did not correspond well to the individual scores in the datasets, and participants proposed two additional evaluation metrics, both of them based on Pearson correlation.	0
7090	7090	S12-6	Systems Evaluation	7	112	0.7	0.574358974358974	The organizers of the task decided that it was more informative, and on the benefit of the community, to also adopt those evaluation metrics, and the idea of having a single main evaluation metric was dropped.	0
7091	7091	S12-6	Systems Evaluation	8	113	0.8	0.57948717948718	This decision was not without controversy, but the organizers gave more priority to openness and inclusiveness and to the involvement of participants.	0
7092	7092	S12-6	Systems Evaluation	9	114	0.9	0.584615384615385	The final result table thus included three evaluation metrics.	0
7093	7093	S12-6	Systems Evaluation	10	115	1.0	0.58974358974359	For the future we plan to analyze the evaluation metrics, including non-parametric metrics like Spearman.	0
8124	8124	S13-5	Introduction	1	7	0.043478260869565	0.034653465346535	Numerous past tasks have focused on leveraging the meaning of word types or words in context.	0
8125	8125	S13-5	Introduction	2	8	0.086956521739131	0.03960396039604	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.	0
8126	8126	S13-5	Introduction	3	9	0.130434782608696	0.044554455445545	As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.	0
8127	8127	S13-5	Introduction	4	10	0.173913043478261	0.04950495049505	In this paper, we present two subtasks designed to evaluate such phrasal models: a. Semantic similarity of words and compositional phrases b.	1
8128	8128	S13-5	Introduction	5	11	0.217391304347826	0.054455445544555	Evaluating the compositionality of phrases in context	1
8129	8129	S13-5	Introduction	6	12	0.260869565217391	0.059405940594059	"For example, the first subtask addresses computing how similar the word ""valuation"" is to the compositional sequence ""price assessment"", while the second subtask addresses deciding whether the phrase ""piece of cake"" is used literally or figuratively in the sentence ""Labour was a piece of cake!""."	0
8130	8130	S13-5	Introduction	7	13	0.304347826086957	0.064356435643564	The aim of these subtasks is two-fold.	0
8131	8131	S13-5	Introduction	8	14	0.347826086956522	0.069306930693069	Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.	0
8132	8132	S13-5	Introduction	9	15	0.391304347826087	0.074257425742574	It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.	0
8133	8133	S13-5	Introduction	10	16	0.434782608695652	0.079207920792079	Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.	0
8134	8134	S13-5	Introduction	11	17	0.478260869565217	0.084158415841584	Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.	0
8135	8135	S13-5	Introduction	12	18	0.521739130434783	0.089108910891089	We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.	0
8136	8136	S13-5	Introduction	13	19	0.565217391304348	0.094059405940594	Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.	0
8137	8137	S13-5	Introduction	14	20	0.608695652173913	0.099009900990099	In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.	0
8138	8138	S13-5	Introduction	15	21	0.652173913043478	0.103960396039604	The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.	0
8139	8139	S13-5	Introduction	16	22	0.695652173913043	0.108910891089109	Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.	0
8140	8140	S13-5	Introduction	17	23	0.739130434782609	0.113861386138614	In subsection 3.1 the subtask is described in detail together with some information about its background.	0
8141	8141	S13-5	Introduction	18	24	0.782608695652174	0.118811881188119	Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.	0
8142	8142	S13-5	Introduction	19	25	0.826086956521739	0.123762376237624	Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.	0
8143	8143	S13-5	Introduction	20	26	0.869565217391304	0.128712871287129	Subsection 4.1 explains the data creation process for this subtask.	0
8144	8144	S13-5	Introduction	21	27	0.91304347826087	0.133663366336634	In subsection 4.2 the evaluation statistics of participating systems are presented.	0
8145	8145	S13-5	Introduction	22	28	0.956521739130435	0.138613861386139	Section 5 is a discussion about the conclusions of the entire task.	0
8146	8146	S13-5	Introduction	23	29	1.0	0.143564356435644	Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.	0
9050	9050	S13-11	abstract	1	2	0.333333333333333	0.015267175572519	In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification.	0
9051	9051	S13-11	abstract	2	3	0.666666666666667	0.022900763358779	Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.	1
9052	9052	S13-11	abstract	3	4	1.0	0.030534351145038	The task enables the end-to-end evaluation and comparison of systems.	0
9390	9390	S13-13	Task Description	1	22	0.090909090909091	0.115183246073298	This task required participating systems to annotate instances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected due to its fine-grained senses.	1
9391	9391	S13-13	Task Description	2	23	0.181818181818182	0.120418848167539	Participants could label each instance with one or more senses, weighting	1
9392	9392	S13-13	Task Description	3	24	0.272727272727273	0.12565445026178	We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to encompass the thoughts of every entity we know.	0
9393	9393	S13-13	Task Description	4	25	0.363636363636364	0.130890052356021	dark%3:00:01:: -devoid of or deficient in light or brightness; shadowed or black dark%3:00:00:: -secret I ask because my practice has always been to allow about five minutes grace, then remove it.	0
9394	9394	S13-13	Task Description	5	26	0.454545454545455	0.136125654450262	ask%2:32:02:: -direct or put; seek an answer to ask%2:32:04:: -address a question to and expect an answer from Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual ambiguity (bottom).	0
9395	9395	S13-13	Task Description	6	27	0.545454545454545	0.141361256544503	Senses are specified using their WordNet 3.1 sense keys.	0
9396	9396	S13-13	Task Description	7	28	0.636363636363636	0.146596858638743	each by their applicability.	0
9397	9397	S13-13	Task Description	8	29	0.727272727272727	0.151832460732984	Table 1 highlights two example contexts where multiple senses apply.	0
9398	9398	S13-13	Task Description	9	30	0.818181818181818	0.157068062827225	The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.	0
9399	9399	S13-13	Task Description	10	31	0.909090909090909	0.162303664921466	a as being devoid of light and the causal result of being secret.	0
9400	9400	S13-13	Task Description	11	32	1.0	0.167539267015707	"In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations ""I ask [you] because"" (sense ask%2:32:04::) or ""I ask [the question] because"" (sense ask%2:32:02::)."	0
10487	10487	S14-6	abstract	1	2	0.166666666666667	0.01010101010101	SemEval-2014	0
10488	10488	S14-6	abstract	2	3	0.333333333333333	0.015151515151515	Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches.	0
10489	10489	S14-6	abstract	3	4	0.5	0.02020202020202	The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm.	1
10490	10490	S14-6	abstract	4	5	0.666666666666667	0.025252525252525	Six teams submitted systems using both rule-based and statistical methods.	0
10491	10491	S14-6	abstract	5	6	0.833333333333333	0.03030303030303	The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context.	0
10492	10492	S14-6	abstract	6	7	1.0	0.035353535353535	However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.	0
11047	11047	S14-9	Subtask A: Contextual Polarity Disambiguation	1	38	0.5	0.267605633802817	Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context.	1
11048	11048	S14-9	Subtask A: Contextual Polarity Disambiguation	2	39	1.0	0.274647887323944	The instance boundaries were provided: this was a classification task, not an entity recognition task.	0
12454	12454	S15-6	abstract	1	2	0.25	0.014492753623188	Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.	0
12455	12455	S15-6	abstract	2	3	0.5	0.021739130434783	Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.	1
12456	12456	S15-6	abstract	3	4	0.75	0.028985507246377	Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.	0
12457	12457	S15-6	abstract	4	5	1.0	0.036231884057971	Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.	0
13717	13717	S15-13	title	1	1	1.0	0.005882352941176	SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking	1
13718	13718	S15-13	abstract	1	2	0.2	0.011764705882353	In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.	1
13719	13719	S15-13	abstract	2	3	0.4	0.01764705882353	Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.	0
13720	13720	S15-13	abstract	3	4	0.6	0.023529411764706	Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.	0
13721	13721	S15-13	abstract	4	5	0.8	0.029411764705882	Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.	0
13722	13722	S15-13	abstract	5	6	1.0	0.035294117647059	Moreover, we investigate this task in a multilingual setting and for some specific domains.	0
23178	23178	S19-2	abstract	1	2	0.142857142857143	0.00749063670412	This paper presents Unsupervised Lexical	0
23179	23179	S19-2	abstract	2	3	0.285714285714286	0.01123595505618	Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.	0
23180	23180	S19-2	abstract	3	4	0.428571428571429	0.01498127340824	Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.	1
23181	23181	S19-2	abstract	4	5	0.571428571428571	0.0187265917603	Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.	0
23182	23182	S19-2	abstract	5	6	0.714285714285714	0.02247191011236	Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.	0
23183	23183	S19-2	abstract	6	7	0.857142857142857	0.02621722846442	The shared task attracted nine teams, of whom three reported promising results.	0
23184	23184	S19-2	abstract	7	8	1.0	0.029962546816479	This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.	0
26005	26005	S20-4	abstract	1	2	0.111111111111111	0.006968641114983	In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons.	1
26006	26006	S20-4	abstract	2	3	0.222222222222222	0.010452961672474	Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not.	0
26007	26007	S20-4	abstract	3	4	0.333333333333333	0.013937282229965	The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense.	0
26008	26008	S20-4	abstract	4	5	0.444444444444444	0.017421602787457	In the third subtask, a participating system needs to generate the reason.	0
26009	26009	S20-4	abstract	5	6	0.555555555555556	0.020905923344948	We finally attracted 39 teams participating at least one of the three subtasks.	0
26010	26010	S20-4	abstract	6	7	0.666666666666667	0.024390243902439	For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans.	0
26011	26011	S20-4	abstract	7	8	0.777777777777778	0.02787456445993	However, for Subtask C, there is still a relatively large gap between systems and human performance.	0
26012	26012	S20-4	abstract	8	9	0.888888888888889	0.031358885017422	The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation;	0
26013	26013	S20-4	abstract	9	10	1.0	0.034843205574913	The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.	0
26300	26300	S20-5	Introduction	1	10	0.027027027027027	0.043859649122807	"Counterfactual statements describe events that did not happen or cannot happen, and the possible consequences had those events happened, e.g., ""if kangaroos had no tails, they would topple over"" (Lewis, 2013)."	0
26301	26301	S20-5	Introduction	2	11	0.054054054054054	0.048245614035088	"By developing a connection between the antecedent (e.g., ""kangaroos had no tails"") and consequent (e.g., ""they would topple over""), based on the imagination of possible worlds, humans can naturally form some causal judgments; e.g., having tails can prevent kangaroos from toppling over."	0
26302	26302	S20-5	Introduction	3	12	0.081081081081081	0.052631578947369	One can understand counterfactuals using knowledge and explore the relationship between causes and effects.	0
26303	26303	S20-5	Introduction	4	13	0.108108108108108	0.057017543859649	Although we may not be able to rollback the events which have happened or make impossible events occur in the real world, we can still think of potential outcomes of alternatives.	0
26304	26304	S20-5	Introduction	5	14	0.135135135135135	0.06140350877193	Counterfactual thinking is a remarkable ability of human beings and is considered by many researchers, to act as the highest level of causation in the ladder of causal reasoning.	0
26305	26305	S20-5	Introduction	6	15	0.162162162162162	0.065789473684211	Even the most advanced artificial intelligence system may still be far from achieving human-like counterfactual reasoning.	0
26306	26306	S20-5	Introduction	7	16	0.189189189189189	0.070175438596491	Counterfactual reasoning is an important component for AI systems in obtaining stronger capability in generalization (Pearl and Mackenzie, 2018).	0
26307	26307	S20-5	Introduction	8	17	0.216216216216216	0.074561403508772	Modeling counterfactuals has been studied in many different disciplines.	0
26308	26308	S20-5	Introduction	9	18	0.243243243243243	0.078947368421053	For example, research in psychology has shown that counterfactual thinking can affect human cognition and behaviors (Epstude and Roese, 2008;	0
26309	26309	S20-5	Introduction	10	19	0.27027027027027	0.083333333333333	Kray et al., 2010).	0
26310	26310	S20-5	Introduction	11	20	0.297297297297297	0.087719298245614	The landmark paper of (Goodman, 1947) gives a detailed analysis of counterfactual conditionals in philosophy and logistics.	0
26311	26311	S20-5	Introduction	12	21	0.324324324324324	0.092105263157895	As another example, counterfactuals have also been investigated in epidemiology to reveal the relationship between certain diseases and potential risk factors for those diseases (Vandenbroucke et al., 2016;	0
26312	26312	S20-5	Introduction	13	22	0.351351351351351	0.096491228070176	Krieger and Davey Smith, 2016).	0
26313	26313	S20-5	Introduction	14	23	0.378378378378378	0.100877192982456	We present a counterfactual recognition (CR) task, the task of determining whether a given statement conveys counterfactual thinking or not, and further analyzing the causal relations indicated by counterfactual statements.	0
26314	26314	S20-5	Introduction	15	24	0.405405405405405	0.105263157894737	In our counterfactual recognition task, we aim to model counterfactual semantics and reasoning in natural language.	1
26315	26315	S20-5	Introduction	16	25	0.432432432432432	0.109649122807018	Specifically, we provide a benchmark for counterfactual recognition with two subtasks.	0
26316	26316	S20-5	Introduction	17	26	0.45945945945946	0.114035087719298	Subtask-1 requires systems to determine whether a given statement is counterfactual or not.	0
26317	26317	S20-5	Introduction	18	27	0.486486486486487	0.118421052631579	The counterfactual detection task can serve as a foundation for downstream counterfactual analysis.	0
26318	26318	S20-5	Introduction	19	28	0.513513513513513	0.12280701754386	Subtask-2 requires systems to further locate the antecedent and consequent text spans in a given counterfactual statement, as the connection between an antecedent and consequent can reveal core causal inference clues.	0
26319	26319	S20-5	Introduction	20	29	0.540540540540541	0.12719298245614	To build the dataset for counterfactual recognition, we extract over 60,000 candidate counterfactual statements by scanning through news reports in three domains: finance, politics, and healthcare.	0
26320	26320	S20-5	Introduction	21	30	0.567567567567568	0.131578947368421	The first round of annotation focuses on labeling each sample as true or false, where true denotes a sample is counterfactual and false otherwise in Subtask-1.	0
26321	26321	S20-5	Introduction	22	31	0.594594594594595	0.135964912280702	A portion of samples labeled as true will be further used in Subtask-2 to detect the text spans that describe the antecedent and consequent.	0
26322	26322	S20-5	Introduction	23	32	0.621621621621622	0.140350877192982	Specifically, we carefully select 20,000 high-quality samples from the 60,000 statements and use them in Subtask-1, with 13,000 (65%) as the training set and the rest for testing.	0
26323	26323	S20-5	Introduction	24	33	0.648648648648649	0.144736842105263	The dataset for Subtask-2 contains 5,501 samples, among which we use 3,551 (65%) for training and the rest for testing.	0
26324	26324	S20-5	Introduction	25	34	0.675675675675676	0.149122807017544	To achieve a decent performance in our shared task, we expect the systems should have a certain level of language understanding capacity in both semantics and syntax, together with a certain level of commonsense reasoning ability.	0
26325	26325	S20-5	Introduction	26	35	0.702702702702703	0.153508771929825	In Subtask-1, the top-ranked submissions all use pre-trained neural models, which appear to be an effective way to integrate knowledge learned from large corpus.	0
26326	26326	S20-5	Introduction	27	36	0.72972972972973	0.157894736842105	All of these models use neural networks, which further confirms the effectiveness of distributed representation and subsymbolic approaches for this task.	0
26327	26327	S20-5	Introduction	28	37	0.756756756756757	0.162280701754386	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural networks with symbolic approaches.	0
26328	26328	S20-5	Introduction	29	38	0.783783783783784	0.166666666666667	The first-place model also utilizes data augmentation to further improve system performance.	0
26329	26329	S20-5	Introduction	30	39	0.810810810810811	0.171052631578947	In Subtask-2, top systems take two main approaches: sequence labelling or question answering.	0
26330	26330	S20-5	Introduction	31	40	0.837837837837838	0.175438596491228	Same as systems in Subtask-1, all of them benefit from pre-training.	0
26331	26331	S20-5	Introduction	32	41	0.864864864864865	0.179824561403509	We will provide a more detailed analysis in the system and result section.	0
26332	26332	S20-5	Introduction	33	42	0.891891891891892	0.184210526315789	We built a dataset for this shared task from scratch.	0
26333	26333	S20-5	Introduction	34	43	0.918918918918919	0.18859649122807	Our data, baseline code, and leaderboard can be found at https://competitions.codalab.org/competitions/21691.	0
26334	26334	S20-5	Introduction	35	44	0.945945945945946	0.192982456140351	The data and baseline code are also available at https://zenodo.org/record/3932442.	0
26335	26335	S20-5	Introduction	36	45	0.972972972972973	0.197368421052632	In general, our task here is a relatively basic one in counterfactual analysis in natural language.	0
26336	26336	S20-5	Introduction	37	46	1.0	0.201754385964912	We hope it will intrigue and facilitate further research on counterfactual analysis and can benefit other related downstream tasks.	0
30744	30744	2020.nlptea-1.4	abstract	1	2	0.2	0.016528925619835	This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language.	1
30745	30745	2020.nlptea-1.4	abstract	2	3	0.4	0.024793388429752	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
30746	30746	2020.nlptea-1.4	abstract	3	4	0.6	0.03305785123967	Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs.	0
30747	30747	2020.nlptea-1.4	abstract	4	5	0.8	0.041322314049587	System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level.	0
30748	30748	2020.nlptea-1.4	abstract	5	6	1.0	0.049586776859504	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
31215	31215	W18-3601	Introduction and Task Overview	1	8	0.045454545454546	0.042328042328042	Natural Language Generation (NLG) is attracting growing interest both in the form of end-toend tasks (e.g. data-to-text and text-to-text generation), and as embedded component tasks (e.g. in abstractive summarisation, dialogue-based interaction and question answering).	0
31216	31216	W18-3601	Introduction and Task Overview	2	9	0.090909090909091	0.047619047619048	NLG research has been given a boost by two recent developments: the rapid spread of neural language generation techniques, and the growing availability of multilingual treebanks annotated with Universal Dependencies 1 (UD), to the point 1 http://universaldependencies.org/ where as many as 70 treebanks covering about 50 languages can now be downloaded freely.	0
31217	31217	W18-3601	Introduction and Task Overview	3	10	0.136363636363636	0.052910052910053	2 UD treebanks facilitate the development of applications that work potentially across all languages for which UD treebanks are available in a uniform fashion, which is a big advantage for system developers.	0
31218	31218	W18-3601	Introduction and Task Overview	4	11	0.181818181818182	0.058201058201058	As has already been seen in parsing, UD treebanks are also a good basis for multilingual shared tasks: a method that works for some languages may also work for others.	0
31219	31219	W18-3601	Introduction and Task Overview	5	12	0.227272727272727	0.063492063492064	The SR'18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation.	1
31220	31220	W18-3601	Introduction and Task Overview	6	13	0.272727272727273	0.068783068783069	SR'18 also addresses questions about just how suitable and useful the notion of universal dependencies-which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular-is for NLG.	0
31221	31221	W18-3601	Introduction and Task Overview	7	14	0.318181818181818	0.074074074074074	SR'18 follows the SR'11 pilot surface realisation task for English  which was part of Generation Challenges 2011 (GenChal'11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks.	0
31222	31222	W18-3601	Introduction and Task Overview	8	15	0.363636363636364	0.079365079365079	Outside of the SR tasks, just three 'deep' NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG 3 (Gardent et al., 2017), Se-mEval Task 9 4 (May and Priyadarshi, 2017), and E2E 5 (Novikova et al., 2017).	0
31223	31223	W18-3601	Introduction and Task Overview	9	16	0.409090909090909	0.084656084656085	What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016):	0
31224	31224	W18-3601	Introduction and Task Overview	10	17	0.454545454545455	0.08994708994709	http:// universaldependencies.org/conll17/.	0
31225	31225	W18-3601	Introduction and Task Overview	11	18	0.5	0.095238095238095	3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ Interaction	0
31226	31226	W18-3601	Introduction and Task Overview	12	19	0.545454545454545	0.100529100529101	Lab/E2E/ tasks have only been offered for English.	0
31227	31227	W18-3601	Introduction and Task Overview	13	20	0.590909090909091	0.105820105820106	As in SR'11, the Multilingual Surface Realisation shared task (SR'18) comprises two tracks with different levels of difficulty:	0
31228	31228	W18-3601	Introduction and Task Overview	14	21	0.636363636363636	0.111111111111111	Shallow Track:	0
31229	31229	W18-3601	Introduction and Task Overview	15	22	0.681818181818182	0.116402116402116	This track starts from genuine UD structures in which word order information has been removed and tokens have been lemmatised.	0
31230	31230	W18-3601	Introduction and Task Overview	16	23	0.727272727272727	0.121693121693122	In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations.	0
31231	31231	W18-3601	Introduction and Task Overview	17	24	0.772727272727273	0.126984126984127	The task amounts to determining the word order and inflecting words.	0
31232	31232	W18-3601	Introduction and Task Overview	18	25	0.818181818181818	0.132275132275132	Deep Track:	0
31233	31233	W18-3601	Introduction and Task Overview	19	26	0.863636363636364	0.137566137566138	This track starts from UD structures from which functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological and syntactic information have been removed.	0
31234	31234	W18-3601	Introduction and Task Overview	20	27	0.909090909090909	0.142857142857143	In addition to what is required for the Shallow Track, the task in the Deep Track thus also requires reintroduction of the removed functional words and morphological features.	0
31235	31235	W18-3601	Introduction and Task Overview	21	28	0.954545454545455	0.148148148148148	In the remainder of this paper, we describe the data we used in the two tracks (Section 2), and the evaluation methods we used to evaluate submitted systems (Sections 3.1 and 3.2).	0
31236	31236	W18-3601	Introduction and Task Overview	22	29	1.0	0.153439153439153	We then briefly introduce the participating systems (Section 4), report and discuss evaluation results (Section 5), and conclude with some discussion and a look to the future (Section 6).	0
31517	31517	W19-3203	abstract	1	3	0.111111111111111	0.023076923076923	The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking on a regular basis 1 . Advances in automated data processing and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media.	0
31518	31518	W19-3203	abstract	2	4	0.222222222222222	0.030769230769231	We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users.	0
31519	31519	W19-3203	abstract	3	5	0.333333333333333	0.038461538461539	For the fourth execution of this challenge, we proposed four different tasks.	0
31520	31520	W19-3203	abstract	4	6	0.444444444444444	0.046153846153846	Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not.	1
31521	31521	W19-3203	abstract	5	7	0.555555555555556	0.053846153846154	Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs.	1
31522	31522	W19-3203	abstract	6	8	0.666666666666667	0.061538461538462	Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary.	1
31523	31523	W19-3203	abstract	7	9	0.777777777777778	0.069230769230769	Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one's health, a more general discussion of the health issue, or is an unrelated mention.	1
31524	31524	W19-3203	abstract	8	10	0.888888888888889	0.076923076923077	A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run.	0
31525	31525	W19-3203	abstract	9	11	1.0	0.084615384615385	We summarize here the corpora for this challenge which are freely available at https://competitions.codalab. org/competitions/22521, and present an overview of the methods and the results of the competing systems.	0
32775	32775	2020.sigmorphon-1.1	Task Description	1	44	0.066666666666667	0.119891008174387	The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form.	1
32776	32776	2020.sigmorphon-1.1	Task Description	2	45	0.133333333333333	0.122615803814714	For each language we provide a separate training, development, and test set.	0
32777	32777	2020.sigmorphon-1.1	Task Description	3	46	0.2	0.125340599455041	"More historically, all of these tasks resemble the classic ""wug""-test that Berko (1958) developed to test child and human knowledge of English nominal morphology."	0
32778	32778	2020.sigmorphon-1.1	Task Description	4	47	0.266666666666667	0.128065395095368	Unlike the task from earlier years, this year's task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces previously unseen data.	0
32779	32779	2020.sigmorphon-1.1	Task Description	5	48	0.333333333333333	0.130790190735695	The task starts with the Development Phase, which was an elongated period of time (about two months), during which participants develop a model of morphological inflection.	0
32780	32780	2020.sigmorphon-1.1	Task Description	6	49	0.4	0.133514986376022	In this phase, we provide training and development splits for 45 languages representing the Austronesian, Niger-Congo, Oto-Manguean, Uralic and Indo-European language families.	0
32781	32781	2020.sigmorphon-1.1	Task Description	7	50	0.466666666666667	0.136239782016349	Table 1 provides details on the languages.	0
32782	32782	2020.sigmorphon-1.1	Task Description	8	51	0.533333333333333	0.138964577656676	The Generalization Phase is a short period of time (it started about a week before the Evaluation Phase) during which participants fine-tune their models on new data.	0
32783	32783	2020.sigmorphon-1.1	Task Description	9	52	0.6	0.141689373297003	At the start of the phase, we provide training and development splits for 45 new languages where approximately half are genetically related (belong to the same family) and half are genetically unrelated (are isolates or belong to a different family) to the languages presented in the Development Phase.	0
32784	32784	2020.sigmorphon-1.1	Task Description	10	53	0.666666666666667	0.14441416893733	More specifically, we introduce (surprise) languages from Afro-Asiatic, Algic, Dravidian, Indo-European, Niger-Congo, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan families.	0
32785	32785	2020.sigmorphon-1.1	Task Description	11	54	0.733333333333333	0.147138964577657	See Table 2 for more details.	0
32786	32786	2020.sigmorphon-1.1	Task Description	12	55	0.8	0.149863760217984	Finally, test splits for all 90 languages are released in the Evaluation Phase.	0
32787	32787	2020.sigmorphon-1.1	Task Description	13	56	0.866666666666667	0.152588555858311	During this phase, the models are evaluated on held-out forms.	0
32788	32788	2020.sigmorphon-1.1	Task Description	14	57	0.933333333333333	0.155313351498638	Importantly, the languages from both previous phases are evaluated simultaneously.	0
32789	32789	2020.sigmorphon-1.1	Task Description	15	58	1.0	0.158038147138965	This way, we evaluate the extent to which models (especially those with shared parameters) overfit to the development data: a model based on the morphological patterning of the Indo-European languages may end up with a bias towards suffixing and will struggle to learn prefixing or infixation.	0
38175	38175	K15-2001	Task Definition	1	47	0.1	0.156146179401993	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
38176	38176	K15-2001	Task Definition	2	48	0.2	0.159468438538206	Specifically, given a newswire article as input, a participating system is asked to return a set of discourse relations contained in the text.	1
38177	38177	K15-2001	Task Definition	3	49	0.3	0.162790697674419	A discourse relation, as defined in the PDTB, from which the training data for the shared task is drawn, is a relation taking two abstract objects (events, states, facts, or propositions) as arguments.	1
38178	38178	K15-2001	Task Definition	4	50	0.4	0.166112956810631	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between abstract object units.	0
38179	38179	K15-2001	Task Definition	5	51	0.5	0.169435215946844	In the current version of the PDTB, non-explicit relations are inferred only between adjacent units.	0
38180	38180	K15-2001	Task Definition	6	52	0.6	0.172757475083056	Each discourse relation is labeled with a sense selected from a sense hierarchy, and its arguments are generally in the form of sentences, clauses, or in some rare cases, noun phrases.	0
38181	38181	K15-2001	Task Definition	7	53	0.7	0.176079734219269	To detect a discourse relation, a participating system needs to:	0
38182	38182	K15-2001	Task Definition	8	54	0.8	0.179401993355482	1. Identify the text span of an explicit discourse connective, if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments;	0
38183	38183	K15-2001	Task Definition	9	55	0.9	0.182724252491694	"4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
38184	38184	K15-2001	Task Definition	10	56	1.0	0.186046511627907	3 Data	0
42558	42558	D19-5719	abstract	1	2	0.2	0.010050251256281	This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019.	0
42559	42559	D19-5719	abstract	2	3	0.4	0.015075376884422	The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology).	1
42560	42560	D19-5719	abstract	3	4	0.6	0.020100502512563	The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology.	0
42561	42561	D19-5719	abstract	4	5	0.8	0.025125628140704	The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization.	0
42562	42562	D19-5719	abstract	5	6	1.0	0.030150753768844	We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.	0
42963	42963	D19-6007	Introduction	1	6	0.047619047619048	0.033333333333333	Due to the rise of powerful pre-trained word and sentence representations, automated text processing has come a long way in recent years, with systems that perform even better than humans on some datasets (Rajpurkar et al., 2016a).	0
42964	42964	D19-6007	Introduction	2	7	0.095238095238095	0.038888888888889	However, natural language understanding also involves complex challenges.	0
42965	42965	D19-6007	Introduction	3	8	0.142857142857143	0.044444444444445	One important difference between human and machine text understanding lies in the fact that humans can access commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that are assumed to be common ground.	0
42966	42966	D19-6007	Introduction	4	9	0.19047619047619	0.05	"(1) Max: ""It's 1 pm already, I think we should get lunch."""	0
42967	42967	D19-6007	Introduction	5	10	0.238095238095238	0.055555555555556	"Dustin: ""Let me get my wallet."""	0
42968	42968	D19-6007	Introduction	6	11	0.285714285714286	0.061111111111111	Consider the conversation in Example 1: Max will not be surprised that Dustin needs to get his wallet, since she knows that paying is a part of getting lunch.	0
42969	42969	D19-6007	Introduction	7	12	0.333333333333333	0.066666666666667	Also, she knows that a wallet is needed for paying, so Dustin needs to get a wallet for lunch.	0
42970	42970	D19-6007	Introduction	8	13	0.380952380952381	0.072222222222222	This is part of the commonsense knowledge about getting lunch and should be known by both persons.	0
42971	42971	D19-6007	Introduction	9	14	0.428571428571429	0.077777777777778	For a computer system, inferring such unmentioned facts is a non-trivial challenge.	0
42972	42972	D19-6007	Introduction	10	15	0.476190476190476	0.083333333333333	The workshop on Commonsense Inference in NLP (COIN) is focused on such phenomena, looking at models, data, and evaluation methods for commonsense inference.	0
42973	42973	D19-6007	Introduction	11	16	0.523809523809524	0.088888888888889	This report summarizes the results of the COIN shared tasks, an unofficial extension of the Sem-Eval 2018 shared task 11, Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).	0
42974	42974	D19-6007	Introduction	12	17	0.571428571428571	0.094444444444445	The tasks aim to evaluate the commonsense inference capabilities of text understanding systems in two settings: Commonsense inference in everyday narrations (task 1) and commonsense inference in news texts (task 2).	1
42975	42975	D19-6007	Introduction	13	18	0.619047619047619	0.1	Framed as machine comprehension evaluations, the datasets used for both tasks contain challenging reading comprehension questions asking for facts that are not explicitly mentioned in the given reading texts.	0
42976	42976	D19-6007	Introduction	14	19	0.666666666666667	0.105555555555556	Several teams participated in the shared tasks and submitted system description papers.	0
42977	42977	D19-6007	Introduction	15	20	0.714285714285714	0.111111111111111	All systems are based on Transformer architectures (Vaswani et al., 2017), some of them explicitly incorporating commonsense knowledge resources, whereas others only use pretraining on other machine comprehension data sets.	0
42978	42978	D19-6007	Introduction	16	21	0.761904761904762	0.116666666666667	The best submitted system achieves 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
42979	42979	D19-6007	Introduction	17	22	0.80952380952381	0.122222222222222	Still, there are cases that remain elusive: Humans outperform this system by a margin of 7% (task 1) and 8% (task 2).	0
42980	42980	D19-6007	Introduction	18	23	0.857142857142857	0.127777777777778	Our results indicate that while Transformer models are able to perform extremely well on the data used in our shared task, there are still some remaining cases demonstrating that human level is not achieved yet.	0
42981	42981	D19-6007	Introduction	19	24	0.904761904761905	0.133333333333333	Still, we believe that our results also imply the need for more challenging data sets.	0
42982	42982	D19-6007	Introduction	20	25	0.952380952380952	0.138888888888889	In particular, we need data sets that make it harder to benefit from redundancy in the training data or large-scale pretraining on similar domains.	0
42983	42983	D19-6007	Introduction	21	26	1.0	0.144444444444444	In the following, we briefly describe the data sets ( ¬ß2), baselines and evaluation metrics of the shared tasks ( ¬ß3) and we present a summary of the participating systems ( ¬ß4), their results ( ¬ß5) as well as a discussion thereof ( ¬ß6).	0
45242	45242	I17-4002	Task Description	1	25	0.052631578947369	0.328947368421053	This task seeks to evaluate the capability of systems for predicting dimensional sentiments of Chinese words and phrases.	0
45243	45243	I17-4002	Task Description	2	26	0.105263157894737	0.342105263157895	For a given word or phrase, participants were asked to provide a realvalued score from 1 to 9 for both the valence and arousal dimensions, respectively indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal.	1
45244	45244	I17-4002	Task Description	3	27	0.157894736842105	0.355263157894737	"The input format is ""term_id, term"", and the output format is ""term_id, valence_rating, arousal_rating""."	0
45245	45245	I17-4002	Task Description	4	28	0.210526315789474	0.368421052631579	"Below are the input/output formats of the example words ""Â•Ω"" (good), ""ÈùûÂ∏∏Â•Ω"" (very good), ""ÊªøÊÑè"" (satisfy) and ""‰∏çÊªøÊÑè"" (not satisfy)."	0
45246	45246	I17-4002	Task Description	5	29	0.263157894736842	0.381578947368421	with valence-arousal ratings.	0
45247	45247	I17-4002	Task Description	6	30	0.31578947368421	0.394736842105263	For multi-word phrases, we first selected a set of modifiers such as negators (e.g., not), degree adverbs (e.g., very) and modals (e.g., would).	0
45248	45248	I17-4002	Task Description	7	31	0.368421052631579	0.407894736842105	These modifiers were combined with the affective words in CVAW to form multi-word phrases.	0
45249	45249	I17-4002	Task Description	8	32	0.421052631578947	0.421052631578947	The frequency of each phrase was then retrieved from a large web-based corpus.	0
45250	45250	I17-4002	Task Description	9	33	0.473684210526316	0.43421052631579	Only phrases with a frequency greater than or equal to 3 were retained as candidates.	0
45251	45251	I17-4002	Task Description	10	34	0.526315789473684	0.447368421052632	To avoid several modifiers dominating the whole dataset, each modifier (or modifier combination) can have at most 50 phrases.	0
45252	45252	I17-4002	Task Description	11	35	0.578947368421053	0.460526315789474	In addition, the phrases were selected to maximize the balance between positive and negative words.	0
45253	45253	I17-4002	Task Description	12	36	0.631578947368421	0.473684210526316	Finally, a total of 3,000 phrases were collected by excluding unusual and semantically incomplete candidate phrases, of which 2,250 phrases were randomly selected as the training set according to the proportions of each modifier (or modifier combination) in the original set, and the remaining 750 phrases were used as the test set.	0
45254	45254	I17-4002	Task Description	13	37	0.68421052631579	0.486842105263158	Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words .	0
45255	45255	I17-4002	Task Description	14	38	0.736842105263158	0.5	Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth.	0
45256	45256	I17-4002	Task Description	15	39	0.789473684210526	0.513157894736842	Each multi-word phrase was rated by at least 10 different annotators.	0
45257	45257	I17-4002	Task Description	16	40	0.842105263157895	0.526315789473684	Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations.	0
45258	45258	I17-4002	Task Description	17	41	0.894736842105263	0.539473684210526	They were then excluded from the calculation of the average ratings for each phrase.	0
45259	45259	I17-4002	Task Description	18	42	0.947368421052632	0.552631578947368	The policy of this shared task was implemented as is an open test.	0
45260	45260	I17-4002	Task Description	19	43	1.0	0.56578947368421	That is, in addition to the above official datasets, participating teams were allowed to use other publicly available data for system development, but such sources should be specified in the final technical report.	0
46116	46116	W11-1802	abstract	1	2	0.5	0.01980198019802	The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.	1
46117	46117	W11-1802	abstract	2	3	0.75	0.02970297029703	As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.	0
46118	46118	W11-1802	abstract	3	4	1.0	0.03960396039604	After a 3-month system development period, 15 teams submitted their performance results on test cases.	0
46119	46119	W11-1802	abstract	4	5	0.052631578947369	0.04950495049505	The results show the community has made a significant advancement in terms of both performance improvement and generalization.	0
