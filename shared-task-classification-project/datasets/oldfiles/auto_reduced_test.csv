	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
0	0	S01-basque	title	1	1	1.0	0.008130081300813	The Basque task: did systems perform in the upperbound?	0
1	1	S01-basque	abstract	1	2	0.142857142857143	0.016260162601626	In this paper we describe the Senseval 2 Basque lexical-sample task.	0
2	2	S01-basque	abstract	2	3	0.285714285714286	0.024390243902439	The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary.	0
3	3	S01-basque	abstract	3	4	0.428571428571429	0.032520325203252	Most examples were taken from the Egunkaria newspaper.	0
4	4	S01-basque	abstract	4	5	0.571428571428571	0.040650406504065	The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration.	0
5	5	S01-basque	abstract	5	6	0.714285714285714	0.048780487804878	The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage.	0
6	6	S01-basque	abstract	6	7	0.857142857142857	0.056910569105691	The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems.	0
7	7	S01-basque	abstract	7	8	1.0	0.065040650406504	In particular, we argue that inter-tagger agreement is not a real upperbound for the B,asque WSD task.	0
8	8	S01-basque	Introduction	1	9	0.2	0.073170731707317	This paper reviews the design of the lexicalsample task for Basque.	0
9	9	S01-basque	Introduction	2	10	0.4	0.08130081300813	The following steps were taken in order to build the hand-tagged corpus: 1. set the exercise a. choose sense inventory b. choose target corpus c. choose target words d. select examples from the corpus 2. hand-tagging a. define procedure b. tag c. analysis of inter-tagger agreement d. arbitration	0
10	10	S01-basque	Introduction	3	11	0.6	0.089430894308943	The following section presents the setting of the exercise.	0
11	11	S01-basque	Introduction	4	12	0.8	0.097560975609756	Section 3 reviews the hand-tagging, and section 4 the results of the participant systems.	0
12	12	S01-basque	Introduction	5	13	1.0	0.105691056910569	Section 5 discusses the design of the task, as well 9 as the results, and section 6 presents some future work.	0
2093	2093	S07-2	title	1	1	1.0	0.006134969325153	Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems	1
2094	2094	S07-2	abstract	1	2	0.25	0.012269938650307	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems.	0
2095	2095	S07-2	abstract	2	3	0.5	0.01840490797546	In total there were 6 participating systems.	0
2096	2096	S07-2	abstract	3	4	0.75	0.024539877300614	We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).	0
2097	2097	S07-2	abstract	4	5	1.0	0.030674846625767	We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
2098	2098	S07-2	Introduction	1	6	0.052631578947369	0.03680981595092	Word Sense Disambiguation (WSD) is a key enabling-technology.	0
2099	2099	S07-2	Introduction	2	7	0.105263157894737	0.042944785276074	Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.	0
2100	2100	S07-2	Introduction	3	8	0.157894736842105	0.049079754601227	Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).	0
2101	2101	S07-2	Introduction	4	9	0.210526315789474	0.05521472392638	In theory, larger amounts of training data (SemCor has approx.	0
2102	2102	S07-2	Introduction	5	10	0.263157894736842	0.061349693251534	500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.	0
2103	2103	S07-2	Introduction	6	11	0.31578947368421	0.067484662576687	Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart√≠nez and Agirre, 2000;	0
2104	2104	S07-2	Introduction	7	12	0.368421052631579	0.073619631901841	Koeling et al., 2005).	0
2105	2105	S07-2	Introduction	8	13	0.421052631578947	0.079754601226994	"Supervised WSD is based on the ""fixed-list of senses"" paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon."	0
2106	2106	S07-2	Introduction	9	14	0.473684210526316	0.085889570552147	Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).	0
2107	2107	S07-2	Introduction	10	15	0.526315789473684	0.092024539877301	Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.	1
2108	2108	S07-2	Introduction	11	16	0.578947368421053	0.098159509202454	Typical WSID systems involve clustering techniques, which group together similar examples.	0
2109	2109	S07-2	Introduction	12	17	0.631578947368421	0.104294478527607	Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.	0
2110	2110	S07-2	Introduction	13	18	0.68421052631579	0.110429447852761	One of the problems of unsupervised systems is that of managing to do a fair evaluation.	0
2111	2111	S07-2	Introduction	14	19	0.736842105263158	0.116564417177914	Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.	0
2112	2112	S07-2	Introduction	15	20	0.789473684210526	0.122699386503067	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.	0
2113	2113	S07-2	Introduction	16	21	0.842105263157895	0.128834355828221	The paper is organized as follows.	0
2114	2114	S07-2	Introduction	17	22	0.894736842105263	0.134969325153374	Section 2 presents the evaluation framework used in this task.	0
2115	2115	S07-2	Introduction	18	23	0.947368421052632	0.141104294478528	Section 3 presents the systems that participated in the task, and the official results.	0
2116	2116	S07-2	Introduction	19	24	1.0	0.147239263803681	Finally, Section 5 draws the conclusions.	0
2247	2247	S07-2	Conclusions	1	155	0.111111111111111	0.950920245398773	We have presented the design and results of the SemEval-2007 task 02 on evaluating word sense induction and discrimination systems.	0
2248	2248	S07-2	Conclusions	2	156	0.222222222222222	0.957055214723926	6 systems participated, but one of them was not a sense induction system.	0
2249	2249	S07-2	Conclusions	3	157	0.333333333333333	0.96319018404908	We reused the data from the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the training part of the dataset for mapping).	0
2250	2250	S07-2	Conclusions	4	158	0.444444444444444	0.969325153374233	We also provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
2251	2251	S07-2	Conclusions	5	159	0.555555555555556	0.975460122699387	Evaluating clustering solutions is not straightforward.	0
2252	2252	S07-2	Conclusions	6	160	0.666666666666667	0.98159509202454	The unsupervised evaluation seems to be sensitive to the number of senses in the gold standard, and the coarse grained sense inventory used in the gold standard had a great impact in the results.	0
2253	2253	S07-2	Conclusions	7	161	0.777777777777778	0.987730061349693	The supervised evaluation introduces a mapping step which interacts with the clustering solution.	0
2254	2254	S07-2	Conclusions	8	162	0.888888888888889	0.993865030674847	In fact, the ranking of the participating systems 3 All systems in the case of a random train/test split varies according to the evaluation method used.	0
2255	2255	S07-2	Conclusions	9	163	1.0	1.0	We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systems needs further developments, perhaps linked to a certain application or purpose.	0
2815	2815	S07-9	title	1	1	1.0	0.007692307692308	SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish	0
2816	2816	S07-9	abstract	1	2	0.333333333333333	0.015384615384615	In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish).	0
2817	2817	S07-9	abstract	2	3	0.666666666666667	0.023076923076923	In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish.	1
2818	2818	S07-9	abstract	3	4	1.0	0.030769230769231	Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.	0
2819	2819	S07-9	Introduction	1	5	0.071428571428572	0.038461538461539	The Multilevel Semantic Annotation of Catalan and Spanish task is split into the following three subtasks:	0
2820	2820	S07-9	Introduction	2	6	0.142857142857143	0.046153846153846	Noun Sense Disambiguation (NSD):	0
2821	2821	S07-9	Introduction	3	7	0.214285714285714	0.053846153846154	"Disambiguation of all frequent nouns (""all words"" style)."	0
2822	2822	S07-9	Introduction	4	8	0.285714285714286	0.061538461538462	Named Entity Recognition (NER):	0
2823	2823	S07-9	Introduction	5	9	0.357142857142857	0.069230769230769	The annotation of (possibly embedding) named entities with basic entity types.	0
2824	2824	S07-9	Introduction	6	10	0.428571428571429	0.076923076923077	Semantic Role Labeling (SRL):	0
2825	2825	S07-9	Introduction	7	11	0.5	0.084615384615385	Including also two subtasks, i.e., the annotation of verbal predicates with semantic roles (SR), and verb tagging with semantic-class labels (SC).	0
2826	2826	S07-9	Introduction	8	12	0.571428571428571	0.092307692307692	All semantic annotation tasks are performed on exactly the same corpora for each language.	0
2827	2827	S07-9	Introduction	9	13	0.642857142857143	0.1	We presented all the annotation levels together as a complex global task, since we were interested in approaches which address these problems jointly, possibly taking into account cross-dependencies among them.	0
2828	2828	S07-9	Introduction	10	14	0.714285714285714	0.107692307692308	However, we were also accepting systems approaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of the languages.	0
2829	2829	S07-9	Introduction	11	15	0.785714285714286	0.115384615384615	In Section 2 we describe the methodology followed to develop the linguistic corpora for the task.	0
2830	2830	S07-9	Introduction	12	16	0.857142857142857	0.123076923076923	Sections 3 and 4 summarize the task setting and the participant systems, respectively.	0
2831	2831	S07-9	Introduction	13	17	0.928571428571429	0.130769230769231	Finally, Section 5 presents a comparative analysis of the results.	0
2832	2832	S07-9	Introduction	14	18	1.0	0.138461538461538	For any additional information on corpora, resources, formats, tagsets, annotation manuals, etc. we refer the reader to the official website of the task 1 .	0
5272	5272	S10-12	title	1	1	1.0	0.006849315068493	SemEval-2010 Task 12: Parser Evaluation using Textual Entailments	0
5273	5273	S10-12	abstract	1	2	0.333333333333333	0.013698630136986	Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.	0
5274	5274	S10-12	abstract	2	3	0.666666666666667	0.02054794520548	The task involves recognizing textual entailments based on syntactic information alone.	1
5275	5275	S10-12	abstract	3	4	1.0	0.027397260273973	PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.	0
5276	5276	S10-12	Introduction	1	5	0.016393442622951	0.034246575342466	Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.	1
5277	5277	S10-12	Introduction	2	6	0.032786885245902	0.041095890410959	"Given two text fragments called ""text"" and ""hypothesis"", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text."	0
5278	5278	S10-12	Introduction	3	7	0.049180327868853	0.047945205479452	In contrast with general RTE tasks (Dagan et al., 2009) the PETE task focuses on syntactic entailments:	0
5279	5279	S10-12	Introduction	4	8	0.065573770491803	0.054794520547945	Text:	0
5280	5280	S10-12	Introduction	5	9	0.081967213114754	0.061643835616438	The man with the hat was tired.	0
5281	5281	S10-12	Introduction	6	10	0.098360655737705	0.068493150684932	Hypothesis-1: The man was tired.	0
5282	5282	S10-12	Introduction	7	11	0.114754098360656	0.075342465753425	(yes) Hypothesis-2: The hat was tired.	0
5283	5283	S10-12	Introduction	8	12	0.131147540983607	0.082191780821918	(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).	0
5284	5284	S10-12	Introduction	9	13	0.147540983606557	0.089041095890411	We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
5285	5285	S10-12	Introduction	10	14	0.163934426229508	0.095890410958904	The PARSEVAL measures introduced nearly two decades ago (Black et al., 1991) still dominate the field of parser evaluation.	0
5286	5286	S10-12	Introduction	11	15	0.180327868852459	0.102739726027397	"These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or ""treebank""."	0
5287	5287	S10-12	Introduction	12	16	0.19672131147541	0.10958904109589	Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.	0
5288	5288	S10-12	Introduction	13	17	0.213114754098361	0.116438356164384	Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.	0
5289	5289	S10-12	Introduction	14	18	0.229508196721311	0.123287671232877	Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.	0
5290	5290	S10-12	Introduction	15	19	0.245901639344262	0.13013698630137	The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.	0
5291	5291	S10-12	Introduction	16	20	0.262295081967213	0.136986301369863	In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank (Marcus et al., 1994), 5646 (15%) have multiple conflicting annotations.	0
5292	5292	S10-12	Introduction	17	21	0.278688524590164	0.143835616438356	If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90% (Charniak and Johnson, 2005).	0
5293	5293	S10-12	Introduction	18	22	0.295081967213115	0.150684931506849	Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.	0
5294	5294	S10-12	Introduction	19	23	0.311475409836066	0.157534246575342	Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.	0
5295	5295	S10-12	Introduction	20	24	0.327868852459016	0.164383561643836	Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.	0
5296	5296	S10-12	Introduction	21	25	0.344262295081967	0.171232876712329	Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (Bonnema et al., 1997).	0
5297	5297	S10-12	Introduction	22	26	0.360655737704918	0.178082191780822	In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.	0
5298	5298	S10-12	Introduction	23	27	0.377049180327869	0.184931506849315	Framework independence: Entailment recognition is a formalism independent task.	0
5299	5299	S10-12	Introduction	24	28	0.39344262295082	0.191780821917808	A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (Nivre et al., 2007a;	0
5300	5300	S10-12	Introduction	25	29	0.40983606557377	0.198630136986301	Hockenmaier and Steedman, 2007).	0
5301	5301	S10-12	Introduction	26	30	0.426229508196721	0.205479452054794	The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.	0
5302	5302	S10-12	Introduction	27	31	0.442622950819672	0.212328767123288	In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.	0
5303	5303	S10-12	Introduction	28	32	0.459016393442623	0.219178082191781	Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.	0
5304	5304	S10-12	Introduction	29	33	0.475409836065574	0.226027397260274	PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.	0
5305	5305	S10-12	Introduction	30	34	0.491803278688525	0.232876712328767	These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals).	0
5306	5306	S10-12	Introduction	31	35	0.508196721311475	0.23972602739726	Each use a set of binary relations between words in a sentence as the primary unit of representation.	0
5307	5307	S10-12	Introduction	32	36	0.524590163934426	0.246575342465753	They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.	0
5308	5308	S10-12	Introduction	33	37	0.540983606557377	0.253424657534247	Here is an example sentence and its SD representation (De Marneffe and Manning, 2008):	0
5309	5309	S10-12	Introduction	34	38	0.557377049180328	0.26027397260274	Bell, based in Los Angeles, makes and distributes electronic, computer and building products.	0
5310	5310	S10-12	Introduction	35	39	0.573770491803279	0.267123287671233	nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod (products-16, electronic-11) conj-and(electronic-11, computer-13) amod (products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.	0
5311	5311	S10-12	Introduction	36	40	0.590163934426229	0.273972602739726	Bell makes something.	0
5312	5312	S10-12	Introduction	37	41	0.60655737704918	0.280821917808219	Bell distributes something.	0
5313	5313	S10-12	Introduction	38	42	0.622950819672131	0.287671232876712	Someone is based in Los Angeles.	0
5314	5314	S10-12	Introduction	39	43	0.639344262295082	0.294520547945205	Someone makes products.	0
5315	5315	S10-12	Introduction	40	44	0.655737704918033	0.301369863013699	PETE has some advantages over representations based on grammatical relations.	0
5316	5316	S10-12	Introduction	41	45	0.672131147540984	0.308219178082192	For example SD defines 55 relations organized in a hierarchy, and it may be non-trivial for a non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).	0
5317	5317	S10-12	Introduction	42	46	0.688524590163934	0.315068493150685	In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.	0
5318	5318	S10-12	Introduction	43	47	0.704918032786885	0.321917808219178	In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.	0
5319	5319	S10-12	Introduction	44	48	0.721311475409836	0.328767123287671	However there are also significant challenges associated with an evaluation scheme like PETE.	0
5320	5320	S10-12	Introduction	45	49	0.737704918032787	0.335616438356164	It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.	0
5321	5321	S10-12	Introduction	46	50	0.754098360655738	0.342465753424657	Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.	0
5322	5322	S10-12	Introduction	47	51	0.770491803278688	0.349315068493151	Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.	0
5323	5323	S10-12	Introduction	48	52	0.786885245901639	0.356164383561644	"Introducing dummy words like ""someone"" or ""something"" alleviates part of the problem but does not help in the case of clausal complements."	0
5324	5324	S10-12	Introduction	49	53	0.80327868852459	0.363013698630137	In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.	0
5325	5325	S10-12	Introduction	50	54	0.819672131147541	0.36986301369863	PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.	0
5326	5326	S10-12	Introduction	51	55	0.836065573770492	0.376712328767123	This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.	0
5327	5327	S10-12	Introduction	52	56	0.852459016393443	0.383561643835616	However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.	0
5328	5328	S10-12	Introduction	53	57	0.868852459016393	0.39041095890411	The balance of what is being evaluated is also important.	0
5329	5329	S10-12	Introduction	54	58	0.885245901639344	0.397260273972603	A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.	0
5330	5330	S10-12	Introduction	55	59	0.901639344262295	0.404109589041096	In this evaluation, we focused on sentences and relations where state of the art parsers disagree.	0
5331	5331	S10-12	Introduction	56	60	0.918032786885246	0.410958904109589	We hope this methodology will uncover weaknesses that the next generation systems can focus on.	0
5332	5332	S10-12	Introduction	57	61	0.934426229508197	0.417808219178082	The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.	0
5333	5333	S10-12	Introduction	58	62	0.950819672131147	0.424657534246575	Section 2 explains the method followed to create the PETE dataset.	0
5334	5334	S10-12	Introduction	59	63	0.967213114754098	0.431506849315069	Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.	0
5335	5335	S10-12	Introduction	60	64	0.983606557377049	0.438356164383562	Section 4 presents the participating systems, their methods and results.	0
5336	5336	S10-12	Introduction	61	65	1.0	0.445205479452055	Section 5 summarizes our contribution.	0
5809	5809	S10-18	title	1	1	1.0	0.006289308176101	SemEval-2010 Task 18: Disambiguating Sentiment Ambiguous Adjectives	0
5810	5810	S10-18	abstract	1	2	0.25	0.012578616352201	Sentiment ambiguous adjectives cause major difficulties for existing algorithms of sentiment analysis.	0
5811	5811	S10-18	abstract	2	3	0.5	0.018867924528302	We present an evaluation task designed to provide a framework for comparing different approaches in this problem.	0
5812	5812	S10-18	abstract	3	4	0.75	0.025157232704403	We define the task, describe the data creation, list the participating systems and discuss their results.	0
5813	5813	S10-18	abstract	4	5	1.0	0.031446540880503	There are 8 teams and 16 systems.	0
5814	5814	S10-18	Introduction	1	6	0.04	0.037735849056604	In recent years, sentiment analysis has attracted considerable attention (Pang and Lee, 2008).	0
5815	5815	S10-18	Introduction	2	7	0.08	0.044025157232704	It is the task of mining positive and negative opinions from natural language, which can be applied to many natural language processing tasks, such as document summarization and question answering.	0
5816	5816	S10-18	Introduction	3	8	0.12	0.050314465408805	Previous work on this problem falls into three groups: opinion mining of documents, sentiment classification of sentences and polarity prediction of words.	0
5817	5817	S10-18	Introduction	4	9	0.16	0.056603773584906	Sentiment analysis both at document and sentence level rely heavily on word level.	0
5818	5818	S10-18	Introduction	5	10	0.2	0.062893081761006	The most frequently explored task at word level is to determine the semantic orientation (SO) of words, in which most work centers on assigning a prior polarity to words or word senses in the lexicon out of context.	0
5819	5819	S10-18	Introduction	6	11	0.24	0.069182389937107	However, for some words, the polarity varies strongly with context, making it hard to attach each to a specific sentiment category in the lexicon.	0
5820	5820	S10-18	Introduction	7	12	0.28	0.075471698113208	"For example, consider "" low cost"" versus "" low salary"" ."	0
5821	5821	S10-18	Introduction	8	13	0.32	0.081761006289308	"The word "" low"" has a positive orientation in the first case but a negative orientation in the second case."	0
5822	5822	S10-18	Introduction	9	14	0.36	0.088050314465409	Turney and Littman (2003) claimed that sentiment ambiguous words could not be avoided easily in a real-world application in the future research.	0
5823	5823	S10-18	Introduction	10	15	0.4	0.09433962264151	But unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (Hatzivassiloglou and McKeown, 1997;	0
5824	5824	S10-18	Introduction	11	16	0.44	0.10062893081761	Turney and Littman, 2003;Kim and Hovy, 2004).	0
5825	5825	S10-18	Introduction	12	17	0.48	0.106918238993711	The exception work is Ding et al. (2008).	0
5826	5826	S10-18	Introduction	13	18	0.52	0.113207547169811	They call these words as context dependant opinions and propose a holistic lexicon-based approach to solve this problem.	0
5827	5827	S10-18	Introduction	14	19	0.56	0.119496855345912	The language they deal with is English.	0
5828	5828	S10-18	Introduction	15	20	0.6	0.125786163522013	The disambiguation of sentiment ambiguous words can also be considered as a problem of phrase-level sentiment analysis.	0
5829	5829	S10-18	Introduction	16	21	0.64	0.132075471698113	Wilson et al. (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features.	0
5830	5830	S10-18	Introduction	17	22	0.68	0.138364779874214	Takamura et al. (2006	0
5831	5831	S10-18	Introduction	18	23	0.72	0.144654088050314	"Takamura et al. ( , 2007 propose latent variable model and lexical network to determine SO of phrases, focusing on "" noun+adjective"" pairs."	0
5832	5832	S10-18	Introduction	19	24	0.76	0.150943396226415	Their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.	0
5833	5833	S10-18	Introduction	20	25	0.8	0.157232704402516	The task 18 at SemEval 2010 provides a benchmark data set to encourage studies on this problem.	0
5834	5834	S10-18	Introduction	21	26	0.84	0.163522012578616	This paper is organized as follows.	0
5835	5835	S10-18	Introduction	22	27	0.88	0.169811320754717	Section 2 defines the task.	0
5836	5836	S10-18	Introduction	23	28	0.92	0.176100628930818	Section 3 describes the data annotation.	0
5837	5837	S10-18	Introduction	24	29	0.96	0.182389937106918	Section 4 gives a brief summary of 16 participating systems.	0
5838	5838	S10-18	Introduction	25	30	1.0	0.188679245283019	Finally Section 5 draws conclusions.	0
5965	5965	S10-18	Conclusion	1	157	0.333333333333333	0.987421383647799	This paper describes task 18 at SemEval-2010, disambiguating sentiment ambiguous adjectives.	0
5966	5966	S10-18	Conclusion	2	158	0.666666666666667	0.993710691823899	The experimental results of the 16 participating systems are promising, and the used approaches are quite novel.	0
5967	5967	S10-18	Conclusion	3	159	1.0	1.0	We encourage further research into this issue, and integration of the disambiguation of sentiment ambiguous adjectives into applications of sentiment analysis.	0
6979	6979	S12-6	title	1	1	1.0	0.005128205128205	SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity	0
6980	6980	S12-6	abstract	1	2	0.125	0.01025641025641	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts.	0
6981	6981	S12-6	abstract	2	3	0.25	0.015384615384615	This paper presents the results of the STS pilot task in Semeval.	0
6982	6982	S12-6	abstract	3	4	0.375	0.020512820512821	The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources.	0
6983	6983	S12-6	abstract	4	5	0.5	0.025641025641026	The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise.	0
6984	6984	S12-6	abstract	5	6	0.625	0.030769230769231	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%.	0
6985	6985	S12-6	abstract	6	7	0.75	0.035897435897436	35 teams participated in the task, submitting 88 runs.	0
6986	6986	S12-6	abstract	7	8	0.875	0.041025641025641	The best results scored a Pearson correlation &gt;80%, well above a simple lexical baseline that only scored a 31% correlation.	0
6987	6987	S12-6	abstract	8	9	1.0	0.046153846153846	This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.	0
6988	6988	S12-6	Introduction	1	10	0.055555555555556	0.051282051282051	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two sentences.	0
6989	6989	S12-6	Introduction	2	11	0.111111111111111	0.056410256410257	STS is related to both Textual Entailment (TE) and Paraphrase (PARA).	0
6990	6990	S12-6	Introduction	3	12	0.166666666666667	0.061538461538462	STS is more directly applicable in a number of NLP tasks than TE and PARA such as Machine Translation and evaluation, Summarization, Machine Reading, Deep Question Answering, etc. STS differs from TE in as much as it assumes symmetric graded equivalence between the pair of textual snippets.	0
6991	6991	S12-6	Introduction	4	13	0.222222222222222	0.066666666666667	In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.	0
6992	6992	S12-6	Introduction	5	14	0.277777777777778	0.071794871794872	Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).	0
6993	6993	S12-6	Introduction	6	15	0.333333333333333	0.076923076923077	STS provides a unified framework that allows for an extrinsic evaluation of multiple semantic components that otherwise have tended to be evaluated independently and without broad characterization of their impact on NLP applications.	0
6994	6994	S12-6	Introduction	7	16	0.388888888888889	0.082051282051282	Such components include word sense disambiguation and induction, lexical substitution, semantic role labeling, multiword expression detection and handling, anaphora and coreference resolution, time and date resolution, named-entity handling, underspecification, hedging, semantic scoping and discourse analysis.	0
6995	6995	S12-6	Introduction	8	17	0.444444444444444	0.087179487179487	Though not in the scope of the current pilot task, we plan to explore building an open source toolkit for integrating and applying diverse linguistic analysis modules to the STS task.	0
6996	6996	S12-6	Introduction	9	18	0.5	0.092307692307692	While the characterization of STS is still preliminary, we observed that there was no comparable existing dataset extensively annotated for pairwise semantic sentence similarity.	0
6997	6997	S12-6	Introduction	10	19	0.555555555555556	0.097435897435898	We approached the construction of the first STS dataset with the following goals: (1)	0
6998	6998	S12-6	Introduction	11	20	0.611111111111111	0.102564102564103	To set a definition of STS as a graded notion which can be easily communicated to non-expert annotators beyond the likert-scale; (2)	0
6999	6999	S12-6	Introduction	12	21	0.666666666666667	0.107692307692308	To gather a substantial amount of sentence pairs from diverse datasets, and to annotate them with high quality; (3)	0
7000	7000	S12-6	Introduction	13	22	0.722222222222222	0.112820512820513	To explore evaluation measures for STS; (4)	0
7001	7001	S12-6	Introduction	14	23	0.777777777777778	0.117948717948718	To explore the relation of STS to PARA and Machine Translation Evaluation exercises.	0
7002	7002	S12-6	Introduction	15	24	0.833333333333333	0.123076923076923	In the next section we present the various sources of the STS data and the annotation procedure used.	0
7003	7003	S12-6	Introduction	16	25	0.888888888888889	0.128205128205128	Section 4 investigates the evaluation of STS systems.	0
7004	7004	S12-6	Introduction	17	26	0.944444444444444	0.133333333333333	Section 5 summarizes the resources and tools used by participant systems.	0
7005	7005	S12-6	Introduction	18	27	1.0	0.138461538461538	Finally, Section 6 draws the conclusions.	0
8118	8118	S13-5	title	1	1	1.0	0.004950495049505	SemEval-2013 Task 5: Evaluating Phrasal Semantics	0
8119	8119	S13-5	abstract	1	2	0.2	0.00990099009901	"This paper describes the SemEval-2013 Task 5: ""Evaluating Phrasal Semantics""."	0
8120	8120	S13-5	abstract	2	3	0.4	0.014851485148515	Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length.	0
8121	8121	S13-5	abstract	3	4	0.6	0.01980198019802	The second one addresses deciding the compositionality of phrases in a given context.	0
8122	8122	S13-5	abstract	4	5	0.8	0.024752475247525	The paper discusses the importance and background of these subtasks and their structure.	0
8123	8123	S13-5	abstract	5	6	1.0	0.02970297029703	In succession, it introduces the systems that participated and discusses evaluation results.	0
8124	8124	S13-5	Introduction	1	7	0.043478260869565	0.034653465346535	Numerous past tasks have focused on leveraging the meaning of word types or words in context.	0
8125	8125	S13-5	Introduction	2	8	0.086956521739131	0.03960396039604	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.	0
8126	8126	S13-5	Introduction	3	9	0.130434782608696	0.044554455445545	As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.	0
8127	8127	S13-5	Introduction	4	10	0.173913043478261	0.04950495049505	In this paper, we present two subtasks designed to evaluate such phrasal models: a. Semantic similarity of words and compositional phrases b.	1
8128	8128	S13-5	Introduction	5	11	0.217391304347826	0.054455445544555	Evaluating the compositionality of phrases in context	1
8129	8129	S13-5	Introduction	6	12	0.260869565217391	0.059405940594059	"For example, the first subtask addresses computing how similar the word ""valuation"" is to the compositional sequence ""price assessment"", while the second subtask addresses deciding whether the phrase ""piece of cake"" is used literally or figuratively in the sentence ""Labour was a piece of cake!""."	0
8130	8130	S13-5	Introduction	7	13	0.304347826086957	0.064356435643564	The aim of these subtasks is two-fold.	0
8131	8131	S13-5	Introduction	8	14	0.347826086956522	0.069306930693069	Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.	0
8132	8132	S13-5	Introduction	9	15	0.391304347826087	0.074257425742574	It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.	0
8133	8133	S13-5	Introduction	10	16	0.434782608695652	0.079207920792079	Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.	0
8134	8134	S13-5	Introduction	11	17	0.478260869565217	0.084158415841584	Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.	0
8135	8135	S13-5	Introduction	12	18	0.521739130434783	0.089108910891089	We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.	0
8136	8136	S13-5	Introduction	13	19	0.565217391304348	0.094059405940594	Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.	0
8137	8137	S13-5	Introduction	14	20	0.608695652173913	0.099009900990099	In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.	0
8138	8138	S13-5	Introduction	15	21	0.652173913043478	0.103960396039604	The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.	0
8139	8139	S13-5	Introduction	16	22	0.695652173913043	0.108910891089109	Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.	0
8140	8140	S13-5	Introduction	17	23	0.739130434782609	0.113861386138614	In subsection 3.1 the subtask is described in detail together with some information about its background.	0
8141	8141	S13-5	Introduction	18	24	0.782608695652174	0.118811881188119	Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.	0
8142	8142	S13-5	Introduction	19	25	0.826086956521739	0.123762376237624	Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.	0
8143	8143	S13-5	Introduction	20	26	0.869565217391304	0.128712871287129	Subsection 4.1 explains the data creation process for this subtask.	0
8144	8144	S13-5	Introduction	21	27	0.91304347826087	0.133663366336634	In subsection 4.2 the evaluation statistics of participating systems are presented.	0
8145	8145	S13-5	Introduction	22	28	0.956521739130435	0.138613861386139	Section 5 is a discussion about the conclusions of the entire task.	0
8146	8146	S13-5	Introduction	23	29	1.0	0.143564356435644	Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.	0
9049	9049	S13-11	title	1	1	1.0	0.00763358778626	SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application	0
9050	9050	S13-11	abstract	1	2	0.333333333333333	0.015267175572519	In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification.	0
9051	9051	S13-11	abstract	2	3	0.666666666666667	0.022900763358779	Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.	1
9052	9052	S13-11	abstract	3	4	1.0	0.030534351145038	The task enables the end-to-end evaluation and comparison of systems.	0
9053	9053	S13-11	Introduction	1	5	0.05	0.038167938931298	Word ambiguity is a pervasive issue in Natural Language Processing.	0
9054	9054	S13-11	Introduction	2	6	0.1	0.045801526717557	Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009;	0
9055	9055	S13-11	Introduction	3	7	0.15	0.053435114503817	Navigli, 2012) for a survey).	0
9056	9056	S13-11	Introduction	4	8	0.2	0.061068702290076	Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications.	0
9057	9057	S13-11	Introduction	5	9	0.25	0.068702290076336	In fact, the performance of WSD systems depends heavily on which sense inventory is chosen.	0
9058	9058	S13-11	Introduction	6	10	0.3	0.076335877862596	For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002;	0
9059	9059	S13-11	Introduction	7	11	0.35	0.083969465648855	Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008).	0
9060	9060	S13-11	Introduction	8	12	0.4	0.091603053435115	On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses.	0
9061	9061	S13-11	Introduction	9	13	0.45	0.099236641221374	In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output.	0
9062	9062	S13-11	Introduction	10	14	0.5	0.106870229007634	Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.	0
9063	9063	S13-11	Introduction	11	15	0.55	0.114503816793893	Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively.	0
9064	9064	S13-11	Introduction	12	16	0.6	0.122137404580153	A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds.	0
9065	9065	S13-11	Introduction	13	17	0.65	0.129770992366412	Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010;	0
9066	9066	S13-11	Introduction	14	18	0.7	0.137404580152672	Di Marco and Navigli, 2013).	0
9067	9067	S13-11	Introduction	15	19	0.75	0.145038167938931	In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.	0
9068	9068	S13-11	Introduction	16	20	0.8	0.152671755725191	The Semeval-2013 task described in this paper 1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems.	0
9069	9069	S13-11	Introduction	17	21	0.85	0.16030534351145	The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre and Soroa, 2007;Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms.	0
9070	9070	S13-11	Introduction	18	22	0.9	0.16793893129771	Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning).	0
9071	9071	S13-11	Introduction	19	23	0.95	0.175572519083969	For instance, consider the target query apple and the following 3 search result snippets:	0
9072	9072	S13-11	Introduction	20	24	1.0	0.183206106870229	1. Apple Inc., formerly Apple Computer, Inc., is...	0
9369	9369	S13-13	title	1	1	1.0	0.005235602094241	SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses	0
9370	9370	S13-13	abstract	1	2	0.25	0.010471204188482	Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense.	0
9371	9371	S13-13	abstract	2	3	0.5	0.015706806282723	However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage.	0
9372	9372	S13-13	abstract	3	4	0.75	0.020942408376963	We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability.	0
9373	9373	S13-13	abstract	4	5	1.0	0.026178010471204	Four teams submitted nine systems, which were evaluated in two settings.	0
9374	9374	S13-13	Introduction	1	6	0.0625	0.031413612565445	Word Sense Disambiguation (WSD) attempts to identify which of a word's meanings applies in a given context.	0
9375	9375	S13-13	Introduction	2	7	0.125	0.036649214659686	A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009).	0
9376	9376	S13-13	Introduction	3	8	0.1875	0.041884816753927	Typically, each usage of a word is treated as expressing only a single sense.	0
9377	9377	S13-13	Introduction	4	9	0.25	0.047120418848168	However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations.	0
9378	9378	S13-13	Introduction	5	10	0.3125	0.052356020942408	Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V√©ronis, 1998;Murray and Green, 2004;	0
9379	9379	S13-13	Introduction	6	11	0.375	0.057591623036649	Passonneau et al., 2012b;Jurgens, 2013;Navigli et al., 2013).	0
9380	9380	S13-13	Introduction	7	12	0.4375	0.06282722513089	Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled.	0
9381	9381	S13-13	Introduction	8	13	0.5	0.068062827225131	Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability.	0
9382	9382	S13-13	Introduction	9	14	0.5625	0.073298429319372	WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD systems for specific types of text (e.g., social media), or the inventory may lack domain-specific senses.	0
9383	9383	S13-13	Introduction	10	15	0.625	0.078534031413613	Word Sense Induction (WSI) has been proposed as a method for overcoming such limitations by learning the senses automatically from text.	0
9384	9384	S13-13	Introduction	11	16	0.6875	0.083769633507853	In essence, a WSI algorithm acts as a lexicographer by grouping word usages according to their shared meaning.	0
9385	9385	S13-13	Introduction	12	17	0.75	0.089005235602094	The second goal of this task is to assess the performance of WSI algorithms when they are able to model multiple meanings of a usage with graded senses.	0
9386	9386	S13-13	Introduction	13	18	0.8125	0.094240837696335	Task 12 focuses on disambiguating senses for 50 target lemmas: 20 nouns, 20 verbs, and 10 adjectives (Sec. 2).	0
9387	9387	S13-13	Introduction	14	19	0.875	0.099476439790576	Since the Task evaluates only unsupervised systems, no training data was provided; however, to enable more comparison, Unsupervised WSD systems were also allowed to participate.	0
9388	9388	S13-13	Introduction	15	20	0.9375	0.104712041884817	Participating systems were evaluated in two settings (Sec. 3), depending on whether they used induced senses or WordNet 3.1 senses for their annotations.	0
9389	9389	S13-13	Introduction	16	21	1.0	0.109947643979058	The results (Sec. 5) demonstrate a substantial improvement over the competitive most frequent sense baseline.	0
9390	9390	S13-13	Task Description	1	22	0.090909090909091	0.115183246073298	This task required participating systems to annotate instances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected due to its fine-grained senses.	1
9391	9391	S13-13	Task Description	2	23	0.181818181818182	0.120418848167539	Participants could label each instance with one or more senses, weighting	1
9392	9392	S13-13	Task Description	3	24	0.272727272727273	0.12565445026178	We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to encompass the thoughts of every entity we know.	0
9393	9393	S13-13	Task Description	4	25	0.363636363636364	0.130890052356021	dark%3:00:01:: -devoid of or deficient in light or brightness; shadowed or black dark%3:00:00:: -secret I ask because my practice has always been to allow about five minutes grace, then remove it.	0
9394	9394	S13-13	Task Description	5	26	0.454545454545455	0.136125654450262	ask%2:32:02:: -direct or put; seek an answer to ask%2:32:04:: -address a question to and expect an answer from Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual ambiguity (bottom).	0
9395	9395	S13-13	Task Description	6	27	0.545454545454545	0.141361256544503	Senses are specified using their WordNet 3.1 sense keys.	0
9396	9396	S13-13	Task Description	7	28	0.636363636363636	0.146596858638743	each by their applicability.	0
9397	9397	S13-13	Task Description	8	29	0.727272727272727	0.151832460732984	Table 1 highlights two example contexts where multiple senses apply.	0
9398	9398	S13-13	Task Description	9	30	0.818181818181818	0.157068062827225	The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.	0
9399	9399	S13-13	Task Description	10	31	0.909090909090909	0.162303664921466	a as being devoid of light and the causal result of being secret.	0
9400	9400	S13-13	Task Description	11	32	1.0	0.167539267015707	"In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations ""I ask [you] because"" (sense ask%2:32:04::) or ""I ask [the question] because"" (sense ask%2:32:02::)."	0
9553	9553	S13-13	Conclusion	1	185	0.142857142857143	0.968586387434555	We have introduced a new evaluation setting for WSI and WSD systems where systems are measured by their ability to detect and weight multiple applicable senses for a single context.	0
9554	9554	S13-13	Conclusion	2	186	0.285714285714286	0.973821989528796	Four teams submitted nine systems, annotating a total of 4664 contexts for 50 words from the OANC.	0
9555	9555	S13-13	Conclusion	3	187	0.428571428571429	0.979057591623037	Many systems were able to surpass the competitive MFS baseline.	0
9556	9556	S13-13	Conclusion	4	188	0.571428571428571	0.984293193717278	Furthermore, when WSI systems were trained to produce only a single sense label, the performance of resulting semi-supervised WSD systems surpassed that of many supervised systems in previous WSD evaluations.	0
9557	9557	S13-13	Conclusion	5	189	0.714285714285714	0.989528795811518	Future work may assess the impact of graded sense annotations in a task-based setting.	0
9558	9558	S13-13	Conclusion	6	190	0.857142857142857	0.994764397905759	All materials have been released on the task website.	0
9559	9559	S13-13	Conclusion	7	191	1.0	1.0	1	0
10486	10486	S14-6	title	1	1	1.0	0.005050505050505	SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands	0
10487	10487	S14-6	abstract	1	2	0.166666666666667	0.01010101010101	SemEval-2014	0
10488	10488	S14-6	abstract	2	3	0.333333333333333	0.015151515151515	Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches.	0
10489	10489	S14-6	abstract	3	4	0.5	0.02020202020202	The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm.	1
10490	10490	S14-6	abstract	4	5	0.666666666666667	0.025252525252525	Six teams submitted systems using both rule-based and statistical methods.	0
10491	10491	S14-6	abstract	5	6	0.833333333333333	0.03030303030303	The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context.	0
10492	10492	S14-6	abstract	6	7	1.0	0.035353535353535	However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.	0
10493	10493	S14-6	Introduction	1	8	0.043478260869565	0.04040404040404	Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language.	0
10494	10494	S14-6	Introduction	2	9	0.086956521739131	0.045454545454546	Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013;	0
10495	10495	S14-6	Introduction	3	10	0.130434782608696	0.050505050505051	Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011;	0
10496	10496	S14-6	Introduction	4	11	0.173913043478261	0.055555555555556	Kim and Mooney, 2012).	0
10497	10497	S14-6	Introduction	5	12	0.217391304347826	0.060606060606061	Different parsers can be distinguished by the level of supervision they require during training.	0
10498	10498	S14-6	Introduction	6	13	0.260869565217391	0.065656565656566	Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form.	0
10499	10499	S14-6	Introduction	7	14	0.304347826086957	0.070707070707071	However, because annotated data is often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information.	0
10500	10500	S14-6	Introduction	8	15	0.347826086956522	0.075757575757576	For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system.	0
10501	10501	S14-6	Introduction	9	16	0.391304347826087	0.080808080808081	In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction.	0
10502	10502	S14-6	Introduction	10	17	0.434782608695652	0.085858585858586	In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing.	0
10503	10503	S14-6	Introduction	11	18	0.478260869565217	0.090909090909091	The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board.	0
10504	10504	S14-6	Introduction	12	19	0.521739130434783	0.095959595959596	Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context.	0
10505	10505	S14-6	Introduction	13	20	0.565217391304348	0.101010101010101	For example, the command in Figure 1 may have several plausible interpretations, given different board configurations.	0
10506	10506	S14-6	Introduction	14	21	0.608695652173913	0.106060606060606	'	0
10507	10507	S14-6	Introduction	15	22	0.652173913043478	0.111111111111111	Move the pyramid on the blue cube on the gray one.'	0
10508	10508	S14-6	Introduction	16	23	0.695652173913043	0.116161616161616	The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009;	0
10509	10509	S14-6	Introduction	17	24	0.739130434782609	0.121212121212121	Mitkov, 1999).	0
10510	10510	S14-6	Introduction	18	25	0.782608695652174	0.126262626262626	More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation.	0
10511	10511	S14-6	Introduction	19	26	0.826086956521739	0.131313131313131	Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph.	0
10512	10512	S14-6	Introduction	20	27	0.869565217391304	0.136363636363636	Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands.	0
10513	10513	S14-6	Introduction	21	28	0.91304347826087	0.141414141414141	In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research.	0
10514	10514	S14-6	Introduction	22	29	0.956521739130435	0.146464646464646	In the remainder of this paper, we describe the task, the dataset and the metrics used for evaluation.	0
10515	10515	S14-6	Introduction	23	30	1.0	0.151515151515152	We then compare the approaches used by participant systems and conclude with suggested improvements for future work.	0
10516	10516	S14-6	Task Description	1	31	0.166666666666667	0.156565656565657	The long term research goal encouraged by the task is to develop a system that will robustly execute NL robotic commands.	0
10517	10517	S14-6	Task Description	2	32	0.333333333333333	0.161616161616162	In general, this is a highly complex problem involving computational processing of language, spatial reasoning, contextual awareness and knowledge representation.	0
10518	10518	S14-6	Task Description	3	33	0.5	0.166666666666667	To simplify the problem, participants were provided with additional tools and resources, allowing them to focus on developing a semantic parser for a fixed domain that would fit into an existing component architecture.	0
10519	10519	S14-6	Task Description	4	34	0.666666666666667	0.171717171717172	Figure 2 shows how these components interact.	0
10520	10520	S14-6	Task Description	5	35	0.833333333333333	0.176767676767677	Semantic parser: Systems submitted by participants are semantic parsers that accept an NL command as input, mapping this to a formal Robot Control Language (RCL), described further in section 3.3.	0
10521	10521	S14-6	Task Description	6	36	1.0	0.181818181818182	The Robot Commands Treebank used for the both training and evaluation is an annotated corpus that pairs NL commands with contextual RCL statements.	0
10671	10671	S14-6	Conclusion and Future Work	1	186	0.076923076923077	0.939393939393939	This paper described a new task for SemEval: Supervised Semantic Parsing of Robotic Spatial Commands.	0
10672	10672	S14-6	Conclusion and Future Work	2	187	0.153846153846154	0.944444444444444	Despite its novel nature, the task attracted high-quality submissions from six teams, using a variety of semantic parsing strategies.	0
10673	10673	S14-6	Conclusion and Future Work	3	188	0.230769230769231	0.94949494949495	It is hoped that this task will reappear at Se-mEval.	0
10674	10674	S14-6	Conclusion and Future Work	4	189	0.307692307692308	0.954545454545455	Several lessons were learnt from this first version of the shared task which can be used to improve the task in future.	0
10675	10675	S14-6	Conclusion and Future Work	5	190	0.384615384615385	0.95959595959596	One issue which several participants noted was the way in which the treebank was split into training and evaluation datasets.	0
10676	10676	S14-6	Conclusion and Future Work	6	191	0.461538461538462	0.964646464646465	Out of the 3,409 sentences in the treebank, the first 2,500 sequential sentences were chosen for training.	0
10677	10677	S14-6	Conclusion and Future Work	7	192	0.538461538461538	0.96969696969697	Because this data was not randomized, certain syntactic structures were only found during evaluation and were not present in the training data.	0
10678	10678	S14-6	Conclusion and Future Work	8	193	0.615384615384615	0.974747474747475	Although this may have affected results, all participants evaluated their systems against the same datasets.	0
10679	10679	S14-6	Conclusion and Future Work	9	194	0.692307692307692	0.97979797979798	Based on participant feedback, in addition to reporting P and NP-measures, it would also be illuminating to include a metric such as Parseval F1-scores to measure partial accuracy.	0
10680	10680	S14-6	Conclusion and Future Work	10	195	0.769230769230769	0.984848484848485	An improved version of the task could also feature a better dataset by expanding the treebank, not only in terms of size but also in terms of linguistic structure.	0
10681	10681	S14-6	Conclusion and Future Work	11	196	0.846153846153846	0.98989898989899	Many commands captured in the annotation game are not yet represented in RCL due to linguistic phenomena such as negation and conditional statements.	0
10682	10682	S14-6	Conclusion and Future Work	12	197	0.923076923076923	0.994949494949495	Looking forward, a more promising approach to improving the spatial planner could be probabilistic planning, so that semantic parsers could interface with probabilistic facts with confidence measures.	0
10683	10683	S14-6	Conclusion and Future Work	13	198	1.0	1.0	This approach is particularly suitable for robotics, where sensors often supply noisy signals about the robot's environment.	0
11010	11010	S14-9	title	1	1	1.0	0.007042253521127	SemEval-2014 Task 9: Sentiment Analysis in Twitter	0
11011	11011	S14-9	abstract	1	2	0.166666666666667	0.014084507042254	We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014.	0
11012	11012	S14-9	abstract	2	3	0.333333333333333	0.02112676056338	It is a continuation of the last year's task that ran successfully as part of SemEval-2013.	0
11013	11013	S14-9	abstract	3	4	0.5	0.028169014084507	As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams).	0
11014	11014	S14-9	abstract	4	5	0.666666666666667	0.035211267605634	This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii) LiveJournal sentences.	0
11015	11015	S14-9	abstract	5	6	0.833333333333333	0.042253521126761	We further tested on (iv) 2013 tweets, and (v) 2013 SMS messages.	0
11016	11016	S14-9	abstract	6	7	1.0	0.049295774647887	The highest F1score on (i) was achieved by NRC-Canada at 86.63 for subtask A and by TeamX at 70.96 for subtask B.	0
11017	11017	S14-9	Introduction	1	8	0.038461538461539	0.056338028169014	In the past decade, new forms of communication have emerged and have become ubiquitous through social media.	0
11018	11018	S14-9	Introduction	2	9	0.076923076923077	0.063380281690141	Microblogs (e.g., Twitter), Weblogs (e.g., LiveJournal) and cell phone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.	0
11019	11019	S14-9	Introduction	3	10	0.115384615384615	0.070422535211268	Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.	0
11020	11020	S14-9	Introduction	4	11	0.153846153846154	0.077464788732394	The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, e.g., RT for re-tweet and #hashtags 1 .	0
11021	11021	S14-9	Introduction	5	12	0.192307692307692	0.084507042253521	This work is licensed under a Creative Commons Attribution 4.0 International Licence.	0
11022	11022	S14-9	Introduction	6	13	0.230769230769231	0.091549295774648	Page numbers and proceedings footer are added by the organisers.	0
11023	11023	S14-9	Introduction	7	14	0.269230769230769	0.098591549295775	Licence details: http://creativecommons.org/licenses/by/4.0/	0
11024	11024	S14-9	Introduction	8	15	0.307692307692308	0.105633802816901	1 Hashtags are a type of tagging for Twitter messages.	0
11025	11025	S14-9	Introduction	9	16	0.346153846153846	0.112676056338028	Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document.	0
11026	11026	S14-9	Introduction	10	17	0.384615384615385	0.119718309859155	How to handle such challenges so as to automatically mine and understand people's opinions and sentiments has only recently been the subject of research (Jansen et al., 2009;	0
11027	11027	S14-9	Introduction	11	18	0.423076923076923	0.126760563380282	Barbosa and Feng, 2010;	0
11028	11028	S14-9	Introduction	12	19	0.461538461538462	0.133802816901408	Bifet et al., 2011;Davidov et al., 2010;O'Connor et al., 2010;	0
11029	11029	S14-9	Introduction	13	20	0.5	0.140845070422535	Pak and Paroubek, 2010;Tumasjan et al., 2010;Kouloumpis et al., 2011).	0
11030	11030	S14-9	Introduction	14	21	0.538461538461538	0.147887323943662	Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year's SemEval Task 4 (Pontiki et al., 2014).	0
11031	11031	S14-9	Introduction	15	22	0.576923076923077	0.154929577464789	These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets.	0
11032	11032	S14-9	Introduction	16	23	0.615384615384615	0.161971830985916	While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment.	0
11033	11033	S14-9	Introduction	17	24	0.653846153846154	0.169014084507042	Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media.	0
11034	11034	S14-9	Introduction	18	25	0.692307692307692	0.176056338028169	Toward that goal, we created the Se-mEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013).	0
11035	11035	S14-9	Introduction	19	26	0.730769230769231	0.183098591549296	It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity.	0
11036	11036	S14-9	Introduction	20	27	0.769230769230769	0.190140845070423	This year, we extended the corpus by adding new tweets and LiveJournal sentences.	0
11037	11037	S14-9	Introduction	21	28	0.807692307692308	0.197183098591549	Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz√°lez-Ib√°√±ez et al., 2011;	0
11038	11038	S14-9	Introduction	22	29	0.846153846153846	0.204225352112676	Liebrecht et al., 2013).	0
11039	11039	S14-9	Introduction	23	30	0.884615384615385	0.211267605633803	In fact, sarcasm indicates that the message polarity should be flipped.	0
11040	11040	S14-9	Introduction	24	31	0.923076923076923	0.21830985915493	With this in mind, this year, we also evaluate on sarcastic tweets.	0
11041	11041	S14-9	Introduction	25	32	0.961538461538462	0.225352112676056	In the remainder of this paper, we first describe the task, the dataset creation process and the evaluation methodology.	0
11042	11042	S14-9	Introduction	26	33	1.0	0.232394366197183	We then summarize the characteristics of the approaches taken by the participating systems, and we discuss their scores.	0
11043	11043	S14-9	Task Description	1	34	0.25	0.23943661971831	As SemEval-2013	0
11044	11044	S14-9	Task Description	2	35	0.5	0.246478873239437	Task 2, we included two subtasks: an expression-level subtask and a messagelevel subtask.	0
11045	11045	S14-9	Task Description	3	36	0.75	0.253521126760563	Participants could choose to participate in either or both.	0
11046	11046	S14-9	Task Description	4	37	1.0	0.26056338028169	Below we provide short descriptions of the objectives of these two subtasks.	0
11101	11101	S14-9	Subtask A	1	92	0.25	0.647887323943662	Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams.	0
11102	11102	S14-9	Subtask A	2	93	0.5	0.654929577464789	There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
11103	11103	S14-9	Subtask A	3	94	0.75	0.661971830985916	The best systems were constrained.	0
11104	11104	S14-9	Subtask A	4	95	1.0	0.669014084507042	All participating systems outperformed the majority class baseline by a sizable margin.	0
11142	11142	S14-9	Conclusion	1	133	0.1	0.936619718309859	We have described the data, the experimental setup and the results for SemEval-2014 Task 9.	0
11143	11143	S14-9	Conclusion	2	134	0.2	0.943661971830986	As in 2013, our task was the most popular one at SemEval-2014, attracting 46 participating teams: 21 in subtask A (27 submissions) and 44 in subtask B (50 submissions).	0
11144	11144	S14-9	Conclusion	3	135	0.3	0.950704225352113	We introduced three new test sets for 2014: an in-domain Twitter dataset, an out-of-domain Live-Journal test set, and a dataset of tweets containing sarcastic content.	0
11145	11145	S14-9	Conclusion	4	136	0.4	0.957746478873239	While the performance on the LiveJournal test set was mostly comparable to the in-domain Twitter test set, for most teams there was a sharp drop in performance for sarcastic tweets, highlighting better handling of sarcastic language as one important direction for future work in Twitter sentiment analysis.	0
11146	11146	S14-9	Conclusion	5	137	0.5	0.964788732394366	We plan to run the task again in 2015 with the inclusion of a new sub-evaluation on detecting sarcasm with the goal of stimulating research in this area; we further plan to add one more test domain.	0
11147	11147	S14-9	Conclusion	6	138	0.6	0.971830985915493	-test), and the indicates a system that includes a task co-organizer as a team member.	0
11148	11148	S14-9	Conclusion	7	139	0.7	0.97887323943662	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
11149	11149	S14-9	Conclusion	8	140	0.8	0.985915492957746	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
11150	11150	S14-9	Conclusion	9	141	0.9	0.992957746478873	In the 2015 edition of the task, we might also remove the constrained/unconstrained distinction.	0
11151	11151	S14-9	Conclusion	10	142	1.0	1.0	Finally, as there are multiple opinions about a topic in Twitter, we would like to focus on detecting the sentiment trend towards a topic.	0
12453	12453	S15-6	title	1	1	1.0	0.007246376811594	SemEval-2015 Task 6: Clinical TempEval	0
12454	12454	S15-6	abstract	1	2	0.25	0.014492753623188	Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.	0
12455	12455	S15-6	abstract	2	3	0.5	0.021739130434783	Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.	1
12456	12456	S15-6	abstract	3	4	0.75	0.028985507246377	Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.	0
12457	12457	S15-6	abstract	4	5	1.0	0.036231884057971	Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.	0
12458	12458	S15-6	Introduction	1	6	0.083333333333333	0.043478260869565	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
12459	12459	S15-6	Introduction	2	7	0.166666666666667	0.05072463768116	Verhagen et al., 2010;UzZaman et al., 2013).	0
12460	12460	S15-6	Introduction	3	8	0.25	0.057971014492754	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	0
12461	12461	S15-6	Introduction	4	9	0.333333333333333	0.065217391304348	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
12462	12462	S15-6	Introduction	5	10	0.416666666666667	0.072463768115942	Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic.	0
12463	12463	S15-6	Introduction	6	11	0.5	0.079710144927536	This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understanding of the language of time beyond newswire expressions and structure.	0
12464	12464	S15-6	Introduction	7	12	0.583333333333333	0.086956521739131	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
12465	12465	S15-6	Introduction	8	13	0.666666666666667	0.094202898550725	Participating systems are expected to take as input raw text such as:	0
12466	12466	S15-6	Introduction	9	14	0.75	0.101449275362319	April 23, 2014:	0
12467	12467	S15-6	Introduction	10	15	0.833333333333333	0.108695652173913	The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
12468	12468	S15-6	Introduction	11	16	0.916666666666667	0.115942028985507	And output annotations over the text that capture the following kinds of information:	0
12469	12469	S15-6	Introduction	12	17	1.0	0.123188405797101	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
12488	12488	S15-6	Tasks	1	36	0.25	0.260869565217391	A total of nine tasks were included, grouped into three categories:  (Pustejovsky and Stubbs, 2011) between events and/or times, represented by TLINK annotations with TYPE=CONTAINS in the THYME corpus	0
12489	12489	S15-6	Tasks	2	37	0.5	0.268115942028985	The evaluation was run in two phases:	0
12490	12490	S15-6	Tasks	3	38	0.75	0.27536231884058	1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2.	0
12491	12491	S15-6	Tasks	4	39	1.0	0.282608695652174	Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations	0
13717	13717	S15-13	title	1	1	1.0	0.005882352941176	SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking	1
13718	13718	S15-13	abstract	1	2	0.2	0.011764705882353	In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.	1
13719	13719	S15-13	abstract	2	3	0.4	0.01764705882353	Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.	0
13720	13720	S15-13	abstract	3	4	0.6	0.023529411764706	Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.	0
13721	13721	S15-13	abstract	4	5	0.8	0.029411764705882	Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.	0
13722	13722	S15-13	abstract	5	6	1.0	0.035294117647059	Moreover, we investigate this task in a multilingual setting and for some specific domains.	0
13723	13723	S15-13	Introduction	1	7	0.045454545454546	0.041176470588235	The Senseval and SemEval evaluation series represent key moments in the community of computational linguistics and related areas.	0
13724	13724	S15-13	Introduction	2	8	0.090909090909091	0.047058823529412	Their focus has been to provide objective evaluations of methods within the wide spectrum of semantic techniques for tasks mainly related to automatic text understanding.	0
13725	13725	S15-13	Introduction	3	9	0.136363636363636	0.052941176470588	Through SemEval-2015 task 13 we both continue and renew the longstanding tradition of disambiguation tasks, by addressing multilingual WSD and EL in a joint manner.	0
13726	13726	S15-13	Introduction	4	10	0.181818181818182	0.058823529411765	WSD (Navigli, 2009;	0
13727	13727	S15-13	Introduction	5	11	0.227272727272727	0.064705882352941	Navigli, 2012) is a historical task aimed at explicitly assigning meanings to single-word and multi-word occurrences within text, a task which today is more alive than ever in the research community.	0
13728	13728	S15-13	Introduction	6	12	0.272727272727273	0.070588235294118	EL (Erbs et al., 2011;	0
13729	13729	S15-13	Introduction	7	13	0.318181818181818	0.076470588235294	Cornolti et al., 2013;	0
13730	13730	S15-13	Introduction	8	14	0.363636363636364	0.082352941176471	Rao et al., 2013) is a more recent task which aims at discovering mentions of entities within a text and linking them to the most suitable entry in a knowledge base.	0
13731	13731	S15-13	Introduction	9	15	0.409090909090909	0.088235294117647	Both these tasks aim at handling the inherent ambiguity of natural language, however WSD tackles it from a lexicographic perspective, while EL tackles it from an encyclopedic one.	0
13732	13732	S15-13	Introduction	10	16	0.454545454545455	0.094117647058824	Specifically, the main difference between the two tasks lies in the kind of inventory they use.	0
13733	13733	S15-13	Introduction	11	17	0.5	0.1	For instance, WordNet (Miller et al., 1990), a manually curated semantic network for the English language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns.	0
13734	13734	S15-13	Introduction	12	18	0.545454545454545	0.105882352941176	More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become -together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) -the main reference inventory for EL systems.	0
13735	13735	S15-13	Introduction	13	19	0.590909090909091	0.111764705882353	Over the years, the research community has typically focused on each of these tasks separately.	0
13736	13736	S15-13	Introduction	14	20	0.636363636363636	0.117647058823529	Recently, however, joint approaches have been proposed (Moro et al., 2014b).	0
13737	13737	S15-13	Introduction	15	21	0.681818181818182	0.123529411764706	One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013).	0
13738	13738	S15-13	Introduction	16	22	0.727272727272727	0.129411764705882	A case in point here is BabelNet 1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012).	0
13739	13739	S15-13	Introduction	17	23	0.772727272727273	0.135294117647059	Resources like BabelNet provide a common ground for the tasks of WSD and EL.	0
13740	13740	S15-13	Introduction	18	24	0.818181818181818	0.141176470588235	In this task our goal is to promote research in the direction of joint word sense and named entity disambiguation, so as to concentrate research efforts on the aspects that differentiate these two tasks without duplicating research on common problems such as identifying the right meaning in context.	0
13741	13741	S15-13	Introduction	19	25	0.863636363636364	0.147058823529412	However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags.	0
13742	13742	S15-13	Introduction	20	26	0.909090909090909	0.152941176470588	Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers.	0
13743	13743	S15-13	Introduction	21	27	0.954545454545455	0.158823529411765	In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation , our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues).	0
13744	13744	S15-13	Introduction	22	28	1.0	0.164705882352941	Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain.	0
13745	13745	S15-13	Task Setup	1	29	0.2	0.170588235294118	The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided.	0
13746	13746	S15-13	Task Setup	2	30	0.4	0.176470588235294	Differently from previous editions Lefever and Hoste, 2013;Manandhar et al., 2010;	0
13747	13747	S15-13	Task Setup	3	31	0.6	0.182352941176471	Lefever and Hoste, 2010;Pradhan et al., 2007;	0
13748	13748	S15-13	Task Setup	4	32	0.8	0.188235294117647	Navigli et al., 2007;	0
13749	13749	S15-13	Task Setup	5	33	1.0	0.194117647058824	Snyder and Palmer, 2004;Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013 and ERD (Carmel et al., 2014).	0
23177	23177	S19-2	title	1	1	1.0	0.00374531835206	SemEval-2019 Task 2: Unsupervised Lexical Frame Induction	0
23178	23178	S19-2	abstract	1	2	0.142857142857143	0.00749063670412	This paper presents Unsupervised Lexical	0
23179	23179	S19-2	abstract	2	3	0.285714285714286	0.01123595505618	Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.	0
23180	23180	S19-2	abstract	3	4	0.428571428571429	0.01498127340824	Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.	1
23181	23181	S19-2	abstract	4	5	0.571428571428571	0.0187265917603	Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.	0
23182	23182	S19-2	abstract	5	6	0.714285714285714	0.02247191011236	Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.	0
23183	23183	S19-2	abstract	6	7	0.857142857142857	0.02621722846442	The shared task attracted nine teams, of whom three reported promising results.	0
23184	23184	S19-2	abstract	7	8	1.0	0.029962546816479	This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.	0
23185	23185	S19-2	Introduction	1	9	0.0625	0.033707865168539	SemEval 2019	0
23186	23186	S19-2	Introduction	2	10	0.125	0.037453183520599	Task 2 focused on the unsupervised semantic labeling of a set of prespecified (semantically) unlabeled structures (Figure 1).	0
23187	23187	S19-2	Introduction	3	11	0.1875	0.041198501872659	Unsupervised learning methods analyze these structures (Figure 1a) to augment them with semantic labels (Figure 1b).	0
23188	23188	S19-2	Introduction	4	12	0.25	0.044943820224719	The shape of the manually labeled input frames is constrained to an acyclic connected tree of lexical items (words and multi-word units) of maximum depth 1, where just one root governs several arguments.	0
23189	23189	S19-2	Introduction	5	13	0.3125	0.048689138576779	The task used Berkeley FrameNet (FN) (Ruppenhofer et al., 2016) and Q. Zadeh and Petruck (2019), guidelines for this task, to determine the arguments and label them with semantic information.	0
23190	23190	S19-2	Introduction	6	14	0.375	0.052434456928839	We compared the proposed system results for unsupervised semantic tagging with that of human annotated (or, gold-standard) data in three different subtasks (Figure 2).	0
23191	23191	S19-2	Introduction	7	15	0.4375	0.056179775280899	To evaluate the systems, we computed distributional similarities between  their generated unsupervised labeled data and human annotated reference data.	0
23192	23192	S19-2	Introduction	8	16	0.5	0.059925093632959	For computing similarities we used general purpose numeral methods of text clustering, in particular BCUBED F-SCORE (Bagga and Baldwin, 1998) as the single figure of merit to rank the systems.	0
23193	23193	S19-2	Introduction	9	17	0.5625	0.063670411985019	The most important result of the shared task is the creation of a benchmark for a future complex task.	0
23194	23194	S19-2	Introduction	10	18	0.625	0.067415730337079	This benchmark includes a moderately sized, manually annotated set of frames, where only the verbs of each were included, along with their core frame elements (which uniquely define a frame as Ruppenhofer et al. describe).	0
23195	23195	S19-2	Introduction	11	19	0.6875	0.071161048689139	To complement FN's core frame elements that have highly specific meanings, the benchmark also includes the annotated argument structures of the verbs based on the generic semantic roles proposed for verb classes in VerbNet 3.2 (Kipper et al., 2000;	0
23196	23196	S19-2	Introduction	12	20	0.75	0.074906367041199	Palmer et al., 2017).	0
23197	23197	S19-2	Introduction	13	21	0.8125	0.078651685393259	The benchmark comes with simplified annotation guidelines and a modular annotation sys-tem with browsing and editing capabilities.	0
23198	23198	S19-2	Introduction	14	22	0.875	0.082397003745318	1 Complementing the benchmarking are several state-ofthe-art competing baselines, from the participants, that serve as a point of departure for improvements in the future.	0
23199	23199	S19-2	Introduction	15	23	0.9375	0.086142322097378	2	0
23200	23200	S19-2	Introduction	16	24	1.0	0.089887640449438	The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks.	0
23201	23201	S19-2	Background	1	25	0.055555555555556	0.093632958801498	Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structures have developed in parallel over several decades in theoretical linguistic studies about the syntax-semantics interface, as well as in empirical corpus-driven applications in natural language processing.	0
23202	23202	S19-2	Background	2	26	0.111111111111111	0.097378277153558	Building repositories of (lexical) semantic frames is a core component in all of these efforts.	0
23203	23203	S19-2	Background	3	27	0.166666666666667	0.101123595505618	In formal studies, lexical semantic frame knowledge bases instantiate foundational theories with tangible examples, e.g., to provide supporting evidence for the theory.	0
23204	23204	S19-2	Background	4	28	0.222222222222222	0.104868913857678	Practically, frame semantic repositories play a pivotal role in natural language understanding and semantic parsing, both as inspiration for a representation format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others.	0
23205	23205	S19-2	Background	5	29	0.277777777777778	0.108614232209738	However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks.	0
23206	23206	S19-2	Background	6	30	0.333333333333333	0.112359550561798	The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English.	0
23207	23207	S19-2	Background	7	31	0.388888888888889	0.116104868913858	While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at best.	0
23208	23208	S19-2	Background	8	32	0.444444444444444	0.119850187265918	This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017).	0
23209	23209	S19-2	Background	9	33	0.5	0.123595505617978	Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies.	0
23210	23210	S19-2	Background	10	34	0.555555555555556	0.127340823970037	Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet.	0
23211	23211	S19-2	Background	11	35	0.611111111111111	0.131086142322097	Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information.	0
23212	23212	S19-2	Background	12	36	0.666666666666667	0.134831460674157	Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments.	0
23213	23213	S19-2	Background	13	37	0.722222222222222	0.138576779026217	Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)).	0
23214	23214	S19-2	Background	14	38	0.777777777777778	0.142322097378277	This task aimed to benchmark a class of such unsupervised frame induction methods.	0
23215	23215	S19-2	Background	15	39	0.833333333333333	0.146067415730337	The ambitious goal of this task was the unsupervised induction of frame semantic structures from tokenized and morphosyntacally labeled text corpora.	0
23216	23216	S19-2	Background	16	40	0.888888888888889	0.149812734082397	We sought to achieve this goal by building an evaluation benchmark for three tasks.	0
23217	23217	S19-2	Background	17	41	0.944444444444444	0.153558052434457	Task A dealt with unsupervised labeling of verb lemmas with their frame meaning.	0
23218	23218	S19-2	Background	18	42	1.0	0.157303370786517	Task B involved unsupervised argument role labeling, where B.1 benchmarked unsupervised labeling of frame-specific frame elements (FEs) based on FN, and B.2 benchmarked unsupervised role labeling of arguments in Case Grammar terms (Fillmore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet.	0
23219	23219	S19-2	Task Description	1	43	0.25	0.161048689138577	The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation).	0
23220	23220	S19-2	Task Description	2	44	0.5	0.164794007490637	Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information.	0
23221	23221	S19-2	Task Description	3	45	0.75	0.168539325842697	Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels.	0
23222	23222	S19-2	Task Description	4	46	1.0	0.172284644194757	For development purposes, developers received a small labeled development set.	0
26004	26004	S20-4	title	1	1	1.0	0.003484320557491	SemEval-2020 Task 4: Commonsense Validation and Explanation	0
26005	26005	S20-4	abstract	1	2	0.111111111111111	0.006968641114983	In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons.	1
26006	26006	S20-4	abstract	2	3	0.222222222222222	0.010452961672474	Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not.	0
26007	26007	S20-4	abstract	3	4	0.333333333333333	0.013937282229965	The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense.	0
26008	26008	S20-4	abstract	4	5	0.444444444444444	0.017421602787457	In the third subtask, a participating system needs to generate the reason.	0
26009	26009	S20-4	abstract	5	6	0.555555555555556	0.020905923344948	We finally attracted 39 teams participating at least one of the three subtasks.	0
26010	26010	S20-4	abstract	6	7	0.666666666666667	0.024390243902439	For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans.	0
26011	26011	S20-4	abstract	7	8	0.777777777777778	0.02787456445993	However, for Subtask C, there is still a relatively large gap between systems and human performance.	0
26012	26012	S20-4	abstract	8	9	0.888888888888889	0.031358885017422	The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation;	0
26013	26013	S20-4	abstract	9	10	1.0	0.034843205574913	The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.	0
26014	26014	S20-4	Introduction	1	11	0.043478260869565	0.038327526132404	In the past decades, computer' ability in processing natural language has significantly improved.	0
26015	26015	S20-4	Introduction	2	12	0.086956521739131	0.041811846689896	However, its intelligence for understanding common sense expressed in language is still limited.	0
26016	26016	S20-4	Introduction	3	13	0.130434782608696	0.045296167247387	"For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: ""John put a turkey into a fridge"" while ""John put an elephant into the fridge"" does not, but it is non-trivial for a computer to tell the difference."	0
26017	26017	S20-4	Introduction	4	14	0.173913043478261	0.048780487804878	Arguably, commonsense reasoning plays a central role in a natural language understanding system (Davis, 2017).	0
26018	26018	S20-4	Introduction	5	15	0.217391304347826	0.052264808362369	It is essential to gauge how well computers can understand whether a given statement makes sense.	0
26019	26019	S20-4	Introduction	6	16	0.260869565217391	0.055749128919861	In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world.	0
26020	26020	S20-4	Introduction	7	17	0.304347826086957	0.059233449477352	1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012;Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016;	0
26021	26021	S20-4	Introduction	8	18	0.347826086956522	0.062717770034843	Ostermann et al., 2018b;	0
26022	26022	S20-4	Introduction	9	19	0.391304347826087	0.066202090592335	Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016;	0
26023	26023	S20-4	Introduction	10	20	0.434782608695652	0.069686411149826	Talmor et al., 2018;Mihaylov et al., 2018).	0
26024	26024	S20-4	Introduction	11	21	0.478260869565217	0.073170731707317	They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge.	0
26025	26025	S20-4	Introduction	12	22	0.521739130434783	0.076655052264808	The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.	0
26026	26026	S20-4	Introduction	13	23	0.565217391304348	0.0801393728223	The SemEval-2020	0
26027	26027	S20-4	Introduction	14	24	0.608695652173913	0.083623693379791	Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons.	0
26028	26028	S20-4	Introduction	15	25	0.652173913043478	0.087108013937282	"In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., ""John put an elephant into the fridge"" and ""John put a turkey into the fridge"", respectively."	0
26029	26029	S20-4	Introduction	16	26	0.695652173913043	0.090592334494774	The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense.	0
26030	26030	S20-4	Introduction	17	27	0.739130434782609	0.094076655052265	"For example, for the nonsensical statement, ""John put an elephant into the fridge"", the three options are ""An elephant is much bigger than a fridge"", ""Elephants are usually white while fridges are usually white"", and ""An elephant cannot eat a fridge."""	0
26031	26031	S20-4	Introduction	18	28	0.782608695652174	0.097560975609756	A system needs to identify the correct reason.	0
26032	26032	S20-4	Introduction	19	29	0.826086956521739	0.101045296167247	In addition, the third task requires the participating systems to generate the reason automatically.	0
26033	26033	S20-4	Introduction	20	30	0.869565217391304	0.104529616724739	We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.	0
26034	26034	S20-4	Introduction	21	31	0.91304347826087	0.10801393728223	There are 39 teams submitting valid systems to at least one subtask.	0
26035	26035	S20-4	Introduction	22	32	0.956521739130435	0.111498257839721	In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects.	0
26036	26036	S20-4	Introduction	23	33	1.0	0.114982578397213	However, for Subtask C, there is still a relatively large between system and human performances.	0
26037	26037	S20-4	Task Setup	1	34	0.03448275862069	0.118466898954704	Task Definition	0
26038	26038	S20-4	Task Setup	2	35	0.068965517241379	0.121951219512195	Formally, each instance in our dataset is composed of eight sentences:	0
26039	26039	S20-4	Task Setup	3	36	0.103448275862069	0.125435540069686	and s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not.	0
26040	26040	S20-4	Task Setup	4	37	0.137931034482759	0.128919860627178	They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense.	0
26041	26041	S20-4	Task Setup	5	38	0.172413793103448	0.132404181184669	For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense.	0
26042	26042	S20-4	Task Setup	6	39	0.206896551724138	0.13588850174216	So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options.	0
26043	26043	S20-4	Task Setup	7	40	0.241379310344828	0.139372822299652	For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense.	0
26044	26044	S20-4	Task Setup	8	41	0.275862068965517	0.142857142857143	Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.	0
26045	26045	S20-4	Task Setup	9	42	0.310344827586207	0.146341463414634	‚Ä¢ Subtask A: Validation Task:	0
26046	26046	S20-4	Task Setup	10	43	0.344827586206897	0.149825783972125	Select the statement of the two that does not make sense.	0
26047	26047	S20-4	Task Setup	11	44	0.379310344827586	0.153310104529617	s 1 : John put a turkey into a fridge.	0
26048	26048	S20-4	Task Setup	12	45	0.413793103448276	0.156794425087108	s 2 : John put an elephant into the fridge.	0
26049	26049	S20-4	Task Setup	13	46	0.448275862068966	0.160278745644599	In this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .	0
26050	26050	S20-4	Task Setup	14	47	0.482758620689655	0.163763066202091	‚Ä¢ Subtask B: Explanation (Multi-Choice)	0
26051	26051	S20-4	Task Setup	15	48	0.517241379310345	0.167247386759582	Task:	0
26052	26052	S20-4	Task Setup	16	49	0.551724137931034	0.170731707317073	Select the best reason that explains why the given statement does not make sense.	0
26053	26053	S20-4	Task Setup	17	50	0.586206896551724	0.174216027874564	Nonsensical statement (s n ):	0
26054	26054	S20-4	Task Setup	18	51	0.620689655172414	0.177700348432056	John put an elephant into the fridge.	0
26055	26055	S20-4	Task Setup	19	52	0.655172413793103	0.181184668989547	o 1 : An elephant is much bigger than a fridge.	0
26056	26056	S20-4	Task Setup	20	53	0.689655172413793	0.184668989547038	o 2 : Elephants are usually white while fridges are usually white.	0
26057	26057	S20-4	Task Setup	21	54	0.724137931034483	0.18815331010453	o 3 : An elephant cannot eat a fridge.	0
26058	26058	S20-4	Task Setup	22	55	0.758620689655172	0.191637630662021	In this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .	0
26059	26059	S20-4	Task Setup	23	56	0.793103448275862	0.195121951219512	‚Ä¢ Subtask C: Explanation (Generation)	0
26060	26060	S20-4	Task Setup	24	57	0.827586206896552	0.198606271777003	Task: Generate the reason why this statement does not make sense.	0
26061	26061	S20-4	Task Setup	25	58	0.862068965517241	0.202090592334495	Nonsensical statement (s n ):	0
26062	26062	S20-4	Task Setup	26	59	0.896551724137931	0.205574912891986	John put an elephant into the fridge.	0
26063	26063	S20-4	Task Setup	27	60	0.931034482758621	0.209059233449477	Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge.	0
26064	26064	S20-4	Task Setup	28	61	0.96551724137931	0.212543554006969	r 2 : A fridge is much smaller than an elephant.	0
26065	26065	S20-4	Task Setup	29	62	1.0	0.21602787456446	r 3 : Most of the fridges aren't large enough to contain an elephant.	0
26068	26068	S20-4	1	1	65	0.5	0.226480836236934	The reason is just the negation of the statement or a simple paraphrase.	0
26069	26069	S20-4	1	2	66	1.0	0.229965156794425	Obviously, a better explanation can be made.	0
26217	26217	S20-4	Related Work	1	214	0.017857142857143	0.745644599303136	Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention.	0
26218	26218	S20-4	Related Work	2	215	0.035714285714286	0.749128919860627	In the Winograd Schema Challenge (WSC) (Levesque et al., 2012;	0
26219	26219	S20-4	Related Work	3	216	0.053571428571429	0.752613240418119	Morgenstern and Ortiz, 2015), a model needs to solve hard co-reference resolution problems based on commonsense knowledge.	0
26220	26220	S20-4	Related Work	4	217	0.071428571428572	0.75609756097561	"For example, ""The trophy would not fit in the brown suitcase because it was too big."	0
26221	26221	S20-4	Related Work	5	218	0.089285714285714	0.759581881533101	"What was too big (trophy or suitcase)?"""	0
26222	26222	S20-4	Related Work	6	219	0.107142857142857	0.763066202090592	The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) emphasizes on events and consequences.	0
26223	26223	S20-4	Related Work	7	220	0.125	0.766550522648084	Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives.	0
26224	26224	S20-4	Related Work	8	221	0.142857142857143	0.770034843205575	All premises and alternatives are simple sentences.	0
26225	26225	S20-4	Related Work	9	222	0.160714285714286	0.773519163763066	"For example, the premise can be ""The man broke his toe."	0
26226	26226	S20-4	Related Work	10	223	0.178571428571429	0.777003484320557	"What was the CAUSE of this?"" and the two candidate answers are ""(1)"	0
26227	26227	S20-4	Related Work	11	224	0.196428571428571	0.780487804878049	"He got a hole in his sock."" and ""(2)"	0
26228	26228	S20-4	Related Work	12	225	0.214285714285714	0.78397212543554	"He dropped a hammer on his foot."""	0
26229	26229	S20-4	Related Work	13	226	0.232142857142857	0.787456445993031	Several subsequent datasets are inspired by COPA.	0
26230	26230	S20-4	Related Work	14	227	0.25	0.790940766550523	The JHU Ordinal Common-sense Inference (JOCI) (Zhang et al., 2017) aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation.	0
26231	26231	S20-4	Related Work	15	228	0.267857142857143	0.794425087108014	Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation.	0
26232	26232	S20-4	Related Work	16	229	0.285714285714286	0.797909407665505	Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused.	0
26233	26233	S20-4	Related Work	17	230	0.303571428571429	0.801393728222996	Besides, our dataset is not limited to events or situations.	0
26234	26234	S20-4	Related Work	18	231	0.321428571428571	0.804878048780488	It concerns a broader commonsense setting, which includes events, descriptions, assertion etc.	0
26235	26235	S20-4	Related Work	19	232	0.339285714285714	0.808362369337979	Some datasets are inspired by reading comprehension.	0
26236	26236	S20-4	Related Work	20	233	0.357142857142857	0.81184668989547	The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016;	0
26237	26237	S20-4	Related Work	21	234	0.375	0.815331010452962	Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story.	0
26238	26238	S20-4	Related Work	22	235	0.392857142857143	0.818815331010453	For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question.	0
26239	26239	S20-4	Related Work	23	236	0.410714285714286	0.822299651567944	Most questions require knowledge beyond the facts mentioned in the text.	0
26240	26240	S20-4	Related Work	24	237	0.428571428571429	0.825783972125435	Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want.	0
26241	26241	S20-4	Related Work	25	238	0.446428571428571	0.829268292682927	Some other datasets evolve from QA problems and care more about factual commonsense knowledge.	0
26242	26242	S20-4	Related Work	26	239	0.464285714285714	0.832752613240418	SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions.	0
26243	26243	S20-4	Related Work	27	240	0.482142857142857	0.836236933797909	Commonsense	0
26244	26244	S20-4	Related Work	28	241	0.5	0.839721254355401	QA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBook	0
26245	26245	S20-4	Related Work	29	242	0.517857142857143	0.843205574912892	QA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions.	0
26246	26246	S20-4	Related Work	30	243	0.535714285714286	0.846689895470383	The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences.	0
26247	26247	S20-4	Related Work	31	244	0.553571428571429	0.850174216027875	MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020).	0
26248	26248	S20-4	Related Work	32	245	0.571428571428571	0.853658536585366	Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.	0
26249	26249	S20-4	Related Work	33	246	0.589285714285714	0.857142857142857	Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018;	0
26250	26250	S20-4	Related Work	34	247	0.607142857142857	0.860627177700348	"Porada et al., 2019), such as ""gorilla-ride-camel""."	0
26251	26251	S20-4	Related Work	35	248	0.625	0.86411149825784	"In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as ""China's territory is larger than Japan's""."	0
26252	26252	S20-4	Related Work	36	249	0.642857142857143	0.867595818815331	And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017).	0
26253	26253	S20-4	Related Work	37	250	0.660714285714286	0.871080139372822	"Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task ""Tom's mom become (happy)/(upset) when Tom gets high grades in the exam"" is about social and emotional common sense."	0
26254	26254	S20-4	Related Work	38	251	0.678571428571429	0.874564459930314	For our first task, those statements that conforms to commonsense can also be phrased as being plausible.	0
26255	26255	S20-4	Related Work	39	252	0.696428571428571	0.878048780487805	Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.	0
26256	26256	S20-4	Related Work	40	253	0.714285714285714	0.881533101045296	More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions.	0
26257	26257	S20-4	Related Work	41	254	0.732142857142857	0.885017421602787	In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks.	0
26258	26258	S20-4	Related Work	42	255	0.75	0.888501742160279	Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on.	0
26259	26259	S20-4	Related Work	43	256	0.767857142857143	0.89198606271777	Event2	0
26260	26260	S20-4	Related Work	44	257	0.785714285714286	0.895470383275261	Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances.	0
26261	26261	S20-4	Related Work	45	258	0.803571428571429	0.898954703832753	These datasets are not directly useful for our benchmark since they focus only on a small domain.	0
26262	26262	S20-4	Related Work	46	259	0.821428571428571	0.902439024390244	Concept	0
26263	26263	S20-4	Related Work	47	260	0.839285714285714	0.905923344947735	Net is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004;	0
26264	26264	S20-4	Related Work	48	261	0.857142857142857	0.909407665505226	Havasi et al., 2007;	0
26265	26265	S20-4	Related Work	49	262	0.875	0.912891986062718	Speer and Havasi, 2013;	0
26266	26266	S20-4	Related Work	50	263	0.892857142857143	0.916376306620209	Speer et al., 2017).	0
26267	26267	S20-4	Related Work	51	264	0.910714285714286	0.9198606271777	Concept	0
26268	26268	S20-4	Related Work	52	265	0.928571428571429	0.923344947735192	Net constructs triples using labeled edges as relations and various words and/or phrases as entities.	0
26269	26269	S20-4	Related Work	53	266	0.946428571428571	0.926829268292683	It also has the sentences describing the corresponding triples.	0
26270	26270	S20-4	Related Work	54	267	0.964285714285714	0.930313588850174	In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.	0
26271	26271	S20-4	Related Work	55	268	0.982142857142857	0.933797909407666	Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences.	0
26272	26272	S20-4	Related Work	56	269	1.0	0.937282229965157	In our task here, we also provide training data with human annotations.	0
26273	26273	S20-4	Summary	1	270	0.055555555555556	0.940766550522648	This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation.	0
26274	26274	S20-4	Summary	2	271	0.111111111111111	0.944250871080139	In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences.	0
26275	26275	S20-4	Summary	3	272	0.166666666666667	0.947735191637631	The task attracted around 40 participating teams, out of which 31 teams submit their system papers.	0
26276	26276	S20-4	Summary	4	273	0.222222222222222	0.951219512195122	The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.	0
26277	26277	S20-4	Summary	5	274	0.277777777777778	0.954703832752613	We attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences.	0
26278	26278	S20-4	Summary	6	275	0.333333333333333	0.958188153310105	2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge (Zhou et al., 2019), which helps achieve considerably better performance.	0
26279	26279	S20-4	Summary	7	276	0.388888888888889	0.961672473867596	3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers.	0
26280	26280	S20-4	Summary	8	277	0.444444444444444	0.965156794425087	The top-3 systems also use OMCS, which potentially help them to attain better performances.	0
26281	26281	S20-4	Summary	9	278	0.5	0.968641114982578	4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty.	0
26282	26282	S20-4	Summary	10	279	0.555555555555556	0.97212543554007	5) Some instances have obvious patterns.	0
26283	26283	S20-4	Summary	11	280	0.611111111111111	0.975609756097561	"For example, there are tens of instances that contain ""put XXX into YYY"", and ""XXX is bigger than YYY"", making the problems simpler."	0
26284	26284	S20-4	Summary	12	281	0.666666666666667	0.979094076655052	6) Hundreds of crowd-sourcing workers write instances.	0
26285	26285	S20-4	Summary	13	282	0.722222222222222	0.982578397212544	"It is likely for workers to think about the shared commonsense knowledge, such as ""XXX is bigger/shorter/quicker/slower than YYY""."	0
26286	26286	S20-4	Summary	14	283	0.777777777777778	0.986062717770035	We consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation.	0
26287	26287	S20-4	Summary	15	284	0.833333333333333	0.989547038327526	2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns.	0
26288	26288	S20-4	Summary	16	285	0.888888888888889	0.993031358885017	3) Subtask A can be turned into a more difficult form.	0
26289	26289	S20-4	Summary	17	286	0.944444444444444	0.996515679442509	Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not.	0
26290	26290	S20-4	Summary	18	287	1.0	1.0	4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.	0
26291	26291	S20-5	title	1	1	1.0	0.004385964912281	SemEval-2020 Task 5: Counterfactual Recognition	0
26292	26292	S20-5	abstract	1	2	0.125	0.008771929824561	We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020.	0
26293	26293	S20-5	abstract	2	3	0.25	0.013157894736842	Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent).	0
26294	26294	S20-5	abstract	3	4	0.375	0.017543859649123	Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequents with causal relations.	0
26295	26295	S20-5	abstract	4	5	0.5	0.021929824561404	Our task provides a benchmark for counterfactual recognition in natural language with two subtasks.	0
26296	26296	S20-5	abstract	5	6	0.625	0.026315789473684	Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not.	0
26297	26297	S20-5	abstract	6	7	0.75	0.030701754385965	Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement.	0
26298	26298	S20-5	abstract	7	8	0.875	0.035087719298246	During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2.	0
26299	26299	S20-5	abstract	8	9	1.0	0.039473684210526	The data, baseline code, and leaderboard can be found	0
26300	26300	S20-5	Introduction	1	10	0.027027027027027	0.043859649122807	"Counterfactual statements describe events that did not happen or cannot happen, and the possible consequences had those events happened, e.g., ""if kangaroos had no tails, they would topple over"" (Lewis, 2013)."	0
26301	26301	S20-5	Introduction	2	11	0.054054054054054	0.048245614035088	"By developing a connection between the antecedent (e.g., ""kangaroos had no tails"") and consequent (e.g., ""they would topple over""), based on the imagination of possible worlds, humans can naturally form some causal judgments; e.g., having tails can prevent kangaroos from toppling over."	0
26302	26302	S20-5	Introduction	3	12	0.081081081081081	0.052631578947369	One can understand counterfactuals using knowledge and explore the relationship between causes and effects.	0
26303	26303	S20-5	Introduction	4	13	0.108108108108108	0.057017543859649	Although we may not be able to rollback the events which have happened or make impossible events occur in the real world, we can still think of potential outcomes of alternatives.	0
26304	26304	S20-5	Introduction	5	14	0.135135135135135	0.06140350877193	Counterfactual thinking is a remarkable ability of human beings and is considered by many researchers, to act as the highest level of causation in the ladder of causal reasoning.	0
26305	26305	S20-5	Introduction	6	15	0.162162162162162	0.065789473684211	Even the most advanced artificial intelligence system may still be far from achieving human-like counterfactual reasoning.	0
26306	26306	S20-5	Introduction	7	16	0.189189189189189	0.070175438596491	Counterfactual reasoning is an important component for AI systems in obtaining stronger capability in generalization (Pearl and Mackenzie, 2018).	0
26307	26307	S20-5	Introduction	8	17	0.216216216216216	0.074561403508772	Modeling counterfactuals has been studied in many different disciplines.	0
26308	26308	S20-5	Introduction	9	18	0.243243243243243	0.078947368421053	For example, research in psychology has shown that counterfactual thinking can affect human cognition and behaviors (Epstude and Roese, 2008;	0
26309	26309	S20-5	Introduction	10	19	0.27027027027027	0.083333333333333	Kray et al., 2010).	0
26310	26310	S20-5	Introduction	11	20	0.297297297297297	0.087719298245614	The landmark paper of (Goodman, 1947) gives a detailed analysis of counterfactual conditionals in philosophy and logistics.	0
26311	26311	S20-5	Introduction	12	21	0.324324324324324	0.092105263157895	As another example, counterfactuals have also been investigated in epidemiology to reveal the relationship between certain diseases and potential risk factors for those diseases (Vandenbroucke et al., 2016;	0
26312	26312	S20-5	Introduction	13	22	0.351351351351351	0.096491228070176	Krieger and Davey Smith, 2016).	0
26313	26313	S20-5	Introduction	14	23	0.378378378378378	0.100877192982456	We present a counterfactual recognition (CR) task, the task of determining whether a given statement conveys counterfactual thinking or not, and further analyzing the causal relations indicated by counterfactual statements.	0
26314	26314	S20-5	Introduction	15	24	0.405405405405405	0.105263157894737	In our counterfactual recognition task, we aim to model counterfactual semantics and reasoning in natural language.	1
26315	26315	S20-5	Introduction	16	25	0.432432432432432	0.109649122807018	Specifically, we provide a benchmark for counterfactual recognition with two subtasks.	0
26316	26316	S20-5	Introduction	17	26	0.45945945945946	0.114035087719298	Subtask-1 requires systems to determine whether a given statement is counterfactual or not.	0
26317	26317	S20-5	Introduction	18	27	0.486486486486487	0.118421052631579	The counterfactual detection task can serve as a foundation for downstream counterfactual analysis.	0
26318	26318	S20-5	Introduction	19	28	0.513513513513513	0.12280701754386	Subtask-2 requires systems to further locate the antecedent and consequent text spans in a given counterfactual statement, as the connection between an antecedent and consequent can reveal core causal inference clues.	0
26319	26319	S20-5	Introduction	20	29	0.540540540540541	0.12719298245614	To build the dataset for counterfactual recognition, we extract over 60,000 candidate counterfactual statements by scanning through news reports in three domains: finance, politics, and healthcare.	0
26320	26320	S20-5	Introduction	21	30	0.567567567567568	0.131578947368421	The first round of annotation focuses on labeling each sample as true or false, where true denotes a sample is counterfactual and false otherwise in Subtask-1.	0
26321	26321	S20-5	Introduction	22	31	0.594594594594595	0.135964912280702	A portion of samples labeled as true will be further used in Subtask-2 to detect the text spans that describe the antecedent and consequent.	0
26322	26322	S20-5	Introduction	23	32	0.621621621621622	0.140350877192982	Specifically, we carefully select 20,000 high-quality samples from the 60,000 statements and use them in Subtask-1, with 13,000 (65%) as the training set and the rest for testing.	0
26323	26323	S20-5	Introduction	24	33	0.648648648648649	0.144736842105263	The dataset for Subtask-2 contains 5,501 samples, among which we use 3,551 (65%) for training and the rest for testing.	0
26324	26324	S20-5	Introduction	25	34	0.675675675675676	0.149122807017544	To achieve a decent performance in our shared task, we expect the systems should have a certain level of language understanding capacity in both semantics and syntax, together with a certain level of commonsense reasoning ability.	0
26325	26325	S20-5	Introduction	26	35	0.702702702702703	0.153508771929825	In Subtask-1, the top-ranked submissions all use pre-trained neural models, which appear to be an effective way to integrate knowledge learned from large corpus.	0
26326	26326	S20-5	Introduction	27	36	0.72972972972973	0.157894736842105	All of these models use neural networks, which further confirms the effectiveness of distributed representation and subsymbolic approaches for this task.	0
26327	26327	S20-5	Introduction	28	37	0.756756756756757	0.162280701754386	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural networks with symbolic approaches.	0
26328	26328	S20-5	Introduction	29	38	0.783783783783784	0.166666666666667	The first-place model also utilizes data augmentation to further improve system performance.	0
26329	26329	S20-5	Introduction	30	39	0.810810810810811	0.171052631578947	In Subtask-2, top systems take two main approaches: sequence labelling or question answering.	0
26330	26330	S20-5	Introduction	31	40	0.837837837837838	0.175438596491228	Same as systems in Subtask-1, all of them benefit from pre-training.	0
26331	26331	S20-5	Introduction	32	41	0.864864864864865	0.179824561403509	We will provide a more detailed analysis in the system and result section.	0
26332	26332	S20-5	Introduction	33	42	0.891891891891892	0.184210526315789	We built a dataset for this shared task from scratch.	0
26333	26333	S20-5	Introduction	34	43	0.918918918918919	0.18859649122807	Our data, baseline code, and leaderboard can be found at https://competitions.codalab.org/competitions/21691.	0
26334	26334	S20-5	Introduction	35	44	0.945945945945946	0.192982456140351	The data and baseline code are also available at https://zenodo.org/record/3932442.	0
26335	26335	S20-5	Introduction	36	45	0.972972972972973	0.197368421052632	In general, our task here is a relatively basic one in counterfactual analysis in natural language.	0
26336	26336	S20-5	Introduction	37	46	1.0	0.201754385964912	We hope it will intrigue and facilitate further research on counterfactual analysis and can benefit other related downstream tasks.	0
26337	26337	S20-5	Task Setup	1	47	0.5	0.206140350877193	In this section, we detail the two counterfactual recognition subtasks and the metrics used to evaluate the performance.	0
26338	26338	S20-5	Task Setup	2	48	1.0	0.210526315789474	During the evaluation, participants can work on both subtasks or any one of them.	0
26483	26483	S20-5	Related Work	1	193	0.0625	0.846491228070175	Modelling counterfactual thinking has started to attract more interest.	0
26484	26484	S20-5	Related Work	2	194	0.125	0.850877192982456	One of the previous works closest to ours is (Son et al., 2017), in which a small-scale counterfactual tweet dataset is collected from social media.	0
26485	26485	S20-5	Related Work	3	195	0.1875	0.855263157894737	There are three main differences between that dataset and ours.	0
26486	26486	S20-5	Related Work	4	196	0.25	0.859649122807017	First, there are only 2,000 samples in the tweet dataset (including the supplement data mentioned in the paper), while our dataset for counterfactual detection in Subtask-1 is ten times larger, which we believe is important for training deep learning based models.	0
26487	26487	S20-5	Related Work	5	197	0.3125	0.864035087719298	Second, our benchmark provides evaluation for antecedents and consequents extraction, which are essential components of counterfactual analysis.	0
26488	26488	S20-5	Related Work	6	198	0.375	0.868421052631579	Third, our dataset includes statements from three different domains (finance, politics, healthcare).	0
26489	26489	S20-5	Related Work	7	199	0.4375	0.87280701754386	In contrast to the statements collected from tweets, which have a very large portion that are open-ended, vague thoughts, the counterfactuals in our dataset are more meaningful domain-related statements.	0
26490	26490	S20-5	Related Work	8	200	0.5	0.87719298245614	There is another dataset TIMETRAVEL proposed in (Qin et al., 2019)  in which given a short story and an alternative counterfactual event context, the story needs to be minimally revised to keep compatible with the intervening counterfactual event.	0
26491	26491	S20-5	Related Work	9	201	0.5625	0.881578947368421	The empirical results show that it is still challenging for current neural language models to perform well on the counterfactual story rewriting task due to the lack of counterfactual reasoning capabilities.	0
26492	26492	S20-5	Related Work	10	202	0.625	0.885964912280702	In a broader viewpoint, counterfatuals are an important form of causal reasoning.	0
26493	26493	S20-5	Related Work	11	203	0.6875	0.890350877192982	Researchers argue that the notion of counterfactuals is essential for causal reasoning, in which causal modeling is proposed to interpret counterfactual conditionals in natural language, and such work has been discussed since the possible worlds semantics developed in the 1970s (Lewis, 2013;	0
26494	26494	S20-5	Related Work	12	204	0.75	0.894736842105263	Lewis, 1986).	0
26495	26495	S20-5	Related Work	13	205	0.8125	0.899122807017544	The more recent work renders useful insights by formulating causal inference as a three-level hierarchy, which are association, intervention, and counterfactual, respectively (Pearl and Mackenzie, 2018;Pearl, 2019).	0
26496	26496	S20-5	Related Work	14	206	0.875	0.903508771929825	"The top of the hierarchy is counterfactual-if a model can correctly answer counterfactual queries like ""what would happen if we had acted differently"", it should also be able to answer association and intervention queries."	0
26497	26497	S20-5	Related Work	15	207	0.9375	0.907894736842105	The research in (Pearl, 1995;	0
26498	26498	S20-5	Related Work	16	208	1.0	0.912280701754386	Pearl, 2010) also made contributions to a general theory of causal inference, which is based on the Structural Causal Model (SCM), and counterfactual analysis is provided with a formal mathematical formalism.	0
27902	27902	S20-12	title	1	1	0.5	0.004444444444444	SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offens	0
27903	27903	S20-12	title	2	2	1.0	0.008888888888889	Eval 2020)	0
27904	27904	S20-12	abstract	1	3	0.25	0.013333333333333	We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020).	0
27905	27905	S20-12	abstract	2	4	0.5	0.017777777777778	The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish.	0
27906	27906	S20-12	abstract	3	5	0.75	0.022222222222222	Offens	0
27907	27907	S20-12	abstract	4	6	1.0	0.026666666666667	Eval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.	0
27908	27908	S20-12	Introduction	1	7	0.027777777777778	0.031111111111111	Offensive language is ubiquitous in social media platforms such as Facebook, Twitter, and Reddit, and it comes in many forms.	0
27909	27909	S20-12	Introduction	2	8	0.055555555555556	0.035555555555556	Given the multitude of terms and definitions related to offensive language used in the literature, several recent studies have investigated the common aspects of different abusive language detection tasks (Waseem et al., 2017;	0
27910	27910	S20-12	Introduction	3	9	0.083333333333333	0.04	Wiegand et al., 2018).	0
27911	27911	S20-12	Introduction	4	10	0.111111111111111	0.044444444444445	One such example is SemEval-2019 Task 6: Offens	0
27912	27912	S20-12	Introduction	5	11	0.138888888888889	0.048888888888889	Eval 1 (Zampieri et al., 2019b), which is the precursor to the present shared task.	0
27913	27913	S20-12	Introduction	6	12	0.166666666666667	0.053333333333333	Offens	0
27914	27914	S20-12	Introduction	7	13	0.194444444444444	0.057777777777778	Eval-2019 used the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets annotated using a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account (Zampieri et al., 2019a).	0
27915	27915	S20-12	Introduction	8	14	0.222222222222222	0.062222222222222	The assumption behind this annotation schema is that the target of offensive messages is an important variable that allows us to discriminate between, e.g., hate speech, which often consists of insults targeted toward a group, and cyberbullying, which typically targets individuals.	0
27916	27916	S20-12	Introduction	9	15	0.25	0.066666666666667	A number of recently organized related shared tasks followed similar hierarchical models.	0
27917	27917	S20-12	Introduction	10	16	0.277777777777778	0.071111111111111	Examples include HASOC-2019 (Mandl et al., 2019) for English, German, and Hindi, HatEval-2019 (Basile et al., 2019) for English and Spanish, GermEval-2019 for German (Stru√ü et al., 2019), and TRAC-2020 (Kumar et al., 2020) for English, Bengali, and Hindi.	0
27918	27918	S20-12	Introduction	11	17	0.305555555555556	0.075555555555556	Offens	0
27919	27919	S20-12	Introduction	12	18	0.333333333333333	0.08	Eval-2019 attracted nearly 800 team registrations and received 115 official submissions, which demonstrates the interest of the research community in this topic.	0
27920	27920	S20-12	Introduction	13	19	0.361111111111111	0.084444444444445	Therefore, we organized a follow-up, OffensEval-2020 2 (SemEval-2020 Task 12), which is described in this report, building on the success of OffensEval-2019 with several improvements.	0
27921	27921	S20-12	Introduction	14	20	0.388888888888889	0.088888888888889	In particular, we used the same three-level taxonomy to annotate new datasets in five languages, where each level in this taxonomy corresponds to a subtask in the competition:	0
27922	27922	S20-12	Introduction	15	21	0.416666666666667	0.093333333333333	‚Ä¢ Subtask A: Offensive language identification;	0
27923	27923	S20-12	Introduction	16	22	0.444444444444444	0.097777777777778	‚Ä¢ Subtask B: Automatic categorization of offense types;	0
27924	27924	S20-12	Introduction	17	23	0.472222222222222	0.102222222222222	‚Ä¢ Subtask C: Offense target identification.	0
27925	27925	S20-12	Introduction	18	24	0.5	0.106666666666667	This work is licensed under a Creative Commons Attribution 4.0 International License.	0
27926	27926	S20-12	Introduction	19	25	0.527777777777778	0.111111111111111	License details: http: //creativecommons.org/licenses/by/4.0/.	0
27927	27927	S20-12	Introduction	20	26	0.555555555555556	0.115555555555556	1 http://sites.google.com/site/offensevalsharedtask/offenseval2019 2 http://sites.google.com/site/offensevalsharedtask/home	0
27928	27928	S20-12	Introduction	21	27	0.583333333333333	0.12	The contributions of OffensEval-2020 can be summarized as follows:	0
27929	27929	S20-12	Introduction	22	28	0.611111111111111	0.124444444444444	‚Ä¢	0
27930	27930	S20-12	Introduction	23	29	0.638888888888889	0.128888888888889	We provided the participants with a new, large-scale semi-supervised training dataset containing over nine million English tweets (Rosenthal et al., 2020).	0
27931	27931	S20-12	Introduction	24	30	0.666666666666667	0.133333333333333	‚Ä¢	0
27932	27932	S20-12	Introduction	25	31	0.694444444444444	0.137777777777778	We introduced multilingual datasets, and we expanded the task to four new languages: Arabic (Mubarak et al., 2020b), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Turkish (√á√∂ltekin, 2020).	0
27933	27933	S20-12	Introduction	26	32	0.722222222222222	0.142222222222222	This opens the possibility for cross-lingual training and analysis, which several participants indeed explored.	0
27934	27934	S20-12	Introduction	27	33	0.75	0.146666666666667	‚Ä¢ Compared to OffensEval-2019, we used larger test datasets for all subtasks.	0
27935	27935	S20-12	Introduction	28	34	0.777777777777778	0.151111111111111	Overall, Offens	0
27936	27936	S20-12	Introduction	29	35	0.805555555555556	0.155555555555556	Eval-2020 was a very successful task.	0
27937	27937	S20-12	Introduction	30	36	0.833333333333333	0.16	The huge interest demonstrated last year continued this year, with 528 teams signing up to participate in the task, and 145 of them submitting official runs on the test dataset.	0
27938	27938	S20-12	Introduction	31	37	0.861111111111111	0.164444444444444	Furthermore, Offens	0
27939	27939	S20-12	Introduction	32	38	0.888888888888889	0.168888888888889	Eval-2020 received 70 system description papers, which is an all-time record for a SemEval task.	0
27940	27940	S20-12	Introduction	33	39	0.916666666666667	0.173333333333333	The remainder of this paper is organized as follows: Section 2 describes the annotation schema.	0
27941	27941	S20-12	Introduction	34	40	0.944444444444444	0.177777777777778	Section 3 presents the five datasets that we used in the competition.	0
27942	27942	S20-12	Introduction	35	41	0.972222222222222	0.182222222222222	Sections 4-9 present the results and discuss the approaches taken by the participating systems for each of the five languages.	0
27943	27943	S20-12	Introduction	36	42	1.0	0.186666666666667	Finally, Section 10 concludes and suggests some possible directions for future work.	0
28022	28022	S20-12	Subtask A	1	121	0.125	0.537777777777778	A total of 82 teams made submissions for subtask A, and the results can be seen in Table 5.	0
28023	28023	S20-12	Subtask A	2	122	0.25	0.542222222222222	This was the most popular subtask among all subtasks and across all languages.	0
28024	28024	S20-12	Subtask A	3	123	0.375	0.546666666666667	The best team UHH-LT achieved an F1 score of 0.9204 using an ensemble of ALBERT models of different sizes.	0
28025	28025	S20-12	Subtask A	4	124	0.5	0.551111111111111	The team ranked second was UHH-LT with an F1 score of 0.9204, and it used RoBERTa-large that was fine-tuned on the SOLID dataset in an unsupervised way, i.e., using the MLM objective.	0
28026	28026	S20-12	Subtask A	5	125	0.625	0.555555555555556	The third team, Galileo, achieved an F1 score of 0.9198, using an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large trained on the subtask A data for all languages.	0
28027	28027	S20-12	Subtask A	6	126	0.75	0.56	The top-10 teams used BERT, RoBERTa or XLM-RoBERTa, sometimes as part of ensembles that also included CNNs and LSTMs (Hochreiter and Schmidhuber, 1997).	0
28028	28028	S20-12	Subtask A	7	127	0.875	0.564444444444444	Overall, the competition for this subtask was very strong, and the scores are very close: the teams ranked 2-16 are within one point in the third decimal place, and those ranked 2-59 are within two absolute points in the second decimal place from the best team.	0
28029	28029	S20-12	Subtask A	8	128	1.0	0.568888888888889	All but one team beat the majority class baseline (we suspect that team might have accidentally flipped their predicted labels).	0
28112	28112	S20-12	Conclusion and Future Work	1	211	0.066666666666667	0.937777777777778	We presented the results of OffensEval-2020, which featured datasets in five languages: Arabic, Danish, English, Greek, and Turkish.	0
28113	28113	S20-12	Conclusion and Future Work	2	212	0.133333333333333	0.942222222222222	For English, we had three subtasks, representing the three levels of the OLID hierarchy.	0
28114	28114	S20-12	Conclusion and Future Work	3	213	0.2	0.946666666666667	For the other four languages, we had a subtask for the top-level of the OLID hierarchy only.	0
28115	28115	S20-12	Conclusion and Future Work	4	214	0.266666666666667	0.951111111111111	A total of 528 teams signed up to participate in OffensEval-2020, and 145 of them actually submitted results across all languages and subtasks.	0
28116	28116	S20-12	Conclusion and Future Work	5	215	0.333333333333333	0.955555555555556	Out of the 145 participating teams, 96 teams participated in one language only, 13 teams participated in two languages, 11 in three languages, 19 in four languages, and 6 teams submitted systems for all five languages.	0
28117	28117	S20-12	Conclusion and Future Work	6	216	0.4	0.96	The official submissions per language ranged from 37 (for Greek) to 81 (for English).	0
28118	28118	S20-12	Conclusion and Future Work	7	217	0.466666666666667	0.964444444444444	Finally, 70 of the 145 participating teams submitted system description papers, which is an all-time record.	0
28119	28119	S20-12	Conclusion and Future Work	8	218	0.533333333333333	0.968888888888889	The wide participation in the task allowed us to compare a number of approaches across different languages and datasets.	0
28120	28120	S20-12	Conclusion and Future Work	9	219	0.6	0.973333333333333	Similarly to OffensEval-2019, we observed that the best systems for all languages and subtasks used large-scale BERT-style pre-trained Transformers such as BERT, RoBERTa, and mBERT.	0
28121	28121	S20-12	Conclusion and Future Work	10	220	0.666666666666667	0.977777777777778	Unlike 2019, however, the multi-lingual nature of this year's data enabled cross-language approaches, which proved quite effective and were used by some of the top-ranked systems.	0
28122	28122	S20-12	Conclusion and Future Work	11	221	0.733333333333333	0.982222222222222	In future work, we plan to extend the task in several ways.	0
28123	28123	S20-12	Conclusion and Future Work	12	222	0.8	0.986666666666667	First, we want to offer subtasks B and C for all five languages from OffensEval-2020.	0
28124	28124	S20-12	Conclusion and Future Work	13	223	0.866666666666667	0.991111111111111	We further plan to add some additional languages, especially under-represented ones.	0
28125	28125	S20-12	Conclusion and Future Work	14	224	0.933333333333333	0.995555555555555	Other interesting aspects to explore are code-mixing, e.g., mixing Arabic script and Latin alphabet in the same Arabic message, and code-switching, e.g., mixing Arabic and English words and phrases in the same message.	0
28126	28126	S20-12	Conclusion and Future Work	15	225	1.0	1.0	Last but not least, we plan to cover a wider variety of social media platforms.	0
30743	30743	2020.nlptea-1.4	title	1	1	1.0	0.008264462809917	Overview of NLPTEA-2020 Shared Task for Chinese Grammatical Error Diagnosis	0
30744	30744	2020.nlptea-1.4	abstract	1	2	0.2	0.016528925619835	This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language.	1
30745	30745	2020.nlptea-1.4	abstract	2	3	0.4	0.024793388429752	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
30746	30746	2020.nlptea-1.4	abstract	3	4	0.6	0.03305785123967	Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs.	0
30747	30747	2020.nlptea-1.4	abstract	4	5	0.8	0.041322314049587	System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level.	0
30748	30748	2020.nlptea-1.4	abstract	5	6	1.0	0.049586776859504	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
30749	30749	2020.nlptea-1.4	Introduction	1	7	0.045454545454546	0.057851239669422	Automated grammar checking for learners of English as a foreign language has achieved obvious progress.	0
30750	30750	2020.nlptea-1.4	Introduction	2	8	0.090909090909091	0.066115702479339	Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011;	0
30751	30751	2020.nlptea-1.4	Introduction	3	9	0.136363636363636	0.074380165289256	Dale et al., 2012).	0
30752	30752	2020.nlptea-1.4	Introduction	4	10	0.181818181818182	0.082644628099174	The shared tasks at CoNLL 2013 and 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013;.	0
30753	30753	2020.nlptea-1.4	Introduction	5	11	0.227272727272727	0.090909090909091	Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language (CFL) learners.	0
30754	30754	2020.nlptea-1.4	Introduction	6	12	0.272727272727273	0.099173553719008	Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012;	0
30755	30755	2020.nlptea-1.4	Introduction	7	13	0.318181818181818	0.107438016528926	Wu et al, 2010;	0
30756	30756	2020.nlptea-1.4	Introduction	8	14	0.363636363636364	0.115702479338843	Yu and Chen, 2012), rule-based analysis (Lee et al., 2013), neuro network modelling (Zheng et al., 2016;	0
30757	30757	2020.nlptea-1.4	Introduction	9	15	0.409090909090909	0.12396694214876	Fu et al., 2018) and hybrid methods Zhou et al., 2017).	0
30758	30758	2020.nlptea-1.4	Introduction	10	16	0.454545454545455	0.132231404958678	In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL .	0
30759	30759	2020.nlptea-1.4	Introduction	11	17	0.5	0.140495867768595	A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015), COLING-2016.	0
30760	30760	2020.nlptea-1.4	Introduction	12	18	0.545454545454545	0.148760330578512	Its name was fixed from then on: Chinese Grammatical Error Diagnosis (CGED).	0
30761	30761	2020.nlptea-1.4	Introduction	13	19	0.590909090909091	0.15702479338843	As a part of IJCNLP 2017, the shared task was organized (Rao et al., 2017).	0
30762	30762	2020.nlptea-1.4	Introduction	14	20	0.636363636363636	0.165289256198347	In conjunction with NLP-TEA workshop in ACL 2018, CGED was organized again (Rao et al., 2018).	0
30763	30763	2020.nlptea-1.4	Introduction	15	21	0.681818181818182	0.173553719008264	The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results.	0
30764	30764	2020.nlptea-1.4	Introduction	16	22	0.727272727272727	0.181818181818182	Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task.	0
30765	30765	2020.nlptea-1.4	Introduction	17	23	0.772727272727273	0.190082644628099	The rest of this paper is organized as follows.	0
30766	30766	2020.nlptea-1.4	Introduction	18	24	0.818181818181818	0.198347107438017	Section 2 describes the task in detail.	0
30767	30767	2020.nlptea-1.4	Introduction	19	25	0.863636363636364	0.206611570247934	Section 3 introduces the constructed data sets.	0
30768	30768	2020.nlptea-1.4	Introduction	20	26	0.909090909090909	0.214876033057851	Section 4 proposes evaluation metrics.	0
30769	30769	2020.nlptea-1.4	Introduction	21	27	0.954545454545455	0.223140495867769	Section 5 reports the results of the participants' approaches.	0
30770	30770	2020.nlptea-1.4	Introduction	22	28	1.0	0.231404958677686	Conclusions are finally drawn in Section 6.	0
30771	30771	2020.nlptea-1.4	Task Description	1	29	0.1	0.239669421487603	The goal of this shared task is to develop NLP techniques to automatically diagnose (and furtherly correct) grammatical errors in Chinese sentences written by CFL learners.	0
30772	30772	2020.nlptea-1.4	Task Description	2	30	0.2	0.247933884297521	"Such errors are defined as PADS: redundant words (denoted as a capital ""R""), missing words (""M""), word selection errors (""S""), and word ordering errors (""W"")."	0
30773	30773	2020.nlptea-1.4	Task Description	3	31	0.3	0.256198347107438	The input sentence may contain one or more such errors.	0
30774	30774	2020.nlptea-1.4	Task Description	4	32	0.4	0.264462809917355	The developed system should indicate which error types are embedded in the given unit (containing 1 to 5 sentences) and the position at which they occur.	0
30775	30775	2020.nlptea-1.4	Task Description	5	33	0.5	0.272727272727273	"Each input unit is given a unique number ""sid""."	0
30776	30776	2020.nlptea-1.4	Task Description	6	34	0.6	0.28099173553719	"If the inputs contain no grammatical errors, the system should return: ""sid, correct""."	0
30777	30777	2020.nlptea-1.4	Task Description	7	35	0.7	0.289256198347107	"If an input unit contains the grammatical errors, the output format should include four items ""sid, start_off, end_off, error_type"", where start_off and end_off respectively denote the positions of starting and ending character at which the grammatical error occurs, and error_type should be one of the defined errors: ""R"", ""M"", ""S"", and ""W""."	0
30778	30778	2020.nlptea-1.4	Task Description	8	36	0.8	0.297520661157025	Each character or punctuation mark occupies 1 space for counting positions.	0
30779	30779	2020.nlptea-1.4	Task Description	9	37	0.9	0.305785123966942	Example sentences and corresponding notes are shown as Table 1 shows.	0
30780	30780	2020.nlptea-1.4	Task Description	10	38	1.0	0.314049586776859	This year, we only have one track of HSK.	0
30860	30860	2020.nlptea-1.4	Conclusion	1	118	0.25	0.975206611570248	This study describes the NLP-TEA 2020 shared task for Chinese grammatical error diagnosis, including task design, data preparation, performance metrics, and evaluation results.	0
30861	30861	2020.nlptea-1.4	Conclusion	2	119	0.5	0.983471074380165	Regardless of actual performance, all submissions contribute to the common effort to develop Chinese grammatical error diagnosis system, and the individual reports in the proceedings provide useful insights into computer-assisted language learning for CFL learners.	0
30862	30862	2020.nlptea-1.4	Conclusion	3	120	0.75	0.991735537190083	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
30863	30863	2020.nlptea-1.4	Conclusion	4	121	1.0	1.0	Therefore, all data sets with gold standards and scoring scripts are publicly available online at http://www.cged.science.	0
31208	31208	W18-3601	title	1	1	1.0	0.005291005291005	The First Multilingual Surface Realisation Shared Task (SR&apos;18): Overview and Evaluation Results	0
31209	31209	W18-3601	abstract	1	2	0.166666666666667	0.010582010582011	We report results from the SR'18 Shared Task, a new multilingual surface realisation task organised as part of the ACL'18 Workshop on Multilingual Surface Realisation.	0
31210	31210	W18-3601	abstract	2	3	0.333333333333333	0.015873015873016	As in its English-only predecessor task SR'11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed.	0
31211	31211	W18-3601	abstract	3	4	0.5	0.021164021164021	The shallow track was offered in ten, and the deep track in three languages.	0
31212	31212	W18-3601	abstract	4	5	0.666666666666667	0.026455026455027	Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity.	0
31213	31213	W18-3601	abstract	5	6	0.833333333333333	0.031746031746032	This report presents the evaluation results, along with descriptions of the SR'18 tracks, data and evaluation methods.	0
31214	31214	W18-3601	abstract	6	7	1.0	0.037037037037037	For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.	0
31390	31390	W18-3601	Conclusion	1	183	0.142857142857143	0.968253968253968	SR'18 was the second surface realisation shared task, and followed an earlier pilot task for English, SR'11.	0
31391	31391	W18-3601	Conclusion	2	184	0.285714285714286	0.973544973544973	Participation was high for a first instance   of a shared task, at least in the Shallow Track, indicating that interest is high enough to continue running it again next year to enable more teams to participate.	0
31392	31392	W18-3601	Conclusion	3	185	0.428571428571429	0.978835978835979	One important question that needs to be addressed is to what extent UDs are suitable inputs for NLG systems.	0
31393	31393	W18-3601	Conclusion	4	186	0.571428571428571	0.984126984126984	More specifically, can they reasonably be expected to be generated by other, content-determining, modules in an NLG system, do they provide all the information necessary to generate surface realisations, and if not, how can they be augmented to provide it.	0
31394	31394	W18-3601	Conclusion	5	187	0.714285714285714	0.989417989417989	We hope to discuss these and related issues with the research community as we prepare the next instance of the SR Task.	0
31395	31395	W18-3601	Conclusion	6	188	0.857142857142857	0.994708994708995	A goal to aim for may be to make it possible for different NLG components to be connected via standard interface representations, to increase re-usability for NLG components.	0
31396	31396	W18-3601	Conclusion	7	189	1.0	1.0	However, what may constitute a good interface representation for surface realisation remains far from clear.	0
31515	31515	W19-3203	title	1	1	0.5	0.007692307692308	Overview of the Fourth Social Media Mining for Health (#SMM4H)	0
31516	31516	W19-3203	title	2	2	1.0	0.015384615384615	Shared Task at ACL 2019	0
31517	31517	W19-3203	abstract	1	3	0.111111111111111	0.023076923076923	The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking on a regular basis 1 . Advances in automated data processing and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media.	0
31518	31518	W19-3203	abstract	2	4	0.222222222222222	0.030769230769231	We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users.	0
31519	31519	W19-3203	abstract	3	5	0.333333333333333	0.038461538461539	For the fourth execution of this challenge, we proposed four different tasks.	0
31520	31520	W19-3203	abstract	4	6	0.444444444444444	0.046153846153846	Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not.	1
31521	31521	W19-3203	abstract	5	7	0.555555555555556	0.053846153846154	Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs.	1
31522	31522	W19-3203	abstract	6	8	0.666666666666667	0.061538461538462	Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary.	1
31523	31523	W19-3203	abstract	7	9	0.777777777777778	0.069230769230769	Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one's health, a more general discussion of the health issue, or is an unrelated mention.	1
31524	31524	W19-3203	abstract	8	10	0.888888888888889	0.076923076923077	A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run.	0
31525	31525	W19-3203	abstract	9	11	1.0	0.084615384615385	We summarize here the corpora for this challenge which are freely available at https://competitions.codalab. org/competitions/22521, and present an overview of the methods and the results of the competing systems.	0
31526	31526	W19-3203	Introduction	1	12	0.037037037037037	0.092307692307692	The intent of the #SMM4H shared tasks series is to challenge the community with Natural Language Processing tasks for mining relevant data for health monitoring and surveillance in social media.	0
31527	31527	W19-3203	Introduction	2	13	0.074074074074074	0.1	Such challenges require processing imbalanced, noisy, real-world, and substantially creative language expressions from social media.	0
31528	31528	W19-3203	Introduction	3	14	0.111111111111111	0.107692307692308	The competing systems should be able to deal with many linguistic variations and semantic complexities in the various ways people express medication-related concepts and outcomes.	0
31529	31529	W19-3203	Introduction	4	15	0.148148148148148	0.115384615384615	It has been shown in past research (Liu et al., 2011;	0
31530	31530	W19-3203	Introduction	5	16	0.185185185185185	0.123076923076923	Giuseppe et al., 2017) that automated systems frequently under-perform when exposed to social media text because of the presence of novel/creative phrases, misspellings and frequent use of idiomatic, ambiguous and sarcastic expressions.	0
31531	31531	W19-3203	Introduction	6	17	0.222222222222222	0.130769230769231	The tasks act as a discovery and verification process of what approaches work best for social media data.	0
31532	31532	W19-3203	Introduction	7	18	0.259259259259259	0.138461538461538	As in previous years, our tasks focused on mining health information from Twitter.	0
31533	31533	W19-3203	Introduction	8	19	0.296296296296296	0.146153846153846	This year we challenged the community with two different problems.	0
31534	31534	W19-3203	Introduction	9	20	0.333333333333333	0.153846153846154	The first problem focuses on performing pharmacovigilance from social media data.	0
31535	31535	W19-3203	Introduction	10	21	0.37037037037037	0.161538461538462	It is now well understood that social media data may contain reports of adverse drug reactions (ADRs) and these reports may complement traditional adverse event reporting systems, such as the FDA adverse event reporting system (FAERS).	0
31536	31536	W19-3203	Introduction	11	22	0.407407407407407	0.169230769230769	However, automatically curating reports from adverse reactions from Twitter requires the application of a series of NLP methods in an end-to-end pipeline .	0
31537	31537	W19-3203	Introduction	12	23	0.444444444444444	0.176923076923077	The first three tasks of this year's challenge represent three key NLP problems in a social media based pharmacovigilance pipeline -(i) automatic classification of ADRs, (ii) extraction of spans of ADRs and (iii) normal-ization of the extracted ADRs to standardized IDs.	0
31538	31538	W19-3203	Introduction	13	24	0.481481481481481	0.184615384615385	The second problem explores the generalizability of predictive models.	0
31539	31539	W19-3203	Introduction	14	25	0.518518518518518	0.192307692307692	In health research using social media, it is often necessary for researchers to build individual classifiers to identify health mentions of a particular disease in a particular context.	0
31540	31540	W19-3203	Introduction	15	26	0.555555555555556	0.2	Classification models that can generalize to different health contexts would be greatly beneficial to researchers in these fields (e.g., (Payam and Eugene, 2018)), as this would allow researchers to more easily apply existing tools and resources to new problems.	0
31541	31541	W19-3203	Introduction	16	27	0.592592592592593	0.207692307692308	Motivated by these ideas, Task 4 was testing tweet classification methods across diverse health contexts, so the test data included a very different health context than the training data.	0
31542	31542	W19-3203	Introduction	17	28	0.62962962962963	0.215384615384615	This setting measures the ability of tweet classifiers to generalize across health contexts.	0
31543	31543	W19-3203	Introduction	18	29	0.666666666666667	0.223076923076923	The fourth iteration of our series follows the same organization as previous iterations.	0
31544	31544	W19-3203	Introduction	19	30	0.703703703703704	0.230769230769231	We collected posts from Twitter, annotated the data for the four tasks proposed and released the posts to the registered teams.	0
31545	31545	W19-3203	Introduction	20	31	0.740740740740741	0.238461538461538	This year, we conducted the evaluation of all participating systems using Codalab, an open source platform facilitating data science competitions.	0
31546	31546	W19-3203	Introduction	21	32	0.777777777777778	0.246153846153846	The performances of the systems were compared on a blind evaluations sets for each task.	0
31547	31547	W19-3203	Introduction	22	33	0.814814814814815	0.253846153846154	All teams registered were allowed to participate to one or multiple tasks.	0
31548	31548	W19-3203	Introduction	23	34	0.851851851851852	0.261538461538462	We provided the participants with two sets of data for each task, a training and a test set.	0
31549	31549	W19-3203	Introduction	24	35	0.888888888888889	0.269230769230769	Participants had a period of six weeks, from March 5 th to April 15 th , for training their systems on our training sets, and 4 days, from the 16 th to 20 th of April, for calibrating their systems on our test sets and submitting their predictions.	0
31550	31550	W19-3203	Introduction	25	36	0.925925925925926	0.276923076923077	In total 34 teams registered and 19 teams submitted at least one run (each team was allowed to submit, at most, three runs per task).	0
31551	31551	W19-3203	Introduction	26	37	0.962962962962963	0.284615384615385	In detail, we received 43 runs for task 1, 24 for task 2, 10 for task 3 and 15 for task 4.	0
31552	31552	W19-3203	Introduction	27	38	1.0	0.292307692307692	We briefly describe each task and their data in section 2, before discussing the results obtained in section 3.	0
31639	31639	W19-3203	Conclusion	1	125	0.166666666666667	0.961538461538462	In this paper we presented an overview of the results of #SMM4H 2019 which focuses on a) the resolution of adverse drug reaction (ADR) mentioned in Twitter and b) the distinction between tweets reporting personal health status form opinions across different health domains.	0
31640	31640	W19-3203	Conclusion	2	126	0.333333333333333	0.969230769230769	With a total of 92 runs submitted by 19 teams, the challenge was well attended.	0
31641	31641	W19-3203	Conclusion	3	127	0.5	0.976923076923077	The participants, in large part, opted for neural architectures and integrated pretrained word-embedding sensitive to their contexts based on the recent Bidirectional Encoder Representations from Transformers.	0
31642	31642	W19-3203	Conclusion	4	128	0.666666666666667	0.984615384615385	Such architectures were the most efficient on our four tasks.	0
31643	31643	W19-3203	Conclusion	5	129	0.833333333333333	0.992307692307692	Results on tasks 1-3 show that, despite a continuous improvement of performances in the detection of tweets mentioning ADRs over the past years, their end-to-end resolution still remain a major challenge for the community and an opportunity for further research.	0
31644	31644	W19-3203	Conclusion	6	130	1.0	1.0	Results of task 4 were more encouraging, with systems able to generalized their predictions over domains not present in their training data.	0
32732	32732	2020.sigmorphon-1.1	title	1	1	1.0	0.002724795640327	SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection	0
32733	32733	2020.sigmorphon-1.1	abstract	1	2	0.111111111111111	0.005449591280654	A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language.	0
32734	32734	2020.sigmorphon-1.1	abstract	2	3	0.222222222222222	0.008174386920981	Most systems, however, are developed using data from just one language such as English.	0
32735	32735	2020.sigmorphon-1.1	abstract	3	4	0.333333333333333	0.010899182561308	The SIG-MORPHON 2020 shared task on morphological reinflection aims to investigate systems' ability to generalize across typologically distinct languages, many of which are low resource.	0
32736	32736	2020.sigmorphon-1.1	abstract	4	5	0.444444444444444	0.013623978201635	Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages.	0
32737	32737	2020.sigmorphon-1.1	abstract	5	6	0.555555555555556	0.016348773841962	A total of 22 systems (19 neural) from 10 teams were submitted to the task.	0
32738	32738	2020.sigmorphon-1.1	abstract	6	7	0.666666666666667	0.019073569482289	All four winning systems were neural (two monolingual transformers and two massively multilingual RNNbased models with gated attention).	0
32739	32739	2020.sigmorphon-1.1	abstract	7	8	0.777777777777778	0.021798365122616	Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages.	0
32740	32740	2020.sigmorphon-1.1	abstract	8	9	0.888888888888889	0.024523160762943	Nonneural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data.	0
32741	32741	2020.sigmorphon-1.1	abstract	9	10	1.0	0.02724795640327	Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.	0
32742	32742	2020.sigmorphon-1.1	Introduction	1	11	0.03030303030303	0.029972752043597	Human language is marked by considerable diversity around the world.	0
32743	32743	2020.sigmorphon-1.1	Introduction	2	12	0.060606060606061	0.032697547683924	Though the world's languages share many basic attributes (e.g., Swadesh, 1950 and more recently, List et al., 2016), grammatical features, and even abstract implications (proposed in Greenberg, 1963), each language nevertheless has a unique evolutionary trajectory that is affected by geographic, social, cultural, and other factors.	0
32744	32744	2020.sigmorphon-1.1	Introduction	3	13	0.090909090909091	0.035422343324251	As a result, the surface form of languages varies substantially.	0
32745	32745	2020.sigmorphon-1.1	Introduction	4	14	0.121212121212121	0.038147138964578	The morphology of languages can differ in many ways:	0
32746	32746	2020.sigmorphon-1.1	Introduction	5	15	0.151515151515152	0.040871934604905	"Some exhibit rich grammatical case systems (e.g., 12 in Erzya and 24 in Veps) and mark possessiveness, others might have complex verbal morphology (e.g., Oto-Manguean languages; Palancar and L√©onard, 2016) or even ""decline"" nouns for tense (e.g., Tupi-Guarani languages)."	0
32747	32747	2020.sigmorphon-1.1	Introduction	6	16	0.181818181818182	0.043596730245232	Linguistic typology is the discipline that studies these variations by means of a systematic comparison of languages (Croft, 2002;	0
32748	32748	2020.sigmorphon-1.1	Introduction	7	17	0.212121212121212	0.046321525885559	Comrie, 1989).	0
32749	32749	2020.sigmorphon-1.1	Introduction	8	18	0.242424242424242	0.049046321525886	Typologists have defined several dimensions of morphological variation to classify and quantify the degree of crosslinguistic variation.	0
32750	32750	2020.sigmorphon-1.1	Introduction	9	19	0.272727272727273	0.051771117166213	This comparison can be challenging as the categories are based on studies of known languages and are progressively refined with documentation of new languages (Haspelmath, 2007).	0
32751	32751	2020.sigmorphon-1.1	Introduction	10	20	0.303030303030303	0.05449591280654	Nevertheless, to understand the potential range of morphological variation, we take a closer look at three dimensions here: fusion, inflectional synthesis, and position of case affixes (Dryer and Haspelmath, 2013).	0
32752	32752	2020.sigmorphon-1.1	Introduction	11	21	0.333333333333333	0.057220708446867	Fusion, our first dimension of variation, refers to the degree to which morphemes bind to one another in a phonological word (Bickel and Nichols, 2013b).	0
32753	32753	2020.sigmorphon-1.1	Introduction	12	22	0.363636363636364	0.059945504087194	Languages range from strictly isolating (i.e., each morpheme is its own phonological word) to concatenative (i.e., morphemes bind together within a phonological word); nonlinearities such as ablaut or tonal morphology can also be present.	0
32754	32754	2020.sigmorphon-1.1	Introduction	13	23	0.393939393939394	0.062670299727521	From a geographic perspective, isolating languages are found in the Sahel Belt in West Africa, Southeast Asia and the Pacific.	0
32755	32755	2020.sigmorphon-1.1	Introduction	14	24	0.424242424242424	0.065395095367847	Ablaut-concatenative morphology and tonal morphology can be found in African languages.	0
32756	32756	2020.sigmorphon-1.1	Introduction	15	25	0.454545454545455	0.068119891008174	Tonal-concatenative morphology can be found in Mesoamerican languages (e.g., Oto-Manguean).	0
32757	32757	2020.sigmorphon-1.1	Introduction	16	26	0.484848484848485	0.070844686648501	Concatenative morphology is the most common system and can be found around the world.	0
32758	32758	2020.sigmorphon-1.1	Introduction	17	27	0.515151515151515	0.073569482288828	Inflectional synthesis, the second dimension considered, refers to whether grammatical categories like tense, voice or agreement are expressed as affixes (synthetic) or individual words (analytic) (Bickel and Nichols, 2013c).	0
32759	32759	2020.sigmorphon-1.1	Introduction	18	28	0.545454545454545	0.076294277929155	Analytic expressions are common in Eurasia (except the Pacific Rim, and the Himalaya and Caucasus mountain ranges), whereas synthetic expressions are used to a high degree in the Americas.	0
32760	32760	2020.sigmorphon-1.1	Introduction	19	29	0.575757575757576	0.079019073569482	Finally, affixes can variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013).	0
32761	32761	2020.sigmorphon-1.1	Introduction	20	30	0.606060606060606	0.081743869209809	Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013).	0
32762	32762	2020.sigmorphon-1.1	Introduction	21	31	0.636363636363636	0.084468664850136	In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead.	0
32763	32763	2020.sigmorphon-1.1	Introduction	22	32	0.666666666666667	0.087193460490463	These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable.	0
32764	32764	2020.sigmorphon-1.1	Introduction	23	33	0.696969696969697	0.08991825613079	Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging.	0
32765	32765	2020.sigmorphon-1.1	Introduction	24	34	0.727272727272727	0.092643051771117	As Bender (2009	0
32766	32766	2020.sigmorphon-1.1	Introduction	25	35	0.757575757575758	0.095367847411444	Bender ( , 2016 notes, many current architectures and training and tuning algorithms still present language-specific biases.	0
32767	32767	2020.sigmorphon-1.1	Introduction	26	36	0.787878787878788	0.098092643051771	The most commonly used language for developing NLP applications is English.	0
32768	32768	2020.sigmorphon-1.1	Introduction	27	37	0.818181818181818	0.100817438692098	Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology.	0
32769	32769	2020.sigmorphon-1.1	Introduction	28	38	0.848484848484848	0.103542234332425	With respect to languages that exhibit inflectional morphology, English is relatively impoverished.	0
32770	32770	2020.sigmorphon-1.1	Introduction	29	39	0.878787878787879	0.106267029972752	1 Importantly, English is just one morphological system among many.	0
32771	32771	2020.sigmorphon-1.1	Introduction	30	40	0.909090909090909	0.108991825613079	A larger goal of natural language processing is that the system work for any presented language.	0
32772	32772	2020.sigmorphon-1.1	Introduction	31	41	0.939393939393939	0.111716621253406	If an NLP system is trained on just one language, it could be missing important flexibility in its ability to account for cross-linguistic morphological variation.	0
32773	32773	2020.sigmorphon-1.1	Introduction	32	42	0.96969696969697	0.114441416893733	In this year's iteration of the SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems' ability to generalize across typologically distinct languages many of which are low-resource.	0
32774	32774	2020.sigmorphon-1.1	Introduction	33	43	1.0	0.11716621253406	"For example, if a neural network architecture works well for a sample of Indo-European languages, should the same architecture also work well for Tupi-Guarani languages (where nouns are ""declined"" for tense) or Austronesian languages (where verbal morphology is frequently prefixing)?"	0
32775	32775	2020.sigmorphon-1.1	Task Description	1	44	0.066666666666667	0.119891008174387	The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form.	1
32776	32776	2020.sigmorphon-1.1	Task Description	2	45	0.133333333333333	0.122615803814714	For each language we provide a separate training, development, and test set.	0
32777	32777	2020.sigmorphon-1.1	Task Description	3	46	0.2	0.125340599455041	"More historically, all of these tasks resemble the classic ""wug""-test that Berko (1958) developed to test child and human knowledge of English nominal morphology."	0
32778	32778	2020.sigmorphon-1.1	Task Description	4	47	0.266666666666667	0.128065395095368	Unlike the task from earlier years, this year's task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces previously unseen data.	0
32779	32779	2020.sigmorphon-1.1	Task Description	5	48	0.333333333333333	0.130790190735695	The task starts with the Development Phase, which was an elongated period of time (about two months), during which participants develop a model of morphological inflection.	0
32780	32780	2020.sigmorphon-1.1	Task Description	6	49	0.4	0.133514986376022	In this phase, we provide training and development splits for 45 languages representing the Austronesian, Niger-Congo, Oto-Manguean, Uralic and Indo-European language families.	0
32781	32781	2020.sigmorphon-1.1	Task Description	7	50	0.466666666666667	0.136239782016349	Table 1 provides details on the languages.	0
32782	32782	2020.sigmorphon-1.1	Task Description	8	51	0.533333333333333	0.138964577656676	The Generalization Phase is a short period of time (it started about a week before the Evaluation Phase) during which participants fine-tune their models on new data.	0
32783	32783	2020.sigmorphon-1.1	Task Description	9	52	0.6	0.141689373297003	At the start of the phase, we provide training and development splits for 45 new languages where approximately half are genetically related (belong to the same family) and half are genetically unrelated (are isolates or belong to a different family) to the languages presented in the Development Phase.	0
32784	32784	2020.sigmorphon-1.1	Task Description	10	53	0.666666666666667	0.14441416893733	More specifically, we introduce (surprise) languages from Afro-Asiatic, Algic, Dravidian, Indo-European, Niger-Congo, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan families.	0
32785	32785	2020.sigmorphon-1.1	Task Description	11	54	0.733333333333333	0.147138964577657	See Table 2 for more details.	0
32786	32786	2020.sigmorphon-1.1	Task Description	12	55	0.8	0.149863760217984	Finally, test splits for all 90 languages are released in the Evaluation Phase.	0
32787	32787	2020.sigmorphon-1.1	Task Description	13	56	0.866666666666667	0.152588555858311	During this phase, the models are evaluated on held-out forms.	0
32788	32788	2020.sigmorphon-1.1	Task Description	14	57	0.933333333333333	0.155313351498638	Importantly, the languages from both previous phases are evaluated simultaneously.	0
32789	32789	2020.sigmorphon-1.1	Task Description	15	58	1.0	0.158038147138965	This way, we evaluate the extent to which models (especially those with shared parameters) overfit to the development data: a model based on the morphological patterning of the Indo-European languages may end up with a bias towards suffixing and will struggle to learn prefixing or infixation.	0
33088	33088	2020.sigmorphon-1.1	Conclusion	1	357	0.090909090909091	0.97275204359673	This years's shared task on morphological reinflection focused on building models that could generalize across an extremely typologically diverse set of languages, many from understudied language families and with limited available text resources.	0
33089	33089	2020.sigmorphon-1.1	Conclusion	2	358	0.181818181818182	0.975476839237057	As in previous years, neural models performed well, even in relatively low-resource cases.	0
33090	33090	2020.sigmorphon-1.1	Conclusion	3	359	0.272727272727273	0.978201634877384	Submissions were able to make productive use of multilingual training to take advantage of commonalities across languages in the dataset.	0
33091	33091	2020.sigmorphon-1.1	Conclusion	4	360	0.363636363636364	0.980926430517711	Data augmentation techniques such as hallucination helped fill in the gaps and allowed networks to generalize to unseen inputs.	0
33092	33092	2020.sigmorphon-1.1	Conclusion	5	361	0.454545454545455	0.983651226158038	These techniques, combined with architecture tweaks like sparsemax, resulted in excellent overall performance on many languages (over 90% accuracy on average).	0
33093	33093	2020.sigmorphon-1.1	Conclusion	6	362	0.545454545454545	0.986376021798365	However, the task's focus on typological diversity revealed that some morphology types and language families (Tungusic, Oto-Manguean, South-ern Daly) remain a challenge for even the best systems.	0
33094	33094	2020.sigmorphon-1.1	Conclusion	7	363	0.636363636363636	0.989100817438692	These families are extremely low-resource, represented in this dataset by few or a single language.	0
33095	33095	2020.sigmorphon-1.1	Conclusion	8	364	0.727272727272727	0.991825613079019	This makes cross-linguistic transfer of similarities by multilanguage training less viable.	0
33096	33096	2020.sigmorphon-1.1	Conclusion	9	365	0.818181818181818	0.994550408719346	They may also have morphological properties and rules (e.g., Evenki is agglutinating with many possible forms for each lemma) that are particularly difficult for machine learners to induce automatically from sparse data.	0
33097	33097	2020.sigmorphon-1.1	Conclusion	10	366	0.909090909090909	0.997275204359673	For some languages (Ingrian, Tajik, Tagalog, Zarma, and Lingala), optimal performance was only achieved in this shared task by hand-encoding linguist knowledge in finite state grammars.	0
33098	33098	2020.sigmorphon-1.1	Conclusion	11	367	1.0	1.0	It is up to future research to imbue models with the right kinds of linguistic inductive biases to overcome these challenges.	0
38129	38129	K15-2001	title	1	1	1.0	0.003322259136213	The CoNLL-2015 Shared Task on Shallow Discourse Parsing	0
38130	38130	K15-2001	abstract	1	2	0.125	0.006644518272425	The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text.	0
38131	38131	K15-2001	abstract	2	3	0.25	0.009966777408638	A discourse relation can be expressed explicitly or implicitly, and takes two arguments realized as sentences, clauses, or in some rare cases, phrases.	0
38132	38132	K15-2001	abstract	3	4	0.375	0.013289036544851	Sixteen teams from three continents participated in this task.	0
38133	38133	K15-2001	abstract	4	5	0.5	0.016611295681063	For the first time in the history of the CoNLL shared tasks, participating teams, instead of running their systems on the test set and submitting the output, were asked to deploy their systems on a remote virtual machine and use a web-based evaluation platform to run their systems on the test set.	0
38134	38134	K15-2001	abstract	5	6	0.625	0.019933554817276	This meant they were unable to actually see the data set, thus preserving its integrity and ensuring its replicability.	0
38135	38135	K15-2001	abstract	6	7	0.75	0.023255813953488	In this paper, we present the task definition, the training and test sets, and the evaluation protocol and metric used during this shared task.	0
38136	38136	K15-2001	abstract	7	8	0.875	0.026578073089701	We also summarize the different approaches adopted by the participating teams, and present the evaluation results.	0
38137	38137	K15-2001	abstract	8	9	1.0	0.029900332225914	The evaluation data sets and the scorer will serve as a benchmark for future research on shallow discourse parsing.	0
38138	38138	K15-2001	Introduction	1	10	0.027027027027027	0.033222591362126	The shared task for the Nineteenth Conference on Computational Natural Language Learning (CoNLL-2015) is on Shallow Discourse Parsing (SDP).	0
38139	38139	K15-2001	Introduction	2	11	0.054054054054054	0.036544850498339	In the course of the sixteen CoNLL shared tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011;Pradhan et al., 2012).	0
38140	38140	K15-2001	Introduction	3	12	0.081081081081081	0.039867109634552	The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012).	0
38141	38141	K15-2001	Introduction	4	13	0.108108108108108	0.043189368770764	Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text.	0
38142	38142	K15-2001	Introduction	5	14	0.135135135135135	0.046511627906977	Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012;	0
38143	38143	K15-2001	Introduction	6	15	0.162162162162162	0.049833887043189	Webber et al., 2012;Prasad and Bunt, 2015).	0
38144	38144	K15-2001	Introduction	7	16	0.189189189189189	0.053156146179402	For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008;	0
38145	38145	K15-2001	Introduction	8	17	0.216216216216216	0.056478405315615	Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them.	0
38146	38146	K15-2001	Introduction	9	18	0.243243243243243	0.059800664451827	For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.	0
38147	38147	K15-2001	Introduction	10	19	0.27027027027027	0.06312292358804	1	0
38148	38148	K15-2001	Introduction	11	20	0.297297297297297	0.066445182724253	The necessary conditions are also in place for such a task.	0
38149	38149	K15-2001	Introduction	12	21	0.324324324324324	0.069767441860465	The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008;	0
38150	38150	K15-2001	Introduction	13	22	0.351351351351351	0.073089700996678	Duverle and Prendinger, 2009;	0
38151	38151	K15-2001	Introduction	14	23	0.378378378378378	0.07641196013289	Lin et al., 2009;	0
38152	38152	K15-2001	Introduction	15	24	0.405405405405405	0.079734219269103	Pitler et al., 2009;	0
38153	38153	K15-2001	Introduction	16	25	0.432432432432432	0.083056478405316	Subba and Di Eugenio, 2009;	0
38154	38154	K15-2001	Introduction	17	26	0.45945945945946	0.086378737541528	Zhou et al., 2010;	0
38155	38155	K15-2001	Introduction	18	27	0.486486486486487	0.089700996677741	Feng and Hirst, 2012;	0
38156	38156	K15-2001	Introduction	19	28	0.513513513513513	0.093023255813954	Ghosh et al., 2012;	0
38157	38157	K15-2001	Introduction	20	29	0.540540540540541	0.096345514950166	Park and Cardie, 2012;	0
38158	38158	K15-2001	Introduction	21	30	0.567567567567568	0.099667774086379	Wang et al., 2012;Biran and McKeown, 2013;	0
38159	38159	K15-2001	Introduction	22	31	0.594594594594595	0.102990033222591	Lan et al., 2013;	0
38160	38160	K15-2001	Introduction	23	32	0.621621621621622	0.106312292358804	Feng and Hirst, 2014;	0
38161	38161	K15-2001	Introduction	24	33	0.648648648648649	0.109634551495017	Ji and Eisenstein, 2014;	0
38162	38162	K15-2001	Introduction	25	34	0.675675675675676	0.112956810631229	Li and Nenkova, 2014;	0
38163	38163	K15-2001	Introduction	26	35	0.702702702702703	0.116279069767442	Lin et al., 2014;	0
38164	38164	K15-2001	Introduction	27	36	0.72972972972973	0.119601328903655	Rutherford and Xue, 2014), and the momentum is building.	0
38165	38165	K15-2001	Introduction	28	37	0.756756756756757	0.122923588039867	Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference.	0
38166	38166	K15-2001	Introduction	29	38	0.783783783783784	0.12624584717608	The resurgence of deep learning techniques opens the door for innovative approaches to this problem.	0
38167	38167	K15-2001	Introduction	30	39	0.810810810810811	0.129568106312292	"A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of ""standard"" feature-based learning techniques and ""deep"" representation learning techniques."	0
38168	38168	K15-2001	Introduction	31	40	0.837837837837838	0.132890365448505	The rest of this overview paper is structured as follows.	0
38169	38169	K15-2001	Introduction	32	41	0.864864864864865	0.136212624584718	In Section 2, we provide a concise definition of the shared task.	0
38170	38170	K15-2001	Introduction	33	42	0.891891891891892	0.13953488372093	We describe how the training and test data are prepared in Section 3.	0
38171	38171	K15-2001	Introduction	34	43	0.918918918918919	0.142857142857143	In Section 4, we present the evaluation protocol, metric and scorer.	0
38172	38172	K15-2001	Introduction	35	44	0.945945945945946	0.146179401993355	The different approaches that participants took in the shared task are summarized in Section 5.	0
38173	38173	K15-2001	Introduction	36	45	0.972972972972973	0.149501661129568	In Section 6, we present the ranking of participating systems and analyze the evaluation results.	0
38174	38174	K15-2001	Introduction	37	46	1.0	0.152823920265781	We present our conclusions in Section 7.	0
38175	38175	K15-2001	Task Definition	1	47	0.1	0.156146179401993	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
38176	38176	K15-2001	Task Definition	2	48	0.2	0.159468438538206	Specifically, given a newswire article as input, a participating system is asked to return a set of discourse relations contained in the text.	1
38177	38177	K15-2001	Task Definition	3	49	0.3	0.162790697674419	A discourse relation, as defined in the PDTB, from which the training data for the shared task is drawn, is a relation taking two abstract objects (events, states, facts, or propositions) as arguments.	1
38178	38178	K15-2001	Task Definition	4	50	0.4	0.166112956810631	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between abstract object units.	0
38179	38179	K15-2001	Task Definition	5	51	0.5	0.169435215946844	In the current version of the PDTB, non-explicit relations are inferred only between adjacent units.	0
38180	38180	K15-2001	Task Definition	6	52	0.6	0.172757475083056	Each discourse relation is labeled with a sense selected from a sense hierarchy, and its arguments are generally in the form of sentences, clauses, or in some rare cases, noun phrases.	0
38181	38181	K15-2001	Task Definition	7	53	0.7	0.176079734219269	To detect a discourse relation, a participating system needs to:	0
38182	38182	K15-2001	Task Definition	8	54	0.8	0.179401993355482	1. Identify the text span of an explicit discourse connective, if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments;	0
38183	38183	K15-2001	Task Definition	9	55	0.9	0.182724252491694	"4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
38184	38184	K15-2001	Task Definition	10	56	1.0	0.186046511627907	3 Data	0
38419	38419	K15-2001	Conclusions	1	291	0.090909090909091	0.966777408637874	Sixteen teams from three continents participated in the CoNLL-2015 Shared Task on shallow dis-	0
38420	38420	K15-2001	Conclusions	2	292	0.181818181818182	0.970099667774086	The rows are sorted by the parser performance of the participating systems on the Explicit task.	0
38421	38421	K15-2001	Conclusions	3	293	0.272727272727273	0.973421926910299	The Column O, E, I refer to official, Explicit and Non-Explicit task ranks respectively.	0
38422	38422	K15-2001	Conclusions	4	294	0.363636363636364	0.976744186046512	The blue highlighted rows indicate participants that did not attempt the Non-Explicit relation subtask.	0
38423	38423	K15-2001	Conclusions	5	295	0.454545454545455	0.980066445182724	The green highlighted row shows a team that probably overfitted the development set.	0
38424	38424	K15-2001	Conclusions	6	296	0.545454545454545	0.983388704318937	Finally, the red highlighted row indicates a team that possibly focused on the Explicit relations task and even though their overall rank was lower, they did very well on the Explicit relations subtask.	0
38425	38425	K15-2001	Conclusions	7	297	0.636363636363636	0.98671096345515	This is also the system that did not submit a paper, so we do not know more details.	0
38426	38426	K15-2001	Conclusions	8	298	0.727272727272727	0.990033222591362	course parsing.	0
38427	38427	K15-2001	Conclusions	9	299	0.818181818181818	0.993355481727575	The shared task required the development of an end-to-end system, and the best system achieved an F1 score of 24.0% on the blind test set, reflecting the serious error propagation problem in such a system.	0
38428	38428	K15-2001	Conclusions	10	300	0.909090909090909	0.996677740863787	The shared task exposed the most challenging aspect of shallow discourse parsing as a research problem, helping future research better calibrate their efforts.	0
38429	38429	K15-2001	Conclusions	11	301	1.0	1.0	The evaluation data sets and the scorer we prepared for the shared task will be a useful benchmark for future research on shallow discourse parsing.	0
42557	42557	D19-5719	title	1	1	1.0	0.005025125628141	Bacteria Biotope at BioNLP Open Shared Tasks 2019	0
42558	42558	D19-5719	abstract	1	2	0.2	0.010050251256281	This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019.	0
42559	42559	D19-5719	abstract	2	3	0.4	0.015075376884422	The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology).	1
42560	42560	D19-5719	abstract	3	4	0.6	0.020100502512563	The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology.	0
42561	42561	D19-5719	abstract	4	5	0.8	0.025125628140704	The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization.	0
42562	42562	D19-5719	abstract	5	6	1.0	0.030150753768844	We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.	0
42563	42563	D19-5719	Introduction	1	7	0.0625	0.035175879396985	In this paper, we present the fourth edition 1 of the Bacteria Biotope (BB) task.	0
42564	42564	D19-5719	Introduction	2	8	0.125	0.040201005025126	The task was introduced in 2011.	0
42565	42565	D19-5719	Introduction	3	9	0.1875	0.045226130653266	It has the ambition of promoting large-scale information extraction (IE) from scientific documents in order to automatically fill knowledge bases in the microbial diversity field (Bossy et al., 2012).	0
42566	42566	D19-5719	Introduction	4	10	0.25	0.050251256281407	BB 2019 is part of BioNLP Open Shared Tasks 2019 2 . BioNLP-OST is a community-wide effort for the comparison and evaluation of biomedical text mining technologies on manually curated benchmarks.	0
42567	42567	D19-5719	Introduction	5	11	0.3125	0.055276381909548	A large amount of information about microbes and their properties that is critical for microbiology research and development is scattered among millions of publications and databases .	0
42568	42568	D19-5719	Introduction	6	12	0.375	0.060301507537689	Information extraction as framed by the Bacteria Biotope task identifies relevant entities and interrelationships in the text and map them to reference categories from existing knowledge resources.	0
42569	42569	D19-5719	Introduction	7	13	0.4375	0.065326633165829	This information can thus be combined with information from other sources referring to the same knowledge resources.	0
42570	42570	D19-5719	Introduction	8	14	0.5	0.07035175879397	The knowledge resources used in the BB task are the NCBI taxonomy 3  (Federhen, 2011) for microbial taxa and the OntoBiotope ontology 4 (N√©dellec et al., 2018) for microbial habitats and phenotypes.	0
42571	42571	D19-5719	Introduction	9	15	0.5625	0.075376884422111	The large size of these resources relative to the small number of training examples reflects the real conditions of IE application development, whilst it challenges current IE methods.	0
42572	42572	D19-5719	Introduction	10	16	0.625	0.080402010050251	The lexical richness of the two resources partially offsets the difficulty.	0
42573	42573	D19-5719	Introduction	11	17	0.6875	0.085427135678392	Compared to the 2016 corpus that contained only scientific paper abstracts from the PubMed database (Del√©ger et al., 2016), the 2019 corpus is enriched with extracts from full-text articles.	0
42574	42574	D19-5719	Introduction	12	18	0.75	0.090452261306533	We introduced a new entity type (phenotype) and a new relation type (linking microorganisms and phenotypes).	0
42575	42575	D19-5719	Introduction	13	19	0.8125	0.095477386934673	Phenotypes are observable characteristics such as morphology, or environment requirement (e.g. acidity, oxygen).	0
42576	42576	D19-5719	Introduction	14	20	0.875	0.100502512562814	It is very valuable information for studying the ability of a given microbe to adapt to an environment (Brbiƒá et al., 2016).	0
42577	42577	D19-5719	Introduction	15	21	0.9375	0.105527638190955	The definition of microorganism phenotype in the OntoBiotope ontology includes host interaction characteristics (e.g. symbiont) and community behavior and growth habit (e.g. epilithic).	0
42578	42578	D19-5719	Introduction	16	22	1.0	0.110552763819095	The task organization and the evaluation metrics remain unchanged.	0
42579	42579	D19-5719	Task Description	1	23	0.047619047619048	0.115577889447236	The representation scheme of the Bacteria Biotope task contains four entity types:	0
42580	42580	D19-5719	Task Description	2	24	0.095238095238095	0.120603015075377	‚Ä¢ Microorganism: names denoting microorganism taxa.	0
42581	42581	D19-5719	Task Description	3	25	0.142857142857143	0.125628140703518	These taxa correspond to microorganism branches of the NCBI taxon-omy.	0
42582	42582	D19-5719	Task Description	4	26	0.19047619047619	0.130653266331658	The set of relevant taxa is given on the BB task website.	0
42583	42583	D19-5719	Task Description	5	27	0.238095238095238	0.135678391959799	‚Ä¢ Habitat: phrases denoting physical places where microorganisms may be observed;	0
42584	42584	D19-5719	Task Description	6	28	0.285714285714286	0.14070351758794	‚Ä¢ Geographical: names of geographical places;	0
42585	42585	D19-5719	Task Description	7	29	0.333333333333333	0.14572864321608	‚Ä¢ Phenotype: expressions describing microbial characteristics.	0
42586	42586	D19-5719	Task Description	8	30	0.380952380952381	0.150753768844221	The scheme defines two relation types:	0
42587	42587	D19-5719	Task Description	9	31	0.428571428571429	0.155778894472362	‚Ä¢	0
42588	42588	D19-5719	Task Description	10	32	0.476190476190476	0.160804020100503	Lives in relations which link a microorganism entity to its location (either a habitat or a geographical entity, or in few rare cases a microorganism entity);	0
42589	42589	D19-5719	Task Description	11	33	0.523809523809524	0.165829145728643	‚Ä¢	0
42590	42590	D19-5719	Task Description	12	34	0.571428571428571	0.170854271356784	Exhibits relations which link a microorganism entity to a phenotype entity.	0
42591	42591	D19-5719	Task Description	13	35	0.619047619047619	0.175879396984925	Arguments of relations may occur in different sentences.	0
42592	42592	D19-5719	Task Description	14	36	0.666666666666667	0.180904522613065	In addition, microorganisms are normalized to taxa from the NCBI taxonomy.	0
42593	42593	D19-5719	Task Description	15	37	0.714285714285714	0.185929648241206	Habitat and phenotype entities are normalized to concepts from the OntoBiotope ontology.	0
42594	42594	D19-5719	Task Description	16	38	0.761904761904762	0.190954773869347	We used the BioNLP-OST-2019 version of OntoBiotope available on AgroPortal 5 .	0
42595	42595	D19-5719	Task Description	17	39	0.80952380952381	0.195979899497487	We used the NCBI Taxonomy version as available on February 2, 2019 from NCBI website 6 . Copies of both resources can be downloaded from the task website.	0
42596	42596	D19-5719	Task Description	18	40	0.857142857142857	0.201005025125628	The microorganism part of the taxonomy contains 903,191 taxa plus synonyms, while the OntoBiotope ontology includes 3,601 concepts plus synonyms (3,172 for the Habitat branch and 429 for the Phenotype branch of the ontology).	0
42597	42597	D19-5719	Task Description	19	41	0.904761904761905	0.206030150753769	Geographical entities are not normalized.	0
42598	42598	D19-5719	Task Description	20	42	0.952380952380952	0.21105527638191	Figure 1 shows an example of a sentence annotated with normalized entities and relations.	0
42599	42599	D19-5719	Task Description	21	43	1.0	0.21608040201005	As in the 2016 edition, we designed three tasks, each including two modalities, one where entity annotations are provided and one where they are not and have to be predicted.	0
42742	42742	D19-5719	Conclusion	1	186	0.071428571428572	0.934673366834171	The Bacteria Biotope	0
42743	42743	D19-5719	Conclusion	2	187	0.142857142857143	0.939698492462311	Task arouses sustained interest with a total of 10 teams participating in the fourth edition.	0
42744	42744	D19-5719	Conclusion	3	188	0.214285714285714	0.944723618090452	As usual, the relation extraction sub-tasks (BB-rel and BB-rel+ner) were the most popular, demonstrating that this task is still a scientific and technical challenge.	0
42745	42745	D19-5719	Conclusion	4	189	0.285714285714286	0.949748743718593	The most notable evolution of participating systems since the last edition is the pervasiveness of methods based on neural networks and word embeddings.	0
42746	42746	D19-5719	Conclusion	5	190	0.357142857142857	0.954773869346734	These systems yielded superior predictions compared to those in 2016.	0
42747	42747	D19-5719	Conclusion	6	191	0.428571428571429	0.959798994974874	As mentioned previously, there is still much room for improvement in addressing cross-sentence relation extraction.	0
42748	42748	D19-5719	Conclusion	7	192	0.5	0.964824120603015	We also note a growing interest in the normalization sub-tasks (BB-norm and BB-norm+ner).	0
42749	42749	D19-5719	Conclusion	8	193	0.571428571428571	0.969849246231156	The predictions improved for habitat entities, and are very promising for phenotype entities.	0
42750	42750	D19-5719	Conclusion	9	194	0.642857142857143	0.974874371859296	However the generalization from bacteria-only taxa in 2016 to all microorganisms in this edition proved to pose an unexpected challenge.	0
42751	42751	D19-5719	Conclusion	10	195	0.714285714285714	0.979899497487437	Knowledge base population (BB-kb and BB-kb+ner) is the most challenging task, since it requires a wider set of capabilities.	0
42752	42752	D19-5719	Conclusion	11	196	0.785714285714286	0.984924623115578	Nevertheless we demonstrated that the combination of other subtask predictions allows to produce better quality knowledge bases.	0
42753	42753	D19-5719	Conclusion	12	197	0.857142857142857	0.989949748743718	To help participants, supporting resources were provided.	0
42754	42754	D19-5719	Conclusion	13	198	0.928571428571429	0.994974874371859	The most used resources were pretrained word embeddings, and general-domain named entities.	0
42755	42755	D19-5719	Conclusion	14	199	1.0	1.0	The evaluation on the test set will be maintained online 11 in order for future experiments to compare with the current state of the art.	0
42958	42958	D19-6007	title	1	1	1.0	0.005555555555556	Commonsense Inference in Natural Language Processing (COIN) -Shared Task Report	0
42959	42959	D19-6007	abstract	1	2	0.25	0.011111111111111	This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019.	0
42960	42960	D19-6007	abstract	2	3	0.5	0.016666666666667	The tasks consisted of two machine comprehension evaluations, each of which tested a system's ability to answer questions/queries about a text.	0
42961	42961	D19-6007	abstract	3	4	0.75	0.022222222222222	Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text.	0
42962	42962	D19-6007	abstract	4	5	1.0	0.027777777777778	A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
42963	42963	D19-6007	Introduction	1	6	0.047619047619048	0.033333333333333	Due to the rise of powerful pre-trained word and sentence representations, automated text processing has come a long way in recent years, with systems that perform even better than humans on some datasets (Rajpurkar et al., 2016a).	0
42964	42964	D19-6007	Introduction	2	7	0.095238095238095	0.038888888888889	However, natural language understanding also involves complex challenges.	0
42965	42965	D19-6007	Introduction	3	8	0.142857142857143	0.044444444444445	One important difference between human and machine text understanding lies in the fact that humans can access commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that are assumed to be common ground.	0
42966	42966	D19-6007	Introduction	4	9	0.19047619047619	0.05	"(1) Max: ""It's 1 pm already, I think we should get lunch."""	0
42967	42967	D19-6007	Introduction	5	10	0.238095238095238	0.055555555555556	"Dustin: ""Let me get my wallet."""	0
42968	42968	D19-6007	Introduction	6	11	0.285714285714286	0.061111111111111	Consider the conversation in Example 1: Max will not be surprised that Dustin needs to get his wallet, since she knows that paying is a part of getting lunch.	0
42969	42969	D19-6007	Introduction	7	12	0.333333333333333	0.066666666666667	Also, she knows that a wallet is needed for paying, so Dustin needs to get a wallet for lunch.	0
42970	42970	D19-6007	Introduction	8	13	0.380952380952381	0.072222222222222	This is part of the commonsense knowledge about getting lunch and should be known by both persons.	0
42971	42971	D19-6007	Introduction	9	14	0.428571428571429	0.077777777777778	For a computer system, inferring such unmentioned facts is a non-trivial challenge.	0
42972	42972	D19-6007	Introduction	10	15	0.476190476190476	0.083333333333333	The workshop on Commonsense Inference in NLP (COIN) is focused on such phenomena, looking at models, data, and evaluation methods for commonsense inference.	0
42973	42973	D19-6007	Introduction	11	16	0.523809523809524	0.088888888888889	This report summarizes the results of the COIN shared tasks, an unofficial extension of the Sem-Eval 2018 shared task 11, Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).	0
42974	42974	D19-6007	Introduction	12	17	0.571428571428571	0.094444444444445	The tasks aim to evaluate the commonsense inference capabilities of text understanding systems in two settings: Commonsense inference in everyday narrations (task 1) and commonsense inference in news texts (task 2).	1
42975	42975	D19-6007	Introduction	13	18	0.619047619047619	0.1	Framed as machine comprehension evaluations, the datasets used for both tasks contain challenging reading comprehension questions asking for facts that are not explicitly mentioned in the given reading texts.	0
42976	42976	D19-6007	Introduction	14	19	0.666666666666667	0.105555555555556	Several teams participated in the shared tasks and submitted system description papers.	0
42977	42977	D19-6007	Introduction	15	20	0.714285714285714	0.111111111111111	All systems are based on Transformer architectures (Vaswani et al., 2017), some of them explicitly incorporating commonsense knowledge resources, whereas others only use pretraining on other machine comprehension data sets.	0
42978	42978	D19-6007	Introduction	16	21	0.761904761904762	0.116666666666667	The best submitted system achieves 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
42979	42979	D19-6007	Introduction	17	22	0.80952380952381	0.122222222222222	Still, there are cases that remain elusive: Humans outperform this system by a margin of 7% (task 1) and 8% (task 2).	0
42980	42980	D19-6007	Introduction	18	23	0.857142857142857	0.127777777777778	Our results indicate that while Transformer models are able to perform extremely well on the data used in our shared task, there are still some remaining cases demonstrating that human level is not achieved yet.	0
42981	42981	D19-6007	Introduction	19	24	0.904761904761905	0.133333333333333	Still, we believe that our results also imply the need for more challenging data sets.	0
42982	42982	D19-6007	Introduction	20	25	0.952380952380952	0.138888888888889	In particular, we need data sets that make it harder to benefit from redundancy in the training data or large-scale pretraining on similar domains.	0
42983	42983	D19-6007	Introduction	21	26	1.0	0.144444444444444	In the following, we briefly describe the data sets ( ¬ß2), baselines and evaluation metrics of the shared tasks ( ¬ß3) and we present a summary of the participating systems ( ¬ß4), their results ( ¬ß5) as well as a discussion thereof ( ¬ß6).	0
43106	43106	D19-6007	Related Work	1	149	0.045454545454546	0.827777777777778	Evaluating commonsense inference via machine comprehension has recently moved into the focus of interest.	0
43107	43107	D19-6007	Related Work	2	150	0.090909090909091	0.833333333333333	Existing datasets cover various domains:	0
43108	43108	D19-6007	Related Work	3	151	0.136363636363636	0.838888888888889	Web texts.	0
43109	43109	D19-6007	Related Work	4	152	0.181818181818182	0.844444444444444	Trivia	0
43110	43110	D19-6007	Related Work	5	153	0.227272727272727	0.85	QA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web.	0
43111	43111	D19-6007	Related Work	6	154	0.272727272727273	0.855555555555556	A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer.	0
43112	43112	D19-6007	Related Work	7	155	0.318181818181818	0.861111111111111	Commonsense	0
43113	43113	D19-6007	Related Work	8	156	0.363636363636364	0.866666666666667	QA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017).	0
43114	43114	D19-6007	Related Work	9	157	0.409090909090909	0.872222222222222	Evidence documents were webcrawled based on the question and added after the crowdsourcing step.	0
43115	43115	D19-6007	Related Work	10	158	0.454545454545455	0.877777777777778	Fictive texts.	0
43116	43116	D19-6007	Related Work	11	159	0.5	0.883333333333333	Narrative	0
43117	43117	D19-6007	Related Work	12	160	0.545454545454545	0.888888888888889	QA (Koƒçisk√Ω et al., 2018) provides full novels and other long texts as evidence documents and contains approx.	0
43118	43118	D19-6007	Related Work	13	161	0.590909090909091	0.894444444444444	30 crowdsourced questions per text.	0
43119	43119	D19-6007	Related Work	14	162	0.636363636363636	0.9	The questions require a system to understand the whole plot of the text and to conduct many successive complicated inference steps, under the use of various types of background knowledge.	0
43120	43120	D19-6007	Related Work	15	163	0.681818181818182	0.905555555555556	News texts.	0
43121	43121	D19-6007	Related Work	16	164	0.727272727272727	0.911111111111111	News	0
43122	43122	D19-6007	Related Work	17	165	0.772727272727273	0.916666666666667	QA (Trischler et al., 2017) provides news texts with crowdsourced questions and answers, which are spans of the evidence documents.	0
43123	43123	D19-6007	Related Work	18	166	0.818181818181818	0.922222222222222	The question collection procedure for NewsQA resulted in a large number of questions that require factual commonsense knowledge for finding an answer.	0
43124	43124	D19-6007	Related Work	19	167	0.863636363636364	0.927777777777778	Other tasks.	0
43125	43125	D19-6007	Related Work	20	168	0.909090909090909	0.933333333333333	There have been other attempts at evaluating commonsense inference apart from machine comprehension.	0
43126	43126	D19-6007	Related Work	21	169	0.954545454545455	0.938888888888889	One example is the Story cloze test and the ROC dataset (Mostafazadeh et al., 2016), where systems have to find the correct ending to a 5-sentence story, using different types of commonsense knowledge.	0
43127	43127	D19-6007	Related Work	22	170	1.0	0.944444444444444	SWAG (Zellers et al., 2018) is a natural language inference dataset with a focus on difficult commonsense inferences.	0
43128	43128	D19-6007	Conclusion	1	171	0.1	0.95	This report presented the results of the shared tasks at the Workshop for Commonsense Inference in NLP (COIN).	0
43129	43129	D19-6007	Conclusion	2	172	0.2	0.955555555555556	The tasks aimed at evaluating the capability of systems to make use of commonsense knowledge for challenging inference questions in a machine comprehension setting, on everyday narrations (task 1) and news texts (task 2).	0
43130	43130	D19-6007	Conclusion	3	173	0.3	0.961111111111111	In total, 5 systems participated in task 1, and one system participated in task 2.	0
43131	43131	D19-6007	Conclusion	4	174	0.4	0.966666666666667	All submitted models were Transformer models, pretrained with a language modeling objective on large amounts of textual data.	0
43132	43132	D19-6007	Conclusion	5	175	0.5	0.972222222222222	The best system achieved 90.6% accuracy and 83.7% F1-score on task 1 and 2, respectively, leaving a gap of 7% and 8% to human performance.	0
43133	43133	D19-6007	Conclusion	6	176	0.6	0.977777777777778	The results of our shared tasks suggest that existing models cover a large part of the commonsense knowledge required for our data sets in the domains of narrations and news texts.	0
43134	43134	D19-6007	Conclusion	7	177	0.7	0.983333333333333	This does however not mean that commonsense inference is solved:	0
43135	43135	D19-6007	Conclusion	8	178	0.8	0.988888888888889	We found a range of examples in our data that are not successfully covered.	0
43136	43136	D19-6007	Conclusion	9	179	0.9	0.994444444444444	Furthermore, data sets such as HellaSWAG (Zellers et al., 2019) show that commonsense inference tasks can be specifically tailored to be hard for Transformer models.	0
43137	43137	D19-6007	Conclusion	10	180	1.0	1.0	We believe that modeling true language understanding requires a shift towards text types and tasks that test commonsense knowledge go-ing beyond information that can be obtained by exploiting the redundancy of large-scale corpora and/or pretraining on related tasks.	0
45218	45218	I17-4002	title	1	1	1.0	0.013157894736842	IJCNLP-2017 Task 2: Dimensional Sentiment Analysis for Chinese Phrases	0
45219	45219	I17-4002	abstract	1	2	0.2	0.026315789473684	This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions.	0
45220	45220	I17-4002	abstract	2	3	0.4	0.039473684210526	Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm.	0
45221	45221	I17-4002	abstract	3	4	0.6	0.052631578947369	Of the 19 teams registered for this shared task for twodimensional sentiment analysis, 13 submitted results.	0
45222	45222	I17-4002	abstract	4	5	0.8	0.065789473684211	We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing.	0
45223	45223	I17-4002	abstract	5	6	1.0	0.078947368421053	All data sets with gold standards and scoring script are made publicly available to researchers.	0
45224	45224	I17-4002	Introduction	1	7	0.055555555555556	0.092105263157895	Sentiment analysis has emerged as a leading technique to automatically identify affective information within texts.	0
45225	45225	I17-4002	Introduction	2	8	0.111111111111111	0.105263157894737	In sentiment analysis, affective states are generally represented using either categorical or dimensional approaches (Calvo and Kim, 2013).	0
45226	45226	I17-4002	Introduction	3	9	0.166666666666667	0.118421052631579	The categorical approach represents affective states as several discrete classes (e.g., positive, negative, neutral), while the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1.	0
45227	45227	I17-4002	Introduction	4	10	0.222222222222222	0.131578947368421	The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm.	0
45228	45228	I17-4002	Introduction	5	11	0.277777777777778	0.144736842105263	Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011;	0
45229	45229	I17-4002	Introduction	6	12	0.333333333333333	0.157894736842105	Malandrakis et al., 2011; or texts (Kim et al., 2010;	0
45230	45230	I17-4002	Introduction	7	13	0.388888888888889	0.171052631578947	Paltoglou et al, 2013;	0
45231	45231	I17-4002	Introduction	8	14	0.444444444444444	0.184210526315789	Wang et al., 2016b).	0
45232	45232	I17-4002	Introduction	9	15	0.5	0.197368421052632	Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014)	0
45233	45233	I17-4002	Introduction	10	16	0.555555555555556	0.210526315789474	The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing.	0
45234	45234	I17-4002	Introduction	11	17	0.611111111111111	0.223684210526316	Sentiment lexicons with valence-arousal ratings are useful resources for the development of dimensional sentiment applications.	0
45235	45235	I17-4002	Introduction	12	18	0.666666666666667	0.236842105263158	Due to the limited availability of such VA lexicons, especially for Chinese, the objective of the task is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases.	0
45236	45236	I17-4002	Introduction	13	19	0.722222222222222	0.25	The rest of this paper is organized as follows.	0
45237	45237	I17-4002	Introduction	14	20	0.777777777777778	0.263157894736842	Section II describes the task in detail.	0
45238	45238	I17-4002	Introduction	15	21	0.833333333333333	0.276315789473684	Section III introduces the constructed datasets.	0
45239	45239	I17-4002	Introduction	16	22	0.888888888888889	0.289473684210526	Section IV proposes evaluation metrics.	0
45240	45240	I17-4002	Introduction	17	23	0.944444444444444	0.302631578947368	Section V reports the results of the participants' approaches.	0
45241	45241	I17-4002	Introduction	18	24	1.0	0.31578947368421	Conclusions are finally drawn in Section VI.	0
45242	45242	I17-4002	Task Description	1	25	0.052631578947369	0.328947368421053	This task seeks to evaluate the capability of systems for predicting dimensional sentiments of Chinese words and phrases.	0
45243	45243	I17-4002	Task Description	2	26	0.105263157894737	0.342105263157895	For a given word or phrase, participants were asked to provide a realvalued score from 1 to 9 for both the valence and arousal dimensions, respectively indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal.	1
45244	45244	I17-4002	Task Description	3	27	0.157894736842105	0.355263157894737	"The input format is ""term_id, term"", and the output format is ""term_id, valence_rating, arousal_rating""."	0
45245	45245	I17-4002	Task Description	4	28	0.210526315789474	0.368421052631579	"Below are the input/output formats of the example words ""Â•Ω"" (good), ""ÈùûÂ∏∏Â•Ω"" (very good), ""ÊªøÊÑè"" (satisfy) and ""‰∏çÊªøÊÑè"" (not satisfy)."	0
45246	45246	I17-4002	Task Description	5	29	0.263157894736842	0.381578947368421	with valence-arousal ratings.	0
45247	45247	I17-4002	Task Description	6	30	0.31578947368421	0.394736842105263	For multi-word phrases, we first selected a set of modifiers such as negators (e.g., not), degree adverbs (e.g., very) and modals (e.g., would).	0
45248	45248	I17-4002	Task Description	7	31	0.368421052631579	0.407894736842105	These modifiers were combined with the affective words in CVAW to form multi-word phrases.	0
45249	45249	I17-4002	Task Description	8	32	0.421052631578947	0.421052631578947	The frequency of each phrase was then retrieved from a large web-based corpus.	0
45250	45250	I17-4002	Task Description	9	33	0.473684210526316	0.43421052631579	Only phrases with a frequency greater than or equal to 3 were retained as candidates.	0
45251	45251	I17-4002	Task Description	10	34	0.526315789473684	0.447368421052632	To avoid several modifiers dominating the whole dataset, each modifier (or modifier combination) can have at most 50 phrases.	0
45252	45252	I17-4002	Task Description	11	35	0.578947368421053	0.460526315789474	In addition, the phrases were selected to maximize the balance between positive and negative words.	0
45253	45253	I17-4002	Task Description	12	36	0.631578947368421	0.473684210526316	Finally, a total of 3,000 phrases were collected by excluding unusual and semantically incomplete candidate phrases, of which 2,250 phrases were randomly selected as the training set according to the proportions of each modifier (or modifier combination) in the original set, and the remaining 750 phrases were used as the test set.	0
45254	45254	I17-4002	Task Description	13	37	0.68421052631579	0.486842105263158	Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words .	0
45255	45255	I17-4002	Task Description	14	38	0.736842105263158	0.5	Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth.	0
45256	45256	I17-4002	Task Description	15	39	0.789473684210526	0.513157894736842	Each multi-word phrase was rated by at least 10 different annotators.	0
45257	45257	I17-4002	Task Description	16	40	0.842105263157895	0.526315789473684	Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations.	0
45258	45258	I17-4002	Task Description	17	41	0.894736842105263	0.539473684210526	They were then excluded from the calculation of the average ratings for each phrase.	0
45259	45259	I17-4002	Task Description	18	42	0.947368421052632	0.552631578947368	The policy of this shared task was implemented as is an open test.	0
45260	45260	I17-4002	Task Description	19	43	1.0	0.56578947368421	That is, in addition to the above official datasets, participating teams were allowed to use other publicly available data for system development, but such sources should be specified in the final technical report.	0
45290	45290	I17-4002	Conclusions	1	73	0.25	0.960526315789474	This study describes an overview of the IJCNLP 2017 shared task on dimensional sentiment analysis for Chinese phrases, including task design, data preparation, performance metrics, and evaluation results.	0
45291	45291	I17-4002	Conclusions	2	74	0.5	0.973684210526316	Regardless of actual performance, all submissions contribute to the common effort to develop dimensional approaches for affective computing, and the individual report in the proceedings provide useful insights into Chinese sentiment analysis.	0
45292	45292	I17-4002	Conclusions	3	75	0.75	0.986842105263158	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
45293	45293	I17-4002	Conclusions	4	76	1.0	1.0	Therefore, all data sets with gold standard and scoring script are publicly available 5 .	0
46115	46115	W11-1802	title	1	1	0.25	0.00990099009901	Overview of Genia Event Task in BioNLP Shared Task 2011	0
46116	46116	W11-1802	abstract	1	2	0.5	0.01980198019802	The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.	1
46117	46117	W11-1802	abstract	2	3	0.75	0.02970297029703	As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.	0
46118	46118	W11-1802	abstract	3	4	1.0	0.03960396039604	After a 3-month system development period, 15 teams submitted their performance results on test cases.	0
46119	46119	W11-1802	abstract	4	5	0.052631578947369	0.04950495049505	The results show the community has made a significant advancement in terms of both performance improvement and generalization.	0
46120	46120	W11-1802	Introduction	1	6	0.105263157894737	0.059405940594059	The BioNLP Shared Task (BioNLP-ST, hereafter) is a series of efforts to promote a communitywide collaboration towards fine-grained information extraction (IE) in biomedical domain.	0
46121	46121	W11-1802	Introduction	2	7	0.157894736842105	0.069306930693069	The first event, BioNLP-ST 2009, introducing a biomolecular event (bio-event) extraction task to the community, attracted a wide attention, with 42 teams being registered for participation and 24 teams submitting final results (Kim et al., 2009).	0
46122	46122	W11-1802	Introduction	3	8	0.210526315789474	0.079207920792079	To establish a community effort, the organizers provided the task definition, benchmark data, and evaluations, and the participants competed in developing systems to perform the task.	0
46123	46123	W11-1802	Introduction	4	9	0.263157894736842	0.089108910891089	Meanwhile, participants and organizers communicated to develop a better setup of evaluation, and some provided their tools and resources for other participants, making it a collaborative competition.	0
46124	46124	W11-1802	Introduction	5	10	0.31578947368421	0.099009900990099	The final results enabled to observe the state-ofthe-art performance of the community on the bioevent extraction task, which showed that the automatic extraction of simple events -those with unary arguments, e.g. gene expression, localization, phosphorylation -could be achieved at the performance level of 70% in F-score, but the extraction of complex events, e.g. binding and regulation, was a lot more challenging, having achieved 40% of performance level.	0
46125	46125	W11-1802	Introduction	6	11	0.368421052631579	0.108910891089109	After BioNLP-ST 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.	0
46126	46126	W11-1802	Introduction	7	12	0.421052631578947	0.118811881188119	Since then, several improvements have been reported (Miwa et al., 2010b;	0
46127	46127	W11-1802	Introduction	8	13	0.473684210526316	0.128712871287129	Poon and Vanderwende, 2010;Vlachos, 2010;Miwa et al., 2010a;	0
46128	46128	W11-1802	Introduction	9	14	0.526315789473684	0.138613861386139	Bj√∂rne et al., 2010).	0
46129	46129	W11-1802	Introduction	10	15	0.578947368421053	0.148514851485149	For example, Miwa et al.	0
46130	46130	W11-1802	Introduction	11	16	0.631578947368421	0.158415841584158	(Miwa et al., 2010b) reported a significant improvement with binding events, achieving 50% of performance level.	0
46131	46131	W11-1802	Introduction	12	17	0.68421052631579	0.168316831683168	The task introduced in BioNLP-ST 2009 was renamed to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al., 2011).	0
46132	46132	W11-1802	Introduction	13	18	0.736842105263158	0.178217821782178	As the sole task that was repeated in the two events, the GE task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of 2011.	0
46133	46133	W11-1802	Introduction	14	19	0.789473684210526	0.188118811881188	The GE task in 2011 received final submissions from 15 teams.	0
46134	46134	W11-1802	Introduction	15	20	0.842105263157895	0.198019801980198	The results show the community made a significant progress with the task, and also show the technology can be generalized to full papers at moderate cost of performance.	0
46135	46135	W11-1802	Introduction	16	21	0.894736842105263	0.207920792079208	This paper presents the task setup, preparation, and discusses the results.	0
46136	46136	W11-1802	Introduction	17	22	0.947368421052632	0.217821782178218	1: Event types and their arguments for Genia event task.	0
46137	46137	W11-1802	Introduction	18	23	1.0	0.227722772277228	The type of each filler entity is specified in parenthesis.	0
46138	46138	W11-1802	Introduction	19	24	0.043478260869565	0.237623762376238	"Arguments that may be filled more than once per event are marked with ""+""."	0
46139	46139	W11-1802	Task Definition	1	25	0.086956521739131	0.247524752475248	The GE task follows the task definition of BioNLP-ST 2009, which is briefly described in this section.	0
46140	46140	W11-1802	Task Definition	2	26	0.130434782608696	0.257425742574257	For more detail, please refer to (Kim et al., 2009).	0
46141	46141	W11-1802	Task Definition	3	27	0.173913043478261	0.267326732673267	Table 1 shows the event types to be addressed in the task.	0
46142	46142	W11-1802	Task Definition	4	28	0.217391304347826	0.277227722772277	For each event type, the primary and secondary arguments to be extracted with an event are defined.	0
46143	46143	W11-1802	Task Definition	5	29	0.260869565217391	0.287128712871287	For example, a Phosphorylation event is primarily extracted with the protein to be phosphorylated.	0
46144	46144	W11-1802	Task Definition	6	30	0.304347826086957	0.297029702970297	As secondary information, the specific site to be phosphorylated may be extracted.	0
46145	46145	W11-1802	Task Definition	7	31	0.347826086956522	0.306930693069307	From a computational point of view, the event types represent different levels of complexity.	0
46146	46146	W11-1802	Task Definition	8	32	0.391304347826087	0.316831683168317	When only primary arguments are considered, the first five event types in Table 1 are classified as simple event types, requiring only unary arguments.	0
46147	46147	W11-1802	Task Definition	9	33	0.434782608695652	0.326732673267327	The Binding and Regulation types are more complex: Binding requires detection of an arbitrary number of arguments, and Regulation requires detection of recursive event structure.	0
46148	46148	W11-1802	Task Definition	10	34	0.478260869565217	0.336633663366337	Based on the definition of event types, the entire task is divided to three sub-tasks addressing event extraction at different levels of specificity: Task 1.	0
46149	46149	W11-1802	Task Definition	11	35	0.521739130434783	0.346534653465346	Core event extraction addresses the extraction of typed events together with their primary arguments.	0
46150	46150	W11-1802	Task Definition	12	36	0.565217391304348	0.356435643564356	Task 2. Event enrichment addresses the extraction of secondary arguments that further specify the events extracted in Task 1.	0
46151	46151	W11-1802	Task Definition	13	37	0.608695652173913	0.366336633663366	Task 3. Negation/Speculation detection addresses the detection of negations and speculations over the extracted events.	0
46152	46152	W11-1802	Task Definition	14	38	0.652173913043478	0.376237623762376	Task 1 serves as the backbone of the GE task and is mandatory for all participants, while the other two are optional.	0
46153	46153	W11-1802	Task Definition	15	39	0.695652173913043	0.386138613861386	The annotation T1 identifies the entity referred to by the string (p65) between the character offsets, 15 and 18 to be a Protein.	0
46154	46154	W11-1802	Task Definition	16	40	0.739130434782609	0.396039603960396	T2 identifies the string, translocation, to refer to a Localization event.	0
46155	46155	W11-1802	Task Definition	17	41	0.782608695652174	0.405940594059406	Entities other than proteins or event type references are classified into a default class Entity, as in T3. E1 then represents the event defined by the three entities, as defined in Table 1.	0
46156	46156	W11-1802	Task Definition	18	42	0.826086956521739	0.415841584158416	Note that for Task 1, the entity, T3, does not need to be identified, and the event, E1, may be identified without specification of the secondary argument, ToLoc:T1: E1' Localization:	0
46157	46157	W11-1802	Task Definition	19	43	0.869565217391304	0.425742574257426	T2 Theme:	0
46158	46158	W11-1802	Task Definition	20	44	0.91304347826087	0.435643564356436	T1	0
46159	46159	W11-1802	Task Definition	21	45	0.956521739130435	0.445544554455446	Finding the full representation of E1 is the goal of Task 2.	0
46160	46160	W11-1802	Task Definition	22	46	1.0	0.455445544554455	In the example, the localization event, E1, is negated as expressed in the failure of .	0
46161	46161	W11-1802	Task Definition	23	47	0.125	0.465346534653465	Finding the negation, M1 is the goal of Task 3.	0
46213	46213	W11-1802	Conclusions	1	99	0.666666666666667	0.98019801980198	The Genia event task which was repeated for BioNLP-ST 2009 and 2011 took a role of measuring the progress of the community and generalization IE technology to full papers.	0
46214	46214	W11-1802	Conclusions	2	100	1.0	0.99009900990099	The results from 15 teams who made their final submissions to the task show that a clear advance of the community in terms of the performance on a focused domain and also generalization to full papers.	0
46215	46215	W11-1802	Conclusions	3	101	1.0	1.0	To our disappointment, however, an effective use of supporting task results was not observed, which thus remains as future work for further improvement.	0
