	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
123	123	S01-dutch	title	1	1	1.0	0.010526315789474	Dutch Word Sense Disambiguation: Data and Preliminary Results	1
124	124	S01-dutch	abstract	1	2	1.0	0.021052631578947	We describe the Dutch word sense disambiguation data submitted to SENSEVAL-2, and give preliminary results on the data using a WSD system based on memory-based learning and statistical keyword selection.	0
218	218	S01-english-allwords	title	1	1	1.0	0.013333333333333	English Tasks: All-Words and Verb Lexical Sample	1
219	219	S01-english-allwords	abstract	1	2	1.0	0.026666666666667	We describe our experience in preparing the lexicon and sense-tagged corpora used in the English all-words and lexical sample tasks of SENSEVAL-2.	0
544	544	S01-japanese-dictionary	Introduction	1	8	0.066666666666667	0.066666666666667	In SENSEVAL-2, there are two Japanese tasks, a translation task and a dictionary task.	0
545	545	S01-japanese-dictionary	Introduction	2	9	0.133333333333333	0.075	This paper describes the details of the dictionary task.	0
546	546	S01-japanese-dictionary	Introduction	3	10	0.2	0.083333333333333	First of all, let me introduce an overview of the Japanese dictionary task.	0
547	547	S01-japanese-dictionary	Introduction	4	11	0.266666666666667	0.091666666666667	This task is a lexical sample task.	0
548	548	S01-japanese-dictionary	Introduction	5	12	0.333333333333333	0.1	Word senses were defined according to the Iwanami Kokugo Jiten (Nishio et aL, 1994), a Japanese dictionary published by Iwanami Shoten.	0
549	549	S01-japanese-dictionary	Introduction	6	13	0.4	0.108333333333333	It was distributed to all participants as a sense inventory.	0
550	550	S01-japanese-dictionary	Introduction	7	14	0.466666666666667	0.116666666666667	Training data, a corpus consisting of 3,000 newspaper articles and manually annotated with sense IDs, was also distributed to participants.	0
551	551	S01-japanese-dictionary	Introduction	8	15	0.533333333333333	0.125	For evaluation, we distributed newspaper articles with marked target words as test documents.	0
552	552	S01-japanese-dictionary	Introduction	9	16	0.6	0.133333333333333	Participants were required to assign one or more sense IDs to each target word, optionally with associated probabilities.	1
553	553	S01-japanese-dictionary	Introduction	10	17	0.666666666666667	0.141666666666667	The number of target words was 100, 50 nouns and 50 verbs.	0
554	554	S01-japanese-dictionary	Introduction	11	18	0.733333333333333	0.15	One hundred instances of each target word were provided, making for a total of 10,000 instances.	0
555	555	S01-japanese-dictionary	Introduction	12	19	0.8	0.158333333333333	In what follows, Section 2 describes details of data used in the Japanese dictionary task.	0
556	556	S01-japanese-dictionary	Introduction	13	20	0.866666666666667	0.166666666666667	Section 3 describes the process to construct the 33 gold standard data, including the analysis of inter-tagger agreement.	0
557	557	S01-japanese-dictionary	Introduction	14	21	0.933333333333333	0.175	Section 4 briefly introduces participating systems and their results.	0
558	558	S01-japanese-dictionary	Introduction	15	22	1.0	0.183333333333333	Finally, Section 5 concludes this paper.	0
664	664	S01-japanese-translation	Introduction	1	8	0.066666666666667	0.094117647058824	In written texts, words which have multiple senses can be classified into two categories; homonyms and polysemous words.	0
665	665	S01-japanese-translation	Introduction	2	9	0.133333333333333	0.105882352941176	Generally speaking, while homonymy sense distinction is quite clear, polysemy sense distinction is very subtle and hard.	0
666	666	S01-japanese-translation	Introduction	3	10	0.2	0.117647058823529	English texts contain many homonyms.	0
667	667	S01-japanese-translation	Introduction	4	11	0.266666666666667	0.129411764705882	On the other hand, Japanese texts in which most content words are written by ideograms rarely contain homonyms.	0
668	668	S01-japanese-translation	Introduction	5	12	0.333333333333333	0.141176470588235	That is, the main target in Japanese WSD is polysemy, which makes Japanese WSD task setup very hard.	0
669	669	S01-japanese-translation	Introduction	6	13	0.4	0.152941176470588	What sense distinction of polysemous words is reasonable and effective heavily depends on how to use it, that is, an application ofWSD.	0
670	670	S01-japanese-translation	Introduction	7	14	0.466666666666667	0.164705882352941	Considering such a situation, in addition to the ordinary dictionary task we organized another task for Japanese, a translation task, in which word sense is defined according to translation distinction.	0
671	671	S01-japanese-translation	Introduction	8	15	0.533333333333333	0.176470588235294	Here, we set up the task assuming the example-based machine translation paradigm (Nagao, 1981).	0
672	672	S01-japanese-translation	Introduction	9	16	0.6	0.188235294117647	That is, first, a translation memory (TM) is constructed which contains, for each Japanese head word, a list of typical Japanese expressions (phrases/sentences) involving the head word and an English translation for each (Figure 1).	0
673	673	S01-japanese-translation	Introduction	10	17	0.666666666666667	0.2	We call a pair of Japanese and English expressions in the TM as a TM record.	0
674	674	S01-japanese-translation	Introduction	11	18	0.733333333333333	0.211764705882353	Given an evaluation document containing a target word, participants have to submit the TM record best approximating that usage.	1
675	675	S01-japanese-translation	Introduction	12	19	0.8	0.223529411764706	Alternatively, submissions can take the form of actual target word translations, or translations of phrases or sentences including each target word.	0
676	676	S01-japanese-translation	Introduction	13	20	0.866666666666667	0.235294117647059	This allows existing rule-based machine translation (MT) systems to participate in the task, and we can compare TM based systems with existing MT systems.	0
677	677	S01-japanese-translation	Introduction	14	21	0.933333333333333	0.247058823529412	For evaluation, we distributed newspaper articles.	0
678	678	S01-japanese-translation	Introduction	15	22	1.0	0.258823529411765	The number of target words was 40, and 30 instances of each target word were provided, making for a total of 1,200 instances.	0
1014	1014	S04-catalan	title	1	1	1.0	0.012345679012346	Senseval-3: The Catalan Lexical Sample Task	1
1015	1015	S04-catalan	abstract	1	2	0.142857142857143	0.024691358024691	Introduction	0
1016	1016	S04-catalan	abstract	2	3	0.285714285714286	0.037037037037037	In this paper we describe the Catalan Lexical Sample task.	0
1017	1017	S04-catalan	abstract	3	4	0.428571428571429	0.049382716049383	This task was initially devised for evaluating the role of unlabeled examples in supervised and semi-supervised learning systems for WSD and it is the counterpart of the Spanish Lexical Sample task.	0
1018	1018	S04-catalan	abstract	4	5	0.571428571428571	0.061728395061728	It was coordinated also with other lexical sample tasks (Basque, English, Italian, Rumanian, and Spanish) in order to share part of the target words.	0
1019	1019	S04-catalan	abstract	5	6	0.714285714285714	0.074074074074074	Firstly, we describe the methodology followed for developing the specific linguistic resources necessary for the task: the MiniDir-Cat lexicon and the MiniCors-Cat corpus.	0
1020	1020	S04-catalan	abstract	6	7	0.857142857142857	0.08641975308642	Secondly, we briefly describe the seven participant systems, the results obtained, and a comparative evaluation between them.	0
1021	1021	S04-catalan	abstract	7	8	1.0	0.098765432098766	All participant teams applied only pure supervised learning algorithms.	0
1138	1138	S04-english	Conclusion	1	44	0.333333333333333	0.956521739130435	The English lexical sample task in SENSEVAL-3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.	1
1139	1139	S04-english	Conclusion	2	45	0.666666666666667	0.978260869565217	The objective of this task was to: (1)	0
1140	1140	S04-english	Conclusion	3	46	1.0	1.0	Determine feasibility of reliably finding the	0
1196	1196	S04-english-hindi	abstract	1	2	0.333333333333333	0.02247191011236	This paper describes the English-Hindi Multilingual lexical sample task in SENSEVAL-3.	0
1197	1197	S04-english-hindi	abstract	2	3	0.666666666666667	0.033707865168539	Rather than tagging an English word with a sense from an English dictionary, this task seeks to assign the most appropriate Hindi translation to an ambiguous target word.	1
1198	1198	S04-english-hindi	abstract	3	4	1.0	0.044943820224719	Training data was solicited via the Open Mind Word Expert (OMWE) from Web users who are fluent in English and Hindi.	0
1289	1289	S04-italian	Introduction	1	6	0.25	0.076923076923077	The task consisted in automatically determining the correct meaning of a word within a given context (i.e. a short text snippet).	1
1290	1290	S04-italian	Introduction	2	7	0.5	0.08974358974359	Systems' results were compared on the one hand to those achieved by human annotators (upper bound), and on the other hand to those returned by a basic algorithm (baseline).	0
1291	1291	S04-italian	Introduction	3	8	0.75	0.102564102564103	In the second section of this paper an overview of the task preparation is given and in the following one the main features of the participating systems are briefly outlined and the results of the evaluation exercise are presented.	0
1292	1292	S04-italian	Introduction	4	9	1.0	0.115384615384615	In the conclusions we give an overall judgement of the outcome of the task, suggesting possible improvements for the next campaign.	0
1363	1363	S04-italian-allwords	abstract	1	2	0.5	0.022988505747127	This paper describes the Italian all-words sense disambiguation task for Senseval-3.	1
1364	1364	S04-italian-allwords	abstract	2	3	1.0	0.03448275862069	The annotation procedure and criteria together with the encoding of multiwords are presented.	0
1365	1365	S04-italian-allwords	Introduction	1	4	0.142857142857143	0.045977011494253	This paper describes the Italian all-words sense disambiguation task for Senseval-3: about 5000 words were manually disambiguated according to the ItalWordNet (IWN) word senses.	1
1366	1366	S04-italian-allwords	Introduction	2	5	0.285714285714286	0.057471264367816	The first section briefly describes of the corpus and the lexical reference resource.	0
1367	1367	S04-italian-allwords	Introduction	3	6	0.428571428571429	0.068965517241379	The second section contains some general criteria adopted for the annotation of the corpus and illustrated by a series of examples.	0
1368	1368	S04-italian-allwords	Introduction	4	7	0.571428571428571	0.080459770114943	Issues connected to the treatment of phenomena typically found in corpora, e.g. abbreviations, foreign words, jargon, locutions are discussed.	0
1369	1369	S04-italian-allwords	Introduction	5	8	0.714285714285714	0.091954022988506	Furthermore, the encoding of compounds, metaphorical usages, and multiword units is described.	0
1370	1370	S04-italian-allwords	Introduction	6	9	0.857142857142857	0.103448275862069	Problems connected with i) the high granularity of sense distinctions in the lexical resource and ii) unsolvable ambiguities of the contexts are dealt with.	0
1371	1371	S04-italian-allwords	Introduction	7	10	1.0	0.114942528735632	Finally, it is evidenced how the annotation exercise can be of help in updating or tuning IWN, by adding missing senses and/or entries.	0
1455	1455	S04-logic-form-id	Introduction	1	7	0.25	0.085365853658537	The goal of a Logic Form Identification (LFi) task is to evaluate the performance of different methods addressing the issue of LFi.	0
1456	1456	S04-logic-form-id	Introduction	2	8	0.5	0.097560975609756	The Logic Form (LF) that we use is a flat, scope-free first order logic representation that embeds lexical and syntactic information.	0
1457	1457	S04-logic-form-id	Introduction	3	9	0.75	0.109756097560976	Given a set of English sentences, participating systems were supposed to return the sentences in Logic Form as in the example below.	1
1458	1458	S04-logic-form-id	Introduction	4	10	1.0	0.121951219512195	The general approach adopted for evaluation was a gold standard approach in which the test data is first correctly mapped onto its corresponding LF by a team of experts and then this correct LF is automatically compared against outputs provided by participating sytems.	0
1531	1531	S04-romanian	title	1	1	1.0	0.016949152542373	An Evaluation Exercise for Romanian Word Sense Disambiguation	1
1532	1532	S04-romanian	abstract	1	2	0.5	0.033898305084746	This paper presents the task definition, resources, participating systems, and comparative results for a Romanian Word Sense Disambiguation task, which was organized as part of the SENSEVAL-3 evaluation exercise.	1
1533	1533	S04-romanian	abstract	2	3	1.0	0.050847457627119	Five teams with a total of seven systems were drawn to this task.	0
1607	1607	S04-semantic-roles	The Senseval-3 Task	1	18	0.018518518518519	0.178217821782178	This Senseval-3 task calls for the development of systems to meet the same objectives as the Gildea and Jurafsky study.	0
1608	1608	S04-semantic-roles	The Senseval-3 Task	2	19	0.037037037037037	0.188118811881188	The data for this task is a sample of the FrameNet hand-annotated data.	0
1609	1609	S04-semantic-roles	The Senseval-3 Task	3	20	0.055555555555556	0.198019801980198	Evaluation of systems is measured using precision and recall of frame elements and overlap of a system's frame element sentence positions with those identified in the FrameNet data.	0
1610	1610	S04-semantic-roles	The Senseval-3 Task	4	21	0.074074074074074	0.207920792079208	The basic task for Senseval-3 is: Given a sentence, a target word and its frame, identify the frame elements within that sentence and tag them with the appropriate frame element name.	1
1611	1611	S04-semantic-roles	The Senseval-3 Task	5	22	0.092592592592593	0.217821782178218	The Frame	0
1612	1612	S04-semantic-roles	The Senseval-3 Task	6	23	0.111111111111111	0.227722772277228	Net project has just released a major revision (FrameNet 1.1) to its database, with 487 frames using 696 distinctly-named frame elements (although it is not guaranteed that frame elements with the same name have the same meaning).	0
1613	1613	S04-semantic-roles	The Senseval-3 Task	7	24	0.12962962962963	0.237623762376238	This release includes 132,968 annotated sentences (mostly taken from the British National Corpus).	0
1614	1614	S04-semantic-roles	The Senseval-3 Task	8	25	0.148148148148148	0.247524752475248	The Senseval-3 task used 8,002 of these sentences selected randomly from 40 frames (also selected randomly) having at least 370 annotations (out of the 100 frames having the most annotations).	0
1615	1615	S04-semantic-roles	The Senseval-3 Task	9	26	0.166666666666667	0.257425742574257	1 Participants were provided with a training set that identified, for each of the 40 frames, the lexical unit identification number (which equates to a file name) and a sentence identification name.	0
1616	1616	S04-semantic-roles	The Senseval-3 Task	10	27	0.185185185185185	0.267326732673267	They were also provided with the answers, i.e., the frame element names and their beginning and ending positions.	0
1617	1617	S04-semantic-roles	The Senseval-3 Task	11	28	0.203703703703704	0.277227722772277	Since the training set was much larger than the test set, participants were required to use the FrameNet 1.1 dataset to obtain the full sentence, its target word, and the tagged frame elements.	0
1618	1618	S04-semantic-roles	The Senseval-3 Task	12	29	0.222222222222222	0.287128712871287	For the test data, participants were provided, for each frame, with sentence instances that identified the lexical unit, the lexical unit identification number, the sentence identification number, the full sentence, and a specification of the target along with its start and end positions.	0
1619	1619	S04-semantic-roles	The Senseval-3 Task	13	30	0.240740740740741	0.297029702970297	Participants were required to submit their answers in a text file, with one answer per line.	0
1620	1620	S04-semantic-roles	The Senseval-3 Task	14	31	0.259259259259259	0.306930693069307	Each line was to identify the frame name and the sentence identifier and then all the frame elements with their start and end positions that their systems were able to identify.	0
1621	1621	S04-semantic-roles	The Senseval-3 Task	15	32	0.277777777777778	0.316831683168317	For example, for the sentence However, its task is made much more difficult by the fact that derogations granted to the Welsh water authority allow &lt;	0
1622	1622	S04-semantic-roles	The Senseval-3 Task	16	33	0.296296296296296	0.326732673267327	Agent&gt;it&lt;/&gt; to &lt;Target&gt;pump&lt;/&gt; &lt;Fluid&gt;raw sewage&lt;/&gt; &lt;Goal&gt;into both those rivers&lt;/&gt;. the correct answer would appear as follows:	0
1623	1623	S04-semantic-roles	The Senseval-3 Task	17	34	0.314814814814815	0.336633663366337	Cause_fluidic_motion.256263 Agent (119,120) Fluid (130,139) Goal (141,162)	0
1624	1624	S04-semantic-roles	The Senseval-3 Task	18	35	0.333333333333333	0.346534653465346	The sentences provided to participants were not presegmented (as defined in the Gildea and Jurafsky study); this was left to the participants' systems.	0
1625	1625	S04-semantic-roles	The Senseval-3 Task	19	36	0.351851851851852	0.356435643564356	The FrameNet dataset contains considerable information that was tagged by the FrameNet lexicographers.	0
1626	1626	S04-semantic-roles	The Senseval-3 Task	20	37	0.37037037037037	0.366336633663366	Participants could use (and were strongly encouraged to use) any and all of the FrameNet data in developing and training their systems.	0
1627	1627	S04-semantic-roles	The Senseval-3 Task	21	38	0.388888888888889	0.376237623762376	In the test, participants could use any of this data, but were strongly encouraged to use only data available in the sentence itself and in the frame that is identified.	0
1628	1628	S04-semantic-roles	The Senseval-3 Task	22	39	0.407407407407407	0.386138613861386	"(This corresponds to the ""more difficult task"" identified by Gildea and Jurafsky.)"	0
1629	1629	S04-semantic-roles	The Senseval-3 Task	23	40	0.425925925925926	0.396039603960396	Participants could submit two runs, one with (non-restrictive case) and one without (restrictive case) using the additional data; these were scored separately.	0
1630	1630	S04-semantic-roles	The Senseval-3 Task	24	41	0.444444444444444	0.405940594059406	Frame	0
1631	1631	S04-semantic-roles	The Senseval-3 Task	25	42	0.462962962962963	0.415841584158416	"Net recognizes the permissibility of ""conceptually salient"" frame elements that have not been instantiated in a sentence; these are called null instantiations (see Johnson et al. for a fuller description)."	0
1632	1632	S04-semantic-roles	The Senseval-3 Task	26	43	0.481481481481481	0.425742574257426	"An example occurs in the following sentence (sentID=""1087911"") from the Motion frame: ""I went and stood in the sitting room doorway, but I couldn't get any further --my legs wouldn't move."""	0
1633	1633	S04-semantic-roles	The Senseval-3 Task	27	44	0.5	0.435643564356436	In this case, the FrameNet taggers considered the Path frame element to be an indefinite null instantiation (INI).	0
1634	1634	S04-semantic-roles	The Senseval-3 Task	28	45	0.518518518518518	0.445544554455446	Frame elements that have been so designated for a particular sentence appear to be Core frame elements, but not all core frame elements missing from a sentence have designated as null instantiations.	0
1635	1635	S04-semantic-roles	The Senseval-3 Task	29	46	0.537037037037037	0.455445544554455	The correct answer for this case, based on the tagging, is as follows:	0
1636	1636	S04-semantic-roles	The Senseval-3 Task	30	47	0.555555555555556	0.465346534653465	Motion.1087911	0
1637	1637	S04-semantic-roles	The Senseval-3 Task	31	48	0.574074074074074	0.475247524752475	Theme (82,88) Path (0,0)	0
1638	1638	S04-semantic-roles	The Senseval-3 Task	32	49	0.592592592592593	0.485148514851485	Participants were instructed to identify null instantiations in submissions by giving a (0,0) value for the frame element's position.	0
1639	1639	S04-semantic-roles	The Senseval-3 Task	33	50	0.611111111111111	0.495049504950495	2 Participants were told in the task description that null instantiations would be analyzed separately.	0
1640	1640	S04-semantic-roles	The Senseval-3 Task	34	51	0.62962962962963	0.504950495049505	3 For this Senseval task, participants were allowed to download the training data at any time; the 21-day 1 The test set was generated with the Windows-based program FrameNet Explorer, available at http://www.clres.com/SensSemRoles.html.	0
1641	1641	S04-semantic-roles	The Senseval-3 Task	35	52	0.648148148148148	0.514851485148515	FrameNet Explorer provides several facilities for examining the FrameNet data: by frame, frame element, and lexical units.	0
1642	1642	S04-semantic-roles	The Senseval-3 Task	36	53	0.666666666666667	0.524752475247525	For each unit, a user can explore a frame's elements, associated lexical units, frame-to-frame relations, frame and frame element definitions, lexical units and their definitions, and all sentences.	0
1643	1643	S04-semantic-roles	The Senseval-3 Task	37	54	0.685185185185185	0.534653465346535	restriction on submission of results after downloading the training data was waived since this is a new Senseval task and the dataset is very complex.	0
1644	1644	S04-semantic-roles	The Senseval-3 Task	38	55	0.703703703703704	0.544554455445545	Participants could work with the training data as long as they wished.	0
1645	1645	S04-semantic-roles	The Senseval-3 Task	39	56	0.722222222222222	0.554455445544555	The 7-day restriction of submitting results after downloading the test data still applied.	0
1646	1646	S04-semantic-roles	The Senseval-3 Task	40	57	0.740740740740741	0.564356435643564	In general, FrameNet frames contain many frame elements (perhaps an average of 10), most of which are not instantiated in a given sentence.	0
1647	1647	S04-semantic-roles	The Senseval-3 Task	41	58	0.759259259259259	0.574257425742574	Systems were not penalized if they returned more frame elements than those identified by the FrameNet taggers.	0
1648	1648	S04-semantic-roles	The Senseval-3 Task	42	59	0.777777777777778	0.584158415841584	For the 8002 sentences in the test set, only 16212 frame elements constituted the answer set.	0
1649	1649	S04-semantic-roles	The Senseval-3 Task	43	60	0.796296296296296	0.594059405940594	In scoring the runs, each frame element (not a null instantiation) returned by a system was counted as an item attempted.	0
1650	1650	S04-semantic-roles	The Senseval-3 Task	44	61	0.814814814814815	0.603960396039604	If the frame element was one that had been identified by the FrameNet taggers, the answer was scored as correct.	0
1651	1651	S04-semantic-roles	The Senseval-3 Task	45	62	0.833333333333333	0.613861386138614	In addition, however, the scoring program required that the frame boundaries identified by the system's answer had to overlap with the boundaries identified by FrameNet.	0
1652	1652	S04-semantic-roles	The Senseval-3 Task	46	63	0.851851851851852	0.623762376237624	An additional measure of system performance was the degree of overlap.	0
1653	1653	S04-semantic-roles	The Senseval-3 Task	47	64	0.87037037037037	0.633663366336634	If a system's answer coincided exactly to FrameNet's start and end position, the system received an overlap score of 1.0.	0
1654	1654	S04-semantic-roles	The Senseval-3 Task	48	65	0.888888888888889	0.643564356435644	If not, the overlap score was the number of characters overlapping divided by the length of the FrameNet start and end positions (i.e., end-start+1) 4	0
1655	1655	S04-semantic-roles	The Senseval-3 Task	49	66	0.907407407407407	0.653465346534653	The number attempted was the number of nonnull frame elements generated by a system.	0
1656	1656	S04-semantic-roles	The Senseval-3 Task	50	67	0.925925925925926	0.663366336633663	Precision was computed as the number of correct answers divided by the number attempted.	0
1657	1657	S04-semantic-roles	The Senseval-3 Task	51	68	0.944444444444444	0.673267326732673	Recall was computed as the number of correct answers divided by the number of frame elements in the test set.	0
1658	1658	S04-semantic-roles	The Senseval-3 Task	52	69	0.962962962962963	0.683168316831683	Overlap was the average overlap of all correct answers.	0
1659	1659	S04-semantic-roles	The Senseval-3 Task	53	70	0.981481481481482	0.693069306930693	The percent Attempted was the number of frame elements generated divided by the number of frame elements in the test set, multiplied by 100.	0
1660	1660	S04-semantic-roles	The Senseval-3 Task	54	71	1.0	0.702970297029703	If a system returned frame elements not identified in the test set, its precision would be lower.	0
1761	1761	S04-wordnet-glosses	abstract	1	2	0.166666666666667	0.019230769230769	The SENSEVAL-3 task to perform word-sense disambiguation of WordNet glosses was designed to encourage development of technology to make use of standard lexical resources.	1
1762	1762	S04-wordnet-glosses	abstract	2	3	0.333333333333333	0.028846153846154	The task was based on the availability of sensedisambiguated hand-tagged glosses created in the eXtended WordNet project.	0
1763	1763	S04-wordnet-glosses	abstract	3	4	0.5	0.038461538461539	"The hand-tagged glosses provided a ""gold standard"" for judging the performance of automated disambiguation systems."	0
1764	1764	S04-wordnet-glosses	abstract	4	5	0.666666666666667	0.048076923076923	Seven teams participated in the task, with a total of 10 runs.	0
1765	1765	S04-wordnet-glosses	abstract	5	6	0.833333333333333	0.057692307692308	"Scoring these runs as an ""all-words"" task, along with considerable discussions among participants, provided more insights than just the underlying technology."	0
1766	1766	S04-wordnet-glosses	abstract	6	7	1.0	0.067307692307692	The task identified several issues about the nature of the WordNet sense inventory and the underlying use of wordnet design principles, particularly the significance of WordNet-style relations.	0
1868	1868	S04-wsd-subcategorization-acq	Introduction	1	5	0.076923076923077	0.080645161290323	Gold standard evaluation approaches to evaluating word sense disambiguation (WSD) systems often suffer due to the choice of inventory.	0
1869	1869	S04-wsd-subcategorization-acq	Introduction	2	6	0.153846153846154	0.096774193548387	Fundamentally, the sense distinctions (e.g., in WordNet) tend to be very fine-grained which makes the disambiguation task highly difficult.	0
1870	1870	S04-wsd-subcategorization-acq	Introduction	3	7	0.230769230769231	0.112903225806452	Because the best level of sense granularity is likely to be application-dependent, a good alternative is to evaluate WSD systems in a task-based environment.	0
1871	1871	S04-wsd-subcategorization-acq	Introduction	4	8	0.307692307692308	0.129032258064516	We propose task-based evaluation in the context of automatic subcategorization frame (SCF) acquisition where the optimal sense granularity is fairly coarse.	1
1872	1872	S04-wsd-subcategorization-acq	Introduction	5	9	0.384615384615385	0.145161290322581	Automatic subcategorization acquisition is an important NLP task since access to a comprehensive anda ccurate subcategorization lexicon, acquired via automatic means, is vital e.g. for the development of successful parsing technology.	0
1873	1873	S04-wsd-subcategorization-acq	Introduction	6	10	0.461538461538462	0.161290322580645	It is also a suitable task for evaluation of WSD systems because SCF frequencies are known to vary with word senses from one corpus / text type to another.	0
1874	1874	S04-wsd-subcategorization-acq	Introduction	7	11	0.538461538461538	0.17741935483871	While most current systems for subcategorization acquisition are purely syntax-driven and do not employ WSD, Korhonun and Preiss (2003) have recently proposed a method which makes use of word sense.	0
1875	1875	S04-wsd-subcategorization-acq	Introduction	8	12	0.615384615384615	0.193548387096774	This method guides the acquisition process using back-off (i.e., probability) estimates based on verbs different senses in corpora.	0
1876	1876	S04-wsd-subcategorization-acq	Introduction	9	13	0.692307692307692	0.209677419354839	Where the senses are detected correctly, the method improves system performance considerably as the estimates help to correct the acquired SCF distribution and deal with sparse data.	0
1877	1877	S04-wsd-subcategorization-acq	Introduction	10	14	0.769230769230769	0.225806451612903	Our WSD evaluation makes use of this method.	0
1878	1878	S04-wsd-subcategorization-acq	Introduction	11	15	0.846153846153846	0.241935483870968	The paper is structured as follows: Section 2 describes the SCF acquisition system, and shows how the WSD answers can improve performance.	0
1879	1879	S04-wsd-subcategorization-acq	Introduction	12	16	0.923076923076923	0.258064516129032	The evaluation corpus and the evaluation method are introduced in Section 3.	0
1880	1880	S04-wsd-subcategorization-acq	Introduction	13	17	1.0	0.274193548387097	We present the effect of WSD accuracy on the performance of SCF acquisition in Section 4, and draw our conclusions in Section 5.	0
1959	1959	S07-1	Description of the task	1	34	0.045454545454546	0.203592814371257	This is an application-driven task, where the application is a fixed CLIR system.	0
1960	1960	S07-1	Description of the task	2	35	0.090909090909091	0.209580838323353	Participants disambiguate text by assigning WordNet 1.6 synsets and the system will do the expansion to other languages, index the expanded documents and run the retrieval for all the languages in batch.	1
1961	1961	S07-1	Description of the task	3	36	0.136363636363636	0.215568862275449	The retrieval results are taken as the measure for fitness of the disambiguation.	0
1962	1962	S07-1	Description of the task	4	37	0.181818181818182	0.221556886227545	The modules and rules for the expansion and the retrieval will be exactly the same for all participants.	0
1963	1963	S07-1	Description of the task	5	38	0.227272727272727	0.227544910179641	We proposed two specific subtasks:	0
1964	1964	S07-1	Description of the task	6	39	0.272727272727273	0.233532934131737	1. Participants disambiguate the corpus, the corpus is expanded to synonyms/translations and we measure the effects on IR/CLIR.	0
1965	1965	S07-1	Description of the task	7	40	0.318181818181818	0.239520958083832	Topics 2 are not processed.	0
1966	1966	S07-1	Description of the task	8	41	0.363636363636364	0.245508982035928	2. Participants disambiguate the topics per language, we expand the queries to synonyms/translations and we measure the effects on IR/CLIR.	0
1967	1967	S07-1	Description of the task	9	42	0.409090909090909	0.251497005988024	Documents are not processed	0
1968	1968	S07-1	Description of the task	10	43	0.454545454545455	0.25748502994012	The corpora and topics were obtained from the ad-hoc CLEF tasks.	0
1969	1969	S07-1	Description of the task	11	44	0.5	0.263473053892216	The supported languages in the topics are English and Spanish, but in order to limit the scope of the exercise we decided to only use English documents.	0
1970	1970	S07-1	Description of the task	12	45	0.545454545454545	0.269461077844311	The participants only had to disambiguate the English topics and documents.	0
1971	1971	S07-1	Description of the task	13	46	0.590909090909091	0.275449101796407	Note that most WSD systems only run on English text.	0
1972	1972	S07-1	Description of the task	14	47	0.636363636363636	0.281437125748503	Due to these limitations, we had the following evaluation settings: IR with WSD of topics , where the participants disambiguate the documents, the disambiguated documents are expanded to synonyms, and the original topics are used for querying.	0
1973	1973	S07-1	Description of the task	15	48	0.681818181818182	0.287425149700599	All documents and topics are in English.	0
1974	1974	S07-1	Description of the task	16	49	0.727272727272727	0.293413173652695	IR with WSD of documents , where the participants disambiguate the topics, the disambiguated topics are expanded and used for querying the original documents.	0
1975	1975	S07-1	Description of the task	17	50	0.772727272727273	0.29940119760479	All documents and topics are in English.	0
1976	1976	S07-1	Description of the task	18	51	0.818181818181818	0.305389221556886	CLIR with WSD of documents , where the participants disambiguate the documents, the disambiguated documents are translated, and the original topics in Spanish are used for querying.	0
1977	1977	S07-1	Description of the task	19	52	0.863636363636364	0.311377245508982	The documents are in English and the topics are in Spanish.	0
1978	1978	S07-1	Description of the task	20	53	0.909090909090909	0.317365269461078	We decided to focus on CLIR for evaluation, given the difficulty of improving IR.	0
1979	1979	S07-1	Description of the task	21	54	0.954545454545455	0.323353293413174	The IR results are given as illustration, and as an upperbound of the CLIR task.	0
1980	1980	S07-1	Description of the task	22	55	1.0	0.329341317365269	This use of IR results as a reference for CLIR systems is customary in the CLIR community (Harman, 2005).	0
2262	2262	S07-4	Task Description and Related Work	1	7	0.055555555555556	0.066666666666667	The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities -honey bee, for example, shows an instance of the Product-Producer relation.	1
2263	2263	S07-4	Task Description and Related Work	2	8	0.111111111111111	0.076190476190476	The classification occurs in the context of a sentence in a written English text.	0
2264	2264	S07-4	Task Description and Related Work	3	9	0.166666666666667	0.085714285714286	Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on.	0
2265	2265	S07-4	Task Description and Related Work	4	10	0.222222222222222	0.095238095238095	The recognition of textual entailment  is an example of successful use of this type of deeper analysis in high-end NLP applications.	0
2266	2266	S07-4	Task Description and Related Work	5	11	0.277777777777778	0.104761904761905	The literature shows a wide variety of methods of nominal relation classification.	0
2267	2267	S07-4	Task Description and Related Work	6	12	0.333333333333333	0.114285714285714	They depend as much on the training data as on the domain of application and the available resources.	0
2268	2268	S07-4	Task Description and Related Work	7	13	0.388888888888889	0.123809523809524	Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound.	0
2269	2269	S07-4	Task Description and Related Work	8	14	0.444444444444444	0.133333333333333	Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level.	0
2270	2270	S07-4	Task Description and Related Work	9	15	0.5	0.142857142857143	Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005;	0
2271	2271	S07-4	Task Description and Related Work	10	16	0.555555555555556	0.152380952380952	Turney, 2005;Nastase et al., 2006) have used their class scheme and data set.	0
2272	2272	S07-4	Task Description and Related Work	11	17	0.611111111111111	0.161904761904762	Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005).	0
2273	2273	S07-4	Task Description and Related Work	12	18	0.666666666666667	0.171428571428571	Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations.	0
2274	2274	S07-4	Task Description and Related Work	13	19	0.722222222222222	0.180952380952381	Stephens et al. (2001) propose 17 classes targeted to relations between genes.	0
2275	2275	S07-4	Task Description and Related Work	14	20	0.777777777777778	0.19047619047619	Lapata (2002) presents a binary classification of relations in nominalizations.	0
2276	2276	S07-4	Task Description and Related Work	15	21	0.833333333333333	0.2	There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications.	0
2277	2277	S07-4	Task Description and Related Work	16	22	0.888888888888889	0.20952380952381	For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text.	0
2278	2278	S07-4	Task Description and Related Work	17	23	0.944444444444444	0.219047619047619	We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms.	0
2279	2279	S07-4	Task Description and Related Work	18	24	1.0	0.228571428571429	We do not presume to propose a single classification scheme, however alluring it would	0
2362	2362	S07-5	abstract	1	2	0.5	0.021505376344086	The Multilingual Chinese-English lexical sample task at SemEval-2007 provides a framework to evaluate Chinese word sense disambiguation and to promote research.	1
2363	2363	S07-5	abstract	2	3	1.0	0.032258064516129	This paper reports on the task preparation and the results of six participants.	0
2443	2443	S07-5	Conclusion	1	83	0.090909090909091	0.89247311827957	The goal of this task is to create a framework to evaluate Chinese word sense disambiguation and to promote research.	1
2444	2444	S07-5	Conclusion	2	84	0.181818181818182	0.903225806451613	Together six teams participate in this WSD task, four of them adopt supervised learning methods and two of them used unsupervised algorithms.	0
2445	2445	S07-5	Conclusion	3	85	0.272727272727273	0.913978494623656	All of the four supervised learning systems exceed obviously the baseline obtained by the most frequent sense.	0
2446	2446	S07-5	Conclusion	4	86	0.363636363636364	0.924731182795699	It is noted that the performances of the first three systems are very close.	0
2447	2447	S07-5	Conclusion	5	87	0.454545454545455	0.935483870967742	Two unsupervised methods' scores are below the baseline.	0
2448	2448	S07-5	Conclusion	6	88	0.545454545454545	0.946236559139785	More unlabeled data maybe improve their performance.	0
2449	2449	S07-5	Conclusion	7	89	0.636363636363636	0.956989247311828	Although the SRCB-WSD system got the highest scores among the six participants, it does not perform always better than other system from table 2 and table 3.	0
2450	2450	S07-5	Conclusion	8	90	0.727272727272727	0.967741935483871	But to each word, the four supervised systems always predict correctly more instances than the two un-supervised systems.	0
2451	2451	S07-5	Conclusion	9	91	0.818181818181818	0.978494623655914	Besides the corpus, we provide a specification of the PoS tag set.	0
2452	2452	S07-5	Conclusion	10	92	0.909090909090909	0.989247311827957	Only SRCB-WSD system utilized this knowledge in feature selection.	0
2453	2453	S07-5	Conclusion	11	93	1.0	1.0	We will provide more instances in the next campaign.	0
2454	2454	S07-6	title	1	1	1.0	0.007042253521127	SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions	1
2455	2455	S07-6	abstract	1	2	0.142857142857143	0.014084507042254	The SemEval-2007 task to disambiguate prepositions was designed as a lexical sample task.	0
2456	2456	S07-6	abstract	2	3	0.285714285714286	0.02112676056338	A set of over 25,000 instances was developed, covering 34 of the most frequent English prepositions, with two-thirds of the instances for training and one-third as the test set.	0
2457	2457	S07-6	abstract	3	4	0.428571428571429	0.028169014084507	Each instance identified a preposition to be tagged in a full sentence taken from the FrameNet corpus (mostly from the British National Corpus).	0
2458	2458	S07-6	abstract	4	5	0.571428571428571	0.035211267605634	Definitions from the Oxford Dictionary of English formed the sense inventories.	0
2459	2459	S07-6	abstract	5	6	0.714285714285714	0.042253521126761	Three teams participated, with all achieving supervised results significantly better than baselines, with a high fine-grained precision of 0.693.	0
2460	2460	S07-6	abstract	6	7	0.857142857142857	0.049295774647887	This level is somewhat similar to results on lexical sample tasks with open class words, indicating that significant progress has been made.	0
2461	2461	S07-6	abstract	7	8	1.0	0.056338028169014	The data generated in the task provides ample opportunitites for further investigations of preposition behavior.	0
2613	2613	S07-7	Task Setup	1	18	1.0	0.195652173913043	The task required participating systems to annotate open-class words (i.e. nouns, verbs, adjectives, and adverbs) in a test corpus with the most appropriate sense from a coarse-grained version of the WordNet sense inventory.	1
2692	2692	S07-8	Introduction	1	5	0.03125	0.039370078740158	Both word sense disambiguation and named entity recognition have benefited enormously from shared task evaluations, for example in the Senseval, MUC and CoNLL frameworks.	0
2693	2693	S07-8	Introduction	2	6	0.0625	0.047244094488189	Similar campaigns have not been developed for the resolution of figurative language, such as metaphor, metonymy, idioms and irony.	0
2694	2694	S07-8	Introduction	3	7	0.09375	0.055118110236221	However, resolution of figurative language is an important complement to and extension of word sense disambiguation as it often deals with word senses that are not listed in the lexicon.	0
2695	2695	S07-8	Introduction	4	8	0.125	0.062992125984252	For example, the meaning of stopover in the sentence	0
2696	2696	S07-8	Introduction	5	9	0.15625	0.070866141732284	"He saw teaching as a stopover on his way to bigger things is a metaphorical sense of the sense ""stopping place in a physical journey"", with the literal sense listed in WordNet 2.0 but the metaphorical one not being listed."	0
2697	2697	S07-8	Introduction	6	10	0.1875	0.078740157480315	1	0
2698	2698	S07-8	Introduction	7	11	0.21875	0.086614173228347	The same holds for the metonymic reading of rattlesnake (for the animal's meat) in Roast rattlesnake tastes like chicken.	0
2699	2699	S07-8	Introduction	8	12	0.25	0.094488188976378	2 Again, the meat read-ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is.	0
2700	2700	S07-8	Introduction	9	13	0.28125	0.102362204724409	As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997;	0
2701	2701	S07-8	Introduction	10	14	0.3125	0.110236220472441	Hobbs et al., 1993;	0
2702	2702	S07-8	Introduction	11	15	0.34375	0.118110236220472	Barnden et al., 2003, among others) carry out only smallscale evaluations.	0
2703	2703	S07-8	Introduction	12	16	0.375	0.125984251968504	In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994;	0
2704	2704	S07-8	Introduction	13	17	0.40625	0.133858267716535	Nissim and Markert, 2003;Mason, 2004;Peirsman, 2006;	0
2705	2705	S07-8	Introduction	14	18	0.4375	0.141732283464567	Birke and Sarkaar, 2006;Krishnakamuran and Zhu, 2007).	0
2706	2706	S07-8	Introduction	15	19	0.46875	0.149606299212598	Still, apart from (Nissim and Markert, 2003;	0
2707	2707	S07-8	Introduction	16	20	0.5	0.15748031496063	Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks.	0
2708	2708	S07-8	Introduction	17	21	0.53125	0.165354330708661	This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy.	0
2709	2709	S07-8	Introduction	18	22	0.5625	0.173228346456693	In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat.	0
2710	2710	S07-8	Introduction	19	23	0.59375	0.181102362204724	Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there.	0
2711	2711	S07-8	Introduction	20	24	0.625	0.188976377952756	(1) Sex, drugs, and Vietnam have haunted Bill Clinton's campaign.	0
2712	2712	S07-8	Introduction	21	25	0.65625	0.196850393700787	In Ex. 2 and 3, BMW, the name of a company, stands for its index on the stock market, or a vehicle manufactured by BMW, respectively.	0
2713	2713	S07-8	Introduction	22	26	0.6875	0.204724409448819	(2) BMW slipped 4p to 31p	0
2714	2714	S07-8	Introduction	23	27	0.71875	0.21259842519685	(3) His BMW went on to race at Le Mans	0
2715	2715	S07-8	Introduction	24	28	0.75	0.220472440944882	The importance of resolving metonymies has been shown for a variety of NLP tasks, such as ma-chine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993), anaphora resolution (Harabagiu, 1998;	0
2716	2716	S07-8	Introduction	25	29	0.78125	0.228346456692913	Markert and Hahn, 2002) and geographical information retrieval (Leveling and Hartrumpf, 2006).	0
2717	2717	S07-8	Introduction	26	30	0.8125	0.236220472440945	Although metonymic readings are, like all figurative readings, potentially open ended and can be innovative, the regularity of usage for word groups helps in establishing a common evaluation framework.	0
2718	2718	S07-8	Introduction	27	31	0.84375	0.244094488188976	Many other location names, for instance, can be used in the same fashion as Vietnam in Ex. 1.	0
2719	2719	S07-8	Introduction	28	32	0.875	0.251968503937008	Thus, given a semantic class (e.g. location), one can specify several regular metonymic patterns (e.g. place-for-event) that instances of the class are likely to undergo.	0
2720	2720	S07-8	Introduction	29	33	0.90625	0.259842519685039	In addition to literal readings, regular metonymic patterns and innovative metonymic readings, there can also be so-called mixed readings, similar to zeugma, where both a literal and a metonymic reading are evoked (Nunberg, 1995).	0
2721	2721	S07-8	Introduction	30	34	0.9375	0.267716535433071	The metonymy task is a lexical sample task for English, consisting of two subtasks, one concentrating on the semantic class location, exemplified by country names, and another one concentrating on organisation, exemplified by company names.	0
2722	2722	S07-8	Introduction	31	35	0.96875	0.275590551181102	Participants had to automatically classify preselected country/company names as having a literal or non-literal meaning, given a four-sentence context.	1
2723	2723	S07-8	Introduction	32	36	1.0	0.283464566929134	Additionally, participants could attempt finer-grained interpretations, further specifying readings into prespecified metonymic patterns (such as place-for-event) and recognising innovative readings.	0
2946	2946	S07-10	abstract	1	2	0.2	0.015873015873016	In this paper we describe the English Lexical Substitution task for SemEval.	0
2947	2947	S07-10	abstract	2	3	0.4	0.023809523809524	In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.	1
2948	2948	S07-10	abstract	3	4	0.6	0.031746031746032	The task involves both finding the synonyms and disambiguating the context.	1
2949	2949	S07-10	abstract	4	5	0.8	0.03968253968254	Participating systems are free to use any lexical resource.	0
2950	2950	S07-10	abstract	5	6	1.0	0.047619047619048	There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is.	0
3076	3076	S07-11	Introduction	1	6	0.047619047619048	0.08	As part of the SemEval-2007 evaluation exercise, we organized an English lexical sample task for word sense disambiguation (WSD), where the senseannotated examples were semi-automatically gathered from word-aligned English-Chinese parallel texts.	1
3077	3077	S07-11	Introduction	2	7	0.095238095238095	0.093333333333333	Two tracks were organized for this task, each gathering data from a different corpus.	0
3078	3078	S07-11	Introduction	3	8	0.142857142857143	0.106666666666667	In this paper, we describe our motivation for organizing the task, our task framework, and the results of participants.	0
3079	3079	S07-11	Introduction	4	9	0.19047619047619	0.12	Past research has shown that supervised learning is one of the most successful approaches to WSD.	0
3080	3080	S07-11	Introduction	5	10	0.238095238095238	0.133333333333333	However, this approach involves the collection of a large text corpus in which each ambiguous word has been annotated with the correct sense to serve as training data.	0
3081	3081	S07-11	Introduction	6	11	0.285714285714286	0.146666666666667	Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available.	0
3082	3082	S07-11	Introduction	7	12	0.333333333333333	0.16	An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users.	0
3083	3083	S07-11	Introduction	8	13	0.380952380952381	0.173333333333333	Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task.	0
3084	3084	S07-11	Introduction	9	14	0.428571428571429	0.186666666666667	In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmyth 1 was used as the sense inventory for verbs.	0
3085	3085	S07-11	Introduction	10	15	0.476190476190476	0.2	Another source of potential training data is parallel texts.	0
3086	3086	S07-11	Introduction	11	16	0.523809523809524	0.213333333333333	Our past research in (Ng et al., 2003;	0
3087	3087	S07-11	Introduction	12	17	0.571428571428571	0.226666666666667	Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD.	0
3088	3088	S07-11	Introduction	13	18	0.619047619047619	0.24	"Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English side of a word-aligned parallel text can then serve as the training data, as they are considered to have been disambiguated and ""sense-tagged"" by the appropriate Chinese translations."	0
3089	3089	S07-11	Introduction	14	19	0.666666666666667	0.253333333333333	Using the above approach, we gathered the training and test examples for our task from parallel texts.	0
3090	3090	S07-11	Introduction	15	20	0.714285714285714	0.266666666666667	Note that our examples are collected without manually annotating each individual ambiguous word occurrence, allowing us to gather our examples in a much shorter time.	0
3091	3091	S07-11	Introduction	16	21	0.761904761904762	0.28	This contrasts with the setting of the English lexical sample task in previous SENSE-VAL evaluations.	0
3092	3092	S07-11	Introduction	17	22	0.80952380952381	0.293333333333333	In the English lexical sample task of SENSEVAL-2, the sense tagged data were created through manual annotation by trained lexicographers.	0
3093	3093	S07-11	Introduction	18	23	0.857142857142857	0.306666666666667	In SENSEVAL-3, the data were gathered through manual sense annotation by Internet users.	0
3094	3094	S07-11	Introduction	19	24	0.904761904761905	0.32	In the next section, we describe in more detail the process of gathering examples from parallel texts and the two different parallel corpora we used.	0
3095	3095	S07-11	Introduction	20	25	0.952380952380952	0.333333333333333	We then give a brief description of each of the partici-pating systems.	0
3096	3096	S07-11	Introduction	21	26	1.0	0.346666666666667	In Section 4, we present the results obtained by the participants, before concluding in Section 5.	0
3267	3267	S07-13	Introduction	1	4	0.043478260869565	0.03030303030303	Finding information about people in the World Wide Web is one of the most common activities of Internet users.	0
3268	3268	S07-13	Introduction	2	5	0.086956521739131	0.037878787878788	Person names, however, are highly ambiguous.	0
3269	3269	S07-13	Introduction	3	6	0.130434782608696	0.045454545454546	In most cases, the results for a person name search are a mix of pages about different people sharing the same name.	0
3270	3270	S07-13	Introduction	4	7	0.173913043478261	0.053030303030303	The user is then forced either to add terms to the query (probably losing recall and focusing on one single aspect of the person), or to browse every document in order to filter the information about the person he is actually looking for.	0
3271	3271	S07-13	Introduction	5	8	0.217391304347826	0.060606060606061	In an ideal system the user would simply type a person name, and receive search results clustered according to the different people sharing that name.	0
3272	3272	S07-13	Introduction	6	9	0.260869565217391	0.068181818181818	And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants: systems receive a set of web pages (which are the result of a web search for a person name), and they have to cluster them in as many sets as entities sharing the name.	1
3273	3273	S07-13	Introduction	7	10	0.304347826086957	0.075757575757576	This task has close links with Word Sense Disambiguation (WSD), which is generally formulated as the task of deciding which sense a word has in a given con-text.	0
3274	3274	S07-13	Introduction	8	11	0.347826086956522	0.083333333333333	In both cases, the problem addressed is the resolution of the ambiguity in a natural language expression.	0
3275	3275	S07-13	Introduction	9	12	0.391304347826087	0.090909090909091	A couple of differences make our problem different.	0
3276	3276	S07-13	Introduction	10	13	0.434782608695652	0.098484848484849	WSD is usually focused on openclass words (common nouns, adjectives, verbs and adverbs).	0
3277	3277	S07-13	Introduction	11	14	0.478260869565217	0.106060606060606	The first difference is that boundaries between word senses in a dictionary are often subtle or even conflicting, making binary decisions harder and sometimes even useless depending on the application.	0
3278	3278	S07-13	Introduction	12	15	0.521739130434783	0.113636363636364	In contrast, distinctions between people should be easier to establish.	0
3279	3279	S07-13	Introduction	13	16	0.565217391304348	0.121212121212121	The second difference is that WSD usually operates with a dictionary containing a relatively small number of senses that can be assigned to each word.	0
3280	3280	S07-13	Introduction	14	17	0.608695652173913	0.128787878787879	"Our task is rather a case of Word Sense Discrimination, because the number of ""senses"" (actual people) is unknown a priori, and it is in average much higher than in the WSD task (there are 90,000 different names shared by 100 million people according to the U.S. Census Bureau)."	0
3281	3281	S07-13	Introduction	15	18	0.652173913043478	0.136363636363636	There is also a strong relation of our proposed task with the Co-reference Resolution problem, focused on linking mentions (including pronouns) in a text.	0
3282	3282	S07-13	Introduction	16	19	0.695652173913043	0.143939393939394	Our task can be seen as a co-reference resolution problem where the focus is on solving interdocument co-reference, disregarding the linking of all the mentions of an entity inside each document.	0
3283	3283	S07-13	Introduction	17	20	0.739130434782609	0.151515151515152	"An early work in name disambiguation (Bagga and Baldwin, 1998) uses the similarity between documents in a Vector Space using a ""bag of words"" representation."	0
3284	3284	S07-13	Introduction	18	21	0.782608695652174	0.159090909090909	An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information.	0
3285	3285	S07-13	Introduction	19	22	0.826086956521739	0.166666666666667	Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that two names refer to the same individual 1 .	0
3286	3286	S07-13	Introduction	20	23	0.869565217391304	0.174242424242424	The paper is organized as follows.	0
3287	3287	S07-13	Introduction	21	24	0.91304347826087	0.181818181818182	Section 2 provides a description of the experimental methodology, the training and test data provided to the participants, the evaluation measures, baseline systems and the campaign design.	0
3288	3288	S07-13	Introduction	22	25	0.956521739130435	0.189393939393939	Section 3 gives a description of the participant systems and provides the evaluation results.	0
3289	3289	S07-13	Introduction	23	26	1.0	0.196969696969697	Finally, Section 4 presents some conclusions.	0
3397	3397	S07-14	abstract	1	2	0.5	0.017543859649123	"The ""Affective Text"" task focuses on the classification of emotions and valence (positive/negative polarity) in news headlines, and is meant as an exploration of the connection between emotions and lexical semantics."	1
3398	3398	S07-14	abstract	2	3	1.0	0.026315789473684	In this paper, we describe the data set used in the evaluation and the results obtained by the participating systems.	0
3511	3511	S07-15	abstract	1	2	0.333333333333333	0.013422818791946	The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations.	1
3512	3512	S07-15	abstract	2	3	0.666666666666667	0.02013422818792	It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise evaluation of temporal relations.	0
3513	3513	S07-15	abstract	3	4	1.0	0.026845637583893	The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing.	0
3660	3660	S07-16	abstract	1	2	0.2	0.015384615384615	This task tries to establish the relative quality of available semantic resources (derived by manual or automatic means).	0
3661	3661	S07-16	abstract	2	3	0.4	0.023076923076923	The quality of each large-scale knowledge resource is indirectly evaluated on a Word Sense Disambiguation task.	1
3662	3662	S07-16	abstract	3	4	0.6	0.030769230769231	In particular, we use Senseval-3 and SemEval-2007 English Lexical Sample tasks as evaluation bechmarks to evaluate the relative quality of each resource.	0
3663	3663	S07-16	abstract	4	5	0.8	0.038461538461539	Furthermore, trying to be as neutral as possible with respect the knowledge bases studied, we apply systematically the same disambiguation method to all the resources.	0
3664	3664	S07-16	abstract	5	6	1.0	0.046153846153846	A completely different behaviour is observed on both lexical data sets (Senseval-3 and SemEval-2007).	0
3792	3792	S07-17	Introduction	1	4	0.333333333333333	0.037383177570094	Correctly disambiguating words (WSD), and correctly identifying the semantic relationships between those words (SRL), is an important step for building successful natural language processing applications, such as text summarization, question answering, and machine translation.	1
3793	3793	S07-17	Introduction	2	5	0.666666666666667	0.046728971962617	SemEval-2007 Task-17 (English Lexical Sample, SRL and All-Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.	0
3794	3794	S07-17	Introduction	3	6	1.0	0.05607476635514	It includes three subtasks: i) the traditional All-Words task comprising fine-grained word sense disambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grained word sense disambiguation on a selected set of lexemes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.	0
3897	3897	S07-18	abstract	1	2	0.25	0.010928961748634	In this paper, we present the details of the Arabic Semantic Labeling task.	0
3898	3898	S07-18	abstract	2	3	0.5	0.016393442622951	We describe some of the features of Arabic that are relevant for the task.	0
3899	3899	S07-18	abstract	3	4	0.75	0.021857923497268	The task comprises two subtasks: Arabic word sense disambiguation and Arabic semantic role labeling.	1
3900	3900	S07-18	abstract	4	5	1.0	0.027322404371585	The task focuses on modern standard Arabic.	0
4080	4080	S07-19	abstract	1	2	0.25	0.019230769230769	This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects).	1
4081	4081	S07-19	abstract	2	3	0.5	0.028846153846154	The training data was FN annotated sentences.	0
4082	4082	S07-19	abstract	3	4	0.75	0.038461538461539	In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles.	0
4083	4083	S07-19	abstract	4	5	1.0	0.048076923076923	Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.	0
4187	4187	S10-1	Introduction	1	5	0.038461538461539	0.037593984962406	The task of coreference resolution, defined as the identification of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community.	0
4188	4188	S10-1	Introduction	2	6	0.076923076923077	0.045112781954887	(1)	0
4189	4189	S10-1	Introduction	3	7	0.115384615384615	0.052631578947369	Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months.	0
4190	4190	S10-1	Introduction	4	8	0.153846153846154	0.06015037593985	The league is reviewing security at all ballparks to crack down on spectator violence.	0
4191	4191	S10-1	Introduction	5	9	0.192307692307692	0.067669172932331	Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation.	0
4192	4192	S10-1	Introduction	6	10	0.230769230769231	0.075187969924812	There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open:	0
4193	4193	S10-1	Introduction	7	11	0.269230769230769	0.082706766917293		0
4194	4194	S10-1	Introduction	8	12	0.307692307692308	0.090225563909775	To what extent is it possible to implement a general coreference resolution system portable to different languages?	0
4195	4195	S10-1	Introduction	9	13	0.346153846153846	0.097744360902256	How much language-specific tuning is necessary?	0
4196	4196	S10-1	Introduction	10	14	0.384615384615385	0.105263157894737		0
4197	4197	S10-1	Introduction	11	15	0.423076923076923	0.112781954887218	How helpful are morphology, syntax and semantics for solving coreference relations?	0
4198	4198	S10-1	Introduction	12	16	0.461538461538462	0.120300751879699	How much preprocessing is needed?	0
4199	4199	S10-1	Introduction	13	17	0.5	0.12781954887218	Does its quality (perfect linguistic input versus noisy automatic input) really matter?	0
4200	4200	S10-1	Introduction	14	18	0.538461538461538	0.135338345864662		0
4201	4201	S10-1	Introduction	15	19	0.576923076923077	0.142857142857143	How (dis)similar are different coreference evaluation metrics-MUC, B-CUBED, CEAF and BLANC?	0
4202	4202	S10-1	Introduction	16	20	0.615384615384615	0.150375939849624	Do they all provide the same ranking?	0
4203	4203	S10-1	Introduction	17	21	0.653846153846154	0.157894736842105	Are they correlated?	0
4204	4204	S10-1	Introduction	18	22	0.692307692307692	0.165413533834586	Our goal was to address these questions in a shared task.	0
4205	4205	S10-1	Introduction	19	23	0.730769230769231	0.172932330827068	Given six datasets in Catalan, Dutch, English, German, Italian, and Spanish, the task we present involved automatically detecting full coreference chains-composed of named entities (NEs), pronouns, and full noun phrases-in four different scenarios.	1
4206	4206	S10-1	Introduction	20	24	0.769230769230769	0.180451127819549	For more information, the reader is referred to the task website.	0
4207	4207	S10-1	Introduction	21	25	0.807692307692308	0.18796992481203	1	0
4208	4208	S10-1	Introduction	22	26	0.846153846153846	0.195488721804511	The rest of the paper is organized as follows.	0
4209	4209	S10-1	Introduction	23	27	0.884615384615385	0.203007518796992	Section 2 presents the corpora from which the task datasets were extracted, and the automatic tools used to preprocess them.	0
4210	4210	S10-1	Introduction	24	28	0.923076923076923	0.210526315789474	In Section 3, we describe the task by providing information about the data format, evaluation settings, and evaluation metrics.	0
4211	4211	S10-1	Introduction	25	29	0.961538461538462	0.218045112781955	Participating systems are described in Section 4, and their results are analyzed and compared in Section 5.	0
4212	4212	S10-1	Introduction	26	30	1.0	0.225563909774436	Finally, Section 6 concludes.	0
4317	4317	S10-2	abstract	1	2	0.333333333333333	0.013605442176871	In this paper we describe the SemEval-2010 Cross-Lingual Lexical Substitution task, where given an English target word in context, participating systems had to find an alternative substitute word or phrase in Spanish.	1
4318	4318	S10-2	abstract	2	3	0.666666666666667	0.020408163265306	The task is based on the English Lexical Substitution task run at SemEval-2007.	0
4319	4319	S10-2	abstract	3	4	1.0	0.027210884353742	In this paper we provide background and motivation for the task, we describe the data annotation process and the scoring system, and present the results of the participating systems.	0
4583	4583	S10-5	abstract	1	2	0.25	0.018691588785047	This paper describes Task 5 of the Workshop on Semantic Evaluation 2010 (SemEval-2010).	0
4584	4584	S10-5	abstract	2	3	0.5	0.02803738317757	Systems are to automatically assign keyphrases or keywords to given scientific articles.	1
4585	4585	S10-5	abstract	3	4	0.75	0.037383177570094	The participating systems were evaluated by matching their extracted keyphrases against manually assigned ones.	0
4586	4586	S10-5	abstract	4	5	1.0	0.046728971962617	We present the overall ranking of the submitted systems and discuss our findings to suggest future directions for this task.	0
4690	4690	S10-7	abstract	1	2	0.25	0.014814814814815	We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise.	0
4691	4691	S10-7	abstract	2	3	0.5	0.022222222222222	This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects.	0
4692	4692	S10-7	abstract	3	4	0.75	0.02962962962963	Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing.	1
4693	4693	S10-7	abstract	4	5	1.0	0.037037037037037	We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.	0
4828	4828	S10-8	Introduction	1	5	0.083333333333333	0.035971223021583	SemEval-2010	0
4829	4829	S10-8	Introduction	2	6	0.166666666666667	0.043165467625899	Task 8 focused on semantic relations between pairs of nominals.	0
4830	4830	S10-8	Introduction	3	7	0.25	0.050359712230216	"For example, tea and ginseng are in an ENTITY-ORIGIN relation in ""The cup contained tea from dried ginseng.""."	0
4831	4831	S10-8	Introduction	4	8	0.333333333333333	0.057553956834532	The automatic recognition of semantic relations has many applications, such as information extraction, document summarization, machine translation, or construction of thesauri and semantic networks.	0
4832	4832	S10-8	Introduction	5	9	0.416666666666667	0.064748201438849	It can also facilitate auxiliary tasks such as word sense disambiguation, language modeling, paraphrasing, and recognizing textual entailment.	0
4833	4833	S10-8	Introduction	6	10	0.5	0.071942446043166	Our goal was to create a testbed for automatic classification of semantic relations.	0
4834	4834	S10-8	Introduction	7	11	0.583333333333333	0.079136690647482	In developing the task we met several challenges: selecting a suitable set of relations, specifying the annotation procedure, and deciding on the details of the task itself.	0
4835	4835	S10-8	Introduction	8	12	0.666666666666667	0.086330935251799	They are discussed briefly in Section 2; see also Hendrickx et al. (2009), which includes a survey of related work.	0
4836	4836	S10-8	Introduction	9	13	0.75	0.093525179856115	The direct predecessor of Task 8 was Classification of semantic relations between nominals, Task 4 at SemEval-1 (Girju et al., 2009), which had a separate binary-labeled dataset for each of seven relations.	0
4837	4837	S10-8	Introduction	10	14	0.833333333333333	0.100719424460432	We have defined SemEval-2010	0
4838	4838	S10-8	Introduction	11	15	0.916666666666667	0.107913669064748	Task 8 as a multi-way classification task in which the label for each example must be chosen from the complete set of ten relations and the mapping from nouns to argument slots is not provided in advance.	1
4839	4839	S10-8	Introduction	12	16	1.0	0.115107913669065	We also provide more data: 10,717 annotated examples, compared to 1,529 in SemEval-1 Task 4.	0
4994	4994	S10-9	Task Description	1	32	0.090909090909091	0.264462809917355	The Objective	0
4995	4995	S10-9	Task Description	2	33	0.181818181818182	0.272727272727273	For the purpose of the task, we focused on twoword NCs which are modifier-head pairs of nouns, such as apple pie or malaria mosquito.	0
4996	4996	S10-9	Task Description	3	34	0.272727272727273	0.28099173553719	"There are several ways to ""attack"" the paraphrase-based semantics of such NCs."	0
4997	4997	S10-9	Task Description	4	35	0.363636363636364	0.289256198347107	We have proposed a rather simple problem: assume that many paraphrases can be found -perhaps via clever Web search -but their relevance is up in the air.	0
4998	4998	S10-9	Task Description	5	36	0.454545454545455	0.297520661157025	Given sufficient training data, we seek to estimate the quality of candidate paraphrases in a test set.	0
4999	4999	S10-9	Task Description	6	37	0.545454545454545	0.305785123966942	Each NC in the training set comes with a long list of verbs in the infinitive (often with a preposition) which may paraphrase the NC adequately.	0
5000	5000	S10-9	Task Description	7	38	0.636363636363636	0.314049586776859	Examples of apt paraphrasing verbs: olive oilbe extracted from, drug death -be caused by, flu shot -prevent.	0
5001	5001	S10-9	Task Description	8	39	0.727272727272727	0.322314049586777	These lists have been constructed from human-proposed paraphrases.	0
5002	5002	S10-9	Task Description	9	40	0.818181818181818	0.330578512396694	For the training data, we also provide the participants with a quality score for each paraphrase, which is a simple count of the number of human subjects who proposed that paraphrase.	0
5003	5003	S10-9	Task Description	10	41	0.909090909090909	0.338842975206612	At test time, given a noun compound and a list of paraphrasing verbs, a participating system needs to produce aptness scores that correlate well (in terms of relative ranking) with the held out human judgments.	1
5004	5004	S10-9	Task Description	11	42	1.0	0.347107438016529	There may be a diverse range of paraphrases for a given compound, some of them in fact might be inappropriate, but it can be expected that the distribution over paraphrases estimated from a large number of subjects will indeed be representative of the compound's meaning.	0
5085	5085	S10-10	abstract	1	2	0.2	0.0125	"We describe the SemEval-2010 shared task on ""Linking Events and Their Participants in Discourse""."	0
5086	5086	S10-10	abstract	2	3	0.4	0.01875	This task is an extension to the classical semantic role labeling task.	0
5087	5087	S10-10	abstract	3	4	0.6	0.025	While semantic role labeling is traditionally viewed as a sentence-internal task, local semantic argument structures clearly interact with each other in a larger context, e.g., by sharing references to specific discourse entities or events.	0
5088	5088	S10-10	abstract	4	5	0.8	0.03125	In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist).	1
5089	5089	S10-10	abstract	5	6	1.0	0.0375	This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization.	0
5245	5245	S10-11	abstract	1	2	0.037037037037037	0.071428571428572	table .	0
5246	5246	S10-11	abstract	2	3	0.074074074074074	0.107142857142857	The goal of the task is to detect and analyze the event contents in real world Chinese news texts.	1
5247	5247	S10-11	abstract	3	4	0.111111111111111	0.142857142857143	It consists of finding key verbs or verb phrases to describe these events in the Chinese sentences after word segmentation and part-of-speech tagging, selecting suitable situation descriptions for them, and anchoring different situation arguments with suitable syntactic chunks in the sentence.	1
5248	5248	S10-11	abstract	4	5	0.148148148148148	0.178571428571429	Three main sub-tasks are as follows: (1) Target verb WSD; (2) Sentence SRL; (3) Event detection.	0
5249	5249	S10-11	abstract	5	6	0.185185185185185	0.214285714285714	We will select 100 high-frequency Chinese target verbs for this task.	0
5250	5250	S10-11	abstract	6	7	0.222222222222222	0.25	Among them, 30 verbs have multiple senses and 70 verbs have single sense.	0
5251	5251	S10-11	abstract	7	8	0.259259259259259	0.285714285714286	Each target verb will be assigned more than 50 annotated sentences to consist of training and test sets.	0
5252	5252	S10-11	abstract	8	9	0.296296296296296	0.321428571428571	Each annotated sentence will have following event information: (1) word segmentation and POS tags; (2) the target verb (or verb phrase) and its position in the sentence; (3) the event description (situation description formula or natural explanation text) of the target verb (or verb phrase) in the context of the sentences; (4) the chunks annotated with suitable syntactic constituent tags, functional tags and event argument role tags.	0
5253	5253	S10-11	abstract	9	10	0.333333333333333	0.357142857142857	The training and test set will be extracted from the data set with ratio 8:2.	0
5254	5254	S10-11	abstract	10	11	0.37037037037037	0.392857142857143	For the WSD subtask, we give two evaluation measures: WSD-Micro-Accuracy and WSD-Macro-Accuracy.	0
5255	5255	S10-11	abstract	11	12	0.407407407407407	0.428571428571429	The correct conditions are: the selected situation description formula and natural explanation text of the target verbs will be same with the gold-standard codes.	0
5256	5256	S10-11	abstract	12	13	0.444444444444444	0.464285714285714	We evaluated 27 multiple-sense target verbs in the test set.	0
5257	5257	S10-11	abstract	13	14	0.481481481481481	0.5	For the SRL subtask, we give three evaluation measures: Chunk-Precision, Chunk-Recall, and Chunk-F-measure.	0
5258	5258	S10-11	abstract	14	15	0.518518518518518	0.535714285714286	The correct conditions are: the recognized chunks should have the same boundaries, syntactic constituent and functional tags, and situation argument tags with the gold-standard argument chunks of the key verbs or verb phrases.	0
5259	5259	S10-11	abstract	15	16	0.555555555555556	0.571428571428571	We only select the key argument chunks (with semantic role tags: x, y, z, L or O) for evaluation.	0
5260	5260	S10-11	abstract	16	17	0.592592592592593	0.607142857142857	For the event detection subtask, we give two evaluation measures: Event-Micro-Accuracy and Event-Macro-Accuracy.	0
5261	5261	S10-11	abstract	17	18	0.62962962962963	0.642857142857143	The correct conditions are: (1)	0
5262	5262	S10-11	abstract	18	19	0.666666666666667	0.678571428571429	The event situation description formula and natural explanation text of the target verb should be same with the gold-standard ones; (2)	0
5263	5263	S10-11	abstract	19	20	0.703703703703704	0.714285714285714	All the argument chunks of the event descriptions should be same with the gold-standard ones; (3)	0
5264	5264	S10-11	abstract	20	21	0.740740740740741	0.75	The number of the recognized argument chunks should be same with the gold-standard one.	0
5265	5265	S10-11	abstract	21	22	0.777777777777778	0.785714285714286	8 participants downloaded the training and test data.	0
5266	5266	S10-11	abstract	22	23	0.814814814814815	0.821428571428571	Only 3 participants uploaded the final results.	0
5267	5267	S10-11	abstract	23	24	0.851851851851852	0.857142857142857	Among them, 1 participant (User ID = 156) submitted 4 results and 1 participant (User ID = 485) submitted 2 results.	0
5268	5268	S10-11	abstract	24	25	0.888888888888889	0.892857142857143	So we received 7 uploaded results for evaluation.	0
5269	5269	S10-11	abstract	25	26	0.925925925925926	0.928571428571429	The mean elaboration time of the test data is about 30 hours.	0
5270	5270	S10-11	abstract	26	27	0.962962962962963	0.964285714285714	The following is the evaluation result	0
5271	5271	S10-11	abstract	27	28	1.0	1.0	The results show the event detection task is still an open problem for exploring in the Chinese language.	0
5419	5419	S10-13	abstract	1	2	0.5	0.018181818181818	Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.	1
5420	5420	S10-13	abstract	2	3	1.0	0.027272727272727	Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish.	0
5529	5529	S10-14	abstract	1	2	0.333333333333333	0.016393442622951	This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction &amp; Disambiguation task, as well as the evaluation results of 26 participating systems.	0
5530	5530	S10-14	abstract	2	3	0.666666666666667	0.024590163934426	In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.	1
5531	5531	S10-14	abstract	3	4	1.0	0.032786885245902	Systems' answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.	0
5651	5651	S10-15	abstract	1	2	0.066666666666667	0.083333333333333	Introduction	0
5652	5652	S10-15	abstract	2	3	0.133333333333333	0.125	There are seven cases of grapheme to phoneme in a text to speech system (Yarowsky, 1997).	0
5653	5653	S10-15	abstract	3	4	0.2	0.166666666666667	Among them, the most difficult task is disambiguating the homograph word, which has the same POS but different pronunciation.	1
5654	5654	S10-15	abstract	4	5	0.266666666666667	0.208333333333333	In this case, different pronunciations of the same word always correspond to different word senses.	0
5655	5655	S10-15	abstract	5	6	0.333333333333333	0.25	Once the word senses are disambiguated, the problem of GTP is resolved.	0
5656	5656	S10-15	abstract	6	7	0.4	0.291666666666667	There is a little different from traditional WSD, in this task two or more senses may correspond to one pronunciation.	0
5657	5657	S10-15	abstract	7	8	0.466666666666667	0.333333333333333	That is, the sense granularity is coarser than WSD.	0
5658	5658	S10-15	abstract	8	9	0.533333333333333	0.375	"For example, the preposition """" has three senses: sense1 and sense2 have the same pronunciation {wei 4}, while sense3 corresponds to {wei 2}."	0
5659	5659	S10-15	abstract	9	10	0.6	0.416666666666667	In this task, to the target word, not only the pronunciations but also the sense labels are provided for training; but for test, only the pronunciations are evaluated.	0
5660	5660	S10-15	abstract	10	11	0.666666666666667	0.458333333333333	The challenge of this task is the much skewed distribution in real text: the most frequent pronunciation occupies usually over 80%.	0
5661	5661	S10-15	abstract	11	12	0.733333333333333	0.5	In this task, we will provide a large volume of training data (each homograph word has at least 300 instances) accordance with the truly distribution in real text.	0
5662	5662	S10-15	abstract	12	13	0.8	0.541666666666667	In the test data, we will provide at least 100 instances for each target word.	0
5663	5663	S10-15	abstract	13	14	0.866666666666667	0.583333333333333	The senses distribution in test data is the same as in training data.	0
5664	5664	S10-15	abstract	14	15	0.933333333333333	0.625	All instances come from People Daily newspaper (the most popular newspaper in Mandarin).	0
5665	5665	S10-15	abstract	15	16	1.0	0.666666666666667	Double blind annotations are executed manually, and a third annotator checks the annotation.	0
5800	5800	S10-17	Conclusions	1	127	0.111111111111111	0.940740740740741	Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges.	0
5801	5801	S10-17	Conclusions	2	128	0.222222222222222	0.948148148148148	The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems.	0
5802	5802	S10-17	Conclusions	3	129	0.333333333333333	0.955555555555556	With this paper we have motivated the creation of an all-words test dataset for WSD on the environment domain in several languages, and presented the overall design of this SemEval task.	1
5803	5803	S10-17	Conclusions	4	130	0.444444444444444	0.962962962962963	One of the goals of the exercise was to show that WSD systems could make use of unannotated background corpora to adapt to the domain and improve their results.	0
5804	5804	S10-17	Conclusions	5	131	0.555555555555556	0.97037037037037	Although it's early to reach hard conclusions, the results show that in each of the datasets, knowledge-based systems are able to improve their results using background text, and in two datasets the adaptation of knowledge-based systems leads to results over the MFS baseline.	0
5805	5805	S10-17	Conclusions	6	132	0.666666666666667	0.977777777777778	The evidence of domain adaptation of supervised systems is weaker, as only one team tried, and the differences with respect to MFS are very small.	0
5806	5806	S10-17	Conclusions	7	133	0.777777777777778	0.985185185185185	The best results for English are obtained by a system that combines a knowledge-based system with some targeted hand-tagging.	0
5807	5807	S10-17	Conclusions	8	134	0.888888888888889	0.992592592592593	Regarding the techniques used, graph-based methods over WordNet and distributional thesaurus acquisition methods have been used by several teams.	0
5808	5808	S10-17	Conclusions	9	135	1.0	1.0	All datasets and related information are publicly available from the task websites 6 .	0
5975	5975	S10-19	Introduction	1	8	0.03448275862069	0.053333333333333	This paper reports an overview of the SemEval-2 Japanese Word Sense Disambiguation (WSD) task.	0
5976	5976	S10-19	Introduction	2	9	0.068965517241379	0.06	It can be considered an extension of the SENSEVAL-2 Japanese monolingual dictionarybased task (Shirai, 2001), so it is a lexical sample task.	0
5977	5977	S10-19	Introduction	3	10	0.103448275862069	0.066666666666667	Word senses are defined according to the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary published by Iwanami Shoten.	0
5978	5978	S10-19	Introduction	4	11	0.137931034482759	0.073333333333333	It was distributed to participants as a sense inventory.	0
5979	5979	S10-19	Introduction	5	12	0.172413793103448	0.08	Our task has the following two new characteristics:	0
5980	5980	S10-19	Introduction	6	13	0.206896551724138	0.086666666666667	1	0
5981	5981	S10-19	Introduction	7	14	0.241379310344828	0.093333333333333	All previous Japanese sense-tagged corpora were from newspaper articles, while sensetagged corpora were constructed in English on balanced corpora, such as Brown corpus and BNC corpus.	0
5982	5982	S10-19	Introduction	8	15	0.275862068965517	0.1	The first balanced corpus of contemporary written Japanese (BCCWJ corpus) is now being constructed as part of a national project in Japan (Maekawa, 2008), and we are now constructing a sense-tagged corpus based on it.	0
5983	5983	S10-19	Introduction	9	16	0.310344827586207	0.106666666666667	Therefore, the task will use the first balanced Japanese sense-tagged corpus.	0
5984	5984	S10-19	Introduction	10	17	0.344827586206897	0.113333333333333	Because a balanced corpus consists of documents from multiple genres, the corpus can be divided into multiple sub-corpora of a genre.	0
5985	5985	S10-19	Introduction	11	18	0.379310344827586	0.12	In supervised learning approaches on word sense disambiguation, because word sense distribution might vary across different sub-corpora, we need to take into account the genres of training and test corpora.	0
5986	5986	S10-19	Introduction	12	19	0.413793103448276	0.126666666666667	Therefore, word sense disambiguation on a balanced corpus requires tackling a kind of domain (genre) adaptation problem (Chang and Ng, 2006;	0
5987	5987	S10-19	Introduction	13	20	0.448275862068966	0.133333333333333	Agirre and de Lacalle, 2008).	0
5988	5988	S10-19	Introduction	14	21	0.482758620689655	0.14	2. In previous WSD tasks, systems have been required to select a sense from a given set of senses in a dictionary for a word in one context (an instance).	1
5989	5989	S10-19	Introduction	15	22	0.517241379310345	0.146666666666667	However, the set of senses in the dictionary is not always complete.	0
5990	5990	S10-19	Introduction	16	23	0.551724137931034	0.153333333333333	New word senses sometimes appear after the dictionary has been compiled.	0
5991	5991	S10-19	Introduction	17	24	0.586206896551724	0.16	Therefore, some instances might have a sense that cannot be found in the dictionary's set.	0
5992	5992	S10-19	Introduction	18	25	0.620689655172414	0.166666666666667	The task will take into account not only the instances that have a sense in the given set but also the instances that have a sense that cannot be found in the set.	0
5993	5993	S10-19	Introduction	19	26	0.655172413793103	0.173333333333333	In the latter case, systems should output that the instances have a sense that is not in the set.	0
5994	5994	S10-19	Introduction	20	27	0.689655172413793	0.18	Training data, a corpus that consists of three genres (books, newspaper articles, and white papers) and is manually annotated with sense IDs, was also distributed to participants.	0
5995	5995	S10-19	Introduction	21	28	0.724137931034483	0.186666666666667	For the evaluation, we distributed a corpus that consists of four genres (books, newspaper articles, white papers, and documents from a Q&amp;	0
5996	5996	S10-19	Introduction	22	29	0.758620689655172	0.193333333333333	A site on the WWW) with marked target words as test data.	0
5997	5997	S10-19	Introduction	23	30	0.793103448275862	0.2	Participants were requested to assign one or more sense IDs to each target word, optionally with associated probabilities.	0
5998	5998	S10-19	Introduction	24	31	0.827586206896552	0.206666666666667	The number of target words was 50, with 22 nouns, 23 verbs, and 5 adjectives.	0
5999	5999	S10-19	Introduction	25	32	0.862068965517241	0.213333333333333	Fifty instances of each target word were provided, con-sisting of a total of 2,500 instances for the evaluation.	0
6000	6000	S10-19	Introduction	26	33	0.896551724137931	0.22	In what follows, section two describes the details of the data used in the Japanese WSD task.	0
6001	6001	S10-19	Introduction	27	34	0.931034482758621	0.226666666666667	Section three describes the process to construct the sense tagged data, including the analysis of an inter-annotator agreement.	0
6002	6002	S10-19	Introduction	28	35	0.96551724137931	0.233333333333333	Section four briefly introduces participating systems and section five describes their results.	0
6003	6003	S10-19	Introduction	29	36	1.0	0.24	Finally, section six concludes the paper.	0
6144	6144	S12-1	Task definition	1	27	0.142857142857143	0.140625	"Given a short context, a target word in English, and several substitutes for the target word that are deemed adequate for that context, the goal of the English Simplification task at SemEval-2012 is to rank these substitutes according to how ""simple"" they are, allowing ties."	1
6145	6145	S12-1	Task definition	2	28	0.285714285714286	0.145833333333333	Simple words/phrases are loosely defined as those which can be understood by a wide range of people, including those with low literacy levels or some cognitive disability, children, and non-native speakers of English.	0
6146	6146	S12-1	Task definition	3	29	0.428571428571429	0.151041666666667	In particular, the data provided as part of the task is annotated by fluent but non-native speakers of English.	0
6147	6147	S12-1	Task definition	4	30	0.571428571428571	0.15625	The task thus essentially involves comparing words or phrases and determining their order of complexity.	0
6148	6148	S12-1	Task definition	5	31	0.714285714285714	0.161458333333333	By ranking the candidates, as opposed to categorizing them into specific labels (simple, moderate, complex, etc.), we avoid the need for a fixed number of categories and for more subjective judgments.	0
6149	6149	S12-1	Task definition	6	32	0.857142857142857	0.166666666666667	Also ranking enables a more natural and intuitive way for humans (and systems) to perform annotations by preventing them from treating each individual case in isolation, as opposed to relative to each other.	0
6150	6150	S12-1	Task definition	7	33	1.0	0.171875	However, the inherent subjectivity introduced by ranking entails higher disagreement among human annotators, and more complexity for systems to tackle.	0
6351	6351	S12-2	Objective	1	42	0.142857142857143	0.190045248868778	Our task is to rate word pairs by the degree to which they are prototypical members of a given relation class.	1
6352	6352	S12-2	Objective	2	43	0.285714285714286	0.194570135746606	The relation class is specified by a few paradigmatic (highly prototypical) examples of word pairs that belong to the class and also by a schematic representation of the relation class.	0
6353	6353	S12-2	Objective	3	44	0.428571428571429	0.199095022624434	The task requires comparing a word pair to the paradigmatic examples and/or the schematic representation.	0
6354	6354	S12-2	Objective	4	45	0.571428571428571	0.203619909502262	For example, suppose the relation class is REVERSE.	0
6355	6355	S12-2	Objective	5	46	0.714285714285714	0.208144796380091	"We may specify this class by the paradigmatic examples attack:defend, buy:sell, love:hate, and the schematic representation ""X is the reverse act of Y "" or ""X may be undone by Y ."""	0
6356	6356	S12-2	Objective	6	47	0.857142857142857	0.212669683257919	Given a pair such as repair:break, we compare this pair to the paradigmatic examples and/or the schematic representation, in order to estimate its degree of prototypicality.	0
6357	6357	S12-2	Objective	7	48	1.0	0.217194570135747	The challenges are (1) to infer the relation from the paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples.	0
6532	6532	S12-3	abstract	1	2	0.166666666666667	0.008547008547009	This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling.	0
6533	6533	S12-3	abstract	2	3	0.333333333333333	0.012820512820513	The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators.	1
6534	6534	S12-3	abstract	3	4	0.5	0.017094017094017	In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted.	1
6535	6535	S12-3	abstract	4	5	0.666666666666667	0.021367521367521	The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark.	0
6536	6536	S12-3	abstract	5	6	0.833333333333333	0.025641025641026	We have one participant system with two runs.	0
6537	6537	S12-3	abstract	6	7	1.0	0.02991452991453	The participant's runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers.	0
6766	6766	S12-4	abstract	1	2	0.125	0.022222222222222	This task focuses on evaluating word similarity computation in Chinese.	1
6767	6767	S12-4	abstract	2	3	0.25	0.033333333333333	We follow the way of Finkelstein et al. (2002) to select word pairs.	0
6768	6768	S12-4	abstract	3	4	0.375	0.044444444444445	Then we organize twenty undergraduates who are major in Chinese linguistics to annotate the data.	0
6769	6769	S12-4	abstract	4	5	0.5	0.055555555555556	Each pair is assigned a similarity score by each annotator.	0
6770	6770	S12-4	abstract	5	6	0.625	0.066666666666667	We rank the word pairs by the average value of similar scores among the twenty annotators.	0
6771	6771	S12-4	abstract	6	7	0.75	0.077777777777778	This data is used as gold standard.	0
6772	6772	S12-4	abstract	7	8	0.875	0.088888888888889	Four systems participating in this task return their results.	0
6773	6773	S12-4	abstract	8	9	1.0	0.1	We evaluate their results on gold standard data in term of Kendall's tau value, and the results show three of them have a positive correlation with the rank manually created while the taus' value is very small.	0
6856	6856	S12-5	abstract	1	2	0.2	0.016129032258065	The paper presents the SemEval-2012 Shared Task 5: Chinese Semantic Dependency Parsing.	0
6857	6857	S12-5	abstract	2	3	0.4	0.024193548387097	The goal of this task is to identify the dependency structure of Chinese sentences from the semantic view.	1
6858	6858	S12-5	abstract	3	4	0.6	0.032258064516129	We firstly introduce the motivation of providing Chinese semantic dependency parsing task, and then describe the task in detail including data preparation, data format, task evaluation, and so on.	0
6859	6859	S12-5	abstract	4	5	0.8	0.040322580645161	Over ten thousand sentences were labeled for participants to train and evaluate their systems.	0
6860	6860	S12-5	abstract	5	6	1.0	0.048387096774194	At last, we briefly describe the submitted systems and analyze these results.	0
7175	7175	S12-7	abstract	1	2	0.2	0.015873015873016	SemEval-2012	0
7176	7176	S12-7	abstract	2	3	0.4	0.023809523809524	Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise.	1
7177	7177	S12-7	abstract	3	4	0.6	0.031746031746032	In this paper, we describe the development of this task and its motivation.	0
7178	7178	S12-7	abstract	4	5	0.8	0.03968253968254	We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research.	0
7179	7179	S12-7	abstract	5	6	1.0	0.047619047619048	We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future.	0
7432	7432	S13-1	Introduction	1	5	0.05	0.023809523809524	The TempEval task (Verhagen et al., 2009) was added as a new task in SemEval-2007.	0
7433	7433	S13-1	Introduction	2	6	0.1	0.028571428571429	The ultimate aim of research in this area is the automatic identification of temporal expressions (timexes), events, and temporal relations within a text as specified in TimeML annotation (Pustejovsky et al., 2005).	1
7434	7434	S13-1	Introduction	3	7	0.15	0.033333333333333	However, since addressing this aim in a first evaluation challenge was deemed too difficult a staged approach was suggested.	0
7435	7435	S13-1	Introduction	4	8	0.2	0.038095238095238	TempEval (henceforth TempEval-1) was an initial evaluation exercise focusing only on the categorization of temporal relations and only in English.	0
7436	7436	S13-1	Introduction	5	9	0.25	0.042857142857143	It included three relation types: event-timex, event-dct, 1 and relations between main events in consecutive sentences.	0
7437	7437	S13-1	Introduction	6	10	0.3	0.047619047619048	TempEval-2 (Verhagen et al., 2010) extended TempEval-1, growing into a multilingual task, and consisting of six subtasks rather than three.	0
7438	7438	S13-1	Introduction	7	11	0.35	0.052380952380952	This included event and timex extraction, as well as the three relation tasks from TempEval-1, with the addition of a relation task where one event subordinates another.	0
7439	7439	S13-1	Introduction	8	12	0.4	0.057142857142857	TempEval-3 (UzZaman et al., 2012b) is a follow-up to TempEval 1 and 2, covering English and Spanish.	0
7440	7440	S13-1	Introduction	9	13	0.45	0.061904761904762	Temp	0
7441	7441	S13-1	Introduction	10	14	0.5	0.066666666666667	Eval-3 is different from its predecessors in a few respects:	0
7442	7442	S13-1	Introduction	11	15	0.55	0.071428571428572	1 DCT stands for document creation time Size of the corpus: the dataset used has about 600K word silver standard data and about 100K word gold standard data for training, compared to around 50K word corpus used in TempEval 1 and 2.	0
7443	7443	S13-1	Introduction	12	16	0.6	0.076190476190476	Temporal annotation is a time-consuming task for humans, which has limited the size of annotated data in previous TempEval exercises.	0
7444	7444	S13-1	Introduction	13	17	0.65	0.080952380952381	Current systems, however, are performing close to the inter-annotator reliability, which suggests that larger corpora could be built from automatically annotated data with minor human reviews.	0
7445	7445	S13-1	Introduction	14	18	0.7	0.085714285714286	We want to explore whether there is value in adding a large automatically created silver standard to a hand-crafted gold standard.	0
7446	7446	S13-1	Introduction	15	19	0.75	0.090476190476191	End-to-end temporal relation processing task: the temporal relation classification tasks are performed from raw text, i.e. participants need to extract their own events and temporal expressions first, determine which ones to link and then obtain the relation types.	0
7447	7447	S13-1	Introduction	16	20	0.8	0.095238095238095	In previous Tem-pEvals, gold timexes, events, and relations (without category) were given to participants.	0
7448	7448	S13-1	Introduction	17	21	0.85	0.1	Temporal relation types: the full set of temporal relations in TimeML are used, rather than the reduced set used in earlier TempEvals.	0
7449	7449	S13-1	Introduction	18	22	0.9	0.104761904761905	Platinum test set: A new test dataset has been developed for this edition.	0
7450	7450	S13-1	Introduction	19	23	0.95	0.10952380952381	It is based on manual annotations by experts over new text (unseen in previous editions).	0
7451	7451	S13-1	Introduction	20	24	1.0	0.114285714285714	Evaluation: we report a temporal awareness score for evaluating temporal relations, which helps to rank systems with a single score.	0
7674	7674	S13-2	Subtask A: Contextual Polarity Disambiguation	1	37	0.333333333333333	0.205555555555556	Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context.	1
7675	7675	S13-2	Subtask A: Contextual Polarity Disambiguation	2	38	0.666666666666667	0.211111111111111	The boundaries for the marked instance were provided: this was a classification task, not an entity recognition task.	0
7676	7676	S13-2	Subtask A: Contextual Polarity Disambiguation	3	39	1.0	0.216666666666667	2 http://www.daedalus.es/TASS/corpus.php	0
7819	7819	S13-3	abstract	1	2	0.25	0.012903225806452	Many NLP applications require information about locations of objects referenced in text, or relations between them in space.	0
7820	7820	S13-3	abstract	2	3	0.5	0.019354838709678	For example, the phrase a book on the desk contains information about the location of the object book, as trajector, with respect to another object desk, as landmark.	0
7821	7821	S13-3	abstract	3	4	0.75	0.025806451612903	Spatial Role Labeling (SpRL) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them.	1
7822	7822	S13-3	abstract	4	5	1.0	0.032258064516129	This paper describes the task in Semantic Evaluations 2013, annotation schema, corpora, participants, methods and results obtained by the participants.	0
7966	7966	S13-3	Conclusion	1	149	0.142857142857143	0.961290322580645	In this paper we described an evaluation task on Spatial Role Labeling in the context of Semantic Evaluations 2013.	0
7967	7967	S13-3	Conclusion	2	150	0.285714285714286	0.967741935483871	The task sets a goal to automatically process text and identify objects of spatial scenes and relations between them.	1
7968	7968	S13-3	Conclusion	3	151	0.428571428571429	0.974193548387097	Building largely upon the previous evaluation campaign, SpRL-2012, in SpRL-2013 we introduced additional spatial roles and relations for capturing motions in text.	0
7969	7969	S13-3	Conclusion	4	152	0.571428571428571	0.980645161290322	In addition, a new annotated corpus for spatial roles (including annotated motions) was produced and released to the participants.	0
7970	7970	S13-3	Conclusion	5	153	0.714285714285714	0.987096774193548	It comprises a set of 117 files with about 40,000 tokens in total.	0
7971	7971	S13-3	Conclusion	6	154	0.857142857142857	0.993548387096774	With the registered number of 10 participants and the final number of submissions (only one) we can conclude that spatial role labeling is an interesting task within the research community, however sometimes underestimated in its complexity.	0
7972	7972	S13-3	Conclusion	7	155	1.0	1.0	Our further steps in promoting spatial role labeling will be a detailed description of the annotation scheme and annotation guidelines, analysis of the corpora and obtained results.	0
8033	8033	S13-4	Task description	1	61	0.142857142857143	0.420689655172414	This is an English NC interpretation task, which explores the idea of interpreting the semantics of NCs via free paraphrases.	0
8034	8034	S13-4	Task description	2	62	0.285714285714286	0.427586206896552	Given a noun-noun compound such as air filter, the participating systems are asked to produce an explicitly ranked list of free paraphrases, as in the following example:	1
8035	8035	S13-4	Task description	3	63	0.428571428571429	0.43448275862069	1 filter for air 2 filter of air 3 filter that cleans the air 4 filter which makes air healthier 5 a filter that removes impurities from the air . . .	0
8036	8036	S13-4	Task description	4	64	0.571428571428571	0.441379310344828	Such a list is then automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed via Amazon's Mechanical Turk.	0
8037	8037	S13-4	Task description	5	65	0.714285714285714	0.448275862068966	The comparison of raw paraphrases is sensitive to syntactic and morphological variation.	0
8038	8038	S13-4	Task description	6	66	0.857142857142857	0.455172413793104	The ranking of paraphrases is based on their relative popularity among different annotators.	0
8039	8039	S13-4	Task description	7	67	1.0	0.462068965517241	To make the ranking more reliable, highly similar paraphrases are grouped so as to downplay superficial differences in syntax and morphology.	0
8326	8326	S13-7	Introduction	1	7	0.02	0.026923076923077	One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring.	0
8327	8327	S13-7	Introduction	2	8	0.04	0.030769230769231	Much previous work has been devoted to the automated scoring of essays (Attali and Burstein, 2006;	0
8328	8328	S13-7	Introduction	3	9	0.06	0.034615384615385	Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009;	0
8329	8329	S13-7	Introduction	4	10	0.08	0.038461538461539	Sheehan et al., 2010;Nelson et al., 2012).	0
8330	8330	S13-7	Introduction	5	11	0.1	0.042307692307692	In these applications, NLP methods based on shallow features and supervised learning are often highly effective.	0
8331	8331	S13-7	Introduction	6	12	0.12	0.046153846153846	However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003;	0
8332	8332	S13-7	Introduction	7	13	0.14	0.05	Pulman and Sukkarieh, 2005;	0
8333	8333	S13-7	Introduction	8	14	0.16	0.053846153846154	Nielsen et al., 2008a;	0
8334	8334	S13-7	Introduction	9	15	0.18	0.057692307692308	Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999;	0
8335	8335	S13-7	Introduction	10	16	0.2	0.061538461538462	Glass, 2000;	0
8336	8336	S13-7	Introduction	11	17	0.22	0.065384615384615	Pon-Barry et al., 2004;	0
8337	8337	S13-7	Introduction	12	18	0.24	0.069230769230769	Jordan et al., 2006;	0
8338	8338	S13-7	Introduction	13	19	0.26	0.073076923076923	Van	0
8339	8339	S13-7	Introduction	14	20	0.28	0.076923076923077	Lehn et al., 2007;Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate.	0
8340	8340	S13-7	Introduction	15	21	0.3	0.080769230769231	Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community.	0
8341	8341	S13-7	Introduction	16	22	0.32	0.084615384615385	Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could Example 1 QUESTION	0
8342	8342	S13-7	Introduction	17	23	0.34	0.088461538461539	You used several methods to separate and identify the substances in mock rocks.	0
8343	8343	S13-7	Introduction	18	24	0.36	0.092307692307692	How did you separate the salt from the water?	0
8344	8344	S13-7	Introduction	19	25	0.38	0.096153846153846	REF.	0
8345	8345	S13-7	Introduction	20	26	0.4	0.1	ANS.	0
8346	8346	S13-7	Introduction	21	27	0.42	0.103846153846154	The water was evaporated, leaving the salt.	0
8347	8347	S13-7	Introduction	22	28	0.44	0.107692307692308	STUD.	0
8348	8348	S13-7	Introduction	23	29	0.46	0.111538461538462	ANS.	0
8349	8349	S13-7	Introduction	24	30	0.48	0.115384615384615	The water dried up and left the salt.	0
8350	8350	S13-7	Introduction	25	31	0.5	0.119230769230769	Example 2 QUESTION	0
8351	8351	S13-7	Introduction	26	32	0.52	0.123076923076923	Georgia found one brown mineral and one black mineral.	0
8352	8352	S13-7	Introduction	27	33	0.54	0.126923076923077	How will she know which one is harder?	0
8353	8353	S13-7	Introduction	28	34	0.56	0.130769230769231	REF.	0
8354	8354	S13-7	Introduction	29	35	0.58	0.134615384615385	ANS.	0
8355	8355	S13-7	Introduction	30	36	0.6	0.138461538461538	The harder mineral will leave a scratch on the less hard mineral.	0
8356	8356	S13-7	Introduction	31	37	0.62	0.142307692307692	If the black mineral is harder, the brown mineral will have a scratch.	0
8357	8357	S13-7	Introduction	32	38	0.64	0.146153846153846	STUD.	0
8358	8358	S13-7	Introduction	33	39	0.66	0.15	ANS.	0
8359	8359	S13-7	Introduction	34	40	0.68	0.153846153846154	The harder will leave a scratch on the other.	0
8360	8360	S13-7	Introduction	35	41	0.7	0.157692307692308	Figure 1: Example questions and answers help a full dialog system to generate appropriate and effective feedback on errors.	0
8361	8361	S13-7	Introduction	36	42	0.72	0.161538461538462	System designers typically create a repertoire of questions that the system can ask a student, together with reference answers (see Figure 1 for an example).	0
8362	8362	S13-7	Introduction	37	43	0.74	0.165384615384615	For each student answer, the system needs to decide on the appropriate tutorial feedback, either confirming that the answer was correct, or providing additional help to indicate how the answer is flawed and help the student improve.	1
8363	8363	S13-7	Introduction	38	44	0.76	0.169230769230769	This task requires semantic inference, for example, to detect when the student answers are explaining the same content but in different words, or when they are contradicting the reference answers.	0
8364	8364	S13-7	Introduction	39	45	0.78	0.173076923076923	Recognizing Textual Entailment (RTE) is a series of highly successful challenges used to evaluate tasks related to semantic inference, held annually since 2005.	0
8365	8365	S13-7	Introduction	40	46	0.8	0.176923076923077	Initial challenges used examples from information retrieval, question answering, machine translation and information extraction tasks (Dagan et al., 2006;	0
8366	8366	S13-7	Introduction	41	47	0.82	0.180769230769231	Giampiccolo et al., 2008).	0
8367	8367	S13-7	Introduction	42	48	0.84	0.184615384615385	Later challenges started to explore the applicability and impact of RTE technology on specific application settings such as Summarization and Knowledge Base Population (Bentivogli et al., 2009;	0
8368	8368	S13-7	Introduction	43	49	0.86	0.188461538461538	Bentivogli et al., 2010;Bentivogli et al., 2011).	0
8369	8369	S13-7	Introduction	44	50	0.88	0.192307692307692	The SRA Task offers a similar opportunity.	0
8370	8370	S13-7	Introduction	45	51	0.9	0.196153846153846	We therefore organized a joint challenge at SemEval-2013, aiming to bring together the educational NLP and the semantic inference communities.	0
8371	8371	S13-7	Introduction	46	52	0.92	0.2	The goal of the challenge is to compare approaches for student answer assessment and to evaluate the methods typically used in RTE on data from educational applications.	0
8372	8372	S13-7	Introduction	47	53	0.94	0.203846153846154	We present the corpus used in the task (Section 2) and describe the Main task, including educational NLP and textual entailment perspectives and data set creation (Section 3).	0
8373	8373	S13-7	Introduction	48	54	0.96	0.207692307692308	We discuss evaluation metrics and results in Section 4.	0
8374	8374	S13-7	Introduction	49	55	0.98	0.211538461538462	Section 5 describes the Pilot task, including data set creation and evaluation results.	0
8375	8375	S13-7	Introduction	50	56	1.0	0.215384615384615	Section 6 presents conclusions and future directions.	0
8404	8404	S13-7	RTE perspective and 2-and 3-way Tasks	1	85	0.066666666666667	0.326923076923077	According to the standard definition of Textual Entailment, given two text fragments called Text (T) and Hypothesis (H), it is said that T entails H if, typically, a human reading T would infer that H is most likely true (Dagan et al., 2006).	0
8405	8405	S13-7	RTE perspective and 2-and 3-way Tasks	2	86	0.133333333333333	0.330769230769231	In a typical answer assessment scenario, we expect that a correct student answer would entail the reference answer, while an incorrect answer would not.	0
8406	8406	S13-7	RTE perspective and 2-and 3-way Tasks	3	87	0.2	0.334615384615385	However, students often skip details that are mentioned in the question or may be inferred from it, while reference answers often repeat or make explicit information that appears in or is implied from the question, as in Example 2 in Figure 1.	0
8407	8407	S13-7	RTE perspective and 2-and 3-way Tasks	4	88	0.266666666666667	0.338461538461538	Hence, a more precise formulation of the task in this context considers the entailing text T as consisting of both the original question and the student answer, while H is the reference answer.	1
8408	8408	S13-7	RTE perspective and 2-and 3-way Tasks	5	89	0.333333333333333	0.342307692307692	We carried out a feasibility study to check how well the entailment judgments in this formulation align with the annotated response assessment, by annotating a sample of the data used in the SRA task with entailment judgments.	0
8409	8409	S13-7	RTE perspective and 2-and 3-way Tasks	6	90	0.4	0.346153846153846	"We found that some answers labeled as ""correct"" implied inferred or assumed pieces of information not present in the text."	0
8410	8410	S13-7	RTE perspective and 2-and 3-way Tasks	7	91	0.466666666666667	0.35	These reflected the teachers' assessment of student understanding but would not be considered entailed from the traditional RTE perspective.	0
8411	8411	S13-7	RTE perspective and 2-and 3-way Tasks	8	92	0.533333333333333	0.353846153846154	However, we observed that in most such cases, a substantial part of the hypothesis was still implied by the text.	0
8412	8412	S13-7	RTE perspective and 2-and 3-way Tasks	9	93	0.6	0.357692307692308	"Moreover, answers assigned labels other than ""correct"" were always judged as ""not entailed""."	0
8413	8413	S13-7	RTE perspective and 2-and 3-way Tasks	10	94	0.666666666666667	0.361538461538461	Overall, we concluded that the correlation between assessment judgments of the two types was sufficiently high to consider an RTE approach.	0
8414	8414	S13-7	RTE perspective and 2-and 3-way Tasks	11	95	0.733333333333333	0.365384615384615	The challenge for the textual entailment community was to address the answer assessment task at varying levels of granularity, using textual entailment techniques, and explore how well these techniques can help in this real-world educational setting.	0
8415	8415	S13-7	RTE perspective and 2-and 3-way Tasks	12	96	0.8	0.369230769230769	In order to make the setup more similar to pre-vious RTE tasks, we introduced 3-way and 2-way versions of the task.	0
8416	8416	S13-7	RTE perspective and 2-and 3-way Tasks	13	97	0.866666666666667	0.373076923076923	The data for those tasks were obtained by automatically collapsing the 5-way labels.	0
8417	8417	S13-7	RTE perspective and 2-and 3-way Tasks	14	98	0.933333333333333	0.376923076923077	In the 3-way task, the systems were required to classify the student answer as either (i) correct; (ii) contradictory; or (iii) incorrect (combining the categories partially correct but incomplete, irrelevant and not in the domain from the 5-way classification).	0
8418	8418	S13-7	RTE perspective and 2-and 3-way Tasks	15	99	1.0	0.380769230769231	In the two-way task, the systems were required to classify the student answer as either correct or incorrect (combining the categories contradictory and incorrect from the 3-way classification)	0
8723	8723	S13-9	abstract	1	2	0.2	0.013422818791946	The DDIExtraction 2013 task concerns the recognition of drugs and extraction of drugdrug interactions that appear in biomedical literature.	1
8724	8724	S13-9	abstract	2	3	0.4	0.02013422818792	We propose two subtasks for the DDIExtraction 2013 Shared Task challenge: 1) the recognition and classification of drug names and 2) the extraction and classification of their interactions.	0
8725	8725	S13-9	abstract	3	4	0.6	0.026845637583893	Both subtasks have been very successful in participation and results.	0
8726	8726	S13-9	abstract	4	5	0.8	0.033557046979866	There were 14 teams who submitted a total of 38 runs.	0
8727	8727	S13-9	abstract	5	6	1.0	0.040268456375839	The best result reported for the first subtask was F1 of 71.5% and 65.1% for the second one.	0
9204	9204	S13-12	Task Setup	1	25	0.5	0.132275132275132	The task required participating systems to annotate nouns in a test corpus with the most appropriate sense from the BabelNet sense inventory or, alternatively, from two main subsets of it, namely the WordNet or Wikipedia sense inventories.	1
9205	9205	S13-12	Task Setup	2	26	1.0	0.137566137566138	In contrast to previous all-words WSD tasks we did not focus on the other three open classes (i.e., verbs, adjectives and adverbs) since BabelNet does not currently provide non-English coverage for them.	0
9582	9582	S14-1	The Task	1	23	0.076923076923077	0.172932330827068	The Task involved two subtasks.	1
9583	9583	S14-1	The Task	2	24	0.153846153846154	0.180451127819549	(i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them (see below for the exact definition).	1
9584	9584	S14-1	The Task	3	25	0.230769230769231	0.18796992481203	Sentence relatedness scores provide a direct way to evaluate CDSMs, insofar as their outputs are able to quantify the degree of semantic similarity between sentences.	0
9585	9585	S14-1	The Task	4	26	0.307692307692308	0.195488721804511	On the other hand, starting from the assumption that understanding a sentence means knowing when it is true, being able to verify whether an entailment is valid is a crucial challenge for semantic systems.	0
9586	9586	S14-1	The Task	5	27	0.384615384615385	0.203007518796992	In the semantic relatedness subtask, given two sentences, systems were required to produce a relatedness score (on a continuous scale) indicating the extent to which the sentences were expressing a related meaning.	0
9587	9587	S14-1	The Task	6	28	0.461538461538462	0.210526315789474	Table 1 shows examples of sentence pairs with different degrees of semantic relatedness; gold relatedness scores are expressed on a 5-point rating scale.	0
9588	9588	S14-1	The Task	7	29	0.538461538461538	0.218045112781955	In the entailment subtask, given two sentences A and B, systems had to determine whether the meaning of B was entailed by A.	0
9589	9589	S14-1	The Task	8	30	0.615384615384615	0.225563909774436	In particular, systems were required to assign to each pair either the ENTAILMENT label (when A entails B, viz., B cannot be false when A is true), the CONTRA-DICTION label (when A contradicted B, viz.	0
9590	9590	S14-1	The Task	9	31	0.692307692307692	0.233082706766917	B is false whenever A is true), or the NEUTRAL label (when the truth of B could not be determined on the basis of A).	0
9591	9591	S14-1	The Task	10	32	0.769230769230769	0.240601503759398	Table 2 shows examples of sentence pairs holding different entailment relations.	0
9592	9592	S14-1	The Task	11	33	0.846153846153846	0.24812030075188	Participants were invited to submit up to five system runs for one or both subtasks.	0
9593	9593	S14-1	The Task	12	34	0.923076923076923	0.255639097744361	Developers of CDSMs were especially encouraged to participate, but developers of other systems that could tackle sentence relatedness or entailment tasks were also welcome.	0
9594	9594	S14-1	The Task	13	35	1.0	0.263157894736842	Besides being of intrinsic interest, the latter systems' performance will serve to situate CDSM performance within the broader landscape of computational semantics.	0
9694	9694	S14-2	abstract	1	2	0.125	0.01025641025641	In this paper we present the SemEval-2014	0
9695	9695	S14-2	abstract	2	3	0.25	0.015384615384615	Task 2 on spoken dialogue grammar induction.	0
9696	9696	S14-2	abstract	3	4	0.375	0.020512820512821	The task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems.	1
9697	9697	S14-2	abstract	4	5	0.5	0.025641025641026	We describe four subtasks covering two languages, English and Greek, and three speech application domains, travel reservation, tourism and finance.	0
9698	9698	S14-2	abstract	5	6	0.625	0.030769230769231	The classification results are compared against the groundtruth.	0
9699	9699	S14-2	abstract	6	7	0.75	0.035897435897436	Weighted and unweighted precision, recall and fmeasure are reported.	0
9700	9700	S14-2	abstract	7	8	0.875	0.041025641025641	Three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training.	0
9701	9701	S14-2	abstract	8	9	1.0	0.046153846153846	The submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the baseline, averaged across all subtasks.	0
9894	9894	S14-3	Introduction	1	7	0.05	0.032710280373832	Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning.	0
9895	9895	S14-3	Introduction	2	8	0.1	0.037383177570094	Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval (Hliaoutakis et al., 2006) or paraphrasing (Glickman and Dagan, 2003).	0
9896	9896	S14-3	Introduction	3	9	0.15	0.042056074766355	Semantic similarity evaluations have largely focused on comparing similar types of lexical items.	0
9897	9897	S14-3	Introduction	4	10	0.2	0.046728971962617	Most recently, tasks in SemEval (Agirre et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases.	0
9898	9898	S14-3	Introduction	5	11	0.25	0.051401869158879	Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License.	0
9899	9899	S14-3	Introduction	6	12	0.3	0.05607476635514	Page numbers and proceedings footer are added by the organizers.	0
9900	9900	S14-3	Introduction	7	13	0.35	0.060747663551402	License details: http: //creativecommons.org/licenses/by/4.0/ of Rubenstein and Goodenough (1965) measure similarity between word pairs, while the data sets of Navigli (2006) and Kilgarriff (2001) offer a binary similar-dissimilar distinction between senses.	0
9901	9901	S14-3	Introduction	8	14	0.4	0.065420560747664	Notably, all of these evaluations have focused on comparisons between a single type, in contrast to application-based evaluations such as summarization and compositionality which incorporate textual items of different sizes, e.g., measuring the quality of a paragraph's sentence summarization.	0
9902	9902	S14-3	Introduction	9	15	0.45	0.070093457943925	Task 3 introduces a new evaluation where similarity is measured between items of different types: paragraphs, sentences, phrases, words and senses.	0
9903	9903	S14-3	Introduction	10	16	0.5	0.074766355140187	Given an item of the lexically-larger type, a system measures the degree to which the meaning of the larger item is captured in the smaller type, e.g., comparing a paragraph to a sentence.	1
9904	9904	S14-3	Introduction	11	17	0.55	0.079439252336449	We refer to this task as Cross-Level Semantic Similarity (CLSS).	0
9905	9905	S14-3	Introduction	12	18	0.6	0.08411214953271	A major motivation of this task is to produce semantic similarity systems that are able to compare all types of text, thereby freeing downstream NLP applications from needing to consider the type of text being compared.	0
9906	9906	S14-3	Introduction	13	19	0.65	0.088785046728972	"Task 3 enables assessing the extent to which the meaning of the sentence ""do u know where i can watch free older movies online without download?"" is captured in the phrase ""streaming vintage movies for free"", or how similar is ""circumscribe"" to the phrase ""beating around the bush."""	0
9907	9907	S14-3	Introduction	14	20	0.7	0.093457943925234	Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality.	0
9908	9908	S14-3	Introduction	15	21	0.75	0.098130841121495	Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications.	0
9909	9909	S14-3	Introduction	16	22	0.8	0.102803738317757	Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy andNavigli, 2009), summariza-tion (Sprck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a).	0
9910	9910	S14-3	Introduction	17	23	0.85	0.107476635514019	Task 3 was designed with three main objectives.	0
9911	9911	S14-3	Introduction	18	24	0.9	0.11214953271028	First, the task should include multiple types of comparison in order to assess each type's difficulty and whether specialized resources are needed for each.	0
9912	9912	S14-3	Introduction	19	25	0.95	0.116822429906542	Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types.	0
9913	9913	S14-3	Introduction	20	26	1.0	0.121495327102804	Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text-and sense-based similarity methods within a single framework.	0
10103	10103	S14-4	abstract	1	2	0.166666666666667	0.01010101010101	Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint.	0
10104	10104	S14-4	abstract	2	3	0.333333333333333	0.015151515151515	The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen).	0
10105	10105	S14-4	abstract	3	4	0.5	0.02020202020202	SemEval-2014	0
10106	10106	S14-4	abstract	4	5	0.666666666666667	0.025252525252525	Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect.	1
10107	10107	S14-4	abstract	5	6	0.833333333333333	0.03030303030303	The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure.	0
10108	10108	S14-4	abstract	6	7	1.0	0.035353535353535	It attracted 163 submissions from 32 teams.	0
10305	10305	S14-5	Introduction	1	6	0.111111111111111	0.032258064516129	We present a new cross-lingual and applicationoriented task for SemEval that is situated in the area where Word Sense Disambiguation and Machine Translation meet.	0
10306	10306	S14-5	Introduction	2	7	0.222222222222222	0.037634408602151	Finding the proper translation of a word or phrase in a given context is much like the problem of disambiguating between multiple senses.	0
10307	10307	S14-5	Introduction	3	8	0.333333333333333	0.043010752688172	In this task participants are asked to build a translation/writing assistance system that translates specifically marked L1 fragments in an L2 context to their proper L2 translation.	1
10308	10308	S14-5	Introduction	4	9	0.444444444444444	0.048387096774194	This type of translation can be applied in writing assistance systems for language learners in which users write in a target language, but are allowed to occasionally back off to their native L1 when they are uncertain of the proper lexical or grammatical form in L2.	0
10309	10309	S14-5	Introduction	5	10	0.555555555555556	0.053763440860215	The task concerns the NLP back-end rather than any user interface.	0
10310	10310	S14-5	Introduction	6	11	0.666666666666667	0.059139784946237	Full-on machine translation typically concerns the translation of complete sentences or texts from L1 to L2.	0
10311	10311	S14-5	Introduction	7	12	0.777777777777778	0.064516129032258	This task, in contrast, focuses on smaller fragments, side-tracking the problem of full word reordering.	0
10312	10312	S14-5	Introduction	8	13	0.888888888888889	0.06989247311828	We focus on the following language combinations of L1 and L2 pairs: English-German, English-Spanish, French-English and Dutch-English.	0
10313	10313	S14-5	Introduction	9	14	1.0	0.075268817204301	Task participants could participate for all language pairs or any subset thereof.	0
10685	10685	S14-7	abstract	1	2	0.142857142857143	0.012658227848101	This paper describes the SemEval-2014, Task 7 on the Analysis of Clinical Text and presents the evaluation results.	0
10686	10686	S14-7	abstract	2	3	0.285714285714286	0.018987341772152	It focused on two subtasks: (i) identification (Task A) and (ii) normalization (Task B) of diseases and disorders in clinical reports as annotated in the Shared Annotated Resources (ShARe) 1 corpus.	1
10687	10687	S14-7	abstract	3	4	0.428571428571429	0.025316455696203	This task was a follow-up to the ShARe/CLEF eHealth 2013 shared task, subtasks 1a and 1b, 2 but using a larger test set.	0
10688	10688	S14-7	abstract	4	5	0.571428571428571	0.031645569620253	A total of 21 teams competed in Task A, and 18 of those also participated in Task B. For Task A, the best system had a strict F 1 -score of 81.3, with a precision of 84.3 and recall of 78.6.	0
10689	10689	S14-7	abstract	5	6	0.714285714285714	0.037974683544304	For Task B, the same group had the best strict accuracy of 74.1.	0
10690	10690	S14-7	abstract	6	7	0.857142857142857	0.044303797468355	The organizers have made the text corpora, annotations, and evaluation tools available for future research and development at the shared task website.	0
10691	10691	S14-7	abstract	7	8	1.0	0.050632911392405	3 evaluation 3	0
10843	10843	S14-8	abstract	1	2	0.5	0.011904761904762	Task 8 at SemEval 2014 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning.	1
10844	10844	S14-8	abstract	2	3	1.0	0.017857142857143	In this task description, we position the problem in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results.	0
11153	11153	S14-10	abstract	1	2	0.142857142857143	0.008474576271186	In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets.	1
11154	11154	S14-10	abstract	2	3	0.285714285714286	0.01271186440678	This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity.	0
11155	11155	S14-10	abstract	3	4	0.428571428571429	0.016949152542373	For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotes-Word	0
11156	11156	S14-10	abstract	4	5	0.571428571428571	0.021186440677966	Net sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings.	0
11157	11157	S14-10	abstract	5	6	0.714285714285714	0.025423728813559	For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire.	0
11158	11158	S14-10	abstract	6	7	0.857142857142857	0.029661016949153	The annotations for both tasks leveraged crowdsourcing.	0
11159	11159	S14-10	abstract	7	8	1.0	0.033898305084746	The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs.	0
11389	11389	S15-1	abstract	1	2	0.111111111111111	0.012048192771084	In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data.	0
11390	11390	S15-1	abstract	2	3	0.222222222222222	0.018072289156627	Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence.	1
11391	11391	S15-1	abstract	3	4	0.333333333333333	0.024096385542169	The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.	0
11392	11392	S15-1	abstract	4	5	0.444444444444444	0.030120481927711	A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task.	0
11393	11393	S15-1	abstract	5	6	0.555555555555556	0.036144578313253	The evaluation shows encouraging results and open challenges for future research.	0
11394	11394	S15-1	abstract	6	7	0.666666666666667	0.042168674698795	The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach &gt;0.80 Pearson on well-formed text.	0
11395	11395	S15-1	abstract	7	8	0.777777777777778	0.048192771084337	This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together.	0
11396	11396	S15-1	abstract	8	9	0.888888888888889	0.05421686746988	We make all the data, baseline systems and evaluation scripts publicly available.	0
11397	11397	S15-1	abstract	9	10	1.0	0.060240963855422	1	0
11555	11555	S15-2	abstract	1	2	0.142857142857143	0.007272727272727	In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets.	1
11556	11556	S15-2	abstract	2	3	0.285714285714286	0.010909090909091	This year, the participants were challenged with new datasets in English and Spanish.	0
11557	11557	S15-2	abstract	3	4	0.428571428571429	0.014545454545455	The annotations for both subtasks leveraged crowdsourcing.	0
11558	11558	S15-2	abstract	4	5	0.571428571428571	0.018181818181818	The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs.	0
11559	11559	S15-2	abstract	5	6	0.714285714285714	0.021818181818182	In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair.	0
11560	11560	S15-2	abstract	6	7	0.857142857142857	0.025454545454546	The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years.	0
11561	11561	S15-2	abstract	7	8	1.0	0.029090909090909	7 teams participated with 29 runs.	0
11830	11830	S15-3	abstract	1	2	0.166666666666667	0.008620689655172	Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts.	0
11831	11831	S15-3	abstract	2	3	0.333333333333333	0.012931034482759	In this context, we organized SemEval-2015 Task 3 on Answer Selection in cQA, which included two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect to the question, and (b) answering a YES/NO question with yes, no, or unsure, based on the list of all answers.	1
11832	11832	S15-3	abstract	3	4	0.5	0.017241379310345	We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic.	0
11833	11833	S15-3	abstract	4	5	0.666666666666667	0.021551724137931	We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community.	0
11834	11834	S15-3	abstract	5	6	0.833333333333333	0.025862068965517	Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive.	0
11835	11835	S15-3	abstract	6	7	1.0	0.030172413793104	The best systems achieved an official score (macro-averaged F 1 ) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.	0
12062	12062	S15-4	abstract	1	2	0.25	0.010928961748634	This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015.	0
12063	12063	S15-4	abstract	2	3	0.5	0.016393442622951	Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity, by detecting, anchoring in time and ordering the events involving that entity.	1
12064	12064	S15-4	abstract	3	4	0.75	0.021857923497268	The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents.	0
12065	12065	S15-4	abstract	4	5	1.0	0.027322404371585	Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F 1 -score of 7.85 in the main track (timeline creation from raw text).	0
12270	12270	S15-5	Task Description	1	27	0.111111111111111	0.129186602870813	The task for participant systems is equivalent to TempEval-3 task ABC, see Figure 1.	0
12271	12271	S15-5	Task Description	2	28	0.222222222222222	0.133971291866029	Systems must annotate temporal expressions, events, and temporal relations between them 1 .	1
12272	12272	S15-5	Task Description	3	29	0.333333333333333	0.138755980861244	The input to participants is a set of unannotated text documents in TempEval-3 format.	0
12273	12273	S15-5	Task Description	4	30	0.444444444444444	0.143540669856459	Participating systems are required to annotate the plain documents following the TimeML scheme, divided into two types of elements:	0
12274	12274	S15-5	Task Description	5	31	0.555555555555556	0.148325358851675	" Temporal entities: These include events (EVENT tag, ""came"", ""attack"") and temporal expressions (timexes, TIMEX3 tag, e.g., ""yesterday"", ""8 p.m."") as well as their attributes like event class, timex type, and normalized values."	0
12275	12275	S15-5	Task Description	6	32	0.666666666666667	0.15311004784689	 (1) John was in the gym between 6:00 p.m and 7:00 p.m.	0
12276	12276	S15-5	Task Description	7	33	0.777777777777778	0.157894736842105	Each system's annotations represent its temporal knowledge of the documents.	0
12277	12277	S15-5	Task Description	8	34	0.888888888888889	0.162679425837321	These annotations are 1 http://alt.qcri.org/semeval2015/task5	0
12278	12278	S15-5	Task Description	9	35	1.0	0.167464114832536	Figure 1: Task -Equivalent to TempEval-3 task ABC then used as input to a temporal QA system (Uz-Zaman et al., 2012) that will answer questions on behalf of the systems, and the accuracy of their answers is compared across systems.	0
12592	12592	S15-7	abstract	1	2	0.166666666666667	0.008658008658009	In this paper we describe a novel task, namely the Diachronic Text Evaluation task.	0
12593	12593	S15-7	abstract	2	3	0.333333333333333	0.012987012987013	A corpus of snippets which contain relevant information for the time when the text was created is extracted from a large collection of newspapers published between 1700 and 2010.	0
12594	12594	S15-7	abstract	3	4	0.5	0.017316017316017	The task, subdivided in three subtasks, requires the automatic system to identify the time interval when the piece of news was written.	1
12595	12595	S15-7	abstract	4	5	0.666666666666667	0.021645021645022	The subtasks concern specific type of information that might be available in news.	0
12596	12596	S15-7	abstract	5	6	0.833333333333333	0.025974025974026	The intervals come in three grades: fine, medium and coarse according to their length.	0
12597	12597	S15-7	abstract	6	7	1.0	0.03030303030303	The systems participating in the tasks have proved that this a doable task with very interesting possible continuations.	0
12852	12852	S15-8	The Task	1	31	1.0	0.203947368421053	The goals of SpaceEval include identifying and classifying items from an inventory of spatial concepts:	1
12980	12980	S15-9	Introduction	1	7	0.083333333333333	0.053030303030303	Current research in sentiment analysis (SA, henceforth) is mostly focused on lexical resources that store polarity values.	0
12981	12981	S15-9	Introduction	2	8	0.166666666666667	0.060606060606061	For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items.	0
12982	12982	S15-9	Introduction	3	9	0.25	0.068181818181818	This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events -involving perspectives and points of view -are expressed.	0
12983	12983	S15-9	Introduction	4	10	0.333333333333333	0.075757575757576	In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions -mainly adjectives, adverbs and several nouns -leaving verbs on the foreground.	0
12984	12984	S15-9	Introduction	5	11	0.416666666666667	0.083333333333333	Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009;	0
12985	12985	S15-9	Introduction	6	12	0.5	0.090909090909091	Akkaya et al., 2009, Balhaur et al., 2011	0
12986	12986	S15-9	Introduction	7	13	0.583333333333333	0.098484848484849	Russo et al. 2011;Cambria et al., 2012, Deng et al., 2013.	0
12987	12987	S15-9	Introduction	8	14	0.666666666666667	0.106060606060606	One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity.	0
12988	12988	S15-9	Introduction	9	15	0.75	0.113636363636364	By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment.	0
12989	12989	S15-9	Introduction	10	16	0.833333333333333	0.121212121212121	Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions.	0
12990	12990	S15-9	Introduction	11	17	0.916666666666667	0.128787878787879	Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (Lewinsohn and Amenson, 1978;	1
12991	12991	S15-9	Introduction	12	18	1.0	0.136363636363636	MacPhillamy and Lewinsohn, 1982).	0
13357	13357	S15-10	Conclusion	1	252	0.125	0.972972972972973	We have described the five subtasks organized as part of SemEval-2015 Task 10 on Sentiment Analysis in Twitter: detecting sentiment of terms in context (subtask A), classifiying the sentiment of an entire tweet, SMS message or blog post (subtask B), predicting polarity towards a topic (subtask C), quantifying polarity towards a topic (subtask D), and proposing real-valued prior sentiment scores for Twitter terms (subtask E).	1
13358	13358	S15-10	Conclusion	2	253	0.25	0.976833976833977	Over 40 teams participated in these subtasks, using various techniques.	0
13359	13359	S15-10	Conclusion	3	254	0.375	0.980694980694981	We plan a new edition of the task as part of SemEval-2016, where we will focus on sentiment with respect to a topic, but this time on a fivepoint scale, which is used for human review ratings on popular websites such as Amazon, TripAdvisor, Yelp, etc.	0
13360	13360	S15-10	Conclusion	4	255	0.5	0.984555984555985	From a research perspective, moving to an ordered five-point scale means moving from binary classification to ordinal regression.	0
13361	13361	S15-10	Conclusion	5	256	0.625	0.988416988416988	We further plan to continue the trend detection subtask, which represents a move from classification to quantification, and is on par with what applications need.	0
13362	13362	S15-10	Conclusion	6	257	0.75	0.992277992277992	They are not interested in the sentiment of a particular tweet but rather in the percentage of tweets that are positive/negative.	0
13363	13363	S15-10	Conclusion	7	258	0.875	0.996138996138996	Finally, we plan a new subtask on trend detection, but using a five-point scale, which would get us even closer to what business (e.g. marketing studies), and researchers, (e.g. in political science or public policy), want nowadays.	0
13364	13364	S15-10	Conclusion	8	259	1.0	1.0	From a research perspective, this is a problem of ordinal quantification.	0
13400	13400	S15-11	Task Description	1	36	0.142857142857143	0.253521126760563	The task concerns itself with the classification of overall sentiment in micro-texts drawn from the micro-blogging service Twitter.	0
13401	13401	S15-11	Task Description	2	37	0.285714285714286	0.26056338028169	These texts, called tweets, are chosen so that the set as a whole contains a great deal of irony, sarcasm or metaphor, so no particular tweet is guaranteed to manifest a specific figurative phenomenon.	0
13402	13402	S15-11	Task Description	3	38	0.428571428571429	0.267605633802817	Since irony and sarcasm are typically used to criticize or to mock, and thus skew the perception of sentiment toward the negative, it is not enough for a system to simply determine whether the sentiment of a given tweet is positive or negative.	0
13403	13403	S15-11	Task Description	4	39	0.571428571428571	0.274647887323944	We thus use an 11point scale, ranging from -5 (very negative, for tweets with highly critical meanings) to +5 (very positive, for tweets with flattering or very upbeat meanings).	0
13404	13404	S15-11	Task Description	5	40	0.714285714285714	0.28169014084507	The point 0 on this scale is used for neutral tweets, or those whose positivity and negativity cancel each other out.	0
13405	13405	S15-11	Task Description	6	41	0.857142857142857	0.288732394366197	While the majority of tweets will have sentiments in the negative part of the scale, the challenge for participating systems is to decide just how negative or positive a tweet seems to be.	0
13406	13406	S15-11	Task Description	7	42	1.0	0.295774647887324	So, given a set of tweets that are rich in metaphor, sarcasm and irony, the goal is to determine whether a user has expressed a positive, negative or neutral sentiment in each, and the degree to which this sentiment has been communicated.	1
13508	13508	S15-12	abstract	1	2	0.125	0.00952380952381	SemEval-2015	0
13509	13509	S15-12	abstract	2	3	0.25	0.014285714285714	Task 12, a continuation of SemEval-2014	0
13510	13510	S15-12	abstract	3	4	0.375	0.019047619047619	Task 4, aimed to foster research beyond sentence-or text-level sentiment classification towards Aspect Based Sentiment Analysis.	0
13511	13511	S15-12	abstract	4	5	0.5	0.023809523809524	The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price).	1
13512	13512	S15-12	abstract	5	6	0.625	0.028571428571429	The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure.	0
13513	13513	S15-12	abstract	6	7	0.75	0.033333333333333	It attracted 93 submissions from 16 teams.	0
13514	13514	S15-12	abstract	7	8	0.875	0.038095238095238	1	0
13515	13515	S15-12	abstract	8	9	1.0	0.042857142857143	A subset of the datasets has been annotated with aspects at the sentence level.	0
13888	13888	S15-14	abstract	1	2	0.1	0.012738853503185	We describe two tasks-named entity recognition (Task 1) and template slot filling (Task 2)-for clinical texts.	1
13889	13889	S15-14	abstract	2	3	0.2	0.019108280254777	The tasks leverage annotations from the ShARe corpus, which consists of clinical notes with annotated mentions disorders, along with their normalization to a medical terminology and eight additional attributes.	0
13890	13890	S15-14	abstract	3	4	0.3	0.02547770700637	The purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling.	0
13891	13891	S15-14	abstract	4	5	0.4	0.031847133757962	Task 2 consisted of two subtasks: template slot filling given gold-standard disorder spans (Task 2a) and end-to-end disorder span identification together with template slot filling (Task 2b).	0
13892	13892	S15-14	abstract	5	6	0.5	0.038216560509554	For Task 1 (disorder span detection and normalization), 16 teams participated.	0
13893	13893	S15-14	abstract	6	7	0.6	0.044585987261147	The best system yielded a strict F1-score of 75.7, with a precision of 78.3 and recall of 73.2.	0
13894	13894	S15-14	abstract	7	8	0.7	0.050955414012739	For Task 2a (template slot filling given goldstandard disorder spans), six teams participated.	0
13895	13895	S15-14	abstract	8	9	0.8	0.057324840764331	The best system yielded a combined overall weighted accuracy for slot filling of 88.6.	0
13896	13896	S15-14	abstract	9	10	0.9	0.063694267515924	For Task 2b (disorder recognition and template slot filling), nine teams participated.	0
13897	13897	S15-14	abstract	10	11	1.0	0.070063694267516	The best system yielded a combined relaxed F (for span detection) and overall weighted accuracy of 80.8.	0
14227	14227	S15-15	Conclusion	1	184	0.090909090909091	0.948453608247423	This paper introduces a new SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis.	0
14228	14228	S15-15	Conclusion	2	185	0.181818181818182	0.95360824742268	Dictionary entry building is split into three subtasks: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically.	1
14229	14229	S15-15	Conclusion	3	186	0.272727272727273	0.958762886597938	Drawing from the Pattern Dictionary of English Verbs, we have produced a high-quality resource for the advancement of semantic processing: it contains 121 verbs connected to a corpus of 17,000 sentences.	0
14230	14230	S15-15	Conclusion	4	187	0.363636363636364	0.963917525773196	This resource will be made freely accessible from the task website for more in depth future research.	0
14231	14231	S15-15	Conclusion	5	188	0.454545454545455	0.969072164948454	Task 15 has attracted 5 participants, 3 on subtask 1 and 2 on subtask 2.	0
14232	14232	S15-15	Conclusion	6	189	0.545454545454545	0.974226804123711	Subtask 1 proved to be more difficult for participants than expected, since no system beat the baseline.	0
14233	14233	S15-15	Conclusion	7	190	0.636363636363636	0.979381443298969	We however show that the submissions possess interesting features that should be put to use in future experiments on the dataset.	0
14234	14234	S15-15	Conclusion	8	191	0.727272727272727	0.984536082474227	Subtask 2's baseline was beaten by one of the participants on a large margin, despite the fact that the baseline is very competitive.	0
14235	14235	S15-15	Conclusion	9	192	0.818181818181818	0.989690721649485	It seems that splitting the task into 3 subtasks has had the benefit of attracting different approaches (supervised and unsupervised) towards the common target of the task, which is to build a dictionary entry.	0
14236	14236	S15-15	Conclusion	10	193	0.909090909090909	0.994845360824742	Lexicography is such a complex task that it needs major efforts from the NLP community to support it.	0
14237	14237	S15-15	Conclusion	11	194	1.0	1.0	We hope that this task will stimulate more research and the development of new approaches to the automatic creation of lexical resources.	0
14239	14239	S15-17	abstract	1	2	0.25	0.013422818791946	This paper describes the first shared task on Taxonomy Extraction Evaluation organised as part of SemEval-2015.	0
14240	14240	S15-17	abstract	2	3	0.5	0.02013422818792	Participants were asked to find hypernym-hyponym relations between given terms.	1
14241	14241	S15-17	abstract	3	4	0.75	0.026845637583893	For each of the four selected target domains the participants were provided with two lists of domainspecific terms: a WordNet collection of terms and a well-known terminology extracted from an online publicly available taxonomy.	0
14242	14242	S15-17	abstract	4	5	1.0	0.033557046979866	A total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures, the structural similarity with a gold standard taxonomy, and through manual quality assessment of sampled novel relations.	0
14388	14388	S15-18	abstract	1	2	0.5	0.011834319526627	Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning.	1
14389	14389	S15-18	abstract	2	3	1.0	0.017751479289941	In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.	0
14634	14634	S16-1	Task Overview	1	79	0.043478260869565	0.276223776223776	STS presents participating systems with paired text snippets of approximately one sentence in length.	0
14635	14635	S16-1	Task Overview	2	80	0.086956521739131	0.27972027972028	The systems are then asked to return a numerical score indicating the degree of semantic similarity between the two snippets.	1
14636	14636	S16-1	Task Overview	3	81	0.130434782608696	0.283216783216783	Canonical STS scores fall on an ordinal scale with 6 specifically defined degrees of semantic similarity (see Table 1).	0
14637	14637	S16-1	Task Overview	4	82	0.173913043478261	0.286713286713287	While the underlying labels and their interpretation are ordinal, systems can provide real valued scores to indicate their semantic similarity prediction.	0
14638	14638	S16-1	Task Overview	5	83	0.217391304347826	0.29020979020979	Participating systems are then evaluated based on the degree to which their predicted similarity scores correlate with STS human judgements.	0
14639	14639	S16-1	Task Overview	6	84	0.260869565217391	0.293706293706294	Algorithms are free to use any scale or range of values for the scores they return.	0
14640	14640	S16-1	Task Overview	7	85	0.304347826086957	0.297202797202797	They are not punished for outputting scores outside the range of the interpretable human annotated STS labels.	0
14641	14641	S16-1	Task Overview	8	86	0.347826086956522	0.300699300699301	This evaluation strategy is motivated by a desire to maximize the flexibility in the design of machine learning models and systems for STS.	0
14642	14642	S16-1	Task Overview	9	87	0.391304347826087	0.304195804195804	It reinforces the assumption that computing textual similarity is an enabling component for other natural language processing applications, rather than being an end in itself.	0
14643	14643	S16-1	Task Overview	10	88	0.434782608695652	0.307692307692308	Table 1 illustrates the ordinal similarity scale the shared task uses.	0
14644	14644	S16-1	Task Overview	11	89	0.478260869565217	0.311188811188811	Both the English and the crosslingual Spanish-English STS subtasks use a 6 point similarity scale.	0
14645	14645	S16-1	Task Overview	12	90	0.521739130434783	0.314685314685315	A similarity label of 0 means that two texts are completely dissimilar; this can be interpreted as two sentences with no overlap in their meanings.	0
14646	14646	S16-1	Task Overview	13	91	0.565217391304348	0.318181818181818	The next level up, a similarity label of 1, indicates that the two snippets are not equivalent but are topically related to each other.	0
14647	14647	S16-1	Task Overview	14	92	0.608695652173913	0.321678321678322	A label of 2 indicates that the two texts are still not equivalent but agree on some details of what is being said.	0
14648	14648	S16-1	Task Overview	15	93	0.652173913043478	0.325174825174825	The labels 3 and 4, both indicate that the two sentences are approximately equivalent.	0
14649	14649	S16-1	Task Overview	16	94	0.695652173913043	0.328671328671329	However, a score of 3 implies that there are some differences in important details, while a score of 4 indicates that the differing details are not important.	0
14650	14650	S16-1	Task Overview	17	95	0.739130434782609	0.332167832167832	The top score of 5, denotes that the two texts being evaluated have complete semantic equivalence.	0
14651	14651	S16-1	Task Overview	18	96	0.782608695652174	0.335664335664336	In the context of the STS task, meaning equivalence is defined operationally as two snippets of text that mean the same thing when interpreted by a reasonable human judge.	0
14652	14652	S16-1	Task Overview	19	97	0.826086956521739	0.339160839160839	The operational approach to sentence level semantics was popularized by the recognizing textual entailment task (Dagan et al., 2010).	0
14653	14653	S16-1	Task Overview	20	98	0.869565217391304	0.342657342657343	It has the advantage that it allows the labeling of sentence pairs by human annotators without any training in formal semantics, while also being more useful and intuitive to work with for downstream systems.	0
14654	14654	S16-1	Task Overview	21	99	0.91304347826087	0.346153846153846	Beyond just sentence level semantics, the operationally defined STS labels also reflect both world knowledge and pragmatic phenomena.	0
14655	14655	S16-1	Task Overview	22	100	0.956521739130435	0.34965034965035	As in prior years, 2016 shared task participants are allowed to make use of existing resources and tools (e.g., WordNet, Mikolov et al. (2013)'s word2vec).	0
14656	14656	S16-1	Task Overview	23	101	1.0	0.353146853146853	Participants are also allowed to make unsupervised use of arbitrary data sets, even if such data overlaps with the announced sources of the evaluation data.	0
14849	14849	S16-2	Introduction	1	8	0.027027027027027	0.04040404040404	Semantic Textual Similarity (STS) (Agirre et al., 2015) measures the degree of equivalence in the underlying semantics of paired snippets of text.	0
14850	14850	S16-2	Introduction	2	9	0.054054054054054	0.045454545454546	The idea of Interpretable STS (iSTS) is to explain why two sentences may be related/unrelated, by supplementing the STS similarity score with an explanatory layer.	0
14851	14851	S16-2	Introduction	3	10	0.081081081081081	0.050505050505051	Our final goal would be to enable interpretable systems, that is, systems that are able to explain which are the differences and commonalities between two sentences.	0
14852	14852	S16-2	Introduction	4	11	0.108108108108108	0.055555555555556	For instance, let's assume the following two sentences drawn from a corpus of news headlines: 12 killed in bus accident in Pakistan 10 killed in road accident in NW Pakistan * * Authors listed in alphabetical order 1 http://at.qrci.org/semeval2016/task2/	0
14853	14853	S16-2	Introduction	5	12	0.135135135135135	0.060606060606061	The output of such a system would be something like the following:	0
14854	14854	S16-2	Introduction	6	13	0.162162162162162	0.065656565656566	The two sentences talk about accidents with casualties in Pakistan, but they differ in the number of people killed (12 vs. 10) and level of detail: the first one specifies that it is a bus accident, and the second one specifies that the location is NW Pakistan.	0
14855	14855	S16-2	Introduction	7	14	0.189189189189189	0.070707070707071	While giving such explanations comes naturally to people, constructing algorithms and computational models that mimic human level performance represents a difficult Natural Language Understanding (NLU) problem, with applications in dialogue systems, interactive systems and educational systems.	0
14856	14856	S16-2	Introduction	8	15	0.216216216216216	0.075757575757576	In the iSTS 2015 pilot task (Agirre et al., 2015), we defined a first step of such an ambitious system, which we follow in 2016.	0
14857	14857	S16-2	Introduction	9	16	0.243243243243243	0.080808080808081	Given the input (a pair of sentences), participant systems need first to identify the chunks in each sentence, and then, align chunks across the two sentences, indicating the relation and similarity score of each alignment.	1
14858	14858	S16-2	Introduction	10	17	0.27027027027027	0.085858585858586	The relation can be one of equivalence, opposition, specificity, similarity or relatedness, and the similarity score can range from 1 to 5.	0
14859	14859	S16-2	Introduction	11	18	0.297297297297297	0.090909090909091	Unrelated chunks are left unaligned.	0
14860	14860	S16-2	Introduction	12	19	0.324324324324324	0.095959595959596	An optional tag can be added to alignments for the cases where there is a difference in factuality or polarity.	0
14861	14861	S16-2	Introduction	13	20	0.351351351351351	0.101010101010101	See Figure 1 for the manual alignment of the two sample sentences.	0
14862	14862	S16-2	Introduction	14	21	0.378378378378378	0.106060606060606	The alignments between chunks in Figure 1 can be used to produce the kind of explanations shown in the previous example.	0
14863	14863	S16-2	Introduction	15	22	0.405405405405405	0.111111111111111	"In previous work, Brockett (2007) and Rus et al. (2012) produced a dataset where corresponding Figure 1: Example of a manual alignment of two sentences: ""12 killed in bus accident in Pakistan"" and ""10 killed in road accident in NW Pakistan""."	0
14864	14864	S16-2	Introduction	16	23	0.432432432432432	0.116161616161616	Each aligned pair of chunks included information on the type of alignment, and the score of alignment.	0
14865	14865	S16-2	Introduction	17	24	0.45945945945946	0.121212121212121	words (including some multiword expressions like named-entities) were aligned.	0
14866	14866	S16-2	Introduction	18	25	0.486486486486487	0.126262626262626	Although this alignment is useful, we wanted to move forward to the alignment of segments, and decided to align chunks (Abney, 1991).	0
14867	14867	S16-2	Introduction	19	26	0.513513513513513	0.131313131313131	Brockett (2007) did not provide any label to alignments, while Rus et al. ( 2012) defined a basic typology.	0
14868	14868	S16-2	Introduction	20	27	0.540540540540541	0.136363636363636	In our task, we provided a more detailed typology for the aligned chunks as well as a similarity/relatedness score for each alignment.	0
14869	14869	S16-2	Introduction	21	28	0.567567567567568	0.141414141414141	Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them.	0
14870	14870	S16-2	Introduction	22	29	0.594594594594595	0.146464646464646	"In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the ""facets"" (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer."	0
14871	14871	S16-2	Introduction	23	30	0.621621621621622	0.151515151515152	The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment.	0
14872	14872	S16-2	Introduction	24	31	0.648648648648649	0.156565656565657	This model was later followed by Levy et al. (2013).	0
14873	14873	S16-2	Introduction	25	32	0.675675675675676	0.161616161616162	Our task was different in that we identified the corresponding chunks in both sentences.	0
14874	14874	S16-2	Introduction	26	33	0.702702702702703	0.166666666666667	We think that, in the future, the aligned facets could provide complementary information to chunks.	0
14875	14875	S16-2	Introduction	27	34	0.72972972972973	0.171717171717172	The SemEval Semantic Textual Similarity (STS) task in 2015 contained a subtask on Interpretable STS (Agirre et al., 2015), showing that the task is feasible, with high inter-annotator agreement and system scores well above baselines.	0
14876	14876	S16-2	Introduction	28	35	0.756756756756757	0.176767676767677	The datasets comprised news headlines and image captions.	0
14877	14877	S16-2	Introduction	29	36	0.783783783783784	0.181818181818182	For 2016, the pilot subtask has been updated into a standalone task.	0
14878	14878	S16-2	Introduction	30	37	0.810810810810811	0.186868686868687	The restriction from the iSTS 2015 task to allow only one-to-one alignments has been now lifted, and we thus allow any number of chunks to be aligned to any number of chunks.	0
14879	14879	S16-2	Introduction	31	38	0.837837837837838	0.191919191919192	Annotation guidelines have been revised accordingly, including an updated chunking criterium for subordinate clauses and a better explanation of the instruc-tions.	0
14880	14880	S16-2	Introduction	32	39	0.864864864864865	0.196969696969697	The 2015 datasets were re-annotated and released as training data.	0
14881	14881	S16-2	Introduction	33	40	0.891891891891892	0.202020202020202	New pairs from news headlines and image captions have been annotated and used for test.	0
14882	14882	S16-2	Introduction	34	41	0.918918918918919	0.207070707070707	In addition, a new dataset of sentence pairs from the education domain has been produced, including train and test data.	0
14883	14883	S16-2	Introduction	35	42	0.945945945945946	0.212121212121212	The paper is organized as follows.	0
14884	14884	S16-2	Introduction	36	43	0.972972972972973	0.217171717171717	We first provide the description of the task, followed by the evaluation metrics and the baseline system.	0
14885	14885	S16-2	Introduction	37	44	1.0	0.222222222222222	Section 5 describes the participation, Section 6 the results, and Section 7 comments on the systems, tools and resources used.	0
15053	15053	S16-3	Introduction	1	14	0.027027027027027	0.040114613180516	"Building on the success of SemEval-2015 Task 3 ""Answer Selection in Community Question Answering"" 1 , we run an extension in 2016, which covers a full task on Community Question Answering (CQA) and which is, therefore, closer to the real application needs."	0
15054	15054	S16-3	Introduction	2	15	0.054054054054054	0.04297994269341	All the information related to the task, data, participants, results and publications can be found on the SemEval-2016 Task 3 website.	0
15055	15055	S16-3	Introduction	3	16	0.081081081081081	0.045845272206304	2 1 http://alt.qcri.org/semeval2015/task3 2 http://alt.qcri.org/semeval2016/task3 CQA forums such as Stack Overflow 3 and Qatar Living 4 , are gaining popularity online.	0
15056	15056	S16-3	Introduction	4	17	0.108108108108108	0.048710601719198	These forums are seldom moderated, quite open, and thus they typically have little restrictions, if any, on who can post and who can answer a question.	0
15057	15057	S16-3	Introduction	5	18	0.135135135135135	0.051575931232092	On the positive side, this means that one can freely ask any question and can then expect some good, honest answers.	0
15058	15058	S16-3	Introduction	6	19	0.162162162162162	0.054441260744986	On the negative side, it takes effort to go through all possible answers and to make sense of them.	0
15059	15059	S16-3	Introduction	7	20	0.189189189189189	0.05730659025788	For example, it is not unusual for a question to have hundreds of answers, which makes it very time-consuming for the user to inspect and to winnow through them all.	0
15060	15060	S16-3	Introduction	8	21	0.216216216216216	0.060171919770774	The present task could help to automate the process of finding good answers to new questions in a community-created discussion forum, e.g., by retrieving similar questions in the forum and by identifying the posts in the comment threads of those similar questions that answer the original question well.	0
15061	15061	S16-3	Introduction	9	22	0.243243243243243	0.063037249283668	"In essence, the main CQA task can be defined as follows: ""given (i) a new question and (ii) a large collection of question-comment threads created by a user community, rank the comments that are most useful for answering the new question""."	1
15062	15062	S16-3	Introduction	10	23	0.27027027027027	0.065902578796562	The test question is new with respect to the collection, but it is expected to be related to one or several questions in the collection.	0
15063	15063	S16-3	Introduction	11	24	0.297297297297297	0.068767908309456	The best answers can come from different question-comment threads.	0
15064	15064	S16-3	Introduction	12	25	0.324324324324324	0.07163323782235	In the collection, the threads are independent of each other and the lists of comments are chronologically sorted and contain some meta information, e.g., date, user, topic, etc.	0
15065	15065	S16-3	Introduction	13	26	0.351351351351351	0.074498567335244	The comments in a particular thread are intended to answer the question initiating that thread, but since this is a resource created by a community of casual users, there is a lot of noise and irrelevant material, apart from informal language usage and lots of typos and grammatical mistakes.	0
15066	15066	S16-3	Introduction	14	27	0.378378378378378	0.077363896848138	Interestingly, the questions in the collection can be semantically related to each other, although not explicitly.	0
15067	15067	S16-3	Introduction	15	28	0.405405405405405	0.080229226361032	Our intention was not to run just another regular Question Answering task.	0
15068	15068	S16-3	Introduction	16	29	0.432432432432432	0.083094555873926	"Similarly to the 2015 edition, we had three objectives: (i) to focus on semantic-based solutions beyond simple ""bag-ofwords"" representations and ""word matching"" techniques; (ii) to study the new natural language processing (NLP) phenomena arising in the community question answering scenario, e.g., relations between the comments in a thread, relations between different threads and question-to-question similarity; and (iii) to facilitate the participation of non IR/QA experts to our challenge."	0
15069	15069	S16-3	Introduction	17	30	0.45945945945946	0.08595988538682	The third point was achieved by explicitly providing the set of potential answers-the search engine step was carried out by us-to be (re)ranked and by defining two optional subtasks apart from the main CQA task.	0
15070	15070	S16-3	Introduction	18	31	0.486486486486487	0.088825214899714	Subtask A (Question-Comment Similarity): given a question from a question-comment thread, rank the comments according to their relevance (similarity) with respect to the question; Subtask B (Question-Question Similarity): given the new question, rerank all similar questions retrieved by a search engine, assuming that the answers to the similar questions should be answering the new question too.	0
15071	15071	S16-3	Introduction	19	32	0.513513513513513	0.091690544412608	Subtasks	0
15072	15072	S16-3	Introduction	20	33	0.540540540540541	0.094555873925502	A and B should give participants enough tools to create a CQA system to solve the main task.	0
15073	15073	S16-3	Introduction	21	34	0.567567567567568	0.097421203438396	Nonetheless, one can approach CQA without necessarily solving the two tasks above.	0
15074	15074	S16-3	Introduction	22	35	0.594594594594595	0.100286532951289	Participants were free to use whatever approach they wanted, and the participation in the main task and/or the two subtasks was optional.	0
15075	15075	S16-3	Introduction	23	36	0.621621621621622	0.103151862464183	A more precise definition of all subtasks can be found in Section 3.	0
15076	15076	S16-3	Introduction	24	37	0.648648648648649	0.106017191977077	Keeping the multilinguality from 2015, we provided data for two languages: English and Arabic.	0
15077	15077	S16-3	Introduction	25	38	0.675675675675676	0.108882521489971	For English, we used real data from the communitycreated Qatar Living forum.	0
15078	15078	S16-3	Introduction	26	39	0.702702702702703	0.111747851002865	The Arabic data was collected from medical forums, with a slightly different procedure.	0
15079	15079	S16-3	Introduction	27	40	0.72972972972973	0.114613180515759	We only proposed the main ranking CQA task on this data, i.e., finding good answers for a given new question.	0
15080	15080	S16-3	Introduction	28	41	0.756756756756757	0.117478510028653	Finally, we provided training data for all languages and subtasks with human supervision.	0
15081	15081	S16-3	Introduction	29	42	0.783783783783784	0.120343839541547	All examples were manually labeled by a community of annotators in a crowdsourcing platform.	0
15082	15082	S16-3	Introduction	30	43	0.810810810810811	0.123209169054441	The datasets and the annotation procedure are described in Section 4, and some examples can be found in Figures 3 and 4.	0
15083	15083	S16-3	Introduction	31	44	0.837837837837838	0.126074498567335	The rest of the paper is organized as follows: Section 2 introduces some related work.	0
15084	15084	S16-3	Introduction	32	45	0.864864864864865	0.128939828080229	Section 3 gives a more detailed definition of the task.	0
15085	15085	S16-3	Introduction	33	46	0.891891891891892	0.131805157593123	Section 4 describes the datasets and the process of their creation.	0
15086	15086	S16-3	Introduction	34	47	0.918918918918919	0.134670487106017	Section 5 explains the evaluation measures.	0
15087	15087	S16-3	Introduction	35	48	0.945945945945946	0.137535816618911	Section 6 presents the results for all subtasks and for all participating systems.	0
15088	15088	S16-3	Introduction	36	49	0.972972972972973	0.140401146131805	Section 7 summarizes the main approaches and features used by these systems.	0
15089	15089	S16-3	Introduction	37	50	1.0	0.143266475644699	Finally, Section 8 offers some further discussion and presents the main conclusions.	0
15390	15390	S16-4	abstract	1	2	0.111111111111111	0.006849315068493	"This paper discusses the fourth year of the ""Sentiment Analysis in Twitter Task""."	0
15391	15391	S16-4	abstract	2	3	0.222222222222222	0.01027397260274	SemEval-2016	0
15392	15392	S16-4	abstract	3	4	0.333333333333333	0.013698630136986	Task 4 comprises five subtasks, three of which represent a significant departure from previous editions.	0
15393	15393	S16-4	abstract	4	5	0.444444444444444	0.017123287671233	The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet.	1
15394	15394	S16-4	abstract	5	6	0.555555555555556	0.02054794520548	"The three new subtasks focus on two variants of the basic ""sentiment classification in Twitter"" task."	0
15395	15395	S16-4	abstract	6	7	0.666666666666667	0.023972602739726	The first variant adopts a five-point scale, which confers an ordinal character to the classification task.	0
15396	15396	S16-4	abstract	7	8	0.777777777777778	0.027397260273973	The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature.	0
15397	15397	S16-4	abstract	8	9	0.888888888888889	0.030821917808219	The task continues to be very popular, attracting a total of 43 teams.	0
15398	15398	S16-4	abstract	9	10	1.0	0.034246575342466	* Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy.	0
15686	15686	S16-5	Introduction	1	6	0.027777777777778	0.029126213592233	Many consumers use the Web to share their experiences about products, services or travel destinations (Yoo and Gretzel, 2008).	0
15687	15687	S16-5	Introduction	2	7	0.055555555555556	0.033980582524272	Online opinionated texts (e.g., reviews, tweets) are important for consumer decision making (Chevalier and Mayzlin, 2006) and constitute a source of valuable customer feedback that can help companies to measure satisfaction and improve their products or services.	0
15688	15688	S16-5	Introduction	3	8	0.083333333333333	0.038834951456311	In this setting, Aspect Based Sentiment Analysis (ABSA) -i.e., mining opinions from text about specific entities and their aspects (Liu, 2012) -can provide valuable insights to both consumers and businesses.	1
15689	15689	S16-5	Introduction	4	9	0.111111111111111	0.04368932038835	An ABSA * *Corresponding author: mpontiki@ilsp.gr. method can analyze large amounts of unstructured texts and extract (coarse-or fine-grained) information not included in the user ratings that are available in some review sites (e.g., Fig. 1).	0
15690	15690	S16-5	Introduction	5	10	0.138888888888889	0.048543689320388	"Sentiment Analysis (SA) touches every aspect (e.g., entity recognition, coreference resolution, negation handling) of Natural Language Processing (Liu, 2012) and as Cambria et al. (2013) mention ""it requires a deep understanding of the explicit and implicit, regular and irregular, and syntactic and semantic language rules""."	0
15691	15691	S16-5	Introduction	6	11	0.166666666666667	0.053398058252427	Within the last few years several SA-related shared tasks have been organized in the context of workshops and conferences focus-ing on somewhat different research problems (Seki et al., 2007;	0
15692	15692	S16-5	Introduction	7	12	0.194444444444444	0.058252427184466	Seki et al., 2008;	0
15693	15693	S16-5	Introduction	8	13	0.222222222222222	0.063106796116505	Seki et al., 2010;	0
15694	15694	S16-5	Introduction	9	14	0.25	0.067961165048544	Mitchell, 2013;	0
15695	15695	S16-5	Introduction	10	15	0.277777777777778	0.072815533980583	Nakov et al., 2013;	0
15696	15696	S16-5	Introduction	11	16	0.305555555555556	0.077669902912621	Rosenthal et al., 2014;	0
15697	15697	S16-5	Introduction	12	17	0.333333333333333	0.08252427184466	Pontiki et al., 2014;	0
15698	15698	S16-5	Introduction	13	18	0.361111111111111	0.087378640776699	Rosenthal et al., 2015;	0
15699	15699	S16-5	Introduction	14	19	0.388888888888889	0.092233009708738	Ghosh et al., 2015;	0
15700	15700	S16-5	Introduction	15	20	0.416666666666667	0.097087378640777	Pontiki et al., 2015;Mohammad et al., 2016;	0
15701	15701	S16-5	Introduction	16	21	0.444444444444444	0.101941747572816	Recupero and Cambria, 2014;	0
15702	15702	S16-5	Introduction	17	22	0.472222222222222	0.106796116504854	Ruppenhofer et al., 2014;	0
15703	15703	S16-5	Introduction	18	23	0.5	0.111650485436893	Loukachevitch et al., 2015).	0
15704	15704	S16-5	Introduction	19	24	0.527777777777778	0.116504854368932	Such competitions provide training datasets and the opportunity for direct comparison of different approaches on common test sets.	0
15705	15705	S16-5	Introduction	20	25	0.555555555555556	0.121359223300971	Currently, most of the available SA-related datasets, whether released in the context of shared tasks or not (Socher et al., 2013;	0
15706	15706	S16-5	Introduction	21	26	0.583333333333333	0.12621359223301	Ganu et al., 2009), are monolingual and usually focus on English texts.	0
15707	15707	S16-5	Introduction	22	27	0.611111111111111	0.131067961165049	Multilingual datasets (Klinger and Cimiano, 2014;	0
15708	15708	S16-5	Introduction	23	28	0.638888888888889	0.135922330097087	Jimnez-Zafra et al., 2015) provide additional benefits enabling the development and testing of crosslingual methods (Lambert, 2015).	0
15709	15709	S16-5	Introduction	24	29	0.666666666666667	0.140776699029126	Following this direction, this year the SemEval ABSA task provided datasets in a variety of languages.	0
15710	15710	S16-5	Introduction	25	30	0.694444444444444	0.145631067961165	"ABSA was introduced as a shared task for the first time in the context of SemEval in 2014; SemEval-2014 Task 4 1 (SE-ABSA14) provided datasets of English reviews annotated at the sentence level with aspect terms (e.g., ""mouse"", ""pizza"") and their polarity for the laptop and restaurant domains, as well as coarser aspect categories (e.g., ""food"") and their polarity only for restaurants (Pontiki et al., 2014)."	0
15711	15711	S16-5	Introduction	26	31	0.722222222222222	0.150485436893204	SemEval-2015 Task 12 2 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks into a unified framework in which all the identified constituents of the expressed opinions (i.e., aspects, opinion target expressions and sentiment polarities) meet a set of guidelines and are linked to each other within sentence-level tuples (Pontiki et al., 2015).	0
15712	15712	S16-5	Introduction	27	32	0.75	0.155339805825243	These tuples are important since they indicate the part of text within which a specific opinion is expressed.	0
15713	15713	S16-5	Introduction	28	33	0.777777777777778	0.160194174757282	However, a user might also be interested in the overall rating of the text towards a particular aspect.	0
15714	15714	S16-5	Introduction	29	34	0.805555555555556	0.16504854368932	Such ratings can be used to estimate the mean sentiment per aspect from multiple reviews (McAuley et al., 2012).	0
15715	15715	S16-5	Introduction	30	35	0.833333333333333	0.169902912621359	Therefore, in addition to sentence-level annotations, SE-ABSA16 3 accommodated also text-level ABSA annotations and provided the respective training and testing data.	0
15716	15716	S16-5	Introduction	31	36	0.861111111111111	0.174757281553398	Fur-1 http://alt.qcri.org/semeval2014/task4/ 2 http://alt.qcri.org/semeval2015/task12/ 3 http://alt.qcri.org/semeval2016/task5/ thermore, the SE-ABSA15 annotation framework was extended to new domains and applied to languages other than English (Arabic, Chinese, Dutch, French, Russian, Spanish, and Turkish).	0
15717	15717	S16-5	Introduction	32	37	0.888888888888889	0.179611650485437	The remainder of this paper is organized as follows: the task set-up is described in Section 2.	0
15718	15718	S16-5	Introduction	33	38	0.916666666666667	0.184466019417476	Section 3 provides information about the datasets and the annotation process, while Section 4 presents the evaluation measures and the baselines.	0
15719	15719	S16-5	Introduction	34	39	0.944444444444444	0.189320388349515	General information about participation in the task is provided in Section 5.	0
15720	15720	S16-5	Introduction	35	40	0.972222222222222	0.194174757281553	The evaluation scores of the participating systems are presented and discussed in Section 6.	0
15721	15721	S16-5	Introduction	36	41	1.0	0.199029126213592	The paper concludes with an overall assessment of the task.	0
15888	15888	S16-6	abstract	1	2	0.142857142857143	0.008810572687225	Here for the first time we present a shared task on detecting stance from tweets: given a tweet and a target entity (person, organization, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely.	1
15889	15889	S16-6	abstract	2	3	0.285714285714286	0.013215859030837	The target of interest may or may not be referred to in the tweet, and it may or may not be the target of opinion.	0
15890	15890	S16-6	abstract	3	4	0.428571428571429	0.017621145374449	Two tasks are proposed.	0
15891	15891	S16-6	abstract	4	5	0.571428571428571	0.022026431718062	Task A is a traditional supervised classification task where 70% of the annotated data for a target is used as training and the rest for testing.	0
15892	15892	S16-6	abstract	5	6	0.714285714285714	0.026431718061674	For Task B, we use as test data all of the instances for a new target (not used in task A) and no training data is provided.	0
15893	15893	S16-6	abstract	6	7	0.857142857142857	0.030837004405286	Our shared task received submissions from 19 teams for Task A and from 9 teams for Task B.	0
15894	15894	S16-6	abstract	7	8	1.0	0.035242290748899	The highest classification F-score obtained was 67.82 for Task A and 56.28 for Task B. However, systems found it markedly more difficult to infer stance towards the target of interest from tweets that express opinion towards another entity.	0
16163	16163	S16-7	Task Description	1	50	0.038461538461539	0.251256281407035	The task is formulated as follows: given a list of terms (single words and multi-word phrases), an automatic system needs to provide a score between 0 and 1 that is indicative of the term's strength of association with positive sentiment.	1
16164	16164	S16-7	Task Description	2	51	0.076923076923077	0.256281407035176	A score of 1 indicates maximum association with positive sentiment (or least association with negative sentiment) and a score of 0 indicates least association with positive sentiment (or maximum association with negative sentiment).	0
16165	16165	S16-7	Task Description	3	52	0.115384615384615	0.261306532663317	If a term is more positive than another, then it should have a higher score than the other.	0
16166	16166	S16-7	Task Description	4	53	0.153846153846154	0.266331658291457	There are three tasks, one for each of the three domains:	0
16167	16167	S16-7	Task Description	5	54	0.192307692307692	0.271356783919598	 General English Sentiment Modifiers Set:	0
16168	16168	S16-7	Task Description	6	55	0.230769230769231	0.276381909547739	This dataset comprises English single words and multi-word phrases from the general domain.	0
16169	16169	S16-7	Task Description	7	56	0.269230769230769	0.281407035175879	The phrases are formed by combining a word and a modifier, where a modifier is a negator, an auxilary verb, a degree adverb, or a combination of those, for example, would be very easy, did not harm, and would have been nice.	0
16170	16170	S16-7	Task Description	8	57	0.307692307692308	0.28643216080402	The single word terms are chosen from the set of words that are part of the multi-word phrases, for example, easy, harm, and nice.	0
16171	16171	S16-7	Task Description	9	58	0.346153846153846	0.291457286432161	 English Twitter Mixed Polarity Set:	0
16172	16172	S16-7	Task Description	10	59	0.384615384615385	0.296482412060301	This dataset focuses on English phrases made up of opposing polarity terms, for example, phrases such as lazy sundays, best winter break, happy accident and couldn't stop smiling.	0
16173	16173	S16-7	Task Description	11	60	0.423076923076923	0.301507537688442	The dataset also includes single word terms (as separate entries).	0
16174	16174	S16-7	Task Description	12	61	0.461538461538462	0.306532663316583	These terms are chosen from the set of words that are part of the multi-word phrases.	0
16175	16175	S16-7	Task Description	13	62	0.5	0.311557788944724	The multi-word phrases and single-word terms are drawn from a corpus of tweets, and include a small number of hashtag words (e.g., #wantit) and creatively spelled words (e.g., plssss).	0
16176	16176	S16-7	Task Description	14	63	0.538461538461538	0.316582914572864	However, a majority of the terms are those that one would use in everyday English.	0
16177	16177	S16-7	Task Description	15	64	0.576923076923077	0.321608040201005	 Arabic Twitter Set:	0
16178	16178	S16-7	Task Description	16	65	0.615384615384615	0.326633165829146	This dataset includes single words and phrases commonly found in Arabic tweets.	0
16179	16179	S16-7	Task Description	17	66	0.653846153846154	0.331658291457286	The phrases in this set are formed only by combining a negator and a word.	0
16180	16180	S16-7	Task Description	18	67	0.692307692307692	0.336683417085427	Teams could participate in any one, two, or all three tasks; however, only one submission was al-  lowed per task.	0
16181	16181	S16-7	Task Description	19	68	0.730769230769231	0.341708542713568	For each task, the above description and a development set (200 terms) were provided to the participants in advance; there were no training sets.	0
16182	16182	S16-7	Task Description	20	69	0.769230769230769	0.346733668341709	The three test sets, one for each task, were released at the start of the evaluation period.	0
16183	16183	S16-7	Task Description	21	70	0.807692307692308	0.351758793969849	The test sets and the development sets have no terms in common.	0
16184	16184	S16-7	Task Description	22	71	0.846153846153846	0.35678391959799	The participants were allowed to use the development sets in any way (for example, for tuning or training), and they were allowed to use any additional manually or automatically generated resources.	0
16185	16185	S16-7	Task Description	23	72	0.884615384615385	0.361809045226131	In 2015, the task was set up similarly (Rosenthal et al., 2015).	0
16186	16186	S16-7	Task Description	24	73	0.923076923076923	0.366834170854271	Single words and multi-word phrases from English Twitter comprised the development and test sets (1,515 terms in total).	0
16187	16187	S16-7	Task Description	25	74	0.961538461538462	0.371859296482412	The phrases were simple two-word negated expressions (e.g., cant waitttt).	0
16188	16188	S16-7	Task Description	26	75	1.0	0.376884422110553	Participants were allowed to use these datasets for the development of their systems.	0
16314	16314	S16-8	abstract	1	2	0.125	0.011834319526627	In this report we summarize the results of the SemEval 2016 Task 8: Meaning Representation Parsing.	0
16315	16315	S16-8	abstract	2	3	0.25	0.017751479289941	Participants were asked to generate Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the news and discussion forum domains.	1
16316	16316	S16-8	abstract	3	4	0.375	0.023668639053255	Eleven sites submitted valid systems.	0
16317	16317	S16-8	abstract	4	5	0.5	0.029585798816568	The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b;	0
16318	16318	S16-8	abstract	5	6	0.625	0.035502958579882	Wang et al., 2015a) as a baseline system and added extensions to it to improve scores.	0
16319	16319	S16-8	abstract	6	7	0.75	0.041420118343195	The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion.	0
16320	16320	S16-8	abstract	7	8	0.875	0.047337278106509	The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic.	0
16321	16321	S16-8	abstract	8	9	1.0	0.053254437869823	We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies.	0
16483	16483	S16-9	abstract	1	2	0.2	0.014598540145985	This paper describes the SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing.	0
16484	16484	S16-9	abstract	2	3	0.4	0.021897810218978	We extend the traditional treestructured representation of Chinese sentence to directed acyclic graphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences.	1
16485	16485	S16-9	abstract	3	4	0.6	0.029197080291971	We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXT-BOOKS domain with 14,793 sentences respectively.	0
16486	16486	S16-9	abstract	4	5	0.8	0.036496350364964	We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on.	0
16487	16487	S16-9	abstract	5	6	1.0	0.043795620437956	At last, we briefly describe the submitted systems and analyze these results.	0
16623	16623	S16-10	Introduction	1	5	0.066666666666667	0.018587360594796	Grammatical analysis tasks, e.g., part-of-speech tagging, are rather successful applications of natural language processing (NLP).	0
16624	16624	S16-10	Introduction	2	6	0.133333333333333	0.022304832713755	They are comprehensive, i.e., they operate under the assumption that all grammatically-relevant parts of a sentence will be analyzed:	0
16625	16625	S16-10	Introduction	3	7	0.2	0.026022304832714	We do not expect a POS tagger to only know a subset of the tags in the language.	0
16626	16626	S16-10	Introduction	4	8	0.266666666666667	0.029739776951673	Most POS tags accommodate unseen words and adapt readily to new text genres.	0
16627	16627	S16-10	Introduction	5	9	0.333333333333333	0.033457249070632	Together, these factors indicate a representation which achieves broad coverage.	0
16628	16628	S16-10	Introduction	6	10	0.4	0.037174721189591	Explicit analysis of lexical semantics, by contrast, has been more difficult to scale to broad coverage owing to limited comprehensiveness and extensibility.	0
16629	16629	S16-10	Introduction	7	11	0.466666666666667	0.04089219330855	The dominant paradigm of fine-grained word sense disambiguation, WordNet (Fellbaum, 1998), is difficult to annotate in corpora, results in considerable data sparseness, and does not readily generalize to out-of-vocabulary words.	0
16630	16630	S16-10	Introduction	8	12	0.533333333333333	0.044609665427509	While the main corpus with WordNet senses, SemCor (Miller et al., 1993), does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech.	0
16631	16631	S16-10	Introduction	9	13	0.6	0.048327137546468	This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora.	0
16632	16632	S16-10	Introduction	10	14	0.666666666666667	0.052044609665428	To address this limitation, in the DiMSUM 2016 shared task, 1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following Schneider and Smith, 2015), on multiple nontraditional genres of text.	1
16633	16633	S16-10	Introduction	11	15	0.733333333333333	0.055762081784387	By moving away from fine-grained sense inventories and lexicalized, language-specific 2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis.	0
16634	16634	S16-10	Introduction	12	16	0.8	0.059479553903346	We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres.	0
16635	16635	S16-10	Introduction	13	17	0.866666666666667	0.063197026022305	The integrated lexical semantic representation ( 2, 3) has been annotated in an extensive benchmark data set comprising several nontraditional domains ( 4).	0
16636	16636	S16-10	Introduction	14	18	0.933333333333333	0.066914498141264	Objective, controlled evaluation procedures ( 5) facilitate a comparison of the 9 systems submitted as part of the official task ( 6).	0
16637	16637	S16-10	Introduction	15	19	1.0	0.070631970260223	While the systems range in performance, all are below 60% in our composite evaluation, suggesting that further work is needed to make progress on this difficult task.	0
16913	16913	S16-11	Task Description	1	26	0.125	0.131313131313131	The Complex Word Identification task of SemEval 2016 invited participants to create systems that, given a sentence and a target word within it, can predict whether or not a non-native English speaker would be able to understand the meaning of the target word.	1
16914	16914	S16-11	Task Description	2	27	0.25	0.136363636363636	We chose non-native speakers as a target audience because, unlike second language learners and those with low literacy levels or conditions such as Aphasia and Dyslexia, non-native speakers of English have not yet been explicitly assessed with respect to their simplification needs.	0
16915	16915	S16-11	Task Description	3	28	0.375	0.141414141414141	In addition, the broad availability of such an audience makes data collection more feasible.	0
16916	16916	S16-11	Task Description	4	29	0.5	0.146464646464646	We have established main goals for the task:	0
16917	16917	S16-11	Task Description	5	30	0.625	0.151515151515152	1	0
16918	16918	S16-11	Task Description	6	31	0.75	0.156565656565657	To learn which words challenge non-native English speakers and to understand what their traits are.	0
16919	16919	S16-11	Task Description	7	32	0.875	0.161616161616162	2	0
16920	16920	S16-11	Task Description	8	33	1.0	0.166666666666667	To investigate how well one's individual vocabulary limitations can be predicted from the overall vocabulary limitations of others in the same category.	0
17092	17092	S16-12	Introduction	1	7	0.066666666666667	0.058823529411765	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
17093	17093	S16-12	Introduction	2	8	0.133333333333333	0.067226890756303	Verhagen et al., 2010;UzZaman et al., 2013).	0
17094	17094	S16-12	Introduction	3	9	0.2	0.07563025210084	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	1
17095	17095	S16-12	Introduction	4	10	0.266666666666667	0.084033613445378	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
17096	17096	S16-12	Introduction	5	11	0.333333333333333	0.092436974789916	In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; to broaden our understanding of the language of time beyond newswire expressions and structure.	0
17097	17097	S16-12	Introduction	6	12	0.4	0.100840336134454	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
17098	17098	S16-12	Introduction	7	13	0.466666666666667	0.109243697478992	Participating systems are expected to take as input raw text, for example: April 23, 2014:	0
17099	17099	S16-12	Introduction	8	14	0.533333333333333	0.117647058823529	The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
17100	17100	S16-12	Introduction	9	15	0.6	0.126050420168067	The systems are then expected to output annotations over the text, for example, those shown in Figure 1.	0
17101	17101	S16-12	Introduction	10	16	0.666666666666667	0.134453781512605	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
17102	17102	S16-12	Introduction	11	17	0.733333333333333	0.142857142857143	Clinical TempEval 2016 addressed one of the major challenges in Clinical TempEval 2015: data distribution.	0
17103	17103	S16-12	Introduction	12	18	0.8	0.151260504201681	Because Clinical TempEval is based on real patient notes from the Mayo Clinic, participants go through a lengthy authorization process involving a data use agreement and an interview.	0
17104	17104	S16-12	Introduction	13	19	0.866666666666667	0.159663865546218	For Clinical TempEval 2016, we streamlined this process and were able to authorize data access for more than twice as many participants as Clinical TempEval 2015.	0
17105	17105	S16-12	Introduction	14	20	0.933333333333333	0.168067226890756	And since all the training and evaluation data distributed for Clinical TempEval 2015 was used as the training data for Clinical TempEval 2016, participants had more than a year to work on their systems.	0
17106	17106	S16-12	Introduction	15	21	1.0	0.176470588235294	The result was that four times as many teams participated.	0
17206	17206	S16-13	abstract	1	2	0.25	0.010050251256281	This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016.	0
17207	17207	S16-13	abstract	2	3	0.5	0.015075376884422	This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them.	1
17208	17208	S16-13	abstract	3	4	0.75	0.020100502512563	TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science.	0
17209	17209	S16-13	abstract	4	5	1.0	0.025125628140704	A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations.	0
17405	17405	S16-14	abstract	1	2	0.166666666666667	0.012048192771084	Manually constructed taxonomies provide a crucial resource for many NLP technologies, yet these resources are often limited in their lexical coverage due to their construction procedure.	0
17406	17406	S16-14	abstract	2	3	0.333333333333333	0.018072289156627	While multiple approaches have been proposed to enrich such taxonomies with new concepts, these techniques are typically evaluated by measuring the accuracy at identifying relationships between words, e.g., that a dog is a canine, rather relationships between specific concepts.	0
17407	17407	S16-14	abstract	3	4	0.5	0.024096385542169	Task 14 provides an evaluation framework for automatic taxonomy enrichment techniques by measuring the placement of a new concept into an existing taxonomy: Given a new word and its definition, systems were asked to attach or merge the concept into an existing WordNet concept.	1
17408	17408	S16-14	abstract	4	5	0.666666666666667	0.030120481927711	Five teams submitted 13 systems to the task, all of which were able to improve over the random baseline system.	0
17409	17409	S16-14	abstract	5	6	0.833333333333333	0.036144578313253	However, only one participating system outperformed the second, morecompetitive baseline that attaches a new term to the first word in its gloss with the appropriate part of speech, which indicates that techniques must be adapted to exploit the structure of glosses.	0
17410	17410	S16-14	abstract	6	7	1.0	0.042168674698795	the new synset as a hyponym of S in the Word-Net's subsumption hierarchy.	0
17609	17609	S17-1	Task Overview	1	40	0.125	0.143884892086331	STS is the assessment of pairs of sentences according to their degree of semantic similarity.	0
17610	17610	S17-1	Task Overview	2	41	0.25	0.147482014388489	The task involves producing real-valued similarity scores for sentence pairs.	1
17611	17611	S17-1	Task Overview	3	42	0.375	0.151079136690647	Performance is measured by the Pearson correlation of machine scores with human judgments.	0
17612	17612	S17-1	Task Overview	4	43	0.5	0.154676258992806	The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence.	0
17613	17613	S17-1	Task Overview	5	44	0.625	0.158273381294964	Intermediate values reflect interpretable levels of partial overlap in meaning.	0
17614	17614	S17-1	Task Overview	6	45	0.75	0.161870503597122	The annotation scale is designed to be accessible by reasonable human judges without any formal expertise in linguistics.	0
17615	17615	S17-1	Task Overview	7	46	0.875	0.165467625899281	Using reasonable human interpretations of natural language semantics was popularized by the related textual entailment task (Dagan et al., 2010).	0
17616	17616	S17-1	Task Overview	8	47	1.0	0.169064748201439	The resulting annotations reflect both pragmatic and world knowledge and are more interpretable and useful within downstream systems.	0
17849	17849	S17-2	abstract	1	2	0.166666666666667	0.01025641025641	This paper introduces a new task on Multilingual and Cross-lingual Semantic WordSimilarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish.	1
17850	17850	S17-2	abstract	2	3	0.333333333333333	0.015384615384615	High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark).	0
17851	17851	S17-2	abstract	3	4	0.5	0.020512820512821	These were used for semi-automatic construction of ten cross-lingual datasets.	0
17852	17852	S17-2	abstract	4	5	0.666666666666667	0.025641025641026	17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2.	0
17853	17853	S17-2	abstract	5	6	0.833333333333333	0.030769230769231	Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks.	0
17854	17854	S17-2	abstract	6	7	1.0	0.035897435897436	More information can be found on the task website:	0
18044	18044	S17-3	abstract	1	2	0.166666666666667	0.004950495049505	We describe SemEval2017 Task 3 on Community Question Answering.	0
18045	18045	S17-3	abstract	2	3	0.333333333333333	0.007425742574257	This year, we reran the four subtasks from SemEval-2016: (A) Question-Comment Similarity, (B) Question-Question Similarity, (C) Question-External Comment Similarity, and (D) Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing.	1
18046	18046	S17-3	abstract	3	4	0.5	0.00990099009901	Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums.	1
18047	18047	S17-3	abstract	4	5	0.666666666666667	0.012376237623762	A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks A-D. Unfortunately, no teams participated in subtask E. A variety of approaches and features were used by the participating systems to address the different subtasks.	0
18048	18048	S17-3	abstract	5	6	0.833333333333333	0.014851485148515	The best systems achieved an official score (MAP) of 88.43, 47.22,  15.46, and 61.16  in subtasks A, B, C, and D, respectively.	0
18049	18049	S17-3	abstract	6	7	1.0	0.017326732673267	These scores are better than the baselines, especially for subtasks A-C.	0
18720	18720	S17-5	abstract	1	2	0.2	0.006896551724138	"This paper discusses the ""Fine-Grained Sentiment Analysis on Financial Microblogs and News"" task as part of SemEval-2017, specifically under the ""Detecting sentiment, humour, and truth"" theme."	0
18721	18721	S17-5	abstract	2	3	0.4	0.010344827586207	This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines.	0
18722	18722	S17-5	abstract	3	4	0.6	0.013793103448276	The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks.	1
18723	18723	S17-5	abstract	4	5	0.8	0.017241379310345	The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment.	0
18724	18724	S17-5	abstract	5	6	1.0	0.020689655172414	This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2.	0
19011	19011	S17-6	abstract	1	3	0.166666666666667	0.011904761904762	This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead.	0
19012	19012	S17-6	abstract	2	4	0.333333333333333	0.015873015873016	The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the 'Hashtag Wars' segment of the TV show @midnight.	0
19013	19013	S17-6	abstract	3	5	0.5	0.01984126984127	The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are.	1
19014	19014	S17-6	abstract	4	6	0.666666666666667	0.023809523809524	7 teams participated in subtask A, and 5 teams participated in subtask B.	0
19015	19015	S17-6	abstract	5	7	0.833333333333333	0.027777777777778	The best accuracy in subtask A was 0.675.	0
19016	19016	S17-6	abstract	6	8	1.0	0.031746031746032	The best (lowest) rank edit distance for subtask B was 0.872.	0
19262	19262	S17-7	abstract	1	2	0.2	0.008	A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect.	0
19263	19263	S17-7	abstract	2	3	0.4	0.012	Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-percontext assumption.	0
19264	19264	S17-7	abstract	3	4	0.6	0.016	This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns.	1
19265	19265	S17-7	abstract	4	5	0.8	0.02	We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set.	0
19266	19266	S17-7	abstract	5	6	1.0	0.024	Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.	0
19517	19517	S17-8	Introduction and Motivation	1	7	0.047619047619048	0.045454545454546	Rumours are rife on the web.	0
19518	19518	S17-8	Introduction and Motivation	2	8	0.095238095238095	0.051948051948052	False claims affect people's perceptions of events and their behaviour, sometimes in harmful ways.	0
19519	19519	S17-8	Introduction and Motivation	3	9	0.142857142857143	0.058441558441559	With the increasing reliance on the Web -social media, in particularas a source of information and news updates by individuals, news professionals, and automated systems, the potential disruptive impact of rumours is further accentuated.	0
19520	19520	S17-8	Introduction and Motivation	4	10	0.19047619047619	0.064935064935065	The task of analysing and determining veracity of social media content has been of recent interest to the field of natural language processing.	0
19521	19521	S17-8	Introduction and Motivation	5	11	0.238095238095238	0.071428571428572	After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014;	0
19522	19522	S17-8	Introduction and Motivation	6	12	0.285714285714286	0.077922077922078	Zhang et al., 2015;	0
19523	19523	S17-8	Introduction and Motivation	7	13	0.333333333333333	0.084415584415585	Shao et al., 2016;Zubiaga et al., 2016b).	0
19524	19524	S17-8	Introduction and Motivation	8	14	0.380952380952381	0.090909090909091	Veracity judgment can be decomposed intuitively in terms of a comparison between assertions made in -and entailments from -a candidate text, and external world knowledge.	0
19525	19525	S17-8	Introduction and Motivation	9	15	0.428571428571429	0.097402597402597	Intermediate linguistic cues have also been shown to play a role.	0
19526	19526	S17-8	Introduction and Motivation	10	16	0.476190476190476	0.103896103896104	Critically, based on recent work the task appears deeply nuanced and very challenging, while having important applications in, for example, journalism and disaster mitigation (Hermida, 2012;	0
19527	19527	S17-8	Introduction and Motivation	11	17	0.523809523809524	0.11038961038961	Procter et al., 2013a;	0
19528	19528	S17-8	Introduction and Motivation	12	18	0.571428571428571	0.116883116883117	Veil et al., 2011).	0
19529	19529	S17-8	Introduction and Motivation	13	19	0.619047619047619	0.123376623376623	We propose a shared task where participants analyse rumours in the form of claims made in user-generated content, and where users respond to one another within conversations attempting to resolve the veracity of the rumour.	1
19530	19530	S17-8	Introduction and Motivation	14	20	0.666666666666667	0.12987012987013	"We define a rumour as a ""circulating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient scepticism and/or anxiety so as to motivate finding out the actual truth"" (Zubiaga et al., 2015b)."	0
19531	19531	S17-8	Introduction and Motivation	15	21	0.714285714285714	0.136363636363636	While breaking news unfold, gathering opinions and evidence from as many sources as possible as communities react becomes crucial to determine the veracity of rumours and consequently reduce the impact of the spread of misinformation.	0
19532	19532	S17-8	Introduction and Motivation	16	22	0.761904761904762	0.142857142857143	Within this scenario where one needs to listen to, and assess the testimony of, different sources to make a final decision with respect to a rumour's veracity, we ran a task in SemEval consisting of two subtasks: (a) stance classification towards rumours, and (b) veracity classification.	0
19533	19533	S17-8	Introduction and Motivation	17	23	0.80952380952381	0.149350649350649	Subtask	0
19534	19534	S17-8	Introduction and Motivation	18	24	0.857142857142857	0.155844155844156	A corresponds to the core problem in crowd response analysis when using discourse around claims to verify or disprove them.	0
19535	19535	S17-8	Introduction and Motivation	19	25	0.904761904761905	0.162337662337662	Subtask B corresponds to the AI-hard task of assessing directly whether or not a claim is false.	0
19536	19536	S17-8	Introduction and Motivation	20	26	0.952380952380952	0.168831168831169	determine how other users in social media regard the rumour (Procter et al., 2013b).	0
19537	19537	S17-8	Introduction and Motivation	21	27	1.0	0.175324675324675	We propose to tackle this analysis by looking at the conversation stemming from direct and nested replies to the tweet originating the rumour (source tweet).	0
19665	19665	S17-9	title	1	1	1.0	0.005347593582888	SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation	1
19666	19666	S17-9	abstract	1	2	0.166666666666667	0.010695187165775	In this report we summarize the results of the 2017 AMR SemEval shared task.	0
19667	19667	S17-9	abstract	2	3	0.333333333333333	0.016042780748663	The task consisted of two separate yet related subtasks.	0
19668	19668	S17-9	abstract	3	4	0.5	0.021390374331551	In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain.	0
19669	19669	S17-9	abstract	4	5	0.666666666666667	0.026737967914439	In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain.	0
19670	19670	S17-9	abstract	5	6	0.833333333333333	0.032085561497326	A total of five sites participated in the parsing subtask, and four participated in the generation subtask.	0
19671	19671	S17-9	abstract	6	7	1.0	0.037433155080214	Along with a description of the task and the participants' systems, we show various score ablations and some sample outputs.	0
19854	19854	S17-10	abstract	1	3	0.2	0.016393442622951	We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials.	1
19855	19855	S17-10	abstract	2	4	0.4	0.021857923497268	Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios.	0
19856	19856	S17-10	abstract	3	5	0.6	0.027322404371585	We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.	0
19857	19857	S17-10	abstract	4	6	0.8	0.032786885245902	Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction.	0
19858	19858	S17-10	abstract	5	7	1.0	0.038251366120219	These tasks are related to the tasks of named entity recognition, named entity	0
20036	20036	S17-11	abstract	1	2	0.5	0.01010101010101	This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language.	1
20037	20037	S17-11	abstract	2	3	1.0	0.015151515151515	It aims to support the evaluation and evolution of the discussions surrounding the application natural language processing techniques within the context of end-user natural language programming, under scenarios of high lexical and semantic heterogeneity.	0
20239	20239	S17-12	Introduction	1	7	0.1	0.07	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007(Verhagen et al., , 2010UzZaman et al., 2013).	0
20240	20240	S17-12	Introduction	2	8	0.2	0.08	In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures.	0
20241	20241	S17-12	Introduction	3	9	0.3	0.09	In the Clinical TempEval shared tasks (Bethard et al., 2015(Bethard et al., , 2016, participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations.	1
20242	20242	S17-12	Introduction	4	10	0.4	0.1	For example, Figure 1 shows the annotations that a system is expected to produce when given the text:	0
20243	20243	S17-12	Introduction	5	11	0.5	0.11	April 23, 2014:	0
20244	20244	S17-12	Introduction	6	12	0.6	0.12	The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
20245	20245	S17-12	Introduction	7	13	0.7	0.13	Clinical TempEval 2017 introduced a new aspect to this problem: domain adaptation.	0
20246	20246	S17-12	Introduction	8	14	0.8	0.14	Whereas in Clinical TempEval 2015 and 2016, systems were both trained and tested on notes from colon cancer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients.	0
20247	20247	S17-12	Introduction	9	15	0.9	0.15	The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients.	0
20248	20248	S17-12	Introduction	10	16	1.0	0.16	As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016.	0
20341	20341	S18-1	1 Introduction	1	9	0.023255813953488	0.021176470588235	Emotions are central to language and thought.	0
20342	20342	S18-1	1 Introduction	2	10	0.046511627906977	0.023529411764706	They are familiar and commonplace, yet they are complex and nuanced.	0
20343	20343	S18-1	1 Introduction	3	11	0.069767441860465	0.025882352941177	Humans are known to perceive hundreds of different emotions.	0
20344	20344	S18-1	1 Introduction	4	12	0.093023255813954	0.028235294117647	According to the basic emotion model (aka the categorical model) (Ekman, 1992;	0
20345	20345	S18-1	1 Introduction	5	13	0.116279069767442	0.030588235294118	Plutchik, 1980;Parrot, 2001;Frijda, 1988), some emotions, such as joy, sadness, and fear, are more basic than others-physiologically, cognitively, and in terms of the mechanisms to express these emotions.	0
20346	20346	S18-1	1 Introduction	6	14	0.13953488372093	0.032941176470588	Each of these emotions can be felt or expressed in varying intensities.	0
20347	20347	S18-1	1 Introduction	7	15	0.162790697674419	0.035294117647059	For example, our utterances can convey that we are very angry, slightly sad, absolutely elated, etc.	0
20348	20348	S18-1	1 Introduction	8	16	0.186046511627907	0.037647058823529	Here, intensity refers to the degree or amount of an emotion such as anger or sadness.	0
20349	20349	S18-1	1 Introduction	9	17	0.209302325581395	0.04	1	0
20350	20350	S18-1	1 Introduction	10	18	0.232558139534884	0.042352941176471	As per the valence-arousal-dominance (VAD) model (Russell, 1980(Russell, , 2003, emotions are points in a three-dimensional space of valence (positivenessnegativeness), arousal (active-passive), and dominance (dominant-submissive).	0
20351	20351	S18-1	1 Introduction	11	19	0.255813953488372	0.044705882352941	We use the term affect to refer to various emotion-related categories such as joy, fear, valence, and arousal.	0
20352	20352	S18-1	1 Introduction	12	20	0.27906976744186	0.047058823529412	Natural language applications in commerce, public health, disaster management, and public policy can benefit from knowing the affectual states of people-both the categories and the intensities of the emotions they feel.	0
20353	20353	S18-1	1 Introduction	13	21	0.302325581395349	0.049411764705882	We thus present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks where automatic systems have to infer the affectual state of a person from their tweet.	1
20354	20354	S18-1	1 Introduction	14	22	0.325581395348837	0.051764705882353	2	0
20355	20355	S18-1	1 Introduction	15	23	0.348837209302326	0.054117647058824	We will refer to the author of a tweet as the tweeter.	0
20356	20356	S18-1	1 Introduction	16	24	0.372093023255814	0.056470588235294	Some of the tasks are on the intensities of four basic emotions common to many proposals of basic emotions: anger, fear, joy, and sadness.	0
20357	20357	S18-1	1 Introduction	17	25	0.395348837209302	0.058823529411765	Some of the tasks are on valence or sentiment intensity.	0
20358	20358	S18-1	1 Introduction	18	26	0.418604651162791	0.061176470588235	Finally, we include an emotion classification task over eleven emotions commonly expressed in tweets.	0
20359	20359	S18-1	1 Introduction	19	27	0.441860465116279	0.063529411764706	3 For each task, we provide separate training, development, and test datasets for English, Arabic, and Spanish tweets.	0
20360	20360	S18-1	1 Introduction	20	28	0.465116279069767	0.065882352941177	The tasks are as follows:	0
20361	20361	S18-1	1 Introduction	21	29	0.488372093023256	0.068235294117647	1. Emotion Intensity Regression (EI-reg):	0
20362	20362	S18-1	1 Introduction	22	30	0.511627906976744	0.070588235294118	Given a tweet and an emotion E, determine the intensity of E that best represents the mental state of the tweeter-a real-valued score between 0 (least E) and 1 (most E);	0
20363	20363	S18-1	1 Introduction	23	31	0.534883720930232	0.072941176470588	2. Emotion Intensity Ordinal Classification (EIoc):	0
20364	20364	S18-1	1 Introduction	24	32	0.558139534883721	0.075294117647059	Given a tweet and an emotion E, classify the tweet into one of four ordinal classes of intensity of E that best represents the mental state of the tweeter; 3. Valence (Sentiment) Regression (V-reg):	0
20365	20365	S18-1	1 Introduction	25	33	0.581395348837209	0.077647058823529	Given a tweet, determine the intensity of sentiment or valence (V) that best represents the mental state of the tweeter-a real-valued score between 0 (most negative) and 1 (most positive);	0
20366	20366	S18-1	1 Introduction	26	34	0.604651162790698	0.08	4. Valence Ordinal Classification (V-oc):	0
20367	20367	S18-1	1 Introduction	27	35	0.627906976744186	0.082352941176471	Given a tweet, classify it into one of seven ordinal classes, corresponding to various levels of positive and negative sentiment intensity, that best represents the mental state of the tweeter;	0
20368	20368	S18-1	1 Introduction	28	36	0.651162790697674	0.084705882352941	5. Emotion Classification (E-c):	0
20369	20369	S18-1	1 Introduction	29	37	0.674418604651163	0.087058823529412	Given a tweet, classify it as 'neutral or no emotion' or as one, or more, of eleven given emotions that best represent the mental state of the tweeter.	0
20370	20370	S18-1	1 Introduction	30	38	0.697674418604651	0.089411764705882	Here, E refers to emotion, EI refers to emotion intensity, V refers to valence, reg refers to regression, oc refers to ordinal classification, c refers to classification.	0
20371	20371	S18-1	1 Introduction	31	39	0.72093023255814	0.091764705882353	For each language, we create a large single textual dataset, subsets of which are annotated for many emotion (or affect) dimensions (from both the basic emotion model and the VAD model).	0
20372	20372	S18-1	1 Introduction	32	40	0.744186046511628	0.094117647058824	For each emotion dimension, we annotate the data not just for coarse classes (such as anger or no anger) but also for fine-grained real-valued scores indicating the intensity of emotion.	0
20373	20373	S18-1	1 Introduction	33	41	0.767441860465116	0.096470588235294	We use Best-Worst Scaling (BWS), a comparative annotation method, to address the limitations of traditional rating scale methods such as inter-and intraannotator inconsistency.	0
20374	20374	S18-1	1 Introduction	34	42	0.790697674418605	0.098823529411765	We show that the finegrained intensity scores thus obtained are reliable (repeat annotations lead to similar scores).	0
20375	20375	S18-1	1 Introduction	35	43	0.813953488372093	0.101176470588235	In total, about 700,000 annotations were obtained from about 22,000 English, Arabic, and Spanish tweets.	0
20376	20376	S18-1	1 Introduction	36	44	0.837209302325581	0.103529411764706	Seventy-five teams (about 200 team members) participated in the shared task, making this the largest SemEval shared task to date.	0
20377	20377	S18-1	1 Introduction	37	45	0.86046511627907	0.105882352941176	In total, 319 submissions were made to the 15 task-language pairs.	0
20378	20378	S18-1	1 Introduction	38	46	0.883720930232558	0.108235294117647	Each team was allowed only one official submission for each task-language pair.	0
20379	20379	S18-1	1 Introduction	39	47	0.906976744186046	0.110588235294118	We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful.	0
20380	20380	S18-1	1 Introduction	40	48	0.930232558139535	0.112941176470588	We also analyze system predictions for consistent bias towards a particular race or gender using a corpus specifically compiled for that purpose.	0
20381	20381	S18-1	1 Introduction	41	49	0.953488372093023	0.115294117647059	We find that a majority of systems consistently assign higher scores to sentences involving one race or gender.	0
20382	20382	S18-1	1 Introduction	42	50	0.976744186046512	0.117647058823529	We also find that the bias may change depending on the specific affective dimension being predicted.	0
20383	20383	S18-1	1 Introduction	43	51	1.0	0.12	All of the tweet data (labeled and unlabeled), annotation questionnaires, evaluation scripts, and the bias evaluation corpus are made freely available on the task website.	0
20813	20813	S18-2	Task Description	1	56	0.076923076923077	0.402877697841727	Given a text message including an emoji, the emoji prediction task consists of predicting that emoji by relying exclusively on the textual content of that message.	1
20814	20814	S18-2	Task Description	2	57	0.153846153846154	0.410071942446043	In particular, in this task we focused on the one emoji occurring inside tweets, thus relying on Twitter data.	0
20815	20815	S18-2	Task Description	3	58	0.230769230769231	0.41726618705036	Last hike in our awesome camping weekend!	0
20816	20816	S18-2	Task Description	4	59	0.307692307692308	0.424460431654676	The task is divided into two subtasks respectively dealing with the prediction of the emoji associated to English and Spanish tweets.	0
20817	20817	S18-2	Task Description	5	60	0.384615384615385	0.431654676258993	The motivation for providing a multilingual setting stems from previous findings about the idiosyncrasy of use of emojis across languages (Barbieri et al., 2016b) (see Figure 3): one emoji may be used with completely different meanings depending not only on the language of the speaker, but also on regional dialects (Barbieri et al., 2016a).	0
20818	20818	S18-2	Task Description	6	61	0.461538461538462	0.438848920863309	For each subtask we selected the tweets that included one of the twenty emojis that occur most frequently in the Twitter data we collected (Table 1).	0
20819	20819	S18-2	Task Description	7	62	0.538461538461538	0.446043165467626	Therefore, the task can be viewed as a multilabel classification problem with twenty labels.	0
20820	20820	S18-2	Task Description	8	63	0.615384615384615	0.453237410071943	Twitter datasets were shared among participants by providing a list of tweet IDs 4 or directly the 4 Participants were provided with a Javabased crawler (https://github.com/fra82/ twitter-crawler) to ease the download of the textual	0
20821	20821	S18-2	Task Description	9	64	0.692307692307692	0.460431654676259	It's flipping hot out here!	0
20822	20822	S18-2	Task Description	10	65	0.769230769230769	0.467625899280576	Iniciamos el nuevo ao con ilusin!	0
20823	20823	S18-2	Task Description	11	66	0.846153846153846	0.474820143884892	Figure 3: Example of distinct use of the fire emoji across languages: the first tweet (English) comments on the torrid weather, while the second one (Spanish) exploits the same emoji to wish an happy new year ('We start the new year with enthusiasm!').	0
20824	20824	S18-2	Task Description	12	67	0.923076923076923	0.482014388489209	text of each tweet.	0
20825	20825	S18-2	Task Description	13	68	1.0	0.489208633093525	The last approach was adopted to share the test sets (more details are provided in Section 4).	0
20904	20904	S18-3	Introduction	1	8	0.033333333333333	0.041666666666667	The development of the social web has stimulated the use of figurative and creative language, including irony, in public (Ghosh et al., 2015).	0
20905	20905	S18-3	Introduction	2	9	0.066666666666667	0.046875	From a philosophical/psychological perspective, discerning the mechanisms that underlie ironic speech improves our understanding of human reasoning and communication, and more and more, this interest in understanding irony also emerges in the machine learning community (Wallace, 2015).	0
20906	20906	S18-3	Introduction	3	10	0.1	0.052083333333333	Although an unanimous definition of irony is still lacking in the literature, it is often identified as a trope whose actual meaning differs from what is literally enunciated.	0
20907	20907	S18-3	Introduction	4	11	0.133333333333333	0.057291666666667	Due to its nature, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce human language.	0
20908	20908	S18-3	Introduction	5	12	0.166666666666667	0.0625	In fact, automatic irony detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis.	0
20909	20909	S18-3	Introduction	6	13	0.2	0.067708333333333	Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)).	0
20910	20910	S18-3	Introduction	7	14	0.233333333333333	0.072916666666667	However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets.	0
20911	20911	S18-3	Introduction	8	15	0.266666666666667	0.078125	The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.	0
20912	20912	S18-3	Introduction	9	16	0.3	0.083333333333333	In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012;	0
20913	20913	S18-3	Introduction	10	17	0.333333333333333	0.088541666666667	Maynard and Greenwood, 2014;	0
20914	20914	S18-3	Introduction	11	18	0.366666666666667	0.09375	Ghosh and Veale, 2016).	0
20915	20915	S18-3	Introduction	12	19	0.4	0.098958333333333	Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge.	0
20916	20916	S18-3	Introduction	13	20	0.433333333333333	0.104166666666667	Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative.	0
20917	20917	S18-3	Introduction	14	21	0.466666666666667	0.109375	(1) I feel so blessed to get ocular migraines.	0
20918	20918	S18-3	Introduction	15	22	0.5	0.114583333333333	(2)	0
20919	20919	S18-3	Introduction	16	23	0.533333333333333	0.119791666666667	Go ahead drop me hate, I'm looking forward to it.	0
20920	20920	S18-3	Introduction	17	24	0.566666666666667	0.125	"For human readers, it is clear that the author of example 1 does not feel blessed at all, which can be inferred from the contrast between the positive sentiment expression ""I feel so blessed"", and the negative connotation associated with getting ocular migraines."	0
20921	20921	S18-3	Introduction	18	25	0.6	0.130208333333333	Although such connotative infor-mation is easily understood by most people, it is difficult to access by machines.	0
20922	20922	S18-3	Introduction	19	26	0.633333333333333	0.135416666666667	Example 2 illustrates implicit cyberbullying; instances that typically lack explicit profane words and where the offense is often made through irony.	0
20923	20923	S18-3	Introduction	20	27	0.666666666666667	0.140625	"Similarly to example 1, a contrast can be perceived between a positive statement (""I'm looking forward to"") and a negative situation (i.e. experiencing hate)."	0
20924	20924	S18-3	Introduction	21	28	0.7	0.145833333333333	To be able to interpret the above examples correctly, machines need, similarly to humans, to be aware that irony is used, and that the intended sentiment is opposite to what is literally enunciated.	0
20925	20925	S18-3	Introduction	22	29	0.733333333333333	0.151041666666667	The irony detection task 1 we propose is formulated as follows: given a single post (i.e. a tweet), participants are challenged to automatically determine whether irony is used and which type of irony is expressed.	1
20926	20926	S18-3	Introduction	23	30	0.766666666666667	0.15625	We thus defined two subtasks:	0
20927	20927	S18-3	Introduction	24	31	0.8	0.161458333333333		0
20928	20928	S18-3	Introduction	25	32	0.833333333333333	0.166666666666667	Task A describes a binary irony classification task to define, for a given tweet, whether irony is expressed.	0
20929	20929	S18-3	Introduction	26	33	0.866666666666667	0.171875		0
20930	20930	S18-3	Introduction	27	34	0.9	0.177083333333333	Task B describes a multiclass irony classification task to define whether it contains a specific type of irony (verbal irony by means of a polarity clash, situational irony, or another type of verbal irony, see further) or is not ironic.	0
20931	20931	S18-3	Introduction	28	35	0.933333333333333	0.182291666666667	Concretely, participants should define which one out of four categories a tweet contains: ironic by clash, situational irony, other verbal irony or not ironic.	0
20932	20932	S18-3	Introduction	29	36	0.966666666666667	0.1875	It is important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location).	0
20933	20933	S18-3	Introduction	30	37	1.0	0.192708333333333	Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised.	0
21120	21120	S18-4	Task Description	1	32	0.1	0.187134502923977	Let a mention be a nominal that refers to a singular or a collective entity (e.g., she, mom, Judy), and an entity be the actual person that the mention refers to.	0
21121	21121	S18-4	Task Description	2	33	0.2	0.192982456140351	Given a dialogue transcribed in text where all mentions are detected, the objective is to find the entity for each mention, who can be either active or passive in the dialogue.	1
21122	21122	S18-4	Task Description	3	34	0.3	0.198830409356725	In Figure 1, entities such as Ross, Monica, and Joey are the active speakers of the dialogue, whereas Jack and Judy are not although they are passively mentioned as mom and dad in this context.	0
21123	21123	S18-4	Task Description	4	35	0.4	0.204678362573099	Linking such mentions to their global entities demands inferred knowledge about the kinship from other dialogues, challenging crossdocument resolution.	0
21124	21124	S18-4	Task Description	5	36	0.5	0.210526315789474	Thus, character identification can be viewed as an entity linking task that aims for holistic understanding in multiparty dialogue.	0
21125	21125	S18-4	Task Description	6	37	0.6	0.216374269005848	Most of previous works on entity linking have focused on Wikification, which links named entity mentions to their relevant Wikipedia articles (Mihalcea and Csomai, 2007;	0
21126	21126	S18-4	Task Description	7	38	0.7	0.222222222222222	Ratinov et al., 2011;Guo et al., 2013).	0
21127	21127	S18-4	Task Description	8	39	0.8	0.228070175438596	Unlike Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom-piled information, which makes this task even more challenging.	0
21128	21128	S18-4	Task Description	9	40	0.9	0.233918128654971	It is similar to coreference resolution in a sense that it groups mentions into entities, but distinguished because this task requires to identify each mention group as a known person.	0
21129	21129	S18-4	Task Description	10	41	1.0	0.239766081871345	In Figure 1, coreference resolution would give a cluster of the four mentions, {mom, woman, I, I}; however, it would not identify that cluster to be the entity Judy, which in this case is not possible to identify without getting contexts from other dialogues.	0
21261	21261	S18-5	abstract	1	2	0.125	0.008771929824561	This paper discusses SemEval-2018	0
21262	21262	S18-5	abstract	2	3	0.25	0.013157894736842	Task 5: a referential quantification task of counting events and participants in local, long-tail news documents with high ambiguity.	1
21263	21263	S18-5	abstract	3	4	0.375	0.017543859649123	The complexity of this task challenges systems to establish the meaning, reference and identity across documents.	0
21264	21264	S18-5	abstract	4	5	0.5	0.021929824561404	The task consists of three subtasks and spans across three domains.	0
21265	21265	S18-5	abstract	5	6	0.625	0.026315789473684	We detail the design of this referential quantification task, describe the participating systems, and present additional analysis to gain deeper insight into their performance.	0
21266	21266	S18-5	abstract	6	7	0.75	0.030701754385965	70 Answer: 3 Question:	0
21267	21267	S18-5	abstract	7	8	0.875	0.035087719298246	How many killing incidents happened in 2016 in Columbus, Mississippi?	0
21268	21268	S18-5	abstract	8	9	1.0	0.039473684210526	Mississippi boy killed in gun accident Shooting suspect charged with domestic aggravated assault NEWLYWED ACCUSED OF SHOOTING NEW BRIDE Columbus Police investigating early morning shooting High Winds Play Role in 2-Alarm District Heights Apartment Fire input documents	0
21516	21516	S18-6	Tasks	1	29	0.03448275862069	0.177914110429448	The ultimate goal of the shared task is to interpret time expressions, identifying appropriate intervals that can be placed on a timeline.	1
21517	21517	S18-6	Tasks	2	30	0.068965517241379	0.184049079754601	Given a document, a system must identify the time entities by detecting the spans of characters and labeling them with the proper SCATE type.	1
21518	21518	S18-6	Tasks	3	31	0.103448275862069	0.190184049079755	Examples of time entities and their corresponding types in Figure 1 would be (6, DAY-OF-MONTH), (Saturday, DAY-OF-WEEK), (March, MONTH-OF-YEAR) or (since, BETWEEN).	0
21519	21519	S18-6	Tasks	4	32	0.137931034482759	0.196319018404908	Besides the time entities explicitly expressed in the text, implicit occurrences must also be identified, like the THIS and LAST time entities in Figure 1 that do not have any explicit triggers in the text.	0
21520	21520	S18-6	Tasks	5	33	0.172413793103448	0.202453987730061	Once time entities have been identified, they should be linked together using the relations described in the SCATE schema.	0
21521	21521	S18-6	Tasks	6	34	0.206896551724138	0.208588957055215	Following with the example in Figure 1, the time entity 6 should be linked as a SUB-INTERVAL of March, Saturday should be a REPEATING-INTERVAL of the time entity other, and so on.	0
21522	21522	S18-6	Tasks	7	35	0.241379310344828	0.214723926380368	Finally, all the time entities must be completed with some additional properties needed for their interpretation.	0
21523	21523	S18-6	Tasks	8	36	0.275862068965517	0.220858895705521	For example, the time entity other should have a VALUE of 2, the END-INTERVAL of since is the Document Creation Time, etc.	0
21524	21524	S18-6	Tasks	9	37	0.310344827586207	0.226993865030675	Once again, the properties required by each time entity type are defined by the SCATE schema.	0
21525	21525	S18-6	Tasks	10	38	0.344827586206897	0.233128834355828	1 Every resulting graph, composed of a set of linked time entities, represents a time expression that can be semantically interpreted.	0
21526	21526	S18-6	Tasks	11	39	0.379310344827586	0.239263803680982	For this purpose, we provide a Scala library 2 that reads the graphs in Anafora XML format (Chen and Styler, 2013) and converts them into intervals on the timeline.	0
21527	21527	S18-6	Tasks	12	40	0.413793103448276	0.245398773006135	An example of interpreting the time entities corresponding to the expression every Saturday since March 6 relative to an anchor time of April 21, 2017 is given in Figure 2.	0
21528	21528	S18-6	Tasks	13	41	0.448275862068966	0.251533742331288	In this example, the values today and result store the entities that represent the time expressions April 21, 2017 and every Saturday since March 6 respectively.	0
21529	21529	S18-6	Tasks	14	42	0.482758620689655	0.257668711656442	The Scala command on the right side interprets the latter and produces the corresponding time intervals.	0
21530	21530	S18-6	Tasks	15	43	0.517241379310345	0.263803680981595	The task includes two evaluation methods, one for the parsing step, i.e. time entity identification scala&gt; for (Interval(start, end) &lt;-result.intervals) | println(start, end) (2017-03-11T00:00,2017-03-12T00:00) (2017-03-18T00:00,2017-03-19T00:00) (2017-03-25T00:00,2017-03-26T00:00) (2017-04-01T00:00,2017-04-02T00:00) (2017-04-08T00:00,2017-04-09T00:00) (2017-04-15T00:00,2017-04-16T00:00) (2017-04-22T00:00,2017-04-23T00:00) ... and linking, and one to score the resulting time intervals.	0
21531	21531	S18-6	Tasks	16	44	0.551724137931034	0.269938650306748	For the later, we only consider time expressions that yield a finite set of bounded intervals, for example, last Monday.	0
21532	21532	S18-6	Tasks	17	45	0.586206896551724	0.276073619631902	Time expressions that refer to an infinite set of intervals, like every month, are not considered in the interval-based part of the evaluation.	0
21533	21533	S18-6	Tasks	18	46	0.620689655172414	0.282208588957055	Participants only need to produce Anafora outputs with parsed time entities; the interpretation is carried out by the evaluation system.	0
21534	21534	S18-6	Tasks	19	47	0.655172413793103	0.288343558282209	The evaluation system is also able to obtain the intervals from timestamps in TimeML format.	0
21535	21535	S18-6	Tasks	20	48	0.689655172413793	0.294478527607362	Thus, systems can be evaluated by both methods or just by the interval-based one, depending on the output format.	0
21536	21536	S18-6	Tasks	21	49	0.724137931034483	0.300613496932515	In summary, the tasks offers two tracks:	0
21537	21537	S18-6	Tasks	22	50	0.758620689655172	0.306748466257669	Track 1: Parse text to time entities.	0
21538	21538	S18-6	Tasks	23	51	0.793103448275862	0.312883435582822	Systems must identify time entities in text and link them correctly to signal how they have to be composed.	0
21539	21539	S18-6	Tasks	24	52	0.827586206896552	0.319018404907975	The output must be given in Anafora format.	0
21540	21540	S18-6	Tasks	25	53	0.862068965517241	0.325153374233129	In this track, all time entities and relations of every time expression are evaluated.	0
21541	21541	S18-6	Tasks	26	54	0.896551724137931	0.331288343558282	Track 2: Produce time intervals.	0
21542	21542	S18-6	Tasks	27	55	0.931034482758621	0.337423312883436	Systems can participate through Track 1 or by providing TimeML annotations.	0
21543	21543	S18-6	Tasks	28	56	0.96551724137931	0.343558282208589	In both cases, the intervals are inferred by our interpreter.	0
21544	21544	S18-6	Tasks	29	57	1.0	0.349693251533742	In this track, only bounded time intervals are scored.	0
21657	21657	S18-7	Introduction	1	7	0.1	0.039325842696629	One of the emerging trends of natural language technologies is their use for the humanities and sciences.	0
21658	21658	S18-7	Introduction	2	8	0.2	0.044943820224719	Recent works in the semantic web (Osborne and Motta, 2015;	0
21659	21659	S18-7	Introduction	3	9	0.3	0.050561797752809	Wolfram, 2016) and natural language processing (Tsai et al., 2013;	0
21660	21660	S18-7	Introduction	4	10	0.4	0.056179775280899	Luan et al., 2017;	0
21661	21661	S18-7	Introduction	5	11	0.5	0.061797752808989	Augenstein and Sgaard, 2017; aimed to improve the access to scientific literature, and in particular to respond to information needs that are currently beyond the capabilities of standard search engines.	0
21662	21662	S18-7	Introduction	6	12	0.6	0.067415730337079	Such queries include finding all papers that address a problem in a specific way, or discovering the roots of a certain idea.	0
21663	21663	S18-7	Introduction	7	13	0.7	0.073033707865169	This ambition involves the identification and classification of concepts, and the relations connecting them.	0
21664	21664	S18-7	Introduction	8	14	0.8	0.078651685393259	The purpose of the task is to automatically identify relevant domain-specific semantic relations in a corpus of scientific publications.	1
21665	21665	S18-7	Introduction	9	15	0.9	0.084269662921348	"In particular, we search for and classify relations that provide snippets of information such as ""a (new) method is proposed for a task"", or ""a phenomenon is found in a certain context"", or ""results of different experiments are compared to each other""."	0
21666	21666	S18-7	Introduction	10	16	1.0	0.089887640449438	Identifying such semantic relations between domain-specific concepts allows us to detect research papers which deal with the same problem, or to track the evolution of results on a certain task.	0
21830	21830	S18-8	abstract	1	2	0.25	0.008333333333333	This paper describes the SemEval 2018 shared task on semantic extraction from cybersecurity reports, which is introduced for the first time as a shared task on SemEval.	0
21831	21831	S18-8	abstract	2	3	0.5	0.0125	This task comprises four SubTasks done incrementally to predict the characteristics of a specific malware using cybersecurity reports.	1
21832	21832	S18-8	abstract	3	4	0.75	0.016666666666667	To the best of our knowledge, we introduce the world's largest publicly available dataset of annotated malware reports in this task.	0
21833	21833	S18-8	abstract	4	5	1.0	0.020833333333333	This task received in total 18 submissions from 9 participating teams.	0
22070	22070	S18-9	abstract	1	2	0.142857142857143	0.009049773755656	This paper describes the SemEval 2018 Shared Task on Hypernym Discovery.	0
22071	22071	S18-9	abstract	2	3	0.285714285714286	0.013574660633484	We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input.	0
22072	22072	S18-9	abstract	3	4	0.428571428571429	0.018099547511312	Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus.	1
22073	22073	S18-9	abstract	4	5	0.571428571428571	0.02262443438914	We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music).	0
22074	22074	S18-9	abstract	5	6	0.714285714285714	0.027149321266968	Participants were allowed to compete in any or all of the subtasks.	0
22075	22075	S18-9	abstract	6	7	0.857142857142857	0.031674208144796	Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks.	0
22076	22076	S18-9	abstract	7	8	1.0	0.036199095022625	Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119.	0
22301	22301	S18-10	Task description	1	12	0.0625	0.072289156626506	A semantic model that has only been evaluated on similarity detection may very well fail to be of practical use for specific applications.	0
22302	22302	S18-10	Task description	2	13	0.125	0.078313253012048	For example, word sense disambiguation could benefit greatly from representations that can model complex semantic relations.	0
22303	22303	S18-10	Task description	3	14	0.1875	0.08433734939759	This means that the evaluation of word representation models should not only be centered on semantic similarity and relatedness, and should include different, complementary tasks.	0
22304	22304	S18-10	Task description	4	15	0.25	0.090361445783133	To fill this gap, we proposed a novel task of semantic difference detection as Task 10 of the SemEval 2018 workshop.	0
22305	22305	S18-10	Task description	5	16	0.3125	0.096385542168675	The goal of the systems in this case was to predict whether a word is a discriminative attribute between two other words.	1
22306	22306	S18-10	Task description	6	17	0.375	0.102409638554217	For example, given the words apple and banana, is the word red a discriminative attribute?	0
22307	22307	S18-10	Task description	7	18	0.4375	0.108433734939759	Semantic difference is a ternary relation between two concepts (apple, banana) and a discriminative attribute (red) that characterizes the first concept but not the other.	0
22308	22308	S18-10	Task description	8	19	0.5	0.114457831325301	By its nature, semantic difference detection is a binary classification task: given a triple apple,banana,red, the task is to determine whether it exemplifies a semantic difference or not.	0
22309	22309	S18-10	Task description	9	20	0.5625	0.120481927710843	In practice, when preparing the task, we started out with defining potential discriminative attributes as semantic features in the sense of (McRae et al., 2005): properties that people tend to think are important for a given concept.	0
22310	22310	S18-10	Task description	10	21	0.625	0.126506024096386	McRae et al.'s features are expressed as phrases, but these phrases can usually be reconstructed from a single word (e.g. red as a feature of apple stands for the phrase is red, carpentry as a feature of hammer can be used as a shorthand of used for carpentry, etc.)	0
22311	22311	S18-10	Task description	11	22	0.6875	0.132530120481928	Given this general reconstructability, we have for simplicity used single words rather than phrases to represent features.	0
22312	22312	S18-10	Task description	12	23	0.75	0.13855421686747	The same solution was also adopted in the feature norming studies by (Vinson and Vigliocco, 2008) and (Lenci et al., 2013).	0
22313	22313	S18-10	Task description	13	24	0.8125	0.144578313253012	Following McRae et al., we did not define discriminative features in purely logical but rather in psychological terms.	0
22314	22314	S18-10	Task description	14	25	0.875	0.150602409638554	Accordingly, features are prototypical properties that subjects tend to associate to a certain concept.	0
22315	22315	S18-10	Task description	15	26	0.9375	0.156626506024096	For example, not all apples are red and some bananas are, but red tends to be judged as an important feature of apples and not of bananas.	0
22316	22316	S18-10	Task description	16	27	1.0	0.162650602409639	We therefore fully trust human anno-tators in deciding what counts as a distinguishing attribute and what does not.	0
22462	22462	S18-11	Introduction	1	7	0.015873015873016	0.026819923371648	Developing algorithms for understanding natural language is not trivial.	0
22463	22463	S18-11	Introduction	2	8	0.031746031746032	0.030651340996169	Natural language comes with its own complexity and inherent ambiguities.	0
22464	22464	S18-11	Introduction	3	9	0.047619047619048	0.03448275862069	Ambiguities can occur, for example, at the level of word meaning, syntactic structure, or semantic interpretation.	0
22465	22465	S18-11	Introduction	4	10	0.063492063492064	0.038314176245211	Traditionally, Natural Language Understanding (NLU) systems have resolved ambiguities using information from the textual context (e.g. neighboring words and sentences), for example via distributional methods (Lenci, 2008).	0
22466	22466	S18-11	Introduction	5	11	0.079365079365079	0.042145593869732	However, many times context may be absent or may lack sufficient information to resolve the ambiguity.	0
22467	22467	S18-11	Introduction	6	12	0.095238095238095	0.045977011494253	In such cases, it would be beneficial to include commonsense knowledge about the world in an NLU system.	0
22468	22468	S18-11	Introduction	7	13	0.111111111111111	0.049808429118774	For example, consider example (1).	0
22469	22469	S18-11	Introduction	8	14	0.126984126984127	0.053639846743295	(1) The waitress brought Rachel's order.	0
22470	22470	S18-11	Introduction	9	15	0.142857142857143	0.057471264367816	She ate the food with great pleasure.	0
22471	22471	S18-11	Introduction	10	16	0.158730158730159	0.061302681992337	Looking at the example in isolation, the person eating the food could be either Rachel or the waitress.	0
22472	22472	S18-11	Introduction	11	17	0.174603174603175	0.065134099616858	Using commonsense knowledge, or, more specifically, script knowledge about the RESTAU-RANT scenario, helps to resolve the referent of the pronoun: Rachel ordered the food.	0
22473	22473	S18-11	Introduction	12	18	0.19047619047619	0.068965517241379	The person who orders the food is the customer.	0
22474	22474	S18-11	Introduction	13	19	0.206349206349206	0.0727969348659	So Rachel should eat the food, she thus refers to Rachel.	0
22475	22475	S18-11	Introduction	14	20	0.222222222222222	0.076628352490422	This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems.	0
22476	22476	S18-11	Introduction	15	21	0.238095238095238	0.080459770114943	In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts.	0
22477	22477	S18-11	Introduction	16	22	0.253968253968254	0.084291187739464	Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc.	0
22478	22478	S18-11	Introduction	17	23	0.26984126984127	0.088122605363985	(Schank and Abelson, 1975).	0
22479	22479	S18-11	Introduction	18	24	0.285714285714286	0.091954022988506	The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979;	0
22480	22480	S18-11	Introduction	19	25	0.301587301587302	0.095785440613027	Schank, 1982;Modi et al., 2017).	0
22481	22481	S18-11	Introduction	20	26	0.317460317460317	0.099616858237548	From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993).	0
22482	22482	S18-11	Introduction	21	27	0.333333333333333	0.103448275862069	Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers.	0
22483	22483	S18-11	Introduction	22	28	0.349206349206349	0.10727969348659	On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender.	0
22484	22484	S18-11	Introduction	23	29	0.365079365079365	0.111111111111111	Because of this implicitness, learning script knowledge from texts is very challenging.	0
22485	22485	S18-11	Introduction	24	30	0.380952380952381	0.114942528735632	There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge.	0
22486	22486	S18-11	Introduction	25	31	0.396825396825397	0.118773946360153	An example is the InScript , which contains short and simple narratives, that very explicitly mention script events and participants.	0
22487	22487	S18-11	Introduction	26	32	0.412698412698413	0.122605363984674	The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario.	0
22488	22488	S18-11	Introduction	27	33	0.428571428571429	0.126436781609195	In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010;	0
22489	22489	S18-11	Introduction	28	34	0.444444444444444	0.130268199233716	Wanzare et al., 2017), event prediction (namely, the narrative cloze task)	0
22490	22490	S18-11	Introduction	29	35	0.46031746031746	0.134099616858238	Jurafsky, 2008, 2009;	0
22491	22491	S18-11	Introduction	30	36	0.476190476190476	0.137931034482759	Rudinger et al., 2015b;Modi, 2016) or story completion (e.g. the story cloze task T	0
22492	22492	S18-11	Introduction	31	37	0.492063492063492	0.14176245210728	It was a long day at work and I decided to stop at the gym before going home.	0
22493	22493	S18-11	Introduction	32	38	0.507936507936508	0.145593869731801	I ran on the treadmill and lifted some weights.	0
22494	22494	S18-11	Introduction	33	39	0.523809523809524	0.149425287356322	I decided I would also swim a few laps in the pool.	0
22495	22495	S18-11	Introduction	34	40	0.53968253968254	0.153256704980843	Once I was done working out, I went in the locker room and stripped down and wrapped myself in a towel.	0
22496	22496	S18-11	Introduction	35	41	0.555555555555556	0.157088122605364	I went into the sauna and turned on the heat.	0
22497	22497	S18-11	Introduction	36	42	0.571428571428571	0.160919540229885	I let it get nice and steamy.	0
22498	22498	S18-11	Introduction	37	43	0.587301587301587	0.164750957854406	I sat down and relaxed.	0
22499	22499	S18-11	Introduction	38	44	0.603174603174603	0.168582375478927	I let my mind think about nothing but peaceful, happy thoughts.	0
22500	22500	S18-11	Introduction	39	45	0.619047619047619	0.172413793103448	I stayed in there for only about ten minutes because it was so hot and steamy.	0
22501	22501	S18-11	Introduction	40	46	0.634920634920635	0.176245210727969	When I got out, I turned the sauna off to save energy and took a cool shower.	0
22502	22502	S18-11	Introduction	41	47	0.650793650793651	0.18007662835249	I got out of the shower and dried off.	0
22503	22503	S18-11	Introduction	42	48	0.666666666666667	0.183908045977011	After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home.	0
22504	22504	S18-11	Introduction	43	49	0.682539682539683	0.187739463601533	Q1	0
22505	22505	S18-11	Introduction	44	50	0.698412698412698	0.191570881226054	Where did they sit inside the sauna?	0
22506	22506	S18-11	Introduction	45	51	0.714285714285714	0.195402298850575	a. on the floor b. on a bench Q2 How long did they stay in the sauna?	0
22507	22507	S18-11	Introduction	46	52	0.73015873015873	0.199233716475096	a. about ten minutes b. over thirty minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions.	0
22508	22508	S18-11	Introduction	47	53	0.746031746031746	0.203065134099617	(Mostafazadeh et al., 2016)).	0
22509	22509	S18-11	Introduction	48	54	0.761904761904762	0.206896551724138	These tasks test a system's ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks.	0
22510	22510	S18-11	Introduction	49	55	0.777777777777778	0.210727969348659	Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension.	0
22511	22511	S18-11	Introduction	50	56	0.793650793650794	0.21455938697318	The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text.	1
22512	22512	S18-11	Introduction	51	57	0.80952380952381	0.218390804597701	In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text.	1
22513	22513	S18-11	Introduction	52	58	0.825396825396825	0.222222222222222	In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts.	1
22514	22514	S18-11	Introduction	53	59	0.841269841269841	0.226053639846743	For example, consider the short narrative in (1).	0
22515	22515	S18-11	Introduction	54	60	0.857142857142857	0.229885057471264	For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text:	0
22516	22516	S18-11	Introduction	55	61	0.873015873015873	0.233716475095785	Usually, people sit on benches inside a sauna, an information that is not given in the text.	0
22517	22517	S18-11	Introduction	56	62	0.888888888888889	0.237547892720307	The dataset also comprises questions that can just be answered from the text, as the second question:	0
22518	22518	S18-11	Introduction	57	63	0.904761904761905	0.241379310344828	The information about the duration of the stay is given literally in the text.	0
22519	22519	S18-11	Introduction	58	64	0.920634920634921	0.245210727969349	The paper is organized as follows:	0
22520	22520	S18-11	Introduction	59	65	0.936507936507936	0.24904214559387	In Section 2, we give an overview of other machine comprehension datasets.	0
22521	22521	S18-11	Introduction	60	66	0.952380952380952	0.252873563218391	In Section 3, we describe the dataset used for our shared task.	0
22522	22522	S18-11	Introduction	61	67	0.968253968253968	0.256704980842912	Section 4.2 gives details about the setup of our task.	0
22523	22523	S18-11	Introduction	62	68	0.984126984126984	0.260536398467433	In Section 5, information about participating systems is given.	0
22524	22524	S18-11	Introduction	63	69	1.0	0.264367816091954	Results are presented and discussed in Sections 6 and 8, respectively.	0
22718	22718	S18-12	abstract	1	2	0.111111111111111	0.007380073800738	A natural language argument is composed of a claim as well as reasons given as premises for the claim.	0
22719	22719	S18-12	abstract	2	3	0.222222222222222	0.011070110701107	The warrant explaining the reasoning is usually left implicit, as it is clear from the context and common sense.	0
22720	22720	S18-12	abstract	3	4	0.333333333333333	0.014760147601476	This makes a comprehension of arguments easy for humans but hard for machines.	0
22721	22721	S18-12	abstract	4	5	0.444444444444444	0.018450184501845	This paper summarizes the first shared task on argument reasoning comprehension.	0
22722	22722	S18-12	abstract	5	6	0.555555555555556	0.022140221402214	Given a premise and a claim along with some topic information, the goal is to automatically identify the correct warrant among two candidates that are plausible and lexically close, but in fact imply opposite claims.	1
22723	22723	S18-12	abstract	6	7	0.666666666666667	0.025830258302583	We describe the dataset with 1970 instances that we built for the task, and we outline the 21 computational approaches that participated, most of which used neural networks.	0
22724	22724	S18-12	abstract	7	8	0.777777777777778	0.029520295202952	The results reveal the complexity of the task, with many approaches hardly improving over the random accuracy of  0.5.	0
22725	22725	S18-12	abstract	8	9	0.888888888888889	0.033210332103321	Still, the best observed accuracy (0.712) underlines the principle feasibility of identifying warrants.	0
22726	22726	S18-12	abstract	9	10	1.0	0.03690036900369	Our analysis indicates that an inclusion of external knowledge is key to reasoning comprehension.	0
22988	22988	S19-1	title	1	1	1.0	0.005291005291005	SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA	1
22989	22989	S19-1	abstract	1	2	0.2	0.010582010582011	We present the SemEval 2019 shared task on Universal Conceptual Cognitive Annotation (UCCA) parsing in English, German and French, and discuss the participating systems and results.	0
22990	22990	S19-1	abstract	2	3	0.4	0.015873015873016	UCCA is a crosslinguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation.	0
22991	22991	S19-1	abstract	3	4	0.6	0.021164021164021	UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units.	0
22992	22992	S19-1	abstract	4	5	0.8	0.026455026455027	The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings.	0
22993	22993	S19-1	abstract	5	6	1.0	0.031746031746032	Full results can be found in the task's website https://competitions. codalab.org/competitions/19160.	0
23445	23445	S19-3	abstract	1	2	0.1	0.012345679012346	In this paper, we present the SemEval-2019 Task 3 -EmoContext: Contextual Emotion Detection in Text.	0
23446	23446	S19-3	abstract	2	3	0.2	0.018518518518519	Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem.	0
23447	23447	S19-3	abstract	3	4	0.3	0.024691358024691	"For instance, as humans, on reading ""Why don't you ever text me!"" we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines."	0
23448	23448	S19-3	abstract	4	5	0.4	0.030864197530864	However, the context of dialogue can prove helpful in detection of the emotion.	0
23449	23449	S19-3	abstract	5	6	0.5	0.037037037037037	In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes -Happy, Sad, Angry and Others.	1
23450	23450	S19-3	abstract	6	7	0.6	0.04320987654321	To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps.	0
23451	23451	S19-3	abstract	7	8	0.7	0.049382716049383	A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants.	0
23452	23452	S19-3	abstract	8	9	0.8	0.055555555555556	A total of 311 teams made submissions to this task.	0
23453	23453	S19-3	abstract	9	10	0.9	0.061728395061728	The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score.	0
23454	23454	S19-3	abstract	10	11	1.0	0.067901234567901	Our analysis of systems submitted to the task indicate that Bi-directional LSTM was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class.	0
23632	23632	S19-4	Task Definition	1	27	0.2	0.108433734939759	We define hyperpartisan news detection as follows:	0
23633	23633	S19-4	Task Definition	2	28	0.4	0.112449799196787	Given the text and markup of an online news article, decide whether the article is hyperpartisan or not.	1
23634	23634	S19-4	Task Definition	3	29	0.6	0.116465863453815	Hyperpartisan articles mimic the form of regular news articles, but are one-sided in the sense that opposing views are either ignored or fiercely attacked.	0
23635	23635	S19-4	Task Definition	4	30	0.8	0.120481927710843	We deliberately disregard the distinction between left and right, since previous work has found that, in hyperpartisan form, both are more similar to each other in terms of style than either are to the mainstream (Potthast et al., 2018c).	0
23636	23636	S19-4	Task Definition	5	31	1.0	0.124497991967871	The challenge of this task is to unveil the mimicking and to detect the hyperpartisan language, which may be distinguishable from regular news at the levels of style, syntax, semantics, and pragmatics.	0
23862	23862	S19-5	Introduction	1	8	0.043478260869565	0.038461538461539	Hate Speech (HS) is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000).	0
23863	23863	S19-5	Introduction	2	9	0.086956521739131	0.043269230769231	Given the huge amount of user-generated contents on the Web, and in particular on social media, the problem of detecting, and therefore possibly contrasting the HS diffusion, is becoming fundamental, for instance for fighting against misogyny and xenophobia.	0
23864	23864	S19-5	Introduction	3	10	0.130434782608696	0.048076923076923	Some key aspects feature online HS, such as virality, or presumed anonymity, which distinguish it from offline communication and make it potentially also more dangerous and hurtful.	0
23865	23865	S19-5	Introduction	4	11	0.173913043478261	0.052884615384615	Often hate speech fosters discrimination against particular categories and undermines equality, an everlasting issue for each civil society.	0
23866	23866	S19-5	Introduction	5	12	0.217391304347826	0.057692307692308	Among the mainly targeted categories there are immigrants and women.	0
23867	23867	S19-5	Introduction	6	13	0.260869565217391	0.0625	For the first target, especially raised by refugee crisis and political changes occurred in the last few years, several governments and policy makers are currently trying to address it, making especially interesting the development of tools for the identification and monitoring such kind of hate .	0
23868	23868	S19-5	Introduction	7	14	0.304347826086957	0.067307692307692	For the second one instead, hate against the female gender is a long-time and well-known form of discrimination (Manne, 2017).	0
23869	23869	S19-5	Introduction	8	15	0.347826086956522	0.072115384615385	Both these forms of hate content impact on the development of society and may be confronted by developing tools that automatically detect them.	0
23870	23870	S19-5	Introduction	9	16	0.391304347826087	0.076923076923077	A large number of academic events and shared tasks for different languages (i.e. English, Spanish, Italian, German, Mexican-Spanish, Hindi) took place in the very recent past which are centered on HS and related topics, thus reflecting the interest by the NLP community.	0
23871	23871	S19-5	Introduction	10	17	0.434782608695652	0.081730769230769	Let us mention the first and second edition of the Workshop on Abusive Language 1 (Waseem et al., 2017), the First Workshop on Trolling, Aggression and Cyberbullying (Kumar et al., 2018), that also included a shared task on aggression identification, the tracks on Automatic Misogyny Identification (AMI) (Fersini et al., 2018b) and on Authorship and Aggressiveness Analysis (MEX-A3T) (Carmona et al., 2018) proposed at the 2018 edition of IberEval 2 , the GermEval Shared Task on the Identification of Offensive Language (Wiegand et al., 2018), and finally the Automatic Misogyny Identification task (AMI) (Fersini et al., 2018a) and the Hate Speech Detection task (HaSpeeDe)  at EVALITA 2018 3 for investigating respectively misogyny and HS in Italian.	0
23872	23872	S19-5	Introduction	11	18	0.478260869565217	0.086538461538462	Hat	0
23873	23873	S19-5	Introduction	12	19	0.521739130434783	0.091346153846154	Eval consists in detecting hateful contents in social media texts, specifically in Twitter's posts, against two targets: immigrants and women.	1
23874	23874	S19-5	Introduction	13	20	0.565217391304348	0.096153846153846	Moreover, the task implements a multilingual perspective where data for two widespread languages, English and Spanish, are provided for training and testing participant systems.	0
23875	23875	S19-5	Introduction	14	21	0.608695652173913	0.100961538461538	The motivations for organizing HatEval go beyond the advancement of the state of the art for HS detection for each of the involved languages and targets.	0
23876	23876	S19-5	Introduction	15	22	0.652173913043478	0.105769230769231	The variety of targets of hate and languages provides a unique comparative setting, both with respect to the amount of data collected and annotated applying the same scheme, and with respect to the results achieved by participants training their systems on those data.	0
23877	23877	S19-5	Introduction	16	23	0.695652173913043	0.110576923076923	Such comparative setting may help in shedding new light on the linguistic and communication behaviour against these targets, paving the way for the integration of HS detection tools in several application contexts.	0
23878	23878	S19-5	Introduction	17	24	0.739130434782609	0.115384615384615	Moreover, the participation of a very large amount of research groups in this task (see Section 4) has improved the possibility of in-depth investigation of the involved phenomena.	0
23879	23879	S19-5	Introduction	18	25	0.782608695652174	0.120192307692308	The paper is organized as follows.	0
23880	23880	S19-5	Introduction	19	26	0.826086956521739	0.125	In the next section, the datasets released to the participants for training and testing the systems are described.	0
23881	23881	S19-5	Introduction	20	27	0.869565217391304	0.129807692307692	Section 3 presents the two subtasks and the measures we exploited in the evaluation.	0
23882	23882	S19-5	Introduction	21	28	0.91304347826087	0.134615384615385	Section 4 reports on approaches and results of the participant systems.	0
23883	23883	S19-5	Introduction	22	29	0.956521739130435	0.139423076923077	In Section 5, a preliminary analysis of common errors in top-ranked systems is proposed.	0
23884	23884	S19-5	Introduction	23	30	1.0	0.144230769230769	Section 6 concludes the paper.	0
24220	24220	S19-6	Conclusion	1	158	0.055555555555556	0.902857142857143	We have described SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval).	0
24221	24221	S19-6	Conclusion	2	159	0.111111111111111	0.908571428571429	The task used OLID (Zampieri et al., 2019), a dataset of English tweets annotated for offensive language use, following a three-level hierarchical schema that considers (i) whether a message is offensive or not (for subtask A), (ii) what is the type of the offensive message (for sub-task B), and (iii) who is the target of the offensive message (for sub-task C).	1
24222	24222	S19-6	Conclusion	3	160	0.166666666666667	0.914285714285714	Overall, about 800 teams signed up for Of-fensEval, and 115 of them actually participated in at least one sub-task.	0
24223	24223	S19-6	Conclusion	4	161	0.222222222222222	0.92	The evaluation results have shown that the best systems used ensembles and state-of-the-art deep learning models such as BERT.	0
24224	24224	S19-6	Conclusion	5	162	0.277777777777778	0.925714285714286	Overall, both deep learning and traditional machine learning classifiers were widely used.	0
24225	24225	S19-6	Conclusion	6	163	0.333333333333333	0.931428571428571	More details about the indvididual systems can be found in their respective system description papers, which are published in the SemEval-2019 proceedings.	0
24226	24226	S19-6	Conclusion	7	164	0.388888888888889	0.937142857142857	A list with references to these publications can be found in Table 2; note, however, that only 50 of the 115 participating teams submitted a system description paper.	0
24227	24227	S19-6	Conclusion	8	165	0.444444444444444	0.942857142857143	As is traditional for SemEval, we have made OLID publicly available to the research community beyond the SemEval competition, hoping to facilitate future research on this important topic.	0
24228	24228	S19-6	Conclusion	9	166	0.5	0.948571428571429	In fact, the OLID dataset and the SemEval-2019 Task 6 competition setup have already been used in teaching curricula in universities in UK and USA.	0
24229	24229	S19-6	Conclusion	10	167	0.555555555555556	0.954285714285714	For example, student competitions based on Offens	0
24230	24230	S19-6	Conclusion	11	168	0.611111111111111	0.96	Eval using OLID have been organized as part of Natural Language Processing and Text Analytics courses in two universities in UK: Imperial College London and the University of Leeds.	0
24231	24231	S19-6	Conclusion	12	169	0.666666666666667	0.965714285714286	System papers describing some of the students' work are publicly accessible 11 and have also been made available on arXiv.org (Cambray and Podsadowski, 2019;	0
24232	24232	S19-6	Conclusion	13	170	0.722222222222222	0.971428571428571	Frisiani et al., 2019;	0
24233	24233	S19-6	Conclusion	14	171	0.777777777777778	0.977142857142857	Ong, 2019;Sapora et al., 2019;Puiu and Brabete, 2019;	0
24234	24234	S19-6	Conclusion	15	172	0.833333333333333	0.982857142857143	Uglow et al., 2019).	0
24235	24235	S19-6	Conclusion	16	173	0.888888888888889	0.988571428571428	Similarly, a number of students in Linguistics and Computer Science at the University of Arizona in USA have been using OLID in their coursework.	0
24236	24236	S19-6	Conclusion	17	174	0.944444444444444	0.994285714285714	In future work, we plan to increase the size of the OLID dataset, while addressing issues such as class imbalance and the small size for the test partition, particularly for sub-tasks B and C.	0
24237	24237	S19-6	Conclusion	18	175	1.0	1.0	We would also like to expand the dataset and the task to other languages.	0
24239	24239	S19-7	abstract	1	2	0.125	0.009478672985782	"Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of ""fake news"" has become a mainstream concern."	0
24240	24240	S19-7	abstract	2	3	0.25	0.014218009478673	However automated support for rumour verification remains in its infancy.	0
24241	24241	S19-7	abstract	3	4	0.375	0.018957345971564	It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase.	0
24242	24242	S19-7	abstract	4	5	0.5	0.023696682464455	Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour's veracity.	0
24243	24243	S19-7	abstract	5	6	0.625	0.028436018957346	As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity.	0
24244	24244	S19-7	abstract	6	7	0.75	0.033175355450237	The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts.	0
24245	24245	S19-7	abstract	7	8	0.875	0.037914691943128	There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants.	1
24246	24246	S19-7	abstract	8	9	1.0	0.042654028436019	We received 22 system submissions (a 70% increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved.	0
24458	24458	S19-8	Overview	1	10	0.032258064516129	0.051813471502591	The current coverage of the political landscape in both the press and in social media has led to an unprecedented situation.	0
24459	24459	S19-8	Overview	2	11	0.064516129032258	0.05699481865285	Like never before, a statement in an interview, a press release, a blog note, or a tweet can spread almost instantaneously.	0
24460	24460	S19-8	Overview	3	12	0.096774193548387	0.062176165803109	The speed of proliferation leaves little time for double-checking claims against the facts, which has proven critical in politics, e.g., during the 2016 presidential campaign in the USA, which was dominated by fake news in social media and by false claims.	0
24461	24461	S19-8	Overview	4	13	0.129032258064516	0.067357512953368	Investigative journalists and volunteers have been working hard to get to the root of a claim and to present solid evidence in favor or against it.	0
24462	24462	S19-8	Overview	5	14	0.161290322580645	0.072538860103627	Manual fact-checking is very time-consuming, and thus automatic methods have been proposed to speed-up the process, e.g., there has been work on checking the factuality/credibility of a claim, of a news article, or of an information source (Ba et al., 2016;	0
24463	24463	S19-8	Overview	6	15	0.193548387096774	0.077720207253886	Zubiaga et al., 2016;	0
24464	24464	S19-8	Overview	7	16	0.225806451612903	0.082901554404145	Ma et al., 2016;	0
24465	24465	S19-8	Overview	8	17	0.258064516129032	0.088082901554404	Castillo et al., 2011;	0
24466	24466	S19-8	Overview	9	18	0.290322580645161	0.093264248704663	Baly et al., 2018).	0
24467	24467	S19-8	Overview	10	19	0.32258064516129	0.098445595854922	The process starts when a document is made public.	0
24468	24468	S19-8	Overview	11	20	0.354838709677419	0.103626943005181	First, an intrinsic analysis is carried out in which check-worthy text fragments are identified.	0
24469	24469	S19-8	Overview	12	21	0.387096774193548	0.10880829015544	Then, other documents that might support or rebut a claim in the document are retrieved from various sources.	0
24470	24470	S19-8	Overview	13	22	0.419354838709677	0.113989637305699	Finally, by comparing a claim against the retrieved evidence, a system can determine whether the claim is likely true or likely false (or unsure, if no strong enough evidence either way could be found).	0
24471	24471	S19-8	Overview	14	23	0.451612903225806	0.119170984455959	For instance, Ciampaglia et al. (2015) do this using a knowledge graph derived from Wikipedia.	0
24472	24472	S19-8	Overview	15	24	0.483870967741936	0.124352331606218	The outcome could then be presented to a human expert for final judgement.	0
24473	24473	S19-8	Overview	16	25	0.516129032258065	0.129533678756477	1 For our two subtasks, we explore factuality in the context of Community Question Answering (cQA) forums.	1
24474	24474	S19-8	Overview	17	26	0.548387096774194	0.134715025906736	Forums such as StackOverflow, Yahoo!	0
24475	24475	S19-8	Overview	18	27	0.580645161290323	0.139896373056995	Answers, and Quora are very popular these days, as they represent effective means for communities around particular topics to share information.	0
24476	24476	S19-8	Overview	19	28	0.612903225806452	0.145077720207254	However, the information shared by the users is not always correct or accurate.	0
24477	24477	S19-8	Overview	20	29	0.645161290322581	0.150259067357513	There are multiple factors explaining the presence of incorrect answers in cQA forums, e.g., misunderstanding of the question, ignorance or maliciousness of the responder.	0
24478	24478	S19-8	Overview	21	30	0.67741935483871	0.155440414507772	Also, as a result of our dynamic world, the truth is time-sensitive: something that was true yesterday may be false today.	0
24479	24479	S19-8	Overview	22	31	0.709677419354839	0.160621761658031	Moreover, forums are often barely moderated and thus lack systematic quality control.	0
24480	24480	S19-8	Overview	23	32	0.741935483870968	0.16580310880829	Here we focus on checking the factuality of questions and answers in cQA forums.	0
24481	24481	S19-8	Overview	24	33	0.774193548387097	0.170984455958549	This aspect was ignored in recent cQA tasks (Ishikawa et al., 2010;	0
24482	24482	S19-8	Overview	25	34	0.806451612903226	0.176165803108808	Nakov et al., 2015	0
24483	24483	S19-8	Overview	26	35	0.838709677419355	0.181347150259067	Nakov et al., , 2016a, where an answer is considered GOOD if it addresses the question, irrespective of its veracity, accuracy, etc.	0
24484	24484	S19-8	Overview	27	36	0.870967741935484	0.186528497409326	Figure 1 presents an excerpt of an example from the Qatar Living Forum, with one question and three answers selected from a longer thread.	0
24485	24485	S19-8	Overview	28	37	0.903225806451613	0.191709844559585	According to SemEval-2016 Task 3 (Nakov et al., 2016a), all three answers would be considered GOOD since they are formally answering the question.	0
24486	24486	S19-8	Overview	29	38	0.935483870967742	0.196891191709845	Nevertheless, a 1 contains false information, while a 2 and a 3 are correct, as can be established from an official government website.	0
24487	24487	S19-8	Overview	30	39	0.967741935483871	0.202072538860104	2 Checking the veracity of answers in a cQA forum is a hard problem, which requires putting together aspects of language understanding, modelling the context, integrating several information sources, uisng world knowledge and complex inference, among others.	0
24488	24488	S19-8	Overview	31	40	1.0	0.207253886010363	Moreover, high-quality automatic fact-checking would offer better experience to users of cQA systems, e.g., the user could be presented with veracity scores, where low scores would warn the user not to completely trust the answer or to double-check it.	0
24649	24649	S19-9	Introduction	1	8	0.025641025641026	0.036697247706422	State of the art opinion mining systems provide numerical summaries of sentiments and tend to overlook additional descriptive and potentially useful content present in the opinionated text.	0
24650	24650	S19-9	Introduction	2	9	0.051282051282051	0.041284403669725	We stress that such content also encompass information like suggestions, tips, and advice, which is otherwise explicitly sought by the stakeholders.	0
24651	24651	S19-9	Introduction	3	10	0.076923076923077	0.045871559633028	For example, hotel reviews often contain room tips, i.e., which room should be preferred in a hotel.	0
24652	24652	S19-9	Introduction	4	11	0.102564102564103	0.05045871559633	Likewise, tips on restaurants, shops, sightseeing, etc. are also present within the hotel reviews.	0
24653	24653	S19-9	Introduction	5	12	0.128205128205128	0.055045871559633	On the other hand, platforms like Tripadvisor 1 , which collect hotel and restaurant related opinions, request the reviewers to fill up the room tips section in addition to the hotel review.	0
24654	24654	S19-9	Introduction	6	13	0.153846153846154	0.059633027522936	Likewise, sentences expressing advice, tips, and recommendations relating to a target entity can often be present in text available from different types of data sources, like blogs, microblogs, discussions, etc.	0
24655	24655	S19-9	Introduction	7	14	0.179487179487179	0.064220183486239	Such sentences can be collectively referred to as suggestions.	0
24656	24656	S19-9	Introduction	8	15	0.205128205128205	0.068807339449541	With the increasing availability of opinionated text, methods for automatic detection of suggestions can be employed for different use cases.	0
24657	24657	S19-9	Introduction	9	16	0.230769230769231	0.073394495412844	Some example use cases are the extraction of suggestions for brand improvement, the extraction of tips and advice for customers, the extraction of the expressions of recommendations from unstructured data in order to aid recommender systems, or the summarisation of suggestion forums where suggestion providers often tend to provide context in their responses (Figure 1) which gets repetitive over a large number of responses relating to the same entity.	0
24658	24658	S19-9	Introduction	10	17	0.256410256410256	0.077981651376147	The task of automatic identification of suggestions in a given text is referred to as suggestion mining (Brun and Hagege, 2013).	0
24659	24659	S19-9	Introduction	11	18	0.282051282051282	0.08256880733945	Studies performed on suggestion mining have defined it as a sentence classification task, where class prediction has to be made on each sentence of a given text, classes being suggestion and non suggestion (Negi, 2016).	1
24660	24660	S19-9	Introduction	12	19	0.307692307692308	0.087155963302752	State of the art opinion mining systems have mostly focused on identifying sentiment polarity of the text.	0
24661	24661	S19-9	Introduction	13	20	0.333333333333333	0.091743119266055	Therefore, suggestion mining remains a very less explored problem as compared to sentiment analysis, specially in the context of recent advancements in neural network based approaches for feature learning and transfer learning.	0
24662	24662	S19-9	Introduction	14	21	0.358974358974359	0.096330275229358	As suggestion mining is still an emerging research area, it lacks benchmark datasets and well defined annotation guidelines.	0
24663	24663	S19-9	Introduction	15	22	0.384615384615385	0.100917431192661	A few early works were mostly rule based methods, mainly targeted towards the use case of extracting suggestions for product improvements (Brun and Hagege, 2013;	0
24664	24664	S19-9	Introduction	16	23	0.41025641025641	0.105504587155963	Ramanand et al., 2010;Moghaddam, 2015).	0
24665	24665	S19-9	Introduction	17	24	0.435897435897436	0.110091743119266	In our prior work, we performed early investigations on the problem definition and datasets, aiming for the statistical methods which also require benchmark train datasets in addition to the evaluation Figure 1: A post from the suggestion forum for Microsoft developers datasets (Negi and Buitelaar, 2015;.	0
24666	24666	S19-9	Introduction	18	25	0.461538461538462	0.114678899082569	A few other works also evaluated statistical classifiers (Wicaksono and Myaeng, 2012;	0
24667	24667	S19-9	Introduction	19	26	0.487179487179487	0.119266055045872	Dong et al., 2013), which employed mostly manually identified features, however only two other works (Wicaksono and Myaeng, 2012;	0
24668	24668	S19-9	Introduction	20	27	0.512820512820513	0.123853211009174	Dong et al., 2013) provided their datasets.	0
24669	24669	S19-9	Introduction	21	28	0.538461538461538	0.128440366972477	Suggestion mining still lacks well defined annotation guidelines, a multi-domain and cross-domain approach to the problem and benchmark datasets, which we address in our recent work (Negi et al., 2018).	0
24670	24670	S19-9	Introduction	22	29	0.564102564102564	0.13302752293578	Therefore, we introduce this pilot shared task to disseminate suggestion mining benchmarks and evaluate state of the art methods for text classification on domain specific and cross domain training scenarios.	0
24671	24671	S19-9	Introduction	23	30	0.58974358974359	0.137614678899083	The datasets released as a part of the shared task include the domains hotel reviews and software developers suggestion forum (see Table 1).	0
24672	24672	S19-9	Introduction	24	31	0.615384615384615	0.142201834862385	Suggestion mining faces similar text processing challenges as other sentence or short text classification tasks related to opinion mining and subjectivity analysis, such as stance detection (Mohammad et al., 2016), or tweet sentiment classification (Rosenthal et al., 2015).	0
24673	24673	S19-9	Introduction	25	32	0.641025641025641	0.146788990825688	Some of the observed challenges in suggestion mining are elaborated below:	0
24674	24674	S19-9	Introduction	26	33	0.666666666666667	0.151376146788991	 Class imbalance: Usually, suggestions tend to appear sparsely among opinionated text, which leads to higher data annotation costs and results in a class distribution bias in the trained models.	0
24675	24675	S19-9	Introduction	27	34	0.692307692307692	0.155963302752294		0
24676	24676	S19-9	Introduction	28	35	0.717948717948718	0.160550458715596	Figurative expressions:	0
24677	24677	S19-9	Introduction	29	36	0.743589743589744	0.165137614678899	Text from social media and other sources usually contains figurative use of language, which demands pragmatic understanding from the models.	0
24678	24678	S19-9	Introduction	30	37	0.769230769230769	0.169724770642202	For example, 'Try asking for extra juice at breakfast -its 22 euros!!!!!' is more of a sarcasm than a suggestion.	0
24679	24679	S19-9	Introduction	31	38	0.794871794871795	0.174311926605505	Therefore, a sentence framed as a typical suggestions may not always be a suggestion and vice versa.	0
24680	24680	S19-9	Introduction	32	39	0.82051282051282	0.178899082568807	A variety of linguistic strategies used in suggestions also make this task interesting from a computational linguistics perspective and labeled datasets can be leveraged for linguistic studies as well.	0
24681	24681	S19-9	Introduction	33	40	0.846153846153846	0.18348623853211	 Context dependency:	0
24682	24682	S19-9	Introduction	34	41	0.871794871794872	0.188073394495413	In some cases, context plays a major role in determining whether a sentence is a suggestion or not.	0
24683	24683	S19-9	Introduction	35	42	0.897435897435897	0.192660550458716	For example, 'There is a parking garage on the corner of the Forbes showroom.' can be labeled as a suggestion (for parking space) when it appears in a restaurant review and a human annotator gets to read the full review.	0
24684	24684	S19-9	Introduction	36	43	0.923076923076923	0.197247706422018	However, the same sentence would not be labeled as a suggestion if the text is aimed to describe the surroundings of the Forbes showroom.	0
24685	24685	S19-9	Introduction	37	44	0.948717948717949	0.201834862385321		0
24686	24686	S19-9	Introduction	38	45	0.974358974358974	0.206422018348624	Long and complex sentences:	0
24687	24687	S19-9	Introduction	39	46	1.0	0.211009174311927	Often, a suggestion is expressed in either one part of a sentence, or it is elaborated as a long sentence, like, 'I think that there should be a nice feature where you can be able to slide the status bar down and view all the push notifications that you got but you didn't view, just like	0
24860	24860	S19-10	title	1	1	1.0	0.007462686567164	SemEval 2019 Task 10: Math Question Answering	1
24861	24861	S19-10	abstract	1	2	0.2	0.014925373134328	We report on the SemEval 2019 task on math question answering.	0
24862	24862	S19-10	abstract	2	3	0.4	0.022388059701493	We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions.	0
24863	24863	S19-10	abstract	3	4	0.6	0.029850746268657	For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms.	0
24864	24864	S19-10	abstract	4	5	0.8	0.037313432835821	Systems were evaluated based on the percentage of correctly answered questions.	0
24865	24865	S19-10	abstract	5	6	1.0	0.044776119402985	The top system correctly answered 45% of the test questions, a considerable improvement over the 17% random guessing baseline.	0
24995	24995	S19-12	abstract	1	2	0.090909090909091	0.010989010989011	We present the SemEval-2019	0
24996	24996	S19-12	abstract	2	3	0.181818181818182	0.016483516483517	Task 12 which focuses on toponym resolution in scientific articles.	0
24997	24997	S19-12	abstract	3	4	0.272727272727273	0.021978021978022	Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations.	1
24998	24998	S19-12	abstract	4	5	0.363636363636364	0.027472527472528	We proposed three subtasks.	0
24999	24999	S19-12	abstract	5	6	0.454545454545455	0.032967032967033	In Subtask 1, we asked participants to detect all toponyms in an article.	0
25000	25000	S19-12	abstract	6	7	0.545454545454545	0.038461538461539	In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames.	0
25001	25001	S19-12	abstract	7	8	0.636363636363636	0.043956043956044	In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms.	0
25002	25002	S19-12	abstract	8	9	0.727272727272727	0.04945054945055	A total of 29 teams registered, and 8 teams submitted a system run.	0
25003	25003	S19-12	abstract	9	10	0.818181818181818	0.054945054945055	We summarize the corpus and the tools created for the challenge.	0
25004	25004	S19-12	abstract	10	11	0.909090909090909	0.060439560439561	They are freely available at https://competitions.codalab. org/competitions/19948.	0
25005	25005	S19-12	abstract	11	12	1.0	0.065934065934066	We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.	0
25182	25182	S20-1	Overview	1	7	0.066666666666667	0.017587939698493	Recent years have seen an exponentially rising interest in computational Lexical Semantic Change (LSC) detection (Tahmasebi et al., 2018;Kutuzov et al., 2018).	0
25183	25183	S20-1	Overview	2	8	0.133333333333333	0.020100502512563	However, the field is lacking standard evaluation tasks and data.	0
25184	25184	S20-1	Overview	3	9	0.2	0.022613065326633	Almost all papers differ in how the evaluation is performed and what factors are considered in the evaluation.	0
25185	25185	S20-1	Overview	4	10	0.266666666666667	0.025125628140704	Very few are evaluated on a manually annotated diachronic corpus Perrone et al., 2019;	0
25186	25186	S20-1	Overview	5	11	0.333333333333333	0.027638190954774	Schlechtweg et al., 2019, e.g.).	0
25187	25187	S20-1	Overview	6	12	0.4	0.030150753768844	This puts a damper on the development of computational models for LSC, and is a barrier for high-quality, comparable results that can be used in follow-up tasks.	0
25188	25188	S20-1	Overview	7	13	0.466666666666667	0.032663316582915	We report the results of the first SemEval shared task on Unsupervised LSC detection.	0
25189	25189	S20-1	Overview	8	14	0.533333333333333	0.035175879396985	1	0
25190	25190	S20-1	Overview	9	15	0.6	0.037688442211055	We introduce two related subtasks for computational LSC detection, which aim to identify the change in meaning of words over time using corpus data.	1
25191	25191	S20-1	Overview	10	16	0.666666666666667	0.040201005025126	We provide a high-quality multilingual (English, German, Latin, Swedish) LSC gold standard relying on approximately 100,000 instances of human judgment.	0
25192	25192	S20-1	Overview	11	17	0.733333333333333	0.042713567839196	For the first time, it is possible to compare the variety of proposed models on relatively solid grounds and across languages, and to put previously reached conclusions on trial.	0
25193	25193	S20-1	Overview	12	18	0.8	0.045226130653266	We may now provide answers to questions concerning the performance of different types of semantic representations (such as token embeddings vs. type embeddings, and topic models vs. vector space models), alignment methods and change measures.	0
25194	25194	S20-1	Overview	13	19	0.866666666666667	0.047738693467337	We provide a thorough analysis of the submitted results uncovering trends for models and opening perspectives for further improvements.	0
25195	25195	S20-1	Overview	14	20	0.933333333333333	0.050251256281407	In addition to this, the CodaLab website will remain open to allow any reader to directly and easily compare their results to the participating systems.	0
25196	25196	S20-1	Overview	15	21	1.0	0.052763819095477	We expect the long-term impact of the task to be significant, and hope to encourage the study of LSC in more languages than are currently studied, in particular less-resourced languages.	0
25574	25574	S20-2	title	1	1	1.0	0.005405405405405	SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment	1
25575	25575	S20-2	abstract	1	2	0.142857142857143	0.010810810810811	Lexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction.	0
25576	25576	S20-2	abstract	2	3	0.285714285714286	0.016216216216216	Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries.	0
25577	25577	S20-2	abstract	3	4	0.428571428571429	0.021621621621622	As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE.	0
25578	25578	S20-2	abstract	4	5	0.571428571428571	0.027027027027027	The shared task spans three dimensions: (1) monolingual LE in multiple languages versus cross-lingual LE, (2) binary versus graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs).	0
25579	25579	S20-2	abstract	5	6	0.714285714285714	0.032432432432433	We offered two different evaluation tracks: (a) distributional (Dist): for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b)	0
25580	25580	S20-2	abstract	6	7	0.857142857142857	0.037837837837838	Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet).	0
25581	25581	S20-2	abstract	7	8	1.0	0.043243243243243	In the Any track, we received system runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction.	0
25760	25760	S20-3	abstract	1	2	0.2	0.008163265306122	This paper presents the Graded Word Similarity in Context (GWSC) task which asked participants to predict the effects of context on human perception of similarity in English, Croatian, Slovene and Finnish.	1
25761	25761	S20-3	abstract	2	3	0.4	0.012244897959184	We received 15 submissions and 11 system description papers.	0
25762	25762	S20-3	abstract	3	4	0.6	0.016326530612245	A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages.	0
25763	25763	S20-3	abstract	4	5	0.8	0.020408163265306	Systems beat the baselines by significant margins, but few did well in more than one language or subtask.	0
25764	25764	S20-3	abstract	5	6	1.0	0.024489795918367	Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect.	0
26520	26520	S20-6	abstract	1	2	0.2	0.015384615384615	Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered.	0
26521	26521	S20-6	abstract	2	3	0.4	0.023076923076923	In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a termdefinition pair corpus that reflects the complex reality of definitions in natural language.	1
26522	26522	S20-6	abstract	3	4	0.6	0.030769230769231	Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner.	0
26523	26523	S20-6	abstract	4	5	0.8	0.038461538461539	Deft	0
26524	26524	S20-6	abstract	5	6	1.0	0.046153846153846	Eval involved 3 distinct subtasks: 1) Sentence classification, 2) sequence labeling, and 3) relation extraction.	0
26700	26700	S20-7	Task Description	1	52	0.142857142857143	0.237442922374429	The objective of this shared task is to build systems for rating a humorous effect that is caused by small changes in text.	1
26701	26701	S20-7	Task Description	2	53	0.285714285714286	0.242009132420091	To this end, we focus on humor obtained by applying micro-edits to news headlines.	1
26702	26702	S20-7	Task Description	3	54	0.428571428571429	0.246575342465753	Editing headlines presents a unique opportunity for humor research since headlines convey substantial information using only a few words.	0
26703	26703	S20-7	Task Description	4	55	0.571428571428571	0.251141552511415	This creates a rich background against which a micro-edit can lead to a humorous effect.	0
26704	26704	S20-7	Task Description	5	56	0.714285714285714	0.255707762557078	With that data, a computational humor model can focus on the exact localized cause of the humorous effect in a short textual context.	0
26705	26705	S20-7	Task Description	6	57	0.857142857142857	0.26027397260274	We split our task into two subtasks.	0
26706	26706	S20-7	Task Description	7	58	1.0	0.264840182648402	The dataset statistics for these subtasks are shown in Table 2.	0
26869	26869	S20-8	abstract	1	2	0.111111111111111	0.011299435028249	Information on social media comprises of various modalities such as textual, visual and audio.	0
26870	26870	S20-8	abstract	2	3	0.222222222222222	0.016949152542373	NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media.	0
26871	26871	S20-8	abstract	3	4	0.333333333333333	0.022598870056497	However, computational processing of Internet memes needs a hybrid approach.	0
26872	26872	S20-8	abstract	4	5	0.444444444444444	0.028248587570622	The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content anymore.	0
26873	26873	S20-8	abstract	5	6	0.555555555555556	0.033898305084746	To the best of our knowledge, there is not much attention towards meme emotion analysis.	0
26874	26874	S20-8	abstract	6	7	0.666666666666667	0.03954802259887	The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes.	0
26875	26875	S20-8	abstract	7	8	0.777777777777778	0.045197740112994	The task Memotion analysis released approx 10K annotated memes-with human annotated labels namely sentiment(positive, negative, neutral), type of emotion(sarcastic,funny,offensive, motivation) and their corresponding intensity.	0
26876	26876	S20-8	abstract	8	9	0.888888888888889	0.050847457627119	The challenge consisted of three subtasks: sentiment (positive, negative, and neutral) analysis of memes, overall emotion (humor, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion.	1
26877	26877	S20-8	abstract	9	10	1.0	0.056497175141243	The best performances achieved were F 1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks.	0
27105	27105	S20-9	Task Description	1	61	0.058823529411765	0.230188679245283	Although code-mixing has received some attention recently, properly annotated data is still scarce.	0
27106	27106	S20-9	Task Description	2	62	0.117647058823529	0.233962264150943	We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media.	1
27107	27107	S20-9	Task Description	3	63	0.176470588235294	0.237735849056604	Each tweet is classified into one of the three polarity classes -Positive, Negative, Neutral.	0
27108	27108	S20-9	Task Description	4	64	0.235294117647059	0.241509433962264	Each tweet also has word-level language marking.	0
27109	27109	S20-9	Task Description	5	65	0.294117647058823	0.245283018867925	We release two datasets -Spanglish and Hinglish.	0
27110	27110	S20-9	Task Description	6	66	0.352941176470588	0.249056603773585	We used CodaLab 4,5 to release the datasets and evaluate submissions.	0
27111	27111	S20-9	Task Description	7	67	0.411764705882353	0.252830188679245	Initially, the participants had access only to train and validation data.	0
27112	27112	S20-9	Task Description	8	68	0.470588235294118	0.256603773584906	They could check their system's performance on the validation set on a public leaderboard.	0
27113	27113	S20-9	Task Description	9	69	0.529411764705882	0.260377358490566	Later, a previously unseen test set was released, and the performance on the test set was used to rank the participants.	0
27114	27114	S20-9	Task Description	10	70	0.588235294117647	0.264150943396226	Only the first three submissions on the test set by each participant were considered, to avoid over-fitting on the test set.	0
27115	27115	S20-9	Task Description	11	71	0.647058823529412	0.267924528301887	The ranking was done based on the best out of the three submissions.	0
27116	27116	S20-9	Task Description	12	72	0.705882352941176	0.271698113207547	There was no distinction between constrained and unconstrained systems, but the participants were asked to report what additional resources they have used for each submitted run.	0
27117	27117	S20-9	Task Description	13	73	0.764705882352941	0.275471698113208	We release 20k labeled tweets for Hinglish and  19k labeled tweets for Spanglish.	0
27118	27118	S20-9	Task Description	14	74	0.823529411764706	0.279245283018868	In both the datasets, 6 in addition to the tweet level sentiment label, each tweet also has a word-level language label.	0
27119	27119	S20-9	Task Description	15	75	0.882352941176471	0.283018867924528	The detailed distribution is provided in Table 1.	0
27120	27120	S20-9	Task Description	16	76	0.941176470588235	0.286792452830189	Some annotated examples are provided in Table 2.	0
27121	27121	S20-9	Task Description	17	77	1.0	0.290566037735849	Although this task focuses on sentiment analysis, the data has word-level language marking and can be used for other NLP tasks.	0
27311	27311	S20-10	abstract	1	2	0.125	0.011834319526627	In this paper, we present the main findings and compare the results of SemEval-2020 Task 10, Emphasis Selection for Written Text in Visual Media.	0
27312	27312	S20-10	abstract	2	3	0.25	0.017751479289941	The goal of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in textual content to enable automated design assistance in authoring.	1
27313	27313	S20-10	abstract	3	4	0.375	0.023668639053255	The main focus is on short text instances for social media, with a variety of examples, from social media posts to inspirational quotes.	0
27314	27314	S20-10	abstract	4	5	0.5	0.029585798816568	Participants were asked to model emphasis using plain text with no additional context from the user or other design considerations.	0
27315	27315	S20-10	abstract	5	6	0.625	0.035502958579882	SemEval-2020 Emphasis Selection shared task attracted 197 participants in the early phase and a total of 31 teams made submissions to this task.	0
27316	27316	S20-10	abstract	6	7	0.75	0.041420118343195	The highest-ranked submission achieved 0.823 Match m score.	0
27317	27317	S20-10	abstract	7	8	0.875	0.047337278106509	The analysis of systems submitted to the task indicates that BERT and RoBERTa were the most common choice of pre-trained models used, and part of speech tag (POS) was the most useful feature.	0
27318	27318	S20-10	abstract	8	9	1.0	0.053254437869823	Full results can be found on the task's website 1 .	0
27479	27479	S20-11	title	1	1	1.0	0.002364066193853	SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles	1
27480	27480	S20-11	abstract	1	2	0.142857142857143	0.004728132387707	We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles.	1
27481	27481	S20-11	abstract	2	3	0.285714285714286	0.00709219858156	The task featured two subtasks.	0
27482	27482	S20-11	abstract	3	4	0.428571428571429	0.009456264775414	Subtask SI is about Span Identification: given a plain-text document, spot the specific text fragments containing propaganda.	0
27483	27483	S20-11	abstract	4	5	0.571428571428571	0.011820330969267	Subtask TC is about Technique Classification: given a specific text fragment, in the context of a full document, determine the propaganda technique it uses, choosing from an inventory of 14 possible propaganda techniques.	0
27484	27484	S20-11	abstract	5	6	0.714285714285714	0.014184397163121	The task attracted a large number of participants: 250 teams signed up to participate and 44 made a submission on the test set.	0
27485	27485	S20-11	abstract	6	7	0.857142857142857	0.016548463356974	In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used.	0
27486	27486	S20-11	abstract	7	8	1.0	0.018912529550828	For both subtasks, the best systems used pre-trained Transformers and ensembles.	0
27797	27797	S20-11	Conclusion and Future Work	1	319	0.142857142857143	0.754137115839243	We have described SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles.	1
27798	27798	S20-11	Conclusion and Future Work	2	320	0.285714285714286	0.756501182033097	The task attracted the interest of a number of researchers: 250 teams signed up to participate, and 44 made submissions on the test dataset.	0
27799	27799	S20-11	Conclusion and Future Work	3	321	0.428571428571429	0.75886524822695	We received 35 and 31 submissions for subtask SI and subtask TC, respectively.	0
27800	27800	S20-11	Conclusion and Future Work	4	322	0.571428571428571	0.761229314420804	Overall, subtask SI (segment identification) was easier and all systems managed to improve over the baseline.	0
27801	27801	S20-11	Conclusion and Future Work	5	323	0.714285714285714	0.763593380614657	However, subtask TC (technique classification) proved to be much more challenging, and some teams could not improve over our baseline.	0
27802	27802	S20-11	Conclusion and Future Work	6	324	0.857142857142857	0.765957446808511	In future work, we plan to extend the dataset to cover more examples as well as more propaganda techniques.	0
27803	27803	S20-11	Conclusion and Future Work	7	325	1.0	0.768321513002364	We further plan to develop similar datasets for other languages.	0
28133	28133	S21-1	Introduction	1	7	0.013333333333333	0.02755905511811	The occurrence of an unknown word in a sentence can adversely affect its comprehension by readers.	0
28134	28134	S21-1	Introduction	2	8	0.026666666666667	0.031496062992126	Either they give up, misinterpret, or plough on without understanding.	0
28135	28135	S21-1	Introduction	3	9	0.04	0.035433070866142	A committed reader may take the time to look up a word and expand their vocabulary, but even in this case they must leave the text, undermining their concentration.	0
28136	28136	S21-1	Introduction	4	10	0.053333333333333	0.039370078740158	The natural language processing solution is to identify candidate words in a text that may be too difficult for a reader (Shardlow, 2013;	0
28137	28137	S21-1	Introduction	5	11	0.066666666666667	0.043307086614173	Paetzold and Specia, 2016a).	0
28138	28138	S21-1	Introduction	6	12	0.08	0.047244094488189	Each potential word is assigned a judgment by a system to determine if it was deemed 'complex' or not.	0
28139	28139	S21-1	Introduction	7	13	0.093333333333333	0.051181102362205	These scores indicate which words are likely to cause problems for a reader.	0
28140	28140	S21-1	Introduction	8	14	0.106666666666667	0.055118110236221	The words that are identified as problematic can be the subject of numerous types of intervention, such as direct replacement in the setting of lexical simplification (Gooding and Kochmar, 2019), or extra information being given in the context of explanation generation .	0
28141	28141	S21-1	Introduction	9	15	0.12	0.059055118110236	Whereas previous solutions to this task have typically considered the Complex Word Identification (CWI) task (Paetzold and Specia, 2016a;	0
28142	28142	S21-1	Introduction	10	16	0.133333333333333	0.062992125984252	Yimam et al., 2018) in which a binary judgment of a word's complexity is given (i.e., is a word complex or not?), we instead focus on the Lexical Complexity Prediction (LCP) task (Shardlow et al., 2020) in which a value is assigned from a continuous scale to identify a word's complexity (i.e., how complex is this word?).	1
28143	28143	S21-1	Introduction	11	17	0.146666666666667	0.066929133858268	We ask multiple annotators to give a judgment on each instance in our corpus and take the average prediction as our complexity label.	0
28144	28144	S21-1	Introduction	12	18	0.16	0.070866141732284	The former task (CWI) forces each user to make a subjective judgment about the nature of the word that models their personal vocabulary.	0
28145	28145	S21-1	Introduction	13	19	0.173333333333333	0.074803149606299	Many factors may affect the annotator's judgment including their education level, first language, specialism or familiarity with the text at hand.	0
28146	28146	S21-1	Introduction	14	20	0.186666666666667	0.078740157480315	The annotators may also disagree on the level of difficulty at which to label a word as complex.	0
28147	28147	S21-1	Introduction	15	21	0.2	0.082677165354331	One annotator may label every word they feel is above average difficulty, another may label words that they feel unfamiliar with, but understand from the context, whereas another annotator may only label those words that they find totally incomprehensible, even in context.	0
28148	28148	S21-1	Introduction	16	22	0.213333333333333	0.086614173228347	Our introduction of the LCP task seeks to address this annotator confusion by giving annotators a Likert scale to provide their judgments.	0
28149	28149	S21-1	Introduction	17	23	0.226666666666667	0.090551181102362	Whilst annotators must still give a subjective judgment depending on their own understanding, familiarity and vocabulary -they do so in a way that better captures the meaning behind each judgment they have given.	0
28150	28150	S21-1	Introduction	18	24	0.24	0.094488188976378	By aggregating these judgments we have developed a dataset that contains continuous labels in the range of 0-1 for each instance.	0
28151	28151	S21-1	Introduction	19	25	0.253333333333333	0.098425196850394	This means that rather than a system predicting whether a word is complex or not (0 or 1), instead a system must now predict where, on our continuous scale, a word falls (0-1).	0
28152	28152	S21-1	Introduction	20	26	0.266666666666667	0.102362204724409	Consider the following sentence taken from a biomedical source, where the target word 'observation' has been highlighted:	0
28153	28153	S21-1	Introduction	21	27	0.28	0.106299212598425	(1) The observation of unequal expression leads to a number of questions.	0
28154	28154	S21-1	Introduction	22	28	0.293333333333333	0.110236220472441	In the binary annotation setting of CWI some annotators may rightly consider this term non-complex, whereas others may rightly consider it to be complex.	0
28155	28155	S21-1	Introduction	23	29	0.306666666666667	0.114173228346457	Whilst the meaning of the word is reasonably clear to someone with scientific training, the context in which it is used is unfamiliar for a lay reader and will likely lead to them considering it complex.	0
28156	28156	S21-1	Introduction	24	30	0.32	0.118110236220472	In our new LCP setting, we are able to ask annotators to mark the word on a scale from very easy to very difficult.	0
28157	28157	S21-1	Introduction	25	31	0.333333333333333	0.122047244094488	Each user can give their subjective interpretation on this scale indicating how difficult they found the word.	0
28158	28158	S21-1	Introduction	26	32	0.346666666666667	0.125984251968504	Whilst annotators will inevitably disagree (some finding it more or less difficult), this is captured and quantified as part of our annotations, with a word of this type likely to lead to a medium complexity value.	0
28159	28159	S21-1	Introduction	27	33	0.36	0.12992125984252	LCP is useful as part of the wider task of lexical simplification (Devlin and Tait, 1998), where it can be used to both identify candidate words for simplification (Shardlow, 2013) and rank potential words as replacements . LCP is also relevant to the field of readability assessment, where knowing the proportion of complex words in a text helps to identify the overall complexity of the text (Dale and Chall., 1948).	0
28160	28160	S21-1	Introduction	28	34	0.373333333333333	0.133858267716535	This paper presents SemEval-2021 Task 1: Lexical Complexity Prediction.	0
28161	28161	S21-1	Introduction	29	35	0.386666666666667	0.137795275590551	In this task we developed a new dataset for complexity prediction based on the previously published CompLex dataset.	0
28162	28162	S21-1	Introduction	30	36	0.4	0.141732283464567	Our dataset covers 10,800 instances spanning 3 genres and containing unigrams and bigrams as targets for complexity prediction.	0
28163	28163	S21-1	Introduction	31	37	0.413333333333333	0.145669291338583	We solicited participants in our task and released a trial, training and test split in accordance with the SemEval schedule.	0
28164	28164	S21-1	Introduction	32	38	0.426666666666667	0.149606299212598	We accepted submissions in two separate Sub-tasks, the first being single words only and the second taking single words and multi-word expressions (modelled by our bigrams).	0
28165	28165	S21-1	Introduction	33	39	0.44	0.153543307086614	In total 55 teams participated across the two Sub-tasks.	0
28166	28166	S21-1	Introduction	34	40	0.453333333333333	0.15748031496063	The rest of this paper is structured as folllows:	0
28167	28167	S21-1	Introduction	35	41	0.466666666666667	0.161417322834646	In Section 2 we discuss the previous two iterations of the CWI task.	0
28168	28168	S21-1	Introduction	36	42	0.48	0.165354330708661	In Section 3, we present the CompLex 2.0 dataset that we have used for our task, including the methodology we used to produce trial, test and training splits.	0
28169	28169	S21-1	Introduction	37	43	0.493333333333333	0.169291338582677	In Section 5, we show the results of the participating systems and compare the features that were used by each system.	0
28170	28170	S21-1	Introduction	38	44	0.506666666666667	0.173228346456693	We finally discuss the nature of LCP in Section 7 and give concluding remarks in Section 8 2 Related Tasks CWI 2016 at SemEval	0
28171	28171	S21-1	Introduction	39	45	0.52	0.177165354330709	The CWI shared task was organized at SemEval 2016 (Paetzold and Specia, 2016a).	0
28172	28172	S21-1	Introduction	40	46	0.533333333333333	0.181102362204724	The CWI 2016 organizers introduced a new CWI dataset and reported the results of 42 CWI systems developed by 21 teams.	0
28173	28173	S21-1	Introduction	41	47	0.546666666666667	0.18503937007874	Words in their dataset were considered complex if they were difficult to understand for non-native English speakers according to a binary labelling protocol.	0
28174	28174	S21-1	Introduction	42	48	0.56	0.188976377952756	A word was considered complex if at least one of the annotators found it to be difficult.	0
28175	28175	S21-1	Introduction	43	49	0.573333333333333	0.192913385826772	The training dataset consisted of 2,237 instances, each labelled by 20 annotators and the test dataset had 88,221 instances, each labelled by 1 annotator (Paetzold and Specia, 2016a).	0
28176	28176	S21-1	Introduction	44	50	0.586666666666667	0.196850393700787	The participating systems leveraged lexical features (Choubey and Pateria, 2016;	0
28177	28177	S21-1	Introduction	45	51	0.6	0.200787401574803	Bingel et al., 2016;Quijada and Medero, 2016) and word embeddings (Kuru, 2016;S.	0
28178	28178	S21-1	Introduction	46	52	0.613333333333333	0.204724409448819	P et al., 2016;	0
28179	28179	S21-1	Introduction	47	53	0.626666666666667	0.208661417322835	Gillin, 2016), as well as finding that frequency features, such as those taken from Wikipedia (Konkol, 2016;	0
28180	28180	S21-1	Introduction	48	54	0.64	0.21259842519685	Wrbel, 2016) were useful.	0
28181	28181	S21-1	Introduction	49	55	0.653333333333333	0.216535433070866	Systems used binary classifiers such as SVMs (Kuru, 2016;S.	0
28182	28182	S21-1	Introduction	50	56	0.666666666666667	0.220472440944882	P et al., 2016;	0
28183	28183	S21-1	Introduction	51	57	0.68	0.224409448818898	Choubey and Pateria, 2016), Decision Trees (Choubey and Pateria, 2016;	0
28184	28184	S21-1	Introduction	52	58	0.693333333333333	0.228346456692913	Quijada and Medero, 2016;Malmasi et al., 2016), Random Forests (Ronzano et al., 2016	0
28185	28185	S21-1	Introduction	53	59	0.706666666666667	0.232283464566929	Brooke et al., 2016;	0
28186	28186	S21-1	Introduction	54	60	0.72	0.236220472440945	Mukherjee et al., 2016) and thresholdbased metrics (Kauchak, 2016;	0
28187	28187	S21-1	Introduction	55	61	0.733333333333333	0.240157480314961	Wrbel, 2016) to predict the complexity labels.	0
28188	28188	S21-1	Introduction	56	62	0.746666666666667	0.244094488188976	The winning system made use of threshold-based methods and features extracted from Simple Wikipedia (Paetzold and Specia, 2016b).	0
28189	28189	S21-1	Introduction	57	63	0.76	0.248031496062992	A post-competition analysis (Zampieri et al., 2017) with oracle and ensemble methods showed that most systems performed poorly due mostly to the way in which the data was annotated and the the small size of the training dataset.	0
28190	28190	S21-1	Introduction	58	64	0.773333333333333	0.251968503937008	CWI 2018 at BEA	0
28191	28191	S21-1	Introduction	59	65	0.786666666666667	0.255905511811024	The second CWI Shared Task was organized at the BEA workshop 2018 (Yimam et al., 2018).	0
28192	28192	S21-1	Introduction	60	66	0.8	0.259842519685039	Unlike the first task, this second task had two objectives.	0
28193	28193	S21-1	Introduction	61	67	0.813333333333333	0.263779527559055	The first objective was the binary complex or non-complex classification of target words.	0
28194	28194	S21-1	Introduction	62	68	0.826666666666667	0.267716535433071	The second objective was regression or probabilistic classification in which 13 teams were asked to assign the probability of a target word being considered complex by a set of language learners.	0
28195	28195	S21-1	Introduction	63	69	0.84	0.271653543307087	A major difference in this second task was that datasets of differing genres: (TEXT GENRES) as well as English, German and Spanish datasets for monolingual speakers and a French dataset for multilingual speakers were provided (Yimam et al., 2018).	0
28196	28196	S21-1	Introduction	64	70	0.853333333333333	0.275590551181102	Similar to 2016, systems made use of a variety of lexical features including word length (Wani et al., 2018;	0
28197	28197	S21-1	Introduction	65	71	0.866666666666667	0.279527559055118	De Hertog and Tack, 2018;	0
28198	28198	S21-1	Introduction	66	72	0.88	0.283464566929134	AbuRa'ed and Saggion, 2018;Hartmann and dos Santos, 2018;	0
28199	28199	S21-1	Introduction	67	73	0.893333333333333	0.28740157480315	Alfter and Piln, 2018;Kajiwara and Komachi, 2018), frequency (De Hertog and Tack, 2018;	0
28200	28200	S21-1	Introduction	68	74	0.906666666666667	0.291338582677165	Aroyehun et al., 2018;	0
28201	28201	S21-1	Introduction	69	75	0.92	0.295275590551181	Alfter and Piln, 2018;Kajiwara and Komachi, 2018), N-gram features (Gooding and Kochmar, 2018;	0
28202	28202	S21-1	Introduction	70	76	0.933333333333333	0.299212598425197	Popovi, 2018;Hartmann and dos Santos, 2018;	0
28203	28203	S21-1	Introduction	71	77	0.946666666666667	0.303149606299213	Alfter and Piln, 2018;Butnaru and Ionescu, 2018) and word embeddings (De Hertog and Tack, 2018;	0
28204	28204	S21-1	Introduction	72	78	0.96	0.307086614173228	AbuRa'ed and Saggion, 2018;Aroyehun et al., 2018;Butnaru and Ionescu, 2018).	0
28205	28205	S21-1	Introduction	73	79	0.973333333333333	0.311023622047244	A variety of classifiers were used ranging from traditional machine learning classifiers (Gooding and Kochmar, 2018;	0
28206	28206	S21-1	Introduction	74	80	0.986666666666667	0.31496062992126	Popovi, 2018;AbuRa'ed andSaggion, 2018), to Neural Networks (De Hertog andAroyehun et al., 2018).	0
28207	28207	S21-1	Introduction	75	81	1.0	0.318897637795276	The winning system made use of Adaboost with WordNet features, POS tags, dependency parsing relations and psycholinguistic features (Gooding and Kochmar, 2018).	0
28382	28382	S21-2	abstract	1	2	0.142857142857143	0.008510638297872	In this paper, we introduce the first SemEval task on Multilingual and Cross-Lingual Wordin-Context disambiguation (MCL-WiC).	0
28383	28383	S21-2	abstract	2	3	0.285714285714286	0.012765957446809	This task allows the largely under-investigated inherent ability of systems to discriminate between word senses within and across languages to be evaluated, dropping the requirement of a fixed sense inventory.	0
28384	28384	S21-2	abstract	3	4	0.428571428571429	0.017021276595745	Framed as a binary classification, our task is divided into two parts.	0
28385	28385	S21-2	abstract	4	5	0.571428571428571	0.021276595744681	In the multilingual sub-task, participating systems are required to determine whether two target words, each occurring in a different context within the same language, express the same meaning or not.	1
28386	28386	S21-2	abstract	5	6	0.714285714285714	0.025531914893617	Instead, in the crosslingual part, systems are asked to perform the task in a cross-lingual scenario, in which the two target words and their corresponding contexts are provided in two different languages.	1
28387	28387	S21-2	abstract	6	7	0.857142857142857	0.029787234042553	We illustrate our task, as well as the construction of our manually-created dataset including five languages, namely Arabic, Chinese, English, French and Russian, and the results of the participating systems.	0
28388	28388	S21-2	abstract	7	8	1.0	0.034042553191489	Datasets and results are available at: https://github.com/ SapienzaNLP/mcl-wic.	0
28617	28617	S21-4	abstract	1	2	0.111111111111111	0.007662835249042	This paper introduces the SemEval-2021 shared task 4: Reading Comprehension of Abstract Meaning (ReCAM).	0
28618	28618	S21-4	abstract	2	3	0.222222222222222	0.011494252873563	This shared task is designed to help evaluate the ability of machines in representing and understanding abstract concepts.	0
28619	28619	S21-4	abstract	3	4	0.333333333333333	0.015325670498084	Given a passage and the corresponding question, a participating system is expected to choose the correct answer from five candidates of abstract concepts in a cloze-style machine reading comprehension setup.	1
28620	28620	S21-4	abstract	4	5	0.444444444444444	0.019157088122605	Based on two typical definitions of abstractness, i.e., the imperceptibility and nonspecificity, our task provides three subtasks to evaluate the participating models.	0
28621	28621	S21-4	abstract	5	6	0.555555555555556	0.022988505747127	Specifically, Subtask 1 aims to evaluate how well a system can model concepts that cannot be directly perceived in the physical world.	0
28622	28622	S21-4	abstract	6	7	0.666666666666667	0.026819923371648	Subtask 2 focuses on models' ability in comprehending nonspecific concepts located high in a hypernym hierarchy given the context of a passage.	0
28623	28623	S21-4	abstract	7	8	0.777777777777778	0.030651340996169	Subtask 3 aims to provide some insights into models' generalizability over the two types of abstractness.	0
28624	28624	S21-4	abstract	8	9	0.888888888888889	0.03448275862069	During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2.	0
28625	28625	S21-4	abstract	9	10	1.0	0.038314176245211	The participating teams additionally made 29 submissions to Subtask 3.	0
28884	28884	S21-5	Introduction	1	8	0.0625	0.03305785123967	Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a).	0
28885	28885	S21-5	Introduction	2	9	0.125	0.037190082644628	Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017;	0
28886	28886	S21-5	Introduction	3	10	0.1875	0.041322314049587	Pavlopoulos et al., 2017;Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible.	0
28887	28887	S21-5	Introduction	4	11	0.25	0.045454545454546	In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic.	0
28888	28888	S21-5	Introduction	5	12	0.3125	0.049586776859504	This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems.	0
28889	28889	S21-5	Introduction	6	13	0.375	0.053719008264463	As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible.	1
28890	28890	S21-5	Introduction	7	14	0.4375	0.057851239669422	Note that a post may include no toxic span and still be marked as toxic.	0
28891	28891	S21-5	Introduction	8	15	0.5	0.06198347107438	On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts.	0
28892	28892	S21-5	Introduction	9	16	0.5625	0.066115702479339	We provided a dataset of English posts with gold annotations of toxic spans, and evaluated participating systems on a held-out test subset using character-based F1.	0
28893	28893	S21-5	Introduction	10	17	0.625	0.070247933884298	The task could be addressed as supervised sequence labeling, training on the provided posts with gold toxic spans.	0
28894	28894	S21-5	Introduction	11	18	0.6875	0.074380165289256	It could also be treated as rationale extraction (Li et al., 2016;	0
28895	28895	S21-5	Introduction	12	19	0.75	0.078512396694215	Ribeiro et al., 2016), using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.	0
28896	28896	S21-5	Introduction	13	20	0.8125	0.082644628099174	There were almost 500 individual participants, and 36 out of the 92 teams that were formed submitted reports and results that we survey here.	0
28897	28897	S21-5	Introduction	14	21	0.875	0.086776859504132	Most teams adopted the supervised sequence labeling approach.	0
28898	28898	S21-5	Introduction	15	22	0.9375	0.090909090909091	Hence, there is still scope for further work on the rationale extraction approach.	0
28899	28899	S21-5	Introduction	16	23	1.0	0.09504132231405	We also discuss other possible improvements in the definition and data of the task.	0
29120	29120	S21-6	abstract	1	2	0.2	0.009049773755656	We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems.	0
29121	29121	S21-6	abstract	2	3	0.4	0.013574660633484	The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image.	1
29122	29122	S21-6	abstract	3	4	0.6	0.018099547511312	It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set.	0
29123	29123	S21-6	abstract	4	5	0.8	0.02262443438914	The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image.	0
29124	29124	S21-6	abstract	5	6	1.0	0.027149321266968	Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.	0
29341	29341	S21-7	abstract	1	2	0.125	0.008	SemEval 2021	0
29342	29342	S21-7	abstract	2	3	0.25	0.012	Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection.	0
29343	29343	S21-7	abstract	3	4	0.375	0.016	We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70.	0
29344	29344	S21-7	abstract	4	5	0.5	0.02	Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold.	1
29345	29345	S21-7	abstract	5	6	0.625	0.024	The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models.	0
29346	29346	S21-7	abstract	6	7	0.75	0.028	Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training.	0
29347	29347	S21-7	abstract	7	8	0.875	0.032	The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task.	0
29348	29348	S21-7	abstract	8	9	1.0	0.036	We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems.	0
29639	29639	S21-8	Task Description	1	50	0.047619047619048	0.205761316872428	Meas	0
29640	29640	S21-8	Task Description	2	51	0.095238095238095	0.209876543209876	Eval is an entity recognition and semantic relation extraction task focused on finding counts and measurements, attributes of those quantities, and additional information including measured entities, properties, and measurement contexts.	1
29641	29641	S21-8	Task Description	3	52	0.142857142857143	0.213991769547325	Meas	0
29642	29642	S21-8	Task Description	4	53	0.19047619047619	0.218106995884774	MeasEval is composed of five sub-tasks that cover span extraction, classification, and relation extraction, including cross-sentence relations.	1
29643	29643	S21-8	Task Description	5	54	0.238095238095238	0.222222222222222	Given a paragraph from a scientific text:	0
29644	29644	S21-8	Task Description	6	55	0.285714285714286	0.226337448559671		0
29645	29645	S21-8	Task Description	7	56	0.333333333333333	0.230452674897119	For each paragraph of text, identify all spans containing quantities (e.g. 12 kg).	0
29646	29646	S21-8	Task Description	8	57	0.380952380952381	0.234567901234568	Quantities are treated as strings, and are not converted or normalized.	0
29647	29647	S21-8	Task Description	9	58	0.428571428571429	0.238683127572016		0
29648	29648	S21-8	Task Description	10	59	0.476190476190476	0.242798353909465	For each identified Quantity, identify the Unit of Measurement (e.g. kg), if one exists.	0
29649	29649	S21-8	Task Description	11	60	0.523809523809524	0.246913580246914	For each Quantity classify additional value Modifiers (e.g. count, range, approximate, mean, etc.) that apply to the Quantity.	0
29650	29650	S21-8	Task Description	12	61	0.571428571428571	0.251028806584362		0
29651	29651	S21-8	Task Description	13	62	0.619047619047619	0.255144032921811	For each identified Quantity, identify the Measured Entity (e.g. bed inventory) it applies to (if one exists) and mark its span.	0
29652	29652	S21-8	Task Description	14	63	0.666666666666667	0.259259259259259	If an associated Measured Property (e.g. concentration) also exists, identify it and mark its span.	0
29653	29653	S21-8	Task Description	15	64	0.714285714285714	0.263374485596708		0
29654	29654	S21-8	Task Description	16	65	0.761904761904762	0.267489711934156	Identify and mark the span of any Qualifier (e.g. after incubation) that is needed to record additional related context to either validate or understand each identified Quantity.	0
29655	29655	S21-8	Task Description	17	66	0.80952380952381	0.271604938271605	 Identify relationships between Quantity, Measured Entity, Measured Property, and Qualifier spans using the HasQuantity, HasProperty, and Qualifies relation types.	0
29656	29656	S21-8	Task Description	18	67	0.857142857142857	0.275720164609053	More detailed definitions can be found be reviewing the MeasEval Annotation Guidelines.	0
29657	29657	S21-8	Task Description	19	68	0.904761904761905	0.279835390946502	5	0
29658	29658	S21-8	Task Description	20	69	0.952380952380952	0.283950617283951	We describe each of the elements to be extracted in more detail in the next section.	0
29659	29659	S21-8	Task Description	21	70	1.0	0.288065843621399	4 Annotated Data	0
29834	29834	S21-9	abstract	1	2	0.125	0.01123595505618	Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells.	0
29835	29835	S21-9	abstract	2	3	0.25	0.01685393258427	In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS).	0
29836	29836	S21-9	abstract	3	4	0.375	0.02247191011236	Our dataset contains 981 manuallygenerated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations.	0
29837	29837	S21-9	abstract	4	5	0.5	0.02808988764045	SEM-TAB-FACTS featured two sub-tasks.	0
29838	29838	S21-9	abstract	5	6	0.625	0.033707865168539	In subtask A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table.	1
29839	29839	S21-9	abstract	6	7	0.75	0.039325842696629	In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement.	1
29840	29840	S21-9	abstract	7	8	0.875	0.044943820224719	69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B.	0
29841	29841	S21-9	abstract	8	9	1.0	0.050561797752809	We present our results and main findings from the competition.	0
30019	30019	S21-10	Introduction	1	9	0.03030303030303	0.051136363636364	Data sharing restrictions are common in NLP datasets.	0
30020	30020	S21-10	Introduction	2	10	0.060606060606061	0.056818181818182	For example, Twitter policies do not allow sharing of tweet text, though tweet IDs may be shared.	0
30021	30021	S21-10	Introduction	3	11	0.090909090909091	0.0625	The situation is even more common in clinical NLP, where patient health information must be protected, and annotations over health text, when released at all, often require the signing of complex data use agreements.	0
30022	30022	S21-10	Introduction	4	12	0.121212121212121	0.068181818181818	The Source-Free Domain Adaptation shared task presents a new framework that asks participants to develop semantic annotation systems in the face of data sharing constraints.	0
30023	30023	S21-10	Introduction	5	13	0.151515151515152	0.073863636363636	A participant's goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed.	0
30024	30024	S21-10	Introduction	6	14	0.181818181818182	0.079545454545455	Instead of annotated training data, participants are given a model trained on the annotations.	0
30025	30025	S21-10	Introduction	7	15	0.212121212121212	0.085227272727273	Then, given unlabeled target domain data, they are asked to make predictions.	0
30026	30026	S21-10	Introduction	8	16	0.242424242424242	0.090909090909091	This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016;	0
30027	30027	S21-10	Introduction	9	17	0.272727272727273	0.096590909090909	Ziser and Reichart, 2017;	0
30028	30028	S21-10	Introduction	10	18	0.303030303030303	0.102272727272727	Saito et al., 2017;Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum III, 2007;	0
30029	30029	S21-10	Introduction	11	19	0.333333333333333	0.107954545454545	Xia et al., 2013;	0
30030	30030	S21-10	Introduction	12	20	0.363636363636364	0.113636363636364	Kim et al., 2016;	0
30031	30031	S21-10	Introduction	13	21	0.393939393939394	0.119318181818182	Peng and Dredze, 2017).	0
30032	30032	S21-10	Introduction	14	22	0.424242424242424	0.125	Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition.	0
30033	30033	S21-10	Introduction	15	23	0.454545454545455	0.130681818181818	These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem.	0
30034	30034	S21-10	Introduction	16	24	0.484848484848485	0.136363636363636	Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowing us to annotate data in a new domain.	0
30035	30035	S21-10	Introduction	17	25	0.515151515151515	0.142045454545455	Negation detection is the task of identifying negation cues in text.	0
30036	30036	S21-10	Introduction	18	26	0.545454545454545	0.147727272727273	This task has been widely studied by previous work (Chapman et al., 2007(Chapman et al., , 2001; Harkema et al., 2009;	0
30037	30037	S21-10	Introduction	19	27	0.575757575757576	0.153409090909091	Sohn et al., 2012) including the development of a variety of datasets (Uzuner et al., 2011;Mehrabi et al., 2015).	0
30038	30038	S21-10	Introduction	20	28	0.606060606060606	0.159090909090909	However, there are still large performance losses in the cross-domain setting (Wu et al., 2014).	0
30039	30039	S21-10	Introduction	21	29	0.636363636363636	0.164772727272727	"For negation detection, we provided a ""spanin-context"" classification model, fine-tuned on instances of the SHARP Seed dataset of Mayo Clinic clinical notes, which the organizers have access to but cannot currently be distributed."	0
30040	30040	S21-10	Introduction	22	30	0.666666666666667	0.170454545454545	(Models were approved to be distributed, as the data is deidentified.)	0
30041	30041	S21-10	Introduction	23	31	0.696969696969697	0.176136363636364	In the SHARP data, clinical events are marked with a boolean polarity indicator, with values of either asserted or negated.	0
30042	30042	S21-10	Introduction	24	32	0.727272727272727	0.181818181818182	As development   data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners Health-Care.	0
30043	30043	S21-10	Introduction	25	33	0.757575757575758	0.1875	The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016).	0
30044	30044	S21-10	Introduction	26	34	0.787878787878788	0.193181818181818	Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013).	0
30045	30045	S21-10	Introduction	27	35	0.818181818181818	0.198863636363636	For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in Sem-Eval 2018 Task 6 (Laparra et al., 2018).	0
30046	30046	S21-10	Introduction	28	36	0.848484848484848	0.204545454545455	As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019).	0
30047	30047	S21-10	Introduction	29	37	0.878787878787879	0.210227272727273	For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary.	0
30048	30048	S21-10	Introduction	30	38	0.909090909090909	0.215909090909091	(Models were approved to be distributed, as the data is deidentified.)	0
30049	30049	S21-10	Introduction	31	39	0.939393939393939	0.221590909090909	The development data was the annotated news portion of the SemEval 2018 Task 6 data whose source text is from the freely available TimeBank.	0
30050	30050	S21-10	Introduction	32	40	0.96969696969697	0.227272727272727	For evaluation, we used a set of annotated documents extracted from food security warning systems.	0
30051	30051	S21-10	Introduction	33	41	1.0	0.232954545454545	The main impact of this task is to drive the NLP community to address the serious challenges of data sharing constraints by designing new domain adaptation algorithms that allow source data and target data to remain separate, rather than assuming they can be shared freely with each other.	1
30289	30289	S21-11	Task Description	1	103	0.035714285714286	0.356401384083045	Our comprehensive NCG Shared Task formalism was as follows.	0
30290	30290	S21-11	Task Description	2	104	0.071428571428572	0.359861591695502	Given a scholarly article A in plaintext format, the goal was to extract (1) a set of contribution sentences C sent = {C sent 1 , ..., C sent N }, (2) a set of scientific knowledge terms and predicates from C sent referred to as entities E = {e 1 , ..., e N }, and (3) to organize the entities E as a set of (subject,predicate,object) triple statements T = {t 1 , ..., t N } toward KG building organized under three or more of the 12 total IUs.	1
30291	30291	S21-11	Task Description	3	105	0.107142857142857	0.363321799307958	Task Evaluation Phases.	0
30292	30292	S21-11	Task Description	4	106	0.142857142857143	0.366782006920415	The task comprised three evaluation phases, thereby enabling detailed system evaluations.	0
30293	30293	S21-11	Task Description	5	107	0.178571428571429	0.370242214532872	Evaluation Phase 1: End-to-end Pipeline.	0
30294	30294	S21-11	Task Description	6	108	0.214285714285714	0.373702422145329	In this phase, systems were tested for the comprehensive end-to-end KG building task described in the formalism above.	0
30295	30295	S21-11	Task Description	7	109	0.25	0.377162629757785	Given a test set of articles A in plaintext format, the participating systems were expected to return: (1) a set of contribution sentences C sent , (2) a set of scientific knowledge terms and predicates from C sent , i.e. entities E, and (3) the entities in E organized in a set of triple statements T toward KG building.	0
30296	30296	S21-11	Task Description	8	110	0.285714285714286	0.380622837370242	System outputs were evaluated for the three aspects and overall.	0
30297	30297	S21-11	Task Description	9	111	0.321428571428571	0.384083044982699	Evaluation Phase 2, Part 1: Phrases and Triples.	0
30298	30298	S21-11	Task Description	10	112	0.357142857142857	0.387543252595156	In this phase, systems were tested only for their capacity to extract phrases and organize them as triples.	0
30299	30299	S21-11	Task Description	11	113	0.392857142857143	0.391003460207612	Given a test set of articles A in plain-text format and contribution sentences C sent from each article, each system was expected to return: (1) the entities E, and (2) the set of triple statements T .	0
30300	30300	S21-11	Task Description	12	114	0.428571428571429	0.394463667820069	Evaluation Phase 2, Part 2: Triples.	0
30301	30301	S21-11	Task Description	13	115	0.464285714285714	0.397923875432526	In this phase, systems were tested only for the triples formation task.	0
30302	30302	S21-11	Task Description	14	116	0.5	0.401384083044983	Thus, given gold entities E for the set of C sent , systems were expected to form triple statements T .	0
30303	30303	S21-11	Task Description	15	117	0.535714285714286	0.404844290657439	In the Evaluation phases that lasted from Jan 10 till Feb 1, 2021, we provided the participants with masked versions of the test set based on the current evaluation phase.	0
30304	30304	S21-11	Task Description	16	118	0.571428571428571	0.408304498269896	The test set annotations in each phase were uploaded to CodaLab and were not available to the participants.	0
30305	30305	S21-11	Task Description	17	119	0.607142857142857	0.411764705882353	To obtain results, the participants were expected to upload their system outputs to Codalab where they were automatically evaluated by our script and reference data stored on the platform.	0
30306	30306	S21-11	Task Description	18	120	0.642857142857143	0.41522491349481	In each evaluation phase, teams were restricted to make only 10 submissions and only one result, i.e. the top-scoring result, was shown on the leaderboard.	0
30307	30307	S21-11	Task Description	19	121	0.678571428571429	0.418685121107266	Before the task began, our participants were onboarded via our task website https://ncg-task.github.io/.	0
30308	30308	S21-11	Task Description	20	122	0.714285714285714	0.422145328719723	Further, participants were encouraged to discuss their task-related questions via our task Google groups page at https://groups.google.com/forum/#! forum/ncg-task-semeval-2021.	0
30309	30309	S21-11	Task Description	21	123	0.75	0.42560553633218	The NCG Data Collection of Articles	0
30310	30310	S21-11	Task Description	22	124	0.785714285714286	0.429065743944637	Our base collection of scholarly articles was downloaded from the publicly available leaderboard of tasks in AI called https://paperswithcode.com/.	0
30311	30311	S21-11	Task Description	23	125	0.821428571428571	0.432525951557093	While paperswithcode predominantly represents the NLP and Computer Vision research fields in AI, we restricted ourselves just to its NLP papers.	0
30312	30312	S21-11	Task Description	24	126	0.857142857142857	0.43598615916955	From their overall collection of articles, the tasks and articles in our final data were randomly selected.	0
30313	30313	S21-11	Task Description	25	127	0.892857142857143	0.439446366782007	The raw articles' pdfs needed to undergo a two-step preprocessing before the annotation task.	0
30314	30314	S21-11	Task Description	26	128	0.928571428571429	0.442906574394464	1) For pdf-to-text conversion, the GROBID parser (GRO, 2008(GRO, -2020 was applied; following which, 2) for plaintext pre-processing in terms of tokenization and sentence splitting, the Stanza toolkit (Qi et al., 2020) was used.	0
30315	30315	S21-11	Task Description	27	129	0.964285714285714	0.44636678200692	The resulting pre-processed articles could then be annotated in plaintext format.	0
30316	30316	S21-11	Task Description	28	130	1.0	0.449826989619377	Note, our data consists of articles in English.	0
30477	30477	S21-12	abstract	1	2	0.25	0.010526315789474	Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision.	0
30478	30478	S21-12	abstract	2	3	0.5	0.015789473684211	However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization.	0
30479	30479	S21-12	abstract	3	4	0.75	0.021052631578947	The aim of the SemEval-2021 shared task on Learning with Disagreements (L --D ) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images.	1
30480	30480	S21-12	abstract	4	5	1.0	0.026315789473684	In this paper we describe the shared task and its results.	0
30899	30899	W18-2409	Shared Task Description	1	36	0.076923076923077	0.266666666666667	As in previous editions of the workshop series, the shared task in NEWS 2018 consists of developing machine transliteration systems in one or more of the specified language pairs.	0
30900	30900	W18-2409	Shared Task Description	2	37	0.153846153846154	0.274074074074074	Each language pair of the shared task consists of a source and a target language, implicitly specifying the transliteration direction.	0
30901	30901	W18-2409	Shared Task Description	3	38	0.230769230769231	0.281481481481481	Training and development data in each of the language pairs was made available to all registered participants for developing their transliteration systems.	0
30902	30902	W18-2409	Shared Task Description	4	39	0.307692307692308	0.288888888888889	At the evaluation time, hand-crafted test sets of source names were released to the participants, who were required to produce a ranked list of transliteration candidates in the target language for each source name.	1
30903	30903	W18-2409	Shared Task Description	5	40	0.384615384615385	0.296296296296296	The system outputs were tested against their corresponding reference sets (which may include multiple correct transliterations for some source names).	0
30904	30904	W18-2409	Shared Task Description	6	41	0.461538461538462	0.303703703703704	The performance of a system is quantified using multiple metrics (defined in Section 3).	0
30905	30905	W18-2409	Shared Task Description	7	42	0.538461538461538	0.311111111111111	In this edition of the workshop, only standard runs (restricted to the train and development data provided) were considered.	0
30906	30906	W18-2409	Shared Task Description	8	43	0.615384615384615	0.318518518518519	No other data or linguistic resources were allowed for standard runs.	0
30907	30907	W18-2409	Shared Task Description	9	44	0.692307692307692	0.325925925925926	This ensures parity between systems and enables meaningful comparison of performance of various algorithmic approaches in a given language pair.	0
30908	30908	W18-2409	Shared Task Description	10	45	0.769230769230769	0.333333333333333	Participants were allowed to submit one or more standard runs for each task they participated in.	0
30909	30909	W18-2409	Shared Task Description	11	46	0.846153846153846	0.340740740740741	"If more than one standard runs were submitted, it was required to select one as the ""primary"" run by publishing it into the leaderboard."	0
30910	30910	W18-2409	Shared Task Description	12	47	0.923076923076923	0.348148148148148	The primary runs are the ones used to compare results across different systems.	0
30911	30911	W18-2409	Shared Task Description	13	48	1.0	0.355555555555556	The NEWS 2018 Shared Task was run on Co-daLab (http://codalab.org/).	0
31000	31000	W18-3219	abstract	1	2	0.166666666666667	0.009569377990431	In the third shared task of the Computational Approaches to Linguistic Code-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on code-switched social-media data.	1
31001	31001	W18-3219	abstract	2	3	0.333333333333333	0.014354066985646	We divide the shared task into two competitions based on the English-Spanish (ENG-SPA) and Modern Standard Arabic-Egyptian (MSA-EGY) language pairs.	1
31002	31002	W18-3219	abstract	3	4	0.5	0.019138755980861	We use Twitter data and 9 entity types to establish a new dataset for code-switched NER benchmarks.	1
31003	31003	W18-3219	abstract	4	5	0.666666666666667	0.023923444976077	In addition to the CS phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process.	0
31004	31004	W18-3219	abstract	5	6	0.833333333333333	0.028708133971292	As a result, the best scores of the competitions are 63.76% and 71.61% for ENG-SPA and MSA-EGY, respectively.	0
31005	31005	W18-3219	abstract	6	7	1.0	0.033492822966507	We present the scores of 9 participants and discuss the most common challenges among submissions.	0
31398	31398	W18-3706	abstract	1	2	0.2	0.016949152542373	This paper presents the NLPTEA 2018 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as foreign language.	1
31399	31399	W18-3706	abstract	2	3	0.4	0.025423728813559	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
31400	31400	W18-3706	abstract	3	4	0.6	0.033898305084746	Of the 20 teams registered for this shared task, 13 teams developed the system and submitted a total of 32 runs.	0
31401	31401	W18-3706	abstract	4	5	0.8	0.042372881355932	Progress in system performances was obviously, reaching F1 of 36.12% in position level and 25.27% in correction level.	0
31402	31402	W18-3706	abstract	5	6	1.0	0.050847457627119	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
31666	31666	W19-3801	Task	1	22	0.1	0.201834862385321	The goal of our shared task was to encourage research in gender-fair models for NLP by providing a well-defined task that is known to be sensitive to gender bias and an evaluation procedure addressing this issue.	0
31667	31667	W19-3801	Task	2	23	0.2	0.211009174311927	We chose the GAP resolution task (Webster et al., 2018), which measures the ability of systems to resolve gendered pronoun reference from real-world contexts in a gender-fair way.	1
31668	31668	W19-3801	Task	3	24	0.3	0.220183486238532	Specifically, GAP asks systems to resolve a target personal pronoun to one of two names, or neither name.	0
31669	31669	W19-3801	Task	4	25	0.4	0.229357798165138	For instance, a perfect resolver would resolve that she refers to Fujisawa and not to Mari Motohashi in the Wikipedia excerpt:	0
31670	31670	W19-3801	Task	5	26	0.5	0.238532110091743	(1)	0
31671	31671	W19-3801	Task	6	27	0.6	0.247706422018349	In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.	0
31672	31672	W19-3801	Task	7	28	0.7	0.256880733944954	The original GAP challenge encourages fairness by balancing its datasets by the gender of the pronoun, as well as using disaggregated evaluation with separate scores for masculine and feminine examples.	0
31673	31673	W19-3801	Task	8	29	0.8	0.26605504587156	To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.	0
31674	31674	W19-3801	Task	9	30	0.9	0.275229357798165	1	0
31675	31675	W19-3801	Task	10	31	1.0	0.284403669724771	The competition was run on Kaggle 2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support.	0
31776	31776	W19-4107	Task	1	23	0.111111111111111	0.172932330827068	This task pushed the state-of-the-art in goaloriented dialogue systems in four directions deemed necessary for practical automated agents, using two new datasets.	0
31777	31777	W19-4107	Task	2	24	0.222222222222222	0.180451127819549	We sidestepped the challenge of evaluating generated utterances by formulating the problem as next utterance selection, as proposed by Lowe et al. (2015).	0
31778	31778	W19-4107	Task	3	25	0.333333333333333	0.18796992481203	At test time, participants were provided with partial conversations, each paired with a set of utterances that could be the next utterance in the conversation.	1
31779	31779	W19-4107	Task	4	26	0.444444444444444	0.195488721804511	Systems needed to rank these options, with the goal of placing the true utterance first.	1
31780	31780	W19-4107	Task	5	27	0.555555555555556	0.203007518796992	Prior work used sets of 2 or 10 utterances.	0
31781	31781	W19-4107	Task	6	28	0.666666666666667	0.210526315789474	We make the task harder by expanding the size of the sets, and considered several advanced variations: Subtask 1 100 candidates, including 1 correct option.	0
31782	31782	W19-4107	Task	7	29	0.777777777777778	0.218045112781955	Subtask 2 120,000 candidates, including 1 correct option (Ubuntu data only).	0
31783	31783	W19-4107	Task	8	30	0.888888888888889	0.225563909774436	Subtask 3 100 candidates, including 1-5 correct options that are paraphrases (Advising data only).	0
31784	31784	W19-4107	Task	9	31	1.0	0.233082706766917	Subtask 4 100 candidates, including 0-1 correct options.	0
31893	31893	W19-4226	Introduction	1	7	0.083333333333333	0.038251366120219	While producing a sentence, humans combine various types of knowledge to produce fluent outputvarious shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology.	0
31894	31894	W19-4226	Introduction	2	8	0.166666666666667	0.043715846994536	Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear.	0
31895	31895	W19-4226	Introduction	3	9	0.25	0.049180327868853	Automatic systems must also consider these constraints when constructing or processing language.	0
31896	31896	W19-4226	Introduction	4	10	0.333333333333333	0.054644808743169	Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology.	0
31897	31897	W19-4226	Introduction	5	11	0.416666666666667	0.060109289617486	Many languages implement large inflectional paradigms that mark both function and content words with a varying levels of morphosyntactic information.	0
31898	31898	W19-4226	Introduction	6	12	0.5	0.065573770491803	* Now at Google For instance, Romanian verb forms inflect for person, number, tense, mood, and voice; meanwhile, Archi verbs can take on thousands of forms (Kibrik, 1998).	0
31899	31899	W19-4226	Introduction	7	13	0.583333333333333	0.07103825136612	Such complex paradigms produce large inventories of words, all of which must be producible by a realistic system, even though a large percentage of them will never be observed over billions of lines of linguistic input.	0
31900	31900	W19-4226	Introduction	8	14	0.666666666666667	0.076502732240437	Compounding the issue, good inflectional systems often require large amounts of supervised training data, which is infeasible in many of the world's languages.	0
31901	31901	W19-4226	Introduction	9	15	0.75	0.081967213114754	This year's shared task is concentrated on encouraging the construction of strong morphological systems that perform two related but different inflectional tasks.	0
31902	31902	W19-4226	Introduction	10	16	0.833333333333333	0.087431693989071	The first task asks participants to create morphological inflectors for a large number of under-resourced languages, encouraging systems that use highly-resourced, related languages as a cross-lingual training signal.	1
31903	31903	W19-4226	Introduction	11	17	0.916666666666667	0.092896174863388	The second task welcomes submissions that invert this operation in light of contextual information: Given an unannotated sentence, lemmatize each word, and tag them with a morphosyntactic description.	1
31904	31904	W19-4226	Introduction	12	18	1.0	0.098360655737705	Both of these tasks extend upon previous morphological competitions, and the best submitted systems now represent the state of the art in their respective tasks.	0
32104	32104	W19-4622	Task Description	1	35	1.0	0.28	The MADAR Shared Task included two subtasks: the MADAR Travel Domain Dialect Identification subtask, and the MADAR Twitter User Dialect Identification subtask.	1
32196	32196	W19-5302	abstract	1	2	0.166666666666667	0.006896551724138	This paper presents the results of the WMT19 Metrics Shared Task.	0
32197	32197	W19-5302	abstract	2	3	0.333333333333333	0.010344827586207	Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics.	1
32198	32198	W19-5302	abstract	3	4	0.5	0.013793103448276	"13 research groups submitted 24 metrics, 10 of which are reference-less ""metrics"" and constitute submissions to the joint task with WMT19 Quality Estimation Task, ""QE as a Metric""."	0
32199	32199	W19-5302	abstract	4	5	0.666666666666667	0.017241379310345	In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF).	0
32200	32200	W19-5302	abstract	5	6	0.833333333333333	0.020689655172414	Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality.	0
32201	32201	W19-5302	abstract	6	7	1.0	0.024137931034483	This year, we use direct assessment (DA) as our only form of manual evaluation.	0
32489	32489	W19-5404	Introduction	1	5	0.0625	0.020242914979757	Machine Translation (MT) has experienced significant advances in recent years thanks to improvements in modeling, and in particular neural models (Bahdanau et al., 2015;	0
32490	32490	W19-5404	Introduction	2	6	0.125	0.024291497975709	Gehring et al., 2016;Vaswani et al., 2017).	0
32491	32491	W19-5404	Introduction	3	7	0.1875	0.02834008097166	Unfortunately, today's neural machine translation models, perform poorly on low-resource language pairs, for which clean, parallel training data is high-quality training data is lacking, by definition (Koehn and Knowles, 2017).	0
32492	32492	W19-5404	Introduction	4	8	0.25	0.032388663967611	Improving performance on low resource language pairs is very impactful considering that these languages are spoken by a large fraction of the world population.	0
32493	32493	W19-5404	Introduction	5	9	0.3125	0.036437246963563	This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base.	0
32494	32494	W19-5404	Introduction	6	10	0.375	0.040485829959514	In face of the scarcity of clean parallel data, learning to translate from any multilingual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl 1 ) is an important option.	0
32495	32495	W19-5404	Introduction	7	11	0.4375	0.044534412955466	1 http://www.paracrawl.eu/	0
32496	32496	W19-5404	Introduction	8	12	0.5	0.048582995951417	Recently, there is an increased interest in the filtering of noisy parallel corpora to increase the amount of data that can be used to train translation systems .	0
32497	32497	W19-5404	Introduction	9	13	0.5625	0.052631578947369	While the state-of-the-art methods that use NMT models have proven effective in mining parallel sentences (Junczys-Dowmunt, 2018) for high-resource languages, their effectiveness has not been tested in low-resource languages.	0
32498	32498	W19-5404	Introduction	10	14	0.625	0.05668016194332	The implications of low availability of training data for parallel-scoring methods is not known yet.	0
32499	32499	W19-5404	Introduction	11	15	0.6875	0.060728744939271	The Shared Task on Parallel Corpus Filtering at the Conference for Machine Translation (WMT 2019) was organized to promote research to learning from noisy data more viable for low-resource languages.	0
32500	32500	W19-5404	Introduction	12	16	0.75	0.064777327935223	Compared to last year's edition , we only provide about 50-60 million word noisy parallel data, as opposed to 1 billion words.	0
32501	32501	W19-5404	Introduction	13	17	0.8125	0.068825910931174	We also provide only a few million words of clean parallel data of varying quality, instead of over 100 million words of high-quality parallel data.	0
32502	32502	W19-5404	Introduction	14	18	0.875	0.072874493927126	Participants developed methods to filter web-crawled Nepali-English and Sinhala-English parallel corpora by assigning a quality score for each sentence pair.	1
32503	32503	W19-5404	Introduction	15	19	0.9375	0.076923076923077	These scores are used to filter the web crawled corpora down to fixed sizes (1 million and 5 million English words), trained statistical and neural machine translation systems on these subsets, and measured their quality with the BLEU score on a test set of multi-domain Wikipedia content .	0
32504	32504	W19-5404	Introduction	16	20	1.0	0.080971659919028	This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes and the average sentence length of sub-selected data.	0
33100	33100	2020.sigmorphon-1.2	abstract	1	2	0.25	0.010362694300518	We describe the design and findings of the SIGMORPHON 2020 shared task on multilingual grapheme-to-phoneme conversion.	0
33101	33101	2020.sigmorphon-1.2	abstract	2	3	0.5	0.015544041450777	Participants were asked to submit systems which consume a sequence of graphemes then emit output a sequence of phonemes representing the pronunciation of that grapheme sequence in one of fifteen languages.	1
33102	33102	2020.sigmorphon-1.2	abstract	3	4	0.75	0.020725388601036	Nine teams submitted a total of 23 systems, at best achieving an 18% relative reduction in word error rate (macro-averaged over languages), versus strong neural sequence-to-sequence baselines.	0
33103	33103	2020.sigmorphon-1.2	abstract	4	5	1.0	0.025906735751295	To facilitate error analysis, we publicly release the complete outputs for all systems-a first for the SIGMORPHON workshop.	0
33293	33293	2020.sigmorphon-1.3	abstract	1	2	0.111111111111111	0.008620689655172	In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology.	0
33294	33294	2020.sigmorphon-1.3	abstract	2	3	0.222222222222222	0.012931034482759	Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma.	1
33295	33295	2020.sigmorphon-1.3	abstract	3	4	0.333333333333333	0.017241379310345	In order to simulate a realistic use case, we first released data for 5 development languages.	0
33296	33296	2020.sigmorphon-1.3	abstract	4	5	0.444444444444444	0.021551724137931	However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline.	0
33297	33297	2020.sigmorphon-1.3	abstract	5	6	0.555555555555556	0.025862068965517	We provided a modular baseline system, which is a pipeline of 4 components.	0
33298	33298	2020.sigmorphon-1.3	abstract	6	7	0.666666666666667	0.030172413793104	3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages.	0
33299	33299	2020.sigmorphon-1.3	abstract	7	8	0.777777777777778	0.03448275862069	Only on 3 languages did a submitted system obtain the best results.	0
33300	33300	2020.sigmorphon-1.3	abstract	8	9	0.888888888888889	0.038793103448276	This shows that unsupervised morphological paradigm completion is still largely unsolved.	0
33301	33301	2020.sigmorphon-1.3	abstract	9	10	1.0	0.043103448275862	We present an analysis here, so that this shared task will ground further research on the topic.	0
33525	33525	2021.case-1.23	abstract	1	2	0.2	0.010416666666667	This paper describes the Shared Task on Fine-grained Event Classification in News-like Text Snippets.	0
33526	33526	2021.case-1.23	abstract	2	3	0.4	0.015625	The Shared Task is divided into three subtasks: (a) classification of text snippets reporting socio-political events (25 classes) for which vast amount of training data exists, although exhibiting different structure and style vis-a-vis test data, (b) enhancement to a generalized zero-shot learning problem, where 3 additional event types were introduced in advance, but without any training data ('unseen' classes), and (c) further extension, which introduced 2 additional event types, announced shortly prior to the evaluation phase.	1
33527	33527	2021.case-1.23	abstract	3	4	0.6	0.020833333333333	The reported Shared Task focuses on classification of events in English texts and is organized as part of the Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), co-located with the ACL-IJCNLP 2021 Conference.	0
33528	33528	2021.case-1.23	abstract	4	5	0.8	0.026041666666667	Four teams participated in the task.	0
33529	33529	2021.case-1.23	abstract	5	6	1.0	0.03125	Best performing systems for the three aforementioned subtasks achieved 83.9%, 79.7% and 77.1% weighted F 1 scores respectively.	0
33718	33718	2021.dialdoc-1.1	abstract	1	3	0.142857142857143	0.025210084033614	We present the results of Shared Task at Workshop Dial	0
33719	33719	2021.dialdoc-1.1	abstract	2	4	0.285714285714286	0.033613445378151	Doc 2021 that is focused on document-grounded dialogue and conversational question answering.	0
33720	33720	2021.dialdoc-1.1	abstract	3	5	0.428571428571429	0.042016806722689	The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that can identify the most relevant knowledge in the associated document for generating agent responses in natural language.	1
33721	33721	2021.dialdoc-1.1	abstract	4	6	0.571428571428571	0.050420168067227	It includes two subtasks on predicting agent responses: the first subtask is to predict the grounding text span in the given document for next agent response; the second subtask is to generate agent response in natural language given the context.	1
33722	33722	2021.dialdoc-1.1	abstract	5	7	0.714285714285714	0.058823529411765	Many submissions outperform baseline significantly.	0
33723	33723	2021.dialdoc-1.1	abstract	6	8	0.857142857142857	0.067226890756303	For the first task, the best-performing system achieved 67.1 Exact Match and 76.3 F1.	0
33724	33724	2021.dialdoc-1.1	abstract	7	9	1.0	0.07563025210084	For the second subtask, the best system achieved 41.1 SacreBLEU and highest rank by human evaluation.	0
33880	33880	2021.sigmorphon-1.13	Task definition	1	46	1.0	0.242105263157895	In this task, participants were provided with a collection of words and their pronunciations, and then scored on their ability to predict the pronunciation of a set of unseen words.	1
34029	34029	2021.unimplicit-1.4	Introduction	1	5	0.111111111111111	0.045871559633028	The goal of this shared task is to evaluate the ability of NLP systems to detect whether a sentence from an instructional text requires clarification.	0
34030	34030	2021.unimplicit-1.4	Introduction	2	6	0.222222222222222	0.055045871559633	Such clarifications can be critical to ensure that instructions are clear enough to be followed and the desired goal can be reached.	0
34031	34031	2021.unimplicit-1.4	Introduction	3	7	0.333333333333333	0.064220183486239	We set up this task as a binary classification task, in which systems have to predict whether a given sentence in context requires clarification.	1
34032	34032	2021.unimplicit-1.4	Introduction	4	8	0.444444444444444	0.073394495412844	Our data is based on texts for which revision histories exist, making it possible to identify (a) sentences that received edits which made the sentence more precise, and (b) sentences that remained unchanged over multiple text revisions.	0
34033	34033	2021.unimplicit-1.4	Introduction	5	9	0.555555555555556	0.08256880733945	The task of predicting revision requirements in instructional texts was originally proposed by Bhat et al. (2020), who attempted to predict whether a given sentence will be edited according to an article's revision history.	0
34034	34034	2021.unimplicit-1.4	Introduction	6	10	0.666666666666667	0.091743119266055	The shared task follows this setup, with two critical differences:	0
34035	34035	2021.unimplicit-1.4	Introduction	7	11	0.777777777777778	0.100917431192661	First, we apply a set of rules to identify a subset of edits that provide clarifying information.	0
34036	34036	2021.unimplicit-1.4	Introduction	8	12	0.888888888888889	0.110091743119266	This makes it possible to focus mainly on those edits that are related to implicit and underspecified language, excluding grammar corrections and other edit types.	0
34037	34037	2021.unimplicit-1.4	Introduction	9	13	1.0	0.119266055045872	Since the need for such edits may depend on discourse context, a second difference is that we provide context for each sentence to be classified (see Table 1).	0
34135	34135	W00-0726	abstract	1	2	0.5	0.016806722689076	We describe the CoNLL-2000 shared task: dividing text into syntactically related nonoverlapping groups of words, so-called text chunking.	1
34136	34136	W00-0726	abstract	2	3	1.0	0.025210084033614	We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.	0
34264	34264	W01-0708	Task description	1	12	0.076923076923077	0.115384615384615	Defining clause boundaries is not trivial (Leffa, 1998).	0
34265	34265	W01-0708	Task description	2	13	0.153846153846154	0.125	In this task, the gold standard clause segmentation is provided by the Penn Treebank (Marcus et al., 1993).	0
34266	34266	W01-0708	Task description	3	14	0.230769230769231	0.134615384615385	The guidelines of the Penn Treebank describe in detail how sentences are segmented into clauses (Bies et al., 1995).	0
34267	34267	W01-0708	Task description	4	15	0.307692307692308	0.144230769230769	Here is an example of a sentence and its clauses obtained from Wall Street Journal section 15 of the Penn Treebank (Marcus et al., 1993):	0
34268	34268	W01-0708	Task description	5	16	0.384615384615385	0.153846153846154	(S Coach them in (S-NOM handling complaints) (SBAR-PRP so that (S they can resolve problems immediately) )	0
34269	34269	W01-0708	Task description	6	17	0.461538461538462	0.163461538461538	. )	0
34270	34270	W01-0708	Task description	7	18	0.538461538461538	0.173076923076923	The clauses of this sentence have been enclosed between brackets.	0
34271	34271	W01-0708	Task description	8	19	0.615384615384615	0.182692307692308	A tag next to the open bracket denotes the type of the clause.	0
34272	34272	W01-0708	Task description	9	20	0.692307692307692	0.192307692307692	In the CoNLL-2001 shared task, the goal is to identify clauses in text.	0
34273	34273	W01-0708	Task description	10	21	0.769230769230769	0.201923076923077	Since clauses can be embedded in each other, this task is considerably more difficult than last year's task, recognizing non-embedded text chunks.	0
34274	34274	W01-0708	Task description	11	22	0.846153846153846	0.211538461538462	For that reason, we have disregarded type and function information of the clauses: every clause has been tagged with S rather than with an elaborate tag such as SBAR-PRP.	0
34275	34275	W01-0708	Task description	12	23	0.923076923076923	0.221153846153846	Furthermore, the shared task has been divided in three parts: identifying clause starts, recognizing clause ends and finding complete clauses.	1
34276	34276	W01-0708	Task description	13	24	1.0	0.230769230769231	The results obtained for the first two parts can be used in the third part of the task.	0
34360	34360	W02-2024	Introduction	1	4	0.090909090909091	0.044943820224719	Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.	0
34361	34361	W02-2024	Introduction	2	5	0.181818181818182	0.056179775280899	Example:	0
34362	34362	W02-2024	Introduction	3	6	0.272727272727273	0.067415730337079	PER	0
34363	34363	W02-2024	Introduction	4	7	0.363636363636364	0.078651685393259	Wol ] , currently a journalist in LOC Argentina ] , played with PER Del Bosque ] in the nal years of the seventies in ORG Real Madrid ] .	0
34364	34364	W02-2024	Introduction	5	8	0.454545454545455	0.089887640449438	This sentence contains four named entities:	0
34365	34365	W02-2024	Introduction	6	9	0.545454545454545	0.101123595505618	Wol and Del Bosque are persons, Argentina is a location and Real Madrid is a organization.	0
34366	34366	W02-2024	Introduction	7	10	0.636363636363636	0.112359550561798	The shared task of CoNLL-2002 concerns language-independent named entity recognition.	0
34367	34367	W02-2024	Introduction	8	11	0.727272727272727	0.123595505617978	We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.	0
34368	34368	W02-2024	Introduction	9	12	0.818181818181818	0.134831460674157	The participants of the shared task have been o ered training and test data for two European languages: Spanish and Dutch.	1
34369	34369	W02-2024	Introduction	10	13	0.909090909090909	0.146067415730337	They have used the data for developing a named-entity recognition system that includes a machine learning component.	1
34370	34370	W02-2024	Introduction	11	14	1.0	0.157303370786517	The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.	0
34449	34449	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	1	4	0.090909090909091	0.030769230769231	Named entity recognition is an important task of information extraction systems.	0
34450	34450	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	2	5	0.181818181818182	0.038461538461539	There has been a lot of work on named entity recognition, especially for English (see Borthwick (1999) for an overview).	0
34451	34451	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	3	6	0.272727272727273	0.046153846153846	The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition.	0
34452	34452	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	4	7	0.363636363636364	0.053846153846154	They have also produced a scheme for entity annotation (Chinchor et al., 1999).	0
34453	34453	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	5	8	0.454545454545455	0.061538461538462	More recently, there have been other system development competitions which dealt with different languages (IREX and CoNLL-2002).	0
34454	34454	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	6	9	0.545454545454545	0.069230769230769	The shared task of CoNLL-2003 concerns language-independent named entity recognition.	0
34455	34455	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	7	10	0.636363636363636	0.076923076923077	We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.	0
34456	34456	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	8	11	0.727272727272727	0.084615384615385	The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).	0
34457	34457	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	9	12	0.818181818181818	0.092307692307692	The participants of the 2003 shared task have been offered training and test data for two other European languages: English and German.	1
34458	34458	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	10	13	0.909090909090909	0.1	They have used the data for developing a named-entity recognition system that includes a machine learning component.	1
34459	34459	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	11	14	1.0	0.107692307692308	The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data.	0
34579	34579	W04-2412	Introduction	1	4	0.032258064516129	0.016194331983806	In recent years there has been an increasing interest in semantic parsing of natural language, which is becoming a key issue in Information Extraction, Question Answering, Summarization, and, in general, in all NLP applications requiring some kind of semantic interpretation.	0
34580	34580	W04-2412	Introduction	2	5	0.064516129032258	0.020242914979757	The shared task of CoNLL-2004 1 concerns the recognition of semantic roles, for the English language.	0
34581	34581	W04-2412	Introduction	3	6	0.096774193548387	0.024291497975709	We will refer to it as Semantic Role Labeling (SRL).	0
34582	34582	W04-2412	Introduction	4	7	0.129032258064516	0.02834008097166	Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.	1
34583	34583	W04-2412	Introduction	5	8	0.161290322580645	0.032388663967611	In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be extracted (see Figure 1 for a detailed example).	1
34584	34584	W04-2412	Introduction	6	9	0.193548387096774	0.036437246963563	Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.	0
34585	34585	W04-2412	Introduction	7	10	0.225806451612903	0.040485829959514	Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels.	0
34586	34586	W04-2412	Introduction	8	11	0.258064516129032	0.044534412955466	Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments.	0
34587	34587	W04-2412	Introduction	9	12	0.290322580645161	0.048582995951417	Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002;	0
34588	34588	W04-2412	Introduction	10	13	0.32258064516129	0.052631578947369	Gildea and Palmer, 2002;Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003;	0
34589	34589	W04-2412	Introduction	11	14	0.354838709677419	0.05668016194332	Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003;	0
34590	34590	W04-2412	Introduction	12	15	0.387096774193548	0.060728744939271	Pradhan et al., 2003a;Pradhan et al., 2003b).	0
34591	34591	W04-2412	Introduction	13	16	0.419354838709677	0.064777327935223	There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.	0
34592	34592	W04-2412	Introduction	14	17	0.451612903225806	0.068825910931174	For instance, in (Pradhan et al., 2003a;	0
34593	34593	W04-2412	Introduction	15	18	0.483870967741936	0.072874493927126	Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks.	0
34594	34594	W04-2412	Introduction	16	19	0.516129032258065	0.076923076923077	Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001).	0
34595	34595	W04-2412	Introduction	17	20	0.548387096774194	0.080971659919028	In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate-argument structures.	0
34596	34596	W04-2412	Introduction	18	21	0.580645161290323	0.08502024291498	It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates.	0
34597	34597	W04-2412	Introduction	19	22	0.612903225806452	0.089068825910931	A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations.	0
34598	34598	W04-2412	Introduction	20	23	0.645161290322581	0.093117408906883	To date, the best results reported on the PropBank correspond to a F 1 measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information (Pradhan et al., 2003b).	0
34599	34599	W04-2412	Introduction	21	24	0.67741935483871	0.097165991902834	This performance drops to 77 when a real parser is used instead.	0
34600	34600	W04-2412	Introduction	22	25	0.709677419354839	0.101214574898785	Comparatively, the best SRL system based solely on shallow syntactic information (Pradhan et al., 2003a) performs more than 15 points below.	0
34601	34601	W04-2412	Introduction	23	26	0.741935483870968	0.105263157894737	Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task.	0
34602	34602	W04-2412	Introduction	24	27	0.774193548387097	0.109311740890688	The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases.	0
34603	34603	W04-2412	Introduction	25	28	0.806451612903226	0.11336032388664	The annotations provided for the development of systems include, apart from the argument boundaries and role labels, the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.	0
34604	34604	W04-2412	Introduction	26	29	0.838709677419355	0.117408906882591	The rest of the paper is organized as follows.	0
34605	34605	W04-2412	Introduction	27	30	0.870967741935484	0.121457489878543	Section 2 describes the general setting of the task.	0
34606	34606	W04-2412	Introduction	28	31	0.903225806451613	0.125506072874494	Section 3 provides a detailed description of training, development and test data.	0
34607	34607	W04-2412	Introduction	29	32	0.935483870967742	0.129554655870445	Participant systems are described and compared in section 4.	0
34608	34608	W04-2412	Introduction	30	33	0.967741935483871	0.133603238866397	In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.	0
34609	34609	W04-2412	Introduction	31	34	1.0	0.137651821862348	Finally, section 5 concludes.	0
34826	34826	W05-0620	Introduction	1	4	0.029411764705882	0.013333333333333	In the few last years there has been an increasing interest in shallow semantic parsing of natural language, which is becoming an important component in all kind of NLP applications.	0
34827	34827	W05-0620	Introduction	2	5	0.058823529411765	0.016666666666667	As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and comparative evaluation.	0
34828	34828	W05-0620	Introduction	3	6	0.088235294117647	0.02	Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.	1
34829	34829	W05-0620	Introduction	4	7	0.117647058823529	0.023333333333333	In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized.	1
34830	34830	W05-0620	Introduction	5	8	0.147058823529412	0.026666666666667	Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.	0
34831	34831	W05-0620	Introduction	6	9	0.176470588235294	0.03	Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information.	0
34832	34832	W05-0620	Introduction	7	10	0.205882352941176	0.033333333333333	In (Carreras and Mrquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004.	0
34833	34833	W05-0620	Introduction	8	11	0.235294117647059	0.036666666666667	Ten systems contributed to the task, which was evaluated using the PropBank corpus .	0
34834	34834	W05-0620	Introduction	9	12	0.264705882352941	0.04	The best results were around 70 in F 1 measure.	0
34835	34835	W05-0620	Introduction	10	13	0.294117647058823	0.043333333333333	Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F 1 slightly over 79).	0
34836	34836	W05-0620	Introduction	11	14	0.323529411764706	0.046666666666667	In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004).	0
34837	34837	W05-0620	Introduction	12	15	0.352941176470588	0.05	Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001).	0
34838	34838	W05-0620	Introduction	13	16	0.382352941176471	0.053333333333333	From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004;	0
34839	34839	W05-0620	Introduction	14	17	0.411764705882353	0.056666666666667	Moschitti, 2004;Xue and Palmer, 2004;Pradhan et al., 2005a).	0
34840	34840	W05-0620	Introduction	15	18	0.441176470588235	0.06	Following last year's initiative, the CoNLL-2005 shared task 1 will concern again the recognition of semantic roles for the English language.	0
34841	34841	W05-0620	Introduction	16	19	0.470588235294118	0.063333333333333	Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are:	0
34842	34842	W05-0620	Introduction	17	20	0.5	0.066666666666667		0
34843	34843	W05-0620	Introduction	18	21	0.529411764705882	0.07	Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task.	0
34844	34844	W05-0620	Introduction	19	22	0.558823529411765	0.073333333333333	The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.	0
34845	34845	W05-0620	Introduction	20	23	0.588235294117647	0.076666666666667		0
34846	34846	W05-0620	Introduction	21	24	0.617647058823529	0.08	The training corpus has been substantially enlarged.	0
34847	34847	W05-0620	Introduction	22	25	0.647058823529412	0.083333333333333	This allows to test the scalability of learning-based SRL systems to big datasets and to compute learning curves to see how much data is necessary to train.	0
34848	34848	W05-0620	Introduction	23	26	0.676470588235294	0.086666666666667	Again, we concentrate on the PropBank corpus , which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate-argument structures.	0
34849	34849	W05-0620	Introduction	24	27	0.705882352941176	0.09		0
34850	34850	W05-0620	Introduction	25	28	0.735294117647059	0.093333333333333	In order to test the robustness of the presented systems, a cross-corpora evaluation is performed using a fresh test set from the Brown corpus.	0
34851	34851	W05-0620	Introduction	26	29	0.764705882352941	0.096666666666667	Regarding evaluation, two different settings were devised depending if the systems use the information strictly contained in the training data (closed challenge) or they make use of external sources of information and/or tools (open challenge).	0
34852	34852	W05-0620	Introduction	27	30	0.794117647058823	0.1	The closed setting allows to compare systems under strict conditions, while the open setting aimed at exploring the contributions of other sources of information and the limits of the current learning-based systems on the SRL task.	0
34853	34853	W05-0620	Introduction	28	31	0.823529411764706	0.103333333333333	At the end, all 19 systems took part in the closed challenge and none of them in the open challenge.	0
34854	34854	W05-0620	Introduction	29	32	0.852941176470588	0.106666666666667	The rest of the paper is organized as follows.	0
34855	34855	W05-0620	Introduction	30	33	0.882352941176471	0.11	Section 2 describes the general setting of the task.	0
34856	34856	W05-0620	Introduction	31	34	0.911764705882353	0.113333333333333	Section 3 provides a detailed description of training, development and test data.	0
34857	34857	W05-0620	Introduction	32	35	0.941176470588235	0.116666666666667	Participant systems are described and compared in section 4.	0
34858	34858	W05-0620	Introduction	33	36	0.970588235294118	0.12	In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.	0
34859	34859	W05-0620	Introduction	34	37	1.0	0.123333333333333	Finally, section 5 concludes.	0
35128	35128	D07-1096	Introduction	1	6	0.090909090909091	0.015503875968992	Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999,2000), clause identification (2001), named entity recognition (2002,2003), and semantic role labeling (2004,2005).	0
35129	35129	D07-1096	Introduction	2	7	0.181818181818182	0.018087855297158	In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006).	0
35130	35130	D07-1096	Introduction	3	8	0.272727272727273	0.020671834625323	In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence.	1
35131	35131	D07-1096	Introduction	4	9	0.363636363636364	0.023255813953488	This defines a dependency graph, where the nodes are the words of the input sentence and the arcs are the binary relations from head to dependent.	0
35132	35132	D07-1096	Introduction	5	10	0.454545454545455	0.025839793281654	Often, but not always, it is assumed that all words except one have a syntactic head, which means that the graph will be a tree with the single independent word as the root.	0
35133	35133	D07-1096	Introduction	6	11	0.545454545454545	0.028423772609819	In labeled dependency parsing, we additionally require the parser to assign a specific type (or label) to each dependency relation holding between a head word and a dependent word.	0
35134	35134	D07-1096	Introduction	7	12	0.636363636363636	0.031007751937985	In this year's shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation.	0
35135	35135	D07-1096	Introduction	8	13	0.727272727272727	0.03359173126615	The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where the task was to use machine learning to adapt a parser for a single language to a new domain.	0
35136	35136	D07-1096	Introduction	9	14	0.818181818181818	0.036175710594315	In total, test results were submitted for twenty-three systems in the multilingual track, and ten systems in the domain adaptation track (six of which also participated in the multilingual track).	0
35137	35137	D07-1096	Introduction	10	15	0.909090909090909	0.038759689922481	Not everyone submitted papers describing their system, and some papers describe more than one system (or the same system in both tracks), which explains why there are only (!) twenty-one papers in the proceedings.	0
35138	35138	D07-1096	Introduction	11	16	1.0	0.041343669250646	In this paper, we provide task definitions for the two tracks (section 2), describe data sets extracted from available treebanks (section 3), report results for all systems in both tracks (section 4), give an overview of approaches used (section 5), provide a first analysis of the results (section 6), and conclude with some future directions (section 7).	0
35516	35516	W08-2121	Introduction	1	7	0.038461538461539	0.020114942528736	In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English).	0
35517	35517	W08-2121	Introduction	2	8	0.076923076923077	0.022988505747127	In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.	0
35518	35518	W08-2121	Introduction	3	9	0.115384615384615	0.025862068965517	The CoNLL-2008 shared task 1 proposes a unified dependency-based c 2008.	0
35519	35519	W08-2121	Introduction	4	10	0.153846153846154	0.028735632183908	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).	0
35520	35520	W08-2121	Introduction	5	11	0.192307692307692	0.031609195402299	Some rights reserved.	0
35521	35521	W08-2121	Introduction	6	12	0.230769230769231	0.03448275862069	1 http://www.yr-bcn.es/conll2008	0
35522	35522	W08-2121	Introduction	7	13	0.269230769230769	0.037356321839081	formalism, which models both syntactic dependencies and semantic roles.	0
35523	35523	W08-2121	Introduction	8	14	0.307692307692308	0.040229885057471	Using this formalism, this shared task merges both the task of syntactic dependency parsing and the task of identifying semantic arguments and labeling them with semantic roles.	0
35524	35524	W08-2121	Introduction	9	15	0.346153846153846	0.043103448275862	Conceptually, the 2008 shared task can be divided into three subtasks: (i) parsing of syntactic dependencies, (ii) identification and disambiguation of semantic predicates, and (iii) identification of arguments and assignment of semantic roles for each predicate.	1
35525	35525	W08-2121	Introduction	10	16	0.384615384615385	0.045977011494253	Several objectives were addressed in this shared task:	0
35526	35526	W08-2121	Introduction	11	17	0.423076923076923	0.048850574712644	 SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies.	0
35527	35527	W08-2121	Introduction	12	18	0.461538461538462	0.051724137931035	While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates.	0
35528	35528	W08-2121	Introduction	13	19	0.5	0.054597701149425	 Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks.	0
35529	35529	W08-2121	Introduction	14	20	0.538461538461538	0.057471264367816	For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations.	0
35530	35530	W08-2121	Introduction	15	21	0.576923076923077	0.060344827586207		0
35531	35531	W08-2121	Introduction	16	22	0.615384615384615	0.063218390804598	A practical framework is provided for the joint learning of syntactic and semantic dependencies.	0
35532	35532	W08-2121	Introduction	17	23	0.653846153846154	0.066091954022989	Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly setting.	0
35533	35533	W08-2121	Introduction	18	24	0.692307692307692	0.068965517241379	The evaluation is separated into two different challenges: a closed challenge, where systems have to be trained strictly with information contained in the given training corpus, and an open challenge, where systems can be developed making use of any kind of external tools and resources.	0
35534	35534	W08-2121	Introduction	19	25	0.730769230769231	0.07183908045977	The participants could submit results in either one or both challenges.	0
35535	35535	W08-2121	Introduction	20	26	0.769230769230769	0.074712643678161	This paper is organized as follows.	0
35536	35536	W08-2121	Introduction	21	27	0.807692307692308	0.077586206896552	Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges.	0
35537	35537	W08-2121	Introduction	22	28	0.846153846153846	0.080459770114943	Section 3 introduces the corpora used and our constituent-to-dependency conversion procedure.	0
35538	35538	W08-2121	Introduction	23	29	0.884615384615385	0.083333333333333	Section 4 summarizes the results of the submitted systems.	0
35539	35539	W08-2121	Introduction	24	30	0.923076923076923	0.086206896551724	Section 5 discusses the approaches implemented by participants.	0
35540	35540	W08-2121	Introduction	25	31	0.961538461538462	0.089080459770115	Section 6 analyzes the results using additional non-official evaluation measures.	0
35541	35541	W08-2121	Introduction	26	32	1.0	0.091954022988506	Section 7 concludes the paper.	0
35863	35863	W09-1201	Introduction	1	6	0.041666666666667	0.017543859649123	"Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) launches a competitive, open ""Shared Task""."	0
35864	35864	W09-1201	Introduction	2	7	0.083333333333333	0.02046783625731	"A common (""shared"") task is defined and datasets are provided for its participants."	0
35865	35865	W09-1201	Introduction	3	8	0.125	0.023391812865497	In 2004 and 2005, the shared tasks were dedicated to semantic role labeling (SRL) in a monolingual setting (English).	0
35866	35866	W09-1201	Introduction	4	9	0.166666666666667	0.026315789473684	In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.	0
35867	35867	W09-1201	Introduction	5	10	0.208333333333333	0.029239766081871	In 2008, the shared task (Surdeanu et al., 2008) used a unified dependencybased formalism, which modeled both syntactic dependencies and semantic roles for English.	0
35868	35868	W09-1201	Introduction	6	11	0.25	0.032163742690059	The CoNLL-2009 Shared Task has built on the 2008 results by providing data for six more languages (Catalan, Chinese, Czech, German, Japanese and Spanish) in addition to the original English 1 .	0
35869	35869	W09-1201	Introduction	7	12	0.291666666666667	0.035087719298246	It has thus naturally extended the path taken by the five most recent CoNLL shared tasks.	0
35870	35870	W09-1201	Introduction	8	13	0.333333333333333	0.038011695906433	As in 2008, the CoNLL-2009 shared task combined dependency parsing and the task of identifying and labeling semantic arguments of verbs (and other parts of speech whenever available).	1
35871	35871	W09-1201	Introduction	9	14	0.375	0.04093567251462	Participants had to choose from two tasks:	0
35872	35872	W09-1201	Introduction	10	15	0.416666666666667	0.043859649122807		0
35873	35873	W09-1201	Introduction	11	16	0.458333333333333	0.046783625730994	Joint task (syntactic dependency parsing and semantic role labeling), or	0
35874	35874	W09-1201	Introduction	12	17	0.5	0.049707602339181	 SRL-only task (syntactic dependency parses have been provided by the organizers, using state-of-the art parsers for the individual languages).	0
35875	35875	W09-1201	Introduction	13	18	0.541666666666667	0.052631578947369	In contrast to the previous year, the evaluation data indicated which words were to be dealt with (for the SRL task).	0
35876	35876	W09-1201	Introduction	14	19	0.583333333333333	0.055555555555556	In other words, (predicate) disambiguation was still part of the task, whereas the identification of argument-bearing words was not.	0
35877	35877	W09-1201	Introduction	15	20	0.625	0.058479532163743	This decision was made to compensate for the significant differences between languages and between the annotation schemes used.	0
35878	35878	W09-1201	Introduction	16	21	0.666666666666667	0.06140350877193	"The ""closed"" and ""open"" challenges have been kept from last year as well; participants could have chosen one or both."	0
35879	35879	W09-1201	Introduction	17	22	0.708333333333333	0.064327485380117	In the closed challenge, systems had to be trained strictly with information contained in the given training corpus; in the open challenge, systems could have been developed making use of any kind of external tools and resources.	0
35880	35880	W09-1201	Introduction	18	23	0.75	0.067251461988304	This paper is organized as follows.	0
35881	35881	W09-1201	Introduction	19	24	0.791666666666667	0.070175438596491	Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges.	0
35882	35882	W09-1201	Introduction	20	25	0.833333333333333	0.073099415204678	A substantial portion of the paper (Section 3) is devoted to the description of the conversion and development of the data sets in the additional languages.	0
35883	35883	W09-1201	Introduction	21	26	0.875	0.076023391812866	Section 4 shows the main results of the submitted systems in the Joint and SRL-only tasks.	0
35884	35884	W09-1201	Introduction	22	27	0.916666666666667	0.078947368421053	Section 5 summarizes the approaches implemented by participants.	0
35885	35885	W09-1201	Introduction	23	28	0.958333333333333	0.08187134502924	Section 6 concludes the paper.	0
35886	35886	W09-1201	Introduction	24	29	1.0	0.084795321637427	In all sections, we will mention some of the differences between last year's and this year's tasks while keeping the text self-contained whenever possible; for details and observations on the English data, please refer to the overview paper of the CoNLL-2008 Shared Task (Surdeanu et al., 2008) and to the references mentioned in the sections describing the other languages.	0
36304	36304	W10-3001	Task Definitions	1	105	1.0	0.472972972972973	Two uncertainty detection tasks (sentence classification and in-sentence hedge scope detection) in two domains (biological publications and Wikipedia articles) with three types of submissions (closed, cross and open) were given to the participants of the CoNLL-2010 Shared Task.	1
36604	36604	W11-1901	Task Description	1	183	0.2	0.358823529411765	The CoNLL-2011 shared task was based on the English portion of the OntoNotes 4.0 data.	1
36605	36605	W11-1901	Task Description	2	184	0.4	0.36078431372549	The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains.	1
36606	36606	W11-1901	Task Description	3	185	0.6	0.362745098039216	The target coreference decisions could be made using automatically predicted information on the other structural layers including the parses, semantic roles, word senses, and named entities.	0
36607	36607	W11-1901	Task Description	4	186	0.8	0.364705882352941	As is customary for CoNLL tasks, there were two tracks, closed and open.	0
36608	36608	W11-1901	Task Description	5	187	1.0	0.366666666666667	For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data.	0
37215	37215	W12-4501	CoNLL-2012 Coreference Task	1	284	0.058823529411765	0.34093637454982	The CoNLL-2012 shared task was held across all three languages -English, Chinese and Arabicof the OntoNotes v5.0 data.	1
37216	37216	W12-4501	CoNLL-2012 Coreference Task	2	285	0.117647058823529	0.342136854741897	The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains.	1
37217	37217	W12-4501	CoNLL-2012 Coreference Task	3	286	0.176470588235294	0.343337334933974	The coreference decisions had to be made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities.	0
37218	37218	W12-4501	CoNLL-2012 Coreference Task	4	287	0.235294117647059	0.34453781512605	Given various factors, such as the lack of resources and state-of-the-art tools, and time constraints, we could not provide some layers of information for the Chinese and Arabic portion of the data.	0
37219	37219	W12-4501	CoNLL-2012 Coreference Task	5	288	0.294117647058823	0.345738295318127	The three languages are from quite different language families.	0
37220	37220	W12-4501	CoNLL-2012 Coreference Task	6	289	0.352941176470588	0.346938775510204	The morphology of these languages is quite different.	0
37221	37221	W12-4501	CoNLL-2012 Coreference Task	7	290	0.411764705882353	0.348139255702281	Arabic has a complex morphology, English has limited morphology, whereas Chinese has very little morphology.	0
37222	37222	W12-4501	CoNLL-2012 Coreference Task	8	291	0.470588235294118	0.349339735894358	English word segmentation amounts to rule-based tokenization, and is close to perfect.	0
37223	37223	W12-4501	CoNLL-2012 Coreference Task	9	292	0.529411764705882	0.350540216086435	In the case of Chinese and Arabic, although the tokenization/segmentation is not as good as English, the accuracies are in the high 90s.	0
37224	37224	W12-4501	CoNLL-2012 Coreference Task	10	293	0.588235294117647	0.351740696278511	Syntactically, there are many dropped subjects and objects in Arabic and Chinese, whereas English is not a pro-drop language.	0
37225	37225	W12-4501	CoNLL-2012 Coreference Task	11	294	0.647058823529412	0.352941176470588	Another difference is the amount of resources available for each language.	0
37226	37226	W12-4501	CoNLL-2012 Coreference Task	12	295	0.705882352941176	0.354141656662665	English has probably the most resources at its disposal, whereas Chinese and Arabic lack significantly -Arabic more so than Chinese.	0
37227	37227	W12-4501	CoNLL-2012 Coreference Task	13	296	0.764705882352941	0.355342136854742	Given this fact, plus the fact that the CoNLL format cannot handle multiple segmentations, and that it would complicate scoring since we are using exact token boundaries (as discussed later in Section 4.5), we decided to allow the use of gold, treebank segmentation for all languages.	0
37228	37228	W12-4501	CoNLL-2012 Coreference Task	14	297	0.823529411764706	0.356542617046819	In the case of Chinese, the words themselves are lemmas, so no additional information needs to be provided.	0
37229	37229	W12-4501	CoNLL-2012 Coreference Task	15	298	0.882352941176471	0.357743097238896	For Arabic, by default written text is unvocalised, so we decided to also provide correct, gold standard lemmas, along with the correct vocalized version of the tokens.	0
37230	37230	W12-4501	CoNLL-2012 Coreference Task	16	299	0.941176470588235	0.358943577430972	Table 2 lists which layers were available and quality of the provided layers (when provided.)	0
37231	37231	W12-4501	CoNLL-2012 Coreference Task	17	300	1.0	0.360144057623049	 	0
37769	37769	W13-3601	Introduction	1	5	0.066666666666667	0.026595744680851	Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL-2013).	0
37770	37770	W13-3601	Introduction	2	6	0.133333333333333	0.031914893617021	In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay.	1
37771	37771	W13-3601	Introduction	3	7	0.2	0.037234042553192	This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011;	0
37772	37772	W13-3601	Introduction	4	8	0.266666666666667	0.042553191489362	Dale et al., 2012).	0
37773	37773	W13-3601	Introduction	5	9	0.333333333333333	0.047872340425532	In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.	0
37774	37774	W13-3601	Introduction	6	10	0.4	0.053191489361702	This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.	0
37775	37775	W13-3601	Introduction	7	11	0.466666666666667	0.058510638297872	Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.	0
37776	37776	W13-3601	Introduction	8	12	0.533333333333333	0.063829787234043	The CoNLL-2013 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.	0
37777	37777	W13-3601	Introduction	9	13	0.6	0.069148936170213	This overview paper contains a detailed description of the shared task, and is organized as follows.	0
37778	37778	W13-3601	Introduction	10	14	0.666666666666667	0.074468085106383	Section 2 provides the task definition.	0
37779	37779	W13-3601	Introduction	11	15	0.733333333333333	0.079787234042553	Section 3 describes the annotated training data provided and the blind test data.	0
37780	37780	W13-3601	Introduction	12	16	0.8	0.085106382978723	Section 4 describes the evaluation metric and the scorer.	0
37781	37781	W13-3601	Introduction	13	17	0.866666666666667	0.090425531914894	Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.	0
37782	37782	W13-3601	Introduction	14	18	0.933333333333333	0.095744680851064	Section 6 presents the results of the shared task.	0
37783	37783	W13-3601	Introduction	15	19	1.0	0.101063829787234	Section 7 concludes the paper.	0
37960	37960	W14-1701	Introduction	1	8	0.066666666666667	0.045454545454546	Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014(CoNLL-2014.	0
37961	37961	W14-1701	Introduction	2	9	0.133333333333333	0.051136363636364	In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay.	1
37962	37962	W14-1701	Introduction	3	10	0.2	0.056818181818182	This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011;	0
37963	37963	W14-1701	Introduction	4	11	0.266666666666667	0.0625	Dale et al., 2012), and a CoNLL shared task on grammatical error correction organized in 2013 .	0
37964	37964	W14-1701	Introduction	5	12	0.333333333333333	0.068181818181818	In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.	0
37965	37965	W14-1701	Introduction	6	13	0.4	0.073863636363636	This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.	0
37966	37966	W14-1701	Introduction	7	14	0.466666666666667	0.079545454545455	Also, tackling this task has farreaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.	0
37967	37967	W14-1701	Introduction	8	15	0.533333333333333	0.085227272727273	The CoNLL-2014 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.	0
37968	37968	W14-1701	Introduction	9	16	0.6	0.090909090909091	This overview paper contains a detailed description of the shared task, and is organized as follows.	0
37969	37969	W14-1701	Introduction	10	17	0.666666666666667	0.096590909090909	Section 2 provides the task definition.	0
37970	37970	W14-1701	Introduction	11	18	0.733333333333333	0.102272727272727	Section 3 describes the annotated training data provided and the blind test data.	0
37971	37971	W14-1701	Introduction	12	19	0.8	0.107954545454545	Section 4 describes the evaluation metric and the scorer.	0
37972	37972	W14-1701	Introduction	13	20	0.866666666666667	0.113636363636364	Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.	0
37973	37973	W14-1701	Introduction	14	21	0.933333333333333	0.119318181818182	Section 6 presents the results of the shared task, including a discussion on cross annotator comparison.	0
37974	37974	W14-1701	Introduction	15	22	1.0	0.125	Section 7 concludes the paper.	0
38460	38460	K16-2001	Task Definition	1	31	0.076923076923077	0.116541353383459	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
38461	38461	K16-2001	Task Definition	2	32	0.153846153846154	0.120300751879699	Specifically, given a newswire article as input, a participating system is asked to return the set of discourse relations it can identify in the text.	1
38462	38462	K16-2001	Task Definition	3	33	0.230769230769231	0.12406015037594	A discourse relation is defined as a relation taking two abstract objects (events, states, facts, or propositions) as arguments (Prasad et al., 2008;	1
38463	38463	K16-2001	Task Definition	4	34	0.307692307692308	0.12781954887218	Prasad et al., 2014).	0
38464	38464	K16-2001	Task Definition	5	35	0.384615384615385	0.131578947368421	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between two argument spans interpretable as abstract objects.	0
38465	38465	K16-2001	Task Definition	6	36	0.461538461538462	0.135338345864662	In the current version of the PDTB, only adjacent spans are considered.	0
38466	38466	K16-2001	Task Definition	7	37	0.538461538461538	0.139097744360902	Each discourse relation is labeled with a sense selected from a sense hierarchy.	0
38467	38467	K16-2001	Task Definition	8	38	0.615384615384615	0.142857142857143	Its argument spans may be sentences, clauses, or in some rare cases, noun phrases.	0
38468	38468	K16-2001	Task Definition	9	39	0.692307692307692	0.146616541353383	To detect a discourse relation, a participating system needs to:	0
38469	38469	K16-2001	Task Definition	10	40	0.769230769230769	0.150375939849624	1	0
38470	38470	K16-2001	Task Definition	11	41	0.846153846153846	0.154135338345865	"Identify the text span of an explicit discourse connective, if present, or the po-sition between adjacent sentences as the proxy site of an implicit discourse relation; 2. Identify the two text spans that serve as arguments to the relation; 3. Label the arguments as Arg1 or Arg2, as appropriate; 4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
38471	38471	K16-2001	Task Definition	12	42	0.923076923076923	0.157894736842105	A full system that outputs all four components of the discourse relations usually comprises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system.	0
38472	38472	K16-2001	Task Definition	13	43	1.0	0.161654135338346	This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments.	0
38701	38701	K17-3001	Introduction	1	6	0.083333333333333	0.02013422818792	Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular.	0
38702	38702	K17-3001	Introduction	2	7	0.166666666666667	0.023489932885906	For the first time dependency treebanks in more than ten languages were available for learning parsers.	0
38703	38703	K17-3001	Introduction	3	8	0.25	0.026845637583893	Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications.	0
38704	38704	K17-3001	Introduction	4	9	0.333333333333333	0.030201342281879	While the two tasks (Buchholz and Marsi, 2006;	0
38705	38705	K17-3001	Introduction	5	10	0.416666666666667	0.033557046979866	Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to-kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible.	0
38706	38706	K17-3001	Introduction	6	11	0.5	0.036912751677852	CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues.	0
38707	38707	K17-3001	Introduction	7	12	0.583333333333333	0.040268456375839	1	0
38708	38708	K17-3001	Introduction	8	13	0.666666666666667	0.043624161073826	The focus of the 2017 task was learning syntactic dependency parsers that can work in a realworld setting, starting from raw text, and that can work over many typologically different languages, even surprise languages for which there is little or no training data, by exploiting a common syntactic annotation standard.	0
38709	38709	K17-3001	Introduction	9	14	0.75	0.046979865771812	This task has been made possible by the Universal Dependencies initiative (UD) (Nivre et al., 2016), which has developed treebanks for 50+ languages with crosslinguistically consistent annotation and recoverability of the original raw texts.	0
38710	38710	K17-3001	Introduction	10	15	0.833333333333333	0.050335570469799	Participating systems had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.	1
38711	38711	K17-3001	Introduction	11	16	0.916666666666667	0.053691275167785	No gold-standard annotation (tokenization, sentence segmentation, lemmas, morphology) was available in the input text.	0
38712	38712	K17-3001	Introduction	12	17	1.0	0.057046979865772	However, teams wishing to concentrate just on parsing were able to use segmentation and morphology predicted by the baseline UDPipe system (Straka et al., 2016).	0
39001	39001	K18-2001	Introduction	1	8	0.111111111111111	0.031746031746032	The 2017 CoNLL shared task on universal dependency parsing  picked up the thread from the influential shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006;	0
39002	39002	K18-2001	Introduction	2	9	0.222222222222222	0.035714285714286	Nivre et al., 2007) and evolved it in two ways: (1) the parsing process started from raw text rather than gold standard tokenization and part-of-speech tagging, and (2) the syntactic representations were consistent across languages thanks to the Universal Dependencies framework (Nivre et al., 2016).	0
39003	39003	K18-2001	Introduction	3	10	0.333333333333333	0.03968253968254	The 2018 CoNLL shared task on universal dependency parsing starts from the same premises but adds a focus on morphological analysis as well as data from new languages.	0
39004	39004	K18-2001	Introduction	4	11	0.444444444444444	0.043650793650794	Like last year, participating systems minimally had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.	1
39005	39005	K18-2001	Introduction	5	12	0.555555555555556	0.047619047619048	In addition, this year's task featured new metrics that also scored a system's capacity to predict a morphological analysis of each word, including a part-of-speech tag, morphological features, and a lemma.	1
39006	39006	K18-2001	Introduction	6	13	0.666666666666667	0.051587301587302	Regardless of metric, the assumption was that the input should be raw text, with no gold-standard word or sentence segmentation, and no gold-standard morphological annotation.	0
39007	39007	K18-2001	Introduction	7	14	0.777777777777778	0.055555555555556	However, for teams who wanted to concentrate on one or more subtasks, segmentation and morphology predicted by the baseline UDPipe system (Straka et al., 2016) was made available just like last year.	0
39008	39008	K18-2001	Introduction	8	15	0.888888888888889	0.05952380952381	There are eight new languages this year: Afrikaans, Armenian, Breton, Faroese, Naija, Old French, Serbian, and Thai; see Section 2 for more details.	0
39009	39009	K18-2001	Introduction	9	16	1.0	0.063492063492064	The two new evaluation metrics are described in Section 3.	0
39255	39255	K18-3001	Introduction	1	10	0.058823529411765	0.032258064516129	Some of a word's syntactic and semantic properties are expressed on the word form through a process termed morphological inflection.	0
39256	39256	K18-3001	Introduction	2	11	0.117647058823529	0.035483870967742	For example, each English count noun has both singular and plural forms (robot/robots, process/processes), known as the inflected forms of the noun.	0
39257	39257	K18-3001	Introduction	3	12	0.176470588235294	0.038709677419355	Some languages display little inflection, while others possess a proliferation of forms.	0
39258	39258	K18-3001	Introduction	4	13	0.235294117647059	0.041935483870968	A Polish verb can have nearly 100 inflected forms and an Archi verb has thousands (Kibrik, 1998).	0
39259	39259	K18-3001	Introduction	5	14	0.294117647058823	0.045161290322581	Natural language processing systems must be able to analyze and generate these inflected forms.	0
39260	39260	K18-3001	Introduction	6	15	0.352941176470588	0.048387096774194	Fortunately, inflected forms tend to be systematically related to one another.	0
39261	39261	K18-3001	Introduction	7	16	0.411764705882353	0.051612903225807	This is why English  example maps a lemma and inflection to an inflected form, The inflection is a bundle of morphosyntactic features.	0
39262	39262	K18-3001	Introduction	8	17	0.470588235294118	0.054838709677419	Note that inflected forms (and lemmata) can encompass multiple words.	0
39263	39263	K18-3001	Introduction	9	18	0.529411764705882	0.058064516129032	In the test data, the last column (the inflected form) must be predicted by the system.	0
39264	39264	K18-3001	Introduction	10	19	0.588235294117647	0.061290322580645	speakers can usually predict the singular form from the plural and vice versa, even for words they have never seen before: given a novel noun wug, an English speaker knows that the plural is wugs.	0
39265	39265	K18-3001	Introduction	11	20	0.647058823529412	0.064516129032258	We conducted a competition on generating inflected forms.	0
39266	39266	K18-3001	Introduction	12	21	0.705882352941176	0.067741935483871	"This ""shared task"" consisted of two separate scenarios."	0
39267	39267	K18-3001	Introduction	13	22	0.764705882352941	0.070967741935484	In Task 1, participating systems must inflect word forms based on labeled examples.	1
39268	39268	K18-3001	Introduction	14	23	0.823529411764706	0.074193548387097	In English, an example of inflection is the conversion of a citation form 1 run to its present participle, running.	1
39269	39269	K18-3001	Introduction	15	24	0.882352941176471	0.07741935483871	The system is provided with the source form and the morphosyntactic description (MSD) of the target form, and must generate the actual target form.	1
39270	39270	K18-3001	Introduction	16	25	0.941176470588235	0.080645161290323	Task 2 is a harder version of Task 1, where the system must infer the appropriate MSD from a sentential context.	1
39271	39271	K18-3001	Introduction	17	26	1.0	0.083870967741936	This is essentially a cloze task, asking participants to provide the correct form of a lemma in context.	0
39561	39561	K19-2001	Background and Motivation	1	6	0.0625	0.014319809069213	All things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, 'classic' (discrete and hierarchically structured) semantic representations will continue to play an important role in 'making sense' of natural language.	0
39562	39562	K19-2001	Background and Motivation	2	7	0.125	0.016706443914081	While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surface syntax, and in particular for the representation of semantic structure.	0
39563	39563	K19-2001	Background and Motivation	3	8	0.1875	0.01909307875895	The 2019 Conference on Computational Language Learning (CoNLL) hosts a shared task (or 'system bake-off') on Cross-Framework Meaning Representation Parsing (MRP 2019).	0
39564	39564	K19-2001	Background and Motivation	4	9	0.25	0.021479713603819	The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning.	0
39565	39565	K19-2001	Background and Motivation	5	10	0.3125	0.023866348448687	For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup.	0
39566	39566	K19-2001	Background and Motivation	6	11	0.375	0.026252983293556	Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see 3 below)which all encode core predicate-argument structure, among other things-in the same implementation.	1
39567	39567	K19-2001	Background and Motivation	7	12	0.4375	0.028639618138425	Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel.	1
39568	39568	K19-2001	Background and Motivation	8	13	0.5	0.031026252983294	Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required.	0
39569	39569	K19-2001	Background and Motivation	9	14	0.5625	0.033412887828162	Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017;	0
39570	39570	K19-2001	Background and Motivation	10	15	0.625	0.035799522673031	Hershcovich et al., 2018;or Stanovsky and Dagan, 2018).	0
39571	39571	K19-2001	Background and Motivation	11	16	0.6875	0.0381861575179	Training and evaluation data were provided for all five frameworks.	0
39572	39572	K19-2001	Background and Motivation	12	17	0.75	0.040572792362769	The task design aims to reduce framework-specific 'balkanization' in the field of meaning representation parsing.	0
39573	39573	K19-2001	Background and Motivation	13	18	0.8125	0.042959427207637	Its contributions include (a) a unifying formal model over different semantic graph banks ( 2), (b) uniform representations and scoring ( 4 and 6), (c) contrastive evaluation across frameworks ( 5), and (d) increased cross-fertilization via transfer and multi-task learning ( 7).	0
39574	39574	K19-2001	Background and Motivation	14	19	0.875	0.045346062052506	Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 May, 2016;	0
39575	39575	K19-2001	Background and Motivation	15	20	0.9375	0.047732696897375	May and Priyadarshi, 2017;.	0
39576	39576	K19-2001	Background and Motivation	16	21	1.0	0.050119331742244	Owing to the scarcity of semantic anno-tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being.	0
40360	40360	2021.dravidianlangtech-1.15	Task	1	61	0.142857142857143	0.61	The shared task was hosted on Codalab.	0
40361	40361	2021.dravidianlangtech-1.15	Task	2	62	0.285714285714286	0.62	Four translation sub-tasks were organized as a part of this task: English to Tamil, English to Malayalam, English to Telugu and Tamil to Telugu.	1
40362	40362	2021.dravidianlangtech-1.15	Task	3	63	0.428571428571429	0.63	Participants were given a choice to participate in the sub-tasks they wanted to.	0
40363	40363	2021.dravidianlangtech-1.15	Task	4	64	0.571428571428571	0.64	Training, development and test datasets of parallel sentences for each language pair were provided to all the participants.	0
40364	40364	2021.dravidianlangtech-1.15	Task	5	65	0.714285714285714	0.65	The task was to train/develop machine translation systems for the given languages.	0
40365	40365	2021.dravidianlangtech-1.15	Task	6	66	0.857142857142857	0.66	For evaluation, the participants translated the test data using the translation systems and the results were submitted to the organizers of the workshop.	0
40366	40366	2021.dravidianlangtech-1.15	Task	7	67	1.0	0.67	The submissions were evaluated by comparing them with the gold standard test set translations available, for which BLEU scores were used as the metric to rank the participants and subsequently the results were returned to the participants.	0
40438	40438	2021.dravidianlangtech-1.16	Task Description	1	39	0.083333333333333	0.375	"The goal of the ""Troll Meme Classification in Tamil"" shared task was to classify if a given meme is a ""troll"" or ""not-troll"" based on the image and text associated with the meme in the Tamil language."	1
40439	40439	2021.dravidianlangtech-1.16	Task Description	2	40	0.166666666666667	0.384615384615385	The text from the meme is written in either the Tamil grammar and English lexicon or English grammar and Tamil lexicon.	0
40440	40440	2021.dravidianlangtech-1.16	Task Description	3	41	0.25	0.394230769230769	However, for consistency, we transcripted the text in Latin.	0
40441	40441	2021.dravidianlangtech-1.16	Task Description	4	42	0.333333333333333	0.403846153846154	Troll meme is a meme, which consists of offensive text and non-offensive images, offensive images with non-offensive text, sarcastically offensive text with non-offensive images, or sarcastic images with offensive text to provoke, distract and has digressive or off-topic content with intend to demean or offend particular people, group or race, otherwise, a not-troll meme .	0
40442	40442	2021.dravidianlangtech-1.16	Task Description	5	43	0.416666666666667	0.413461538461538	Figure 1 shows examples of a troll and nottroll meme from the TamilMeme dataset.	0
40443	40443	2021.dravidianlangtech-1.16	Task Description	6	44	0.5	0.423076923076923	"Example 1 is a troll meme targeted towards the potato chip brand called ""Lays""."	0
40444	40444	2021.dravidianlangtech-1.16	Task Description	7	45	0.583333333333333	0.432692307692308	"In this example, an image is harmless with just a picture of the potato chips  packet, but the translation of the text is ""If you buy one packet air, then 5 chips free"" which is offensive for the brand."	0
40445	40445	2021.dravidianlangtech-1.16	Task Description	8	46	0.666666666666667	0.442307692307692	"The translation of Example 2 would be ""Sorry my friend (girl)""."	0
40446	40446	2021.dravidianlangtech-1.16	Task Description	9	47	0.75	0.451923076923077	This example does not contain any provoking or offensive image or text and hence, it is a not-troll meme.	0
40447	40447	2021.dravidianlangtech-1.16	Task Description	10	48	0.833333333333333	0.461538461538462	Previously, we developed this TamilMemes dataset and treated the task of identifying a troll meme as an image classification problem.	0
40448	40448	2021.dravidianlangtech-1.16	Task Description	11	49	0.916666666666667	0.471153846153846	Since the text associated with the meme acts as a context of the image, we enhanced our TamilMemes dataset by providing the text as a separate modality for the shared task.	0
40449	40449	2021.dravidianlangtech-1.16	Task Description	12	50	1.0	0.480769230769231	We expected our participant to approach the task in a multimodal way.	0
40543	40543	2021.dravidianlangtech-1.17	Task Description	1	40	0.037037037037037	0.263157894736842	Offensive language identification for Dravidian languages at different levels of complexity were developed following the work of (Zampieri et al., 2019).	1
40544	40544	2021.dravidianlangtech-1.17	Task Description	2	41	0.074074074074074	0.269736842105263	It was customized to our annotation method from three-level hierarchical annotation schema.	0
40545	40545	2021.dravidianlangtech-1.17	Task Description	3	42	0.111111111111111	0.276315789473684	A new category Not in intended language was added to include comments written in a language other than the Dravidian languages.	0
40546	40546	2021.dravidianlangtech-1.17	Task Description	4	43	0.148148148148148	0.282894736842105	Annotations decision for offensive language categories were split into six labels to simplify the annotation process.	0
40547	40547	2021.dravidianlangtech-1.17	Task Description	5	44	0.185185185185185	0.289473684210526	 Not Offensive: Comment/post does not have offence, obscenity, swearing, or profanity.	0
40548	40548	2021.dravidianlangtech-1.17	Task Description	6	45	0.222222222222222	0.296052631578947	 Offensive Untargeted: Comment/post have offence, obscenity, swearing, or profanity not directed towards any target.	0
40549	40549	2021.dravidianlangtech-1.17	Task Description	7	46	0.259259259259259	0.302631578947368	These are the comments/posts which have inadmissible language without targeting anyone.	0
40550	40550	2021.dravidianlangtech-1.17	Task Description	8	47	0.296296296296296	0.309210526315789	 Offensive Targeted Individual: Comment/post have offence, obscenity, swearing, or profanity which targets an individual.	0
40551	40551	2021.dravidianlangtech-1.17	Task Description	9	48	0.333333333333333	0.31578947368421	 Offensive Targeted Group: Comment/post have offence, obscenity, swearing, or profanity which targets a group or a community.	0
40552	40552	2021.dravidianlangtech-1.17	Task Description	10	49	0.37037037037037	0.322368421052632	 Offensive Targeted Other: Comment/post have offence, obscenity, swearing, or profanity which does not belong to any of the previous two classes.	0
40553	40553	2021.dravidianlangtech-1.17	Task Description	11	50	0.407407407407407	0.328947368421053		0
40554	40554	2021.dravidianlangtech-1.17	Task Description	12	51	0.444444444444444	0.335526315789474	Not in indented language:	0
40555	40555	2021.dravidianlangtech-1.17	Task Description	13	52	0.481481481481481	0.342105263157895	If the comment is not in the intended language.	0
40556	40556	2021.dravidianlangtech-1.17	Task Description	14	53	0.518518518518518	0.348684210526316	For example, in the Malayalam task, if the sentence does not contain Malayalam written in Malayalam script or Latin script, then it is not Malayalam.	0
40557	40557	2021.dravidianlangtech-1.17	Task Description	15	54	0.555555555555556	0.355263157894737	Sample of not-offensive comments in our dataset provided for the participants is given below with the corresponding English glosses..	0
40558	40558	2021.dravidianlangtech-1.17	Task Description	16	55	0.592592592592593	0.361842105263158	This is an example of Offensive Targeted Individual.	0
40559	40559	2021.dravidianlangtech-1.17	Task Description	17	56	0.62962962962963	0.368421052631579	Mr. Ghibran is a popular music composer in Tamil films, and the author tries to insult him.	0
40560	40560	2021.dravidianlangtech-1.17	Task Description	18	57	0.666666666666667	0.375	 Malayalam-English: Verupikkal manju ullathu ozhichal baki ellam super aanu ee padam.	0
40561	40561	2021.dravidianlangtech-1.17	Task Description	19	58	0.703703703703704	0.381578947368421	"English: ""Except the presence of disgusting Manju, everything is super in this movie"""	0
40562	40562	2021.dravidianlangtech-1.17	Task Description	20	59	0.740740740740741	0.388157894736842	This is an example of Offensive Targeted Individual.	0
40563	40563	2021.dravidianlangtech-1.17	Task Description	21	60	0.777777777777778	0.394736842105263	Mrs. Manju Warrier is a popular actress in South Indian films, and the author tries to discredit her by clearly mentioning that her presence is disgusting.	0
40564	40564	2021.dravidianlangtech-1.17	Task Description	22	61	0.814814814814815	0.401315789473684	 Malayalam-English: Nalla trailer nu okke keri dislike adikunne ethelum thanthayillathavanmar aayirikum.	0
40565	40565	2021.dravidianlangtech-1.17	Task Description	23	62	0.851851851851852	0.407894736842105	poyi chavinedey...	0
40566	40566	2021.dravidianlangtech-1.17	Task Description	24	63	0.888888888888889	0.414473684210526	"English: ""Those who dislike any trailers will probably be assholes."	0
40567	40567	2021.dravidianlangtech-1.17	Task Description	25	64	0.925925925925926	0.421052631578947	"Go to hell..."""	0
40568	40568	2021.dravidianlangtech-1.17	Task Description	26	65	0.962962962962963	0.427631578947368	This is an example of Offensive Untargeted comment.	0
40569	40569	2021.dravidianlangtech-1.17	Task Description	27	66	1.0	0.43421052631579	This text have inadmissible language without targeting anyone.	0
40664	40664	2021.ltedi-1.8	Introduction	1	9	0.041666666666667	0.060810810810811	In the recent years, there has been an exponential rise in the number of studies focusing on the detection and management of hate speech and offensive language in social media (Kumar et al., 2018;	0
40665	40665	2021.ltedi-1.8	Introduction	2	10	0.083333333333333	0.067567567567568	Mandl et al., 2020;Zampieri et al., 2020).	0
40666	40666	2021.ltedi-1.8	Introduction	3	11	0.125	0.074324324324324	However, this has led to controlling user expression instead of improving user experience.	0
40667	40667	2021.ltedi-1.8	Introduction	4	12	0.166666666666667	0.081081081081081	(Ghanghor et al., 2021a;	0
40668	40668	2021.ltedi-1.8	Introduction	5	13	0.208333333333333	0.087837837837838	Hegde et al., 2021;Yasaswini et al., 2021).	0
40669	40669	2021.ltedi-1.8	Introduction	6	14	0.25	0.094594594594595	This has also resulted in putting barriers on modes of expression of different groups of people resulting in the violation of the principles of Equality, Diversity, and Inclusion (EDI).	0
40670	40670	2021.ltedi-1.8	Introduction	7	15	0.291666666666667	0.101351351351351	For instance, some NLP systems have classified the comments made in African American English as offensive language without accommodating the linguistic features peculiar to the dialect.	0
40671	40671	2021.ltedi-1.8	Introduction	8	16	0.333333333333333	0.108108108108108	Hence there is a need for a shift in the approaches taken to handle hate speech without compromising on the principles of EDI.	0
40672	40672	2021.ltedi-1.8	Introduction	9	17	0.375	0.114864864864865	We propose to shift the prevailing research direction in Natural Language Processing from controlling negativity (curbing hate speech etc.) to encouraging positivity (promoting hope speech).	0
40673	40673	2021.ltedi-1.8	Introduction	10	18	0.416666666666667	0.121621621621622	Hope speech is any expression that is positive, encouraging, supportive, and / or inspires promise of the future (Chakravarthi, 2020a).	0
40674	40674	2021.ltedi-1.8	Introduction	11	19	0.458333333333333	0.128378378378378	For the shared task organised in this connection, participants were provided with development, training and test dataset in English as well as in two under-resourced languages -Tamil and Malayalam.	0
40675	40675	2021.ltedi-1.8	Introduction	12	20	0.5	0.135135135135135	Tamil (ISO 639-3: tam) belongs to the Dravidian language family and is widely spoken in the southern state of Tamil Nadu in India, Sri Lanka, Malaysia and Singapore (Krishnamurti, 2003;	0
40676	40676	2021.ltedi-1.8	Introduction	13	21	0.541666666666667	0.141891891891892	Kolipakam et al., 2018;Chakravarthi, 2020b;Mahesan, 2019, 2020a,b). Malayalam (ISO 639-3: mal) also belongs to the Dravidian language family and is spoken in the Indian state of Kerala and the Union Territories of Lakshdweep and Puducherry (Chakravarthi et al., 2020).	0
40677	40677	2021.ltedi-1.8	Introduction	14	22	0.583333333333333	0.148648648648649	Both Tamil and Malayalam have their own scripts which are alphasyllabaries like other Indic scripts i.e. they are partially alphabetic and partially syllabic.	0
40678	40678	2021.ltedi-1.8	Introduction	15	23	0.625	0.155405405405405	The Tamil language was written using Tamili, Vattezhuthu, Chola, Pallava and Chola-Pallava scripts at different points in history.	0
40679	40679	2021.ltedi-1.8	Introduction	16	24	0.666666666666667	0.162162162162162	The modern Tamil script descended from the Chola-Pallava script that was conceived around the 6th century CE (Srinivasan, 2019).	0
40680	40680	2021.ltedi-1.8	Introduction	17	25	0.708333333333333	0.168918918918919	Malayalam was first written with Vattezhuthu script that evolved from Tamili script around 4-5th century CE (Mahadevan, 2003).	0
40681	40681	2021.ltedi-1.8	Introduction	18	26	0.75	0.175675675675676	Modern Malayalam is written using the Vattezhuttu alphabets extended with symbols from the Grantha script to accomodate non-native Sanskrit sounds (Krishnamurti, 2003).	0
40682	40682	2021.ltedi-1.8	Introduction	19	27	0.791666666666667	0.182432432432432	Although they have their own scripts, the social media comments in these languages are often written in Latin script as it is easy to input.	0
40683	40683	2021.ltedi-1.8	Introduction	20	28	0.833333333333333	0.189189189189189	Our dataset for the hope speech for EDI shared task was created from user-generated content in Tamil, Malayalam, and English (Chakravarthi and Muralidaran, 2021).	0
40684	40684	2021.ltedi-1.8	Introduction	21	29	0.875	0.195945945945946	The user-generated comments in our dataset for Tamil and Malayalam were codemixed (Chakravarthi, 2020a).	0
40685	40685	2021.ltedi-1.8	Introduction	22	30	0.916666666666667	0.202702702702703	We proposed a comment/post level classification task.	0
40686	40686	2021.ltedi-1.8	Introduction	23	31	0.958333333333333	0.209459459459459	The goal for the participants was to classify a given Youtube comment into either 'Hope speech', 'Non hope speech' or 'Not Tamil / Not Malayalam / Not English'.	1
40687	40687	2021.ltedi-1.8	Introduction	24	32	1.0	0.216216216216216	Our CodaLab website will remain open to allow researchers to access the data and build upon this work.	0
40805	40805	2021.wanlp-1.28	abstract	1	2	0.2	0.010471204188482	We present the findings and results of the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021).	0
40806	40806	2021.wanlp-1.28	abstract	2	3	0.4	0.015706806282723	This Shared Task includes four subtasks: country-level Modern Standard Arabic (MSA) identification (Subtask 1.1), country-level dialect identification (Subtask 1.2), province-level MSA identification (Subtask 2.1), and province-level sub-dialect identification (Subtask 2.2).	1
40807	40807	2021.wanlp-1.28	abstract	3	4	0.6	0.020942408376963	The shared task dataset covers a total of 100 provinces from 21 Arab countries, collected from the Twitter domain.	0
40808	40808	2021.wanlp-1.28	abstract	4	5	0.8	0.026178010471204	A total of 53 teams from 23 countries registered to participate in the tasks, thus reflecting the interest of the community in this area.	0
40809	40809	2021.wanlp-1.28	abstract	5	6	1.0	0.031413612565445	We received 16 submissions for Subtask 1.1 from five teams, 27 submissions for Subtask 1.2 from eight teams, 12 submissions for Subtask 2.1 from four teams, and 13 Submissions for subtask 2.2 from four teams.	0
41112	41112	2021.wanlp-1.36	Tasks Description	1	118	0.142857142857143	0.702380952380952	The shared task on sarcasm detection and sentiment analysis in Arabic contains two subtasks as follows:	1
41113	41113	2021.wanlp-1.36	Tasks Description	2	119	0.285714285714286	0.708333333333333	 Sarcasm Detection (subtask 1): the goal is to identify whether a tweet is sarcastic or not.	1
41114	41114	2021.wanlp-1.36	Tasks Description	3	120	0.428571428571429	0.714285714285714	 Sentiment Analysis (subtask 2): the goal is to classify the tweet to one of the sentiment classes: positive, negative or neutral.	1
41115	41115	2021.wanlp-1.36	Tasks Description	4	121	0.571428571428571	0.720238095238095	The data for both subtasks was provided as train/test split without a specific development set.	0
41116	41116	2021.wanlp-1.36	Tasks Description	5	122	0.714285714285714	0.726190476190476	Table 1 shows the statistics of the two sets.	0
41117	41117	2021.wanlp-1.36	Tasks Description	6	123	0.857142857142857	0.732142857142857	The training set consists of 12,548 tweets, while the testing set consists of 3,000 tweets.	0
41118	41118	2021.wanlp-1.36	Tasks Description	7	124	1.0	0.738095238095238	The participants had access to the tweets' text and the dialect label during the testing phase.	0
41171	41171	2021.wassa-1.10	Introduction	1	9	0.029411764705882	0.026627218934911	It is important to be able to analyze empathy and emotion in natural languages.	0
41172	41172	2021.wassa-1.10	Introduction	2	10	0.058823529411765	0.029585798816568	Emotion classification in natural languages has been studied over two decades and many applications successfully used emotion as their major components.	0
41173	41173	2021.wassa-1.10	Introduction	3	11	0.088235294117647	0.032544378698225	Empathy utterances can be emotional, therefore, examining emotion in text-based empathy possibly has a major impact on predicting empathy.	0
41174	41174	2021.wassa-1.10	Introduction	4	12	0.117647058823529	0.035502958579882	Analyzing text-based empathy and emotion have different applications; empathy is a crucial component in applications such as empathic AI agents, effective gesturing of robots, and mental health, emotion has natural language applications such as commerce, public health, and disaster management.	0
41175	41175	2021.wassa-1.10	Introduction	5	13	0.147058823529412	0.038461538461539	In this paper, we present the WASSA 2021 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories.	0
41176	41176	2021.wassa-1.10	Introduction	6	14	0.176470588235294	0.041420118343195	This shared task included two individual tasks where teams develop models to predict emotions and empathy in essays in which people expressed their empathy and distress in reaction to news articles in which an individual, group of people or nature was harmed.	0
41177	41177	2021.wassa-1.10	Introduction	7	15	0.205882352941176	0.044378698224852	Additionally, the dataset also included the demographic information of the authors of the essays such as age, gender, ethnicity, income, and education level, and personality information (details of the collection of the dataset is provided in section 3).	0
41178	41178	2021.wassa-1.10	Introduction	8	16	0.235294117647059	0.047337278106509	Optionally, we suggested that the teams could also use emotion labels when modeling empathy to learn more about the impact of emotions on empathy.	0
41179	41179	2021.wassa-1.10	Introduction	9	17	0.264705882352941	0.050295857988166	The shared task consisted of two tracks:	1
41180	41180	2021.wassa-1.10	Introduction	10	18	0.294117647058823	0.053254437869823	"1. Predicting Empathy (EMP): the formulation of this track is to predict the Batson empathic concern (""feeling for someone"") and personal distress (""suffering with someone"") using the essay, personality information, demographic information, and emotion."	1
41181	41181	2021.wassa-1.10	Introduction	11	19	0.323529411764706	0.056213017751479	2. Emotion Label Prediction (EMO): the formulation of this track is to predict emotion tags (sadness, joy, disgust, surprise, anger, or fear), taken from Ekman's six basic emotions (Ekman, 1971), plus no-emotion tag for essays.	1
41182	41182	2021.wassa-1.10	Introduction	12	20	0.352941176470588	0.059171597633136	In this setting personality and demographic information as well as empathy and distress scores were also made available and optional to use.	0
41183	41183	2021.wassa-1.10	Introduction	13	21	0.382352941176471	0.062130177514793	For both tasks, an identical train-dev-test split was provided.	0
41184	41184	2021.wassa-1.10	Introduction	14	22	0.411764705882353	0.06508875739645	The dataset consists of essays that were collected from participants, who had read disturbing news articles about a person, a group of people, or painful situations.	0
41185	41185	2021.wassa-1.10	Introduction	15	23	0.441176470588235	0.068047337278107	Empathy, distress, demographic, and personality information was taken from the original work by Buechel et al. (2018).	0
41186	41186	2021.wassa-1.10	Introduction	16	24	0.470588235294118	0.071005917159763	They used Batson's Empathic Concern -Personal Distress Scale (Batson et al., 1987), i.e, rating 6 items for empathy (i.e., warm, tender, sympathetic, softhearted, moved, compassionate) and 8 items for distress (i.e., worried, upset, troubled, perturbed, grieved, disturbed, alarmed, distressed) using a 7point scale for each of these items (detailed information can be found in the Appendix section of the original paper).	0
41187	41187	2021.wassa-1.10	Introduction	17	25	0.5	0.07396449704142	Regarding emotion, all data was annotated with the six basic Ekman emotions (sadness, joy, disgust, surprise, anger, or fear).	0
41188	41188	2021.wassa-1.10	Introduction	18	26	0.529411764705882	0.076923076923077	Five teams participated in this shared task, three participated in both tracks, and each time one additional team participated in either the EMP or EMO track.	0
41189	41189	2021.wassa-1.10	Introduction	19	27	0.558823529411765	0.079881656804734	During the evaluation phase, every team was allowed to submit their results until a certain deadline, after which the final submission was taken into consideration for the ranking.	0
41190	41190	2021.wassa-1.10	Introduction	20	28	0.588235294117647	0.082840236686391	The best result for the empathy prediction track was an average Pearson correlation of 0.545 and the best macro F1-score for the emotion track amounted to 55%.	0
41191	41191	2021.wassa-1.10	Introduction	21	29	0.617647058823529	0.085798816568047	All tasks were designed in CodaLab 1 and the teams were allowed to submit one official result during evaluation phase and several ones during the training phase.	0
41192	41192	2021.wassa-1.10	Introduction	22	30	0.647058823529412	0.088757396449704	In the remainder of this paper we first review related work (Section 2), after which we introduce the dataset used for both tracks (Section 3).	0
41193	41193	2021.wassa-1.10	Introduction	23	31	0.676470588235294	0.091715976331361	The shared task is presented in Section 4 and the official results in Section 5.	0
41194	41194	2021.wassa-1.10	Introduction	24	32	0.705882352941176	0.094674556213018	A discussion of the different systems participating in both tracks is presented in Section 6 and we conclude our work in Section 7.	0
41195	41195	2021.wassa-1.10	Introduction	25	33	0.735294117647059	0.097633136094675	1	0
41196	41196	2021.wassa-1.10	Introduction	26	34	0.764705882352941	0.100591715976331	Task descriptions, datasets, and results are designed in CodaLab https://competitions.codalab.org/ competitions/28713 2 Related Work Emotion has been studied for two decades and a large body of works have provided insights and remarkable findings.	0
41197	41197	2021.wassa-1.10	Introduction	27	35	0.794117647058823	0.103550295857988	In contrast, detecting and predicting empathy and distress in text is a growing field and there is little work on the correlation and relatedness of emotion, empathy, and distress.	0
41198	41198	2021.wassa-1.10	Introduction	28	36	0.823529411764706	0.106508875739645	This shared task is designed to study the modeling of empathy and distress and the correlation among them.	0
41199	41199	2021.wassa-1.10	Introduction	29	37	0.852941176470588	0.109467455621302	In the literature empathy is considered towards negative events, however, recent studies suggest that people's joyful emotions towards positive events can be termed as positive empathy (Morelli et al., 2015).	0
41200	41200	2021.wassa-1.10	Introduction	30	38	0.882352941176471	0.112426035502959	The psychological theory distinguishes two separate constructs for distress and empathy; distress is a self-focused, negative affective state (suffering with someone), and empathy is a warm, tender, and compassionate state (feeling for someone).	0
41201	41201	2021.wassa-1.10	Introduction	31	39	0.911764705882353	0.115384615384615	To quantify empathy and distress, studies present different approaches, the most popular one is Batson's Empathic Concern -Personal Distress Scale (Batson et al., 1987), which is used to obtain empathy and distress scores for each essay in this dataset.	0
41202	41202	2021.wassa-1.10	Introduction	32	40	0.941176470588235	0.118343195266272	To annotate emotions in text, classical studies in NLP suggest categorical tagsets, and most studies are focused on basic emotion models that are suggested by psychological emotion models.	0
41203	41203	2021.wassa-1.10	Introduction	33	41	0.970588235294118	0.121301775147929	The most popular ones are the Ekman 6 basic emotions (Ekman, 1971), the Plutchik 8 basic emotions (Plutchik, 1984), and 4 basic emotions (Frijda, 1988).	0
41204	41204	2021.wassa-1.10	Introduction	34	42	1.0	0.124260355029586	We opted for the Ekman emotions, because this model is well adopted in different downstream NLP tasks of which emotion is a component, and it is most suited to the dataset we aim to study in this shared task.	0
41524	41524	W18-5501	Task Description	1	24	0.5	0.117073170731707	Candidate systems for the FEVER shared task were given a sentence of unknown veracity called a claim.	1
41525	41525	W18-5501	Task Description	2	25	1.0	0.121951219512195	The systems must identify suitable evidence from Wikipedia at the sentence level and Claim: The Rodney King riots took place in the most populous county in the USA.	1
41707	41707	W18-6206	abstract	1	2	0.1	0.009803921568627	Past shared tasks on emotions use data with both overt expressions of emotions (I am so happy to see you!) as well as subtle expressions where the emotions have to be inferred, for instance from event descriptions.	0
41708	41708	W18-6206	abstract	2	3	0.2	0.014705882352941	Further, most datasets do not focus on the cause or the stimulus of the emotion.	0
41709	41709	W18-6206	abstract	3	4	0.3	0.019607843137255	Here, for the first time, we propose a shared task where systems have to predict the emotions in a large automatically labeled dataset of tweets without access to words denoting emotions.	1
41710	41710	W18-6206	abstract	4	5	0.4	0.024509803921569	Based on this intention, we call this the Implicit Emotion Shared Task (IEST) because the systems have to infer the emotion mostly from the context.	0
41711	41711	W18-6206	abstract	5	6	0.5	0.029411764705882	Every tweet has an occurrence of an explicit emotion word that is masked.	0
41712	41712	W18-6206	abstract	6	7	0.6	0.034313725490196	The tweets are collected in a manner such that they are likely to include a description of the cause of the emotion -the stimulus.	0
41713	41713	W18-6206	abstract	7	8	0.7	0.03921568627451	Altogether, 30 teams submitted results which range from macro F 1 scores of 21 % to 71 %.	0
41714	41714	W18-6206	abstract	8	9	0.8	0.044117647058824	The baseline (Max-Ent bag of words and bigrams) obtains an F 1 score of 60 % which was available to the participants during the development phase.	0
41715	41715	W18-6206	abstract	9	10	0.9	0.049019607843137	A study with human annotators suggests that automatic methods outperform human predictions, possibly by honing into subtle textual clues not used by humans.	0
41716	41716	W18-6206	abstract	10	11	1.0	0.053921568627451	Corpora, resources, and results are available at the shared task website at http://implicitemotions.wassa2018.com.	0
41911	41911	W18-6402	abstract	1	2	0.142857142857143	0.007662835249042	We present the results from the third shared task on multimodal machine translation.	0
41912	41912	W18-6402	abstract	2	3	0.285714285714286	0.011494252873563	In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech.	1
41913	41913	W18-6402	abstract	3	4	0.428571428571429	0.015325670498084	The image can be used in addition to (or instead of) the source sentence.	0
41914	41914	W18-6402	abstract	4	5	0.571428571428571	0.019157088122605	This year the task was extended with a third target language (Czech) and a new test set.	0
41915	41915	W18-6402	abstract	5	6	0.714285714285714	0.022988505747127	In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech.	0
41916	41916	W18-6402	abstract	6	7	0.857142857142857	0.026819923371648	Seven teams submitted 45 different systems to the two variants of the task.	0
41917	41917	W18-6402	abstract	7	8	1.0	0.030651340996169	Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive.	0
42173	42173	D19-5309	abstract	1	3	0.166666666666667	0.011904761904762	While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited.	0
42174	42174	D19-5309	abstract	2	4	0.333333333333333	0.015873015873016	The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semistructured tables.	1
42175	42175	D19-5309	abstract	3	5	0.5	0.01984126984127	"Each explanation contains between 1 and 16 interconnected facts that form an ""explanation graph"" spanning core scientific knowledge and detailed world knowledge."	0
42176	42176	D19-5309	abstract	4	6	0.666666666666667	0.023809523809524	It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multihop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus.	0
42177	42177	D19-5309	abstract	5	7	0.833333333333333	0.027777777777778	The top-performing system achieved a mean average precision (MAP) of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model.	0
42178	42178	D19-5309	abstract	6	8	1.0	0.031746031746032	Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multihop reasoning.	0
42434	42434	D19-5701	Introduction	1	12	0.058823529411765	0.08955223880597	Efficient access to mentions of drugs, medications and chemical entities contained in clinical texts, scientific articles, patents or even the web is a pressing need shared by biomedical researchers and clinicians .	0
42435	42435	D19-5701	Introduction	2	13	0.117647058823529	0.097014925373134	Biomedical text mining is one of the most prolific application domains of natural language processing technologies (Zweigenbaum et al., 2007).	0
42436	42436	D19-5701	Introduction	3	14	0.176470588235294	0.104477611940299	The recognition of pharmaceutical drugs/chemical entities is a critical step required for the subsequent detection of relations with other biomedically relevant entities such as genes/proteins, diseases or adverse reactions (Vazquez et al., 2011).	0
42437	42437	D19-5701	Introduction	4	15	0.235294117647059	0.111940298507463	Text mining and information extraction systems were published that tried to find protein-drug relations (including ligand-protein interactions and pharmacogenomics information), medication-related al-lergies, chemical metabolic reactions, drug-drug interactions (Herrero-Zazo et al., 2013), diseasedrug relations, as well as drug safety-related issues.	0
42438	42438	D19-5701	Introduction	5	16	0.294117647058823	0.119402985074627	The correct identification of drug mentions is also needed for other complex relation types like drug dosage recognition, duration of medical treatments or drug repurposing.	0
42439	42439	D19-5701	Introduction	6	17	0.352941176470588	0.126865671641791	The importance of chemical and drug name recognition motivated several-shared tasks in the past, such as the CHEMDNER tracks (Krallinger et al., 2015) or the i2b2 medication challenge (Uzuner et al., 2010b,a), with a considerable number of participants and impact (Doan et al., 2010;	0
42440	42440	D19-5701	Introduction	7	18	0.411764705882353	0.134328358208955	Yang, 2010).	0
42441	42441	D19-5701	Introduction	8	19	0.470588235294118	0.141791044776119	Currently, most of the biomedical and clinical NLP research, is done on English documents, while only few tasks were carried out using non-English texts, or were multilingual.	0
42442	42442	D19-5701	Introduction	9	20	0.529411764705882	0.149253731343284	Nonetheless, it is important to highlight that there is a considerable amount of biomedically relevant content published in other languages than English, and particularly clinical texts are entirely written in the native language of each country.	0
42443	42443	D19-5701	Introduction	10	21	0.588235294117647	0.156716417910448	Spanish is a language spoken by more than 572 million people in the world today, either as a native, second or foreign language.	0
42444	42444	D19-5701	Introduction	11	22	0.647058823529412	0.164179104477612	It is the second language in the world by number of native speakers with more than 477 million people.	0
42445	42445	D19-5701	Introduction	12	23	0.705882352941176	0.171641791044776	According to results derived from WHO statistics, just in Spain there are over 180 thousand practicing physicians, more than 247 thousand nursing and midwifery personnel or 55 thousand pharmaceutical personnel.	0
42446	42446	D19-5701	Introduction	13	24	0.764705882352941	0.17910447761194	These facts, and the extrapolation to other Spanish speaking countries explains why a considerable subset of the PubMed database records corresponds to Spanish medical articles.	0
42447	42447	D19-5701	Introduction	14	25	0.823529411764706	0.186567164179104	Moreover, PubMed does only contain a part of the medical literature originally published in Spanish, which is also stored in other resources such as MEDES, SciELO, IBECS or CUIDEN.	0
42448	42448	D19-5701	Introduction	15	26	0.882352941176471	0.194029850746269	Following the outline of previous chemical/drug NER efforts, in particular the BioCreative CHEMDNER tracks, we have carried out the first task on chemical and drug mention recognition from Spanish medical texts, namely from a corpus of Spanish clinical case studies.	0
42449	42449	D19-5701	Introduction	16	27	0.941176470588235	0.201492537313433	Thus, this track addressed the automatic extraction of chemical, drug, gene/protein mentions from clinical case studies written in Spanish.	1
42450	42450	D19-5701	Introduction	17	28	1.0	0.208955223880597	The main aim was to promote the development of named entity recognition tools of practical relevance, that is, chemi-cal and drug mentions in non-English content, determining the current-state-of-the art, identifying challenges and comparing the strategies and results to those published for English data.	0
42761	42761	D19-5729	Introduction and Motivation	1	6	0.023809523809524	0.02970297029703	The breadth of brain research is too expansive to be effectively curated without computational tools especially involving machine learning models.	0
42762	42762	D19-5729	Introduction and Motivation	2	7	0.047619047619048	0.034653465346535	"For example, a Pubmed search for ""Brain"" on August 12, 2019, revealed 854,612 articles 1 . More specifically, an August 12, 2019 search for the single mental illness diagnosis of ""depression"" revealed 530,519 articles 2 ."	0
42763	42763	D19-5729	Introduction and Motivation	3	8	0.071428571428572	0.03960396039604	And a search for anxiety revealed 224,305 articles 3 .	0
42764	42764	D19-5729	Introduction and Motivation	4	9	0.095238095238095	0.044554455445545	It is not possible for researchers to functionally analyze all of the critical data patterns both within a single diagnosis or across diagnoses that could be revealed by those articles.	0
42765	42765	D19-5729	Introduction and Motivation	5	10	0.119047619047619	0.04950495049505	The challenge of curating brain research has been further complicated by the National Institute of Mental Health's adoption of the Research Domain Criteria (RDoC) [6].	0
42766	42766	D19-5729	Introduction and Motivation	6	11	0.142857142857143	0.054455445544555	"Since 1952, the Diagnostic and Statistical Manual of Mental Disorders 1 Pubmed search for Brain conducted on August 12, 2019 2 Pubmed search for depression conducted on August 12, 2019 3 Pubmed search for anxiety conducted on August 12, 2019 and International Classification of Diseases [5] (popularly known as DSM and ICD, respectively), have ""reigned supreme"" as the single ""overarching model of psychiatric classification"" [14]."	0
42767	42767	D19-5729	Introduction and Motivation	7	12	0.166666666666667	0.059405940594059	That supremacy began to crumble in 2010 when the National Institute of Mental Health launched the RDoC initiative, an alternate framework to conceptually organize and direct biological research on mental disorders [1].	0
42768	42768	D19-5729	Introduction and Motivation	8	13	0.19047619047619	0.064356435643564	"The RDoC initiative intends ""to foster integration not only of psychological and biological measures but also of the psychological and biological constructs those measures measure"" [13]."	0
42769	42769	D19-5729	Introduction and Motivation	9	14	0.214285714285714	0.069306930693069	The RDoC initiative has fostered significant debate among brain health researchers.	0
42770	42770	D19-5729	Introduction and Motivation	10	15	0.238095238095238	0.074257425742574	It has also created a significant categorization challengespecifically how to curate articles completed under the DSM-ICD criteria so their data can be incorporated into the RDoC model.	0
42771	42771	D19-5729	Introduction and Motivation	11	16	0.261904761904762	0.079207920792079	Brain science cannot afford to lose critical insights from the numerous articles on different sides of the categorization divide.	0
42772	42772	D19-5729	Introduction and Motivation	12	17	0.285714285714286	0.084158415841584	Hence, it is vital that all existing and future biomedical literature related to brain research is correctly categorized with respect to the RDoC terminology in addition to DSM-ICD models.	0
42773	42773	D19-5729	Introduction and Motivation	13	18	0.30952380952381	0.089108910891089	However, manual curation of brain research articles using RDoC terminology by human annotators can be highly resource-consuming due to several reasons.	0
42774	42774	D19-5729	Introduction and Motivation	14	19	0.333333333333333	0.094059405940594	RDoC framework is comprehensive and complex.	0
42775	42775	D19-5729	Introduction and Motivation	15	20	0.357142857142857	0.099009900990099	It is made up six major domains of human functioning, which is further broken down to multiple constructs that comprise different aspects of the overall range of functions 4 .	0
42776	42776	D19-5729	Introduction and Motivation	16	21	0.380952380952381	0.103960396039604	The RDoC matrix helps describe these constructs using several units of analysis such as molecules and circuits.	0
42777	42777	D19-5729	Introduction and Motivation	17	22	0.404761904761905	0.108910891089109	On top of this, the rate of publication of biomedical literature (and by extension brain re-search related literature) is growing at an exponential rate [10].	0
42778	42778	D19-5729	Introduction and Motivation	18	23	0.428571428571429	0.113861386138614	This means that the gap between annotated versus unannotated articles will continue to grow at an alarming rate unless more efficient means of automated annotation is developed soon.	0
42779	42779	D19-5729	Introduction and Motivation	19	24	0.452380952380952	0.118811881188119	In order to invite text mining teams around the world to develop informatics models for RDoC, we introduced the RDoC Task 5 at this years'	0
42780	42780	D19-5729	Introduction and Motivation	20	25	0.476190476190476	0.123762376237624	BioNLP-OST 2019 workshop 6 . RDoC task is a combination of two subtasks focusing on a subset of RDoC constructs: (a) Task 1 (RDoC-IR) -retrieving PubMed Abstracts related to RDoC constructs, and (b) Task 2 (RDoC-SE) -extracting the most relevant sentence for a given RDoC construct from a known relevant abstract.	1
42781	42781	D19-5729	Introduction and Motivation	21	26	0.5	0.128712871287129	Both these tasks represent two very important steps of the typical triage process [10], which are finding the articles related to RDoC constructs and then extracting a specific snippet of information that is useful for curation or downstream tasks such as automatic text summarization [15].	0
42782	42782	D19-5729	Introduction and Motivation	22	27	0.523809523809524	0.133663366336634	There have been several shared tasks on text mining from biomedical literature and clinical notes in the last decade [19,12] as well as a few shared tasks related to mental health topics ( [4,18,22,21,30]).	0
42783	42783	D19-5729	Introduction and Motivation	23	28	0.547619047619048	0.138613861386139	CLPsych 2015 Shared Task [4] focused on identifying depression and PTSD users from twitter data, while the same task from the following year (i.e. CLPsych 2016 Shared Task [18]) revolved around classifying the severity of peer support forum posts.	0
42784	42784	D19-5729	Introduction and Motivation	24	29	0.571428571428571	0.143564356435644	One of the i2b2 7 challenges from 2011 focused on the sentiment analysis of suicide notes [22,21].	0
42785	42785	D19-5729	Introduction and Motivation	25	30	0.595238095238095	0.148514851485149	"In 2017, Uzuner et al. introduced the ""The RDoC for Psychiatry"" challenge, which was composed of three tracks: de-identification of mental health records [28], determination of symptom severity from a psychiatric evaluation of a patient) related to one of the RDoC domains) [9], and the use of mental health records released through the challenge for answering novel questions [32,29,7]."	0
42786	42786	D19-5729	Introduction and Motivation	26	31	0.619047619047619	0.153465346534653	In contrast, the RDoC task is a combination of information retrieval and sentence extraction from Biomedical literature related to RDoC constructs.	0
42787	42787	D19-5729	Introduction and Motivation	27	32	0.642857142857143	0.158415841584158	To generate benchmark data for the RDoC task, three annotators were used to curate the goldstandard datasets.	0
42788	42788	D19-5729	Introduction and Motivation	28	33	0.666666666666667	0.163366336633663	The registration for the RDoC Task opened in March of 2019.	0
42789	42789	D19-5729	Introduction and Motivation	29	34	0.69047619047619	0.168316831683168	Over 30 teams around the world registered for the two tasks.	0
42790	42790	D19-5729	Introduction and Motivation	30	35	0.714285714285714	0.173267326732673	Training data in two batches were released in the month of April.	0
42791	42791	D19-5729	Introduction and Motivation	31	36	0.738095238095238	0.178217821782178	Test data, again in two batches, were released in June.	0
42792	42792	D19-5729	Introduction and Motivation	32	37	0.761904761904762	0.183168316831683	The participants were asked to submit their final predictions by June 19.	0
42793	42793	D19-5729	Introduction and Motivation	33	38	0.785714285714286	0.188118811881188	Eventually, 4 and 5 groups each competed in Tasks 1 and 2, respectively.	0
42794	42794	D19-5729	Introduction and Motivation	34	39	0.80952380952381	0.193069306930693	The final results were made public immediately after the submission deadline.	0
42795	42795	D19-5729	Introduction and Motivation	35	40	0.833333333333333	0.198019801980198	Two (out of four) and four (out of five) teams each outperformed the baseline methods in task 1 and 2, respectively.	0
42796	42796	D19-5729	Introduction and Motivation	36	41	0.857142857142857	0.202970297029703	The increase in performance over the baselines were more noticeable in task 2 suggesting that information retrieval for RDoC task may be more challenging.	0
42797	42797	D19-5729	Introduction and Motivation	37	42	0.880952380952381	0.207920792079208	There was quite a lot of variation across the several RDoC constructs used for the tasks suggesting that the complexity of different constructs may hinder certain models and construct-specific methods or models may be a requirement in the future.	0
42798	42798	D19-5729	Introduction and Motivation	38	43	0.904761904761905	0.212871287128713	Overall observations from the RDoC Task highlights the need for more sophisticated method development.	0
42799	42799	D19-5729	Introduction and Motivation	39	44	0.928571428571429	0.217821782178218	The rest of the paper is organized as follows.	0
42800	42800	D19-5729	Introduction and Motivation	40	45	0.952380952380952	0.222772277227723	Section 2 describes the benchmark or goldstandard data preparation process, development of training and test sets, submission requirements, baseline methods used by the organizers, and the performance measures used for the evaluation.	0
42801	42801	D19-5729	Introduction and Motivation	41	46	0.976190476190476	0.227722772277228	Section 3 presents and discusses the overall results for the two tasks.	0
42802	42802	D19-5729	Introduction and Motivation	42	47	1.0	0.232673267326733	Finally, Section 4 summarizes the task findings as well as describes the potential future work.	0
43173	43173	2020.sigtyp-1.1	Task Description	1	36	0.142857142857143	0.214285714285714	The SIGTYP 2020 shared task is concerned with predicting typological features from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013).	1
43174	43174	2020.sigtyp-1.1	Task Description	2	37	0.285714285714286	0.220238095238095	For the task, participants were invited to build systems to predict features for languages unseen at training time.	0
43175	43175	2020.sigtyp-1.1	Task Description	3	38	0.428571428571429	0.226190476190476	The shared task consisted of two subtasks: 1) the constrained setting, for which only the provided training data may be used; 2) the unconstrained setting, for which training data may be extended with any external source of information (e.g. pre-trained embeddings, additional text, etc.)	0
43176	43176	2020.sigtyp-1.1	Task Description	4	39	0.571428571428571	0.232142857142857	Data Format	0
43177	43177	2020.sigtyp-1.1	Task Description	5	40	0.714285714285714	0.238095238095238	For each instance, the following information is provided: the language code, name, latitude, longitude, genus, family, country code, and features.	0
43178	43178	2020.sigtyp-1.1	Task Description	6	41	0.857142857142857	0.244047619047619	At training time, both the feature names and feature values are given, while at test time, submitted systems are required to fill values for the requested features.	0
43179	43179	2020.sigtyp-1.1	Task Description	7	42	1.0	0.25	An example of a test instance is given in Table 1.	0
43307	43307	2020.wmt-1.3	abstract	1	2	0.142857142857143	0.010471204188482	We report the results of the first edition of the WMT shared task on Chat Translation.	0
43308	43308	2020.wmt-1.3	abstract	2	3	0.285714285714286	0.015706806282723	The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer).	1
43309	43309	2020.wmt-1.3	abstract	3	4	0.428571428571429	0.020942408376963	This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical.	0
43310	43310	2020.wmt-1.3	abstract	4	5	0.571428571428571	0.026178010471204	Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns.	0
43311	43311	2020.wmt-1.3	abstract	5	6	0.714285714285714	0.031413612565445	We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En	0
43312	43312	2020.wmt-1.3	abstract	6	7	0.857142857142857	0.036649214659686	De for agent utterances and DeEn for customer messages.	0
43313	43313	2020.wmt-1.3	abstract	7	8	1.0	0.041884816753927	We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments to evaluate the agent translations.	0
43591	43591	2020.wnut-1.33	Summary	1	95	0.2	0.95959595959596	In this paper, we presented a shared task for consisting of two sub-tasks: named entity recognition and relation extraction from the wet lab protocols.	1
43592	43592	2020.wnut-1.33	Summary	2	96	0.4	0.96969696969697	We described the task setup and datasets details, and also outlined the approach taken by the participating systems.	0
43593	43593	2020.wnut-1.33	Summary	3	97	0.6	0.97979797979798	The shared task included larger and improvised dataset compared to the prior literature (Kulkarni et al., 2018).	0
43594	43594	2020.wnut-1.33	Summary	4	98	0.8	0.98989898989899	This improvised dataset enables us to draw stronger conclusions about the true potential of different approaches.	0
43595	43595	2020.wnut-1.33	Summary	5	99	1.0	1.0	It also facilitates us in analyzing the results of the participating systems, which aids us in suggesting potential research directions for both future shared tasks and noisy text processing in user generated lab protocols.	0
43601	43601	2020.wnut-1.41	Introduction	1	6	0.058823529411765	0.061855670103093	As of late-September 2020, the COVID-19 Coronavirus pandemic has led to about 1M deaths and 33M infected patients from 213 countries and territories, creating fear and panic for people all around the world.	0
43602	43602	2020.wnut-1.41	Introduction	2	7	0.117647058823529	0.072164948453608	1 Recently, much attention has been paid to building monitoring systems (e.g. The Johns Hopkins Coronavirus Dashboard) to track the development of the pandemic and to provide users the information related to the virus, 2 e.g. any new suspicious/confirmed cases near/in the users' regions.	0
43603	43603	2020.wnut-1.41	Introduction	3	8	0.176470588235294	0.082474226804124	"It is worth noting that most of the ""official"" sources used in the tracking tools are not frequently kept up to date with the current pandemic situation, e.g. WHO updates the pandemic information only once a day."	0
43604	43604	2020.wnut-1.41	Introduction	4	9	0.235294117647059	0.092783505154639	Those monitoring systems thus use social network data, e.g. from Twit-ter, as a real-time alternative source for updating the pandemic information, generally by crowdsourcing or searching for related information manually.	0
43605	43605	2020.wnut-1.41	Introduction	5	10	0.294117647058823	0.103092783505155	However, the pandemic has been spreading rapidly; we observe a massive amount of data on social networks, e.g. about 3.5M of COVID-19 English Tweets posted daily on the Twitter platform (Lamsal, 2020) in which the majority are uninformative.	0
43606	43606	2020.wnut-1.41	Introduction	6	11	0.352941176470588	0.11340206185567	Thus, it is important to be able to select the informative Tweets (e.g. COVID-19 Tweets related to new cases or suspicious cases) for downstream applications.	0
43607	43607	2020.wnut-1.41	Introduction	7	12	0.411764705882353	0.123711340206186	However, manual approaches to identify the informative Tweets require significant human efforts, do not scale with rapid developments, and are costly.	0
43608	43608	2020.wnut-1.41	Introduction	8	13	0.470588235294118	0.134020618556701	To help handle the problem, we propose a shared task which is to automatically identify whether a COVID-19 English Tweet is informative or not.	0
43609	43609	2020.wnut-1.41	Introduction	9	14	0.529411764705882	0.144329896907216	Our task is defined as a binary classification problem: Given an English Tweet related to COVID-19, decide whether it should be classified as INFORMATIVE or UNINFORMATIVE.	1
43610	43610	2020.wnut-1.41	Introduction	10	15	0.588235294117647	0.154639175257732	Here, informative Tweets provide information about suspected, confirmed, recovered and death cases as well as the location or travel history of the cases.	1
43611	43611	2020.wnut-1.41	Introduction	11	16	0.647058823529412	0.164948453608247	The goals of our shared task are: (i)	0
43612	43612	2020.wnut-1.41	Introduction	12	17	0.705882352941176	0.175257731958763	To develop a language processing task that potentially impacts research and downstream applications, and (ii)	0
43613	43613	2020.wnut-1.41	Introduction	13	18	0.764705882352941	0.185567010309278	To provide the research community with a new dataset for identifying informative COVID-19 English Tweets.	0
43614	43614	2020.wnut-1.41	Introduction	14	19	0.823529411764706	0.195876288659794	To achieve the goals, we manually construct a dataset of 10K COVID-19 English Tweets with INFORMATIVE and UNIN-FORMATIVE labels.	0
43615	43615	2020.wnut-1.41	Introduction	15	20	0.882352941176471	0.206185567010309	We believe that the dataset and systems developed for our task will be beneficial for the development of COVID-19 monitoring systems.	0
43616	43616	2020.wnut-1.41	Introduction	16	21	0.941176470588235	0.216494845360825	All practical information, data download links and the final evaluation results can be found at the CodaLab website of our shared task: https://competitions.codalab. org/competitions/25845.	0
43617	43617	2020.wnut-1.41	Introduction	17	22	1.0	0.22680412371134	2 The WNUT-2020 Task 2 dataset	0
43704	43704	W18-0507	Task Description	1	12	0.055555555555556	0.061538461538462	The goal of the CWI shared task of 2018 is to predict which words challenge non-native speakers based on the annotations collected from both native and non-native speakers.	1
43705	43705	W18-0507	Task Description	2	13	0.111111111111111	0.066666666666667	To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity.	0
43706	43706	W18-0507	Task Description	3	14	0.166666666666667	0.071794871794872	One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation.	0
43707	43707	W18-0507	Task Description	4	15	0.222222222222222	0.076923076923077	More information about the data collection is presented in Section 3.	0
43708	43708	W18-0507	Task Description	5	16	0.277777777777778	0.082051282051282	Given the multilingual dataset provided, the CWI challenge was divided into four tracks:	0
43709	43709	W18-0507	Task Description	6	17	0.333333333333333	0.087179487179487	 English monolingual CWI;	0
43710	43710	W18-0507	Task Description	7	18	0.388888888888889	0.092307692307692	 German monolingual CWI;	0
43711	43711	W18-0507	Task Description	8	19	0.444444444444444	0.097435897435898	 Spanish monolingual CWI; and	0
43712	43712	W18-0507	Task Description	9	20	0.5	0.102564102564103	 Multilingual CWI with a French test set.	0
43713	43713	W18-0507	Task Description	10	21	0.555555555555556	0.107692307692308	For the first three tracks, participants were provided with training and testing data for the same language.	0
43714	43714	W18-0507	Task Description	11	22	0.611111111111111	0.112820512820513	For French, participants were provided only with a French test set and no French training data.	0
43715	43715	W18-0507	Task Description	12	23	0.666666666666667	0.117948717948718	In the CWI 2016, the task was cast as binary classification.	0
43716	43716	W18-0507	Task Description	13	24	0.722222222222222	0.123076923076923	To be able to capture complexity as a continuum, in our CWI 2018 shared task, we additionally included a probabilistic classification task.	0
43717	43717	W18-0507	Task Description	14	25	0.777777777777778	0.128205128205128	The two tasks are summarized as follows:	0
43718	43718	W18-0507	Task Description	15	26	0.833333333333333	0.133333333333333		0
43719	43719	W18-0507	Task Description	16	27	0.888888888888889	0.138461538461538	Binary classification task: Participants were asked to label the target words in context as complex (1) or simple (0).	0
43720	43720	W18-0507	Task Description	17	28	0.944444444444444	0.143589743589744	 Probabilistic classification task: Participants were asked to assign the probability of target words in context being complex.	0
43721	43721	W18-0507	Task Description	18	29	1.0	0.148717948717949	Participants were free to choose the task/track combinations they would like to participate in.	0
43894	43894	W18-0604	Introduction	1	7	0.038461538461539	0.034313725490196	The ability to accurately predict current and future psychological health could be transformative in providing more personalized and efficient mental health care.	0
43895	43895	W18-0604	Introduction	2	8	0.076923076923077	0.03921568627451	Currently, the mental health care industry is strained and overworked, and many conditions are on the rise among certain populations.	0
43896	43896	W18-0604	Introduction	3	9	0.115384615384615	0.044117647058824	For example, suicide rates are climbing among veterans (USDVA, 2016) and youths (CDC, 2017).	0
43897	43897	W18-0604	Introduction	4	10	0.153846153846154	0.049019607843137	Data-driven linguistic analysis offers a particularly attractive complement or alternative to traditional risk assessments, particularly in a clinical setting.	0
43898	43898	W18-0604	Introduction	5	11	0.192307692307692	0.053921568627451	Language analysis is often relatively fast and easy to conduct at scale.	0
43899	43899	W18-0604	Introduction	6	12	0.230769230769231	0.058823529411765	Further, whereas traditional risk assessments are typically limited to capturing one or a few psychological factors, language analysis has the advantage of being theoretically unlimited in what it can capture.	0
43900	43900	W18-0604	Introduction	7	13	0.269230769230769	0.063725490196079	By evaluating the relationship between linguistic markers and lifetime health outcomes, such research may provide benefits for intake assessment, monitoring, and preventative care.	0
43901	43901	W18-0604	Introduction	8	14	0.307692307692308	0.068627450980392	Computational linguistics has now shown strong potential for aiding in mental health assessment and treatment.	0
43902	43902	W18-0604	Introduction	9	15	0.346153846153846	0.073529411764706	With few exceptions (e.g. De Choudhury et al. (2016), Sadeque et al. (2016)), work thus far from the NLP community has focused on predicting current mental health from language, and most exceptions have still only looked at the short-term future.	0
43903	43903	W18-0604	Introduction	10	16	0.384615384615385	0.07843137254902	While such research is valuable, predictions about the long-term future can aid with another class of applications: the understanding of early life markers and development of preventative care.	0
43904	43904	W18-0604	Introduction	11	17	0.423076923076923	0.083333333333333	Here we describe the CLPsych 2018 shared task, the purpose of which is to evaluate multiple methods for analyzing linguistic markers as a signal for current and future psychological outcomes (i.e. risk assessment).	1
43905	43905	W18-0604	Introduction	12	18	0.461538461538462	0.088235294117647	We present three tasks centered around this goal:	1
43906	43906	W18-0604	Introduction	13	19	0.5	0.093137254901961	Task A focuses on cross-sectional psychological health at age 11, based on essays written at childhood.	1
43907	43907	W18-0604	Introduction	14	20	0.538461538461538	0.098039215686275	Task B uses these childhood essays to measure psychological distress across multiple life stages.	1
43908	43908	W18-0604	Introduction	15	21	0.576923076923077	0.102941176470588	Finally, the Innovation Challenge seeks to predict language used forty years in the future.	1
43909	43909	W18-0604	Introduction	16	22	0.615384615384615	0.107843137254902	The data for this work comes from the National Child Development Study (Power and Elliott, 2006), a unique British study which follows a single, nationally-representative cohort of individuals over a sixty-year period starting at birth.	0
43910	43910	W18-0604	Introduction	17	23	0.653846153846154	0.112745098039216	The data available to shared task participants includes over ten thousand anonymized childhood essays, measures of psychological health taken at regular intervals, and adult writing at age 50, all collected as part of the NCDS study.	0
43911	43911	W18-0604	Introduction	18	24	0.692307692307692	0.117647058823529	Related Work.	0
43912	43912	W18-0604	Introduction	19	25	0.730769230769231	0.122549019607843	Relatively little work has been done on future mental health predictions.	0
43913	43913	W18-0604	Introduction	20	26	0.769230769230769	0.127450980392157	De Choudhury et al. (2013) examine depression in individuals by analyzing social media signals up to a year in advance of its reported onset.	0
43914	43914	W18-0604	Introduction	21	27	0.807692307692308	0.132352941176471	Similarly, De Choudhury et al. (2016) aims to identify individuals who are likely to engage in suicidal ideation in the future.	0
43915	43915	W18-0604	Introduction	22	28	0.846153846153846	0.137254901960784	Sadeque et al. (2016) predict whether posters on a mental health forum will leave the forum within a particular (one, six, or twelve month) time frame.	0
43916	43916	W18-0604	Introduction	23	29	0.884615384615385	0.142156862745098	In addition to these cases, some have used temporal information within cross-sectional analyses.	0
43917	43917	W18-0604	Introduction	24	30	0.923076923076923	0.147058823529412	Zirikly et al. (2016), for example, use timestamp data to help classify the severity levels of posts to a mental health forum.	0
43918	43918	W18-0604	Introduction	25	31	0.961538461538462	0.151960784313725	Loveys et al. (2017) explore mental health within the context of micropatterns, or sequences of posts occurring within a small time frame.	0
43919	43919	W18-0604	Introduction	26	32	1.0	0.156862745098039	The goal of this shared task is to predict mental health not only at the time of writing, but years or decades into the future.	0
44093	44093	W19-3003	abstract	1	2	0.2	0.010526315789474	The shared task for the 2019 Workshop on Computational Linguistics and Clinical Psychology (CLPsych'19) introduced an assessment of suicide risk based on social media postings, using data from Reddit to identify users at no, low, moderate, or severe risk.	1
44094	44094	W19-3003	abstract	2	3	0.4	0.015789473684211	Two variations of the task focused on users whose posts to the r/Suicide	1
44095	44095	W19-3003	abstract	3	4	0.6	0.021052631578947	Watch subreddit indicated they might be at risk; a third task looked at screening users based only on their more everyday (non-Suicide	1
44096	44096	W19-3003	abstract	4	5	0.8	0.026315789473684	Watch) posts.	0
44097	44097	W19-3003	abstract	5	6	1.0	0.031578947368421	We received submissions from 15 different teams, and the results provide progress and insight into the value of language signal in helping to predict risk level.	0
44282	44282	2021.americasnlp-1.23	title	1	1	1.0	0.003676470588235	Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas	1
44283	44283	2021.americasnlp-1.23	abstract	1	2	0.142857142857143	0.007352941176471	This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas.	1
44284	44284	2021.americasnlp-1.23	abstract	2	3	0.285714285714286	0.011029411764706	The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages.	0
44285	44285	2021.americasnlp-1.23	abstract	3	4	0.428571428571429	0.014705882352941	Overall, 8 teams participated with a total of 214 submissions.	0
44286	44286	2021.americasnlp-1.23	abstract	4	5	0.571428571428571	0.018382352941177	We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets.	0
44287	44287	2021.americasnlp-1.23	abstract	5	6	0.714285714285714	0.022058823529412	An official baseline trained on this data was also provided.	0
44288	44288	2021.americasnlp-1.23	abstract	6	7	0.857142857142857	0.025735294117647	Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline.	0
44289	44289	2021.americasnlp-1.23	abstract	7	8	1.0	0.029411764705882	The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.	0
44555	44555	2021.bionlp-1.8	abstract	1	2	0.25	0.010362694300518	The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on summarization for medical text: (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings.	1
44556	44556	2021.bionlp-1.8	abstract	2	3	0.5	0.015544041450777	Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of models developed and tested on the shared and external datasets.	0
44557	44557	2021.bionlp-1.8	abstract	3	4	0.75	0.020725388601036	In this paper, we describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures.	0
44558	44558	2021.bionlp-1.8	abstract	4	5	1.0	0.025906735751295	We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation.	0
44748	44748	2021.cmcl-1.7	abstract	1	2	0.142857142857143	0.018867924528302	Eye-tracking data from reading represent an important resource for both linguistics and natural language processing.	0
44749	44749	2021.cmcl-1.7	abstract	2	3	0.285714285714286	0.028301886792453	The ability to accurately model gaze features is crucial to advance our understanding of language processing.	0
44750	44750	2021.cmcl-1.7	abstract	3	4	0.428571428571429	0.037735849056604	This paper describes the Shared Task on Eye-Tracking Data Prediction, jointly organized with the eleventh edition of the Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2021).	0
44751	44751	2021.cmcl-1.7	abstract	4	5	0.571428571428571	0.047169811320755	The goal of the task is to predict 5 different token-level eyetracking metrics from the Zurich Cognitive Language Processing Corpus (ZuCo).	1
44752	44752	2021.cmcl-1.7	abstract	5	6	0.714285714285714	0.056603773584906	Eyetracking data were recorded during natural reading of English sentences.	0
44753	44753	2021.cmcl-1.7	abstract	6	7	0.857142857142857	0.066037735849057	In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches.	0
44754	44754	2021.cmcl-1.7	abstract	7	8	1.0	0.075471698113208	The winning system used a range of linguistic and psychometric features in a gradient boosting framework.	0
44879	44879	2021.smm4h-1.3	Task Description	1	27	0.5	0.207692307692308	Shared Task Goal	0
44880	44880	2021.smm4h-1.3	Task Description	2	28	1.0	0.215384615384615	ProfNER focuses on the automatic recognition of professions and working status mentions on Twitter posts related to the COVID-19 pandemic in Spanish.	1
45060	45060	2021.textgraphs-1.17	Task Description	1	78	0.333333333333333	0.557142857142857	Following the previous editions of the shared task, we frame explanation generation as a ranking problem.	0
45061	45061	2021.textgraphs-1.17	Task Description	2	79	0.666666666666667	0.564285714285714	Specifically, for a given science question, a model is supplied both the question and correct answer text, and must then selectively rank all the atomic scientific and world knowledge facts in the knowledge base such that those that were labelled as most relevant to building an explanation by a human annotator are ranked the highest.	1
45062	45062	2021.textgraphs-1.17	Task Description	3	80	1.0	0.571428571428571	Additional details on the ranking problem are described in the 2019 shared task summary paper (Jansen and Ustalov, 2019).	0
45124	45124	I17-4001	abstract	1	2	0.2	0.021052631578947	This paper presents the IJCNLP 2017 shared task for Chinese grammatical error diagnosis (CGED) which seeks to identify grammatical error types and their range of occurrence within sentences written by learners of Chinese as foreign language.	1
45125	45125	I17-4001	abstract	2	3	0.4	0.031578947368421	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
45126	45126	I17-4001	abstract	3	4	0.6	0.042105263157895	Of the 13 teams registered for this shared task, 5 teams developed the system and submitted a total of 13 runs.	0
45127	45127	I17-4001	abstract	4	5	0.8	0.052631578947369	We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection.	0
45128	45128	I17-4001	abstract	5	6	1.0	0.063157894736842	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
45525	45525	I17-4005	Introduction	1	7	0.095238095238095	0.056910569105691	One critical but challenging problem in natural language understanding (NLU) is to develop a question answering(QA) system which could consistently understand and correctly answer general questions about the world.	0
45526	45526	I17-4005	Introduction	2	8	0.142857142857143	0.065040650406504	"""Multi-choice Question Answering in Exams""(MCQA) is a typical question answering task that aims to test how accurately the participant QA systems could answer the questions in exams."	1
45527	45527	I17-4005	Introduction	3	9	0.19047619047619	0.073170731707317	All questions in this competition come from real examinations.	0
45528	45528	I17-4005	Introduction	4	10	0.238095238095238	0.08130081300813	We collected multiple choice questions from several curriculums, such as Biology, History, Life-Science, with a restrain that all questions are limited in the elementary and middle school level.	0
45529	45529	I17-4005	Introduction	5	11	0.285714285714286	0.089430894308943	For every question, four answer candidates are provided,  where each of them may be a word, a value, a phrase or even a sentence.	0
45530	45530	I17-4005	Introduction	6	12	0.333333333333333	0.097560975609756	The participant QA systems are required to select the best one from these four candidates.	0
45531	45531	I17-4005	Introduction	7	13	0.380952380952381	0.105691056910569	Fig 1 is an example.	0
45532	45532	I17-4005	Introduction	8	14	0.428571428571429	0.113821138211382	To answer these questions, participants could utilize any public toolkits and any resources on the Web, but manually annotation is not permitted.	0
45533	45533	I17-4005	Introduction	9	15	0.476190476190476	0.121951219512195	As for the knowledge resources, we encourage participants to utilize any resource on Internet, including softwares, toolboxes, and all kinds of corpora.	0
45534	45534	I17-4005	Introduction	10	16	0.523809523809524	0.130081300813008	Meanwhile, we also provide a dump of Wikipedia 2 and a collection of related Baidu Baike Corpus 3 under a specific license.	0
45535	45535	I17-4005	Introduction	11	17	0.571428571428571	0.138211382113821	These corpora and released questions are all provided in the XML format, which will be explained in section 2.2.	0
45536	45536	I17-4005	Introduction	12	18	0.619047619047619	0.146341463414634	Main characteristics of our task are as follow:	0
45537	45537	I17-4005	Introduction	13	19	0.666666666666667	0.154471544715447		0
45538	45538	I17-4005	Introduction	14	20	0.714285714285714	0.16260162601626	All the questions are from real word examinations.	0
45539	45539	I17-4005	Introduction	15	21	0.761904761904762	0.170731707317073		0
45540	45540	I17-4005	Introduction	16	22	0.80952380952381	0.178861788617886	Most of questions require considerable inference ability.	0
45541	45541	I17-4005	Introduction	17	23	0.857142857142857	0.186991869918699		0
45542	45542	I17-4005	Introduction	18	24	0.904761904761905	0.195121951219512	Some questions require a deep understanding of context.	0
45543	45543	I17-4005	Introduction	19	25	0.952380952380952	0.203252032520325	 Questions from different categories have different characteristics, which makes it harder for a model to have a good performance on all kinds of questions.	0
45544	45544	I17-4005	Introduction	20	26	1.0	0.211382113821138		0
45545	45545	I17-4005	Introduction	21	27	0.5	0.219512195121951	It concentrates only on the textual content, as questions with figures and tables are all filtered out.	0
45674	45674	S12-1035	Task description	1	33	0.222222222222222	0.14410480349345	The *SEM 2012 Shared Task 2 was dedicated to resolving the scope and focus of negation (Task 1 and 2 respectively).	1
45675	45675	S12-1035	Task description	2	34	0.333333333333333	0.148471615720524	Participants were allowed to engage in any combination of tasks and submit at most two runs per task.	0
45676	45676	S12-1035	Task description	3	35	0.444444444444444	0.152838427947598	A pilot task combining scope and focus detection was initially planned, but was cancelled due to lack of participation.	0
45677	45677	S12-1035	Task description	4	36	0.555555555555556	0.157205240174672	We received a total of 14 runs, 12 for scope detection (7 closed, 5 open) and 2 for focus detection (0 closed, 2 open).	0
45678	45678	S12-1035	Task description	5	37	0.666666666666667	0.161572052401747	Submissions fall into two tracks:	0
45679	45679	S12-1035	Task description	6	38	0.777777777777778	0.165938864628821	Regardless of the track, teams were allowed to submit their final results on the test set using a system trained on both the training and development sets.	0
45680	45680	S12-1035	Task description	7	39	0.888888888888889	0.170305676855895	The data format is the same as in several previous CoNLL Shared Tasks (Surdeanu et al., 2008).	0
45681	45681	S12-1035	Task description	8	40	1.0	0.174672489082969	Sentences are separated by a blank line.	0
45682	45682	S12-1035	Task description	9	41	0.071428571428572	0.179039301310044	Each sentence consists of a sequence of tokens, and a new line is used for each token.	0
45872	45872	S13-1004	abstract	1	2	0.285714285714286	0.008196721311475	In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar.	1
45873	45873	S13-1004	abstract	2	3	0.428571428571429	0.012295081967213	This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).	0
45874	45874	S13-1004	abstract	3	4	0.571428571428571	0.016393442622951	CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets.	0
45875	45875	S13-1004	abstract	4	5	0.714285714285714	0.020491803278689	TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description.	0
45876	45876	S13-1004	abstract	5	6	0.857142857142857	0.024590163934426	Several types of similarity have been defined, including similar author, similar time period or similar location.	0
45877	45877	S13-1004	abstract	6	7	1.0	0.028688524590164	The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%.	0
45878	45878	S13-1004	abstract	7	8	0.019230769230769	0.032786885245902	The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.	0
46217	46217	W11-1803	abstract	1	2	0.666666666666667	0.013422818791946	This paper presents the preparation, resources, results and analysis of the Epigenetics and Post-translational Modifications (EPI) task, a main task of the BioNLP Shared Task 2011.	0
46218	46218	W11-1803	abstract	2	3	1.0	0.02013422818792	The task concerns the extraction of detailed representations of 14 protein and DNA modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances.	1
46219	46219	W11-1803	abstract	3	4	0.052631578947369	0.026845637583893	Seven teams submitted final results to the EPI task in the shared task, with the highest-performing system achieving 53% F-score in the full task and 69% F-score in the extraction of a simplified set of core event arguments.	0
46370	46370	W11-1804	Introduction	1	6	0.166666666666667	0.041095890410959	The Infectious Diseases (ID) task of the BioNLP Shared Task 2011	0
46371	46371	W11-1804	Introduction	2	7	0.25	0.047945205479452	(Kim et al., 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases.	1
46372	46372	W11-1804	Introduction	3	8	0.333333333333333	0.054794520547945	The primary target of the task is event extraction , broadly following the task setup of the BioNLP'09 Shared Task (BioNLP ST'09) .	0
46373	46373	W11-1804	Introduction	4	9	0.416666666666667	0.061643835616438	The task concentrates on the specific domain of two-component systems (TCSs, or two-component regulatory systems), a mechanism widely used by bacteria to sense and respond to the environment (Thomason and Kay, 2000).	0
46374	46374	W11-1804	Introduction	5	10	0.5	0.068493150684932	Typical TCSs consist of two proteins, a membrane-associated sensor kinase and a cytoplasmic response regulator.	0
46375	46375	W11-1804	Introduction	6	11	0.583333333333333	0.075342465753425	The sensor kinase monitors changes in the environment while the response regulator mediates an adaptive response, usually through differential expression of target genes (Mascher et al., 2006).	0
46376	46376	W11-1804	Introduction	7	12	0.666666666666667	0.082191780821918	TCSs have many functions, but those of particular interest for infectious disease researchers include virulence, response to antibiotics, quorum sensing, and bacterial cell attachment (Krell et al., 2010).	0
46377	46377	W11-1804	Introduction	8	13	0.75	0.089041095890411	Not all TCS functions are well known: in some cases, TCSs are involved in metabolic processes that are difficult to precisely characterize (Wang et al., 2010).	0
46378	46378	W11-1804	Introduction	9	14	0.833333333333333	0.095890410958904	TCSs are of interest also as drugs designed to disrupt TCSs may reduce the virulence of bacteria without killing it, thus avoiding the potential selective pressure of antibiotics lethal to some pathogenic bacteria (Gotoh et al., 2010).	0
46379	46379	W11-1804	Introduction	10	15	0.916666666666667	0.102739726027397	Information extraction techniques may support better understanding of these fundamental systems by identifying and structuring the molecular processes underlying two component signaling.	0
46380	46380	W11-1804	Introduction	11	16	1.0	0.10958904109589	The ID task seeks to address these opportunities by adapting the BioNLP ST'09 event extraction model to domain scientific publications.	0
46381	46381	W11-1804	Introduction	12	17	1.0	0.116438356164384	This model was originally introduced to represent biomolecular events relating to transcription factors in human blood cells, and its adaptation to a domain that centrally concerns both bacteria and their hosts involves a variety of novel aspects, such as events concerning whole organisms, the chemical environment of bacteria, prokaryote-specific concepts (e.g. regulons as elements of gene expression), as well as the effects of biomolecules on larger-scale processes involving hosts such as virulence.	0
