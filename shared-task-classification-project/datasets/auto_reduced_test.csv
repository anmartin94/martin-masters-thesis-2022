	Unnamed: 0	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
0	0	0	S01-basque	title	1	1	4.0	1.0	The Basque task: did systems perform in the upperbound?	0
1	1	1	S01-basque	abstract	1	2	1.0	1.0	In this paper we describe the Senseval 2 Basque lexical-sample task.	0
2	2	2	S01-basque	abstract	2	3	2.0	1.0	The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary.	0
3	3	3	S01-basque	abstract	3	4	2.0	1.0	Most examples were taken from the Egunkaria newspaper.	0
4	4	4	S01-basque	abstract	4	5	3.0	1.0	The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration.	0
5	5	5	S01-basque	abstract	5	6	3.0	1.0	The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage.	0
6	6	6	S01-basque	abstract	6	7	4.0	1.0	The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems.	0
7	7	7	S01-basque	abstract	7	8	4.0	1.0	In particular, we argue that inter-tagger agreement is not a real upperbound for the B,asque WSD task.	0
8	8	8	S01-basque	Introduction	1	9	1.0	1.0	This paper reviews the design of the lexicalsample task for Basque.	0
9	9	9	S01-basque	Introduction	2	10	2.0	1.0	The following steps were taken in order to build the hand-tagged corpus: 1. set the exercise a. choose sense inventory b. choose target corpus c. choose target words d. select examples from the corpus 2. hand-tagging a. define procedure b. tag c. analysis of inter-tagger agreement d. arbitration	0
10	10	10	S01-basque	Introduction	3	11	3.0	1.0	The following section presents the setting of the exercise.	0
11	11	11	S01-basque	Introduction	4	12	4.0	1.0	Section 3 reviews the hand-tagging, and section 4 the results of the participant systems.	0
12	12	12	S01-basque	Introduction	5	13	4.0	1.0	Section 5 discusses the design of the task, as well 9 as the results, and section 6 presents some future work.	0
13	2093	2093	S07-2	title	1	1	4.0	1.0	Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems	1
14	2094	2094	S07-2	abstract	1	2	1.0	1.0	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems.	0
15	2095	2095	S07-2	abstract	2	3	2.0	1.0	In total there were 6 participating systems.	0
16	2096	2096	S07-2	abstract	3	4	3.0	1.0	We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).	0
17	2097	2097	S07-2	abstract	4	5	4.0	1.0	We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
18	2098	2098	S07-2	Introduction	1	6	1.0	1.0	Word Sense Disambiguation (WSD) is a key enabling-technology.	0
19	2099	2099	S07-2	Introduction	2	7	1.0	1.0	Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.	0
20	2100	2100	S07-2	Introduction	3	8	1.0	1.0	Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).	0
21	2101	2101	S07-2	Introduction	4	9	1.0	1.0	In theory, larger amounts of training data (SemCor has approx.	0
22	2102	2102	S07-2	Introduction	5	10	2.0	1.0	500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.	0
23	2103	2103	S07-2	Introduction	6	11	2.0	1.0	Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart√≠nez and Agirre, 2000;	0
24	2104	2104	S07-2	Introduction	7	12	2.0	1.0	Koeling et al., 2005).	0
25	2105	2105	S07-2	Introduction	8	13	2.0	1.0	"Supervised WSD is based on the ""fixed-list of senses"" paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon."	0
26	2106	2106	S07-2	Introduction	9	14	2.0	1.0	Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).	0
27	2107	2107	S07-2	Introduction	10	15	3.0	1.0	Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.	1
28	2108	2108	S07-2	Introduction	11	16	3.0	1.0	Typical WSID systems involve clustering techniques, which group together similar examples.	0
29	2109	2109	S07-2	Introduction	12	17	3.0	1.0	Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.	0
30	2110	2110	S07-2	Introduction	13	18	3.0	1.0	One of the problems of unsupervised systems is that of managing to do a fair evaluation.	0
31	2111	2111	S07-2	Introduction	14	19	3.0	1.0	Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.	0
32	2112	2112	S07-2	Introduction	15	20	4.0	1.0	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.	0
33	2113	2113	S07-2	Introduction	16	21	4.0	1.0	The paper is organized as follows.	0
34	2114	2114	S07-2	Introduction	17	22	4.0	1.0	Section 2 presents the evaluation framework used in this task.	0
35	2115	2115	S07-2	Introduction	18	23	4.0	1.0	Section 3 presents the systems that participated in the task, and the official results.	0
36	2116	2116	S07-2	Introduction	19	24	4.0	1.0	Finally, Section 5 draws the conclusions.	0
37	2247	2247	S07-2	Conclusions	1	155	1.0	4.0	We have presented the design and results of the SemEval-2007 task 02 on evaluating word sense induction and discrimination systems.	0
38	2248	2248	S07-2	Conclusions	2	156	1.0	4.0	6 systems participated, but one of them was not a sense induction system.	0
39	2249	2249	S07-2	Conclusions	3	157	2.0	4.0	We reused the data from the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the training part of the dataset for mapping).	0
40	2250	2250	S07-2	Conclusions	4	158	2.0	4.0	We also provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
41	2251	2251	S07-2	Conclusions	5	159	3.0	4.0	Evaluating clustering solutions is not straightforward.	0
42	2252	2252	S07-2	Conclusions	6	160	3.0	4.0	The unsupervised evaluation seems to be sensitive to the number of senses in the gold standard, and the coarse grained sense inventory used in the gold standard had a great impact in the results.	0
43	2253	2253	S07-2	Conclusions	7	161	4.0	4.0	The supervised evaluation introduces a mapping step which interacts with the clustering solution.	0
44	2254	2254	S07-2	Conclusions	8	162	4.0	4.0	In fact, the ranking of the participating systems 3 All systems in the case of a random train/test split varies according to the evaluation method used.	0
45	2255	2255	S07-2	Conclusions	9	163	4.0	4.0	We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systems needs further developments, perhaps linked to a certain application or purpose.	0
46	2815	2815	S07-9	title	1	1	4.0	1.0	SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish	0
47	2816	2816	S07-9	abstract	1	2	2.0	1.0	In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish).	0
48	2817	2817	S07-9	abstract	2	3	3.0	1.0	In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish.	1
49	2818	2818	S07-9	abstract	3	4	4.0	1.0	Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.	0
50	2819	2819	S07-9	Introduction	1	5	1.0	1.0	The Multilevel Semantic Annotation of Catalan and Spanish task is split into the following three subtasks:	0
51	2820	2820	S07-9	Introduction	2	6	1.0	1.0	Noun Sense Disambiguation (NSD):	0
52	2821	2821	S07-9	Introduction	3	7	1.0	1.0	"Disambiguation of all frequent nouns (""all words"" style)."	0
53	2822	2822	S07-9	Introduction	4	8	2.0	1.0	Named Entity Recognition (NER):	0
54	2823	2823	S07-9	Introduction	5	9	2.0	1.0	The annotation of (possibly embedding) named entities with basic entity types.	0
55	2824	2824	S07-9	Introduction	6	10	2.0	1.0	Semantic Role Labeling (SRL):	0
56	2825	2825	S07-9	Introduction	7	11	2.0	1.0	Including also two subtasks, i.e., the annotation of verbal predicates with semantic roles (SR), and verb tagging with semantic-class labels (SC).	0
57	2826	2826	S07-9	Introduction	8	12	3.0	1.0	All semantic annotation tasks are performed on exactly the same corpora for each language.	0
58	2827	2827	S07-9	Introduction	9	13	3.0	1.0	We presented all the annotation levels together as a complex global task, since we were interested in approaches which address these problems jointly, possibly taking into account cross-dependencies among them.	0
59	2828	2828	S07-9	Introduction	10	14	3.0	1.0	However, we were also accepting systems approaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of the languages.	0
60	2829	2829	S07-9	Introduction	11	15	4.0	1.0	In Section 2 we describe the methodology followed to develop the linguistic corpora for the task.	0
61	2830	2830	S07-9	Introduction	12	16	4.0	1.0	Sections 3 and 4 summarize the task setting and the participant systems, respectively.	0
62	2831	2831	S07-9	Introduction	13	17	4.0	1.0	Finally, Section 5 presents a comparative analysis of the results.	0
63	2832	2832	S07-9	Introduction	14	18	4.0	1.0	For any additional information on corpora, resources, formats, tagsets, annotation manuals, etc. we refer the reader to the official website of the task 1 .	0
64	5272	5272	S10-12	title	1	1	4.0	1.0	SemEval-2010 Task 12: Parser Evaluation using Textual Entailments	0
65	5273	5273	S10-12	abstract	1	2	2.0	1.0	Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.	0
66	5274	5274	S10-12	abstract	2	3	3.0	1.0	The task involves recognizing textual entailments based on syntactic information alone.	1
67	5275	5275	S10-12	abstract	3	4	4.0	1.0	PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.	0
68	5276	5276	S10-12	Introduction	1	5	1.0	1.0	Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.	1
69	5277	5277	S10-12	Introduction	2	6	1.0	1.0	"Given two text fragments called ""text"" and ""hypothesis"", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text."	0
70	5278	5278	S10-12	Introduction	3	7	1.0	1.0	In contrast with general RTE tasks (Dagan et al., 2009) the PETE task focuses on syntactic entailments:	0
71	5279	5279	S10-12	Introduction	4	8	1.0	1.0	Text:	0
72	5280	5280	S10-12	Introduction	5	9	1.0	1.0	The man with the hat was tired.	0
73	5281	5281	S10-12	Introduction	6	10	1.0	1.0	Hypothesis-1: The man was tired.	0
74	5282	5282	S10-12	Introduction	7	11	1.0	1.0	(yes) Hypothesis-2: The hat was tired.	0
75	5283	5283	S10-12	Introduction	8	12	1.0	1.0	(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).	0
76	5284	5284	S10-12	Introduction	9	13	1.0	1.0	We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
77	5285	5285	S10-12	Introduction	10	14	1.0	1.0	The PARSEVAL measures introduced nearly two decades ago (Black et al., 1991) still dominate the field of parser evaluation.	0
78	5286	5286	S10-12	Introduction	11	15	1.0	1.0	"These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or ""treebank""."	0
79	5287	5287	S10-12	Introduction	12	16	1.0	1.0	Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.	0
80	5288	5288	S10-12	Introduction	13	17	1.0	1.0	Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.	0
81	5289	5289	S10-12	Introduction	14	18	1.0	1.0	Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.	0
82	5290	5290	S10-12	Introduction	15	19	1.0	1.0	The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.	0
83	5291	5291	S10-12	Introduction	16	20	2.0	1.0	In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank (Marcus et al., 1994), 5646 (15%) have multiple conflicting annotations.	0
84	5292	5292	S10-12	Introduction	17	21	2.0	1.0	If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90% (Charniak and Johnson, 2005).	0
85	5293	5293	S10-12	Introduction	18	22	2.0	1.0	Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.	0
86	5294	5294	S10-12	Introduction	19	23	2.0	1.0	Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.	0
87	5295	5295	S10-12	Introduction	20	24	2.0	1.0	Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.	0
88	5296	5296	S10-12	Introduction	21	25	2.0	1.0	Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (Bonnema et al., 1997).	0
89	5297	5297	S10-12	Introduction	22	26	2.0	1.0	In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.	0
90	5298	5298	S10-12	Introduction	23	27	2.0	1.0	Framework independence: Entailment recognition is a formalism independent task.	0
91	5299	5299	S10-12	Introduction	24	28	2.0	1.0	A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (Nivre et al., 2007a;	0
92	5300	5300	S10-12	Introduction	25	29	2.0	1.0	Hockenmaier and Steedman, 2007).	0
93	5301	5301	S10-12	Introduction	26	30	2.0	1.0	The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.	0
94	5302	5302	S10-12	Introduction	27	31	2.0	1.0	In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.	0
95	5303	5303	S10-12	Introduction	28	32	2.0	1.0	Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.	0
96	5304	5304	S10-12	Introduction	29	33	2.0	1.0	PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.	0
97	5305	5305	S10-12	Introduction	30	34	2.0	1.0	These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals).	0
98	5306	5306	S10-12	Introduction	31	35	3.0	1.0	Each use a set of binary relations between words in a sentence as the primary unit of representation.	0
99	5307	5307	S10-12	Introduction	32	36	3.0	1.0	They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.	0
100	5308	5308	S10-12	Introduction	33	37	3.0	2.0	Here is an example sentence and its SD representation (De Marneffe and Manning, 2008):	0
101	5309	5309	S10-12	Introduction	34	38	3.0	2.0	Bell, based in Los Angeles, makes and distributes electronic, computer and building products.	0
102	5310	5310	S10-12	Introduction	35	39	3.0	2.0	nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod (products-16, electronic-11) conj-and(electronic-11, computer-13) amod (products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.	0
103	5311	5311	S10-12	Introduction	36	40	3.0	2.0	Bell makes something.	0
104	5312	5312	S10-12	Introduction	37	41	3.0	2.0	Bell distributes something.	0
105	5313	5313	S10-12	Introduction	38	42	3.0	2.0	Someone is based in Los Angeles.	0
106	5314	5314	S10-12	Introduction	39	43	3.0	2.0	Someone makes products.	0
107	5315	5315	S10-12	Introduction	40	44	3.0	2.0	PETE has some advantages over representations based on grammatical relations.	0
108	5316	5316	S10-12	Introduction	41	45	3.0	2.0	For example SD defines 55 relations organized in a hierarchy, and it may be non-trivial for a non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).	0
109	5317	5317	S10-12	Introduction	42	46	3.0	2.0	In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.	0
110	5318	5318	S10-12	Introduction	43	47	3.0	2.0	In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.	0
111	5319	5319	S10-12	Introduction	44	48	3.0	2.0	However there are also significant challenges associated with an evaluation scheme like PETE.	0
112	5320	5320	S10-12	Introduction	45	49	3.0	2.0	It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.	0
113	5321	5321	S10-12	Introduction	46	50	4.0	2.0	Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.	0
114	5322	5322	S10-12	Introduction	47	51	4.0	2.0	Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.	0
115	5323	5323	S10-12	Introduction	48	52	4.0	2.0	"Introducing dummy words like ""someone"" or ""something"" alleviates part of the problem but does not help in the case of clausal complements."	0
116	5324	5324	S10-12	Introduction	49	53	4.0	2.0	In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.	0
117	5325	5325	S10-12	Introduction	50	54	4.0	2.0	PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.	0
118	5326	5326	S10-12	Introduction	51	55	4.0	2.0	This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.	0
119	5327	5327	S10-12	Introduction	52	56	4.0	2.0	However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.	0
120	5328	5328	S10-12	Introduction	53	57	4.0	2.0	The balance of what is being evaluated is also important.	0
121	5329	5329	S10-12	Introduction	54	58	4.0	2.0	A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.	0
122	5330	5330	S10-12	Introduction	55	59	4.0	2.0	In this evaluation, we focused on sentences and relations where state of the art parsers disagree.	0
123	5331	5331	S10-12	Introduction	56	60	4.0	2.0	We hope this methodology will uncover weaknesses that the next generation systems can focus on.	0
124	5332	5332	S10-12	Introduction	57	61	4.0	2.0	The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.	0
125	5333	5333	S10-12	Introduction	58	62	4.0	2.0	Section 2 explains the method followed to create the PETE dataset.	0
126	5334	5334	S10-12	Introduction	59	63	4.0	2.0	Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.	0
127	5335	5335	S10-12	Introduction	60	64	4.0	2.0	Section 4 presents the participating systems, their methods and results.	0
128	5336	5336	S10-12	Introduction	61	65	4.0	2.0	Section 5 summarizes our contribution.	0
129	5809	5809	S10-18	title	1	1	4.0	1.0	SemEval-2010 Task 18: Disambiguating Sentiment Ambiguous Adjectives	0
130	5810	5810	S10-18	abstract	1	2	1.0	1.0	Sentiment ambiguous adjectives cause major difficulties for existing algorithms of sentiment analysis.	0
131	5811	5811	S10-18	abstract	2	3	2.0	1.0	We present an evaluation task designed to provide a framework for comparing different approaches in this problem.	0
132	5812	5812	S10-18	abstract	3	4	3.0	1.0	We define the task, describe the data creation, list the participating systems and discuss their results.	0
133	5813	5813	S10-18	abstract	4	5	4.0	1.0	There are 8 teams and 16 systems.	0
134	5814	5814	S10-18	Introduction	1	6	1.0	1.0	In recent years, sentiment analysis has attracted considerable attention (Pang and Lee, 2008).	0
135	5815	5815	S10-18	Introduction	2	7	1.0	1.0	It is the task of mining positive and negative opinions from natural language, which can be applied to many natural language processing tasks, such as document summarization and question answering.	0
136	5816	5816	S10-18	Introduction	3	8	1.0	1.0	Previous work on this problem falls into three groups: opinion mining of documents, sentiment classification of sentences and polarity prediction of words.	0
137	5817	5817	S10-18	Introduction	4	9	1.0	1.0	Sentiment analysis both at document and sentence level rely heavily on word level.	0
138	5818	5818	S10-18	Introduction	5	10	1.0	1.0	The most frequently explored task at word level is to determine the semantic orientation (SO) of words, in which most work centers on assigning a prior polarity to words or word senses in the lexicon out of context.	0
139	5819	5819	S10-18	Introduction	6	11	1.0	1.0	However, for some words, the polarity varies strongly with context, making it hard to attach each to a specific sentiment category in the lexicon.	0
140	5820	5820	S10-18	Introduction	7	12	2.0	1.0	"For example, consider "" low cost"" versus "" low salary"" ."	0
141	5821	5821	S10-18	Introduction	8	13	2.0	1.0	"The word "" low"" has a positive orientation in the first case but a negative orientation in the second case."	0
142	5822	5822	S10-18	Introduction	9	14	2.0	1.0	Turney and Littman (2003) claimed that sentiment ambiguous words could not be avoided easily in a real-world application in the future research.	0
143	5823	5823	S10-18	Introduction	10	15	2.0	1.0	But unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (Hatzivassiloglou and McKeown, 1997;	0
144	5824	5824	S10-18	Introduction	11	16	2.0	1.0	Turney and Littman, 2003;Kim and Hovy, 2004).	0
145	5825	5825	S10-18	Introduction	12	17	2.0	1.0	The exception work is Ding et al. (2008).	0
146	5826	5826	S10-18	Introduction	13	18	3.0	1.0	They call these words as context dependant opinions and propose a holistic lexicon-based approach to solve this problem.	0
147	5827	5827	S10-18	Introduction	14	19	3.0	1.0	The language they deal with is English.	0
148	5828	5828	S10-18	Introduction	15	20	3.0	1.0	The disambiguation of sentiment ambiguous words can also be considered as a problem of phrase-level sentiment analysis.	0
149	5829	5829	S10-18	Introduction	16	21	3.0	1.0	Wilson et al. (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features.	0
150	5830	5830	S10-18	Introduction	17	22	3.0	1.0	Takamura et al. (2006	0
151	5831	5831	S10-18	Introduction	18	23	3.0	1.0	"Takamura et al. ( , 2007 propose latent variable model and lexical network to determine SO of phrases, focusing on "" noun+adjective"" pairs."	0
152	5832	5832	S10-18	Introduction	19	24	4.0	1.0	Their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.	0
153	5833	5833	S10-18	Introduction	20	25	4.0	1.0	The task 18 at SemEval 2010 provides a benchmark data set to encourage studies on this problem.	0
154	5834	5834	S10-18	Introduction	21	26	4.0	1.0	This paper is organized as follows.	0
155	5835	5835	S10-18	Introduction	22	27	4.0	1.0	Section 2 defines the task.	0
156	5836	5836	S10-18	Introduction	23	28	4.0	1.0	Section 3 describes the data annotation.	0
157	5837	5837	S10-18	Introduction	24	29	4.0	1.0	Section 4 gives a brief summary of 16 participating systems.	0
158	5838	5838	S10-18	Introduction	25	30	4.0	1.0	Finally Section 5 draws conclusions.	0
159	5965	5965	S10-18	Conclusion	1	157	2.0	4.0	This paper describes task 18 at SemEval-2010, disambiguating sentiment ambiguous adjectives.	0
160	5966	5966	S10-18	Conclusion	2	158	3.0	4.0	The experimental results of the 16 participating systems are promising, and the used approaches are quite novel.	0
161	5967	5967	S10-18	Conclusion	3	159	4.0	4.0	We encourage further research into this issue, and integration of the disambiguation of sentiment ambiguous adjectives into applications of sentiment analysis.	0
162	6979	6979	S12-6	title	1	1	4.0	1.0	SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity	0
163	6980	6980	S12-6	abstract	1	2	1.0	1.0	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts.	0
164	6981	6981	S12-6	abstract	2	3	1.0	1.0	This paper presents the results of the STS pilot task in Semeval.	0
165	6982	6982	S12-6	abstract	3	4	2.0	1.0	The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources.	0
166	6983	6983	S12-6	abstract	4	5	2.0	1.0	The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise.	0
167	6984	6984	S12-6	abstract	5	6	3.0	1.0	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%.	0
168	6985	6985	S12-6	abstract	6	7	3.0	1.0	35 teams participated in the task, submitting 88 runs.	0
169	6986	6986	S12-6	abstract	7	8	4.0	1.0	The best results scored a Pearson correlation &gt;80%, well above a simple lexical baseline that only scored a 31% correlation.	0
170	6987	6987	S12-6	abstract	8	9	4.0	1.0	This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.	0
171	6988	6988	S12-6	Introduction	1	10	1.0	1.0	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two sentences.	0
172	6989	6989	S12-6	Introduction	2	11	1.0	1.0	STS is related to both Textual Entailment (TE) and Paraphrase (PARA).	0
173	6990	6990	S12-6	Introduction	3	12	1.0	1.0	STS is more directly applicable in a number of NLP tasks than TE and PARA such as Machine Translation and evaluation, Summarization, Machine Reading, Deep Question Answering, etc. STS differs from TE in as much as it assumes symmetric graded equivalence between the pair of textual snippets.	0
174	6991	6991	S12-6	Introduction	4	13	1.0	1.0	In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.	0
175	6992	6992	S12-6	Introduction	5	14	2.0	1.0	Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).	0
176	6993	6993	S12-6	Introduction	6	15	2.0	1.0	STS provides a unified framework that allows for an extrinsic evaluation of multiple semantic components that otherwise have tended to be evaluated independently and without broad characterization of their impact on NLP applications.	0
177	6994	6994	S12-6	Introduction	7	16	2.0	1.0	Such components include word sense disambiguation and induction, lexical substitution, semantic role labeling, multiword expression detection and handling, anaphora and coreference resolution, time and date resolution, named-entity handling, underspecification, hedging, semantic scoping and discourse analysis.	0
178	6995	6995	S12-6	Introduction	8	17	2.0	1.0	Though not in the scope of the current pilot task, we plan to explore building an open source toolkit for integrating and applying diverse linguistic analysis modules to the STS task.	0
179	6996	6996	S12-6	Introduction	9	18	2.0	1.0	While the characterization of STS is still preliminary, we observed that there was no comparable existing dataset extensively annotated for pairwise semantic sentence similarity.	0
180	6997	6997	S12-6	Introduction	10	19	3.0	1.0	We approached the construction of the first STS dataset with the following goals: (1)	0
181	6998	6998	S12-6	Introduction	11	20	3.0	1.0	To set a definition of STS as a graded notion which can be easily communicated to non-expert annotators beyond the likert-scale; (2)	0
182	6999	6999	S12-6	Introduction	12	21	3.0	1.0	To gather a substantial amount of sentence pairs from diverse datasets, and to annotate them with high quality; (3)	0
183	7000	7000	S12-6	Introduction	13	22	3.0	1.0	To explore evaluation measures for STS; (4)	0
184	7001	7001	S12-6	Introduction	14	23	4.0	1.0	To explore the relation of STS to PARA and Machine Translation Evaluation exercises.	0
185	7002	7002	S12-6	Introduction	15	24	4.0	1.0	In the next section we present the various sources of the STS data and the annotation procedure used.	0
186	7003	7003	S12-6	Introduction	16	25	4.0	1.0	Section 4 investigates the evaluation of STS systems.	0
187	7004	7004	S12-6	Introduction	17	26	4.0	1.0	Section 5 summarizes the resources and tools used by participant systems.	0
188	7005	7005	S12-6	Introduction	18	27	4.0	1.0	Finally, Section 6 draws the conclusions.	0
189	8118	8118	S13-5	title	1	1	4.0	1.0	SemEval-2013 Task 5: Evaluating Phrasal Semantics	0
190	8119	8119	S13-5	abstract	1	2	1.0	1.0	"This paper describes the SemEval-2013 Task 5: ""Evaluating Phrasal Semantics""."	0
191	8120	8120	S13-5	abstract	2	3	2.0	1.0	Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length.	0
192	8121	8121	S13-5	abstract	3	4	3.0	1.0	The second one addresses deciding the compositionality of phrases in a given context.	0
193	8122	8122	S13-5	abstract	4	5	4.0	1.0	The paper discusses the importance and background of these subtasks and their structure.	0
194	8123	8123	S13-5	abstract	5	6	4.0	1.0	In succession, it introduces the systems that participated and discusses evaluation results.	0
195	8124	8124	S13-5	Introduction	1	7	1.0	1.0	Numerous past tasks have focused on leveraging the meaning of word types or words in context.	0
196	8125	8125	S13-5	Introduction	2	8	1.0	1.0	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.	0
197	8126	8126	S13-5	Introduction	3	9	1.0	1.0	As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.	0
198	8127	8127	S13-5	Introduction	4	10	1.0	1.0	In this paper, we present two subtasks designed to evaluate such phrasal models: a. Semantic similarity of words and compositional phrases b.	1
199	8128	8128	S13-5	Introduction	5	11	1.0	1.0	Evaluating the compositionality of phrases in context	1
200	8129	8129	S13-5	Introduction	6	12	2.0	1.0	"For example, the first subtask addresses computing how similar the word ""valuation"" is to the compositional sequence ""price assessment"", while the second subtask addresses deciding whether the phrase ""piece of cake"" is used literally or figuratively in the sentence ""Labour was a piece of cake!""."	0
201	8130	8130	S13-5	Introduction	7	13	2.0	1.0	The aim of these subtasks is two-fold.	0
202	8131	8131	S13-5	Introduction	8	14	2.0	1.0	Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.	0
203	8132	8132	S13-5	Introduction	9	15	2.0	1.0	It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.	0
204	8133	8133	S13-5	Introduction	10	16	2.0	1.0	Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.	0
205	8134	8134	S13-5	Introduction	11	17	2.0	1.0	Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.	0
206	8135	8135	S13-5	Introduction	12	18	3.0	1.0	We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.	0
207	8136	8136	S13-5	Introduction	13	19	3.0	1.0	Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.	0
208	8137	8137	S13-5	Introduction	14	20	3.0	1.0	In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.	0
209	8138	8138	S13-5	Introduction	15	21	3.0	1.0	The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.	0
210	8139	8139	S13-5	Introduction	16	22	3.0	1.0	Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.	0
211	8140	8140	S13-5	Introduction	17	23	3.0	1.0	In subsection 3.1 the subtask is described in detail together with some information about its background.	0
212	8141	8141	S13-5	Introduction	18	24	4.0	1.0	Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.	0
213	8142	8142	S13-5	Introduction	19	25	4.0	1.0	Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.	0
214	8143	8143	S13-5	Introduction	20	26	4.0	1.0	Subsection 4.1 explains the data creation process for this subtask.	0
215	8144	8144	S13-5	Introduction	21	27	4.0	1.0	In subsection 4.2 the evaluation statistics of participating systems are presented.	0
216	8145	8145	S13-5	Introduction	22	28	4.0	1.0	Section 5 is a discussion about the conclusions of the entire task.	0
217	8146	8146	S13-5	Introduction	23	29	4.0	1.0	Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.	0
218	9049	9049	S13-11	title	1	1	4.0	1.0	SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application	0
219	9050	9050	S13-11	abstract	1	2	2.0	1.0	In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification.	0
220	9051	9051	S13-11	abstract	2	3	3.0	1.0	Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.	1
221	9052	9052	S13-11	abstract	3	4	4.0	1.0	The task enables the end-to-end evaluation and comparison of systems.	0
222	9053	9053	S13-11	Introduction	1	5	1.0	1.0	Word ambiguity is a pervasive issue in Natural Language Processing.	0
223	9054	9054	S13-11	Introduction	2	6	1.0	1.0	Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009;	0
224	9055	9055	S13-11	Introduction	3	7	1.0	1.0	Navigli, 2012) for a survey).	0
225	9056	9056	S13-11	Introduction	4	8	1.0	1.0	Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications.	0
226	9057	9057	S13-11	Introduction	5	9	1.0	1.0	In fact, the performance of WSD systems depends heavily on which sense inventory is chosen.	0
227	9058	9058	S13-11	Introduction	6	10	2.0	1.0	For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002;	0
228	9059	9059	S13-11	Introduction	7	11	2.0	1.0	Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008).	0
229	9060	9060	S13-11	Introduction	8	12	2.0	1.0	On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses.	0
230	9061	9061	S13-11	Introduction	9	13	2.0	1.0	In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output.	0
231	9062	9062	S13-11	Introduction	10	14	2.0	1.0	Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.	0
232	9063	9063	S13-11	Introduction	11	15	3.0	1.0	Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively.	0
233	9064	9064	S13-11	Introduction	12	16	3.0	1.0	A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds.	0
234	9065	9065	S13-11	Introduction	13	17	3.0	1.0	Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010;	0
235	9066	9066	S13-11	Introduction	14	18	3.0	1.0	Di Marco and Navigli, 2013).	0
236	9067	9067	S13-11	Introduction	15	19	3.0	1.0	In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.	0
237	9068	9068	S13-11	Introduction	16	20	4.0	1.0	The Semeval-2013 task described in this paper 1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems.	0
238	9069	9069	S13-11	Introduction	17	21	4.0	1.0	The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre and Soroa, 2007;Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms.	0
239	9070	9070	S13-11	Introduction	18	22	4.0	1.0	Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning).	0
240	9071	9071	S13-11	Introduction	19	23	4.0	1.0	For instance, consider the target query apple and the following 3 search result snippets:	0
241	9072	9072	S13-11	Introduction	20	24	4.0	1.0	1. Apple Inc., formerly Apple Computer, Inc., is...	0
242	9369	9369	S13-13	title	1	1	4.0	1.0	SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses	0
243	9370	9370	S13-13	abstract	1	2	1.0	1.0	Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense.	0
244	9371	9371	S13-13	abstract	2	3	2.0	1.0	However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage.	0
245	9372	9372	S13-13	abstract	3	4	3.0	1.0	We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability.	0
246	9373	9373	S13-13	abstract	4	5	4.0	1.0	Four teams submitted nine systems, which were evaluated in two settings.	0
247	9374	9374	S13-13	Introduction	1	6	1.0	1.0	Word Sense Disambiguation (WSD) attempts to identify which of a word's meanings applies in a given context.	0
248	9375	9375	S13-13	Introduction	2	7	1.0	1.0	A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009).	0
249	9376	9376	S13-13	Introduction	3	8	1.0	1.0	Typically, each usage of a word is treated as expressing only a single sense.	0
250	9377	9377	S13-13	Introduction	4	9	1.0	1.0	However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations.	0
251	9378	9378	S13-13	Introduction	5	10	2.0	1.0	Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V√©ronis, 1998;Murray and Green, 2004;	0
252	9379	9379	S13-13	Introduction	6	11	2.0	1.0	Passonneau et al., 2012b;Jurgens, 2013;Navigli et al., 2013).	0
253	9380	9380	S13-13	Introduction	7	12	2.0	1.0	Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled.	0
254	9381	9381	S13-13	Introduction	8	13	2.0	1.0	Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability.	0
255	9382	9382	S13-13	Introduction	9	14	3.0	1.0	WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD systems for specific types of text (e.g., social media), or the inventory may lack domain-specific senses.	0
256	9383	9383	S13-13	Introduction	10	15	3.0	1.0	Word Sense Induction (WSI) has been proposed as a method for overcoming such limitations by learning the senses automatically from text.	0
257	9384	9384	S13-13	Introduction	11	16	3.0	1.0	In essence, a WSI algorithm acts as a lexicographer by grouping word usages according to their shared meaning.	0
258	9385	9385	S13-13	Introduction	12	17	3.0	1.0	The second goal of this task is to assess the performance of WSI algorithms when they are able to model multiple meanings of a usage with graded senses.	0
259	9386	9386	S13-13	Introduction	13	18	4.0	1.0	Task 12 focuses on disambiguating senses for 50 target lemmas: 20 nouns, 20 verbs, and 10 adjectives (Sec. 2).	0
260	9387	9387	S13-13	Introduction	14	19	4.0	1.0	Since the Task evaluates only unsupervised systems, no training data was provided; however, to enable more comparison, Unsupervised WSD systems were also allowed to participate.	0
261	9388	9388	S13-13	Introduction	15	20	4.0	1.0	Participating systems were evaluated in two settings (Sec. 3), depending on whether they used induced senses or WordNet 3.1 senses for their annotations.	0
262	9389	9389	S13-13	Introduction	16	21	4.0	1.0	The results (Sec. 5) demonstrate a substantial improvement over the competitive most frequent sense baseline.	0
263	9390	9390	S13-13	Task Description	1	22	1.0	1.0	This task required participating systems to annotate instances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected due to its fine-grained senses.	1
264	9391	9391	S13-13	Task Description	2	23	1.0	1.0	Participants could label each instance with one or more senses, weighting	1
265	9392	9392	S13-13	Task Description	3	24	2.0	1.0	We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to encompass the thoughts of every entity we know.	0
266	9393	9393	S13-13	Task Description	4	25	2.0	1.0	dark%3:00:01:: -devoid of or deficient in light or brightness; shadowed or black dark%3:00:00:: -secret I ask because my practice has always been to allow about five minutes grace, then remove it.	0
267	9394	9394	S13-13	Task Description	5	26	2.0	1.0	ask%2:32:02:: -direct or put; seek an answer to ask%2:32:04:: -address a question to and expect an answer from Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual ambiguity (bottom).	0
268	9395	9395	S13-13	Task Description	6	27	3.0	1.0	Senses are specified using their WordNet 3.1 sense keys.	0
269	9396	9396	S13-13	Task Description	7	28	3.0	1.0	each by their applicability.	0
270	9397	9397	S13-13	Task Description	8	29	3.0	1.0	Table 1 highlights two example contexts where multiple senses apply.	0
271	9398	9398	S13-13	Task Description	9	30	4.0	1.0	The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.	0
272	9399	9399	S13-13	Task Description	10	31	4.0	1.0	a as being devoid of light and the causal result of being secret.	0
273	9400	9400	S13-13	Task Description	11	32	4.0	1.0	"In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations ""I ask [you] because"" (sense ask%2:32:04::) or ""I ask [the question] because"" (sense ask%2:32:02::)."	0
274	9553	9553	S13-13	Conclusion	1	185	1.0	4.0	We have introduced a new evaluation setting for WSI and WSD systems where systems are measured by their ability to detect and weight multiple applicable senses for a single context.	0
275	9554	9554	S13-13	Conclusion	2	186	2.0	4.0	Four teams submitted nine systems, annotating a total of 4664 contexts for 50 words from the OANC.	0
276	9555	9555	S13-13	Conclusion	3	187	2.0	4.0	Many systems were able to surpass the competitive MFS baseline.	0
277	9556	9556	S13-13	Conclusion	4	188	3.0	4.0	Furthermore, when WSI systems were trained to produce only a single sense label, the performance of resulting semi-supervised WSD systems surpassed that of many supervised systems in previous WSD evaluations.	0
278	9557	9557	S13-13	Conclusion	5	189	3.0	4.0	Future work may assess the impact of graded sense annotations in a task-based setting.	0
279	9558	9558	S13-13	Conclusion	6	190	4.0	4.0	All materials have been released on the task website.	0
280	9559	9559	S13-13	Conclusion	7	191	4.0	4.0	1	0
281	10486	10486	S14-6	title	1	1	4.0	1.0	SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands	0
282	10487	10487	S14-6	abstract	1	2	1.0	1.0	SemEval-2014	0
283	10488	10488	S14-6	abstract	2	3	2.0	1.0	Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches.	0
284	10489	10489	S14-6	abstract	3	4	2.0	1.0	The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm.	1
285	10490	10490	S14-6	abstract	4	5	3.0	1.0	Six teams submitted systems using both rule-based and statistical methods.	0
286	10491	10491	S14-6	abstract	5	6	4.0	1.0	The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context.	0
287	10492	10492	S14-6	abstract	6	7	4.0	1.0	However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.	0
288	10493	10493	S14-6	Introduction	1	8	1.0	1.0	Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language.	0
289	10494	10494	S14-6	Introduction	2	9	1.0	1.0	Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013;	0
290	10495	10495	S14-6	Introduction	3	10	1.0	1.0	Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011;	0
291	10496	10496	S14-6	Introduction	4	11	1.0	1.0	Kim and Mooney, 2012).	0
292	10497	10497	S14-6	Introduction	5	12	1.0	1.0	Different parsers can be distinguished by the level of supervision they require during training.	0
293	10498	10498	S14-6	Introduction	6	13	2.0	1.0	Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form.	0
294	10499	10499	S14-6	Introduction	7	14	2.0	1.0	However, because annotated data is often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information.	0
295	10500	10500	S14-6	Introduction	8	15	2.0	1.0	For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system.	0
296	10501	10501	S14-6	Introduction	9	16	2.0	1.0	In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction.	0
297	10502	10502	S14-6	Introduction	10	17	2.0	1.0	In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing.	0
298	10503	10503	S14-6	Introduction	11	18	2.0	1.0	The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board.	0
299	10504	10504	S14-6	Introduction	12	19	3.0	1.0	Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context.	0
300	10505	10505	S14-6	Introduction	13	20	3.0	1.0	For example, the command in Figure 1 may have several plausible interpretations, given different board configurations.	0
301	10506	10506	S14-6	Introduction	14	21	3.0	1.0	'	0
302	10507	10507	S14-6	Introduction	15	22	3.0	1.0	Move the pyramid on the blue cube on the gray one.'	0
303	10508	10508	S14-6	Introduction	16	23	3.0	1.0	The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009;	0
304	10509	10509	S14-6	Introduction	17	24	3.0	1.0	Mitkov, 1999).	0
305	10510	10510	S14-6	Introduction	18	25	4.0	1.0	More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation.	0
306	10511	10511	S14-6	Introduction	19	26	4.0	1.0	Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph.	0
307	10512	10512	S14-6	Introduction	20	27	4.0	1.0	Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands.	0
308	10513	10513	S14-6	Introduction	21	28	4.0	1.0	In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research.	0
309	10514	10514	S14-6	Introduction	22	29	4.0	1.0	In the remainder of this paper, we describe the task, the dataset and the metrics used for evaluation.	0
310	10515	10515	S14-6	Introduction	23	30	4.0	1.0	We then compare the approaches used by participant systems and conclude with suggested improvements for future work.	0
311	10516	10516	S14-6	Task Description	1	31	1.0	1.0	The long term research goal encouraged by the task is to develop a system that will robustly execute NL robotic commands.	0
312	10517	10517	S14-6	Task Description	2	32	2.0	1.0	In general, this is a highly complex problem involving computational processing of language, spatial reasoning, contextual awareness and knowledge representation.	0
313	10518	10518	S14-6	Task Description	3	33	2.0	1.0	To simplify the problem, participants were provided with additional tools and resources, allowing them to focus on developing a semantic parser for a fixed domain that would fit into an existing component architecture.	0
314	10519	10519	S14-6	Task Description	4	34	3.0	1.0	Figure 2 shows how these components interact.	0
315	10520	10520	S14-6	Task Description	5	35	4.0	1.0	Semantic parser: Systems submitted by participants are semantic parsers that accept an NL command as input, mapping this to a formal Robot Control Language (RCL), described further in section 3.3.	0
316	10521	10521	S14-6	Task Description	6	36	4.0	1.0	The Robot Commands Treebank used for the both training and evaluation is an annotated corpus that pairs NL commands with contextual RCL statements.	0
317	10671	10671	S14-6	Conclusion and Future Work	1	186	1.0	4.0	This paper described a new task for SemEval: Supervised Semantic Parsing of Robotic Spatial Commands.	0
318	10672	10672	S14-6	Conclusion and Future Work	2	187	1.0	4.0	Despite its novel nature, the task attracted high-quality submissions from six teams, using a variety of semantic parsing strategies.	0
319	10673	10673	S14-6	Conclusion and Future Work	3	188	1.0	4.0	It is hoped that this task will reappear at Se-mEval.	0
320	10674	10674	S14-6	Conclusion and Future Work	4	189	2.0	4.0	Several lessons were learnt from this first version of the shared task which can be used to improve the task in future.	0
321	10675	10675	S14-6	Conclusion and Future Work	5	190	2.0	4.0	One issue which several participants noted was the way in which the treebank was split into training and evaluation datasets.	0
322	10676	10676	S14-6	Conclusion and Future Work	6	191	2.0	4.0	Out of the 3,409 sentences in the treebank, the first 2,500 sequential sentences were chosen for training.	0
323	10677	10677	S14-6	Conclusion and Future Work	7	192	3.0	4.0	Because this data was not randomized, certain syntactic structures were only found during evaluation and were not present in the training data.	0
324	10678	10678	S14-6	Conclusion and Future Work	8	193	3.0	4.0	Although this may have affected results, all participants evaluated their systems against the same datasets.	0
325	10679	10679	S14-6	Conclusion and Future Work	9	194	3.0	4.0	Based on participant feedback, in addition to reporting P and NP-measures, it would also be illuminating to include a metric such as Parseval F1-scores to measure partial accuracy.	0
326	10680	10680	S14-6	Conclusion and Future Work	10	195	4.0	4.0	An improved version of the task could also feature a better dataset by expanding the treebank, not only in terms of size but also in terms of linguistic structure.	0
327	10681	10681	S14-6	Conclusion and Future Work	11	196	4.0	4.0	Many commands captured in the annotation game are not yet represented in RCL due to linguistic phenomena such as negation and conditional statements.	0
328	10682	10682	S14-6	Conclusion and Future Work	12	197	4.0	4.0	Looking forward, a more promising approach to improving the spatial planner could be probabilistic planning, so that semantic parsers could interface with probabilistic facts with confidence measures.	0
329	10683	10683	S14-6	Conclusion and Future Work	13	198	4.0	4.0	This approach is particularly suitable for robotics, where sensors often supply noisy signals about the robot's environment.	0
330	11010	11010	S14-9	title	1	1	4.0	1.0	SemEval-2014 Task 9: Sentiment Analysis in Twitter	0
331	11011	11011	S14-9	abstract	1	2	1.0	1.0	We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014.	0
332	11012	11012	S14-9	abstract	2	3	2.0	1.0	It is a continuation of the last year's task that ran successfully as part of SemEval-2013.	0
333	11013	11013	S14-9	abstract	3	4	2.0	1.0	As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams).	0
334	11014	11014	S14-9	abstract	4	5	3.0	1.0	This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii) LiveJournal sentences.	0
335	11015	11015	S14-9	abstract	5	6	4.0	1.0	We further tested on (iv) 2013 tweets, and (v) 2013 SMS messages.	0
336	11016	11016	S14-9	abstract	6	7	4.0	1.0	The highest F1score on (i) was achieved by NRC-Canada at 86.63 for subtask A and by TeamX at 70.96 for subtask B.	0
337	11017	11017	S14-9	Introduction	1	8	1.0	1.0	In the past decade, new forms of communication have emerged and have become ubiquitous through social media.	0
338	11018	11018	S14-9	Introduction	2	9	1.0	1.0	Microblogs (e.g., Twitter), Weblogs (e.g., LiveJournal) and cell phone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.	0
339	11019	11019	S14-9	Introduction	3	10	1.0	1.0	Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.	0
340	11020	11020	S14-9	Introduction	4	11	1.0	1.0	The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, e.g., RT for re-tweet and #hashtags 1 .	0
341	11021	11021	S14-9	Introduction	5	12	1.0	1.0	This work is licensed under a Creative Commons Attribution 4.0 International Licence.	0
342	11022	11022	S14-9	Introduction	6	13	1.0	1.0	Page numbers and proceedings footer are added by the organisers.	0
343	11023	11023	S14-9	Introduction	7	14	2.0	1.0	Licence details: http://creativecommons.org/licenses/by/4.0/	0
344	11024	11024	S14-9	Introduction	8	15	2.0	1.0	1 Hashtags are a type of tagging for Twitter messages.	0
345	11025	11025	S14-9	Introduction	9	16	2.0	1.0	Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document.	0
346	11026	11026	S14-9	Introduction	10	17	2.0	1.0	How to handle such challenges so as to automatically mine and understand people's opinions and sentiments has only recently been the subject of research (Jansen et al., 2009;	0
347	11027	11027	S14-9	Introduction	11	18	2.0	1.0	Barbosa and Feng, 2010;	0
348	11028	11028	S14-9	Introduction	12	19	2.0	1.0	Bifet et al., 2011;Davidov et al., 2010;O'Connor et al., 2010;	0
349	11029	11029	S14-9	Introduction	13	20	2.0	1.0	Pak and Paroubek, 2010;Tumasjan et al., 2010;Kouloumpis et al., 2011).	0
350	11030	11030	S14-9	Introduction	14	21	3.0	1.0	Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year's SemEval Task 4 (Pontiki et al., 2014).	0
351	11031	11031	S14-9	Introduction	15	22	3.0	1.0	These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets.	0
352	11032	11032	S14-9	Introduction	16	23	3.0	1.0	While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment.	0
353	11033	11033	S14-9	Introduction	17	24	3.0	1.0	Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media.	0
354	11034	11034	S14-9	Introduction	18	25	3.0	1.0	Toward that goal, we created the Se-mEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013).	0
355	11035	11035	S14-9	Introduction	19	26	3.0	1.0	It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity.	0
356	11036	11036	S14-9	Introduction	20	27	4.0	1.0	This year, we extended the corpus by adding new tweets and LiveJournal sentences.	0
357	11037	11037	S14-9	Introduction	21	28	4.0	1.0	Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz√°lez-Ib√°√±ez et al., 2011;	0
358	11038	11038	S14-9	Introduction	22	29	4.0	1.0	Liebrecht et al., 2013).	0
359	11039	11039	S14-9	Introduction	23	30	4.0	1.0	In fact, sarcasm indicates that the message polarity should be flipped.	0
360	11040	11040	S14-9	Introduction	24	31	4.0	1.0	With this in mind, this year, we also evaluate on sarcastic tweets.	0
361	11041	11041	S14-9	Introduction	25	32	4.0	1.0	In the remainder of this paper, we first describe the task, the dataset creation process and the evaluation methodology.	0
362	11042	11042	S14-9	Introduction	26	33	4.0	1.0	We then summarize the characteristics of the approaches taken by the participating systems, and we discuss their scores.	0
363	11043	11043	S14-9	Task Description	1	34	1.0	1.0	As SemEval-2013	0
364	11044	11044	S14-9	Task Description	2	35	2.0	1.0	Task 2, we included two subtasks: an expression-level subtask and a messagelevel subtask.	0
365	11045	11045	S14-9	Task Description	3	36	3.0	2.0	Participants could choose to participate in either or both.	0
366	11046	11046	S14-9	Task Description	4	37	4.0	2.0	Below we provide short descriptions of the objectives of these two subtasks.	0
367	11101	11101	S14-9	Subtask A	1	92	1.0	3.0	Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams.	0
368	11102	11102	S14-9	Subtask A	2	93	2.0	3.0	There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
369	11103	11103	S14-9	Subtask A	3	94	3.0	3.0	The best systems were constrained.	0
370	11104	11104	S14-9	Subtask A	4	95	4.0	3.0	All participating systems outperformed the majority class baseline by a sizable margin.	0
371	11142	11142	S14-9	Conclusion	1	133	1.0	4.0	We have described the data, the experimental setup and the results for SemEval-2014 Task 9.	0
372	11143	11143	S14-9	Conclusion	2	134	1.0	4.0	As in 2013, our task was the most popular one at SemEval-2014, attracting 46 participating teams: 21 in subtask A (27 submissions) and 44 in subtask B (50 submissions).	0
373	11144	11144	S14-9	Conclusion	3	135	2.0	4.0	We introduced three new test sets for 2014: an in-domain Twitter dataset, an out-of-domain Live-Journal test set, and a dataset of tweets containing sarcastic content.	0
374	11145	11145	S14-9	Conclusion	4	136	2.0	4.0	While the performance on the LiveJournal test set was mostly comparable to the in-domain Twitter test set, for most teams there was a sharp drop in performance for sarcastic tweets, highlighting better handling of sarcastic language as one important direction for future work in Twitter sentiment analysis.	0
375	11146	11146	S14-9	Conclusion	5	137	2.0	4.0	We plan to run the task again in 2015 with the inclusion of a new sub-evaluation on detecting sarcasm with the goal of stimulating research in this area; we further plan to add one more test domain.	0
376	11147	11147	S14-9	Conclusion	6	138	3.0	4.0	-test), and the indicates a system that includes a task co-organizer as a team member.	0
377	11148	11148	S14-9	Conclusion	7	139	3.0	4.0	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
378	11149	11149	S14-9	Conclusion	8	140	4.0	4.0	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
379	11150	11150	S14-9	Conclusion	9	141	4.0	4.0	In the 2015 edition of the task, we might also remove the constrained/unconstrained distinction.	0
380	11151	11151	S14-9	Conclusion	10	142	4.0	4.0	Finally, as there are multiple opinions about a topic in Twitter, we would like to focus on detecting the sentiment trend towards a topic.	0
381	12453	12453	S15-6	title	1	1	4.0	1.0	SemEval-2015 Task 6: Clinical TempEval	0
382	12454	12454	S15-6	abstract	1	2	1.0	1.0	Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.	0
383	12455	12455	S15-6	abstract	2	3	2.0	1.0	Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.	1
384	12456	12456	S15-6	abstract	3	4	3.0	1.0	Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.	0
385	12457	12457	S15-6	abstract	4	5	4.0	1.0	Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.	0
386	12458	12458	S15-6	Introduction	1	6	1.0	1.0	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
387	12459	12459	S15-6	Introduction	2	7	1.0	1.0	Verhagen et al., 2010;UzZaman et al., 2013).	0
388	12460	12460	S15-6	Introduction	3	8	1.0	1.0	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	0
389	12461	12461	S15-6	Introduction	4	9	2.0	1.0	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
390	12462	12462	S15-6	Introduction	5	10	2.0	1.0	Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic.	0
391	12463	12463	S15-6	Introduction	6	11	2.0	1.0	This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understanding of the language of time beyond newswire expressions and structure.	0
392	12464	12464	S15-6	Introduction	7	12	3.0	1.0	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
393	12465	12465	S15-6	Introduction	8	13	3.0	1.0	Participating systems are expected to take as input raw text such as:	0
394	12466	12466	S15-6	Introduction	9	14	3.0	1.0	April 23, 2014:	0
395	12467	12467	S15-6	Introduction	10	15	4.0	1.0	The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
396	12468	12468	S15-6	Introduction	11	16	4.0	1.0	And output annotations over the text that capture the following kinds of information:	0
397	12469	12469	S15-6	Introduction	12	17	4.0	1.0	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
398	12488	12488	S15-6	Tasks	1	36	1.0	2.0	A total of nine tasks were included, grouped into three categories:  (Pustejovsky and Stubbs, 2011) between events and/or times, represented by TLINK annotations with TYPE=CONTAINS in the THYME corpus	0
399	12489	12489	S15-6	Tasks	2	37	2.0	2.0	The evaluation was run in two phases:	0
400	12490	12490	S15-6	Tasks	3	38	3.0	2.0	1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2.	0
401	12491	12491	S15-6	Tasks	4	39	4.0	2.0	Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations	0
402	13717	13717	S15-13	title	1	1	4.0	1.0	SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking	1
403	13718	13718	S15-13	abstract	1	2	1.0	1.0	In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.	1
404	13719	13719	S15-13	abstract	2	3	2.0	1.0	Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.	0
405	13720	13720	S15-13	abstract	3	4	3.0	1.0	Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.	0
406	13721	13721	S15-13	abstract	4	5	4.0	1.0	Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.	0
407	13722	13722	S15-13	abstract	5	6	4.0	1.0	Moreover, we investigate this task in a multilingual setting and for some specific domains.	0
408	13723	13723	S15-13	Introduction	1	7	1.0	1.0	The Senseval and SemEval evaluation series represent key moments in the community of computational linguistics and related areas.	0
409	13724	13724	S15-13	Introduction	2	8	1.0	1.0	Their focus has been to provide objective evaluations of methods within the wide spectrum of semantic techniques for tasks mainly related to automatic text understanding.	0
410	13725	13725	S15-13	Introduction	3	9	1.0	1.0	Through SemEval-2015 task 13 we both continue and renew the longstanding tradition of disambiguation tasks, by addressing multilingual WSD and EL in a joint manner.	0
411	13726	13726	S15-13	Introduction	4	10	1.0	1.0	WSD (Navigli, 2009;	0
412	13727	13727	S15-13	Introduction	5	11	1.0	1.0	Navigli, 2012) is a historical task aimed at explicitly assigning meanings to single-word and multi-word occurrences within text, a task which today is more alive than ever in the research community.	0
413	13728	13728	S15-13	Introduction	6	12	2.0	1.0	EL (Erbs et al., 2011;	0
414	13729	13729	S15-13	Introduction	7	13	2.0	1.0	Cornolti et al., 2013;	0
415	13730	13730	S15-13	Introduction	8	14	2.0	1.0	Rao et al., 2013) is a more recent task which aims at discovering mentions of entities within a text and linking them to the most suitable entry in a knowledge base.	0
416	13731	13731	S15-13	Introduction	9	15	2.0	1.0	Both these tasks aim at handling the inherent ambiguity of natural language, however WSD tackles it from a lexicographic perspective, while EL tackles it from an encyclopedic one.	0
417	13732	13732	S15-13	Introduction	10	16	2.0	1.0	Specifically, the main difference between the two tasks lies in the kind of inventory they use.	0
418	13733	13733	S15-13	Introduction	11	17	2.0	1.0	For instance, WordNet (Miller et al., 1990), a manually curated semantic network for the English language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns.	0
419	13734	13734	S15-13	Introduction	12	18	3.0	1.0	More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become -together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) -the main reference inventory for EL systems.	0
420	13735	13735	S15-13	Introduction	13	19	3.0	1.0	Over the years, the research community has typically focused on each of these tasks separately.	0
421	13736	13736	S15-13	Introduction	14	20	3.0	1.0	Recently, however, joint approaches have been proposed (Moro et al., 2014b).	0
422	13737	13737	S15-13	Introduction	15	21	3.0	1.0	One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013).	0
423	13738	13738	S15-13	Introduction	16	22	3.0	1.0	A case in point here is BabelNet 1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012).	0
424	13739	13739	S15-13	Introduction	17	23	4.0	1.0	Resources like BabelNet provide a common ground for the tasks of WSD and EL.	0
425	13740	13740	S15-13	Introduction	18	24	4.0	1.0	In this task our goal is to promote research in the direction of joint word sense and named entity disambiguation, so as to concentrate research efforts on the aspects that differentiate these two tasks without duplicating research on common problems such as identifying the right meaning in context.	0
426	13741	13741	S15-13	Introduction	19	25	4.0	1.0	However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags.	0
427	13742	13742	S15-13	Introduction	20	26	4.0	1.0	Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers.	0
428	13743	13743	S15-13	Introduction	21	27	4.0	1.0	In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation , our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues).	0
429	13744	13744	S15-13	Introduction	22	28	4.0	1.0	Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain.	0
430	13745	13745	S15-13	Task Setup	1	29	1.0	1.0	The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided.	0
431	13746	13746	S15-13	Task Setup	2	30	2.0	1.0	Differently from previous editions Lefever and Hoste, 2013;Manandhar et al., 2010;	0
432	13747	13747	S15-13	Task Setup	3	31	3.0	1.0	Lefever and Hoste, 2010;Pradhan et al., 2007;	0
433	13748	13748	S15-13	Task Setup	4	32	4.0	1.0	Navigli et al., 2007;	0
434	13749	13749	S15-13	Task Setup	5	33	4.0	1.0	Snyder and Palmer, 2004;Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013 and ERD (Carmel et al., 2014).	0
435	23177	23177	S19-2	title	1	1	4.0	1.0	SemEval-2019 Task 2: Unsupervised Lexical Frame Induction	0
436	23178	23178	S19-2	abstract	1	2	1.0	1.0	This paper presents Unsupervised Lexical	0
437	23179	23179	S19-2	abstract	2	3	2.0	1.0	Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.	0
438	23180	23180	S19-2	abstract	3	4	2.0	1.0	Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.	1
439	23181	23181	S19-2	abstract	4	5	3.0	1.0	Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.	0
440	23182	23182	S19-2	abstract	5	6	3.0	1.0	Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.	0
441	23183	23183	S19-2	abstract	6	7	4.0	1.0	The shared task attracted nine teams, of whom three reported promising results.	0
442	23184	23184	S19-2	abstract	7	8	4.0	1.0	This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.	0
443	23185	23185	S19-2	Introduction	1	9	1.0	1.0	SemEval 2019	0
444	23186	23186	S19-2	Introduction	2	10	1.0	1.0	Task 2 focused on the unsupervised semantic labeling of a set of prespecified (semantically) unlabeled structures (Figure 1).	0
445	23187	23187	S19-2	Introduction	3	11	1.0	1.0	Unsupervised learning methods analyze these structures (Figure 1a) to augment them with semantic labels (Figure 1b).	0
446	23188	23188	S19-2	Introduction	4	12	1.0	1.0	The shape of the manually labeled input frames is constrained to an acyclic connected tree of lexical items (words and multi-word units) of maximum depth 1, where just one root governs several arguments.	0
447	23189	23189	S19-2	Introduction	5	13	2.0	1.0	The task used Berkeley FrameNet (FN) (Ruppenhofer et al., 2016) and Q. Zadeh and Petruck (2019), guidelines for this task, to determine the arguments and label them with semantic information.	0
448	23190	23190	S19-2	Introduction	6	14	2.0	1.0	We compared the proposed system results for unsupervised semantic tagging with that of human annotated (or, gold-standard) data in three different subtasks (Figure 2).	0
449	23191	23191	S19-2	Introduction	7	15	2.0	1.0	To evaluate the systems, we computed distributional similarities between  their generated unsupervised labeled data and human annotated reference data.	0
450	23192	23192	S19-2	Introduction	8	16	2.0	1.0	For computing similarities we used general purpose numeral methods of text clustering, in particular BCUBED F-SCORE (Bagga and Baldwin, 1998) as the single figure of merit to rank the systems.	0
451	23193	23193	S19-2	Introduction	9	17	3.0	1.0	The most important result of the shared task is the creation of a benchmark for a future complex task.	0
452	23194	23194	S19-2	Introduction	10	18	3.0	1.0	This benchmark includes a moderately sized, manually annotated set of frames, where only the verbs of each were included, along with their core frame elements (which uniquely define a frame as Ruppenhofer et al. describe).	0
453	23195	23195	S19-2	Introduction	11	19	3.0	1.0	To complement FN's core frame elements that have highly specific meanings, the benchmark also includes the annotated argument structures of the verbs based on the generic semantic roles proposed for verb classes in VerbNet 3.2 (Kipper et al., 2000;	0
454	23196	23196	S19-2	Introduction	12	20	3.0	1.0	Palmer et al., 2017).	0
455	23197	23197	S19-2	Introduction	13	21	4.0	1.0	The benchmark comes with simplified annotation guidelines and a modular annotation sys-tem with browsing and editing capabilities.	0
456	23198	23198	S19-2	Introduction	14	22	4.0	1.0	1 Complementing the benchmarking are several state-ofthe-art competing baselines, from the participants, that serve as a point of departure for improvements in the future.	0
457	23199	23199	S19-2	Introduction	15	23	4.0	1.0	2	0
458	23200	23200	S19-2	Introduction	16	24	4.0	1.0	The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks.	0
459	23201	23201	S19-2	Background	1	25	1.0	1.0	Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structures have developed in parallel over several decades in theoretical linguistic studies about the syntax-semantics interface, as well as in empirical corpus-driven applications in natural language processing.	0
460	23202	23202	S19-2	Background	2	26	1.0	1.0	Building repositories of (lexical) semantic frames is a core component in all of these efforts.	0
461	23203	23203	S19-2	Background	3	27	1.0	1.0	In formal studies, lexical semantic frame knowledge bases instantiate foundational theories with tangible examples, e.g., to provide supporting evidence for the theory.	0
462	23204	23204	S19-2	Background	4	28	1.0	1.0	Practically, frame semantic repositories play a pivotal role in natural language understanding and semantic parsing, both as inspiration for a representation format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others.	0
463	23205	23205	S19-2	Background	5	29	2.0	1.0	However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks.	0
464	23206	23206	S19-2	Background	6	30	2.0	1.0	The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English.	0
465	23207	23207	S19-2	Background	7	31	2.0	1.0	While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at best.	0
466	23208	23208	S19-2	Background	8	32	2.0	1.0	This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017).	0
467	23209	23209	S19-2	Background	9	33	2.0	1.0	Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies.	0
468	23210	23210	S19-2	Background	10	34	3.0	1.0	Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet.	0
469	23211	23211	S19-2	Background	11	35	3.0	1.0	Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information.	0
470	23212	23212	S19-2	Background	12	36	3.0	1.0	Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments.	0
471	23213	23213	S19-2	Background	13	37	3.0	1.0	Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)).	0
472	23214	23214	S19-2	Background	14	38	4.0	1.0	This task aimed to benchmark a class of such unsupervised frame induction methods.	0
473	23215	23215	S19-2	Background	15	39	4.0	1.0	The ambitious goal of this task was the unsupervised induction of frame semantic structures from tokenized and morphosyntacally labeled text corpora.	0
474	23216	23216	S19-2	Background	16	40	4.0	1.0	We sought to achieve this goal by building an evaluation benchmark for three tasks.	0
475	23217	23217	S19-2	Background	17	41	4.0	1.0	Task A dealt with unsupervised labeling of verb lemmas with their frame meaning.	0
476	23218	23218	S19-2	Background	18	42	4.0	1.0	Task B involved unsupervised argument role labeling, where B.1 benchmarked unsupervised labeling of frame-specific frame elements (FEs) based on FN, and B.2 benchmarked unsupervised role labeling of arguments in Case Grammar terms (Fillmore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet.	0
477	23219	23219	S19-2	Task Description	1	43	1.0	1.0	The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation).	0
478	23220	23220	S19-2	Task Description	2	44	2.0	1.0	Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information.	0
479	23221	23221	S19-2	Task Description	3	45	3.0	1.0	Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels.	0
480	23222	23222	S19-2	Task Description	4	46	4.0	1.0	For development purposes, developers received a small labeled development set.	0
481	26004	26004	S20-4	title	1	1	4.0	1.0	SemEval-2020 Task 4: Commonsense Validation and Explanation	0
482	26005	26005	S20-4	abstract	1	2	1.0	1.0	In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons.	1
483	26006	26006	S20-4	abstract	2	3	1.0	1.0	Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not.	0
484	26007	26007	S20-4	abstract	3	4	2.0	1.0	The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense.	0
485	26008	26008	S20-4	abstract	4	5	2.0	1.0	In the third subtask, a participating system needs to generate the reason.	0
486	26009	26009	S20-4	abstract	5	6	3.0	1.0	We finally attracted 39 teams participating at least one of the three subtasks.	0
487	26010	26010	S20-4	abstract	6	7	3.0	1.0	For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans.	0
488	26011	26011	S20-4	abstract	7	8	4.0	1.0	However, for Subtask C, there is still a relatively large gap between systems and human performance.	0
489	26012	26012	S20-4	abstract	8	9	4.0	1.0	The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation;	0
490	26013	26013	S20-4	abstract	9	10	4.0	1.0	The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.	0
491	26014	26014	S20-4	Introduction	1	11	1.0	1.0	In the past decades, computer' ability in processing natural language has significantly improved.	0
492	26015	26015	S20-4	Introduction	2	12	1.0	1.0	However, its intelligence for understanding common sense expressed in language is still limited.	0
493	26016	26016	S20-4	Introduction	3	13	1.0	1.0	"For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: ""John put a turkey into a fridge"" while ""John put an elephant into the fridge"" does not, but it is non-trivial for a computer to tell the difference."	0
494	26017	26017	S20-4	Introduction	4	14	1.0	1.0	Arguably, commonsense reasoning plays a central role in a natural language understanding system (Davis, 2017).	0
495	26018	26018	S20-4	Introduction	5	15	1.0	1.0	It is essential to gauge how well computers can understand whether a given statement makes sense.	0
496	26019	26019	S20-4	Introduction	6	16	2.0	1.0	In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world.	0
497	26020	26020	S20-4	Introduction	7	17	2.0	1.0	1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012;Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016;	0
498	26021	26021	S20-4	Introduction	8	18	2.0	1.0	Ostermann et al., 2018b;	0
499	26022	26022	S20-4	Introduction	9	19	2.0	1.0	Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016;	0
500	26023	26023	S20-4	Introduction	10	20	2.0	1.0	Talmor et al., 2018;Mihaylov et al., 2018).	0
501	26024	26024	S20-4	Introduction	11	21	2.0	1.0	They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge.	0
502	26025	26025	S20-4	Introduction	12	22	3.0	1.0	The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.	0
503	26026	26026	S20-4	Introduction	13	23	3.0	1.0	The SemEval-2020	0
504	26027	26027	S20-4	Introduction	14	24	3.0	1.0	Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons.	0
505	26028	26028	S20-4	Introduction	15	25	3.0	1.0	"In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., ""John put an elephant into the fridge"" and ""John put a turkey into the fridge"", respectively."	0
506	26029	26029	S20-4	Introduction	16	26	3.0	1.0	The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense.	0
507	26030	26030	S20-4	Introduction	17	27	3.0	1.0	"For example, for the nonsensical statement, ""John put an elephant into the fridge"", the three options are ""An elephant is much bigger than a fridge"", ""Elephants are usually white while fridges are usually white"", and ""An elephant cannot eat a fridge."""	0
508	26031	26031	S20-4	Introduction	18	28	4.0	1.0	A system needs to identify the correct reason.	0
509	26032	26032	S20-4	Introduction	19	29	4.0	1.0	In addition, the third task requires the participating systems to generate the reason automatically.	0
510	26033	26033	S20-4	Introduction	20	30	4.0	1.0	We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.	0
511	26034	26034	S20-4	Introduction	21	31	4.0	1.0	There are 39 teams submitting valid systems to at least one subtask.	0
512	26035	26035	S20-4	Introduction	22	32	4.0	1.0	In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects.	0
513	26036	26036	S20-4	Introduction	23	33	4.0	1.0	However, for Subtask C, there is still a relatively large between system and human performances.	0
514	26037	26037	S20-4	Task Setup	1	34	1.0	1.0	Task Definition	0
515	26038	26038	S20-4	Task Setup	2	35	1.0	1.0	Formally, each instance in our dataset is composed of eight sentences:	0
516	26039	26039	S20-4	Task Setup	3	36	1.0	1.0	and s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not.	0
517	26040	26040	S20-4	Task Setup	4	37	1.0	1.0	They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense.	0
518	26041	26041	S20-4	Task Setup	5	38	1.0	1.0	For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense.	0
519	26042	26042	S20-4	Task Setup	6	39	1.0	1.0	So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options.	0
520	26043	26043	S20-4	Task Setup	7	40	1.0	1.0	For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense.	0
521	26044	26044	S20-4	Task Setup	8	41	2.0	1.0	Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.	0
522	26045	26045	S20-4	Task Setup	9	42	2.0	1.0	‚Ä¢ Subtask A: Validation Task:	0
523	26046	26046	S20-4	Task Setup	10	43	2.0	1.0	Select the statement of the two that does not make sense.	0
524	26047	26047	S20-4	Task Setup	11	44	2.0	1.0	s 1 : John put a turkey into a fridge.	0
525	26048	26048	S20-4	Task Setup	12	45	2.0	1.0	s 2 : John put an elephant into the fridge.	0
526	26049	26049	S20-4	Task Setup	13	46	2.0	1.0	In this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .	0
527	26050	26050	S20-4	Task Setup	14	47	2.0	1.0	‚Ä¢ Subtask B: Explanation (Multi-Choice)	0
528	26051	26051	S20-4	Task Setup	15	48	3.0	1.0	Task:	0
529	26052	26052	S20-4	Task Setup	16	49	3.0	1.0	Select the best reason that explains why the given statement does not make sense.	0
530	26053	26053	S20-4	Task Setup	17	50	3.0	1.0	Nonsensical statement (s n ):	0
531	26054	26054	S20-4	Task Setup	18	51	3.0	1.0	John put an elephant into the fridge.	0
532	26055	26055	S20-4	Task Setup	19	52	3.0	1.0	o 1 : An elephant is much bigger than a fridge.	0
533	26056	26056	S20-4	Task Setup	20	53	3.0	1.0	o 2 : Elephants are usually white while fridges are usually white.	0
534	26057	26057	S20-4	Task Setup	21	54	3.0	1.0	o 3 : An elephant cannot eat a fridge.	0
535	26058	26058	S20-4	Task Setup	22	55	4.0	1.0	In this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .	0
536	26059	26059	S20-4	Task Setup	23	56	4.0	1.0	‚Ä¢ Subtask C: Explanation (Generation)	0
537	26060	26060	S20-4	Task Setup	24	57	4.0	1.0	Task: Generate the reason why this statement does not make sense.	0
538	26061	26061	S20-4	Task Setup	25	58	4.0	1.0	Nonsensical statement (s n ):	0
539	26062	26062	S20-4	Task Setup	26	59	4.0	1.0	John put an elephant into the fridge.	0
540	26063	26063	S20-4	Task Setup	27	60	4.0	1.0	Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge.	0
541	26064	26064	S20-4	Task Setup	28	61	4.0	1.0	r 2 : A fridge is much smaller than an elephant.	0
542	26065	26065	S20-4	Task Setup	29	62	4.0	1.0	r 3 : Most of the fridges aren't large enough to contain an elephant.	0
543	26068	26068	S20-4	1	1	65	2.0	1.0	The reason is just the negation of the statement or a simple paraphrase.	0
544	26069	26069	S20-4	1	2	66	4.0	1.0	Obviously, a better explanation can be made.	0
545	26217	26217	S20-4	Related Work	1	214	1.0	3.0	Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention.	0
546	26218	26218	S20-4	Related Work	2	215	1.0	3.0	In the Winograd Schema Challenge (WSC) (Levesque et al., 2012;	0
547	26219	26219	S20-4	Related Work	3	216	1.0	4.0	Morgenstern and Ortiz, 2015), a model needs to solve hard co-reference resolution problems based on commonsense knowledge.	0
548	26220	26220	S20-4	Related Work	4	217	1.0	4.0	"For example, ""The trophy would not fit in the brown suitcase because it was too big."	0
549	26221	26221	S20-4	Related Work	5	218	1.0	4.0	"What was too big (trophy or suitcase)?"""	0
550	26222	26222	S20-4	Related Work	6	219	1.0	4.0	The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) emphasizes on events and consequences.	0
551	26223	26223	S20-4	Related Work	7	220	1.0	4.0	Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives.	0
552	26224	26224	S20-4	Related Work	8	221	1.0	4.0	All premises and alternatives are simple sentences.	0
553	26225	26225	S20-4	Related Work	9	222	1.0	4.0	"For example, the premise can be ""The man broke his toe."	0
554	26226	26226	S20-4	Related Work	10	223	1.0	4.0	"What was the CAUSE of this?"" and the two candidate answers are ""(1)"	0
555	26227	26227	S20-4	Related Work	11	224	1.0	4.0	"He got a hole in his sock."" and ""(2)"	0
556	26228	26228	S20-4	Related Work	12	225	1.0	4.0	"He dropped a hammer on his foot."""	0
557	26229	26229	S20-4	Related Work	13	226	1.0	4.0	Several subsequent datasets are inspired by COPA.	0
558	26230	26230	S20-4	Related Work	14	227	1.0	4.0	The JHU Ordinal Common-sense Inference (JOCI) (Zhang et al., 2017) aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation.	0
559	26231	26231	S20-4	Related Work	15	228	2.0	4.0	Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation.	0
560	26232	26232	S20-4	Related Work	16	229	2.0	4.0	Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused.	0
561	26233	26233	S20-4	Related Work	17	230	2.0	4.0	Besides, our dataset is not limited to events or situations.	0
562	26234	26234	S20-4	Related Work	18	231	2.0	4.0	It concerns a broader commonsense setting, which includes events, descriptions, assertion etc.	0
563	26235	26235	S20-4	Related Work	19	232	2.0	4.0	Some datasets are inspired by reading comprehension.	0
564	26236	26236	S20-4	Related Work	20	233	2.0	4.0	The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016;	0
565	26237	26237	S20-4	Related Work	21	234	2.0	4.0	Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story.	0
566	26238	26238	S20-4	Related Work	22	235	2.0	4.0	For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question.	0
567	26239	26239	S20-4	Related Work	23	236	2.0	4.0	Most questions require knowledge beyond the facts mentioned in the text.	0
568	26240	26240	S20-4	Related Work	24	237	2.0	4.0	Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want.	0
569	26241	26241	S20-4	Related Work	25	238	2.0	4.0	Some other datasets evolve from QA problems and care more about factual commonsense knowledge.	0
570	26242	26242	S20-4	Related Work	26	239	2.0	4.0	SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions.	0
571	26243	26243	S20-4	Related Work	27	240	2.0	4.0	Commonsense	0
572	26244	26244	S20-4	Related Work	28	241	2.0	4.0	QA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBook	0
573	26245	26245	S20-4	Related Work	29	242	3.0	4.0	QA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions.	0
574	26246	26246	S20-4	Related Work	30	243	3.0	4.0	The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences.	0
575	26247	26247	S20-4	Related Work	31	244	3.0	4.0	MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020).	0
576	26248	26248	S20-4	Related Work	32	245	3.0	4.0	Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.	0
577	26249	26249	S20-4	Related Work	33	246	3.0	4.0	Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018;	0
578	26250	26250	S20-4	Related Work	34	247	3.0	4.0	"Porada et al., 2019), such as ""gorilla-ride-camel""."	0
579	26251	26251	S20-4	Related Work	35	248	3.0	4.0	"In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as ""China's territory is larger than Japan's""."	0
580	26252	26252	S20-4	Related Work	36	249	3.0	4.0	And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017).	0
581	26253	26253	S20-4	Related Work	37	250	3.0	4.0	"Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task ""Tom's mom become (happy)/(upset) when Tom gets high grades in the exam"" is about social and emotional common sense."	0
582	26254	26254	S20-4	Related Work	38	251	3.0	4.0	For our first task, those statements that conforms to commonsense can also be phrased as being plausible.	0
583	26255	26255	S20-4	Related Work	39	252	3.0	4.0	Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.	0
584	26256	26256	S20-4	Related Work	40	253	3.0	4.0	More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions.	0
585	26257	26257	S20-4	Related Work	41	254	3.0	4.0	In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks.	0
586	26258	26258	S20-4	Related Work	42	255	3.0	4.0	Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on.	0
587	26259	26259	S20-4	Related Work	43	256	4.0	4.0	Event2	0
588	26260	26260	S20-4	Related Work	44	257	4.0	4.0	Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances.	0
589	26261	26261	S20-4	Related Work	45	258	4.0	4.0	These datasets are not directly useful for our benchmark since they focus only on a small domain.	0
590	26262	26262	S20-4	Related Work	46	259	4.0	4.0	Concept	0
591	26263	26263	S20-4	Related Work	47	260	4.0	4.0	Net is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004;	0
592	26264	26264	S20-4	Related Work	48	261	4.0	4.0	Havasi et al., 2007;	0
593	26265	26265	S20-4	Related Work	49	262	4.0	4.0	Speer and Havasi, 2013;	0
594	26266	26266	S20-4	Related Work	50	263	4.0	4.0	Speer et al., 2017).	0
595	26267	26267	S20-4	Related Work	51	264	4.0	4.0	Concept	0
596	26268	26268	S20-4	Related Work	52	265	4.0	4.0	Net constructs triples using labeled edges as relations and various words and/or phrases as entities.	0
597	26269	26269	S20-4	Related Work	53	266	4.0	4.0	It also has the sentences describing the corresponding triples.	0
598	26270	26270	S20-4	Related Work	54	267	4.0	4.0	In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.	0
599	26271	26271	S20-4	Related Work	55	268	4.0	4.0	Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences.	0
600	26272	26272	S20-4	Related Work	56	269	4.0	4.0	In our task here, we also provide training data with human annotations.	0
601	26273	26273	S20-4	Summary	1	270	1.0	4.0	This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation.	0
602	26274	26274	S20-4	Summary	2	271	1.0	4.0	In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences.	0
603	26275	26275	S20-4	Summary	3	272	1.0	4.0	The task attracted around 40 participating teams, out of which 31 teams submit their system papers.	0
604	26276	26276	S20-4	Summary	4	273	1.0	4.0	The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.	0
605	26277	26277	S20-4	Summary	5	274	2.0	4.0	We attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences.	0
606	26278	26278	S20-4	Summary	6	275	2.0	4.0	2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge (Zhou et al., 2019), which helps achieve considerably better performance.	0
607	26279	26279	S20-4	Summary	7	276	2.0	4.0	3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers.	0
608	26280	26280	S20-4	Summary	8	277	2.0	4.0	The top-3 systems also use OMCS, which potentially help them to attain better performances.	0
609	26281	26281	S20-4	Summary	9	278	2.0	4.0	4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty.	0
610	26282	26282	S20-4	Summary	10	279	3.0	4.0	5) Some instances have obvious patterns.	0
611	26283	26283	S20-4	Summary	11	280	3.0	4.0	"For example, there are tens of instances that contain ""put XXX into YYY"", and ""XXX is bigger than YYY"", making the problems simpler."	0
612	26284	26284	S20-4	Summary	12	281	3.0	4.0	6) Hundreds of crowd-sourcing workers write instances.	0
613	26285	26285	S20-4	Summary	13	282	3.0	4.0	"It is likely for workers to think about the shared commonsense knowledge, such as ""XXX is bigger/shorter/quicker/slower than YYY""."	0
614	26286	26286	S20-4	Summary	14	283	4.0	4.0	We consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation.	0
615	26287	26287	S20-4	Summary	15	284	4.0	4.0	2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns.	0
616	26288	26288	S20-4	Summary	16	285	4.0	4.0	3) Subtask A can be turned into a more difficult form.	0
617	26289	26289	S20-4	Summary	17	286	4.0	4.0	Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not.	0
618	26290	26290	S20-4	Summary	18	287	4.0	4.0	4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.	0
619	26291	26291	S20-5	title	1	1	4.0	1.0	SemEval-2020 Task 5: Counterfactual Recognition	0
620	26292	26292	S20-5	abstract	1	2	1.0	1.0	We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020.	0
621	26293	26293	S20-5	abstract	2	3	1.0	1.0	Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent).	0
622	26294	26294	S20-5	abstract	3	4	2.0	1.0	Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequents with causal relations.	0
623	26295	26295	S20-5	abstract	4	5	2.0	1.0	Our task provides a benchmark for counterfactual recognition in natural language with two subtasks.	0
624	26296	26296	S20-5	abstract	5	6	3.0	1.0	Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not.	0
625	26297	26297	S20-5	abstract	6	7	3.0	1.0	Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement.	0
626	26298	26298	S20-5	abstract	7	8	4.0	1.0	During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2.	0
627	26299	26299	S20-5	abstract	8	9	4.0	1.0	The data, baseline code, and leaderboard can be found	0
628	26300	26300	S20-5	Introduction	1	10	1.0	1.0	"Counterfactual statements describe events that did not happen or cannot happen, and the possible consequences had those events happened, e.g., ""if kangaroos had no tails, they would topple over"" (Lewis, 2013)."	0
629	26301	26301	S20-5	Introduction	2	11	1.0	1.0	"By developing a connection between the antecedent (e.g., ""kangaroos had no tails"") and consequent (e.g., ""they would topple over""), based on the imagination of possible worlds, humans can naturally form some causal judgments; e.g., having tails can prevent kangaroos from toppling over."	0
630	26302	26302	S20-5	Introduction	3	12	1.0	1.0	One can understand counterfactuals using knowledge and explore the relationship between causes and effects.	0
631	26303	26303	S20-5	Introduction	4	13	1.0	1.0	Although we may not be able to rollback the events which have happened or make impossible events occur in the real world, we can still think of potential outcomes of alternatives.	0
632	26304	26304	S20-5	Introduction	5	14	1.0	1.0	Counterfactual thinking is a remarkable ability of human beings and is considered by many researchers, to act as the highest level of causation in the ladder of causal reasoning.	0
633	26305	26305	S20-5	Introduction	6	15	1.0	1.0	Even the most advanced artificial intelligence system may still be far from achieving human-like counterfactual reasoning.	0
634	26306	26306	S20-5	Introduction	7	16	1.0	1.0	Counterfactual reasoning is an important component for AI systems in obtaining stronger capability in generalization (Pearl and Mackenzie, 2018).	0
635	26307	26307	S20-5	Introduction	8	17	1.0	1.0	Modeling counterfactuals has been studied in many different disciplines.	0
636	26308	26308	S20-5	Introduction	9	18	1.0	1.0	For example, research in psychology has shown that counterfactual thinking can affect human cognition and behaviors (Epstude and Roese, 2008;	0
637	26309	26309	S20-5	Introduction	10	19	2.0	1.0	Kray et al., 2010).	0
638	26310	26310	S20-5	Introduction	11	20	2.0	1.0	The landmark paper of (Goodman, 1947) gives a detailed analysis of counterfactual conditionals in philosophy and logistics.	0
639	26311	26311	S20-5	Introduction	12	21	2.0	1.0	As another example, counterfactuals have also been investigated in epidemiology to reveal the relationship between certain diseases and potential risk factors for those diseases (Vandenbroucke et al., 2016;	0
640	26312	26312	S20-5	Introduction	13	22	2.0	1.0	Krieger and Davey Smith, 2016).	0
641	26313	26313	S20-5	Introduction	14	23	2.0	1.0	We present a counterfactual recognition (CR) task, the task of determining whether a given statement conveys counterfactual thinking or not, and further analyzing the causal relations indicated by counterfactual statements.	0
642	26314	26314	S20-5	Introduction	15	24	2.0	1.0	In our counterfactual recognition task, we aim to model counterfactual semantics and reasoning in natural language.	1
643	26315	26315	S20-5	Introduction	16	25	2.0	1.0	Specifically, we provide a benchmark for counterfactual recognition with two subtasks.	0
644	26316	26316	S20-5	Introduction	17	26	2.0	1.0	Subtask-1 requires systems to determine whether a given statement is counterfactual or not.	0
645	26317	26317	S20-5	Introduction	18	27	2.0	1.0	The counterfactual detection task can serve as a foundation for downstream counterfactual analysis.	0
646	26318	26318	S20-5	Introduction	19	28	3.0	1.0	Subtask-2 requires systems to further locate the antecedent and consequent text spans in a given counterfactual statement, as the connection between an antecedent and consequent can reveal core causal inference clues.	0
647	26319	26319	S20-5	Introduction	20	29	3.0	1.0	To build the dataset for counterfactual recognition, we extract over 60,000 candidate counterfactual statements by scanning through news reports in three domains: finance, politics, and healthcare.	0
648	26320	26320	S20-5	Introduction	21	30	3.0	1.0	The first round of annotation focuses on labeling each sample as true or false, where true denotes a sample is counterfactual and false otherwise in Subtask-1.	0
649	26321	26321	S20-5	Introduction	22	31	3.0	1.0	A portion of samples labeled as true will be further used in Subtask-2 to detect the text spans that describe the antecedent and consequent.	0
650	26322	26322	S20-5	Introduction	23	32	3.0	1.0	Specifically, we carefully select 20,000 high-quality samples from the 60,000 statements and use them in Subtask-1, with 13,000 (65%) as the training set and the rest for testing.	0
651	26323	26323	S20-5	Introduction	24	33	3.0	1.0	The dataset for Subtask-2 contains 5,501 samples, among which we use 3,551 (65%) for training and the rest for testing.	0
652	26324	26324	S20-5	Introduction	25	34	3.0	1.0	To achieve a decent performance in our shared task, we expect the systems should have a certain level of language understanding capacity in both semantics and syntax, together with a certain level of commonsense reasoning ability.	0
653	26325	26325	S20-5	Introduction	26	35	3.0	1.0	In Subtask-1, the top-ranked submissions all use pre-trained neural models, which appear to be an effective way to integrate knowledge learned from large corpus.	0
654	26326	26326	S20-5	Introduction	27	36	3.0	1.0	All of these models use neural networks, which further confirms the effectiveness of distributed representation and subsymbolic approaches for this task.	0
655	26327	26327	S20-5	Introduction	28	37	4.0	1.0	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural networks with symbolic approaches.	0
656	26328	26328	S20-5	Introduction	29	38	4.0	1.0	The first-place model also utilizes data augmentation to further improve system performance.	0
657	26329	26329	S20-5	Introduction	30	39	4.0	1.0	In Subtask-2, top systems take two main approaches: sequence labelling or question answering.	0
658	26330	26330	S20-5	Introduction	31	40	4.0	1.0	Same as systems in Subtask-1, all of them benefit from pre-training.	0
659	26331	26331	S20-5	Introduction	32	41	4.0	1.0	We will provide a more detailed analysis in the system and result section.	0
660	26332	26332	S20-5	Introduction	33	42	4.0	1.0	We built a dataset for this shared task from scratch.	0
661	26333	26333	S20-5	Introduction	34	43	4.0	1.0	Our data, baseline code, and leaderboard can be found at https://competitions.codalab.org/competitions/21691.	0
662	26334	26334	S20-5	Introduction	35	44	4.0	1.0	The data and baseline code are also available at https://zenodo.org/record/3932442.	0
663	26335	26335	S20-5	Introduction	36	45	4.0	1.0	In general, our task here is a relatively basic one in counterfactual analysis in natural language.	0
664	26336	26336	S20-5	Introduction	37	46	4.0	1.0	We hope it will intrigue and facilitate further research on counterfactual analysis and can benefit other related downstream tasks.	0
665	26337	26337	S20-5	Task Setup	1	47	2.0	1.0	In this section, we detail the two counterfactual recognition subtasks and the metrics used to evaluate the performance.	0
666	26338	26338	S20-5	Task Setup	2	48	4.0	1.0	During the evaluation, participants can work on both subtasks or any one of them.	0
667	26483	26483	S20-5	Related Work	1	193	1.0	4.0	Modelling counterfactual thinking has started to attract more interest.	0
668	26484	26484	S20-5	Related Work	2	194	1.0	4.0	One of the previous works closest to ours is (Son et al., 2017), in which a small-scale counterfactual tweet dataset is collected from social media.	0
669	26485	26485	S20-5	Related Work	3	195	1.0	4.0	There are three main differences between that dataset and ours.	0
670	26486	26486	S20-5	Related Work	4	196	1.0	4.0	First, there are only 2,000 samples in the tweet dataset (including the supplement data mentioned in the paper), while our dataset for counterfactual detection in Subtask-1 is ten times larger, which we believe is important for training deep learning based models.	0
671	26487	26487	S20-5	Related Work	5	197	2.0	4.0	Second, our benchmark provides evaluation for antecedents and consequents extraction, which are essential components of counterfactual analysis.	0
672	26488	26488	S20-5	Related Work	6	198	2.0	4.0	Third, our dataset includes statements from three different domains (finance, politics, healthcare).	0
673	26489	26489	S20-5	Related Work	7	199	2.0	4.0	In contrast to the statements collected from tweets, which have a very large portion that are open-ended, vague thoughts, the counterfactuals in our dataset are more meaningful domain-related statements.	0
674	26490	26490	S20-5	Related Work	8	200	2.0	4.0	There is another dataset TIMETRAVEL proposed in (Qin et al., 2019)  in which given a short story and an alternative counterfactual event context, the story needs to be minimally revised to keep compatible with the intervening counterfactual event.	0
675	26491	26491	S20-5	Related Work	9	201	3.0	4.0	The empirical results show that it is still challenging for current neural language models to perform well on the counterfactual story rewriting task due to the lack of counterfactual reasoning capabilities.	0
676	26492	26492	S20-5	Related Work	10	202	3.0	4.0	In a broader viewpoint, counterfatuals are an important form of causal reasoning.	0
677	26493	26493	S20-5	Related Work	11	203	3.0	4.0	Researchers argue that the notion of counterfactuals is essential for causal reasoning, in which causal modeling is proposed to interpret counterfactual conditionals in natural language, and such work has been discussed since the possible worlds semantics developed in the 1970s (Lewis, 2013;	0
678	26494	26494	S20-5	Related Work	12	204	3.0	4.0	Lewis, 1986).	0
679	26495	26495	S20-5	Related Work	13	205	4.0	4.0	The more recent work renders useful insights by formulating causal inference as a three-level hierarchy, which are association, intervention, and counterfactual, respectively (Pearl and Mackenzie, 2018;Pearl, 2019).	0
680	26496	26496	S20-5	Related Work	14	206	4.0	4.0	"The top of the hierarchy is counterfactual-if a model can correctly answer counterfactual queries like ""what would happen if we had acted differently"", it should also be able to answer association and intervention queries."	0
681	26497	26497	S20-5	Related Work	15	207	4.0	4.0	The research in (Pearl, 1995;	0
682	26498	26498	S20-5	Related Work	16	208	4.0	4.0	Pearl, 2010) also made contributions to a general theory of causal inference, which is based on the Structural Causal Model (SCM), and counterfactual analysis is provided with a formal mathematical formalism.	0
683	27902	27902	S20-12	title	1	1	2.0	1.0	SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offens	0
684	27903	27903	S20-12	title	2	2	4.0	1.0	Eval 2020)	0
685	27904	27904	S20-12	abstract	1	3	1.0	1.0	We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020).	0
686	27905	27905	S20-12	abstract	2	4	2.0	1.0	The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish.	0
687	27906	27906	S20-12	abstract	3	5	3.0	1.0	Offens	0
688	27907	27907	S20-12	abstract	4	6	4.0	1.0	Eval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.	0
689	27908	27908	S20-12	Introduction	1	7	1.0	1.0	Offensive language is ubiquitous in social media platforms such as Facebook, Twitter, and Reddit, and it comes in many forms.	0
690	27909	27909	S20-12	Introduction	2	8	1.0	1.0	Given the multitude of terms and definitions related to offensive language used in the literature, several recent studies have investigated the common aspects of different abusive language detection tasks (Waseem et al., 2017;	0
691	27910	27910	S20-12	Introduction	3	9	1.0	1.0	Wiegand et al., 2018).	0
692	27911	27911	S20-12	Introduction	4	10	1.0	1.0	One such example is SemEval-2019 Task 6: Offens	0
693	27912	27912	S20-12	Introduction	5	11	1.0	1.0	Eval 1 (Zampieri et al., 2019b), which is the precursor to the present shared task.	0
694	27913	27913	S20-12	Introduction	6	12	1.0	1.0	Offens	0
695	27914	27914	S20-12	Introduction	7	13	1.0	1.0	Eval-2019 used the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets annotated using a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account (Zampieri et al., 2019a).	0
696	27915	27915	S20-12	Introduction	8	14	1.0	1.0	The assumption behind this annotation schema is that the target of offensive messages is an important variable that allows us to discriminate between, e.g., hate speech, which often consists of insults targeted toward a group, and cyberbullying, which typically targets individuals.	0
697	27916	27916	S20-12	Introduction	9	15	1.0	1.0	A number of recently organized related shared tasks followed similar hierarchical models.	0
698	27917	27917	S20-12	Introduction	10	16	2.0	1.0	Examples include HASOC-2019 (Mandl et al., 2019) for English, German, and Hindi, HatEval-2019 (Basile et al., 2019) for English and Spanish, GermEval-2019 for German (Stru√ü et al., 2019), and TRAC-2020 (Kumar et al., 2020) for English, Bengali, and Hindi.	0
699	27918	27918	S20-12	Introduction	11	17	2.0	1.0	Offens	0
700	27919	27919	S20-12	Introduction	12	18	2.0	1.0	Eval-2019 attracted nearly 800 team registrations and received 115 official submissions, which demonstrates the interest of the research community in this topic.	0
701	27920	27920	S20-12	Introduction	13	19	2.0	1.0	Therefore, we organized a follow-up, OffensEval-2020 2 (SemEval-2020 Task 12), which is described in this report, building on the success of OffensEval-2019 with several improvements.	0
702	27921	27921	S20-12	Introduction	14	20	2.0	1.0	In particular, we used the same three-level taxonomy to annotate new datasets in five languages, where each level in this taxonomy corresponds to a subtask in the competition:	0
703	27922	27922	S20-12	Introduction	15	21	2.0	1.0	‚Ä¢ Subtask A: Offensive language identification;	0
704	27923	27923	S20-12	Introduction	16	22	2.0	1.0	‚Ä¢ Subtask B: Automatic categorization of offense types;	0
705	27924	27924	S20-12	Introduction	17	23	2.0	1.0	‚Ä¢ Subtask C: Offense target identification.	0
706	27925	27925	S20-12	Introduction	18	24	2.0	1.0	This work is licensed under a Creative Commons Attribution 4.0 International License.	0
707	27926	27926	S20-12	Introduction	19	25	3.0	1.0	License details: http: //creativecommons.org/licenses/by/4.0/.	0
708	27927	27927	S20-12	Introduction	20	26	3.0	1.0	1 http://sites.google.com/site/offensevalsharedtask/offenseval2019 2 http://sites.google.com/site/offensevalsharedtask/home	0
709	27928	27928	S20-12	Introduction	21	27	3.0	1.0	The contributions of OffensEval-2020 can be summarized as follows:	0
710	27929	27929	S20-12	Introduction	22	28	3.0	1.0	‚Ä¢	0
711	27930	27930	S20-12	Introduction	23	29	3.0	1.0	We provided the participants with a new, large-scale semi-supervised training dataset containing over nine million English tweets (Rosenthal et al., 2020).	0
712	27931	27931	S20-12	Introduction	24	30	3.0	1.0	‚Ä¢	0
713	27932	27932	S20-12	Introduction	25	31	3.0	1.0	We introduced multilingual datasets, and we expanded the task to four new languages: Arabic (Mubarak et al., 2020b), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Turkish (√á√∂ltekin, 2020).	0
714	27933	27933	S20-12	Introduction	26	32	3.0	1.0	This opens the possibility for cross-lingual training and analysis, which several participants indeed explored.	0
715	27934	27934	S20-12	Introduction	27	33	3.0	1.0	‚Ä¢ Compared to OffensEval-2019, we used larger test datasets for all subtasks.	0
716	27935	27935	S20-12	Introduction	28	34	4.0	1.0	Overall, Offens	0
717	27936	27936	S20-12	Introduction	29	35	4.0	1.0	Eval-2020 was a very successful task.	0
718	27937	27937	S20-12	Introduction	30	36	4.0	1.0	The huge interest demonstrated last year continued this year, with 528 teams signing up to participate in the task, and 145 of them submitting official runs on the test dataset.	0
719	27938	27938	S20-12	Introduction	31	37	4.0	1.0	Furthermore, Offens	0
720	27939	27939	S20-12	Introduction	32	38	4.0	1.0	Eval-2020 received 70 system description papers, which is an all-time record for a SemEval task.	0
721	27940	27940	S20-12	Introduction	33	39	4.0	1.0	The remainder of this paper is organized as follows: Section 2 describes the annotation schema.	0
722	27941	27941	S20-12	Introduction	34	40	4.0	1.0	Section 3 presents the five datasets that we used in the competition.	0
723	27942	27942	S20-12	Introduction	35	41	4.0	1.0	Sections 4-9 present the results and discuss the approaches taken by the participating systems for each of the five languages.	0
724	27943	27943	S20-12	Introduction	36	42	4.0	1.0	Finally, Section 10 concludes and suggests some possible directions for future work.	0
725	28022	28022	S20-12	Subtask A	1	121	1.0	3.0	A total of 82 teams made submissions for subtask A, and the results can be seen in Table 5.	0
726	28023	28023	S20-12	Subtask A	2	122	1.0	3.0	This was the most popular subtask among all subtasks and across all languages.	0
727	28024	28024	S20-12	Subtask A	3	123	2.0	3.0	The best team UHH-LT achieved an F1 score of 0.9204 using an ensemble of ALBERT models of different sizes.	0
728	28025	28025	S20-12	Subtask A	4	124	2.0	3.0	The team ranked second was UHH-LT with an F1 score of 0.9204, and it used RoBERTa-large that was fine-tuned on the SOLID dataset in an unsupervised way, i.e., using the MLM objective.	0
729	28026	28026	S20-12	Subtask A	5	125	3.0	3.0	The third team, Galileo, achieved an F1 score of 0.9198, using an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large trained on the subtask A data for all languages.	0
730	28027	28027	S20-12	Subtask A	6	126	3.0	3.0	The top-10 teams used BERT, RoBERTa or XLM-RoBERTa, sometimes as part of ensembles that also included CNNs and LSTMs (Hochreiter and Schmidhuber, 1997).	0
731	28028	28028	S20-12	Subtask A	7	127	4.0	3.0	Overall, the competition for this subtask was very strong, and the scores are very close: the teams ranked 2-16 are within one point in the third decimal place, and those ranked 2-59 are within two absolute points in the second decimal place from the best team.	0
732	28029	28029	S20-12	Subtask A	8	128	4.0	3.0	All but one team beat the majority class baseline (we suspect that team might have accidentally flipped their predicted labels).	0
733	28112	28112	S20-12	Conclusion and Future Work	1	211	1.0	4.0	We presented the results of OffensEval-2020, which featured datasets in five languages: Arabic, Danish, English, Greek, and Turkish.	0
734	28113	28113	S20-12	Conclusion and Future Work	2	212	1.0	4.0	For English, we had three subtasks, representing the three levels of the OLID hierarchy.	0
735	28114	28114	S20-12	Conclusion and Future Work	3	213	1.0	4.0	For the other four languages, we had a subtask for the top-level of the OLID hierarchy only.	0
736	28115	28115	S20-12	Conclusion and Future Work	4	214	2.0	4.0	A total of 528 teams signed up to participate in OffensEval-2020, and 145 of them actually submitted results across all languages and subtasks.	0
737	28116	28116	S20-12	Conclusion and Future Work	5	215	2.0	4.0	Out of the 145 participating teams, 96 teams participated in one language only, 13 teams participated in two languages, 11 in three languages, 19 in four languages, and 6 teams submitted systems for all five languages.	0
738	28117	28117	S20-12	Conclusion and Future Work	6	216	2.0	4.0	The official submissions per language ranged from 37 (for Greek) to 81 (for English).	0
739	28118	28118	S20-12	Conclusion and Future Work	7	217	2.0	4.0	Finally, 70 of the 145 participating teams submitted system description papers, which is an all-time record.	0
740	28119	28119	S20-12	Conclusion and Future Work	8	218	3.0	4.0	The wide participation in the task allowed us to compare a number of approaches across different languages and datasets.	0
741	28120	28120	S20-12	Conclusion and Future Work	9	219	3.0	4.0	Similarly to OffensEval-2019, we observed that the best systems for all languages and subtasks used large-scale BERT-style pre-trained Transformers such as BERT, RoBERTa, and mBERT.	0
742	28121	28121	S20-12	Conclusion and Future Work	10	220	3.0	4.0	Unlike 2019, however, the multi-lingual nature of this year's data enabled cross-language approaches, which proved quite effective and were used by some of the top-ranked systems.	0
743	28122	28122	S20-12	Conclusion and Future Work	11	221	3.0	4.0	In future work, we plan to extend the task in several ways.	0
744	28123	28123	S20-12	Conclusion and Future Work	12	222	4.0	4.0	First, we want to offer subtasks B and C for all five languages from OffensEval-2020.	0
745	28124	28124	S20-12	Conclusion and Future Work	13	223	4.0	4.0	We further plan to add some additional languages, especially under-represented ones.	0
746	28125	28125	S20-12	Conclusion and Future Work	14	224	4.0	4.0	Other interesting aspects to explore are code-mixing, e.g., mixing Arabic script and Latin alphabet in the same Arabic message, and code-switching, e.g., mixing Arabic and English words and phrases in the same message.	0
747	28126	28126	S20-12	Conclusion and Future Work	15	225	4.0	4.0	Last but not least, we plan to cover a wider variety of social media platforms.	0
748	30743	30743	2020.nlptea-1.4	title	1	1	4.0	1.0	Overview of NLPTEA-2020 Shared Task for Chinese Grammatical Error Diagnosis	0
749	30744	30744	2020.nlptea-1.4	abstract	1	2	1.0	1.0	This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language.	1
750	30745	30745	2020.nlptea-1.4	abstract	2	3	2.0	1.0	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
751	30746	30746	2020.nlptea-1.4	abstract	3	4	3.0	1.0	Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs.	0
752	30747	30747	2020.nlptea-1.4	abstract	4	5	4.0	1.0	System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level.	0
753	30748	30748	2020.nlptea-1.4	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
754	30749	30749	2020.nlptea-1.4	Introduction	1	7	1.0	1.0	Automated grammar checking for learners of English as a foreign language has achieved obvious progress.	0
755	30750	30750	2020.nlptea-1.4	Introduction	2	8	1.0	1.0	Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011;	0
756	30751	30751	2020.nlptea-1.4	Introduction	3	9	1.0	1.0	Dale et al., 2012).	0
757	30752	30752	2020.nlptea-1.4	Introduction	4	10	1.0	1.0	The shared tasks at CoNLL 2013 and 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013;.	0
758	30753	30753	2020.nlptea-1.4	Introduction	5	11	1.0	1.0	Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language (CFL) learners.	0
759	30754	30754	2020.nlptea-1.4	Introduction	6	12	2.0	1.0	Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012;	0
760	30755	30755	2020.nlptea-1.4	Introduction	7	13	2.0	1.0	Wu et al, 2010;	0
761	30756	30756	2020.nlptea-1.4	Introduction	8	14	2.0	1.0	Yu and Chen, 2012), rule-based analysis (Lee et al., 2013), neuro network modelling (Zheng et al., 2016;	0
762	30757	30757	2020.nlptea-1.4	Introduction	9	15	2.0	1.0	Fu et al., 2018) and hybrid methods Zhou et al., 2017).	0
763	30758	30758	2020.nlptea-1.4	Introduction	10	16	2.0	1.0	In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL .	0
764	30759	30759	2020.nlptea-1.4	Introduction	11	17	2.0	1.0	A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015), COLING-2016.	0
765	30760	30760	2020.nlptea-1.4	Introduction	12	18	3.0	1.0	Its name was fixed from then on: Chinese Grammatical Error Diagnosis (CGED).	0
766	30761	30761	2020.nlptea-1.4	Introduction	13	19	3.0	1.0	As a part of IJCNLP 2017, the shared task was organized (Rao et al., 2017).	0
767	30762	30762	2020.nlptea-1.4	Introduction	14	20	3.0	1.0	In conjunction with NLP-TEA workshop in ACL 2018, CGED was organized again (Rao et al., 2018).	0
768	30763	30763	2020.nlptea-1.4	Introduction	15	21	3.0	1.0	The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results.	0
769	30764	30764	2020.nlptea-1.4	Introduction	16	22	3.0	1.0	Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task.	0
770	30765	30765	2020.nlptea-1.4	Introduction	17	23	4.0	1.0	The rest of this paper is organized as follows.	0
771	30766	30766	2020.nlptea-1.4	Introduction	18	24	4.0	1.0	Section 2 describes the task in detail.	0
772	30767	30767	2020.nlptea-1.4	Introduction	19	25	4.0	1.0	Section 3 introduces the constructed data sets.	0
773	30768	30768	2020.nlptea-1.4	Introduction	20	26	4.0	1.0	Section 4 proposes evaluation metrics.	0
774	30769	30769	2020.nlptea-1.4	Introduction	21	27	4.0	1.0	Section 5 reports the results of the participants' approaches.	0
775	30770	30770	2020.nlptea-1.4	Introduction	22	28	4.0	1.0	Conclusions are finally drawn in Section 6.	0
776	30771	30771	2020.nlptea-1.4	Task Description	1	29	1.0	1.0	The goal of this shared task is to develop NLP techniques to automatically diagnose (and furtherly correct) grammatical errors in Chinese sentences written by CFL learners.	0
777	30772	30772	2020.nlptea-1.4	Task Description	2	30	1.0	1.0	"Such errors are defined as PADS: redundant words (denoted as a capital ""R""), missing words (""M""), word selection errors (""S""), and word ordering errors (""W"")."	0
778	30773	30773	2020.nlptea-1.4	Task Description	3	31	2.0	2.0	The input sentence may contain one or more such errors.	0
779	30774	30774	2020.nlptea-1.4	Task Description	4	32	2.0	2.0	The developed system should indicate which error types are embedded in the given unit (containing 1 to 5 sentences) and the position at which they occur.	0
780	30775	30775	2020.nlptea-1.4	Task Description	5	33	2.0	2.0	"Each input unit is given a unique number ""sid""."	0
781	30776	30776	2020.nlptea-1.4	Task Description	6	34	3.0	2.0	"If the inputs contain no grammatical errors, the system should return: ""sid, correct""."	0
782	30777	30777	2020.nlptea-1.4	Task Description	7	35	3.0	2.0	"If an input unit contains the grammatical errors, the output format should include four items ""sid, start_off, end_off, error_type"", where start_off and end_off respectively denote the positions of starting and ending character at which the grammatical error occurs, and error_type should be one of the defined errors: ""R"", ""M"", ""S"", and ""W""."	0
783	30778	30778	2020.nlptea-1.4	Task Description	8	36	4.0	2.0	Each character or punctuation mark occupies 1 space for counting positions.	0
784	30779	30779	2020.nlptea-1.4	Task Description	9	37	4.0	2.0	Example sentences and corresponding notes are shown as Table 1 shows.	0
785	30780	30780	2020.nlptea-1.4	Task Description	10	38	4.0	2.0	This year, we only have one track of HSK.	0
786	30860	30860	2020.nlptea-1.4	Conclusion	1	118	1.0	4.0	This study describes the NLP-TEA 2020 shared task for Chinese grammatical error diagnosis, including task design, data preparation, performance metrics, and evaluation results.	0
787	30861	30861	2020.nlptea-1.4	Conclusion	2	119	2.0	4.0	Regardless of actual performance, all submissions contribute to the common effort to develop Chinese grammatical error diagnosis system, and the individual reports in the proceedings provide useful insights into computer-assisted language learning for CFL learners.	0
788	30862	30862	2020.nlptea-1.4	Conclusion	3	120	3.0	4.0	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
789	30863	30863	2020.nlptea-1.4	Conclusion	4	121	4.0	4.0	Therefore, all data sets with gold standards and scoring scripts are publicly available online at http://www.cged.science.	0
790	31208	31208	W18-3601	title	1	1	4.0	1.0	The First Multilingual Surface Realisation Shared Task (SR&apos;18): Overview and Evaluation Results	0
791	31209	31209	W18-3601	abstract	1	2	1.0	1.0	We report results from the SR'18 Shared Task, a new multilingual surface realisation task organised as part of the ACL'18 Workshop on Multilingual Surface Realisation.	0
792	31210	31210	W18-3601	abstract	2	3	2.0	1.0	As in its English-only predecessor task SR'11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed.	0
793	31211	31211	W18-3601	abstract	3	4	2.0	1.0	The shallow track was offered in ten, and the deep track in three languages.	0
794	31212	31212	W18-3601	abstract	4	5	3.0	1.0	Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity.	0
795	31213	31213	W18-3601	abstract	5	6	4.0	1.0	This report presents the evaluation results, along with descriptions of the SR'18 tracks, data and evaluation methods.	0
796	31214	31214	W18-3601	abstract	6	7	4.0	1.0	For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.	0
797	31390	31390	W18-3601	Conclusion	1	183	1.0	4.0	SR'18 was the second surface realisation shared task, and followed an earlier pilot task for English, SR'11.	0
798	31391	31391	W18-3601	Conclusion	2	184	2.0	4.0	Participation was high for a first instance   of a shared task, at least in the Shallow Track, indicating that interest is high enough to continue running it again next year to enable more teams to participate.	0
799	31392	31392	W18-3601	Conclusion	3	185	2.0	4.0	One important question that needs to be addressed is to what extent UDs are suitable inputs for NLG systems.	0
800	31393	31393	W18-3601	Conclusion	4	186	3.0	4.0	More specifically, can they reasonably be expected to be generated by other, content-determining, modules in an NLG system, do they provide all the information necessary to generate surface realisations, and if not, how can they be augmented to provide it.	0
801	31394	31394	W18-3601	Conclusion	5	187	3.0	4.0	We hope to discuss these and related issues with the research community as we prepare the next instance of the SR Task.	0
802	31395	31395	W18-3601	Conclusion	6	188	4.0	4.0	A goal to aim for may be to make it possible for different NLG components to be connected via standard interface representations, to increase re-usability for NLG components.	0
803	31396	31396	W18-3601	Conclusion	7	189	4.0	4.0	However, what may constitute a good interface representation for surface realisation remains far from clear.	0
804	31515	31515	W19-3203	title	1	1	2.0	1.0	Overview of the Fourth Social Media Mining for Health (#SMM4H)	0
805	31516	31516	W19-3203	title	2	2	4.0	1.0	Shared Task at ACL 2019	0
806	31517	31517	W19-3203	abstract	1	3	1.0	1.0	The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking on a regular basis 1 . Advances in automated data processing and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media.	0
807	31518	31518	W19-3203	abstract	2	4	1.0	1.0	We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users.	0
808	31519	31519	W19-3203	abstract	3	5	2.0	1.0	For the fourth execution of this challenge, we proposed four different tasks.	0
809	31520	31520	W19-3203	abstract	4	6	2.0	1.0	Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not.	1
810	31521	31521	W19-3203	abstract	5	7	3.0	1.0	Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs.	1
811	31522	31522	W19-3203	abstract	6	8	3.0	1.0	Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary.	1
812	31523	31523	W19-3203	abstract	7	9	4.0	1.0	Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one's health, a more general discussion of the health issue, or is an unrelated mention.	1
813	31524	31524	W19-3203	abstract	8	10	4.0	1.0	A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run.	0
814	31525	31525	W19-3203	abstract	9	11	4.0	1.0	We summarize here the corpora for this challenge which are freely available at https://competitions.codalab. org/competitions/22521, and present an overview of the methods and the results of the competing systems.	0
815	31526	31526	W19-3203	Introduction	1	12	1.0	1.0	The intent of the #SMM4H shared tasks series is to challenge the community with Natural Language Processing tasks for mining relevant data for health monitoring and surveillance in social media.	0
816	31527	31527	W19-3203	Introduction	2	13	1.0	1.0	Such challenges require processing imbalanced, noisy, real-world, and substantially creative language expressions from social media.	0
817	31528	31528	W19-3203	Introduction	3	14	1.0	1.0	The competing systems should be able to deal with many linguistic variations and semantic complexities in the various ways people express medication-related concepts and outcomes.	0
818	31529	31529	W19-3203	Introduction	4	15	1.0	1.0	It has been shown in past research (Liu et al., 2011;	0
819	31530	31530	W19-3203	Introduction	5	16	1.0	1.0	Giuseppe et al., 2017) that automated systems frequently under-perform when exposed to social media text because of the presence of novel/creative phrases, misspellings and frequent use of idiomatic, ambiguous and sarcastic expressions.	0
820	31531	31531	W19-3203	Introduction	6	17	1.0	1.0	The tasks act as a discovery and verification process of what approaches work best for social media data.	0
821	31532	31532	W19-3203	Introduction	7	18	2.0	1.0	As in previous years, our tasks focused on mining health information from Twitter.	0
822	31533	31533	W19-3203	Introduction	8	19	2.0	1.0	This year we challenged the community with two different problems.	0
823	31534	31534	W19-3203	Introduction	9	20	2.0	1.0	The first problem focuses on performing pharmacovigilance from social media data.	0
824	31535	31535	W19-3203	Introduction	10	21	2.0	1.0	It is now well understood that social media data may contain reports of adverse drug reactions (ADRs) and these reports may complement traditional adverse event reporting systems, such as the FDA adverse event reporting system (FAERS).	0
825	31536	31536	W19-3203	Introduction	11	22	2.0	1.0	However, automatically curating reports from adverse reactions from Twitter requires the application of a series of NLP methods in an end-to-end pipeline .	0
826	31537	31537	W19-3203	Introduction	12	23	2.0	1.0	The first three tasks of this year's challenge represent three key NLP problems in a social media based pharmacovigilance pipeline -(i) automatic classification of ADRs, (ii) extraction of spans of ADRs and (iii) normal-ization of the extracted ADRs to standardized IDs.	0
827	31538	31538	W19-3203	Introduction	13	24	2.0	1.0	The second problem explores the generalizability of predictive models.	0
828	31539	31539	W19-3203	Introduction	14	25	3.0	1.0	In health research using social media, it is often necessary for researchers to build individual classifiers to identify health mentions of a particular disease in a particular context.	0
829	31540	31540	W19-3203	Introduction	15	26	3.0	1.0	Classification models that can generalize to different health contexts would be greatly beneficial to researchers in these fields (e.g., (Payam and Eugene, 2018)), as this would allow researchers to more easily apply existing tools and resources to new problems.	0
830	31541	31541	W19-3203	Introduction	16	27	3.0	1.0	Motivated by these ideas, Task 4 was testing tweet classification methods across diverse health contexts, so the test data included a very different health context than the training data.	0
831	31542	31542	W19-3203	Introduction	17	28	3.0	1.0	This setting measures the ability of tweet classifiers to generalize across health contexts.	0
832	31543	31543	W19-3203	Introduction	18	29	3.0	1.0	The fourth iteration of our series follows the same organization as previous iterations.	0
833	31544	31544	W19-3203	Introduction	19	30	3.0	1.0	We collected posts from Twitter, annotated the data for the four tasks proposed and released the posts to the registered teams.	0
834	31545	31545	W19-3203	Introduction	20	31	3.0	1.0	This year, we conducted the evaluation of all participating systems using Codalab, an open source platform facilitating data science competitions.	0
835	31546	31546	W19-3203	Introduction	21	32	4.0	1.0	The performances of the systems were compared on a blind evaluations sets for each task.	0
836	31547	31547	W19-3203	Introduction	22	33	4.0	2.0	All teams registered were allowed to participate to one or multiple tasks.	0
837	31548	31548	W19-3203	Introduction	23	34	4.0	2.0	We provided the participants with two sets of data for each task, a training and a test set.	0
838	31549	31549	W19-3203	Introduction	24	35	4.0	2.0	Participants had a period of six weeks, from March 5 th to April 15 th , for training their systems on our training sets, and 4 days, from the 16 th to 20 th of April, for calibrating their systems on our test sets and submitting their predictions.	0
839	31550	31550	W19-3203	Introduction	25	36	4.0	2.0	In total 34 teams registered and 19 teams submitted at least one run (each team was allowed to submit, at most, three runs per task).	0
840	31551	31551	W19-3203	Introduction	26	37	4.0	2.0	In detail, we received 43 runs for task 1, 24 for task 2, 10 for task 3 and 15 for task 4.	0
841	31552	31552	W19-3203	Introduction	27	38	4.0	2.0	We briefly describe each task and their data in section 2, before discussing the results obtained in section 3.	0
842	31639	31639	W19-3203	Conclusion	1	125	1.0	4.0	In this paper we presented an overview of the results of #SMM4H 2019 which focuses on a) the resolution of adverse drug reaction (ADR) mentioned in Twitter and b) the distinction between tweets reporting personal health status form opinions across different health domains.	0
843	31640	31640	W19-3203	Conclusion	2	126	2.0	4.0	With a total of 92 runs submitted by 19 teams, the challenge was well attended.	0
844	31641	31641	W19-3203	Conclusion	3	127	2.0	4.0	The participants, in large part, opted for neural architectures and integrated pretrained word-embedding sensitive to their contexts based on the recent Bidirectional Encoder Representations from Transformers.	0
845	31642	31642	W19-3203	Conclusion	4	128	3.0	4.0	Such architectures were the most efficient on our four tasks.	0
846	31643	31643	W19-3203	Conclusion	5	129	4.0	4.0	Results on tasks 1-3 show that, despite a continuous improvement of performances in the detection of tweets mentioning ADRs over the past years, their end-to-end resolution still remain a major challenge for the community and an opportunity for further research.	0
847	31644	31644	W19-3203	Conclusion	6	130	4.0	4.0	Results of task 4 were more encouraging, with systems able to generalized their predictions over domains not present in their training data.	0
848	32732	32732	2020.sigmorphon-1.1	title	1	1	4.0	1.0	SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection	0
849	32733	32733	2020.sigmorphon-1.1	abstract	1	2	1.0	1.0	A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language.	0
850	32734	32734	2020.sigmorphon-1.1	abstract	2	3	1.0	1.0	Most systems, however, are developed using data from just one language such as English.	0
851	32735	32735	2020.sigmorphon-1.1	abstract	3	4	2.0	1.0	The SIG-MORPHON 2020 shared task on morphological reinflection aims to investigate systems' ability to generalize across typologically distinct languages, many of which are low resource.	0
852	32736	32736	2020.sigmorphon-1.1	abstract	4	5	2.0	1.0	Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages.	0
853	32737	32737	2020.sigmorphon-1.1	abstract	5	6	3.0	1.0	A total of 22 systems (19 neural) from 10 teams were submitted to the task.	0
854	32738	32738	2020.sigmorphon-1.1	abstract	6	7	3.0	1.0	All four winning systems were neural (two monolingual transformers and two massively multilingual RNNbased models with gated attention).	0
855	32739	32739	2020.sigmorphon-1.1	abstract	7	8	4.0	1.0	Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages.	0
856	32740	32740	2020.sigmorphon-1.1	abstract	8	9	4.0	1.0	Nonneural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data.	0
857	32741	32741	2020.sigmorphon-1.1	abstract	9	10	4.0	1.0	Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.	0
858	32742	32742	2020.sigmorphon-1.1	Introduction	1	11	1.0	1.0	Human language is marked by considerable diversity around the world.	0
859	32743	32743	2020.sigmorphon-1.1	Introduction	2	12	1.0	1.0	Though the world's languages share many basic attributes (e.g., Swadesh, 1950 and more recently, List et al., 2016), grammatical features, and even abstract implications (proposed in Greenberg, 1963), each language nevertheless has a unique evolutionary trajectory that is affected by geographic, social, cultural, and other factors.	0
860	32744	32744	2020.sigmorphon-1.1	Introduction	3	13	1.0	1.0	As a result, the surface form of languages varies substantially.	0
861	32745	32745	2020.sigmorphon-1.1	Introduction	4	14	1.0	1.0	The morphology of languages can differ in many ways:	0
862	32746	32746	2020.sigmorphon-1.1	Introduction	5	15	1.0	1.0	"Some exhibit rich grammatical case systems (e.g., 12 in Erzya and 24 in Veps) and mark possessiveness, others might have complex verbal morphology (e.g., Oto-Manguean languages; Palancar and L√©onard, 2016) or even ""decline"" nouns for tense (e.g., Tupi-Guarani languages)."	0
863	32747	32747	2020.sigmorphon-1.1	Introduction	6	16	1.0	1.0	Linguistic typology is the discipline that studies these variations by means of a systematic comparison of languages (Croft, 2002;	0
864	32748	32748	2020.sigmorphon-1.1	Introduction	7	17	1.0	1.0	Comrie, 1989).	0
865	32749	32749	2020.sigmorphon-1.1	Introduction	8	18	1.0	1.0	Typologists have defined several dimensions of morphological variation to classify and quantify the degree of crosslinguistic variation.	0
866	32750	32750	2020.sigmorphon-1.1	Introduction	9	19	2.0	1.0	This comparison can be challenging as the categories are based on studies of known languages and are progressively refined with documentation of new languages (Haspelmath, 2007).	0
867	32751	32751	2020.sigmorphon-1.1	Introduction	10	20	2.0	1.0	Nevertheless, to understand the potential range of morphological variation, we take a closer look at three dimensions here: fusion, inflectional synthesis, and position of case affixes (Dryer and Haspelmath, 2013).	0
868	32752	32752	2020.sigmorphon-1.1	Introduction	11	21	2.0	1.0	Fusion, our first dimension of variation, refers to the degree to which morphemes bind to one another in a phonological word (Bickel and Nichols, 2013b).	0
869	32753	32753	2020.sigmorphon-1.1	Introduction	12	22	2.0	1.0	Languages range from strictly isolating (i.e., each morpheme is its own phonological word) to concatenative (i.e., morphemes bind together within a phonological word); nonlinearities such as ablaut or tonal morphology can also be present.	0
870	32754	32754	2020.sigmorphon-1.1	Introduction	13	23	2.0	1.0	From a geographic perspective, isolating languages are found in the Sahel Belt in West Africa, Southeast Asia and the Pacific.	0
871	32755	32755	2020.sigmorphon-1.1	Introduction	14	24	2.0	1.0	Ablaut-concatenative morphology and tonal morphology can be found in African languages.	0
872	32756	32756	2020.sigmorphon-1.1	Introduction	15	25	2.0	1.0	Tonal-concatenative morphology can be found in Mesoamerican languages (e.g., Oto-Manguean).	0
873	32757	32757	2020.sigmorphon-1.1	Introduction	16	26	2.0	1.0	Concatenative morphology is the most common system and can be found around the world.	0
874	32758	32758	2020.sigmorphon-1.1	Introduction	17	27	3.0	1.0	Inflectional synthesis, the second dimension considered, refers to whether grammatical categories like tense, voice or agreement are expressed as affixes (synthetic) or individual words (analytic) (Bickel and Nichols, 2013c).	0
875	32759	32759	2020.sigmorphon-1.1	Introduction	18	28	3.0	1.0	Analytic expressions are common in Eurasia (except the Pacific Rim, and the Himalaya and Caucasus mountain ranges), whereas synthetic expressions are used to a high degree in the Americas.	0
876	32760	32760	2020.sigmorphon-1.1	Introduction	19	29	3.0	1.0	Finally, affixes can variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013).	0
877	32761	32761	2020.sigmorphon-1.1	Introduction	20	30	3.0	1.0	Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013).	0
878	32762	32762	2020.sigmorphon-1.1	Introduction	21	31	3.0	1.0	In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead.	0
879	32763	32763	2020.sigmorphon-1.1	Introduction	22	32	3.0	1.0	These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable.	0
880	32764	32764	2020.sigmorphon-1.1	Introduction	23	33	3.0	1.0	Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging.	0
881	32765	32765	2020.sigmorphon-1.1	Introduction	24	34	3.0	1.0	As Bender (2009	0
882	32766	32766	2020.sigmorphon-1.1	Introduction	25	35	4.0	1.0	Bender ( , 2016 notes, many current architectures and training and tuning algorithms still present language-specific biases.	0
883	32767	32767	2020.sigmorphon-1.1	Introduction	26	36	4.0	1.0	The most commonly used language for developing NLP applications is English.	0
884	32768	32768	2020.sigmorphon-1.1	Introduction	27	37	4.0	1.0	Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology.	0
885	32769	32769	2020.sigmorphon-1.1	Introduction	28	38	4.0	1.0	With respect to languages that exhibit inflectional morphology, English is relatively impoverished.	0
886	32770	32770	2020.sigmorphon-1.1	Introduction	29	39	4.0	1.0	1 Importantly, English is just one morphological system among many.	0
887	32771	32771	2020.sigmorphon-1.1	Introduction	30	40	4.0	1.0	A larger goal of natural language processing is that the system work for any presented language.	0
888	32772	32772	2020.sigmorphon-1.1	Introduction	31	41	4.0	1.0	If an NLP system is trained on just one language, it could be missing important flexibility in its ability to account for cross-linguistic morphological variation.	0
889	32773	32773	2020.sigmorphon-1.1	Introduction	32	42	4.0	1.0	In this year's iteration of the SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems' ability to generalize across typologically distinct languages many of which are low-resource.	0
890	32774	32774	2020.sigmorphon-1.1	Introduction	33	43	4.0	1.0	"For example, if a neural network architecture works well for a sample of Indo-European languages, should the same architecture also work well for Tupi-Guarani languages (where nouns are ""declined"" for tense) or Austronesian languages (where verbal morphology is frequently prefixing)?"	0
891	32775	32775	2020.sigmorphon-1.1	Task Description	1	44	1.0	1.0	The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form.	1
892	32776	32776	2020.sigmorphon-1.1	Task Description	2	45	1.0	1.0	For each language we provide a separate training, development, and test set.	0
893	32777	32777	2020.sigmorphon-1.1	Task Description	3	46	1.0	1.0	"More historically, all of these tasks resemble the classic ""wug""-test that Berko (1958) developed to test child and human knowledge of English nominal morphology."	0
894	32778	32778	2020.sigmorphon-1.1	Task Description	4	47	2.0	1.0	Unlike the task from earlier years, this year's task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces previously unseen data.	0
895	32779	32779	2020.sigmorphon-1.1	Task Description	5	48	2.0	1.0	The task starts with the Development Phase, which was an elongated period of time (about two months), during which participants develop a model of morphological inflection.	0
896	32780	32780	2020.sigmorphon-1.1	Task Description	6	49	2.0	1.0	In this phase, we provide training and development splits for 45 languages representing the Austronesian, Niger-Congo, Oto-Manguean, Uralic and Indo-European language families.	0
897	32781	32781	2020.sigmorphon-1.1	Task Description	7	50	2.0	1.0	Table 1 provides details on the languages.	0
898	32782	32782	2020.sigmorphon-1.1	Task Description	8	51	3.0	1.0	The Generalization Phase is a short period of time (it started about a week before the Evaluation Phase) during which participants fine-tune their models on new data.	0
899	32783	32783	2020.sigmorphon-1.1	Task Description	9	52	3.0	1.0	At the start of the phase, we provide training and development splits for 45 new languages where approximately half are genetically related (belong to the same family) and half are genetically unrelated (are isolates or belong to a different family) to the languages presented in the Development Phase.	0
900	32784	32784	2020.sigmorphon-1.1	Task Description	10	53	3.0	1.0	More specifically, we introduce (surprise) languages from Afro-Asiatic, Algic, Dravidian, Indo-European, Niger-Congo, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan families.	0
901	32785	32785	2020.sigmorphon-1.1	Task Description	11	54	3.0	1.0	See Table 2 for more details.	0
902	32786	32786	2020.sigmorphon-1.1	Task Description	12	55	4.0	1.0	Finally, test splits for all 90 languages are released in the Evaluation Phase.	0
903	32787	32787	2020.sigmorphon-1.1	Task Description	13	56	4.0	1.0	During this phase, the models are evaluated on held-out forms.	0
904	32788	32788	2020.sigmorphon-1.1	Task Description	14	57	4.0	1.0	Importantly, the languages from both previous phases are evaluated simultaneously.	0
905	32789	32789	2020.sigmorphon-1.1	Task Description	15	58	4.0	1.0	This way, we evaluate the extent to which models (especially those with shared parameters) overfit to the development data: a model based on the morphological patterning of the Indo-European languages may end up with a bias towards suffixing and will struggle to learn prefixing or infixation.	0
906	33088	33088	2020.sigmorphon-1.1	Conclusion	1	357	1.0	4.0	This years's shared task on morphological reinflection focused on building models that could generalize across an extremely typologically diverse set of languages, many from understudied language families and with limited available text resources.	0
907	33089	33089	2020.sigmorphon-1.1	Conclusion	2	358	1.0	4.0	As in previous years, neural models performed well, even in relatively low-resource cases.	0
908	33090	33090	2020.sigmorphon-1.1	Conclusion	3	359	2.0	4.0	Submissions were able to make productive use of multilingual training to take advantage of commonalities across languages in the dataset.	0
909	33091	33091	2020.sigmorphon-1.1	Conclusion	4	360	2.0	4.0	Data augmentation techniques such as hallucination helped fill in the gaps and allowed networks to generalize to unseen inputs.	0
910	33092	33092	2020.sigmorphon-1.1	Conclusion	5	361	2.0	4.0	These techniques, combined with architecture tweaks like sparsemax, resulted in excellent overall performance on many languages (over 90% accuracy on average).	0
911	33093	33093	2020.sigmorphon-1.1	Conclusion	6	362	3.0	4.0	However, the task's focus on typological diversity revealed that some morphology types and language families (Tungusic, Oto-Manguean, South-ern Daly) remain a challenge for even the best systems.	0
912	33094	33094	2020.sigmorphon-1.1	Conclusion	7	363	3.0	4.0	These families are extremely low-resource, represented in this dataset by few or a single language.	0
913	33095	33095	2020.sigmorphon-1.1	Conclusion	8	364	3.0	4.0	This makes cross-linguistic transfer of similarities by multilanguage training less viable.	0
914	33096	33096	2020.sigmorphon-1.1	Conclusion	9	365	4.0	4.0	They may also have morphological properties and rules (e.g., Evenki is agglutinating with many possible forms for each lemma) that are particularly difficult for machine learners to induce automatically from sparse data.	0
915	33097	33097	2020.sigmorphon-1.1	Conclusion	10	366	4.0	4.0	For some languages (Ingrian, Tajik, Tagalog, Zarma, and Lingala), optimal performance was only achieved in this shared task by hand-encoding linguist knowledge in finite state grammars.	0
916	33098	33098	2020.sigmorphon-1.1	Conclusion	11	367	4.0	4.0	It is up to future research to imbue models with the right kinds of linguistic inductive biases to overcome these challenges.	0
917	38129	38129	K15-2001	title	1	1	4.0	1.0	The CoNLL-2015 Shared Task on Shallow Discourse Parsing	0
918	38130	38130	K15-2001	abstract	1	2	1.0	1.0	The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text.	0
919	38131	38131	K15-2001	abstract	2	3	1.0	1.0	A discourse relation can be expressed explicitly or implicitly, and takes two arguments realized as sentences, clauses, or in some rare cases, phrases.	0
920	38132	38132	K15-2001	abstract	3	4	2.0	1.0	Sixteen teams from three continents participated in this task.	0
921	38133	38133	K15-2001	abstract	4	5	2.0	1.0	For the first time in the history of the CoNLL shared tasks, participating teams, instead of running their systems on the test set and submitting the output, were asked to deploy their systems on a remote virtual machine and use a web-based evaluation platform to run their systems on the test set.	0
922	38134	38134	K15-2001	abstract	5	6	3.0	1.0	This meant they were unable to actually see the data set, thus preserving its integrity and ensuring its replicability.	0
923	38135	38135	K15-2001	abstract	6	7	3.0	1.0	In this paper, we present the task definition, the training and test sets, and the evaluation protocol and metric used during this shared task.	0
924	38136	38136	K15-2001	abstract	7	8	4.0	1.0	We also summarize the different approaches adopted by the participating teams, and present the evaluation results.	0
925	38137	38137	K15-2001	abstract	8	9	4.0	1.0	The evaluation data sets and the scorer will serve as a benchmark for future research on shallow discourse parsing.	0
926	38138	38138	K15-2001	Introduction	1	10	1.0	1.0	The shared task for the Nineteenth Conference on Computational Natural Language Learning (CoNLL-2015) is on Shallow Discourse Parsing (SDP).	0
927	38139	38139	K15-2001	Introduction	2	11	1.0	1.0	In the course of the sixteen CoNLL shared tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011;Pradhan et al., 2012).	0
928	38140	38140	K15-2001	Introduction	3	12	1.0	1.0	The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012).	0
929	38141	38141	K15-2001	Introduction	4	13	1.0	1.0	Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text.	0
930	38142	38142	K15-2001	Introduction	5	14	1.0	1.0	Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012;	0
931	38143	38143	K15-2001	Introduction	6	15	1.0	1.0	Webber et al., 2012;Prasad and Bunt, 2015).	0
932	38144	38144	K15-2001	Introduction	7	16	1.0	1.0	For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008;	0
933	38145	38145	K15-2001	Introduction	8	17	1.0	1.0	Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them.	0
934	38146	38146	K15-2001	Introduction	9	18	1.0	1.0	For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.	0
935	38147	38147	K15-2001	Introduction	10	19	2.0	1.0	1	0
936	38148	38148	K15-2001	Introduction	11	20	2.0	1.0	The necessary conditions are also in place for such a task.	0
937	38149	38149	K15-2001	Introduction	12	21	2.0	1.0	The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008;	0
938	38150	38150	K15-2001	Introduction	13	22	2.0	1.0	Duverle and Prendinger, 2009;	0
939	38151	38151	K15-2001	Introduction	14	23	2.0	1.0	Lin et al., 2009;	0
940	38152	38152	K15-2001	Introduction	15	24	2.0	1.0	Pitler et al., 2009;	0
941	38153	38153	K15-2001	Introduction	16	25	2.0	1.0	Subba and Di Eugenio, 2009;	0
942	38154	38154	K15-2001	Introduction	17	26	2.0	1.0	Zhou et al., 2010;	0
943	38155	38155	K15-2001	Introduction	18	27	2.0	1.0	Feng and Hirst, 2012;	0
944	38156	38156	K15-2001	Introduction	19	28	3.0	1.0	Ghosh et al., 2012;	0
945	38157	38157	K15-2001	Introduction	20	29	3.0	1.0	Park and Cardie, 2012;	0
946	38158	38158	K15-2001	Introduction	21	30	3.0	1.0	Wang et al., 2012;Biran and McKeown, 2013;	0
947	38159	38159	K15-2001	Introduction	22	31	3.0	1.0	Lan et al., 2013;	0
948	38160	38160	K15-2001	Introduction	23	32	3.0	1.0	Feng and Hirst, 2014;	0
949	38161	38161	K15-2001	Introduction	24	33	3.0	1.0	Ji and Eisenstein, 2014;	0
950	38162	38162	K15-2001	Introduction	25	34	3.0	1.0	Li and Nenkova, 2014;	0
951	38163	38163	K15-2001	Introduction	26	35	3.0	1.0	Lin et al., 2014;	0
952	38164	38164	K15-2001	Introduction	27	36	3.0	1.0	Rutherford and Xue, 2014), and the momentum is building.	0
953	38165	38165	K15-2001	Introduction	28	37	4.0	1.0	Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference.	0
954	38166	38166	K15-2001	Introduction	29	38	4.0	1.0	The resurgence of deep learning techniques opens the door for innovative approaches to this problem.	0
955	38167	38167	K15-2001	Introduction	30	39	4.0	1.0	"A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of ""standard"" feature-based learning techniques and ""deep"" representation learning techniques."	0
956	38168	38168	K15-2001	Introduction	31	40	4.0	1.0	The rest of this overview paper is structured as follows.	0
957	38169	38169	K15-2001	Introduction	32	41	4.0	1.0	In Section 2, we provide a concise definition of the shared task.	0
958	38170	38170	K15-2001	Introduction	33	42	4.0	1.0	We describe how the training and test data are prepared in Section 3.	0
959	38171	38171	K15-2001	Introduction	34	43	4.0	1.0	In Section 4, we present the evaluation protocol, metric and scorer.	0
960	38172	38172	K15-2001	Introduction	35	44	4.0	1.0	The different approaches that participants took in the shared task are summarized in Section 5.	0
961	38173	38173	K15-2001	Introduction	36	45	4.0	1.0	In Section 6, we present the ranking of participating systems and analyze the evaluation results.	0
962	38174	38174	K15-2001	Introduction	37	46	4.0	1.0	We present our conclusions in Section 7.	0
963	38175	38175	K15-2001	Task Definition	1	47	1.0	1.0	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
964	38176	38176	K15-2001	Task Definition	2	48	1.0	1.0	Specifically, given a newswire article as input, a participating system is asked to return a set of discourse relations contained in the text.	1
965	38177	38177	K15-2001	Task Definition	3	49	2.0	1.0	A discourse relation, as defined in the PDTB, from which the training data for the shared task is drawn, is a relation taking two abstract objects (events, states, facts, or propositions) as arguments.	1
966	38178	38178	K15-2001	Task Definition	4	50	2.0	1.0	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between abstract object units.	0
967	38179	38179	K15-2001	Task Definition	5	51	2.0	1.0	In the current version of the PDTB, non-explicit relations are inferred only between adjacent units.	0
968	38180	38180	K15-2001	Task Definition	6	52	3.0	1.0	Each discourse relation is labeled with a sense selected from a sense hierarchy, and its arguments are generally in the form of sentences, clauses, or in some rare cases, noun phrases.	0
969	38181	38181	K15-2001	Task Definition	7	53	3.0	1.0	To detect a discourse relation, a participating system needs to:	0
970	38182	38182	K15-2001	Task Definition	8	54	4.0	1.0	1. Identify the text span of an explicit discourse connective, if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments;	0
971	38183	38183	K15-2001	Task Definition	9	55	4.0	1.0	"4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
972	38184	38184	K15-2001	Task Definition	10	56	4.0	1.0	3 Data	0
973	38419	38419	K15-2001	Conclusions	1	291	1.0	4.0	Sixteen teams from three continents participated in the CoNLL-2015 Shared Task on shallow dis-	0
974	38420	38420	K15-2001	Conclusions	2	292	1.0	4.0	The rows are sorted by the parser performance of the participating systems on the Explicit task.	0
975	38421	38421	K15-2001	Conclusions	3	293	2.0	4.0	The Column O, E, I refer to official, Explicit and Non-Explicit task ranks respectively.	0
976	38422	38422	K15-2001	Conclusions	4	294	2.0	4.0	The blue highlighted rows indicate participants that did not attempt the Non-Explicit relation subtask.	0
977	38423	38423	K15-2001	Conclusions	5	295	2.0	4.0	The green highlighted row shows a team that probably overfitted the development set.	0
978	38424	38424	K15-2001	Conclusions	6	296	3.0	4.0	Finally, the red highlighted row indicates a team that possibly focused on the Explicit relations task and even though their overall rank was lower, they did very well on the Explicit relations subtask.	0
979	38425	38425	K15-2001	Conclusions	7	297	3.0	4.0	This is also the system that did not submit a paper, so we do not know more details.	0
980	38426	38426	K15-2001	Conclusions	8	298	3.0	4.0	course parsing.	0
981	38427	38427	K15-2001	Conclusions	9	299	4.0	4.0	The shared task required the development of an end-to-end system, and the best system achieved an F1 score of 24.0% on the blind test set, reflecting the serious error propagation problem in such a system.	0
982	38428	38428	K15-2001	Conclusions	10	300	4.0	4.0	The shared task exposed the most challenging aspect of shallow discourse parsing as a research problem, helping future research better calibrate their efforts.	0
983	38429	38429	K15-2001	Conclusions	11	301	4.0	4.0	The evaluation data sets and the scorer we prepared for the shared task will be a useful benchmark for future research on shallow discourse parsing.	0
984	42557	42557	D19-5719	title	1	1	4.0	1.0	Bacteria Biotope at BioNLP Open Shared Tasks 2019	0
985	42558	42558	D19-5719	abstract	1	2	1.0	1.0	This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019.	0
986	42559	42559	D19-5719	abstract	2	3	2.0	1.0	The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology).	1
987	42560	42560	D19-5719	abstract	3	4	3.0	1.0	The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology.	0
988	42561	42561	D19-5719	abstract	4	5	4.0	1.0	The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization.	0
989	42562	42562	D19-5719	abstract	5	6	4.0	1.0	We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.	0
990	42563	42563	D19-5719	Introduction	1	7	1.0	1.0	In this paper, we present the fourth edition 1 of the Bacteria Biotope (BB) task.	0
991	42564	42564	D19-5719	Introduction	2	8	1.0	1.0	The task was introduced in 2011.	0
992	42565	42565	D19-5719	Introduction	3	9	1.0	1.0	It has the ambition of promoting large-scale information extraction (IE) from scientific documents in order to automatically fill knowledge bases in the microbial diversity field (Bossy et al., 2012).	0
993	42566	42566	D19-5719	Introduction	4	10	1.0	1.0	BB 2019 is part of BioNLP Open Shared Tasks 2019 2 . BioNLP-OST is a community-wide effort for the comparison and evaluation of biomedical text mining technologies on manually curated benchmarks.	0
994	42567	42567	D19-5719	Introduction	5	11	2.0	1.0	A large amount of information about microbes and their properties that is critical for microbiology research and development is scattered among millions of publications and databases .	0
995	42568	42568	D19-5719	Introduction	6	12	2.0	1.0	Information extraction as framed by the Bacteria Biotope task identifies relevant entities and interrelationships in the text and map them to reference categories from existing knowledge resources.	0
996	42569	42569	D19-5719	Introduction	7	13	2.0	1.0	This information can thus be combined with information from other sources referring to the same knowledge resources.	0
997	42570	42570	D19-5719	Introduction	8	14	2.0	1.0	The knowledge resources used in the BB task are the NCBI taxonomy 3  (Federhen, 2011) for microbial taxa and the OntoBiotope ontology 4 (N√©dellec et al., 2018) for microbial habitats and phenotypes.	0
998	42571	42571	D19-5719	Introduction	9	15	3.0	1.0	The large size of these resources relative to the small number of training examples reflects the real conditions of IE application development, whilst it challenges current IE methods.	0
999	42572	42572	D19-5719	Introduction	10	16	3.0	1.0	The lexical richness of the two resources partially offsets the difficulty.	0
1000	42573	42573	D19-5719	Introduction	11	17	3.0	1.0	Compared to the 2016 corpus that contained only scientific paper abstracts from the PubMed database (Del√©ger et al., 2016), the 2019 corpus is enriched with extracts from full-text articles.	0
1001	42574	42574	D19-5719	Introduction	12	18	3.0	1.0	We introduced a new entity type (phenotype) and a new relation type (linking microorganisms and phenotypes).	0
1002	42575	42575	D19-5719	Introduction	13	19	4.0	1.0	Phenotypes are observable characteristics such as morphology, or environment requirement (e.g. acidity, oxygen).	0
1003	42576	42576	D19-5719	Introduction	14	20	4.0	1.0	It is very valuable information for studying the ability of a given microbe to adapt to an environment (Brbiƒá et al., 2016).	0
1004	42577	42577	D19-5719	Introduction	15	21	4.0	1.0	The definition of microorganism phenotype in the OntoBiotope ontology includes host interaction characteristics (e.g. symbiont) and community behavior and growth habit (e.g. epilithic).	0
1005	42578	42578	D19-5719	Introduction	16	22	4.0	1.0	The task organization and the evaluation metrics remain unchanged.	0
1006	42579	42579	D19-5719	Task Description	1	23	1.0	1.0	The representation scheme of the Bacteria Biotope task contains four entity types:	0
1007	42580	42580	D19-5719	Task Description	2	24	1.0	1.0	‚Ä¢ Microorganism: names denoting microorganism taxa.	0
1008	42581	42581	D19-5719	Task Description	3	25	1.0	1.0	These taxa correspond to microorganism branches of the NCBI taxon-omy.	0
1009	42582	42582	D19-5719	Task Description	4	26	1.0	1.0	The set of relevant taxa is given on the BB task website.	0
1010	42583	42583	D19-5719	Task Description	5	27	1.0	1.0	‚Ä¢ Habitat: phrases denoting physical places where microorganisms may be observed;	0
1011	42584	42584	D19-5719	Task Description	6	28	2.0	1.0	‚Ä¢ Geographical: names of geographical places;	0
1012	42585	42585	D19-5719	Task Description	7	29	2.0	1.0	‚Ä¢ Phenotype: expressions describing microbial characteristics.	0
1013	42586	42586	D19-5719	Task Description	8	30	2.0	1.0	The scheme defines two relation types:	0
1014	42587	42587	D19-5719	Task Description	9	31	2.0	1.0	‚Ä¢	0
1015	42588	42588	D19-5719	Task Description	10	32	2.0	1.0	Lives in relations which link a microorganism entity to its location (either a habitat or a geographical entity, or in few rare cases a microorganism entity);	0
1016	42589	42589	D19-5719	Task Description	11	33	3.0	1.0	‚Ä¢	0
1017	42590	42590	D19-5719	Task Description	12	34	3.0	1.0	Exhibits relations which link a microorganism entity to a phenotype entity.	0
1018	42591	42591	D19-5719	Task Description	13	35	3.0	1.0	Arguments of relations may occur in different sentences.	0
1019	42592	42592	D19-5719	Task Description	14	36	3.0	1.0	In addition, microorganisms are normalized to taxa from the NCBI taxonomy.	0
1020	42593	42593	D19-5719	Task Description	15	37	3.0	1.0	Habitat and phenotype entities are normalized to concepts from the OntoBiotope ontology.	0
1021	42594	42594	D19-5719	Task Description	16	38	4.0	1.0	We used the BioNLP-OST-2019 version of OntoBiotope available on AgroPortal 5 .	0
1022	42595	42595	D19-5719	Task Description	17	39	4.0	1.0	We used the NCBI Taxonomy version as available on February 2, 2019 from NCBI website 6 . Copies of both resources can be downloaded from the task website.	0
1023	42596	42596	D19-5719	Task Description	18	40	4.0	1.0	The microorganism part of the taxonomy contains 903,191 taxa plus synonyms, while the OntoBiotope ontology includes 3,601 concepts plus synonyms (3,172 for the Habitat branch and 429 for the Phenotype branch of the ontology).	0
1024	42597	42597	D19-5719	Task Description	19	41	4.0	1.0	Geographical entities are not normalized.	0
1025	42598	42598	D19-5719	Task Description	20	42	4.0	1.0	Figure 1 shows an example of a sentence annotated with normalized entities and relations.	0
1026	42599	42599	D19-5719	Task Description	21	43	4.0	1.0	As in the 2016 edition, we designed three tasks, each including two modalities, one where entity annotations are provided and one where they are not and have to be predicted.	0
1027	42742	42742	D19-5719	Conclusion	1	186	1.0	4.0	The Bacteria Biotope	0
1028	42743	42743	D19-5719	Conclusion	2	187	1.0	4.0	Task arouses sustained interest with a total of 10 teams participating in the fourth edition.	0
1029	42744	42744	D19-5719	Conclusion	3	188	1.0	4.0	As usual, the relation extraction sub-tasks (BB-rel and BB-rel+ner) were the most popular, demonstrating that this task is still a scientific and technical challenge.	0
1030	42745	42745	D19-5719	Conclusion	4	189	2.0	4.0	The most notable evolution of participating systems since the last edition is the pervasiveness of methods based on neural networks and word embeddings.	0
1031	42746	42746	D19-5719	Conclusion	5	190	2.0	4.0	These systems yielded superior predictions compared to those in 2016.	0
1032	42747	42747	D19-5719	Conclusion	6	191	2.0	4.0	As mentioned previously, there is still much room for improvement in addressing cross-sentence relation extraction.	0
1033	42748	42748	D19-5719	Conclusion	7	192	2.0	4.0	We also note a growing interest in the normalization sub-tasks (BB-norm and BB-norm+ner).	0
1034	42749	42749	D19-5719	Conclusion	8	193	3.0	4.0	The predictions improved for habitat entities, and are very promising for phenotype entities.	0
1035	42750	42750	D19-5719	Conclusion	9	194	3.0	4.0	However the generalization from bacteria-only taxa in 2016 to all microorganisms in this edition proved to pose an unexpected challenge.	0
1036	42751	42751	D19-5719	Conclusion	10	195	3.0	4.0	Knowledge base population (BB-kb and BB-kb+ner) is the most challenging task, since it requires a wider set of capabilities.	0
1037	42752	42752	D19-5719	Conclusion	11	196	4.0	4.0	Nevertheless we demonstrated that the combination of other subtask predictions allows to produce better quality knowledge bases.	0
1038	42753	42753	D19-5719	Conclusion	12	197	4.0	4.0	To help participants, supporting resources were provided.	0
1039	42754	42754	D19-5719	Conclusion	13	198	4.0	4.0	The most used resources were pretrained word embeddings, and general-domain named entities.	0
1040	42755	42755	D19-5719	Conclusion	14	199	4.0	4.0	The evaluation on the test set will be maintained online 11 in order for future experiments to compare with the current state of the art.	0
1041	42958	42958	D19-6007	title	1	1	4.0	1.0	Commonsense Inference in Natural Language Processing (COIN) -Shared Task Report	0
1042	42959	42959	D19-6007	abstract	1	2	1.0	1.0	This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019.	0
1043	42960	42960	D19-6007	abstract	2	3	2.0	1.0	The tasks consisted of two machine comprehension evaluations, each of which tested a system's ability to answer questions/queries about a text.	0
1044	42961	42961	D19-6007	abstract	3	4	3.0	1.0	Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text.	0
1045	42962	42962	D19-6007	abstract	4	5	4.0	1.0	A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
1046	42963	42963	D19-6007	Introduction	1	6	1.0	1.0	Due to the rise of powerful pre-trained word and sentence representations, automated text processing has come a long way in recent years, with systems that perform even better than humans on some datasets (Rajpurkar et al., 2016a).	0
1047	42964	42964	D19-6007	Introduction	2	7	1.0	1.0	However, natural language understanding also involves complex challenges.	0
1048	42965	42965	D19-6007	Introduction	3	8	1.0	1.0	One important difference between human and machine text understanding lies in the fact that humans can access commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that are assumed to be common ground.	0
1049	42966	42966	D19-6007	Introduction	4	9	1.0	1.0	"(1) Max: ""It's 1 pm already, I think we should get lunch."""	0
1050	42967	42967	D19-6007	Introduction	5	10	1.0	1.0	"Dustin: ""Let me get my wallet."""	0
1051	42968	42968	D19-6007	Introduction	6	11	2.0	1.0	Consider the conversation in Example 1: Max will not be surprised that Dustin needs to get his wallet, since she knows that paying is a part of getting lunch.	0
1052	42969	42969	D19-6007	Introduction	7	12	2.0	1.0	Also, she knows that a wallet is needed for paying, so Dustin needs to get a wallet for lunch.	0
1053	42970	42970	D19-6007	Introduction	8	13	2.0	1.0	This is part of the commonsense knowledge about getting lunch and should be known by both persons.	0
1054	42971	42971	D19-6007	Introduction	9	14	2.0	1.0	For a computer system, inferring such unmentioned facts is a non-trivial challenge.	0
1055	42972	42972	D19-6007	Introduction	10	15	2.0	1.0	The workshop on Commonsense Inference in NLP (COIN) is focused on such phenomena, looking at models, data, and evaluation methods for commonsense inference.	0
1056	42973	42973	D19-6007	Introduction	11	16	3.0	1.0	This report summarizes the results of the COIN shared tasks, an unofficial extension of the Sem-Eval 2018 shared task 11, Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).	0
1057	42974	42974	D19-6007	Introduction	12	17	3.0	1.0	The tasks aim to evaluate the commonsense inference capabilities of text understanding systems in two settings: Commonsense inference in everyday narrations (task 1) and commonsense inference in news texts (task 2).	1
1058	42975	42975	D19-6007	Introduction	13	18	3.0	1.0	Framed as machine comprehension evaluations, the datasets used for both tasks contain challenging reading comprehension questions asking for facts that are not explicitly mentioned in the given reading texts.	0
1059	42976	42976	D19-6007	Introduction	14	19	3.0	1.0	Several teams participated in the shared tasks and submitted system description papers.	0
1060	42977	42977	D19-6007	Introduction	15	20	3.0	1.0	All systems are based on Transformer architectures (Vaswani et al., 2017), some of them explicitly incorporating commonsense knowledge resources, whereas others only use pretraining on other machine comprehension data sets.	0
1061	42978	42978	D19-6007	Introduction	16	21	4.0	1.0	The best submitted system achieves 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
1062	42979	42979	D19-6007	Introduction	17	22	4.0	1.0	Still, there are cases that remain elusive: Humans outperform this system by a margin of 7% (task 1) and 8% (task 2).	0
1063	42980	42980	D19-6007	Introduction	18	23	4.0	1.0	Our results indicate that while Transformer models are able to perform extremely well on the data used in our shared task, there are still some remaining cases demonstrating that human level is not achieved yet.	0
1064	42981	42981	D19-6007	Introduction	19	24	4.0	1.0	Still, we believe that our results also imply the need for more challenging data sets.	0
1065	42982	42982	D19-6007	Introduction	20	25	4.0	1.0	In particular, we need data sets that make it harder to benefit from redundancy in the training data or large-scale pretraining on similar domains.	0
1066	42983	42983	D19-6007	Introduction	21	26	4.0	1.0	In the following, we briefly describe the data sets ( ¬ß2), baselines and evaluation metrics of the shared tasks ( ¬ß3) and we present a summary of the participating systems ( ¬ß4), their results ( ¬ß5) as well as a discussion thereof ( ¬ß6).	0
1067	43106	43106	D19-6007	Related Work	1	149	1.0	4.0	Evaluating commonsense inference via machine comprehension has recently moved into the focus of interest.	0
1068	43107	43107	D19-6007	Related Work	2	150	1.0	4.0	Existing datasets cover various domains:	0
1069	43108	43108	D19-6007	Related Work	3	151	1.0	4.0	Web texts.	0
1070	43109	43109	D19-6007	Related Work	4	152	1.0	4.0	Trivia	0
1071	43110	43110	D19-6007	Related Work	5	153	1.0	4.0	QA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web.	0
1072	43111	43111	D19-6007	Related Work	6	154	2.0	4.0	A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer.	0
1073	43112	43112	D19-6007	Related Work	7	155	2.0	4.0	Commonsense	0
1074	43113	43113	D19-6007	Related Work	8	156	2.0	4.0	QA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017).	0
1075	43114	43114	D19-6007	Related Work	9	157	2.0	4.0	Evidence documents were webcrawled based on the question and added after the crowdsourcing step.	0
1076	43115	43115	D19-6007	Related Work	10	158	2.0	4.0	Fictive texts.	0
1077	43116	43116	D19-6007	Related Work	11	159	2.0	4.0	Narrative	0
1078	43117	43117	D19-6007	Related Work	12	160	3.0	4.0	QA (Koƒçisk√Ω et al., 2018) provides full novels and other long texts as evidence documents and contains approx.	0
1079	43118	43118	D19-6007	Related Work	13	161	3.0	4.0	30 crowdsourced questions per text.	0
1080	43119	43119	D19-6007	Related Work	14	162	3.0	4.0	The questions require a system to understand the whole plot of the text and to conduct many successive complicated inference steps, under the use of various types of background knowledge.	0
1081	43120	43120	D19-6007	Related Work	15	163	3.0	4.0	News texts.	0
1082	43121	43121	D19-6007	Related Work	16	164	3.0	4.0	News	0
1083	43122	43122	D19-6007	Related Work	17	165	4.0	4.0	QA (Trischler et al., 2017) provides news texts with crowdsourced questions and answers, which are spans of the evidence documents.	0
1084	43123	43123	D19-6007	Related Work	18	166	4.0	4.0	The question collection procedure for NewsQA resulted in a large number of questions that require factual commonsense knowledge for finding an answer.	0
1085	43124	43124	D19-6007	Related Work	19	167	4.0	4.0	Other tasks.	0
1086	43125	43125	D19-6007	Related Work	20	168	4.0	4.0	There have been other attempts at evaluating commonsense inference apart from machine comprehension.	0
1087	43126	43126	D19-6007	Related Work	21	169	4.0	4.0	One example is the Story cloze test and the ROC dataset (Mostafazadeh et al., 2016), where systems have to find the correct ending to a 5-sentence story, using different types of commonsense knowledge.	0
1088	43127	43127	D19-6007	Related Work	22	170	4.0	4.0	SWAG (Zellers et al., 2018) is a natural language inference dataset with a focus on difficult commonsense inferences.	0
1089	43128	43128	D19-6007	Conclusion	1	171	1.0	4.0	This report presented the results of the shared tasks at the Workshop for Commonsense Inference in NLP (COIN).	0
1090	43129	43129	D19-6007	Conclusion	2	172	1.0	4.0	The tasks aimed at evaluating the capability of systems to make use of commonsense knowledge for challenging inference questions in a machine comprehension setting, on everyday narrations (task 1) and news texts (task 2).	0
1091	43130	43130	D19-6007	Conclusion	3	173	2.0	4.0	In total, 5 systems participated in task 1, and one system participated in task 2.	0
1092	43131	43131	D19-6007	Conclusion	4	174	2.0	4.0	All submitted models were Transformer models, pretrained with a language modeling objective on large amounts of textual data.	0
1093	43132	43132	D19-6007	Conclusion	5	175	2.0	4.0	The best system achieved 90.6% accuracy and 83.7% F1-score on task 1 and 2, respectively, leaving a gap of 7% and 8% to human performance.	0
1094	43133	43133	D19-6007	Conclusion	6	176	3.0	4.0	The results of our shared tasks suggest that existing models cover a large part of the commonsense knowledge required for our data sets in the domains of narrations and news texts.	0
1095	43134	43134	D19-6007	Conclusion	7	177	3.0	4.0	This does however not mean that commonsense inference is solved:	0
1096	43135	43135	D19-6007	Conclusion	8	178	4.0	4.0	We found a range of examples in our data that are not successfully covered.	0
1097	43136	43136	D19-6007	Conclusion	9	179	4.0	4.0	Furthermore, data sets such as HellaSWAG (Zellers et al., 2019) show that commonsense inference tasks can be specifically tailored to be hard for Transformer models.	0
1098	43137	43137	D19-6007	Conclusion	10	180	4.0	4.0	We believe that modeling true language understanding requires a shift towards text types and tasks that test commonsense knowledge go-ing beyond information that can be obtained by exploiting the redundancy of large-scale corpora and/or pretraining on related tasks.	0
1099	45218	45218	I17-4002	title	1	1	4.0	1.0	IJCNLP-2017 Task 2: Dimensional Sentiment Analysis for Chinese Phrases	0
1100	45219	45219	I17-4002	abstract	1	2	1.0	1.0	This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions.	0
1101	45220	45220	I17-4002	abstract	2	3	2.0	1.0	Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm.	0
1102	45221	45221	I17-4002	abstract	3	4	3.0	1.0	Of the 19 teams registered for this shared task for twodimensional sentiment analysis, 13 submitted results.	0
1103	45222	45222	I17-4002	abstract	4	5	4.0	1.0	We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing.	0
1104	45223	45223	I17-4002	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring script are made publicly available to researchers.	0
1105	45224	45224	I17-4002	Introduction	1	7	1.0	1.0	Sentiment analysis has emerged as a leading technique to automatically identify affective information within texts.	0
1106	45225	45225	I17-4002	Introduction	2	8	1.0	1.0	In sentiment analysis, affective states are generally represented using either categorical or dimensional approaches (Calvo and Kim, 2013).	0
1107	45226	45226	I17-4002	Introduction	3	9	1.0	1.0	The categorical approach represents affective states as several discrete classes (e.g., positive, negative, neutral), while the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1.	0
1108	45227	45227	I17-4002	Introduction	4	10	1.0	1.0	The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm.	0
1109	45228	45228	I17-4002	Introduction	5	11	2.0	1.0	Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011;	0
1110	45229	45229	I17-4002	Introduction	6	12	2.0	1.0	Malandrakis et al., 2011; or texts (Kim et al., 2010;	0
1111	45230	45230	I17-4002	Introduction	7	13	2.0	1.0	Paltoglou et al, 2013;	0
1112	45231	45231	I17-4002	Introduction	8	14	2.0	1.0	Wang et al., 2016b).	0
1113	45232	45232	I17-4002	Introduction	9	15	2.0	1.0	Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014)	0
1114	45233	45233	I17-4002	Introduction	10	16	3.0	1.0	The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing.	0
1115	45234	45234	I17-4002	Introduction	11	17	3.0	1.0	Sentiment lexicons with valence-arousal ratings are useful resources for the development of dimensional sentiment applications.	0
1116	45235	45235	I17-4002	Introduction	12	18	3.0	1.0	Due to the limited availability of such VA lexicons, especially for Chinese, the objective of the task is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases.	0
1117	45236	45236	I17-4002	Introduction	13	19	3.0	1.0	The rest of this paper is organized as follows.	0
1118	45237	45237	I17-4002	Introduction	14	20	4.0	2.0	Section II describes the task in detail.	0
1119	45238	45238	I17-4002	Introduction	15	21	4.0	2.0	Section III introduces the constructed datasets.	0
1120	45239	45239	I17-4002	Introduction	16	22	4.0	2.0	Section IV proposes evaluation metrics.	0
1121	45240	45240	I17-4002	Introduction	17	23	4.0	2.0	Section V reports the results of the participants' approaches.	0
1122	45241	45241	I17-4002	Introduction	18	24	4.0	2.0	Conclusions are finally drawn in Section VI.	0
1123	45242	45242	I17-4002	Task Description	1	25	1.0	2.0	This task seeks to evaluate the capability of systems for predicting dimensional sentiments of Chinese words and phrases.	0
1124	45243	45243	I17-4002	Task Description	2	26	1.0	2.0	For a given word or phrase, participants were asked to provide a realvalued score from 1 to 9 for both the valence and arousal dimensions, respectively indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal.	1
1125	45244	45244	I17-4002	Task Description	3	27	1.0	2.0	"The input format is ""term_id, term"", and the output format is ""term_id, valence_rating, arousal_rating""."	0
1126	45245	45245	I17-4002	Task Description	4	28	1.0	2.0	"Below are the input/output formats of the example words ""Â•Ω"" (good), ""ÈùûÂ∏∏Â•Ω"" (very good), ""ÊªøÊÑè"" (satisfy) and ""‰∏çÊªøÊÑè"" (not satisfy)."	0
1127	45246	45246	I17-4002	Task Description	5	29	2.0	2.0	with valence-arousal ratings.	0
1128	45247	45247	I17-4002	Task Description	6	30	2.0	2.0	For multi-word phrases, we first selected a set of modifiers such as negators (e.g., not), degree adverbs (e.g., very) and modals (e.g., would).	0
1129	45248	45248	I17-4002	Task Description	7	31	2.0	2.0	These modifiers were combined with the affective words in CVAW to form multi-word phrases.	0
1130	45249	45249	I17-4002	Task Description	8	32	2.0	2.0	The frequency of each phrase was then retrieved from a large web-based corpus.	0
1131	45250	45250	I17-4002	Task Description	9	33	2.0	2.0	Only phrases with a frequency greater than or equal to 3 were retained as candidates.	0
1132	45251	45251	I17-4002	Task Description	10	34	3.0	2.0	To avoid several modifiers dominating the whole dataset, each modifier (or modifier combination) can have at most 50 phrases.	0
1133	45252	45252	I17-4002	Task Description	11	35	3.0	2.0	In addition, the phrases were selected to maximize the balance between positive and negative words.	0
1134	45253	45253	I17-4002	Task Description	12	36	3.0	2.0	Finally, a total of 3,000 phrases were collected by excluding unusual and semantically incomplete candidate phrases, of which 2,250 phrases were randomly selected as the training set according to the proportions of each modifier (or modifier combination) in the original set, and the remaining 750 phrases were used as the test set.	0
1135	45254	45254	I17-4002	Task Description	13	37	3.0	2.0	Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words .	0
1136	45255	45255	I17-4002	Task Description	14	38	3.0	2.0	Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth.	0
1137	45256	45256	I17-4002	Task Description	15	39	4.0	3.0	Each multi-word phrase was rated by at least 10 different annotators.	0
1138	45257	45257	I17-4002	Task Description	16	40	4.0	3.0	Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations.	0
1139	45258	45258	I17-4002	Task Description	17	41	4.0	3.0	They were then excluded from the calculation of the average ratings for each phrase.	0
1140	45259	45259	I17-4002	Task Description	18	42	4.0	3.0	The policy of this shared task was implemented as is an open test.	0
1141	45260	45260	I17-4002	Task Description	19	43	4.0	3.0	That is, in addition to the above official datasets, participating teams were allowed to use other publicly available data for system development, but such sources should be specified in the final technical report.	0
1142	45290	45290	I17-4002	Conclusions	1	73	1.0	4.0	This study describes an overview of the IJCNLP 2017 shared task on dimensional sentiment analysis for Chinese phrases, including task design, data preparation, performance metrics, and evaluation results.	0
1143	45291	45291	I17-4002	Conclusions	2	74	2.0	4.0	Regardless of actual performance, all submissions contribute to the common effort to develop dimensional approaches for affective computing, and the individual report in the proceedings provide useful insights into Chinese sentiment analysis.	0
1144	45292	45292	I17-4002	Conclusions	3	75	3.0	4.0	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
1145	45293	45293	I17-4002	Conclusions	4	76	4.0	4.0	Therefore, all data sets with gold standard and scoring script are publicly available 5 .	0
1146	46115	46115	W11-1802	title	1	1	1.0	1.0	Overview of Genia Event Task in BioNLP Shared Task 2011	0
1147	46116	46116	W11-1802	abstract	1	2	2.0	1.0	The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.	1
1148	46117	46117	W11-1802	abstract	2	3	3.0	1.0	As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.	0
1149	46118	46118	W11-1802	abstract	3	4	4.0	1.0	After a 3-month system development period, 15 teams submitted their performance results on test cases.	0
1150	46119	46119	W11-1802	abstract	4	5	1.0	1.0	The results show the community has made a significant advancement in terms of both performance improvement and generalization.	0
1151	46120	46120	W11-1802	Introduction	1	6	1.0	1.0	The BioNLP Shared Task (BioNLP-ST, hereafter) is a series of efforts to promote a communitywide collaboration towards fine-grained information extraction (IE) in biomedical domain.	0
1152	46121	46121	W11-1802	Introduction	2	7	1.0	1.0	The first event, BioNLP-ST 2009, introducing a biomolecular event (bio-event) extraction task to the community, attracted a wide attention, with 42 teams being registered for participation and 24 teams submitting final results (Kim et al., 2009).	0
1153	46122	46122	W11-1802	Introduction	3	8	1.0	1.0	To establish a community effort, the organizers provided the task definition, benchmark data, and evaluations, and the participants competed in developing systems to perform the task.	0
1154	46123	46123	W11-1802	Introduction	4	9	2.0	1.0	Meanwhile, participants and organizers communicated to develop a better setup of evaluation, and some provided their tools and resources for other participants, making it a collaborative competition.	0
1155	46124	46124	W11-1802	Introduction	5	10	2.0	1.0	The final results enabled to observe the state-ofthe-art performance of the community on the bioevent extraction task, which showed that the automatic extraction of simple events -those with unary arguments, e.g. gene expression, localization, phosphorylation -could be achieved at the performance level of 70% in F-score, but the extraction of complex events, e.g. binding and regulation, was a lot more challenging, having achieved 40% of performance level.	0
1156	46125	46125	W11-1802	Introduction	6	11	2.0	1.0	After BioNLP-ST 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.	0
1157	46126	46126	W11-1802	Introduction	7	12	2.0	1.0	Since then, several improvements have been reported (Miwa et al., 2010b;	0
1158	46127	46127	W11-1802	Introduction	8	13	2.0	1.0	Poon and Vanderwende, 2010;Vlachos, 2010;Miwa et al., 2010a;	0
1159	46128	46128	W11-1802	Introduction	9	14	3.0	1.0	Bj√∂rne et al., 2010).	0
1160	46129	46129	W11-1802	Introduction	10	15	3.0	1.0	For example, Miwa et al.	0
1161	46130	46130	W11-1802	Introduction	11	16	3.0	1.0	(Miwa et al., 2010b) reported a significant improvement with binding events, achieving 50% of performance level.	0
1162	46131	46131	W11-1802	Introduction	12	17	3.0	1.0	The task introduced in BioNLP-ST 2009 was renamed to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al., 2011).	0
1163	46132	46132	W11-1802	Introduction	13	18	3.0	1.0	As the sole task that was repeated in the two events, the GE task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of 2011.	0
1164	46133	46133	W11-1802	Introduction	14	19	4.0	1.0	The GE task in 2011 received final submissions from 15 teams.	0
1165	46134	46134	W11-1802	Introduction	15	20	4.0	1.0	The results show the community made a significant progress with the task, and also show the technology can be generalized to full papers at moderate cost of performance.	0
1166	46135	46135	W11-1802	Introduction	16	21	4.0	1.0	This paper presents the task setup, preparation, and discusses the results.	0
1167	46136	46136	W11-1802	Introduction	17	22	4.0	1.0	1: Event types and their arguments for Genia event task.	0
1168	46137	46137	W11-1802	Introduction	18	23	4.0	1.0	The type of each filler entity is specified in parenthesis.	0
1169	46138	46138	W11-1802	Introduction	19	24	1.0	1.0	"Arguments that may be filled more than once per event are marked with ""+""."	0
1170	46139	46139	W11-1802	Task Definition	1	25	1.0	1.0	The GE task follows the task definition of BioNLP-ST 2009, which is briefly described in this section.	0
1171	46140	46140	W11-1802	Task Definition	2	26	1.0	2.0	For more detail, please refer to (Kim et al., 2009).	0
1172	46141	46141	W11-1802	Task Definition	3	27	1.0	2.0	Table 1 shows the event types to be addressed in the task.	0
1173	46142	46142	W11-1802	Task Definition	4	28	1.0	2.0	For each event type, the primary and secondary arguments to be extracted with an event are defined.	0
1174	46143	46143	W11-1802	Task Definition	5	29	2.0	2.0	For example, a Phosphorylation event is primarily extracted with the protein to be phosphorylated.	0
1175	46144	46144	W11-1802	Task Definition	6	30	2.0	2.0	As secondary information, the specific site to be phosphorylated may be extracted.	0
1176	46145	46145	W11-1802	Task Definition	7	31	2.0	2.0	From a computational point of view, the event types represent different levels of complexity.	0
1177	46146	46146	W11-1802	Task Definition	8	32	2.0	2.0	When only primary arguments are considered, the first five event types in Table 1 are classified as simple event types, requiring only unary arguments.	0
1178	46147	46147	W11-1802	Task Definition	9	33	2.0	2.0	The Binding and Regulation types are more complex: Binding requires detection of an arbitrary number of arguments, and Regulation requires detection of recursive event structure.	0
1179	46148	46148	W11-1802	Task Definition	10	34	2.0	2.0	Based on the definition of event types, the entire task is divided to three sub-tasks addressing event extraction at different levels of specificity: Task 1.	0
1180	46149	46149	W11-1802	Task Definition	11	35	3.0	2.0	Core event extraction addresses the extraction of typed events together with their primary arguments.	0
1181	46150	46150	W11-1802	Task Definition	12	36	3.0	2.0	Task 2. Event enrichment addresses the extraction of secondary arguments that further specify the events extracted in Task 1.	0
1182	46151	46151	W11-1802	Task Definition	13	37	3.0	2.0	Task 3. Negation/Speculation detection addresses the detection of negations and speculations over the extracted events.	0
1183	46152	46152	W11-1802	Task Definition	14	38	3.0	2.0	Task 1 serves as the backbone of the GE task and is mandatory for all participants, while the other two are optional.	0
1184	46153	46153	W11-1802	Task Definition	15	39	3.0	2.0	The annotation T1 identifies the entity referred to by the string (p65) between the character offsets, 15 and 18 to be a Protein.	0
1185	46154	46154	W11-1802	Task Definition	16	40	3.0	2.0	T2 identifies the string, translocation, to refer to a Localization event.	0
1186	46155	46155	W11-1802	Task Definition	17	41	4.0	2.0	Entities other than proteins or event type references are classified into a default class Entity, as in T3. E1 then represents the event defined by the three entities, as defined in Table 1.	0
1187	46156	46156	W11-1802	Task Definition	18	42	4.0	2.0	Note that for Task 1, the entity, T3, does not need to be identified, and the event, E1, may be identified without specification of the secondary argument, ToLoc:T1: E1' Localization:	0
1188	46157	46157	W11-1802	Task Definition	19	43	4.0	2.0	T2 Theme:	0
1189	46158	46158	W11-1802	Task Definition	20	44	4.0	2.0	T1	0
1190	46159	46159	W11-1802	Task Definition	21	45	4.0	2.0	Finding the full representation of E1 is the goal of Task 2.	0
1191	46160	46160	W11-1802	Task Definition	22	46	4.0	2.0	In the example, the localization event, E1, is negated as expressed in the failure of .	0
1192	46161	46161	W11-1802	Task Definition	23	47	1.0	2.0	Finding the negation, M1 is the goal of Task 3.	0
1193	46213	46213	W11-1802	Conclusions	1	99	3.0	4.0	The Genia event task which was repeated for BioNLP-ST 2009 and 2011 took a role of measuring the progress of the community and generalization IE technology to full papers.	0
1194	46214	46214	W11-1802	Conclusions	2	100	4.0	4.0	The results from 15 teams who made their final submissions to the task show that a clear advance of the community in terms of the performance on a focused domain and also generalization to full papers.	0
1195	46215	46215	W11-1802	Conclusions	3	101	4.0	4.0	To our disappointment, however, an effective use of supporting task results was not observed, which thus remains as future work for further improvement.	0
