	Unnamed: 0	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
0	0	0	S01-basque	title	1	1	4.0	1.0	The Basque task: did systems perform in the upperbound?	0
1	1	1	S01-basque	abstract	1	2	1.0	1.0	In this paper we describe the Senseval 2 Basque lexical-sample task.	0
2	2	2	S01-basque	abstract	2	3	2.0	1.0	The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary.	0
3	3	3	S01-basque	abstract	3	4	2.0	1.0	Most examples were taken from the Egunkaria newspaper.	0
4	4	4	S01-basque	abstract	4	5	3.0	1.0	The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration.	0
5	5	5	S01-basque	abstract	5	6	3.0	1.0	The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage.	0
6	6	6	S01-basque	abstract	6	7	4.0	1.0	The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems.	0
7	7	7	S01-basque	abstract	7	8	4.0	1.0	In particular, we argue that inter-tagger agreement is not a real upperbound for the B,asque WSD task.	0
8	8	8	S01-basque	Introduction	1	9	1.0	1.0	This paper reviews the design of the lexicalsample task for Basque.	0
9	9	9	S01-basque	Introduction	2	10	2.0	1.0	The following steps were taken in order to build the hand-tagged corpus: 1. set the exercise a. choose sense inventory b. choose target corpus c. choose target words d. select examples from the corpus 2. hand-tagging a. define procedure b. tag c. analysis of inter-tagger agreement d. arbitration	0
10	10	10	S01-basque	Introduction	3	11	3.0	1.0	The following section presents the setting of the exercise.	0
11	11	11	S01-basque	Introduction	4	12	4.0	1.0	Section 3 reviews the hand-tagging, and section 4 the results of the participant systems.	0
12	12	12	S01-basque	Introduction	5	13	4.0	1.0	Section 5 discusses the design of the task, as well 9 as the results, and section 6 presents some future work.	0
13	13	13	S01-basque	2	1	14	2.0	1.0	Setting of the exercise	0
14	14	14	S01-basque	2	2	15	4.0	1.0	In this section we present the setting of the Basque lexical-sample exercise.	0
15	15	15	S01-basque	Basque	1	16	1.0	1.0	Basque is an agglutinative language, that is, for the formation of words, the dictionary entry independently takes each of the elements necessary for the different functions (syntactic case included).	0
16	16	16	S01-basque	Basque	2	17	2.0	1.0	More specifically, the affixes corresponding to the determinant, number and declension case are taken in this order and independently of each other (deep morphological structure).	0
17	17	17	S01-basque	Basque	3	18	3.0	1.0	One of the main characteristics of Basque is its declension system with numerous cases, which differentiates it from the languages spoken in the surrounding countries.	0
18	18	18	S01-basque	Basque	4	19	4.0	1.0	An example follows (the order of the lemmas is the reverse): etxekoari emaiozu [Give it] [to the one in the house]	0
19	19	19	S01-basque	Sense inventory	1	20	1.0	1.0	We chose a published dictionary, Euskal Hiztegia (Sarasola, 1996), for the sense inventory.	0
20	20	20	S01-basque	Sense inventory	2	21	1.0	1.0	It is a monolingual dictionary of Basque.	0
21	21	21	S01-basque	Sense inventory	3	22	2.0	1.0	It is normative and repository of standard Basque.	0
22	22	22	S01-basque	Sense inventory	4	23	2.0	1.0	It was produced based mainly on literary tradition.	0
23	23	23	S01-basque	Sense inventory	5	24	3.0	1.0	The dictionary has 30,715 entries and 41,699 main senses (see comment on nuances below).	0
24	24	24	S01-basque	Sense inventory	6	25	3.0	1.0	The TEl version with all the information for each entry was included in the distribution.	0
25	25	25	S01-basque	Sense inventory	7	26	4.0	1.0	As the format was quite complex, another version was also included, which listed a plain list of word senses and multiword terms.	0
26	26	26	S01-basque	Sense inventory	8	27	4.0	1.0	This dictionary has the particularity that word senses can have very specific sub-senses, called nuances which sometimes are illustrated with just an example and other times have a full definition. •	0
27	27	27	S01-basque	Sense inventory	9	28	4.0	1.0	These nuances were also included in the set of word senses.	0
28	28	28	S01-basque	Corpora used	1	29	1.0	1.0	At first the EEBS balanced corpus was chosen, comprising one million words.	0
29	29	29	S01-basque	Corpora used	2	30	2.0	1.0	Unfortunately this size is too small to provide the number of occurrences per word that was defined in the Senseval task specification.	0
30	30	30	S01-basque	Corpora used	3	31	2.0	2.0	We therefore turned to the biggest corpus at hand, the Egunkaria corpus, comprising texts taken from the newspaper.	0
31	31	31	S01-basque	Corpora used	4	32	3.0	2.0	The size of this corpus allowed us to easily reach the number of examples required.	0
32	32	32	S01-basque	Corpora used	5	33	4.0	2.0	On the negative side, it is a specific corpus, and the distribution of the word senses could be highly biased.	0
33	33	33	S01-basque	Corpora used	6	34	4.0	2.0	We used Egunkaria as the main corpus, but we also used the EEBS corpus in some cases, as we will see below.	0
34	34	34	S01-basque	Words chosen	1	35	1.0	2.0	The criterion to choose the 40 words (15 nouns, 15 verbs and 10 adjectives) was that they should cover all possible combinations of frequency, polysemy and skew 1 .	0
35	35	35	S01-basque	Words chosen	2	36	1.0	2.0	The first two can be objectively determined before starting to hand tag, but skew could only be determined by introspection.	0
36	36	36	S01-basque	Words chosen	3	37	1.0	2.0	After choosing a word, the expected skew was sometimes different from the desired skew.	0
37	37	37	S01-basque	Words chosen	4	38	1.0	2.0	A secondary criterion was the overlap with the words in other languages, and the overlap with a number of verbs that are being used for subcategorization and diathesis alternation studies in our group.	0
38	38	38	S01-basque	Words chosen	5	39	2.0	2.0	The English task organizers and the Spanish task organizers provided us with half of the words chosen in their lexical-sample task.	0
39	39	39	S01-basque	Words chosen	6	40	2.0	2.0	This information could be used for cross-language mapping of word senses.	0
40	40	40	S01-basque	Words chosen	7	41	2.0	2.0	Regarding the overlap with verbs, we plan to explore the influence of 1	0
41	41	41	S01-basque	Words chosen	8	42	2.0	2.0	By skew in this context, we mean the dominance of one sense over the others.	0
42	42	42	S01-basque	Words chosen	9	43	3.0	2.0	It is given as the percentage of occurrences of the most frequent sense over all the others.	0
43	43	43	S01-basque	Words chosen	10	44	3.0	2.0	10 word senses in subcategorization and diathesis alternations.	0
44	44	44	S01-basque	Words chosen	11	45	3.0	2.0	We chose the first set of 40 words that covered more or less all combinations of the above phenomena from the set of translations of the words in the other tasks.	0
45	45	45	S01-basque	Words chosen	12	46	3.0	2.0	This was done blindly, without knowing which specific word was chosen.	0
46	46	46	S01-basque	Words chosen	13	47	4.0	2.0	This first set was used to extract the examples (cf. following section), and the hand-taggers started to tag them.	0
47	47	47	S01-basque	Words chosen	14	48	4.0	2.0	Unfortunately, for a number of words, all examples in the corpus referred to a single word sense.	0
48	48	48	S01-basque	Words chosen	15	49	4.0	2.0	We had not foreseen this situation and took two measures: 1) Search for occurrences in the secondary EEBS corpus.	0
49	49	49	S01-basque	Words chosen	16	50	4.0	2.0	2) If occurrences of new senses were not found, then the word was discarded and a replacement word was chosen.	0
50	50	50	S01-basque	Words chosen	17	51	4.0	2.0	In order to find the replacement, the hand-tagger that was . doing the arbitration scanned the examples of a word with similar polysemy and frequency and decided whether it had occurrences of more than one sense.	0
51	51	51	S01-basque	Selection of examples from corpora	1	52	1.0	2.0	The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+ 15*senses+6mword where senses does not include the nuances (cf. section 2.2) and mword is the number of multiword terms that included the target word.	0
52	52	52	S01-basque	Selection of examples from corpora	2	53	2.0	2.0	The minimum number of examples per word was extracted at random from the Egunkaria corpus, plus a 10% buffer.	0
53	53	53	S01-basque	Selection of examples from corpora	3	54	2.0	2.0	As explained in the previous section, for some words occurring in a single sense in this corpus, additional examples were taken from the secondary EEBS corpus.	0
54	54	54	S01-basque	Selection of examples from corpora	4	55	3.0	2.0	In this case, all available examples from EEBS were used, plus the examples from Egunkaria to meet the minimum number of examples required.	0
55	55	55	S01-basque	Selection of examples from corpora	5	56	3.0	2.0	The context included 5 sentences, with the sentence with the target word appearing in the middle.	0
56	56	56	S01-basque	Selection of examples from corpora	6	57	4.0	2.0	Links were kept to the source corpus, document, and to the newspaper section when applicable.	0
57	57	57	S01-basque	Selection of examples from corpora	7	58	4.0	2.0	The occurrences were split at random in training set (two thirds of all occurrences) and test set.	0
58	58	58	S01-basque	Hand tagging	1	59	1.0	2.0	Three persons, graduate linguistics students, took part in the tagging.	0
59	59	59	S01-basque	Hand tagging	2	60	1.0	2.0	They are familiar with word senses, as they are involved in the development of the Basque WordNet and cleaning the TEl version of the Euskal Hiztegia dictionary.	0
60	60	60	S01-basque	Hand tagging	3	61	1.0	2.0	The following procedure was defined for each word: •	0
61	61	61	S01-basque	Hand tagging	4	62	1.0	3.0	The three of them would meet, read the definitions and examples given in , the dictionary and discuss the meaning of each word sense.	0
62	62	62	S01-basque	Hand tagging	5	63	1.0	3.0	They tried to agree the meaning differences among the word senses. •	0
63	63	63	S01-basque	Hand tagging	6	64	1.0	3.0	Two taggers independently tagged all examples for the word.	0
64	64	64	S01-basque	Hand tagging	7	65	1.0	3.0	No communication was allowed while tagging the word.	0
65	65	65	S01-basque	Hand tagging	8	66	1.0	3.0	• Multiple tags were allowed, as well as the following tags: B new sense or multiword term, U unassignable.:.	0
66	66	66	S01-basque	Hand tagging	9	67	1.0	3.0	Examples with these tags were removed from the final release.	0
67	67	67	S01-basque	Hand tagging	10	68	1.0	3.0	• A program was used to compute agreement rate and output those occurrences where there was disagreement grouped by the senses assigned.	0
68	68	68	S01-basque	Hand tagging	11	69	1.0	3.0	•	0
69	69	69	S01-basque	Hand tagging	12	70	1.0	3.0	The third tagger, the referee, reviewed the disagreements and decided which one was correct.	0
70	70	70	S01-basque	Hand tagging	13	71	1.0	3.0	For the word itzal (shadow), the disagreement was specially high.	0
71	71	71	S01-basque	Hand tagging	14	72	1.0	3.0	The, taggers decided that the definitions and examples were too confusing, and decided to replace it with another word.	0
72	72	72	S01-basque	Hand tagging	15	73	1.0	3.0	Overall, the two taggers agreed 75% of the time.	0
73	73	73	S01-basque	Hand tagging	16	74	2.0	3.0	Some words attained an agreement rate above 95% (e.g. nouns kanal -channelor tentsio -tension -), but others like herritown/people/nation attained only 52% agreement..:.	0
74	74	74	S01-basque	Hand tagging	17	75	2.0	3.0	All in all, 5284 occurrences of the 40 words were released.	0
75	75	75	S01-basque	Hand tagging	18	76	2.0	3.0	On average, one hand-tagger took 0.41 minutes per occurrence and the other 0.55 minutes.	0
76	76	76	S01-basque	Hand tagging	19	77	2.0	3.0	The referee took 0.22 minutes per entry, including selection of replacement words.	0
77	77	77	S01-basque	Hand tagging	20	78	2.0	3.0	Time for arbitration meeting is also included.	0
78	78	78	S01-basque	Hand tagging	21	79	2.0	3.0	These are the main issues that we think are interesting for further discussion.	0
79	79	79	S01-basque	Hand tagging	22	80	2.0	3.0	Dictionary used.	0
80	80	80	S01-basque	Hand tagging	23	81	2.0	3.0	Before designing the task, we had to choose between two possible dictionaries: the Basque WordNet and the Euskal Hiztegia dictionary.	0
81	81	81	S01-basque	Hand tagging	24	82	2.0	3.0	Another alternative was to start the lexicographer's work afresh, defining the word senses as the tagging proceeded.	0
82	82	82	S01-basque	Hand tagging	25	83	2.0	3.0	We thought the printed dictionary would provide clear-cut sense distinctions that would allow the tagging to be easier.	0
83	83	83	S01-basque	Hand tagging	26	84	2.0	3.0	After the tagging, the hand-taggers complained that this was not the case.	0
84	84	84	S01-basque	Hand tagging	27	85	2.0	3.0	They think that the tagging would be much more satisfactory had they defined the word senses directly from the corpus.	0
85	85	85	S01-basque	Hand tagging	28	86	2.0	3.0	In particular, they were not allowed to introduce new senses or multiword terms, and such examples were discarded.	0
86	86	86	S01-basque	Hand tagging	29	87	2.0	3.0	Corpus used.	0
87	87	87	S01-basque	Hand tagging	30	88	2.0	3.0	There was a mismatch between the dictionary and the corpus: the corpus was linked to a specific genre, and this resulted in having some senses which were not included in the dictionary.	0
88	88	88	S01-basque	Hand tagging	31	89	3.0	3.0	Besides, many senses in the dictionary did not appear in our corpus, and some words had to be replaced.	0
89	89	89	S01-basque	Hand tagging	32	90	3.0	3.0	This caused the taggers some overwork, but did not influence the quality of the result.	0
90	90	90	S01-basque	Hand tagging	33	91	3.0	3.0	Hand-tagging is a very unpleasant task.	0
91	91	91	S01-basque	Hand tagging	34	92	3.0	3.0	"When asked about future editions, the hand taggers suggested the following: ""please do get somebody else""."	0
92	92	92	S01-basque	Hand tagging	35	93	3.0	4.0	We have to note that the hand taggers are used to repetitive tasks, such as building the Basque WordNet or cleaning-up the TEl version of Euskal Hiztegia.	0
93	93	93	S01-basque	Hand tagging	36	94	3.0	4.0	Inter-tagger agreement.	0
94	94	94	S01-basque	Hand tagging	37	95	3.0	4.0	Part of the disagreement was caused by typos and mistakes.	0
95	95	95	S01-basque	Hand tagging	38	96	3.0	4.0	Nevertheless, we think that the low inter-tagger agreement (75%) was caused mainly by the procedure used to tag the occurrences.	0
96	96	96	S01-basque	Hand tagging	39	97	3.0	4.0	The taggers met and tried to understand the word senses, but the fact is that it was only after tagging a few occurrences that they started to really conceptualise the word senses and draw specific lines among one sense and the others.	0
97	97	97	S01-basque	Hand tagging	40	98	3.0	4.0	If both taggers had been allowed to meet (at least once) while they were tagging, they could have discussed and agreed on a common conceptuali~ation.	0
98	98	98	S01-basque	Hand tagging	41	99	3.0	4.0	The referee found that most of the times whole sets of examples were systematically tagged differently by each of the taggers, that is, each of the taggers had a different criterion about the word sense applicable to that set of examples.	0
99	99	99	S01-basque	Hand tagging	42	100	3.0	4.0	The referee then had to decide on the tag for those sets of examples.	0
100	100	100	S01-basque	Hand tagging	43	101	3.0	4.0	Systems performing as good as inter-tagger agreement.	0
101	101	101	S01-basque	Hand tagging	44	102	3.0	4.0	Traditionally, inter-tagger agreement has been used as an upperbound for the performance of machines in cognitive tasks.	0
102	102	102	S01-basque	Hand tagging	45	103	3.0	4.0	We think that in this case, a system may perform better on the Basque WSD task than a human, in the sense that if the taggers were evaluated against the gold standard they would score lower that the systems.	0
103	103	103	S01-basque	Hand tagging	46	104	4.0	4.0	In fact, current systems, which are still under development for Basque, reach the same performance as humans.	0
104	104	104	S01-basque	Hand tagging	47	105	4.0	4.0	Are machines performing better than humans?	0
105	105	105	S01-basque	Hand tagging	48	106	4.0	4.0	We think that inter-tagger agreement, at least as derived from the procedure used in this exercise, is not a real upperbound, and that systems can easily perform better.	0
106	106	106	S01-basque	Hand tagging	49	107	4.0	4.0	The gold standard reflects the conceptualization of one human, the referee, which does not have to agree with the conceptualization made by other persons (specially if these are done in isolation).	0
107	107	107	S01-basque	Hand tagging	50	108	4.0	4.0	People disagree whether in a certain occurrence this word sense or the other applies, i.e. they can disagree in 12 the meaning of the word senses as defined in the dictionary.	0
108	108	108	S01-basque	Hand tagging	51	109	4.0	4.0	In fact, trying to achieve a common ground when reading the dictionary definitions sometimes produced heated debate in the meetings.	0
109	109	109	S01-basque	Hand tagging	52	110	4.0	4.0	If the gold standard reflects a systematic conceptualization of a person, machine learning algorithms can learn to replicate these conceptualization (categorizations in this case), and achieve high degrees of agreement with the person behind the gold standard.	0
110	110	110	S01-basque	Hand tagging	53	111	4.0	4.0	This does not mean that the system is smarter than the human taggers, but rather that the system has no opinion on his own, and just imitates one of the persons.	0
111	111	111	S01-basque	Hand tagging	54	112	4.0	4.0	Error reduction similar to English task.	0
112	112	112	S01-basque	Hand tagging	55	113	4.0	4.0	The best recall for Basque was 75% vs. 64% of the MFS baseline.	0
113	113	113	S01-basque	Hand tagging	56	114	4.0	4.0	In English the best system achieved 64% recall vs. 47% of the most frequent sense baseline (called commonest baseline in the official results).	0
114	114	114	S01-basque	Hand tagging	57	115	4.0	4.0	It is clear that the skew of the Basque words allowed for higher results.	0
115	115	115	S01-basque	Hand tagging	58	116	4.0	4.0	On the other hand, the error reduction for Basque was 29%, compared to 32% for English.	0
116	116	116	S01-basque	Hand tagging	59	117	4.0	4.0	This implies that systems could effectively learn from the data in both tasks.	0
117	117	117	S01-basque	Hand tagging	60	118	4.0	4.0	No use of domain tags, full documents.	0
118	118	118	S01-basque	Hand tagging	61	119	4.0	4.0	No system used the extra information provided by the full documents or the domain tags.	0
119	119	119	S01-basque	Future work	1	120	1.0	4.0	First of all, we plan to explore the use of other procedures for the hand-tagging.	0
120	120	120	S01-basque	Future work	2	121	2.0	4.0	We think that the data attained high levels of quality (which has been shown by the error reduction attained by the participating systems over the MFS baseline), but still we are not satisfied with the sense inventory used.	0
121	121	121	S01-basque	Future work	3	122	3.0	4.0	Further analysis of the results of the participating systems is also planned, as Kappa statistics and the performance of the combination of the systems.	0
122	122	122	S01-basque	Future work	4	123	4.0	4.0	Bibliography Sarasola, I., 1996, Euskal Hiztegia, Donostia, Gipuzkoako Kutxa.	0
123	2093	2093	S07-2	title	1	1	4.0	1.0	Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems	1
124	2094	2094	S07-2	abstract	1	2	1.0	1.0	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledgebased systems.	0
125	2095	2095	S07-2	abstract	2	3	2.0	1.0	In total there were 6 participating systems.	0
126	2096	2096	S07-2	abstract	3	4	3.0	1.0	We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).	0
127	2097	2097	S07-2	abstract	4	5	4.0	1.0	We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
128	2098	2098	S07-2	Introduction	1	6	1.0	1.0	Word Sense Disambiguation (WSD) is a key enabling-technology.	0
129	2099	2099	S07-2	Introduction	2	7	1.0	1.0	Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.	0
130	2100	2100	S07-2	Introduction	3	8	1.0	1.0	Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).	0
131	2101	2101	S07-2	Introduction	4	9	1.0	1.0	In theory, larger amounts of training data (SemCor has approx.	0
132	2102	2102	S07-2	Introduction	5	10	2.0	1.0	500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.	0
133	2103	2103	S07-2	Introduction	6	11	2.0	1.0	Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Martínez and Agirre, 2000;	0
134	2104	2104	S07-2	Introduction	7	12	2.0	1.0	Koeling et al., 2005).	0
135	2105	2105	S07-2	Introduction	8	13	2.0	1.0	"Supervised WSD is based on the ""fixed-list of senses"" paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon."	0
136	2106	2106	S07-2	Introduction	9	14	2.0	1.0	Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).	0
137	2107	2107	S07-2	Introduction	10	15	3.0	1.0	Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.	1
138	2108	2108	S07-2	Introduction	11	16	3.0	1.0	Typical WSID systems involve clustering techniques, which group together similar examples.	0
139	2109	2109	S07-2	Introduction	12	17	3.0	1.0	Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.	0
140	2110	2110	S07-2	Introduction	13	18	3.0	1.0	One of the problems of unsupervised systems is that of managing to do a fair evaluation.	0
141	2111	2111	S07-2	Introduction	14	19	3.0	1.0	Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.	0
142	2112	2112	S07-2	Introduction	15	20	4.0	1.0	The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.	0
143	2113	2113	S07-2	Introduction	16	21	4.0	1.0	The paper is organized as follows.	0
144	2114	2114	S07-2	Introduction	17	22	4.0	1.0	Section 2 presents the evaluation framework used in this task.	0
145	2115	2115	S07-2	Introduction	18	23	4.0	1.0	Section 3 presents the systems that participated in the task, and the official results.	0
146	2116	2116	S07-2	Introduction	19	24	4.0	1.0	Finally, Section 5 draws the conclusions.	0
147	2117	2117	S07-2	Evaluating WSID systems	1	25	1.0	1.0	All WSID algorithms need some addition in order to be evaluated.	0
148	2118	2118	S07-2	Evaluating WSID systems	2	26	1.0	1.0	One alternative is to manually decide the correctness of the clusters assigned to each occurrence of the words.	0
149	2119	2119	S07-2	Evaluating WSID systems	3	27	1.0	1.0	This approach has two main disadvantages.	0
150	2120	2120	S07-2	Evaluating WSID systems	4	28	1.0	1.0	First, it is expensive to manually verify each occurrence of the word, and different runs of the algorithm need to be evaluated in turn.	0
151	2121	2121	S07-2	Evaluating WSID systems	5	29	1.0	1.0	Second, it is not an easy task to manually decide if an occurrence of a word effectively corresponds with the use of the word the assigned cluster refers to, especially considering that the person is given a short list of words linked to the cluster.	0
152	2122	2122	S07-2	Evaluating WSID systems	6	30	1.0	1.0	We also think that instead of judging whether the cluster returned by the algorithm is correct, the person should have independently tagged the occurrence with his own senses, which should have been then compared to the cluster returned by the system.	0
153	2123	2123	S07-2	Evaluating WSID systems	7	31	2.0	1.0	This is paramount to compare a corpus which has been hand-tagged with some reference senses (also known as the gold-standard) with the clustering result.	0
154	2124	2124	S07-2	Evaluating WSID systems	8	32	2.0	1.0	The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes.	0
155	2125	2125	S07-2	Evaluating WSID systems	9	33	2.0	1.0	A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon.	0
156	2126	2126	S07-2	Evaluating WSID systems	10	34	2.0	1.0	Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping.	0
157	2127	2127	S07-2	Evaluating WSID systems	11	35	2.0	1.0	More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004;	0
158	2128	2128	S07-2	Evaluating WSID systems	12	36	2.0	1.0	Niu et al., 2005).	0
159	2129	2129	S07-2	Evaluating WSID systems	13	37	3.0	1.0	A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Schütze, 1998).	0
160	2130	2130	S07-2	Evaluating WSID systems	14	38	3.0	1.0	This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance.	0
161	2131	2131	S07-2	Evaluating WSID systems	15	39	3.0	1.0	In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind.	0
162	2132	2132	S07-2	Evaluating WSID systems	16	40	3.0	1.0	With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solution on it.	0
163	2133	2133	S07-2	Evaluating WSID systems	17	41	3.0	2.0	We evaluate the results according to the following types of evaluation:	0
164	2134	2134	S07-2	Evaluating WSID systems	18	42	3.0	2.0	1. Evaluate the induced senses as clusters of examples.	0
165	2135	2135	S07-2	Evaluating WSID systems	19	43	4.0	2.0	The induced clusters are compared to the sets of examples tagged with the given gold standard word senses (classes), and evaluated using the FScore measure for clusters.	0
166	2136	2136	S07-2	Evaluating WSID systems	20	44	4.0	2.0	We will call this evaluation unsupervised.	0
167	2137	2137	S07-2	Evaluating WSID systems	21	45	4.0	2.0	2. Map the induced senses to gold standard senses, and use the mapping to tag the test corpus with gold standard tags.	0
168	2138	2138	S07-2	Evaluating WSID systems	22	46	4.0	2.0	The mapping is automatically produced by the organizers, and the resulting results evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.	0
169	2139	2139	S07-2	Evaluating WSID systems	23	47	4.0	2.0	We call this evaluation supervised.	0
170	2140	2140	S07-2	Evaluating WSID systems	24	48	4.0	2.0	We will see each of them in turn.	0
171	2141	2141	S07-2	Unsupervised evaluation	1	49	1.0	2.0	In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes.	0
172	2142	2142	S07-2	Unsupervised evaluation	2	50	1.0	2.0	In order to compare the clusters with the classes, hand annotated corpora is needed.	0
173	2143	2143	S07-2	Unsupervised evaluation	3	51	1.0	2.0	The test set is first tagged with the induced senses.	0
174	2144	2144	S07-2	Unsupervised evaluation	4	52	1.0	2.0	A perfect clustering solution will be the one where each cluster has exactly the same examples as one of the classes, and vice versa.	0
175	2145	2145	S07-2	Unsupervised evaluation	5	53	2.0	2.0	Following standard cluster evaluation practice (Zhao and Karypis, 2005), we consider the FScore measure for measuring the performance of the systems.	0
176	2146	2146	S07-2	Unsupervised evaluation	6	54	2.0	2.0	"The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly ""retrieved"" examples for a cluster (divided by total cluster size), and recall as the percentage of correctly ""retrieved"" examples for a cluster (divided by total class size)."	0
177	2147	2147	S07-2	Unsupervised evaluation	7	55	2.0	2.0	Given a particular class s r of size n r and a cluster h i of size n i , suppose n i r examples in the class s r belong to h i .	0
178	2148	2148	S07-2	Unsupervised evaluation	8	56	2.0	2.0	The F value of this class and cluster is defined to be:	0
179	2149	2149	S07-2	Unsupervised evaluation	9	57	2.0	2.0	where P (s r , h i ) = n i r nr is the precision value and R(s r , h i ) = n i r n i is the recall value defined for class s r and cluster h i .	0
180	2150	2150	S07-2	Unsupervised evaluation	10	58	3.0	2.0	The FScore of class s r is the maximum F value attained at any cluster, that is,	0
181	2151	2151	S07-2	Unsupervised evaluation	11	59	3.0	2.0	and the FScore of the entire clustering solution is:	0
182	2152	2152	S07-2	Unsupervised evaluation	12	60	3.0	2.0	where q is the number of classes and n is the size of the clustering solution.	0
183	2153	2153	S07-2	Unsupervised evaluation	13	61	3.0	2.0	If the clustering is the identical to the original classes in the datasets, FScore will be equal to one which means that the higher the FScore, the better the clustering is.	0
184	2154	2154	S07-2	Unsupervised evaluation	14	62	3.0	2.0	For the sake of completeness we also include the standard entropy and purity measures in the unsupervised evaluation.	0
185	2155	2155	S07-2	Unsupervised evaluation	15	63	4.0	2.0	The entropy measure considers how the various classes of objects are distributed within each cluster.	0
186	2156	2156	S07-2	Unsupervised evaluation	16	64	4.0	2.0	In general, the smaller the entropy value, the better the clustering algorithm performs.	0
187	2157	2157	S07-2	Unsupervised evaluation	17	65	4.0	2.0	The purity measure considers the extent to which each cluster contained objects from primarily one class.	0
188	2158	2158	S07-2	Unsupervised evaluation	18	66	4.0	2.0	The larger the values of purity, the better the clustering algorithm performs.	0
189	2159	2159	S07-2	Unsupervised evaluation	19	67	4.0	2.0	For a formal definition refer to (Zhao and Karypis, 2005).	0
190	2160	2160	S07-2	Supervised evaluation	1	68	1.0	2.0	We have followed the supervised evaluation framework for evaluating WSID systems as described in (Agirre et al., 2006).	0
191	2161	2161	S07-2	Supervised evaluation	2	69	1.0	2.0	First, we split the corpus into a train/test part.	0
192	2162	2162	S07-2	Supervised evaluation	3	70	2.0	2.0	Using the hand-annotated sense information in the train part, we compute a mapping matrix M that relates clusters and senses in the following way.	0
193	2163	2163	S07-2	Supervised evaluation	4	71	2.0	2.0	Suppose there are m clusters and n senses for the target word.	0
194	2164	2164	S07-2	Supervised evaluation	5	72	2.0	2.0	Then, M = {m ij } 1 ≤ i ≤ m, 1 ≤ j ≤ n, and each m ij = P (s j |h i ), that is, m ij is the probability of a word having sense j given that it has been assigned cluster i.	0
195	2165	2165	S07-2	Supervised evaluation	6	73	3.0	2.0	This probability can be computed counting the times an occurrence with sense s j has been assigned cluster h i in the train corpus.	0
196	2166	2166	S07-2	Supervised evaluation	7	74	3.0	2.0	The mapping matrix is used to transform any cluster score vectorh = (h 1 , . . . , h m ) returned by the WSID algorithm into a sense score vectors = (s 1 , . . . , s n ).	0
197	2167	2167	S07-2	Supervised evaluation	8	75	4.0	2.0	It suffices to multiply the score vector by M , i.e.,s =hM .	0
198	2168	2168	S07-2	Supervised evaluation	9	76	4.0	2.0	We use the M mapping matrix in order to convert the cluster score vector of each test corpus instance into a sense score vector, and assign the sense with  maximum score to that instance.	0
199	2169	2169	S07-2	Supervised evaluation	10	77	4.0	2.0	Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.	0
200	2170	2170	S07-2	Results	1	78	2.0	2.0	In this section we will introduce the gold standard and corpus used, the description of the systems and the results obtained.	0
201	2171	2171	S07-2	Results	2	79	4.0	2.0	Finally we provide some material for discussion.	0
202	2172	2172	S07-2	Gold Standard	1	80	1.0	2.0	"The data used for the actual evaluation was borrowed from the SemEval-2007 ""English lexical sample subtask"" of task 17."	0
203	2173	2173	S07-2	Gold Standard	2	81	2.0	2.0	The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al., 2006).	0
204	2174	2174	S07-2	Gold Standard	3	82	2.0	3.0	Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case.	0
205	2175	2175	S07-2	Gold Standard	4	83	3.0	3.0	Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears.	0
206	2176	2176	S07-2	Gold Standard	5	84	3.0	3.0	After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants.	0
207	2177	2177	S07-2	Gold Standard	6	85	4.0	3.0	Participants had to tag with the induced senses all the examples in this corpus.	0
208	2178	2178	S07-2	Gold Standard	7	86	4.0	3.0	Table 1 summarizes the size of the corpus.	0
209	2179	2179	S07-2	Participant systems	1	87	1.0	3.0	In total there were 6 participant systems.	0
210	2180	2180	S07-2	Participant systems	2	88	2.0	3.0	One of them (UoFL) was not a sense induction system, but rather a knowledge-based WSD system.	0
211	2181	2181	S07-2	Participant systems	3	89	3.0	3.0	We include their data in the results section below for coherence with the official results submitted to participants, but we will not mention it here.	0
212	2182	2182	S07-2	Participant systems	4	90	4.0	3.0	I2R:	0
213	2183	2183	S07-2	Participant systems	5	91	4.0	3.0	This team used a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of this target word into the estimated number of clusters using the sequential Information Bottleneck algorithm.	0
214	2184	2184	S07-2	UBC-AS:	1	92	1.0	3.0	A two stage graph-based clustering where a co-occurrence graph is used to compute similarities against contexts.	0
215	2185	2185	S07-2	UBC-AS:	2	93	1.0	3.0	The context similarity matrix is pruned and the resulting associated graph is clustered by means of a random-walk type algorithm.	0
216	2186	2186	S07-2	UBC-AS:	3	94	1.0	3.0	The parameters of the system are tuned against the Senseval-3 lexical sample dataset, and some manual tuning is performed in order to reduce the overall number of induced senses.	0
217	2187	2187	S07-2	UBC-AS:	4	95	1.0	3.0	Note that this system was submitted by the organizers.	0
218	2188	2188	S07-2	UBC-AS:	5	96	2.0	3.0	The organizers took great care in order to participate under the same conditions as the rest of participants.	0
219	2189	2189	S07-2	UBC-AS:	6	97	2.0	3.0	UMND2: A system which clusters the second order co-occurrence vectors associated with each word in a context.	0
220	2190	2190	S07-2	UBC-AS:	7	98	2.0	3.0	Clustering is done using k-means and the number of clusters was automatically discovered using the Adapted Gap Statistic.	0
221	2191	2191	S07-2	UBC-AS:	8	99	2.0	3.0	No parameter tuning is performed.	0
222	2192	2192	S07-2	UBC-AS:	9	100	3.0	3.0	upv si: A self-term expansion method based on co-ocurrence, where the terms of the corpus are expanded by its best co-ocurrence terms in the same corpus.	0
223	2193	2193	S07-2	UBC-AS:	10	101	3.0	3.0	The clustering is done using one implementation of the KStar method where the stop criterion has been modified.	0
224	2194	2194	S07-2	UBC-AS:	11	102	3.0	3.0	The trial data was used for determining the corpus structure.	0
225	2195	2195	S07-2	UBC-AS:	12	103	3.0	3.0	No further tuning is performed.	0
226	2196	2196	S07-2	UBC-AS:	13	104	4.0	3.0	UOY: A graph based system which creates a cooccurrence hypergraph model.	0
227	2197	2197	S07-2	UBC-AS:	14	105	4.0	3.0	The hypergraph is filtered and weighted according to some association rules.	0
228	2198	2198	S07-2	UBC-AS:	15	106	4.0	3.0	The clustering is performed by selecting the nodes of higher degree until a stop criterion is reached.	0
229	2199	2199	S07-2	UBC-AS:	16	107	4.0	3.0	WSD is performed by assigning to each induced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word.	0
230	2200	2200	S07-2	UBC-AS:	17	108	4.0	3.0	The system was tested and tuned on 10 nouns of Senseval-3 lexical-sample.	0
231	2201	2201	S07-2	Official Results	1	109	1.0	3.0	Participants were required to induce the senses of the target words and cluster all target word contexts accordingly 2 . Table 2 summarizes the average number of induced senses as well as the real senses in the gold standard.	0
232	2202	2202	S07-2	Official Results	2	110	1.0	3.0	2	0
233	2203	2203	S07-2	Official Results	3	111	1.0	3.0	They were allowed to label each context with a weighted score vector, assigning a weight to each induced sense.	0
234	2204	2204	S07-2	Official Results	4	112	1.0	3.0	In the unsupervised evaluation only the sense with maximum weight was considered, but for the supervised one the whole score vector was used.	0
235	2205	2205	S07-2	Official Results	5	113	1.0	3.0	However, none of the participating systems labeled any instance with more than one sense.	0
236	2206	2206	S07-2	Official Results	6	114	2.0	3.0	Table 3 shows the unsupervised evaluation of the systems on the test corpus.	0
237	2207	2207	S07-2	Official Results	7	115	2.0	3.0	"We also include three baselines: the ""one cluster per word"" baseline (1c1word), which groups all instances of a word into a single cluster, the ""one cluster per instance"" baseline (1c1inst), where each instance is a distinct cluster, and a random baseline, where the induced word senses and their associated weights have been randomly produced."	0
238	2208	2208	S07-2	Official Results	8	116	2.0	3.0	The random baseline figures in this paper are averages over 10 runs.	0
239	2209	2209	S07-2	Official Results	9	117	2.0	3.0	As shown in Table 3, no system outperforms the 1c1word baseline, which indicates that this baseline is quite strong, perhaps due the relatively small number of classes in the gold standard.	0
240	2210	2210	S07-2	Official Results	10	118	2.0	3.0	However, all systems outperform by far the random and 1c1inst baselines, meaning that the systems are able to induce correct senses.	0
241	2211	2211	S07-2	Official Results	11	119	2.0	3.0	Note that the purity and entropy measures are not very indicative in this setting.	0
242	2212	2212	S07-2	Official Results	12	120	3.0	3.0	For completeness, we also computed the FScore using the complete corpus (both train and test).	0
243	2213	2213	S07-2	Official Results	13	121	3.0	3.0	The results are similar and the ranking is the same.	0
244	2214	2214	S07-2	Official Results	14	122	3.0	3.0	We omit them for brevity.	0
245	2215	2215	S07-2	Official Results	15	123	3.0	4.0	The results of the supervised evaluation can be seen in Table 4.	0
246	2216	2216	S07-2	Official Results	16	124	3.0	4.0	The evaluation is also performed over the test corpus.	0
247	2217	2217	S07-2	Official Results	17	125	4.0	4.0	Apart from participants, we also show the most frequent sense (MFS), which tags every test instance with the sense that occurred most often in the training part.	0
248	2218	2218	S07-2	Official Results	18	126	4.0	4.0	Note that the supervised evaluation combines the information in the clustering solution implicitly with the MFS information via the mapping in the training part.	0
249	2219	2219	S07-2	Official Results	19	127	4.0	4.0	Previous Senseval evaluation exercises have shown that the MFS baseline is very hard to beat by unsupervised systems.	0
250	2220	2220	S07-2	Official Results	20	128	4.0	4.0	In fact, only three of the participant systems are above the MFS baseline, which shows that the clustering information carries over the mapping successfully for these systems.	0
251	2221	2221	S07-2	Official Results	21	129	4.0	4.0	Note that the 1c1word baseline is equivalent to MFS in this setting.	0
252	2222	2222	S07-2	Official Results	22	130	4.0	4.0	We will review the random baseline in the discussion section below.	0
253	2223	2223	S07-2	Further Results	1	131	1.0	4.0	Table 5 shows the results of the best systems from the lexical sample subtask of task 17.	0
254	2224	2224	S07-2	Further Results	2	132	1.0	4.0	The best sense induction system is only 6.9 percentage points below the best supervised, and 3.5 percentage points below the best (and only) semi-supervised system.	0
255	2225	2225	S07-2	Further Results	3	133	1.0	4.0	If the sense induction system had participated, it would be deemed as semi-supervised, as it uses, albeit in a shallow way, the training data for mapping the clusters into senses.	0
256	2226	2226	S07-2	Further Results	4	134	2.0	4.0	In this sense, our supervised evaluation does not seek to optimize the available training data.	0
257	2227	2227	S07-2	Further Results	5	135	2.0	4.0	After the official evaluation, we realized that contrary to previous lexical sample evaluation exercises task 17 organizers did not follow a random train/test split.	0
258	2228	2228	S07-2	Further Results	6	136	2.0	4.0	We decided to produce a random train/test split following the same 82/18 proportion as the official split, and re-evaluated the systems.	0
259	2229	2229	S07-2	Further Results	7	137	3.0	4.0	The results are presented in   participants are above the MFS baseline, showing that all of them learned useful clustering information.	0
260	2230	2230	S07-2	Further Results	8	138	3.0	4.0	Note that UOY was specially affected by the original split.	0
261	2231	2231	S07-2	Further Results	9	139	3.0	4.0	The distribution of senses in this split did not vary (cf. Table 2).	0
262	2232	2232	S07-2	Further Results	10	140	4.0	4.0	Finally, we also studied the supervised evaluation of several random clustering algorithms, which can attain performances close to MFS, thanks to the mapping information.	0
263	2233	2233	S07-2	Further Results	11	141	4.0	4.0	This is due to the fact that the random clusters would be mapped to the most frequent senses.	0
264	2234	2234	S07-2	Further Results	12	142	4.0	4.0	Table 7 shows the results of random solutions using varying numbers of clusters (e.g. random2 is a random choice between two clusters).	0
265	2235	2235	S07-2	Further Results	13	143	4.0	4.0	Random2 is only 0.1 below MFS, but as the number of clusters increases some clusters don't get mapped, and the recall of the random baselines decrease.	0
266	2236	2236	S07-2	Discussion	1	144	1.0	4.0	The evaluation of clustering solutions is not straightforward.	0
267	2237	2237	S07-2	Discussion	2	145	1.0	4.0	All measures have some bias towards certain clustering strategy, and this is one of the reasons of adding the supervised evaluation as a complementary information to the more standard unsupervised evaluation.	0
268	2238	2238	S07-2	Discussion	3	146	2.0	4.0	In our case, we noticed that the FScore penalized the systems with a high number of clusters, and favored those that induce less senses.	0
269	2239	2239	S07-2	Discussion	4	147	2.0	4.0	Given the fact that FScore tries to balance precision (higher for large numbers of clusters) and recall (higher for small numbers of clusters), this was not expected.	0
270	2240	2240	S07-2	Discussion	5	148	2.0	4.0	"We were also surprised to see that no system could System Supervised evaluation random2 78.6 random10 77.6 ramdom100 64.2 random1000 31.8 beat the ""one cluster one word"" baseline."	0
271	2241	2241	S07-2	Discussion	6	149	3.0	4.0	An explanation might lay in that the gold-standard was based on the coarse-grained OntoNotes senses.	0
272	2242	2242	S07-2	Discussion	7	150	3.0	4.0	We also noticed that some words had hundreds of instances and only a single sense.	0
273	2243	2243	S07-2	Discussion	8	151	3.0	4.0	We suspect that the participating systems would have beaten all baselines if a fine-grained sense inventory like WordNet had been used, as was customary in previous WSD evaluation exercises.	0
274	2244	2244	S07-2	Discussion	9	152	4.0	4.0	Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.	0
275	2245	2245	S07-2	Discussion	10	153	4.0	4.0	Each of the induced clusters is mapped into a weighted vector of senses, and thus inducing a number of clusters similar to the number of senses is not a requirement for good results.	0
276	2246	2246	S07-2	Discussion	11	154	4.0	4.0	With this measure some of the systems 3 are able to beat all baselines.	0
277	2247	2247	S07-2	Conclusions	1	155	1.0	4.0	We have presented the design and results of the SemEval-2007 task 02 on evaluating word sense induction and discrimination systems.	0
278	2248	2248	S07-2	Conclusions	2	156	1.0	4.0	6 systems participated, but one of them was not a sense induction system.	0
279	2249	2249	S07-2	Conclusions	3	157	2.0	4.0	We reused the data from the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the training part of the dataset for mapping).	0
280	2250	2250	S07-2	Conclusions	4	158	2.0	4.0	We also provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.	0
281	2251	2251	S07-2	Conclusions	5	159	3.0	4.0	Evaluating clustering solutions is not straightforward.	0
282	2252	2252	S07-2	Conclusions	6	160	3.0	4.0	The unsupervised evaluation seems to be sensitive to the number of senses in the gold standard, and the coarse grained sense inventory used in the gold standard had a great impact in the results.	0
283	2253	2253	S07-2	Conclusions	7	161	4.0	4.0	The supervised evaluation introduces a mapping step which interacts with the clustering solution.	0
284	2254	2254	S07-2	Conclusions	8	162	4.0	4.0	In fact, the ranking of the participating systems 3 All systems in the case of a random train/test split varies according to the evaluation method used.	0
285	2255	2255	S07-2	Conclusions	9	163	4.0	4.0	We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systems needs further developments, perhaps linked to a certain application or purpose.	0
286	2815	2815	S07-9	title	1	1	4.0	1.0	SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish	0
287	2816	2816	S07-9	abstract	1	2	2.0	1.0	In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish).	0
288	2817	2817	S07-9	abstract	2	3	3.0	1.0	In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish.	1
289	2818	2818	S07-9	abstract	3	4	4.0	1.0	Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.	0
290	2819	2819	S07-9	Introduction	1	5	1.0	1.0	The Multilevel Semantic Annotation of Catalan and Spanish task is split into the following three subtasks:	0
291	2820	2820	S07-9	Introduction	2	6	1.0	1.0	Noun Sense Disambiguation (NSD):	0
292	2821	2821	S07-9	Introduction	3	7	1.0	1.0	"Disambiguation of all frequent nouns (""all words"" style)."	0
293	2822	2822	S07-9	Introduction	4	8	2.0	1.0	Named Entity Recognition (NER):	0
294	2823	2823	S07-9	Introduction	5	9	2.0	1.0	The annotation of (possibly embedding) named entities with basic entity types.	0
295	2824	2824	S07-9	Introduction	6	10	2.0	1.0	Semantic Role Labeling (SRL):	0
296	2825	2825	S07-9	Introduction	7	11	2.0	1.0	Including also two subtasks, i.e., the annotation of verbal predicates with semantic roles (SR), and verb tagging with semantic-class labels (SC).	0
297	2826	2826	S07-9	Introduction	8	12	3.0	1.0	All semantic annotation tasks are performed on exactly the same corpora for each language.	0
298	2827	2827	S07-9	Introduction	9	13	3.0	1.0	We presented all the annotation levels together as a complex global task, since we were interested in approaches which address these problems jointly, possibly taking into account cross-dependencies among them.	0
299	2828	2828	S07-9	Introduction	10	14	3.0	1.0	However, we were also accepting systems approaching the annotation in a pipeline style, or ad-dressing any of the particular subtasks in any of the languages.	0
300	2829	2829	S07-9	Introduction	11	15	4.0	1.0	In Section 2 we describe the methodology followed to develop the linguistic corpora for the task.	0
301	2830	2830	S07-9	Introduction	12	16	4.0	1.0	Sections 3 and 4 summarize the task setting and the participant systems, respectively.	0
302	2831	2831	S07-9	Introduction	13	17	4.0	1.0	Finally, Section 5 presents a comparative analysis of the results.	0
303	2832	2832	S07-9	Introduction	14	18	4.0	1.0	For any additional information on corpora, resources, formats, tagsets, annotation manuals, etc. we refer the reader to the official website of the task 1 .	0
304	2833	2833	S07-9	Linguistic corpora	1	19	1.0	1.0	The corpora used in this SemEval task are a subset of CESS-ECE, a multilingual Treebank, composed of a Spanish (CESS-ESP) and a Catalan (CESS-CAT) corpus of 500K words each (Martí et al., 2007b).	0
305	2834	2834	S07-9	Linguistic corpora	2	20	2.0	1.0	These corpora were enriched with different kinds of semantic information: argument structure, thematic roles, semantic class, named entities, and WordNet synsets for the 150 most frequent nouns.	0
306	2835	2835	S07-9	Linguistic corpora	3	21	3.0	1.0	The annotation process was carried out in a semiautomatic way, with a posterior manual revision of all automatic processes.	0
307	2836	2836	S07-9	Linguistic corpora	4	22	4.0	1.0	A sequential approach was adopted for the annotation of the corpus, beginning with the basic levels of analysis, i.e., POS tagging and chunking (automatically performed) and followed by the more complex levels: syntactic constituents and functions (manually tagged) and semantic annotation (manual and semiautomatic processes with manual completion and posterior revision).	0
308	2837	2837	S07-9	Linguistic corpora	5	23	4.0	1.0	Furthermore, some experiments concerning inter-annotator agreement were carried out at the syntactic (Civit et al., 2003) and semantic levels (Màrquez et al., 2004) in order to evaluate the quality of the results.	0
309	2838	2838	S07-9	Syntactic Annotation	1	24	1.0	1.0	The syntactic annotation consists of the labeling of constituents, including elliptical subjects, and syntactic functions.	0
310	2839	2839	S07-9	Syntactic Annotation	2	25	2.0	1.0	The surface order was maintained and only those constituents directly attached to any kind of 'Sentence' root node were considered ('S', 'S.NF', 'S.F', 'S*').	0
311	2840	2840	S07-9	Syntactic Annotation	3	26	3.0	1.0	The syntactic functions are: subject (SUJ), direct object (OD), indirect object (OI), attribute (ATR), predicative (CPRED), agent complement (CAG), and adjunct (CC).	0
312	2841	2841	S07-9	Syntactic Annotation	4	27	4.0	1.0	Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive any thematic role.	0
313	2842	2842	S07-9	Lexical Semantic Information: WordNet	1	28	1.0	1.0	We selected the 150 most frequent nouns in the whole corpus and annotated their occurrences with WordNet synsets.	0
314	2843	2843	S07-9	Lexical Semantic Information: WordNet	2	29	2.0	1.0	No other word categories were treated (verbs, adjectives and adverbs).	0
315	2844	2844	S07-9	Lexical Semantic Information: WordNet	3	30	3.0	1.0	We used a steady version of Catalan and Spanish WordNets, linked to WordNet 1.6.	0
316	2845	2845	S07-9	Lexical Semantic Information: WordNet	4	31	4.0	1.0	Each noun either matched a WordNet synset or a special label indicating a specific circumstance (for instance, the tag C2S indicates that the word does not appear in the dictionary).	0
317	2846	2846	S07-9	Lexical Semantic Information: WordNet	5	32	4.0	1.0	All this process was carried out manually.	0
318	2847	2847	S07-9	Named Entities	1	33	1.0	2.0	The corpora were annotated with both strong and weak Named Entities.	0
319	2848	2848	S07-9	Named Entities	2	34	1.0	2.0	"Strong NEs correspond to single lexical tokens (e.g., ""[U.S.] LOC ""), while weak NEs include, by definition, some strong entities (e.g., ""The [president of [US] LOC ] P ER "")."	0
320	2849	2849	S07-9	Named Entities	3	35	2.0	2.0	(Arévalo et al., 2004).	0
321	2850	2850	S07-9	Named Entities	4	36	2.0	2.0	Thus, NEs may embed.	0
322	2851	2851	S07-9	Named Entities	5	37	3.0	2.0	Six basic semantic categories were distinguished: Person, Organization, Location, Date, Numerical expression, and Others (Borrega et al., 2007).	0
323	2852	2852	S07-9	Named Entities	6	38	3.0	2.0	Two golden rules underlie the definition of NEs in Spanish and Catalan.	0
324	2853	2853	S07-9	Named Entities	7	39	4.0	2.0	On the one hand, only a noun phrase can be a NE.	0
325	2854	2854	S07-9	Named Entities	8	40	4.0	2.0	On the other hand, its referent must be unique and unambiguous.	0
326	2855	2855	S07-9	Named Entities	9	41	4.0	2.0	Finally, another hard rule (although not 100% reliable) is that only a definite singular noun phrase might be a NE.	0
327	2856	2856	S07-9	Thematic Role Labeling / Semantic Class	1	42	1.0	2.0	Basic syntactic functions were tagged with both arguments and thematic roles, taking into account the semantic class related to the verbal predicate (Taulé et al., 2006b).	0
328	2857	2857	S07-9	Thematic Role Labeling / Semantic Class	2	43	1.0	2.0	We characterized predicates by means of a limited number of Semantic Classes based on Event Structure Patterns, according to four basic event classes: states, activities, accomplishments, and achievements.	0
329	2858	2858	S07-9	Thematic Role Labeling / Semantic Class	3	44	2.0	2.0	These general classes were split into 17 subclasses, depending on thematic roles and diathesis alternations.	0
330	2859	2859	S07-9	Thematic Role Labeling / Semantic Class	4	45	2.0	2.0	Similar to PropBank, the set of arguments selected by the verb are incrementally numbered expressing the degree of proximity of an argument in relation to the verb (Arg0, Arg1, Arg2, Arg3, Arg4).	0
331	2860	2860	S07-9	Thematic Role Labeling / Semantic Class	5	46	2.0	2.0	In our proposal, each argument includes the thematic role in its label (e.g., Arg1-PAT).	0
332	2861	2861	S07-9	Thematic Role Labeling / Semantic Class	6	47	3.0	2.0	Thus, we have two different levels of semantic description: the argument position and the specific thematic role.	0
333	2862	2862	S07-9	Thematic Role Labeling / Semantic Class	7	48	3.0	2.0	This information was previously stored in a verbal lexicon for each language.	0
334	2863	2863	S07-9	Thematic Role Labeling / Semantic Class	8	49	4.0	2.0	In these lexicons, a semantic class was established for each verbal sense, and the mapping between their syntactic functions with the corresponding argument structure and thematic roles was declared.	0
335	2864	2864	S07-9	Thematic Role Labeling / Semantic Class	9	50	4.0	2.0	These classes resulted from the analysis of 1,555 verbs from the Spanish corpus and 1,077 from the Catalan.	0
336	2865	2865	S07-9	Thematic Role Labeling / Semantic Class	10	51	4.0	2.0	The annotation process was performed in two steps: firstly, we annotated automatically the unambiguous correspondences between syntactic functions and thematic roles (Martí et al., 2007a); secondly, we manually checked the outcome of the previous process and completed the rest of thematic role assignments.	0
337	2866	2866	S07-9	Subset for SemEval-2007	1	52	1.0	2.0	The corpora extracted from CESS-ECE to conform SemEval-2007 datasets are: (a) SemEval-CESS-ESP (Spanish), made of 101,136 words (3,611 sentences), with 29% of the corpus coming from the Spanish EFE News Agency and 71% coming from Lexesp, a Spanish balanced corpus; (b) SemEval-CESS-CAT (Catalan), consisting of 108,207 words (3,202 sentences), with 71% of the corpus consistinf of Catalan news from EFE News Agency and 29% coming from the Catalan News Agency (ACN).	0
338	2867	2867	S07-9	Subset for SemEval-2007	2	53	2.0	2.0	These corpora were split into training and test subsets following a a 90%-10% proportion.	0
339	2868	2868	S07-9	Subset for SemEval-2007	3	54	3.0	2.0	Each test set was also partitioned into two subsets: 'indomain' and 'out-of-domain' test corpora.	0
340	2869	2869	S07-9	Subset for SemEval-2007	4	55	4.0	2.0	The first is intended to be homogeneous with respect to the training corpus and the second was extracted from a part of the CESS-ECE corpus annotated later and not involved in the development of the resources (e.g., verbal dictionaries).	0
341	2870	2870	S07-9	Subset for SemEval-2007	5	56	4.0	2.0	2	0
342	2871	2871	S07-9	Task setting	1	57	1.0	2.0	Data formats are similar to those of CoNLL-2004/2005 shared tasks on SRL (column style presentation of levels of annotation), in order to be able to share evaluation tools and already developed scripts for format conversion.	0
343	2872	2872	S07-9	Task setting	2	58	1.0	2.0	In Figure 1 you can find an example of a fully annotated sentence in the column-based format.	0
344	2873	2873	S07-9	Task setting	3	59	2.0	2.0	There is one line for each token, and a blank line after the last token of each sentence.	0
345	2874	2874	S07-9	Task setting	4	60	2.0	2.0	The columns, separated by blank spaces, represent different annotations of the sentence with a tagging along words.	0
346	2875	2875	S07-9	Task setting	5	61	3.0	2.0	For structured annotations (parse trees, named entities, and arguments), we use the Start-End format.	0
347	2876	2876	S07-9	Task setting	6	62	3.0	2.0	Columns 1-6 correspond to the input information; columns 7 and above contain the information to be predicted.	0
348	2877	2877	S07-9	Task setting	7	63	4.0	2.0	We can group annotations in five main categories:	0
349	2878	2878	S07-9	Task setting	8	64	4.0	2.0	All these annotations in column format are extracted automatically from the syntactic-semantic trees from the CESS-ECE corpora, which were distributed with the datasets.	0
350	2879	2879	S07-9	Task setting	9	65	4.0	2.0	Participants were also provided with the whole Catalan and Spanish Word-Nets (v1.6), the verbal lexicons used in the role labeling annotation, the annotation guidelines as well as the annotated corpora.	0
351	2880	2880	S07-9	Participant systems	1	66	1.0	3.0	About a dozen teams expressed their interest in the task.	0
352	2881	2881	S07-9	Participant systems	2	67	1.0	3.0	From those, only 5 registered and downloaded datasets, and finally, only two teams met the deadline and submitted results.	0
353	2882	2882	S07-9	Participant systems	3	68	1.0	3.0	ILK2 (Tilburg University) presented a system addressing Semantic Role Labeling, and UPC* (Technical University of Catalonia) presented a system addressing all subtasks independently 3 .	0
354	2883	2883	S07-9	Participant systems	4	69	2.0	3.0	The ILK2 SRL system is based on memory-based classification of syntactic constituents using a rich feature set.	0
355	2884	2884	S07-9	Participant systems	5	70	2.0	3.0	UPC* used several machine learning algorithms for addressing the different subtasks (AdaBoost, SVM, Perceptron).	0
356	2885	2885	S07-9	Participant systems	6	71	2.0	3.0	For SRL, the system implements a re-ranking strategy using global features.	0
357	2886	2886	S07-9	Participant systems	7	72	3.0	3.0	The candidates are generated using a state-of-the-art SRL base system.	0
358	2887	2887	S07-9	Participant systems	8	73	3.0	3.0	Although the task targeted at systems addressing all subtasks jointly none of the participants did it.	0
359	2888	2888	S07-9	Participant systems	9	74	3.0	3.0	4	0
360	2889	2889	S07-9	Participant systems	10	75	4.0	3.0	We believe that the high complexity of the whole task together with the short period of time available were the main reasons for this failure.	0
361	2890	2890	S07-9	Participant systems	11	76	4.0	3.0	From this point of view, the conclusions are somehow disappointing.	0
362	2891	2891	S07-9	Participant systems	12	77	4.0	3.0	However, we think that we have contributed with a very valuable resource for the future research and, although not complete, the current systems provide also valuable insights about the task and are very good baselines for the systems to come.	0
363	2892	2892	S07-9	Evaluation	1	78	1.0	3.0	In the following subsections we present an analysis of the results obtained by participant systems in the INPUT--------------------------------------------------------------&gt; OUTPUT-----------------------------------BASIC_INPUT_INFO-----&gt; EXTRA_INPUT_INFO---------------------------&gt; NE NS-------&gt; SR------------------------	0
364	2893	2893	S07-9	Evaluation	2	79	2.0	3.0	-------------------------------------------------------------------------------------------------------------  three subtasks.	0
365	2894	2894	S07-9	Evaluation	3	80	2.0	3.0	Results on the test set are presented along 2 dimensions: (a) language ('ca'=Catalan; 'es'=Spanish); (b) corpus source ('in'=in-domain corpus; 'out'=out-of-domain corpus).	0
366	2895	2895	S07-9	Evaluation	4	81	3.0	3.0	We will use a language.	0
367	2896	2896	S07-9	Evaluation	5	82	4.0	3.0	source pair to denote a particular test set.	0
368	2897	2897	S07-9	Evaluation	6	83	4.0	3.0	Finally, '*' will denote the addition of the two subcorpora, either in the language or source dimensions.	0
369	2898	2898	S07-9	NSD	1	84	1.0	3.0	"Results on the NSD subtask are presented in  The left part of the table (""all words"") contains results on the complete test sets, while the right part (""selected words"") contains the results restricted to the set of words with trained SVM classifiers."	0
370	2899	2899	S07-9	NSD	2	85	1.0	3.0	This set covers 31.0% of the word occurrences in the training set and 28.2% in the complete test set.	0
371	2900	2900	S07-9	NSD	3	86	2.0	3.0	The main observation is that training/test corpora contain few sense variations.	0
372	2901	2901	S07-9	NSD	4	87	2.0	3.0	Sense distributions are very skewed and, thus, the simple baseline shows a very high accuracy (almost 85%).	0
373	2902	2902	S07-9	NSD	5	88	3.0	3.0	The UPC* system only improves BSL accuracy by one point.	0
374	2903	2903	S07-9	NSD	6	89	3.0	3.0	This can be partly explained by the small size of the wordbased training corpora.	0
375	2904	2904	S07-9	NSD	7	90	4.0	3.0	Also, this improvement is diminished because UPC* only treated a subset of words.	0
376	2905	2905	S07-9	NSD	8	91	4.0	3.0	However, looking at the right-hand side of the table, the improvement over the baseline is still modest (∼3 points) when focusing only on the treated words.	0
377	2906	2906	S07-9	NSD	9	92	4.0	3.0	As a final observation, no significant differences are observed across languages and corpora sources.	0
378	2907	2907	S07-9	NER	1	93	1.0	3.0	Results on the NER subtask are presented in Table 2.	0
379	2908	2908	S07-9	NER	2	94	1.0	3.0	This time, BSL stands for a baseline system consisting of collecting a gazetteer with the strong NEs appearing in the training set and assigning the longest matches of these NEs in the test set.	0
380	2909	2909	S07-9	NER	3	95	1.0	3.0	Weak entities are simply ignored by BSL.	0
381	2910	2910	S07-9	NER	4	96	1.0	3.0	UPC* presented a system which treats strong and weak NEs in a pipeline of two processors.	0
382	2911	2911	S07-9	NER	5	97	1.0	3.0	Classifiers trained with multiclass AdaBoost are used to predict the strong and weak NEs.	0
383	2912	2912	S07-9	NER	6	98	2.0	4.0	See authors' paper for details.	0
384	2913	2913	S07-9	NER	7	99	2.0	4.0	Table 2: Overall results on the NER subtask UPC* system largely overcomes the baseline, mainly due to the low recall of the latter.	0
385	2914	2914	S07-9	NER	8	100	2.0	4.0	By languages, results on Catalan are significantly better than those on Spanish.	0
386	2915	2915	S07-9	NER	9	101	2.0	4.0	We think this is attributable mainly to corpora variations across languages.	0
387	2916	2916	S07-9	NER	10	102	2.0	4.0	"By corpus source, ""in-domain"" results are slightly better, but the difference is small (1.78 points)."	0
388	2917	2917	S07-9	NER	11	103	3.0	4.0	Overall, the results for the NER task are in the mid seventies, a remarkable result given the small training set and the complexity of predicting embedded NEs.	0
389	2918	2918	S07-9	NER	12	104	3.0	4.0	Detailed results on concrete entity types are presented in Table 3 (sorted by decreasing F 1 ).	0
390	2919	2919	S07-9	NER	13	105	3.0	4.0	As expected, DAT and NUM are the easiest entities to recognize since they can be easily detected by simple patterns and POS tags.	0
391	2920	2920	S07-9	NER	14	106	3.0	4.0	On the contrary, entity types requiring more semantic information present fairly lower results.	0
392	2921	2921	S07-9	NER	15	107	3.0	4.0	ORG PER and LOC are in the seventies, while OTH is by far the most difficult class, showing a very low recall.	0
393	2922	2922	S07-9	NER	16	108	4.0	4.0	This is not surprising since OTH agglutinates a wide variety of entity cases which are difficult to characterize as a whole.	0
394	2923	2923	S07-9	NER	17	109	4.0	4.0	Another interesting analysis is to study the differences between strong and weak entities (see Table 4) .	0
395	2924	2924	S07-9	NER	18	110	4.0	4.0	Contrary to our first expectations, results on weak entities are much better (up to 11 F 1 points higher).	0
396	2925	2925	S07-9	NER	19	111	4.0	4.0	Weak NEs are simpler for two reasons: (a) there exist simple patters to characterize them, with-out the need of fully recognizing their internal strong NEs; (b) there is some redundancy in the corpus when tagging many equivalent weak NEs in embedded noun phrases.	0
397	2926	2926	S07-9	NER	20	112	4.0	4.0	"It is worth noting that the low results for strong NEs come from classification rather than recognition (recognition is almost 100% given the ""proper noun"" PoS tag), thus the recall for weak entities is not diminished by the errors in strong entity classification."	0
398	2927	2927	S07-9	SRL	1	113	1.0	4.0	SRL is the most complex and interesting problem in the task.	0
399	2928	2928	S07-9	SRL	2	114	1.0	4.0	We had two participants ILK2 and UPC*, which participated in both subproblems, i.e., labeling arguments of verbal predicates with thematic roles (SR), and assigning semantic class labels to target verbs (SC).	0
400	2929	2929	S07-9	SRL	3	115	1.0	4.0	Detailed results of the two systems are presented in Tables 5 and 6.	0
401	2930	2930	S07-9	SRL	4	116	1.0	4.0	The ILK2 system outperforms UPC* in both SR and SC.	0
402	2931	2931	S07-9	SRL	5	117	2.0	4.0	For SR, both systems use a traditional architecture of labeling syntactic tree nodes with thematic roles using supervised classifiers.	0
403	2932	2932	S07-9	SRL	6	118	2.0	4.0	We would attribute the overall F 1 difference (2.68 points) to a better feature engineering by ILK2, rather than to differences in the Machine Learning techniques used.	0
404	2933	2933	S07-9	SRL	7	119	2.0	4.0	Overall results in the eighties are remarkably high given the training set size and the granularity of the thematic roles (though we have to take into account that systems work with gold parse trees).	0
405	2934	2934	S07-9	SRL	8	120	2.0	4.0	"Again, the results are comparable across languages and slightly better in the ""in-domain"" test set."	0
406	2935	2935	S07-9	SRL	9	121	2.0	4.0	In the SC subproblem, the differences are similar (2.60 points).	0
407	2936	2936	S07-9	SRL	10	122	3.0	4.0	In this case, ILK2 trained specialized classifiers for the task, while UPC* used heuristics based on the SR outcomes.	0
408	2937	2937	S07-9	SRL	11	123	3.0	4.0	As a reference, the baseline consisting of tagging each verb with its most frequent semantic class achieves F 1 values of 64.01, 63.97, 41.00, and 57.42 on ca.in, ca.out, es.in, es.out, respectively.	0
409	2938	2938	S07-9	SRL	12	124	3.0	4.0	Now, the results are significantly better in Catalan, and, surprisingly, the 'out' test corpora makes F 1 to raise.	0
410	2939	2939	S07-9	SRL	13	125	3.0	4.0	The latter is an anomalous situation provoked by the 'es.in' tset.	0
411	2940	2940	S07-9	SRL	14	126	4.0	4.0	5 Table 7 shows the global SR results by numbered arguments and adjuncts Interestingly, tagging adjuncts is far more difficult than tagging core arguments (this result was also observed for English in previous works).	0
412	2941	2941	S07-9	SRL	15	127	4.0	4.0	Moreover, the global difference between ILK2 and UPC* systems is explained by their ability to tag adjuncts (70.22 vs. 58.37).	0
413	2942	2942	S07-9	SRL	16	128	4.0	4.0	In the core arguments both systems are tied.	0
414	2943	2943	S07-9	SRL	17	129	4.0	4.0	Also in the same table we can see the overall results on a simplified SR setting, in which the thematic roles are eliminated from the SR labels keeping only the argument number (like other evaluations on PropBank).	0
415	2944	2944	S07-9	SRL	18	130	4.0	4.0	The results are only ∼2 points higher in this setting.	0
416	5272	5272	S10-12	title	1	1	4.0	1.0	SemEval-2010 Task 12: Parser Evaluation using Textual Entailments	0
417	5273	5273	S10-12	abstract	1	2	2.0	1.0	Parser Evaluation using Textual Entailments (PETE) is a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evaluation.	0
418	5274	5274	S10-12	abstract	2	3	3.0	1.0	The task involves recognizing textual entailments based on syntactic information alone.	1
419	5275	5275	S10-12	abstract	3	4	4.0	1.0	PETE introduces a new parser evaluation scheme that is formalism independent, less prone to annotation error, and focused on semantically relevant distinctions.	0
420	5276	5276	S10-12	Introduction	1	5	1.0	1.0	Parser Evaluation using Textual Entailments (PETE) is a shared task that involves recognizing textual entailments based on syntactic information alone.	1
421	5277	5277	S10-12	Introduction	2	6	1.0	1.0	"Given two text fragments called ""text"" and ""hypothesis"", textual entailment recognition is the task of determining whether the meaning of the hypothesis is entailed (can be inferred) from the text."	0
422	5278	5278	S10-12	Introduction	3	7	1.0	1.0	In contrast with general RTE tasks (Dagan et al., 2009) the PETE task focuses on syntactic entailments:	0
423	5279	5279	S10-12	Introduction	4	8	1.0	1.0	Text:	0
424	5280	5280	S10-12	Introduction	5	9	1.0	1.0	The man with the hat was tired.	0
425	5281	5281	S10-12	Introduction	6	10	1.0	1.0	Hypothesis-1: The man was tired.	0
426	5282	5282	S10-12	Introduction	7	11	1.0	1.0	(yes) Hypothesis-2: The hat was tired.	0
427	5283	5283	S10-12	Introduction	8	12	1.0	1.0	(no) PETE is an evaluation scheme based on a natural human linguistic competence (i.e. the ability to comprehend sentences and answer simple yes/no questions about them).	0
428	5284	5284	S10-12	Introduction	9	13	1.0	1.0	We believe systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
429	5285	5285	S10-12	Introduction	10	14	1.0	1.0	The PARSEVAL measures introduced nearly two decades ago (Black et al., 1991) still dominate the field of parser evaluation.	0
430	5286	5286	S10-12	Introduction	11	15	1.0	1.0	"These methods compare phrase-structure bracketings produced by the parser with bracketings in the annotated corpus, or ""treebank""."	0
431	5287	5287	S10-12	Introduction	12	16	1.0	1.0	Parser evaluation using short textual entailments has the following advantages compared to treebank based evaluation.	0
432	5288	5288	S10-12	Introduction	13	17	1.0	1.0	Consistency: Recognizing syntactic entailments is a more natural task for people than treebank annotation.	0
433	5289	5289	S10-12	Introduction	14	18	1.0	1.0	Focusing on a natural human competence makes it practical to collect high quality evaluation data from untrained annotators.	0
434	5290	5290	S10-12	Introduction	15	19	1.0	1.0	The PETE dataset was annotated by untrained Amazon Mechanical Turk workers at an insignificant cost and each annotation is based on the unanimous agreement of at least three workers.	0
435	5291	5291	S10-12	Introduction	16	20	2.0	1.0	In contrast, of the 36306 constituent strings that appear multiple times in the Penn Treebank (Marcus et al., 1994), 5646 (15%) have multiple conflicting annotations.	0
436	5292	5292	S10-12	Introduction	17	21	2.0	1.0	If indicative of the general level of inconsistency, 15% is a very high number given that the state of the art parsers claim f-scores above 90% (Charniak and Johnson, 2005).	0
437	5293	5293	S10-12	Introduction	18	22	2.0	1.0	Relevance: PETE automatically focuses attention on semantically relevant phenomena rather than differences in annotation style or linguistic convention.	0
438	5294	5294	S10-12	Introduction	19	23	2.0	1.0	Whether a phrase is tagged ADJP vs ADVP rarely affects semantic interpretation.	0
439	5295	5295	S10-12	Introduction	20	24	2.0	1.0	Attaching the wrong subject to a verb or the wrong prepositional phrase to a noun changes the meaning of the sentence.	0
440	5296	5296	S10-12	Introduction	21	25	2.0	1.0	Standard treebank based evaluation metrics do not distinguish between semantically relevant and irrelevant errors (Bonnema et al., 1997).	0
441	5297	5297	S10-12	Introduction	22	26	2.0	1.0	In PETE semantically relevant differences lead to different entailments, semantically irrelevant differences do not.	0
442	5298	5298	S10-12	Introduction	23	27	2.0	1.0	Framework independence: Entailment recognition is a formalism independent task.	0
443	5299	5299	S10-12	Introduction	24	28	2.0	1.0	A common evaluation method for parsers that do not use the Penn Treebank formalism is to automatically convert the Penn Treebank to the appropriate formalism and to perform treebank based evaluation (Nivre et al., 2007a;	0
444	5300	5300	S10-12	Introduction	25	29	2.0	1.0	Hockenmaier and Steedman, 2007).	0
445	5301	5301	S10-12	Introduction	26	30	2.0	1.0	The inevitable conversion errors compound the already mentioned problems of treebank based evaluation.	0
446	5302	5302	S10-12	Introduction	27	31	2.0	1.0	In addition, manually designed treebanks do not naturally lend themselves to unsupervised parser evaluation.	0
447	5303	5303	S10-12	Introduction	28	32	2.0	1.0	Unlike treebank based evaluation, PETE can compare phrase structure parsers, dependency parsers, unsupervised parsers and other approaches on an equal footing.	0
448	5304	5304	S10-12	Introduction	29	33	2.0	1.0	PETE was inspired by earlier work on representations of grammatical dependency, proposed for ease of use by end users and suitable for parser evaluation.	0
449	5305	5305	S10-12	Introduction	30	34	2.0	1.0	These include the grammatical relations (GR) by (Carroll et al., 1999), the PARC representation (King et al., 2003), and Stanford typed dependencies (SD) (De Marneffe et al., 2006) (See (Bos and others, 2008) for other proposals).	0
450	5306	5306	S10-12	Introduction	31	35	3.0	1.0	Each use a set of binary relations between words in a sentence as the primary unit of representation.	0
451	5307	5307	S10-12	Introduction	32	36	3.0	1.0	They share some common motivations: usability by people who are not (computational) linguists and suitability for relation extraction applications.	0
452	5308	5308	S10-12	Introduction	33	37	3.0	2.0	Here is an example sentence and its SD representation (De Marneffe and Manning, 2008):	0
453	5309	5309	S10-12	Introduction	34	38	3.0	2.0	Bell, based in Los Angeles, makes and distributes electronic, computer and building products.	0
454	5310	5310	S10-12	Introduction	35	39	3.0	2.0	nsubj(makes-8, Bell-1) nsubj(distributes-10, Bell-1) partmod(Bell-1, based-3) nn(Angeles-6, Los-5) prep-in(based-3, Angeles-6) conj-and(makes-8, distributes-10) amod (products-16, electronic-11) conj-and(electronic-11, computer-13) amod (products-16, computer-13) conj-and(electronic-11, building-15) amod(products-16, building-15) dobj(makes-8, products-16) PETE goes one step further by translating most of these dependencies into natural language entailments.	0
455	5311	5311	S10-12	Introduction	36	40	3.0	2.0	Bell makes something.	0
456	5312	5312	S10-12	Introduction	37	41	3.0	2.0	Bell distributes something.	0
457	5313	5313	S10-12	Introduction	38	42	3.0	2.0	Someone is based in Los Angeles.	0
458	5314	5314	S10-12	Introduction	39	43	3.0	2.0	Someone makes products.	0
459	5315	5315	S10-12	Introduction	40	44	3.0	2.0	PETE has some advantages over representations based on grammatical relations.	0
460	5316	5316	S10-12	Introduction	41	45	3.0	2.0	For example SD defines 55 relations organized in a hierarchy, and it may be non-trivial for a non-linguist to understand the difference between ccomp (clausal complement with internal subject) and xcomp (clausal complement with external subject) or between nsubj (nominal subject) and xsubj (controlling subject).	0
461	5317	5317	S10-12	Introduction	42	46	3.0	2.0	In fact it could be argued that proposals like SD replace one artificial annotation formalism with another and no two such proposals agree on the ideal set of binary relations to use.	0
462	5318	5318	S10-12	Introduction	43	47	3.0	2.0	In contrast, untrained annotators have no difficulty unanimously agreeing on the validity of most PETE type entailments.	0
463	5319	5319	S10-12	Introduction	44	48	3.0	2.0	However there are also significant challenges associated with an evaluation scheme like PETE.	0
464	5320	5320	S10-12	Introduction	45	49	3.0	2.0	It is not always clear how to convert certain relations into grammatical hypothesis sentences without including most of the original sentence in the hypothesis.	0
465	5321	5321	S10-12	Introduction	46	50	4.0	2.0	Including too much of the sentence in the hypothesis would increase the chances of getting the right answer with the wrong parse.	0
466	5322	5322	S10-12	Introduction	47	51	4.0	2.0	Grammatical hypothesis sentences are especially difficult to construct when a (negative) entailment is based on a bad parse of the sentence.	0
467	5323	5323	S10-12	Introduction	48	52	4.0	2.0	"Introducing dummy words like ""someone"" or ""something"" alleviates part of the problem but does not help in the case of clausal complements."	0
468	5324	5324	S10-12	Introduction	49	53	4.0	2.0	In summary, PETE makes the annotation phase more practical and consistent but shifts the difficulty to the entailment creation phase.	0
469	5325	5325	S10-12	Introduction	50	54	4.0	2.0	PETE gets closer to an extrinsic evaluation by focusing on semantically relevant, application oriented differences that can be expressed in natural language sentences.	0
470	5326	5326	S10-12	Introduction	51	55	4.0	2.0	This makes the evaluation procedure indirect: a parser developer has to write an extension that can handle entailment questions.	0
471	5327	5327	S10-12	Introduction	52	56	4.0	2.0	However, given the simplicity of the entailments, the complexity of such an extension is comparable to one that extracts grammatical relations.	0
472	5328	5328	S10-12	Introduction	53	57	4.0	2.0	The balance of what is being evaluated is also important.	0
473	5329	5329	S10-12	Introduction	54	58	4.0	2.0	A treebank based evaluation scheme may mix semantically relevant and irrelevant mistakes, but at least it covers every sentence at a uniform level of detail.	0
474	5330	5330	S10-12	Introduction	55	59	4.0	2.0	In this evaluation, we focused on sentences and relations where state of the art parsers disagree.	0
475	5331	5331	S10-12	Introduction	56	60	4.0	2.0	We hope this methodology will uncover weaknesses that the next generation systems can focus on.	0
476	5332	5332	S10-12	Introduction	57	61	4.0	2.0	The remaining sections will go into more detail about these challenges and the solutions we have chosen to implement.	0
477	5333	5333	S10-12	Introduction	58	62	4.0	2.0	Section 2 explains the method followed to create the PETE dataset.	0
478	5334	5334	S10-12	Introduction	59	63	4.0	2.0	Sec-tion 3 evaluates the baseline systems the task organizers created by implementing simple entailment extensions for several state of the art parsers.	0
479	5335	5335	S10-12	Introduction	60	64	4.0	2.0	Section 4 presents the participating systems, their methods and results.	0
480	5336	5336	S10-12	Introduction	61	65	4.0	2.0	Section 5 summarizes our contribution.	0
481	5337	5337	S10-12	Dataset	1	66	1.0	2.0	To generate the entailments for the PETE task we followed the following three steps:	0
482	5338	5338	S10-12	Dataset	2	67	2.0	2.0	1. Identify syntactic dependencies that are challenging to state of the art parsers.	0
483	5339	5339	S10-12	Dataset	3	68	3.0	2.0	2. Construct short entailment sentences that paraphrase those dependencies.	0
484	5340	5340	S10-12	Dataset	4	69	4.0	2.0	3. Identify the subset of the entailments with high inter-annotator agreement.	0
485	5341	5341	S10-12	Identifying Challenging Dependencies	1	70	1.0	2.0	To identify syntactic dependencies that are challenging for current state of the art parsers, we used example sentences from the following sources:	0
486	5342	5342	S10-12	Identifying Challenging Dependencies	2	71	1.0	2.0	•	0
487	5343	5343	S10-12	Identifying Challenging Dependencies	3	72	1.0	2.0	"The ""Unbounded Dependency Corpus"" (Rimell et al., 2009)."	0
488	5344	5344	S10-12	Identifying Challenging Dependencies	4	73	2.0	2.0	"An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting ""gap""."	0
489	5345	5345	S10-12	Identifying Challenging Dependencies	5	74	2.0	3.0	"An unlimited number of clause boundaries may intervene between the moved element and the gap (hence ""unbounded"")."	0
490	5346	5346	S10-12	Identifying Challenging Dependencies	6	75	2.0	3.0	•	0
491	5347	5347	S10-12	Identifying Challenging Dependencies	7	76	2.0	3.0	A list of sentences from the Penn Treebank on which the Charniak parser (Charniak and Johnson, 2005) performs poorly 1 .	0
492	5348	5348	S10-12	Identifying Challenging Dependencies	8	77	3.0	3.0	•	0
493	5349	5349	S10-12	Identifying Challenging Dependencies	9	78	3.0	3.0	The Brown section of the Penn Treebank.	0
494	5350	5350	S10-12	Identifying Challenging Dependencies	10	79	3.0	3.0	We tested a number of parsers (both phrase structure and dependency) on these sentences and identified the differences in their output.	0
495	5351	5351	S10-12	Identifying Challenging Dependencies	11	80	3.0	3.0	We took sentences where at least one of the parsers gave a different answer than the others or the gold parse.	0
496	5352	5352	S10-12	Identifying Challenging Dependencies	12	81	4.0	3.0	Some of these differences reflected linguistic convention rather than semantic disagreement (e.g. representation of coordination) and some did not represent meaningful differences that can be expressed with entailments (e.g. labeling a phrase ADJP vs ADVP).	0
497	5353	5353	S10-12	Identifying Challenging Dependencies	13	82	4.0	3.0	The remaining differences typically reflected genuine semantic disagreements	0
498	5354	5354	S10-12	Identifying Challenging Dependencies	14	83	4.0	3.0	1 http://www.cs.brown.edu/˜ec/papers/badPars.txt.gz that would effect downstream applications.	0
499	5355	5355	S10-12	Identifying Challenging Dependencies	15	84	4.0	3.0	These were chosen to turn into entailments in the next step.	0
500	5356	5356	S10-12	Constructing Entailments	1	85	1.0	3.0	We tried to make the entailments as targeted as possible by building them around two content words that are syntactically related.	0
501	5357	5357	S10-12	Constructing Entailments	2	86	1.0	3.0	When the two content words were not sufficient to construct a grammatical sentence we used one of the following techniques:	0
502	5358	5358	S10-12	Constructing Entailments	3	87	1.0	3.0	•	0
503	5359	5359	S10-12	Constructing Entailments	4	88	2.0	3.0	"Complete the mandatory elements using the words ""somebody"" or ""something""."	0
504	5360	5360	S10-12	Constructing Entailments	5	89	2.0	3.0	(e.g.	0
505	5361	5361	S10-12	Constructing Entailments	6	90	2.0	3.0	"To test the subject-verb dependency in ""John kissed Mary."" we construct the entailment ""John kissed somebody."")"	0
506	5362	5362	S10-12	Constructing Entailments	7	91	2.0	3.0	•	0
507	5363	5363	S10-12	Constructing Entailments	8	92	3.0	3.0	Make a passive sentence to avoid using a spurious subject.	0
508	5364	5364	S10-12	Constructing Entailments	9	93	3.0	3.0	(e.g.	0
509	5365	5365	S10-12	Constructing Entailments	10	94	3.0	3.0	"To test the verb-object dependency in ""John kissed Mary."" we construct the entailment ""Mary was kissed."")"	0
510	5366	5366	S10-12	Constructing Entailments	11	95	4.0	3.0	•	0
511	5367	5367	S10-12	Constructing Entailments	12	96	4.0	3.0	"Make a copular sentence or use existential ""there"" to express noun modification."	0
512	5368	5368	S10-12	Constructing Entailments	13	97	4.0	3.0	(e.g.	0
513	5369	5369	S10-12	Constructing Entailments	14	98	4.0	3.0	"To test the noun-modifier dependency in ""The big red boat sank."" we construct the entailment ""The boat was big."" or ""There was a big boat."")"	0
514	5370	5370	S10-12	Filtering Entailments	1	99	1.0	3.0	To identify the entailments that are clear to human judgement we used the following procedure:	0
515	5371	5371	S10-12	Filtering Entailments	2	100	1.0	3.0	1. Each entailment was tagged by 5 untrained annotators from the Amazon Mechanical Turk crowdsourcing service.	0
516	5372	5372	S10-12	Filtering Entailments	3	101	1.0	3.0	2	0
517	5373	5373	S10-12	Filtering Entailments	4	102	1.0	3.0	The results from the annotators whose agreement with the gold parse fell below 70% were eliminated.	0
518	5374	5374	S10-12	Filtering Entailments	5	103	2.0	3.0	3	0
519	5375	5375	S10-12	Filtering Entailments	6	104	2.0	3.0	The entailments for which there was unanimous agreement of at least 3 annotators were kept.	0
520	5376	5376	S10-12	Filtering Entailments	7	105	2.0	3.0	The instructions for the annotators were brief and targeted people with no linguistic background:	0
521	5377	5377	S10-12	Filtering Entailments	8	106	2.0	3.0	Computers try to understand long sentences by dividing them into a set of short facts.	0
522	5378	5378	S10-12	Filtering Entailments	9	107	2.0	3.0	You will help judge whether the computer extracted the right facts from a given set of 25 English sentences.	0
523	5379	5379	S10-12	Filtering Entailments	10	108	3.0	3.0	Each of the following examples consists of a sentence (T), and a short statement (H) derived from this sentence by a computer.	0
524	5380	5380	S10-12	Filtering Entailments	11	109	3.0	3.0	"Please read both of them carefully and choose ""Yes"" if the meaning of (H) can be inferred from the meaning of (T)."	0
525	5381	5381	S10-12	Filtering Entailments	12	110	3.0	4.0	Here is an example: (T) Any lingering suspicion that this was a trick Al Budd had thought up was dispelled.	0
526	5382	5382	S10-12	Filtering Entailments	13	111	3.0	4.0	(H) The suspicion was dispelled.	0
527	5383	5383	S10-12	Filtering Entailments	14	112	4.0	4.0	Answer: YES (H) The suspicion was a trick.	0
528	5384	5384	S10-12	Filtering Entailments	15	113	4.0	4.0	Answer: NO	0
529	5385	5385	S10-12	Filtering Entailments	16	114	4.0	4.0	"You can choose the third option ""Not sure"" when the (H) statement is unrelated, unclear, ungrammatical or confusing in any other manner."	0
530	5386	5386	S10-12	Filtering Entailments	17	115	4.0	4.0	"The ""Not sure"" answers were grouped with the ""No"" answers during evaluation."	0
531	5387	5387	S10-12	Filtering Entailments	18	116	4.0	4.0	Approximately 50% of the original entailments were retained after the inter-annotator agreement filtering.	0
532	5388	5388	S10-12	Dataset statistics	1	117	1.0	4.0	The final dataset contained 367 entailments which were randomly divided into a 66 sentence development test and a 301 sentence test set.	0
533	5389	5389	S10-12	Dataset statistics	2	118	2.0	4.0	52% of the entailments in the test set were positive.	0
534	5390	5390	S10-12	Dataset statistics	3	119	3.0	4.0	Approximately half of the final entailments were from the Unbounded Dependency Corpus, a third were from the Brown section of the Penn Treebank, and the remaining were from the Charniak sentences.	0
535	5391	5391	S10-12	Dataset statistics	4	120	4.0	4.0	Table 1	0
536	5392	5392	S10-12	Baselines	1	121	1.0	4.0	In order to establish baseline results for this task, we built an entailment decision system for CoNLL format dependency files and tested several publicly available parsers.	0
537	5393	5393	S10-12	Baselines	2	122	1.0	4.0	The parsers used were the Berkeley Parser (Petrov and Klein, 2007), Charniak Parser (Charniak and Johnson, 2005), Collins Parser (Collins, 2003), Malt Parser (Nivre et al., 2007b), MSTParser (McDonald et al., 2005) and Stanford Parser (Klein and Manning, 2003).	0
538	5394	5394	S10-12	Baselines	3	123	1.0	4.0	Each parser was trained on sections 02-21 of the WSJ section of Penn Treebank.	0
539	5395	5395	S10-12	Baselines	4	124	2.0	4.0	Outputs of phrase structure parsers were automatically annotated with function tags using Blaheta's function tagger (Blaheta and Charniak, 2000) and converted to the dependency structure with LTH Constituentto-Dependency Conversion Tool (Johansson and Nugues, 2007).	0
540	5396	5396	S10-12	Baselines	5	125	2.0	4.0	To decide the entailments both the test and hypothesis sentences were parsed.	0
541	5397	5397	S10-12	Baselines	6	126	2.0	4.0	All the content words in the hypothesis sentence were determined by using part-of-speech tags and dependency relations.	0
542	5398	5398	S10-12	Baselines	7	127	3.0	4.0	After applying some heuristics such as active-passive conversion, the extracted dependency path between the content words was searched in the dependency graph of the test sentence.	0
543	5399	5399	S10-12	Baselines	8	128	3.0	4.0	"In this search process, same relation types for the direct relations between the content word pairs and isomorphic subgraphs in the test and hypothesis sentences were required for the ""YES"" answer."	0
544	5400	5400	S10-12	Baselines	9	129	3.0	4.0	Table 2 lists the baseline results achieved.	0
545	5401	5401	S10-12	Baselines	10	130	4.0	4.0	There are significant differences in the entailment accuracies of systems that have comparable unlabeled attachment scores.	0
546	5402	5402	S10-12	Baselines	11	131	4.0	4.0	One potential reason for this difference is the composition of the PETE dataset which emphasizes challenging syntactic constructions that some parsers may be better at.	0
547	5403	5403	S10-12	Baselines	12	132	4.0	4.0	Another reason is the complete indifference of treebank based measures like UAS to the semantic significance of various dependencies and their impact on potential applications.	0
548	5404	5404	S10-12	System	1	133	1.0	4.0	Systems	0
549	5405	5405	S10-12	System	2	134	1.0	4.0	There were 20 systems from 7 teams participating in the PETE task.	0
550	5406	5406	S10-12	System	3	135	2.0	4.0	(Nivre et al., 2007b), MSTParser (McDonald et al., 2005 and Stanford Parser (Klein and Manning, 2003).	0
551	5407	5407	S10-12	System	4	136	2.0	4.0	After the parsing step, the decision for the entailment was based on the comparison of relations, predicates, or dependency paths between the text and the hypothesis.	0
552	5408	5408	S10-12	System	5	137	3.0	4.0	Most systems relied on heuristic methods of comparison.	0
553	5409	5409	S10-12	System	6	138	3.0	4.0	A notable exception is the MARS-3 system which used an SVM-based classifier to decide on the entailment using dependency path features.	0
554	5410	5410	S10-12	System	7	139	4.0	4.0	Table 4 lists the frequency of various grammatical relations in the instances where the top system made mistakes.	0
555	5411	5411	S10-12	System	8	140	4.0	4.0	A comparison with Table 1 shows the direct objects and reduced relative clauses to be the frequent causes of error.	0
556	5412	5412	S10-12	Contributions	1	141	1.0	4.0	We introduced PETE, a new method for parser evaluation using textual entailments.	0
557	5413	5413	S10-12	Contributions	2	142	2.0	4.0	By basing the entailments on dependencies that current state  of the art parsers disagree on, we hoped to create a dataset that would focus attention on the long tail of parsing problems that do not get sufficient attention using common evaluation metrics.	0
558	5414	5414	S10-12	Contributions	3	143	2.0	4.0	By further restricting ourselves to differences that can be expressed by natural language entailments, we hoped to focus on semantically relevant decisions rather than accidents of convention which get mixed up in common evaluation metrics.	0
559	5415	5415	S10-12	Contributions	4	144	3.0	4.0	We chose to rely on untrained annotators on a natural inference task rather than trained annotators on an artificial tagging task because we believe (i) many subfields of computational linguistics are struggling to make progress because of the noise in artificially tagged data, and (ii) systems should try to model natural human linguistic competence rather than their dubious competence in artificial tagging tasks.	0
560	5416	5416	S10-12	Contributions	5	145	4.0	4.0	Our hope is datasets like PETE will be used not only for evaluation but also for training and fine-tuning of systems in the future.	0
561	5417	5417	S10-12	Contributions	6	146	4.0	4.0	Further work is needed to automate the entailment generation process and to balance the composition of syntactic phenomena covered in a PETE dataset.	0
562	5809	5809	S10-18	title	1	1	4.0	1.0	SemEval-2010 Task 18: Disambiguating Sentiment Ambiguous Adjectives	0
563	5810	5810	S10-18	abstract	1	2	1.0	1.0	Sentiment ambiguous adjectives cause major difficulties for existing algorithms of sentiment analysis.	0
564	5811	5811	S10-18	abstract	2	3	2.0	1.0	We present an evaluation task designed to provide a framework for comparing different approaches in this problem.	0
565	5812	5812	S10-18	abstract	3	4	3.0	1.0	We define the task, describe the data creation, list the participating systems and discuss their results.	0
566	5813	5813	S10-18	abstract	4	5	4.0	1.0	There are 8 teams and 16 systems.	0
567	5814	5814	S10-18	Introduction	1	6	1.0	1.0	In recent years, sentiment analysis has attracted considerable attention (Pang and Lee, 2008).	0
568	5815	5815	S10-18	Introduction	2	7	1.0	1.0	It is the task of mining positive and negative opinions from natural language, which can be applied to many natural language processing tasks, such as document summarization and question answering.	0
569	5816	5816	S10-18	Introduction	3	8	1.0	1.0	Previous work on this problem falls into three groups: opinion mining of documents, sentiment classification of sentences and polarity prediction of words.	0
570	5817	5817	S10-18	Introduction	4	9	1.0	1.0	Sentiment analysis both at document and sentence level rely heavily on word level.	0
571	5818	5818	S10-18	Introduction	5	10	1.0	1.0	The most frequently explored task at word level is to determine the semantic orientation (SO) of words, in which most work centers on assigning a prior polarity to words or word senses in the lexicon out of context.	0
572	5819	5819	S10-18	Introduction	6	11	1.0	1.0	However, for some words, the polarity varies strongly with context, making it hard to attach each to a specific sentiment category in the lexicon.	0
573	5820	5820	S10-18	Introduction	7	12	2.0	1.0	"For example, consider "" low cost"" versus "" low salary"" ."	0
574	5821	5821	S10-18	Introduction	8	13	2.0	1.0	"The word "" low"" has a positive orientation in the first case but a negative orientation in the second case."	0
575	5822	5822	S10-18	Introduction	9	14	2.0	1.0	Turney and Littman (2003) claimed that sentiment ambiguous words could not be avoided easily in a real-world application in the future research.	0
576	5823	5823	S10-18	Introduction	10	15	2.0	1.0	But unfortunately, sentiment ambiguous words are discarded by most research concerning sentiment analysis (Hatzivassiloglou and McKeown, 1997;	0
577	5824	5824	S10-18	Introduction	11	16	2.0	1.0	Turney and Littman, 2003;Kim and Hovy, 2004).	0
578	5825	5825	S10-18	Introduction	12	17	2.0	1.0	The exception work is Ding et al. (2008).	0
579	5826	5826	S10-18	Introduction	13	18	3.0	1.0	They call these words as context dependant opinions and propose a holistic lexicon-based approach to solve this problem.	0
580	5827	5827	S10-18	Introduction	14	19	3.0	1.0	The language they deal with is English.	0
581	5828	5828	S10-18	Introduction	15	20	3.0	1.0	The disambiguation of sentiment ambiguous words can also be considered as a problem of phrase-level sentiment analysis.	0
582	5829	5829	S10-18	Introduction	16	21	3.0	1.0	Wilson et al. (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features.	0
583	5830	5830	S10-18	Introduction	17	22	3.0	1.0	Takamura et al. (2006	0
584	5831	5831	S10-18	Introduction	18	23	3.0	1.0	"Takamura et al. ( , 2007 propose latent variable model and lexical network to determine SO of phrases, focusing on "" noun+adjective"" pairs."	0
585	5832	5832	S10-18	Introduction	19	24	4.0	1.0	Their experimental results suggest that the classification of pairs containing ambiguous adjectives is much harder than those with unambiguous adjectives.	0
586	5833	5833	S10-18	Introduction	20	25	4.0	1.0	The task 18 at SemEval 2010 provides a benchmark data set to encourage studies on this problem.	0
587	5834	5834	S10-18	Introduction	21	26	4.0	1.0	This paper is organized as follows.	0
588	5835	5835	S10-18	Introduction	22	27	4.0	1.0	Section 2 defines the task.	0
589	5836	5836	S10-18	Introduction	23	28	4.0	1.0	Section 3 describes the data annotation.	0
590	5837	5837	S10-18	Introduction	24	29	4.0	1.0	Section 4 gives a brief summary of 16 participating systems.	0
591	5838	5838	S10-18	Introduction	25	30	4.0	1.0	Finally Section 5 draws conclusions.	0
592	5839	5839	S10-18	Task Set up 2.1 Task description	1	31	1.0	1.0	In this task, we focus on 14 frequently used sentiment ambiguous adjectives in Chinese, which all have the meaning of measurement, as shown below.	0
593	5840	5840	S10-18	Task Set up 2.1 Task description	2	32	1.0	1.0	"(1) Sentiment ambiguous adjectives(SAAs) ={ 大 da "" large"" , 多 duo "" many"" , 高 gao "" high"" , 厚 hou "" thick"" , 深 shen "" deep"" , 重 zhong "" heavy"" , 巨大 ju-da "" huge"" , 重大 zhong-da "" great"" , 小 xiao "" small"" , 少 shao "" few"" , 低 di "" low"" , 薄 bao "" thin"" , 浅 qian "" shallow"" , 轻 qing "" light"" }"	0
594	5841	5841	S10-18	Task Set up 2.1 Task description	3	33	2.0	1.0	These adjectives are neutral out of context, but when they co-occur with some target nouns, positive or negative emotion will be evoked.	0
595	5842	5842	S10-18	Task Set up 2.1 Task description	4	34	2.0	1.0	Although the number of such ambiguous adjectives is not large, they are frequently used in real text, especially in the texts expressing opinions and emotions.	0
596	5843	5843	S10-18	Task Set up 2.1 Task description	5	35	3.0	1.0	The task is designed to automatically determine the SO of these sentiment ambiguous adjectives within context: positive or negative.	1
597	5844	5844	S10-18	Task Set up 2.1 Task description	6	36	3.0	1.0	"For example, 高 gao "" high""should be assigned as positive in 工 资 高 gong-zi -gao "" salary is high""but negative in 价格高 jia-ge-gao "" price is high"" ."	0
598	5845	5845	S10-18	Task Set up 2.1 Task description	7	37	4.0	1.0	This task was carried out in an unsupervised setting.	0
599	5846	5846	S10-18	Task Set up 2.1 Task description	8	38	4.0	1.0	No training data was provided, but external resources are encouraged to use.	0
600	5847	5847	S10-18	Data Creation	1	39	1.0	1.0	We collected data from two sources.	0
601	5848	5848	S10-18	Data Creation	2	40	1.0	2.0	The main part was extracted from Xinhua News Agency of Chinese Gigaword (Second Edition) released by LDC.	0
602	5849	5849	S10-18	Data Creation	3	41	1.0	2.0	The texts were automatically wordsegmented and POS-tagged using the open software ICTCLAS 1 .	0
603	5850	5850	S10-18	Data Creation	4	42	1.0	2.0	In order to concentrate on the disambiguation of sentiment ambiguous adjectives, and reduce the noise introduced by the parser, we extracted sentences containing strings in pattern of (2), where the target nouns are modified by the adjectives in most cases.	0
604	5851	5851	S10-18	Data Creation	5	43	2.0	2.0	"(2) noun+adverb+adjective (adjective∈SAAs) e.g. 成本/n 较/d 低/a cheng-ben-jiao-di "" the cost is low."""	0
605	5852	5852	S10-18	Data Creation	6	44	2.0	2.0	Another small part of data was extracted from the Web.	0
606	5853	5853	S10-18	Data Creation	7	45	2.0	2.0	Using the search engine Google 2 , we searched the queries as in ( 3):	0
607	5854	5854	S10-18	Data Creation	8	46	2.0	2.0	"(3) 很 hen "" very"" + adjective (adjective∈"	0
608	5855	5855	S10-18	Data Creation	9	47	2.0	2.0	SAAs )	0
609	5856	5856	S10-18	Data Creation	10	48	3.0	2.0	From the returned snippets, we manually picked out some sentences that contain the strings of (2).	0
610	5857	5857	S10-18	Data Creation	11	49	3.0	2.0	Also, the sentences were automatically segmented and POS-tagged using ICTCLAS.	0
611	5858	5858	S10-18	Data Creation	12	50	3.0	2.0	Sentiment ambiguous adjectives in the data were assigned as positive, negative or neutral, independently by two annotators.	0
612	5859	5859	S10-18	Data Creation	13	51	3.0	2.0	Since we focus on the distinction between positive and negative categories, the neutral instances were removed.	0
613	5860	5860	S10-18	Data Creation	14	52	4.0	2.0	The inter-annotator agreement is in a high level with a kappa of 0.91.	0
614	5861	5861	S10-18	Data Creation	15	53	4.0	2.0	After cases with disagreement were negotiated between the two annotators, a gold standard annotation was agreed upon.	0
615	5862	5862	S10-18	Data Creation	16	54	4.0	2.0	In total 2917 instances were provided as the test data in the task, and the number of sentences of per target adjective is listed in Table 2.	0
616	5863	5863	S10-18	Data Creation	17	55	4.0	2.0	Evaluation was performed in micro accuracy and macro accuracy:	0
617	5864	5864	S10-18	Data Creation	18	56	4.0	2.0	where N is the number of all target words, i n is the number of all test instances for a specific word, and i m is the number of correctly labeled instances.	0
618	5865	5865	S10-18	Baseline	1	57	1.0	2.0	We group 14 sentiment ambiguous adjectives into two categories: positive-like adjectives and negative-like adjectives.	0
619	5866	5866	S10-18	Baseline	2	58	1.0	2.0	The former has the connotation towards large measurement, whereas the latter towards small measurement.	0
620	5867	5867	S10-18	Baseline	3	59	2.0	2.0	"(4) Positive-like adjectives (Pa) ={大 da "" large"" , 多 duo "" many"" , 高 gao "" high"" , 厚 hou "" thick"" , 深 shen "" deep"" , 重 zhong "" heavy"" , 巨大 ju-da "" huge"" , 重大 zhong-da "" great"" }"	0
621	5868	5868	S10-18	Baseline	4	60	2.0	2.0	"(5) Negative-like adjectives (Na) ={ 小 xiao "" small"" , 少 shao "" few"" , 低 di "" low"" , 薄 bao "" thin"" , 浅 qian "" shallow"" , 轻 qing "" light"" }"	0
622	5869	5869	S10-18	Baseline	5	61	3.0	2.0	We conduct a baseline in the dataset.	0
623	5870	5870	S10-18	Baseline	6	62	3.0	2.0	Not considering the context, assign all positive-like adjectives as positive and all negative-like adjectives as negative.	0
624	5871	5871	S10-18	Baseline	7	63	4.0	2.0	The micro accuracy of the baseline is 61.20%.	0
625	5872	5872	S10-18	Baseline	8	64	4.0	2.0	The inter-annotator agreement of 0.91 can be considered as the upper bound of the dataset.	0
626	5873	5873	S10-18	Systems and Results	1	65	2.0	2.0	We published firstly trial data and then test data.	0
627	5874	5874	S10-18	Systems and Results	2	66	3.0	2.0	In total 11 different teams downloaded both the trial and test data.	0
628	5875	5875	S10-18	Systems and Results	3	67	4.0	2.0	Finally 8 teams submitted their experimental results, including 16 systems.	0
629	5876	5876	S10-18	Results	1	68	1.0	2.0	Table 1 lists all systems'scores, ranked from best to worst performance measured by micro accuracy.	0
630	5877	5877	S10-18	Results	2	69	1.0	2.0	To our surprise, the performance of different systems differs greatly.	0
631	5878	5878	S10-18	Results	3	70	2.0	2.0	The micro accuracy of the best system is 94.20% that is 43.12% higher than the worst system.	0
632	5879	5879	S10-18	Results	4	71	2.0	2.0	The accuracy of the best three systems is even higher than inter-annotator agreement.	0
633	5880	5880	S10-18	Results	5	72	3.0	2.0	The performance of the worst system is only a little higher than random baseline, which is 50% when we randomly assign the SO of sentiment ambiguous adjectives.	0
634	5881	5881	S10-18	Results	6	73	3.0	2.0	Table 1: The scores of 16 systems	0
635	5882	5882	S10-18	Results	7	74	4.0	2.0	Table 2 shows that the performance of different systems differs greatly on each of 14 target adjectives.	0
636	5883	5883	S10-18	Results	8	75	4.0	2.0	"For example, the accuracy of 大 da "" large""is 95.53% by one system but only 46.51% by another system."	0
637	5884	5884	S10-18	Results	9	76	4.0	2.0	Table 2: The scores of 14 ambiguous adjectives	0
638	5885	5885	S10-18	Systems	1	77	1.0	2.0	In this section, we give a brief description of the systems.	0
639	5886	5886	S10-18	Systems	2	78	1.0	2.0	YSC-DSAA	0
640	5887	5887	S10-18	Systems	3	79	1.0	2.0	This system creates a new word library named SAAOL (SAA-Oriented Library), which is built manually with the help of software.	0
641	5888	5888	S10-18	Systems	4	80	1.0	3.0	SAAOL consists of positive words, negative words, NSSA, PSSA, and inverse words.	0
642	5889	5889	S10-18	Systems	5	81	1.0	3.0	The system divides the sentences into clauses using heuristic rules, and disambiguates SAA by analyzing the relationship between SAA and the keywords.	0
643	5890	5890	S10-18	Systems	6	82	1.0	3.0	HITSZ_CITYU	0
644	5891	5891	S10-18	Systems	7	83	1.0	3.0	This group submitted three systems, including one baseline system and two improved systems.	0
645	5892	5892	S10-18	Systems	8	84	1.0	3.0	HITSZ_CITYU_3: The baseline system is based on collocation of opinion words and their targets.	0
646	5893	5893	S10-18	Systems	9	85	1.0	3.0	For the given adjectives, their collocations are extracted from People' s Daily Corpus.	0
647	5894	5894	S10-18	Systems	10	86	1.0	3.0	With human annotation, the system obtained 412 positive and 191 negative collocations, which are regarded as seed collocations.	0
648	5895	5895	S10-18	Systems	11	87	1.0	3.0	Using the context words of seed collocations as features, the system trains a oneclass SVM classifier.	0
649	5896	5896	S10-18	Systems	12	88	1.0	3.0	HITSZ_CITYU_2 and HITSZ_CITYU_1: Using HowNet-based word similarity as clue, the authors expand the seed collocations on both ambiguous adjectives side and collocated targets side.	0
650	5897	5897	S10-18	Systems	13	89	1.0	3.0	The authors then exploit sentence-level opinion analysis to further improve performance.	0
651	5898	5898	S10-18	Systems	14	90	1.0	3.0	The strategy is that if the neighboring sentences on both sides have the same polarity, the ambiguous adjective is assigned as the same polarity; if the neighboring sentences have conflicted polarity, the SO of ambiguous adjective is determined by its context words and the transitive probability of sentence polarity.	0
652	5899	5899	S10-18	Systems	15	91	2.0	3.0	The two systems use different parameters and combination strategy.	0
653	5900	5900	S10-18	Systems	16	92	2.0	3.0	OpAL	0
654	5901	5901	S10-18	Systems	17	93	2.0	3.0	This system combines supervised methods with unsupervised ones.	0
655	5902	5902	S10-18	Systems	18	94	2.0	3.0	The authors employ Google translator to translate the task dataset from Chinese to English, since their system is working in English.	0
656	5903	5903	S10-18	Systems	19	95	2.0	3.0	The system explores three types of judgments.	0
657	5904	5904	S10-18	Systems	20	96	2.0	3.0	The first one trains a SVM classifier based on NTCIR data and EmotiBlog annotations.	0
658	5905	5905	S10-18	Systems	21	97	2.0	3.0	"The second one uses search engine, issuing queries of "" noun + SAA + AND + non-ambiguous adjective""."	0
659	5906	5906	S10-18	Systems	22	98	2.0	3.0	"The nonambiguous adjectives include positive set ("" positive, beautiful, good"" ) and negative set ("" negative, ugly, bad"" )."	0
660	5907	5907	S10-18	Systems	23	99	2.0	3.0	"An example is "" price high and good"" ."	0
661	5908	5908	S10-18	Systems	24	100	2.0	3.0	"The third one uses "" too, very- rules"" ."	0
662	5909	5909	S10-18	Systems	25	101	2.0	3.0	The final result is determined by the majority vote of the three components.	0
663	5910	5910	S10-18	Systems	26	102	2.0	3.0	CityUHK	0
664	5911	5911	S10-18	Systems	27	103	2.0	3.0	This group submitted four systems.	0
665	5912	5912	S10-18	Systems	28	104	2.0	3.0	Both machine learning method and lexiconbased method are employed in their systems.	0
666	5913	5913	S10-18	Systems	29	105	2.0	3.0	In the machine learning method, maximum entropy model is used to train a classifier based on the Chinese data from NTCIR opinion task.	0
667	5914	5914	S10-18	Systems	30	106	3.0	3.0	Clauselevel and sentence-level classifiers are compared.	0
668	5915	5915	S10-18	Systems	31	107	3.0	3.0	In the lexicon-based method, the authors classify SAAs into two clusters: intensifiers (our positive-like adjectives in ( 4)) and suppressors (our negative-like adjectives in ( 5)), and then use the polarity of context to determine the SO of SAAs.	0
669	5916	5916	S10-18	Systems	32	108	3.0	3.0	City	0
670	5917	5917	S10-18	Systems	33	109	3.0	3.0	UHK4: clause-level machine learning + lexicon.	0
671	5918	5918	S10-18	Systems	34	110	3.0	3.0	City	0
672	5919	5919	S10-18	Systems	35	111	3.0	3.0	UHK3: sentence-level machine learning + lexicon.	0
673	5920	5920	S10-18	Systems	36	112	3.0	3.0	City	0
674	5921	5921	S10-18	Systems	37	113	3.0	3.0	UHK2: clause-level machine learning.	0
675	5922	5922	S10-18	Systems	38	114	3.0	3.0	City	0
676	5923	5923	S10-18	Systems	39	115	3.0	3.0	UHK2: sentence-level machine learning.	0
677	5924	5924	S10-18	Systems	40	116	3.0	3.0	QLK_DSAA	0
678	5925	5925	S10-18	Systems	41	117	3.0	3.0	This group submitted two systems.	0
679	5926	5926	S10-18	Systems	42	118	3.0	3.0	The authors adopt their SELC model (Qiu, et al., 2009), which is proposed to exploit the complementarities between lexicon-based and corpus-based methods to improve the whole performance.	0
680	5927	5927	S10-18	Systems	43	119	3.0	3.0	They determine the sentence polarity by SELC model, and simply regard the sentence polarity as the polarity of SAA in the sentence.	0
681	5928	5928	S10-18	Systems	44	120	3.0	4.0	QLK_DSAA_NR: Based on the result of SELC model, they inverse the SO of SAA when it is modified by negative terms.	0
682	5929	5929	S10-18	Systems	45	121	4.0	4.0	Our task includes only positive and negative categories, so they replace the neutral value obtained by SELC model by the predominant polarity of the adjective.	0
683	5930	5930	S10-18	Systems	46	122	4.0	4.0	"QLK_DSAA_R: Based on the result of QLK_DSAA_NR, they add a rule to cope with two modifiers 偏 pian "" specially"" and 太 tai "" too"" , which always have the negative meaning."	0
684	5931	5931	S10-18	Systems	47	123	4.0	4.0	Twitter sentiment	0
685	5932	5932	S10-18	Systems	48	124	4.0	4.0	This group submitted three systems.	0
686	5933	5933	S10-18	Systems	49	125	4.0	4.0	The authors use a training data collected from microblogging platform.	0
687	5934	5934	S10-18	Systems	50	126	4.0	4.0	By exploiting Twitter, they collected automatically a dataset consisting of negative and positive expressions.	0
688	5935	5935	S10-18	Systems	51	127	4.0	4.0	The sentiment classifier is trained using Naive Bayes with n-grams of words as features.	0
689	5936	5936	S10-18	Systems	52	128	4.0	4.0	Twitter Sentiment: Translating the task dataset from Chinese to English using Google translator, and then based on training data in English texts from Twitter.	0
690	5937	5937	S10-18	Systems	53	129	4.0	4.0	Twitter Sentiment_ext: With Twitter Sentiment as basis, using extended data.	0
691	5938	5938	S10-18	Systems	54	130	4.0	4.0	Twitter Sentiment_zh: Based on training data in Chinese texts from Twitter.	0
692	5939	5939	S10-18	Systems	55	131	4.0	4.0	Biparty	0
693	5940	5940	S10-18	Systems	56	132	4.0	4.0	This system transforms the problem of disambiguating SAAs to predict the polarity of target nouns.	0
694	5941	5941	S10-18	Systems	57	133	4.0	4.0	The system presents a bootstrapping method to automatically build the sentiment lexicon, by building a nouns-verbs biparty graph from a large corpus.	0
695	5942	5942	S10-18	Systems	58	134	4.0	4.0	Firstly they select a few nouns as seed words, and then they use a cross inducing method to expand more nouns and verbs into the lexicon.	0
696	5943	5943	S10-18	Systems	59	135	4.0	4.0	The strategy is based on a random walk model.	0
697	5944	5944	S10-18	Discussion	1	136	1.0	4.0	The experimental results of some systems are promising.	0
698	5945	5945	S10-18	Discussion	2	137	1.0	4.0	The micro accuracy of the best three systems is over 93%.	0
699	5946	5946	S10-18	Discussion	3	138	1.0	4.0	Therefore, the interannotator agreement (91%) is not an upper bound on the accuracy that can be achieved.	0
700	5947	5947	S10-18	Discussion	4	139	1.0	4.0	On the contrary, the experimental results of some systems are disappointing, which are below our predefined simple baseline (61.20%), and are only a little higher than random baseline (50%).	0
701	5948	5948	S10-18	Discussion	5	140	1.0	4.0	The accuracy variance of different systems makes this task more interesting.	0
702	5949	5949	S10-18	Discussion	6	141	2.0	4.0	The participating 8 teams exploit totally different methods.	0
703	5950	5950	S10-18	Discussion	7	142	2.0	4.0	Human annotation.	0
704	5951	5951	S10-18	Discussion	8	143	2.0	4.0	In YSC-DSAA system, the word library of SAAOL is verified by human.	0
705	5952	5952	S10-18	Discussion	9	144	2.0	4.0	In HITSZ_CITYU systems, the seed collocations are annotated by human.	0
706	5953	5953	S10-18	Discussion	10	145	2.0	4.0	The three systems rank top 3.	0
707	5954	5954	S10-18	Discussion	11	146	3.0	4.0	Undoubtedly, human labor can help improve the performance in this task.	0
708	5955	5955	S10-18	Discussion	12	147	3.0	4.0	Training data.	0
709	5956	5956	S10-18	Discussion	13	148	3.0	4.0	The OpAL system employs SVM machine learning based on NTCIR data and EmotiBlog annotations.	0
710	5957	5957	S10-18	Discussion	14	149	3.0	4.0	The CityUHK systems trains a maximum entropy classifier based on the annotated Chinese data from NTCIR.	0
711	5958	5958	S10-18	Discussion	15	150	3.0	4.0	The Twitter Sentiment systems use a training data automatically collected from Twitter.	0
712	5959	5959	S10-18	Discussion	16	151	4.0	4.0	The results show that some of these supervised methods based on training data cannot rival unsupervised ones, partly due to the poor quality of the training data.	0
713	5960	5960	S10-18	Discussion	17	152	4.0	4.0	English resources.	0
714	5961	5961	S10-18	Discussion	18	153	4.0	4.0	Our task is in Chinese.	0
715	5962	5962	S10-18	Discussion	19	154	4.0	4.0	Some systems use English resources by translating Chinese into English, as OpAL and Twitter Sentiment.	0
716	5963	5963	S10-18	Discussion	20	155	4.0	4.0	The OpAL system achieves a quite good result, making this method a promising direction.	0
717	5964	5964	S10-18	Discussion	21	156	4.0	4.0	This also shows that disambiguating SAAs is a common problem in natural language.	0
718	5965	5965	S10-18	Conclusion	1	157	2.0	4.0	This paper describes task 18 at SemEval-2010, disambiguating sentiment ambiguous adjectives.	0
719	5966	5966	S10-18	Conclusion	2	158	3.0	4.0	The experimental results of the 16 participating systems are promising, and the used approaches are quite novel.	0
720	5967	5967	S10-18	Conclusion	3	159	4.0	4.0	We encourage further research into this issue, and integration of the disambiguation of sentiment ambiguous adjectives into applications of sentiment analysis.	0
721	6979	6979	S12-6	title	1	1	4.0	1.0	SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity	0
722	6980	6980	S12-6	abstract	1	2	1.0	1.0	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts.	0
723	6981	6981	S12-6	abstract	2	3	1.0	1.0	This paper presents the results of the STS pilot task in Semeval.	0
724	6982	6982	S12-6	abstract	3	4	2.0	1.0	The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources.	0
725	6983	6983	S12-6	abstract	4	5	2.0	1.0	The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise.	0
726	6984	6984	S12-6	abstract	5	6	3.0	1.0	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%.	0
727	6985	6985	S12-6	abstract	6	7	3.0	1.0	35 teams participated in the task, submitting 88 runs.	0
728	6986	6986	S12-6	abstract	7	8	4.0	1.0	The best results scored a Pearson correlation &gt;80%, well above a simple lexical baseline that only scored a 31% correlation.	0
729	6987	6987	S12-6	abstract	8	9	4.0	1.0	This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.	0
730	6988	6988	S12-6	Introduction	1	10	1.0	1.0	Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two sentences.	0
731	6989	6989	S12-6	Introduction	2	11	1.0	1.0	STS is related to both Textual Entailment (TE) and Paraphrase (PARA).	0
732	6990	6990	S12-6	Introduction	3	12	1.0	1.0	STS is more directly applicable in a number of NLP tasks than TE and PARA such as Machine Translation and evaluation, Summarization, Machine Reading, Deep Question Answering, etc. STS differs from TE in as much as it assumes symmetric graded equivalence between the pair of textual snippets.	0
733	6991	6991	S12-6	Introduction	4	13	1.0	1.0	In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.	0
734	6992	6992	S12-6	Introduction	5	14	2.0	1.0	Additionally, STS differs from both TE and PARA in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS incorporates the notion of graded semantic similarity (e.g. a vehicle and a car are more similar than a wave and a car).	0
735	6993	6993	S12-6	Introduction	6	15	2.0	1.0	STS provides a unified framework that allows for an extrinsic evaluation of multiple semantic components that otherwise have tended to be evaluated independently and without broad characterization of their impact on NLP applications.	0
736	6994	6994	S12-6	Introduction	7	16	2.0	1.0	Such components include word sense disambiguation and induction, lexical substitution, semantic role labeling, multiword expression detection and handling, anaphora and coreference resolution, time and date resolution, named-entity handling, underspecification, hedging, semantic scoping and discourse analysis.	0
737	6995	6995	S12-6	Introduction	8	17	2.0	1.0	Though not in the scope of the current pilot task, we plan to explore building an open source toolkit for integrating and applying diverse linguistic analysis modules to the STS task.	0
738	6996	6996	S12-6	Introduction	9	18	2.0	1.0	While the characterization of STS is still preliminary, we observed that there was no comparable existing dataset extensively annotated for pairwise semantic sentence similarity.	0
739	6997	6997	S12-6	Introduction	10	19	3.0	1.0	We approached the construction of the first STS dataset with the following goals: (1)	0
740	6998	6998	S12-6	Introduction	11	20	3.0	1.0	To set a definition of STS as a graded notion which can be easily communicated to non-expert annotators beyond the likert-scale; (2)	0
741	6999	6999	S12-6	Introduction	12	21	3.0	1.0	To gather a substantial amount of sentence pairs from diverse datasets, and to annotate them with high quality; (3)	0
742	7000	7000	S12-6	Introduction	13	22	3.0	1.0	To explore evaluation measures for STS; (4)	0
743	7001	7001	S12-6	Introduction	14	23	4.0	1.0	To explore the relation of STS to PARA and Machine Translation Evaluation exercises.	0
744	7002	7002	S12-6	Introduction	15	24	4.0	1.0	In the next section we present the various sources of the STS data and the annotation procedure used.	0
745	7003	7003	S12-6	Introduction	16	25	4.0	1.0	Section 4 investigates the evaluation of STS systems.	0
746	7004	7004	S12-6	Introduction	17	26	4.0	1.0	Section 5 summarizes the resources and tools used by participant systems.	0
747	7005	7005	S12-6	Introduction	18	27	4.0	1.0	Finally, Section 6 draws the conclusions.	0
748	7006	7006	S12-6	Source Datasets	1	28	1.0	1.0	Datasets for STS are scarce.	0
749	7007	7007	S12-6	Source Datasets	2	29	1.0	1.0	Existing datasets include (Li et al., 2006) and (Lee et al., 2005).	0
750	7008	7008	S12-6	Source Datasets	3	30	1.0	1.0	The first dataset includes 65 sentence pairs which correspond to the dictionary definitions for the 65 word pairs in Similarity (Rubenstein and Goodenough, 1965).	0
751	7009	7009	S12-6	Source Datasets	4	31	1.0	1.0	The authors asked human informants to assess the meaning of the sentence pairs on a scale from 0.0 (minimum similarity) to 4.0 (maximum similarity).	0
752	7010	7010	S12-6	Source Datasets	5	32	1.0	1.0	While the dataset is very relevant to STS, it is too small to train, develop and test typical machine learning based systems.	0
753	7011	7011	S12-6	Source Datasets	6	33	1.0	1.0	The second dataset comprises 50 documents on news, ranging from 51 to 126 words.	0
754	7012	7012	S12-6	Source Datasets	7	34	1.0	1.0	"Subjects were asked to judge the similarity of document pairs on a five-point scale (with 1.0 indicating ""highly unrelated"" and 5.0 indicating ""highly related"")."	0
755	7013	7013	S12-6	Source Datasets	8	35	1.0	1.0	This second dataset comprises a larger number of document pairs, but it goes beyond sentence similarity into textual similarity.	0
756	7014	7014	S12-6	Source Datasets	9	36	1.0	1.0	When constructing our datasets, gathering naturally occurring pairs of sentences with different degrees of semantic equivalence was a challenge in itself.	0
757	7015	7015	S12-6	Source Datasets	10	37	1.0	1.0	If we took pairs of sentences at random, the vast majority of them would be totally unrelated, and only a very small fragment would show some sort of semantic equivalence.	0
758	7016	7016	S12-6	Source Datasets	11	38	1.0	1.0	Accordingly, we investigated reusing a collection of existing datasets from tasks that are related to STS.	0
759	7017	7017	S12-6	Source Datasets	12	39	1.0	1.0	We first studied the pairs of text from the Recognizing TE challenge.	0
760	7018	7018	S12-6	Source Datasets	13	40	1.0	1.0	The first editions of the challenge included pairs of sentences as the following:	0
761	7019	7019	S12-6	Source Datasets	14	41	1.0	1.0	The first sentence is the text, and the second is the hypothesis.	0
762	7020	7020	S12-6	Source Datasets	15	42	1.0	1.0	The organizers of the challenge annotated several pairs with a binary tag, indicating whether the hypothesis could be entailed from the text.	0
763	7021	7021	S12-6	Source Datasets	16	43	2.0	1.0	Although these pairs of text are interesting we decided to discard them from this pilot because the length of the hypothesis was typically much shorter than the text, and we did not want to bias the STS task in this respect.	0
764	7022	7022	S12-6	Source Datasets	17	44	2.0	1.0	We may, however, explore using TE pairs for STS in the future.	0
765	7023	7023	S12-6	Source Datasets	18	45	2.0	1.0	Microsoft Research (MSR) has pioneered the acquisition of paraphrases with two manually annotated datasets.	0
766	7024	7024	S12-6	Source Datasets	19	46	2.0	1.0	The first, called MSR Paraphrase (MSRpar for short) has been widely used to evaluate text similarity algorithms.	0
767	7025	7025	S12-6	Source Datasets	20	47	2.0	1.0	It contains 5801 pairs of sentences gleaned over a period of 18 months from thousands of news sources on the web (Dolan et al., 2004).	0
768	7026	7026	S12-6	Source Datasets	21	48	2.0	1.0	67% of the pairs were tagged as paraphrases.	0
769	7027	7027	S12-6	Source Datasets	22	49	2.0	2.0	The inter annotator agreement is between 82% and 84%.	0
770	7028	7028	S12-6	Source Datasets	23	50	2.0	2.0	Complete meaning equivalence is not required, and the annotation guidelines allowed for some relaxation.	0
771	7029	7029	S12-6	Source Datasets	24	51	2.0	2.0	The pairs which were annotated as not being paraphrases ranged from completely unrelated semantically, to partially overlapping, to those that were almost-but-not-quite semantically equivalent.	0
772	7030	7030	S12-6	Source Datasets	25	52	2.0	2.0	In this sense our graded annotations enrich the dataset with more nuanced tags, as we will see in the following section.	0
773	7031	7031	S12-6	Source Datasets	26	53	2.0	2.0	We followed the original split of 70% for training and 30% for testing.	0
774	7032	7032	S12-6	Source Datasets	27	54	2.0	2.0	A sample pair from the dataset follows:	0
775	7033	7033	S12-6	Source Datasets	28	55	2.0	2.0	The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq.	0
776	7034	7034	S12-6	Source Datasets	29	56	2.0	2.0	American intelligence leading up to the war on Iraq will be criticized by a powerful US Congressional committee due to report soon, officials said today.	0
777	7035	7035	S12-6	Source Datasets	30	57	2.0	2.0	In order to construct a dataset which would reflect a uniform distribution of similarity ranges, we sampled the MSRpar dataset at certain ranks of string similarity.	0
778	7036	7036	S12-6	Source Datasets	31	58	2.0	2.0	We used the implementation readily accessible at CPAN 1 of a well-known metric (Ukkonen, 1985).	0
779	7037	7037	S12-6	Source Datasets	32	59	3.0	2.0	We sampled equal numbers of pairs from five bands of similarity in the [0.4 .. 0.8] range separately from the paraphrase and non-paraphrase pairs.	0
780	7038	7038	S12-6	Source Datasets	33	60	3.0	2.0	We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.	0
781	7039	7039	S12-6	Source Datasets	34	61	3.0	2.0	The second dataset from MSR is the MSR Video Paraphrase Corpus (MSRvid for short).	0
782	7040	7040	S12-6	Source Datasets	35	62	3.0	2.0	The authors showed brief video segments to Annotators from Amazon Mechanical Turk (AMT) and were asked to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011).	0
783	7041	7041	S12-6	Source Datasets	36	63	3.0	2.0	Nearly 120 thousand sentences were collected for 2000 videos.	0
784	7042	7042	S12-6	Source Datasets	37	64	3.0	2.0	The sentences can be taken to be roughly parallel descriptions, and they included sentences for many languages.	0
785	7043	7043	S12-6	Source Datasets	38	65	3.0	2.0	Figure 1 shows a video and corresponding descriptions.	0
786	7044	7044	S12-6	Source Datasets	39	66	3.0	2.0	The sampling procedure from this dataset is similar to that for MSRpar.	0
787	7045	7045	S12-6	Source Datasets	40	67	3.0	2.0	We construct two bags of data to draw samples.	0
788	7046	7046	S12-6	Source Datasets	41	68	3.0	2.0	The first includes all possible pairs for the same video, and the second includes pairs taken from different videos.	0
789	7047	7047	S12-6	Source Datasets	42	69	3.0	2.0	Note that not all sentences from the same video were equivalent, as some descriptions were contradictory or unrelated.	0
790	7048	7048	S12-6	Source Datasets	43	70	3.0	2.0	Conversely, not all sentences coming from different videos were necessarily unrelated, as many videos were on similar topics.	0
791	7049	7049	S12-6	Source Datasets	44	71	3.0	2.0	We took an equal number of samples from each of these two sets, in an attempt to provide a balanced dataset between equivalent and non-equivalent pairs.	0
792	7050	7050	S12-6	Source Datasets	45	72	3.0	2.0	The sampling was also done according to string similarity, but in four bands in the [0.5 .. 0.8] range, as sentences from the same video had a usually higher string similarity than those in the MSRpar dataset.	0
793	7051	7051	S12-6	Source Datasets	46	73	3.0	2.0	We sampled 1500 pairs overall, which we split 50% for training and 50% for testing.	0
794	7052	7052	S12-6	Source Datasets	47	74	4.0	2.0	Given the strong connection between STS systems and Machine Translation evaluation metrics, we also sampled pairs of segments that had been part of human evaluation exercises.	0
795	7053	7053	S12-6	Source Datasets	48	75	4.0	2.0	Those pairs included a reference translation and a automatic Machine Translation system submission, as follows:	0
796	7054	7054	S12-6	Source Datasets	49	76	4.0	2.0	The only instance in which no tax is levied is when the supplier is in a non-EU country and the recipient is in a Member State of the EU.	0
797	7055	7055	S12-6	Source Datasets	50	77	4.0	2.0	"The only case for which no tax is still perceived ""is an example of supply in the European Community from a third country."	0
798	7056	7056	S12-6	Source Datasets	51	78	4.0	2.0	We selected pairs from the translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2007;Callison-Burch et al., 2008).	0
799	7057	7057	S12-6	Source Datasets	52	79	4.0	2.0	For consistency, we only used French to English system submissions.	0
800	7058	7058	S12-6	Source Datasets	53	80	4.0	2.0	The training data includes all of the Europarl human ranked fr-en system submissions from WMT 2007, with each machine translation being paired with the correct reference translation.	0
801	7059	7059	S12-6	Source Datasets	54	81	4.0	2.0	This resulted in 729 unique training pairs.	0
802	7060	7060	S12-6	Source Datasets	55	82	4.0	2.0	The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less.	0
803	7061	7061	S12-6	Source Datasets	56	83	4.0	2.0	In addition, we selected two other datasets that were used as out-of-domain testing.	0
804	7062	7062	S12-6	Source Datasets	57	84	4.0	2.0	One of them comprised of all the human ranked fr-en system submissions from the WMT 2007 news conversation test set, resulting in 351 unique system reference pairs.	0
805	7063	7063	S12-6	Source Datasets	58	85	4.0	2.0	2	0
806	7064	7064	S12-6	Source Datasets	59	86	4.0	2.0	The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.1 (Fellbaum, 1998) senses.	0
807	7065	7065	S12-6	Source Datasets	60	87	4.0	2.0	The mapping of the senses of both resources comprised 110K sense pairs.	0
808	7066	7066	S12-6	Source Datasets	61	88	4.0	2.0	The similarity between the sense pairs was generated using simple word overlap.	0
809	7067	7067	S12-6	Source Datasets	62	89	4.0	2.0	50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another.	0
810	7068	7068	S12-6	Annotation	1	90	1.0	2.0	In this first dataset we defined a straightforward likert scale ranging from 5 to 0, but we decided to provide definitions for each value in the scale (cf. Figure 2).	0
811	7069	7069	S12-6	Annotation	2	91	1.0	2.0	We first did pilot annotations of 200 pairs se-lected at random from the three main datasets in the training set.	0
812	7070	7070	S12-6	Annotation	3	92	1.0	2.0	We did the annotation, and the pairwise Pearson ranged from 84% to 87% among ourselves.	0
813	7071	7071	S12-6	Annotation	4	93	1.0	2.0	The agreement of each annotator with the average scores of the other was between 87% and 89%.	0
814	7072	7072	S12-6	Annotation	5	94	2.0	2.0	In the future, we would like to explore whether the definitions improve the consistency of the tagging with respect to a likert scale without definitions.	0
815	7073	7073	S12-6	Annotation	6	95	2.0	2.0	Note also that in the assessment of the quality and evaluation of the systems performances, we just took the resulting SS scores and their averages.	0
816	7074	7074	S12-6	Annotation	7	96	2.0	2.0	Using the qualitative descriptions for each score in analysis and evaluation is left for future work.	0
817	7075	7075	S12-6	Annotation	8	97	2.0	2.0	Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.	0
818	7076	7076	S12-6	Annotation	9	98	3.0	3.0	The turkers were required to have achieved a 95% of approval rating in their previous HITs, and had to pass a qualification task which included 6 example pairs.	0
819	7077	7077	S12-6	Annotation	10	99	3.0	3.0	Each HIT included 5 pairs of sentences, and was paid at 0.20$ each.	0
820	7078	7078	S12-6	Annotation	11	100	3.0	3.0	We collected 5 annotations per HIT.	0
821	7079	7079	S12-6	Annotation	12	101	3.0	3.0	In the latest data collection, each HIT required 114.9 second for completion.	0
822	7080	7080	S12-6	Annotation	13	102	4.0	3.0	In order to ensure the quality, we also performed post-hoc validation.	0
823	7081	7081	S12-6	Annotation	14	103	4.0	3.0	Each HIT contained one pair from our pilot.	0
824	7082	7082	S12-6	Annotation	15	104	4.0	3.0	After the tagging was completed we checked the correlation of each individual turker with our scores, and removed annotations of turkers which had low correlations (below 50%).	0
825	7083	7083	S12-6	Annotation	16	105	4.0	3.0	Given the high quality of the annotations among the turkers, we could alternatively use the correlation between the turkers itself to detect poor quality annotators.	0
826	7084	7084	S12-6	Systems Evaluation	1	106	1.0	3.0	Given two sentences, s1 and s2, an STS system would need to return a similarity score.	1
827	7085	7085	S12-6	Systems Evaluation	2	107	1.0	3.0	Participants can also provide a confidence score indicating their confidence level for the result returned for each pair, but this confidence is not used for the main results.	1
828	7086	7086	S12-6	Systems Evaluation	3	108	2.0	3.0	The output of the systems performance is evaluated using the Pearson product-moment correlation coefficient between the system scores and the human scores, as customary in text similarity (Rubenstein and Goodenough, 1965).	0
829	7087	7087	S12-6	Systems Evaluation	4	109	2.0	3.0	We calculated Pearson for each evaluation dataset separately.	0
830	7088	7088	S12-6	Systems Evaluation	5	110	2.0	3.0	In order to have a single Pearson measure for each system we concatenated the gold standard (and system outputs) for all 5 datasets into a single gold stan-dard file (and single system output).	0
831	7089	7089	S12-6	Systems Evaluation	6	111	3.0	3.0	The first version of the results were published using this method, but the overall score did not correspond well to the individual scores in the datasets, and participants proposed two additional evaluation metrics, both of them based on Pearson correlation.	0
832	7090	7090	S12-6	Systems Evaluation	7	112	3.0	3.0	The organizers of the task decided that it was more informative, and on the benefit of the community, to also adopt those evaluation metrics, and the idea of having a single main evaluation metric was dropped.	0
833	7091	7091	S12-6	Systems Evaluation	8	113	4.0	3.0	This decision was not without controversy, but the organizers gave more priority to openness and inclusiveness and to the involvement of participants.	0
834	7092	7092	S12-6	Systems Evaluation	9	114	4.0	3.0	The final result table thus included three evaluation metrics.	0
835	7093	7093	S12-6	Systems Evaluation	10	115	4.0	3.0	For the future we plan to analyze the evaluation metrics, including non-parametric metrics like Spearman.	0
836	7094	7094	S12-6	Evaluation metrics	1	116	1.0	3.0	The first evaluation metric is the Pearson correlation for the concatenation of all five datasets, as described above.	0
837	7095	7095	S12-6	Evaluation metrics	2	117	1.0	3.0	We will use overall Pearson or simply ALL to refer to this measure.	0
838	7096	7096	S12-6	Evaluation metrics	3	118	1.0	3.0	The second evaluation metric normalizes the output for each dataset separately, using the linear least squares method.	0
839	7097	7097	S12-6	Evaluation metrics	4	119	2.0	3.0	We concatenated the system results for five datasets and then computed a single Pearson correlation.	0
840	7098	7098	S12-6	Evaluation metrics	5	120	2.0	3.0	Given Y = {y i } and X = {x i } (the gold standard scores and the system scores, respectively), we transform the system scores into X = {x i } in order to minimize the squared error i (y i − x i ) 2 .	0
841	7099	7099	S12-6	Evaluation metrics	6	121	2.0	3.0	The linear transformation is given by	0
842	7100	7100	S12-6	Evaluation metrics	7	122	3.0	3.0	where β 1 and β 2 are found analytically.	0
843	7101	7101	S12-6	Evaluation metrics	8	123	3.0	3.0	We refer to this measure as Normalized Pearson or simply ALLnorm.	0
844	7102	7102	S12-6	Evaluation metrics	9	124	3.0	3.0	This metric was suggested by one of the participants, Sergio Jimenez.	0
845	7103	7103	S12-6	Evaluation metrics	10	125	4.0	3.0	The third evaluation metric is the weighted mean of the Pearson correlations on individual datasets.	0
846	7104	7104	S12-6	Evaluation metrics	11	126	4.0	3.0	The Pearson returned for each dataset is weighted according to the number of sentence pairs in that dataset.	0
847	7105	7105	S12-6	Evaluation metrics	12	127	4.0	3.0	Given r i the five Pearson scores for each dataset, and n i the number of pairs in each dataset, the weighted mean is given as i=1..5 (r i * n i )/ i=1..5 n i	0
848	7106	7106	S12-6	Evaluation metrics	13	128	4.0	3.0	We refer to this measure as weighted mean of Pearson or Mean for short.	0
849	7107	7107	S12-6	Using confidence scores	1	129	2.0	3.0	Participants were allowed to include a confidence score between 1 and 100 for each of their scores.	0
850	7108	7108	S12-6	Using confidence scores	2	130	3.0	3.0	We used weighted Pearson to use those confidence scores 3 . Table 2 includes the list of systems which provided a non-uniform confidence.	0
851	7109	7109	S12-6	Using confidence scores	3	131	4.0	3.0	The results show that some systems were able to improve their correlation, showing promise for the usefulness of confidence in applications.	0
852	7110	7110	S12-6	The Baseline System	1	132	1.0	3.0	We produced scores using a simple word overlap baseline system.	0
853	7111	7111	S12-6	The Baseline System	2	133	2.0	3.0	We tokenized the input sentences splitting at white spaces, and then represented each sentence as a vector in the multidimensional token space.	0
854	7112	7112	S12-6	The Baseline System	3	134	2.0	3.0	Each dimension had 1 if the token was present in the sentence, 0 otherwise.	0
855	7113	7113	S12-6	The Baseline System	4	135	3.0	3.0	Similarity of vectors was computed using cosine similarity.	0
856	7114	7114	S12-6	The Baseline System	5	136	4.0	3.0	We also run a random baseline several times, yielding close to 0 correlations in all datasets, as expected.	0
857	7115	7115	S12-6	The Baseline System	6	137	4.0	3.0	We will refer to the random baseline again in Section 4.5.	0
858	7116	7116	S12-6	Participation	1	138	1.0	3.0	Participants could send a maximum of three system runs.	0
859	7117	7117	S12-6	Participation	2	139	1.0	3.0	After downloading the test datasets, they had a maximum of 120 hours to upload the results.	0
860	7118	7118	S12-6	Participation	3	140	1.0	3.0	35 teams participated, submitting 88 system runs (cf. first column of Table 1).	0
861	7119	7119	S12-6	Participation	4	141	2.0	3.0	Due to lack of space we can't detail the full names of authors and institutions that participated.	0
862	7120	7120	S12-6	Participation	5	142	2.0	3.0	The interested reader can use the name of the runs to find the relevant paper in these proceedings.	0
863	7121	7121	S12-6	Participation	6	143	2.0	3.0	There were several issues in the submissions.	0
864	7122	7122	S12-6	Participation	7	144	2.0	3.0	The submission software did not ensure that the naming conventions were appropriately used, and this caused some submissions to be missed, and in two cases the results were wrongly assigned.	0
865	7123	7123	S12-6	Participation	8	145	3.0	3.0	Some participants returned Not-a-Number as a score, and the organizers had to request whether those where to be taken as a 0 or as a 5.	0
866	7124	7124	S12-6	Participation	9	146	3.0	3.0	Finally, one team submitted past the 120 hour deadline and some teams sent missing files after the deadline.	0
867	7125	7125	S12-6	Participation	10	147	3.0	4.0	All those are explicitly marked in Table 1.	0
868	7126	7126	S12-6	Participation	11	148	3.0	4.0	The teams that included one of the organizers are also explicitly marked.	0
869	7127	7127	S12-6	Participation	12	149	4.0	4.0	We want to stress that in these teams the organizers did not allow the developers of the system to access any data or information which was not available for the rest of participants.	0
870	7128	7128	S12-6	Participation	13	150	4.0	4.0	One exception is weiwei, as they generated the 110K OntoNotes-Word	0
871	7129	7129	S12-6	Participation	14	151	4.0	4.0	Net dataset from which the other organizers sampled the surprise data set.	0
872	7130	7130	S12-6	Participation	15	152	4.0	4.0	After the submission deadline expired, the organizers published the gold standard in the task website, in order to ensure a transparent evaluation process.	0
873	7131	7131	S12-6	Results	1	153	1.0	4.0	Table 1 shows the results for each run in alphabetic order.	0
874	7132	7132	S12-6	Results	2	154	1.0	4.0	Each result is followed by the rank of the system according to the given evaluation measure.	0
875	7133	7133	S12-6	Results	3	155	1.0	4.0	To the right, the Pearson score for each dataset is given.	0
876	7134	7134	S12-6	Results	4	156	2.0	4.0	In boldface, the three best results in each column.	0
877	7135	7135	S12-6	Results	5	157	2.0	4.0	First of all we want to stress that the large majority of the systems are well above the simple baseline, although the baseline would rank 70 on the Mean measure, improving over 19 runs.	0
878	7136	7136	S12-6	Results	6	158	2.0	4.0	The correlation for the non-MT datasets were really high: the highest correlation was obtained was for MSRvid (0.88 r), followed by MSRpar (0.73 r) and On-WN (0.73 r).	0
879	7137	7137	S12-6	Results	7	159	2.0	4.0	The results for the MT evaluation data are lower, (0.57 r) for SMT-eur and (0.61 r) for SMT-News.	0
880	7138	7138	S12-6	Results	8	160	3.0	4.0	The simple token overlap baseline, on the contrary, obtained the highest results for On-WN (0.59 r), with (0.43 r) on MSRpar and (0.40 r) on MSRvid.	0
881	7139	7139	S12-6	Results	9	161	3.0	4.0	The results for MT evaluation data are also reversed, with (0.40 r) for SMT-eur and (0.45 r) for SMT-News.	0
882	7140	7140	S12-6	Results	10	162	3.0	4.0	The ALLnorm measure yields the highest correlations.	0
883	7141	7141	S12-6	Results	11	163	4.0	4.0	This comes at no surprise, as it involves a normalization which transforms the system outputs using the gold standard.	0
884	7142	7142	S12-6	Results	12	164	4.0	4.0	In fact, a random baseline which gets Pearson correlations close to 0 in all datasets would attain Pearson of 0.5891 4 .	0
885	7143	7143	S12-6	Results	13	165	4.0	4.0	Although not included in the results table for lack of space, we also performed an analysis of confidence intervals.	0
886	7144	7144	S12-6	Results	14	166	4.0	4.0	For instance, the best run according to ALL (r = .8239) has a 95% confidence interval of [.8123,.8349] and the second a confidence interval of [.8016,.8254], meaning that the differences are not statistically different.	0
887	7145	7145	S12-6	Tools and resources used	1	167	1.0	4.0	The organizers asked participants to submit a description file, special emphasis on the tools and resources that they used.	0
888	7146	7146	S12-6	Tools and resources used	2	168	1.0	4.0	fied way the tools and resources used by those participants that did submit a valid description file.	0
889	7147	7147	S12-6	Tools and resources used	3	169	2.0	4.0	In the last row, the totals show that WordNet was the most used resource, followed by monolingual corpora and Wikipedia.	0
890	7148	7148	S12-6	Tools and resources used	4	170	2.0	4.0	Acronyms, dictionaries, multilingual corpora, stopword lists and tables of paraphrases were also used.	0
891	7149	7149	S12-6	Tools and resources used	5	171	2.0	4.0	Generic NLP tools like lemmatization and PoS tagging were widely used, and to a lesser extent, parsing, word sense disambiguation, semantic role labeling and time and date resolution (in this order).	0
892	7150	7150	S12-6	Tools and resources used	6	172	3.0	4.0	Knowledge-based and distributional methods got used nearly equally, and to a lesser extent, alignment and/or statistical machine translation software, lexical substitution, string similarity, textual entailment and machine translation evaluation software.	0
893	7151	7151	S12-6	Tools and resources used	7	173	3.0	4.0	Machine learning was widely used to combine and tune components.	0
894	7152	7152	S12-6	Tools and resources used	8	174	4.0	4.0	Several less used tools were also listed but were used by three or less systems.	0
895	7153	7153	S12-6	Tools and resources used	9	175	4.0	4.0	The top scoring systems tended to use most of the resources and tools listed (UKP, Takelab), with some notable exceptions like Sgjimenez which was based on string similarity.	0
896	7154	7154	S12-6	Tools and resources used	10	176	4.0	4.0	For a more detailed analysis, the reader is directed to the papers of the participants in this volume.	0
897	7155	7155	S12-6	Conclusions and Future Work	1	177	1.0	4.0	This paper presents the SemEval 2012 pilot evaluation exercise on Semantic Textual Similarity.	0
898	7156	7156	S12-6	Conclusions and Future Work	2	178	1.0	4.0	A simple definition of STS beyond the likert-scale was set up, and a wealth of annotated data was produced.	0
899	7157	7157	S12-6	Conclusions and Future Work	3	179	1.0	4.0	The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk.	0
900	7158	7158	S12-6	Conclusions and Future Work	4	180	1.0	4.0	The dataset includes 1500 sentence pairs from MSRpar and MSRvid (each), ca. 1500 pairs from WMT, and 750 sentence pairs from a mapping between OntoNotes and WordNet senses.	0
901	7159	7159	S12-6	Conclusions and Future Work	5	181	2.0	4.0	The correlation be-tween non-expert annotators and annotations from the authors is very high, showing the high quality of the dataset.	0
902	7160	7160	S12-6	Conclusions and Future Work	6	182	2.0	4.0	The dataset was split 50% as train and test, with the exception of the surprise test datasets: a subset of WMT from a different domain and the OntoNotes-WordNet mapping.	0
903	7161	7161	S12-6	Conclusions and Future Work	7	183	2.0	4.0	All datasets are publicly available.	0
904	7162	7162	S12-6	Conclusions and Future Work	8	184	2.0	4.0	5	0
905	7163	7163	S12-6	Conclusions and Future Work	9	185	2.0	4.0	The exercise was very successful in participation and results.	0
906	7164	7164	S12-6	Conclusions and Future Work	10	186	3.0	4.0	35 teams participated, submitting 88 runs.	0
907	7165	7165	S12-6	Conclusions and Future Work	11	187	3.0	4.0	The best results scored a Pearson correlation over 80%, well beyond a simple lexical baseline with 31% of correlation.	0
908	7166	7166	S12-6	Conclusions and Future Work	12	188	3.0	4.0	The metric for evaluation was not completely satisfactory, and three evaluation metrics were finally published.	0
909	7167	7167	S12-6	Conclusions and Future Work	13	189	3.0	4.0	We discuss the shortcomings of those measures.	0
910	7168	7168	S12-6	Conclusions and Future Work	14	190	3.0	4.0	There are several tasks ahead in order to make STS a mature field.	0
911	7169	7169	S12-6	Conclusions and Future Work	15	191	4.0	4.0	The first is to find a satisfactory evaluation metric.	0
912	7170	7170	S12-6	Conclusions and Future Work	16	192	4.0	4.0	The second is to analyze the definition of the task itself, with a thorough analysis of the definitions in the likert scale.	0
913	7171	7171	S12-6	Conclusions and Future Work	17	193	4.0	4.0	We would also like to analyze the relation between the STS scores and the paraphrase judgements in MSR, as well as the human evaluations in WMT.	0
914	7172	7172	S12-6	Conclusions and Future Work	18	194	4.0	4.0	Finally, we would also like to set up an open framework where NLP components and similarity algorithms can be combined by the community.	0
915	7173	7173	S12-6	Conclusions and Future Work	19	195	4.0	4.0	All in all, we would like this dataset to be the focus of the community working on algorithmic approaches for semantic processing and inference at large.	0
916	8118	8118	S13-5	title	1	1	4.0	1.0	SemEval-2013 Task 5: Evaluating Phrasal Semantics	0
917	8119	8119	S13-5	abstract	1	2	1.0	1.0	"This paper describes the SemEval-2013 Task 5: ""Evaluating Phrasal Semantics""."	0
918	8120	8120	S13-5	abstract	2	3	2.0	1.0	Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length.	0
919	8121	8121	S13-5	abstract	3	4	3.0	1.0	The second one addresses deciding the compositionality of phrases in a given context.	0
920	8122	8122	S13-5	abstract	4	5	4.0	1.0	The paper discusses the importance and background of these subtasks and their structure.	0
921	8123	8123	S13-5	abstract	5	6	4.0	1.0	In succession, it introduces the systems that participated and discusses evaluation results.	0
922	8124	8124	S13-5	Introduction	1	7	1.0	1.0	Numerous past tasks have focused on leveraging the meaning of word types or words in context.	0
923	8125	8125	S13-5	Introduction	2	8	1.0	1.0	Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.	0
924	8126	8126	S13-5	Introduction	3	9	1.0	1.0	As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.	0
925	8127	8127	S13-5	Introduction	4	10	1.0	1.0	In this paper, we present two subtasks designed to evaluate such phrasal models: a. Semantic similarity of words and compositional phrases b.	1
926	8128	8128	S13-5	Introduction	5	11	1.0	1.0	Evaluating the compositionality of phrases in context	1
927	8129	8129	S13-5	Introduction	6	12	2.0	1.0	"For example, the first subtask addresses computing how similar the word ""valuation"" is to the compositional sequence ""price assessment"", while the second subtask addresses deciding whether the phrase ""piece of cake"" is used literally or figuratively in the sentence ""Labour was a piece of cake!""."	0
928	8130	8130	S13-5	Introduction	7	13	2.0	1.0	The aim of these subtasks is two-fold.	0
929	8131	8131	S13-5	Introduction	8	14	2.0	1.0	Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.	0
930	8132	8132	S13-5	Introduction	9	15	2.0	1.0	It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.	0
931	8133	8133	S13-5	Introduction	10	16	2.0	1.0	Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.	0
932	8134	8134	S13-5	Introduction	11	17	2.0	1.0	Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.	0
933	8135	8135	S13-5	Introduction	12	18	3.0	1.0	We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.	0
934	8136	8136	S13-5	Introduction	13	19	3.0	1.0	Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.	0
935	8137	8137	S13-5	Introduction	14	20	3.0	1.0	In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.	0
936	8138	8138	S13-5	Introduction	15	21	3.0	1.0	The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.	0
937	8139	8139	S13-5	Introduction	16	22	3.0	1.0	Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.	0
938	8140	8140	S13-5	Introduction	17	23	3.0	1.0	In subsection 3.1 the subtask is described in detail together with some information about its background.	0
939	8141	8141	S13-5	Introduction	18	24	4.0	1.0	Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.	0
940	8142	8142	S13-5	Introduction	19	25	4.0	1.0	Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.	0
941	8143	8143	S13-5	Introduction	20	26	4.0	1.0	Subsection 4.1 explains the data creation process for this subtask.	0
942	8144	8144	S13-5	Introduction	21	27	4.0	1.0	In subsection 4.2 the evaluation statistics of participating systems are presented.	0
943	8145	8145	S13-5	Introduction	22	28	4.0	1.0	Section 5 is a discussion about the conclusions of the entire task.	0
944	8146	8146	S13-5	Introduction	23	29	4.0	1.0	Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.	0
945	8147	8147	S13-5	Data Sources &amp; Methodology	1	30	1.0	1.0	Data instances of both subtasks are drawn from the large-scale, freely available WaCky corpora (Baroni et al., 2009).	0
946	8148	8148	S13-5	Data Sources &amp; Methodology	2	31	1.0	1.0	The resource contains corpora in 4 languages: English, French, German and Italian.	0
947	8149	8149	S13-5	Data Sources &amp; Methodology	3	32	1.0	1.0	The English corpus, ukWaC, consists of 2 billion words and was constructed by crawling to the .uk domain of the web and using medium-frequency words from the BNC as seeds.	0
948	8150	8150	S13-5	Data Sources &amp; Methodology	4	33	1.0	1.0	The corpus is part-of-speech (PoS) tagged and lemmatized using the TreeTagger (Schmid, 1994).	0
949	8151	8151	S13-5	Data Sources &amp; Methodology	5	34	1.0	1.0	The French corpus, frWaC, contains 1.6 billion word corpus and was constructed by web-crawling the .fr domain and using mediumfrequency words from the Le Monde Diplomatique corpus and basic French vocabulary lists as seeds.	0
950	8152	8152	S13-5	Data Sources &amp; Methodology	6	35	1.0	1.0	The corpus was PoS tagged and lemmatized with the TreeTagger.	0
951	8153	8153	S13-5	Data Sources &amp; Methodology	7	36	2.0	1.0	The French corpus, deWaC, consists of 1.7 billion word corpus and was constructed by crawling the .de domain and using mediumfrequency words from the SudDeutsche Zeitung cor-pus and basic German vocabulary lists as seeds.	0
952	8154	8154	S13-5	Data Sources &amp; Methodology	8	37	2.0	1.0	The corpus was PoS tagged and lemmatized with the TreeTagger.	0
953	8155	8155	S13-5	Data Sources &amp; Methodology	9	38	2.0	1.0	The Italian corpus, itWaC, is a 2 billion word corpus constructed from the .it domain of the web using medium-frequency words from the Repubblica corpus and basic Italian vocabulary lists as seeds.	0
954	8156	8156	S13-5	Data Sources &amp; Methodology	10	39	2.0	1.0	The corpus was PoS tagged with the Tree-Tagger, and lemmatized using the Morph-it!	0
955	8157	8157	S13-5	Data Sources &amp; Methodology	11	40	2.0	1.0	lexicon (Zanchetta and Baroni, 2005).	0
956	8158	8158	S13-5	Data Sources &amp; Methodology	12	41	2.0	1.0	Several versions of the WaCky corpora, with various extra annotations or modifications are also available 1 .	0
957	8159	8159	S13-5	Data Sources &amp; Methodology	13	42	2.0	1.0	We ensured that data instances occur frequently enough in the WaCky corpora, so that participating systems could gather statistics for building distributional vectors or other uses.	0
958	8160	8160	S13-5	Data Sources &amp; Methodology	14	43	3.0	1.0	As the evaluation data only contains very small annotated samples from freely available web documents, and the original source is provided, we could provide them without violating copyrights.	0
959	8161	8161	S13-5	Data Sources &amp; Methodology	15	44	3.0	1.0	The size of the WaCky corpora is suitable for training reliable distributional models.	0
960	8162	8162	S13-5	Data Sources &amp; Methodology	16	45	3.0	1.0	Sentences are already lemmatized and part-of-speech tagged.	0
961	8163	8163	S13-5	Data Sources &amp; Methodology	17	46	3.0	1.0	Participating approaches making use of distributional methods, part-of-speech tags or lemmas, were strongly encouraged to use these corpora and their shared preprocessing, to ensure the highest possible comparability of results.	0
962	8164	8164	S13-5	Data Sources &amp; Methodology	18	47	3.0	1.0	Additionally, this had the potential to considerably reduce the workload of participants.	0
963	8165	8165	S13-5	Data Sources &amp; Methodology	19	48	3.0	1.0	For the first subtask, data were provided in English, German and Italian and for the second subtask in English and German.	0
964	8166	8166	S13-5	Data Sources &amp; Methodology	20	49	3.0	1.0	The range of methods applicable to both subtasks was deliberately not limited to any specific branch of methods, such as distributional or vector models of semantic compositionality.	0
965	8167	8167	S13-5	Data Sources &amp; Methodology	21	50	4.0	1.0	We believe that the subtasks can be tackled from different directions and we expect a great deal of the scientific benefit to lie in the comparison of very different approaches, as well as how these approaches can be combined.	0
966	8168	8168	S13-5	Data Sources &amp; Methodology	22	51	4.0	2.0	An exception to this rule is the fact that participants in the first subtask were not allowed to use directly definitions extracted from dictionaries or lexicons.	0
967	8169	8169	S13-5	Data Sources &amp; Methodology	23	52	4.0	2.0	Since the subtask is considered fundamental and its data were created from online knowledge resources, systems using the same tools to address it would be of limited use.	0
968	8170	8170	S13-5	Data Sources &amp; Methodology	24	53	4.0	2.0	However, participants were allowed to use other information residing in dictionaries, such as Wordnet synsets or synset relations.	0
969	8171	8171	S13-5	Data Sources &amp; Methodology	25	54	4.0	2.0	Participating systems were allowed to attempt one or both subtasks, in one or all of the languages supported.	0
970	8172	8172	S13-5	Data Sources &amp; Methodology	26	55	4.0	2.0	However, it was expected that systems performing well at the first basic subtask would provide a good starting point for dealing with the second subtask, which is considered harder.	0
971	8173	8173	S13-5	Data Sources &amp; Methodology	27	56	4.0	2.0	Moreover, language-independent models were of special interest.	0
972	8174	8174	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	1	57	1.0	2.0	The aim of this subtask is to evaluate the component of a semantic model that computes the similarity between word sequences of different length.	0
973	8175	8175	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	2	58	2.0	2.0	Participating systems are asked to estimate the semantic similarity of a word and a short sequence of two words.	0
974	8176	8176	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	3	59	3.0	2.0	For example, they should be able to figure out that contact and close interaction are similar whereas megalomania and great madness are not.	0
975	8177	8177	S13-5	Subtask 5a: Semantic Similarity of Words and Compositional Phrases	4	60	4.0	2.0	This subtask addresses a core problem, since satisfactory performance in computing the similarity of full sentences depends on similarity computations on shorter sequences.	0
976	8178	8178	S13-5	Background and Description	1	61	1.0	2.0	This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step.	0
977	8179	8179	S13-5	Background and Description	2	62	1.0	2.0	For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008).	0
978	8180	8180	S13-5	Background and Description	3	63	2.0	2.0	Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010;	0
979	8181	8181	S13-5	Background and Description	4	64	2.0	2.0	Baroni and Zamparelli, 2010;Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words.	0
980	8182	8182	S13-5	Background and Description	5	65	3.0	2.0	As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word.	0
981	8183	8183	S13-5	Background and Description	6	66	3.0	2.0	This is important as it is the basic step to analyse models that can compare any word sequences of different length.	0
982	8184	8184	S13-5	Background and Description	7	67	4.0	2.0	The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010).	0
983	8185	8185	S13-5	Background and Description	8	68	4.0	2.0	Dictionaries were used as sources of contact/[kon-takt]	0
984	8186	8186	S13-5	Background and Description	9	69	4.0	2.0	1. the act or state of touching; a touching or meeting, as of two things or people.	0
985	8187	8187	S13-5	close interaction	1	70	1.0	2.0	3. an acquaintance, colleague, or relative through whom a person can gain access to information, favors, influential people, and the like.	0
986	8188	8188	S13-5	close interaction	2	71	1.0	2.0	Figure 1 presents the definition of the word contact, from which the pair (contact, close interaction) can be extracted.	0
987	8189	8189	S13-5	close interaction	3	72	2.0	2.0	Such equivalences extracted from dictionaries can be seen as natural and unbiased data instances.	0
988	8190	8190	S13-5	close interaction	4	73	2.0	2.0	This idea opens numerous opportunities:	0
989	8191	8191	S13-5	close interaction	5	74	2.0	2.0	•	0
990	8192	8192	S13-5	close interaction	6	75	3.0	2.0	Since definitions in dictionaries are syntactically rich, we are able to create examples for different syntactic relations.	0
991	8193	8193	S13-5	close interaction	7	76	3.0	2.0	•	0
992	8194	8194	S13-5	close interaction	8	77	4.0	2.0	We have the opportunity to extract positive examples for languages for which dictionaries with sufficient entries are available.	0
993	8195	8195	S13-5	close interaction	9	78	4.0	2.0	Negative examples were generated by matching words under definition with randomly chosen defining sequences.	0
994	8196	8196	S13-5	close interaction	10	79	4.0	2.0	In the following subsection, we provide details about the application of this idea to build the development and testing set for subtask 5a.	0
995	8197	8197	S13-5	Data Creation	1	80	1.0	2.0	Data for this subtask were provided in English, German and Italian.	0
996	8198	8198	S13-5	Data Creation	2	81	1.0	2.0	Pairs of words under definitions and defining sequences were extracted from the English, German and Italian part of Wiktionary, respectively.	0
997	8199	8199	S13-5	Data Creation	3	82	1.0	2.0	In particular, for each language, all Wiktionary entries were downloaded and part-of-speech tagged using the Genia tagger (Tsuruoka et al., 2005  were kept, only.	0
998	8200	8200	S13-5	Data Creation	4	83	1.0	2.0	For the purpose of extracting word and sequence pairs for this subtask, we consider as noun phrases, sequences that consist of adjectives or noun and end with a noun.	0
999	8201	8201	S13-5	Data Creation	5	84	1.0	2.0	In cases where the extracted noun phrase was longer than two words, the right-most two sequences were kept, since in most cases noun phrases are governed by their rightmost component.	0
1000	8202	8202	S13-5	Data Creation	6	85	1.0	2.0	Subsequently, we discarded instances whose words occur too infrequently in the WaCky corpora (Baroni et al., 2009) of each language.	0
1001	8203	8203	S13-5	Data Creation	7	86	2.0	2.0	WaCky corpora are available freely and are large enough for participating systems to extract distributional statistics.	0
1002	8204	8204	S13-5	Data Creation	8	87	2.0	2.0	Taking the numbers of extracted instances into account, we set the frequency thresholds at 10 occurrences for English and 5 for German and Italian.	0
1003	8205	8205	S13-5	Data Creation	9	88	2.0	2.0	Data instances extracted following this process were then checked by a computational linguist.	0
1004	8206	8206	S13-5	Data Creation	10	89	2.0	2.0	Candidate pairs in which the definition sequence was not judged to be a precise and adequate definition of the word under definition were discarded.	0
1005	8207	8207	S13-5	Data Creation	11	90	2.0	2.0	These cases were very limited and mostly account for shortcomings of the very simple pattern used for extraction.	0
1006	8208	8208	S13-5	Data Creation	12	91	2.0	2.0	"For example, the pair (standard, transmission vehicle) coming from the definition of ""standard"" as ""A manual transmission vehicle"" was discarded."	0
1007	8209	8209	S13-5	Data Creation	13	92	3.0	2.0	Similarly in German, the pair (Fremde (Eng. stranger), weibliche Person (Eng. female person)) was discarded.	0
1008	8210	8210	S13-5	Data Creation	14	93	3.0	2.0	"""Fremde"", which is of female grammatical genre, was defined as ""weibliche Person, die man nicht kennt (Eng. female person, one does not know)""."	0
1009	8211	8211	S13-5	Data Creation	15	94	3.0	2.0	"In Italian, the pair (paese (Eng. land, country, region), grande estensione (Eng. large tract)) was discarded, since the original definition was ""grande estensione di terreno abitato e generalmente coltivato (Eng. large tract of land inhabited and cultivated in general)""."	0
1010	8212	8212	S13-5	Data Creation	16	95	3.0	2.0	The final data sets were divided into training and held-out testing sets, according to a 60% and 40% ratio, respectively.	0
1011	8213	8213	S13-5	Data Creation	17	96	3.0	2.0	The first three rows of table 1 present the numbers of the train and test sets for the three languages chosen.	0
1012	8214	8214	S13-5	Data Creation	18	97	3.0	2.0	It was identified that a fair percentage of the German instances (approximately 27%) refer to the definitions of first names or family names.	0
1013	8215	8215	S13-5	Data Creation	19	98	4.0	2.0	This is probably a flaw of the German part of Wiktionary.	0
1014	8216	8216	S13-5	Data Creation	20	99	4.0	2.0	In addition, the pattern used for extraction happens to apply to the definitions of names.	0
1015	8217	8217	S13-5	Data Creation	21	100	4.0	2.0	Name instances were discarded from the German data set to produce the data set described in the last row of table 1.	0
1016	8218	8218	S13-5	Data Creation	22	101	4.0	2.0	The training set was released approximately 3 months earlier than the test data.	0
1017	8219	8219	S13-5	Data Creation	23	102	4.0	3.0	Instances in both set ware annotated as positive or negative.	0
1018	8220	8220	S13-5	Data Creation	24	103	4.0	3.0	Test set annotations were not released to the participants, but were used for evaluation, only.	0
1019	8221	8221	S13-5	Results	1	104	1.0	3.0	Participating systems were evaluated on their ability to predict correctly whether the components of each test instance, i.e. word-sequence pair, are semantically similar or distinct.	0
1020	8222	8222	S13-5	Results	2	105	1.0	3.0	Participants were allowed to use or ignore the training data, i.e. the systems could be supervised or unsupervised.	0
1021	8223	8223	S13-5	Results	3	106	1.0	3.0	Unsupervised systems were allowed to use the training data for development and parameter tuning.	0
1022	8224	8224	S13-5	Results	4	107	1.0	3.0	Since this is a core task, participating systems were not be able to use dictionaries or other prefabricated lists.	0
1023	8225	8225	S13-5	Results	5	108	1.0	3.0	Instead, they were allowed to use distributional similarity models, selectional preferences, measures of semantic similarity etc.	0
1024	8226	8226	S13-5	Results	6	109	1.0	3.0	Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F 1 score (Radev et al., 2003).	0
1025	8227	8227	S13-5	Results	7	110	1.0	3.0	Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted.	0
1026	8228	8228	S13-5	Results	8	111	2.0	3.0	Five research teams participated.	0
1027	8229	8229	S13-5	Results	9	112	2.0	3.0	Ten system runs were submitted for English, one for German (on data set: German -no names) and one for Italian.	0
1028	8230	8230	S13-5	Results	10	113	2.0	3.0	Table 2 illustrates the results of the evaluation process.	0
1029	8231	8231	S13-5	Results	11	114	2.0	3.0	The teams of (HsH) (Wartena, 2013)  these approaches performed better than some supervised ones for this experiment.	0
1030	8232	8232	S13-5	Results	12	115	2.0	3.0	Below, we summarise the properties of participating systems.	0
1031	8233	8233	S13-5	Results	13	116	2.0	3.0	(HsH) (Wartena, 2013) used distributed similarity and especially random indexing to compute similarities between words and possible definitions, under the hypothesis that a word and its definition are distributionally more similar than a word and an arbitrary definition.	0
1032	8234	8234	S13-5	Results	14	117	2.0	3.0	Considering all open-class words, context vectors over the entire WaCky corpus were computed for the word under definition, the defining sequence, its component words separately, the addition and multiplication of the vectors of the component words and a general context vector.	0
1033	8235	8235	S13-5	Results	15	118	2.0	3.0	Then, various similarity measures were computed on the vectors, including an innovative length-normalised version of Jensen-Shannon divergence.	0
1034	8236	8236	S13-5	Results	16	119	3.0	3.0	The similarity values are used to train a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).	0
1035	8237	8237	S13-5	Results	17	120	3.0	3.0	The first approach (run 1) of CLaC (Siblini and Kosseim, 2013) is based on a weighted semantic network to measure semantic relatedness between the word and the components of the phrase.	0
1036	8238	8238	S13-5	Results	18	121	3.0	3.0	A PART classifier is used to generate a partial decision trained on the semantic relatedness information of the labelled training set.	0
1037	8239	8239	S13-5	Results	19	122	3.0	3.0	The second approach uses a supervised distributional method based on words frequently occurring in the Web1TB corpus to calculate relatedness.	0
1038	8240	8240	S13-5	Results	20	123	3.0	3.0	A JRip classifier is used to gen-erate rules trained on the semantic relatedness information of the training set.	0
1039	8241	8241	S13-5	Results	21	124	3.0	3.0	This approach was used in conjunction with the first one as a backup method (run 2).	0
1040	8242	8242	S13-5	Results	22	125	3.0	3.0	In addition, features generated by both approaches were used to train the JRIP classifier collectively (run 3).	0
1041	8243	8243	S13-5	Results	23	126	3.0	3.0	The first approach of MELODI (Van de Cruys et al., 2013), called lvw, uses a dependency-based vector space model computed over the ukWaC corpus, in combination with Latent Vector Weighting ( Van de Cruys et al., 2011).	0
1042	8244	8244	S13-5	Results	24	127	4.0	3.0	The system computes the similarity between the first noun and the head noun of the second phrase, which was weighted according to the semantics of the modifier.	0
1043	8245	8245	S13-5	Results	25	128	4.0	3.0	The second approach, called dm, used a dependency-based vector space model, but, unlike the first approach, disregarded the modifier in the defining sequence.	0
1044	8246	8246	S13-5	Results	26	129	4.0	3.0	Since both systems are unsupervised, the training data was used to train a similarity threshold parameter, only.	0
1045	8247	8247	S13-5	Results	27	130	4.0	3.0	UMCC DLSI-(EPS) (Dávila et al., 2013) locates the synsets of words in data instances and computes the semantic distances between each synset of the word under definition and each synsets of the defining sequence words.	0
1046	8248	8248	S13-5	Results	28	131	4.0	3.0	In succession, a classifier is trained using features based on distance and Word-Net relations.	0
1047	8249	8249	S13-5	Results	29	132	4.0	3.0	The first attempt of ITNLP (run 1) consisted of an SVM classifier trained on semantic similarity computations between the word under definition and the defining sequence in each instance.	0
1048	8250	8250	S13-5	Results	30	133	4.0	3.0	Their second attempt also uses an SVM, however trained on WordNet-based similarities.	0
1049	8251	8251	S13-5	Results	31	134	4.0	3.0	The third attempt of ITNLP is a combination of the previous two; it combines their features to train an SVM classifier.	0
1050	8252	8252	S13-5	Subtask 5b: Semantic Compositionality in Context	1	135	1.0	3.0	An interesting sub-problem of semantic compositionality is to decide whether a target phrase is used in its literal or figurative meaning in a given context.	0
1051	8253	8253	S13-5	Subtask 5b: Semantic Compositionality in Context	2	136	1.0	3.0	"For example ""big picture"" might be used literally as in Click here for a bigger picture or figuratively as in To solve this problem, you have to look at the bigger picture."	0
1052	8254	8254	S13-5	Subtask 5b: Semantic Compositionality in Context	3	137	1.0	3.0	"Another example is ""old school"" which can also be used literally or figuratively:"	0
1053	8255	8255	S13-5	Subtask 5b: Semantic Compositionality in Context	4	138	1.0	3.0	He will go down in history as one of the old school, a true gentlemen.	0
1054	8256	8256	S13-5	Subtask 5b: Semantic Compositionality in Context	5	139	2.0	3.0	vs. During the 1970's the hall of the old school was converted into the library.	0
1055	8257	8257	S13-5	Subtask 5b: Semantic Compositionality in Context	6	140	2.0	3.0	Being able to detect whether a phrase is used literally or figuratively is e.g. especially important for information retrieval, where figuratively used words should be treated separately to avoid false positives.	0
1056	8258	8258	S13-5	Subtask 5b: Semantic Compositionality in Context	7	141	2.0	3.0	For example, the example sentence	0
1057	8259	8259	S13-5	Subtask 5b: Semantic Compositionality in Context	8	142	2.0	3.0	He will go down in history as one of the old school, a true gentlemen.	0
1058	8260	8260	S13-5	Subtask 5b: Semantic Compositionality in Context	9	143	3.0	3.0	"should probably not be retrieved for the query ""school""."	0
1059	8261	8261	S13-5	Subtask 5b: Semantic Compositionality in Context	10	144	3.0	3.0	"Rather, the insights generated from subtask 5a could be utilized to retrieve sentences using a similar phrase such as ""gentleman-like behavior""."	0
1060	8262	8262	S13-5	Subtask 5b: Semantic Compositionality in Context	11	145	3.0	3.0	The task may also be of interest to the related research fields of metaphor detection and idiom identification.	0
1061	8263	8263	S13-5	Subtask 5b: Semantic Compositionality in Context	12	146	3.0	3.0	There were no restrictions regarding the array of methods, and the kind of resources that could be employed for this task.	0
1062	8264	8264	S13-5	Subtask 5b: Semantic Compositionality in Context	13	147	4.0	3.0	In particular, participants were allowed to make use of pre-fabricated lists of phrases annotated with their probability of being used figuratively from publicly available sources, or to produce these lists from corpora.	0
1063	8265	8265	S13-5	Subtask 5b: Semantic Compositionality in Context	14	148	4.0	3.0	Assessing how well the phrase suits its context might be tackled using e.g. measures of semantic relatedness as well as distributional models learned from the underlying corpus.	0
1064	8266	8266	S13-5	Subtask 5b: Semantic Compositionality in Context	15	149	4.0	3.0	Participants of this subtask were provided with real usage examples of target phrases.	0
1065	8267	8267	S13-5	Subtask 5b: Semantic Compositionality in Context	16	150	4.0	3.0	For each usage example, the task is to make a binary decision whether the target phrase is used literally or figu-ratively in this context.	0
1066	8268	8268	S13-5	Subtask 5b: Semantic Compositionality in Context	17	151	4.0	3.0	Systems were tested in two different disciplines: a known phrases task where all target phrases in the test set were contained in the training, and an unknown phrases setting, where all target phrases in the test set were unseen.	0
1067	8269	8269	S13-5	Data Creation	1	152	1.0	4.0	The first step in creating the corpus was to compile a list of phrases that can be used either literally or metaphorically.	0
1068	8270	8270	S13-5	Data Creation	2	153	1.0	4.0	Thus, we created an initial list of several thousand English idioms from Wiktionary by listing all entries under the category ENGLISH ID-IOMS using the JWKTL Wiktionary API (Zesch et al., 2008).	0
1069	8271	8271	S13-5	Data Creation	3	154	1.0	4.0	We manually filtered the list removing most idioms that are very unlikely to be ever used literally (anymore), e.g. to knock on heaven's door.	0
1070	8272	8272	S13-5	Data Creation	4	155	2.0	4.0	For each of the resulting list of phrases, we extracted usage contexts from the ukWaC corpus (Baroni et al., 2009).	0
1071	8273	8273	S13-5	Data Creation	5	156	2.0	4.0	Each usage context contains 5 sentences, where the sentence with the target phrase appears in a randomized position.	0
1072	8274	8274	S13-5	Data Creation	6	157	2.0	4.0	Due to segmentation errors, some usage contexts actually might contain less than 5 sentences, but we manually filtered all usage contexts where the remaining context was insufficient.	0
1073	8275	8275	S13-5	Data Creation	7	158	3.0	4.0	This was done in the final cleaning step where we also manually removed (near) duplicates, obvious spam, encoding problems etc.	0
1074	8276	8276	S13-5	Data Creation	8	159	3.0	4.0	The target phrases in context were annotated for figurative, literal, both or impossible to tell usage, using the CrowdFlower 2 crowdsourcing annotation platform.	0
1075	8277	8277	S13-5	Data Creation	9	160	3.0	4.0	"We used about 8% of items as ""gold"" items for quality assurance, and had each example annotated by three crowdworkers."	0
1076	8278	8278	S13-5	Data Creation	10	161	4.0	4.0	The task was comparably easy for crowdworkers, who reached 90%-94% pairwise agreement, and 95% success on the gold items.	0
1077	8279	8279	S13-5	Data Creation	11	162	4.0	4.0	About 5% of items with low agreement and marked as impossible were removed.	0
1078	8280	8280	S13-5	Data Creation	12	163	4.0	4.0	Table 3 summarizes the quantitative characteristics of all datasets resulting from this process.	0
1079	8281	8281	S13-5	Data Creation	13	164	4.0	4.0	We took care in sampling the data as to keep similar distributions across the training, development and testing parts.	0
1080	8282	8282	S13-5	Results	1	165	1.0	4.0	Training and development datasets were made available in advance, test data was provided during the evaluation period without labels.	0
1081	8283	8283	S13-5	Results	2	166	1.0	4.0	System perfor-    mance was measured in accuracy.	0
1082	8284	8284	S13-5	Results	3	167	2.0	4.0	Since all participants provided classifications for all test items, the accuracy score is equivalent to precision/recall/F1.	0
1083	8285	8285	S13-5	Results	4	168	2.0	4.0	Participants were allowed to enter up to three different runs for evaluation.	0
1084	8286	8286	S13-5	Results	5	169	2.0	4.0	We also provide baseline accuracy scores, which are obtained by always assigning the most frequent class (figurative).	0
1085	8287	8287	S13-5	Results	6	170	3.0	4.0	Table 4 provides the evaluation results for the known phrases task, while Table 5 ranks participants for the unseen phrases task.	0
1086	8288	8288	S13-5	Results	7	171	3.0	4.0	As expected, the unseen phrases setting is much harder than the known phrases setting, as for unseen phrases it is not possible to learn lexicalised contextual clues.	0
1087	8289	8289	S13-5	Results	8	172	4.0	4.0	In both settings, the winning entries were able to beat the MFC baseline.	0
1088	8290	8290	S13-5	Results	9	173	4.0	4.0	While performance in the known phrases setting is close to 80% and thus acceptable, the general task of recognizing the literal or figurative use of unseen phrases remains very challenging, with only a small improvement over the baseline.	0
1089	8291	8291	S13-5	Results	10	174	4.0	4.0	We refer to the system descriptions for more details on the techniques used for this subtask: UNAL (Jimenez et al., 2013), IIRG (Byrne et al., 2013) and CLaC (Siblini and Kosseim, 2013).	0
1090	8292	8292	S13-5	Task Conclusions	1	175	1.0	4.0	"In this section, we further discuss the findings and conclusion of the evaluation challenge in the task of ""Phrasal Semantics""."	0
1091	8293	8293	S13-5	Task Conclusions	2	176	1.0	4.0	Looking at the results of both subtasks, one observes that the maximum performance achieved is higher for the first than the second subtask.	0
1092	8294	8294	S13-5	Task Conclusions	3	177	1.0	4.0	For this comparison to be fair, trivial baselines should be taken into account.	0
1093	8295	8295	S13-5	Task Conclusions	4	178	1.0	4.0	A system randomly assigning an output value would be on average 50% correct in the first subtask, since the numbers of positive and negative instances in the testing set are equal.	0
1094	8296	8296	S13-5	Task Conclusions	5	179	1.0	4.0	Similarly, a system assigning the most frequent class, i.e. the figurative use of any phrase, would be 50.3% and 61.6% accurate in the second subtask for seen and unseen test instances, respectively.	0
1095	8297	8297	S13-5	Task Conclusions	6	180	2.0	4.0	It should also be noted that the testing instances in the first subtask are unseen in the respective training set.	0
1096	8298	8298	S13-5	Task Conclusions	7	181	2.0	4.0	As a result, in terms of baselines, the second subtask on unseen data (Table 5) should be considered easier than the first subtask (Table 2).	0
1097	8299	8299	S13-5	Task Conclusions	8	182	2.0	4.0	However, the best performing systems achieved much higher accuracy in the first than in the second subtask.	0
1098	8300	8300	S13-5	Task Conclusions	9	183	2.0	4.0	This contradiction confirms our conception that the first subtask is less complex than the second.	0
1099	8301	8301	S13-5	Task Conclusions	10	184	2.0	4.0	In the first subtask, it is evident that no method performs much better or much worse than the others.	0
1100	8302	8302	S13-5	Task Conclusions	11	185	3.0	4.0	Although the participating systems have employed a wide variety of approaches and tools, the difference between the best and worst accuracy achieved is relatively limited, in particular approximately 14%.	0
1101	8303	8303	S13-5	Task Conclusions	12	186	3.0	4.0	Even more interestingly, unsupervised approaches performed better than some supervised ones.	0
1102	8304	8304	S13-5	Task Conclusions	13	187	3.0	4.0	"This observation suggests that no ""golden recipe"" has been identified so far for this task."	0
1103	8305	8305	S13-5	Task Conclusions	14	188	3.0	4.0	Thus, probably different processing tools take advantage of different sources of information.	0
1104	8306	8306	S13-5	Task Conclusions	15	189	3.0	4.0	It is a matter of future research to identify these sources and the corresponding tools, and then develop hybrid methods of improved performance.	0
1105	8307	8307	S13-5	Task Conclusions	16	190	4.0	4.0	In the second subtask, the results of evaluation on known phrases are much higher than on unseen phrases.	0
1106	8308	8308	S13-5	Task Conclusions	17	191	4.0	4.0	This was expected, as for unseen phrases it is not possible to learn lexicalised contextual clues.	0
1107	8309	8309	S13-5	Task Conclusions	18	192	4.0	4.0	Thus, the second subtask has succeeded in identifying the complexity threshold up to which the current state-of-the-art can address the computational problem.	0
1108	8310	8310	S13-5	Task Conclusions	19	193	4.0	4.0	Further than this threshold, i.e. for unseen phrases, current systems have not yet succeeded in addressing it.	0
1109	8311	8311	S13-5	Task Conclusions	20	194	4.0	4.0	In conclusion, the difficulty in evaluating the compositionality of previously unseen phrases in context highlights the overall complexity of the second subtask.	0
1110	8312	8312	S13-5	Summary and Future Work	1	195	1.0	4.0	"In this paper we have presented the 5 th task of Se-mEval 2013, ""Evaluating Phrasal Semantics"", which consists of two subtasks: (1) semantic similarity of words and compositional phrases, and (2) compositionality of phrases in context."	0
1111	8313	8313	S13-5	Summary and Future Work	2	196	1.0	4.0	The former subtask, which focussed on the first step of composing the meaning of phrases of any length, is less complex than the latter subtask, which considers the effect of context to the semantics of a phrase.	0
1112	8314	8314	S13-5	Summary and Future Work	3	197	2.0	4.0	The paper presents details about the background and importance of these subtasks, the data creation process, the systems that took part in the evaluation and their results.	0
1113	8315	8315	S13-5	Summary and Future Work	4	198	2.0	4.0	In the future, we expect evaluation challenges on phrasal semantics to progress towards two directions: (a) the synthesis of semantics of sequences longer than two words, and (b) aiming to improve the performance of systems that determine the compositionality of previously unseen phrases in con-text.	0
1114	8316	8316	S13-5	Summary and Future Work	5	199	3.0	4.0	The evaluation results of the first task suggest that state-of-the-art systems can compose the semantics of two word sequences with a promising level of success.	0
1115	8317	8317	S13-5	Summary and Future Work	6	200	3.0	4.0	However, this task should be seen as the first step towards composing the semantics of sentence-long sequences.	0
1116	8318	8318	S13-5	Summary and Future Work	7	201	4.0	4.0	As far as subtask 5b is concerned, the accuracy achieved by the participating systems on unseen testing data was low, only slightly better than the most frequent class baseline, which assigns the figurative use to all test phrases.	0
1117	8319	8319	S13-5	Summary and Future Work	8	202	4.0	4.0	Thus, the subtask cannot be considered well addressed by the state-of-the-art and further progress should be sought.	0
1118	9049	9049	S13-11	title	1	1	4.0	1.0	SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application	0
1119	9050	9050	S13-11	abstract	1	2	2.0	1.0	In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification.	0
1120	9051	9051	S13-11	abstract	2	3	3.0	1.0	Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.	1
1121	9052	9052	S13-11	abstract	3	4	4.0	1.0	The task enables the end-to-end evaluation and comparison of systems.	0
1122	9053	9053	S13-11	Introduction	1	5	1.0	1.0	Word ambiguity is a pervasive issue in Natural Language Processing.	0
1123	9054	9054	S13-11	Introduction	2	6	1.0	1.0	Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009;	0
1124	9055	9055	S13-11	Introduction	3	7	1.0	1.0	Navigli, 2012) for a survey).	0
1125	9056	9056	S13-11	Introduction	4	8	1.0	1.0	Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications.	0
1126	9057	9057	S13-11	Introduction	5	9	1.0	1.0	In fact, the performance of WSD systems depends heavily on which sense inventory is chosen.	0
1127	9058	9058	S13-11	Introduction	6	10	2.0	1.0	For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002;	0
1128	9059	9059	S13-11	Introduction	7	11	2.0	1.0	Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008).	0
1129	9060	9060	S13-11	Introduction	8	12	2.0	1.0	On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses.	0
1130	9061	9061	S13-11	Introduction	9	13	2.0	1.0	In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output.	0
1131	9062	9062	S13-11	Introduction	10	14	2.0	1.0	Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.	0
1132	9063	9063	S13-11	Introduction	11	15	3.0	1.0	Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively.	0
1133	9064	9064	S13-11	Introduction	12	16	3.0	1.0	A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds.	0
1134	9065	9065	S13-11	Introduction	13	17	3.0	1.0	Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010;	0
1135	9066	9066	S13-11	Introduction	14	18	3.0	1.0	Di Marco and Navigli, 2013).	0
1136	9067	9067	S13-11	Introduction	15	19	3.0	1.0	In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.	0
1137	9068	9068	S13-11	Introduction	16	20	4.0	1.0	The Semeval-2013 task described in this paper 1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems.	0
1138	9069	9069	S13-11	Introduction	17	21	4.0	1.0	The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre and Soroa, 2007;Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms.	0
1139	9070	9070	S13-11	Introduction	18	22	4.0	1.0	Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning).	0
1140	9071	9071	S13-11	Introduction	19	23	4.0	1.0	For instance, consider the target query apple and the following 3 search result snippets:	0
1141	9072	9072	S13-11	Introduction	20	24	4.0	1.0	1. Apple Inc., formerly Apple Computer, Inc., is...	0
1142	9073	9073	S13-11	2	1	25	4.0	1.0	The science of apple growing is called pomology...	0
1143	9074	9074	S13-11	3	1	26	2.0	1.0	Apple designs and creates iPod and iTunes...	0
1144	9075	9075	S13-11	3	2	27	4.0	1.0	Participating systems were requested to produce a clustering that groups snippets conveying the same meaning of the input query apple, i.e., ideally {1, 3} and {2} in the above example.	0
1145	9076	9076	S13-11	Task setup	1	28	2.0	1.0	For each ambiguous query the task required participating systems to cluster the top ranking snippets returned by a search engine (we used the Google Search API).	0
1146	9077	9077	S13-11	Task setup	2	29	3.0	1.0	WSI systems were required to identify the meanings of the input query and cluster the snippets into semantically-related groups according to their meanings.	0
1147	9078	9078	S13-11	Task setup	3	30	4.0	1.0	Instead, WSD systems were requested to sense-tag the given snippets with the appropriate senses of the input query, thereby implicitly determining a clustering of snippets (i.e., one cluster per sense).	0
1148	9079	9079	S13-11	Dataset	1	31	1.0	1.0	We created a dataset of 100 ambiguous queries.	0
1149	9080	9080	S13-11	Dataset	2	32	1.0	1.0	The queries were randomly sampled from the AOL search logs so as to ensure that they had been used in real search sessions.	0
1150	9081	9081	S13-11	Dataset	3	33	1.0	2.0	Following previous work on the topic Di Marco and Navigli, 2013) we selected those queries for which a sense inventory exists as a disambiguation page in the English Wikipedia 2 .	0
1151	9082	9082	S13-11	Dataset	4	34	2.0	2.0	This guaranteed that the selected queries consisted of either a single word or a multiword expression for which we had a collaborativelyedited list of meanings, including lexicographic and encyclopedic ones.	0
1152	9083	9083	S13-11	Dataset	5	35	2.0	2.0	We discarded all queries made up of &gt; 4 words, since the length of the great majority of queries lay in the range [1,4].	0
1153	9084	9084	S13-11	Dataset	6	36	2.0	2.0	In Table 1 we compare the percentage distribution of 1-to 4-word queries in the AOL query logs against our dataset of queries.	0
1154	9085	9085	S13-11	Dataset	7	37	3.0	2.0	Note that we increased the percentage of 3-and 4-word queries in order to have a significant coverage of those lengths.	0
1155	9086	9086	S13-11	Dataset	8	38	3.0	2.0	Anyhow, in both cases most queries contained from 1 to 2 words.	0
1156	9087	9087	S13-11	Dataset	9	39	3.0	2.0	Note that the reported percentage distributions of query length is different from recent statistics for two reasons: first, over the years users have increased the average number of words per query in order to refine their searches; second, we selected only queries which were either single words (e.g., apple) or multi-word expressions (e.g., mortal kombat), thereby discarding several long queries composed of different words (such as angelina jolie actress).	0
1157	9088	9088	S13-11	Dataset	10	40	4.0	2.0	Finally, we submitted each query to Google search and retrieved the 64 top-ranking results returned for each query.	0
1158	9089	9089	S13-11	Dataset	11	41	4.0	2.0	Therefore, overall the dataset consists of 100 queries and 6,400 results.	0
1159	9090	9090	S13-11	Dataset	12	42	4.0	2.0	Each search result includes the following information: page title, URL of the page and snippet of the page text.	0
1160	9091	9091	S13-11	Dataset	13	43	4.0	2.0	We show an example of search result for the apple query in Figure 1.	0
1161	9092	9092	S13-11	Dataset Annotation	1	44	1.0	2.0	For each query q we used Amazon Mechanical Turk 3 to annotate each query result with the most suitable sense.	0
1162	9093	9093	S13-11	Dataset Annotation	2	45	1.0	2.0	The sense inventory for q was obtained by listing the senses available in the Wikipedia disambiguation page of q augmented with additional options from the classes obtained from the section headings of the disambiguation page plus the OTHER catch-all meaning.	0
1163	9094	9094	S13-11	Dataset Annotation	3	46	2.0	2.0	For instance, consider the apple query.	0
1164	9095	9095	S13-11	Dataset Annotation	4	47	2.0	2.0	We show its disambiguation page in Figure 2.	0
1165	9096	9096	S13-11	Dataset Annotation	5	48	2.0	2.0	The sense inventory for apple was made up of the senses listed in that page (e.g., MALUS, APPLE INC., APPLE BANK, etc.) plus the set of generic classes OTHER PLANTS AND PLANT PARTS, OTHER COMPANIES, OTHER FILMS, plus OTHER.	0
1166	9097	9097	S13-11	Dataset Annotation	6	49	3.0	2.0	For each query we ensured that three annotators tagged each of the 64 results for that query with the most suitable sense among those in the sense inventory (selecting OTHER if no sense was appropriate).	0
1167	9098	9098	S13-11	Dataset Annotation	7	50	3.0	2.0	"Specifically, each Turker was provided with the following instructions: ""The goal is annotating the search result snippets returned by Google for a given query with the appropriate meaning among those available (obtained from the Wikipedia disambiguation page for the query)."	0
1168	9099	9099	S13-11	Dataset Annotation	8	51	3.0	2.0	"You have to select the meaning that you consider most appropriate""."	0
1169	9100	9100	S13-11	Dataset Annotation	9	52	4.0	2.0	No constraint on the age, gender and citizenship of the annotators was imposed.	0
1170	9101	9101	S13-11	Dataset Annotation	10	53	4.0	2.0	However, in order to avoid random tagging of search results, we provided 3 gold-standard result annotations per query, which could be shown to the Turker more than once during the annotation process.	0
1171	9102	9102	S13-11	Dataset Annotation	11	54	4.0	2.0	In the case (s)he failed to annotate the gold items, the annotator was automatically excluded.	0
1172	9103	9103	S13-11	Inter-Annotator Agreement and Adjudication	1	55	1.0	2.0	In order to determine the reliability of the Turkers' annotations, we calculated the individual values of Fleiss' kappa κ (Fleiss, 1971) for each query q and then averaged them:	0
1173	9104	9104	S13-11	Inter-Annotator Agreement and Adjudication	2	56	2.0	2.0	where κ q is the Fleiss' kappa agreement of the three annotators who tagged the 64 snippets returned by the Google search engine for the query q ∈ Q, and Q is our set of 100 queries.	0
1174	9105	9105	S13-11	Inter-Annotator Agreement and Adjudication	3	57	2.0	2.0	We obtained an average value of κ = 0.66, which according to Landis and Koch (1977) can be seen as substantial agreement, with a standard deviation σ = 0.185.	0
1175	9106	9106	S13-11	Inter-Annotator Agreement and Adjudication	4	58	3.0	2.0	In Table 2 we show the agreement distribution of our 6400 snippets, distinguishing between full agreement (3 out of 3), majority agreement (2 out of 3), and no agreement.	0
1176	9107	9107	S13-11	Inter-Annotator Agreement and Adjudication	5	59	3.0	2.0	Most of the items were annotated with full or majority agreement, indicating that the manual annotation task was generally doable for the layman.	0
1177	9108	9108	S13-11	Inter-Annotator Agreement and Adjudication	6	60	4.0	2.0	We manually checked all the cases of majority agreement, correcting only 7.92% of the majority adjudications, and manually adjudicated all the snippets for which there was no agreement.	0
1178	9109	9109	S13-11	Inter-Annotator Agreement and Adjudication	7	61	4.0	2.0	We observed during adjudication that in many cases the disagreement was due to the existence of subtle sense distinctions, like between MORTAL KOM-BAT (VIDEO GAME) and MORTAL KOMBAT (2011 VIDEO GAME)  per query on average).	0
1179	9110	9110	S13-11	Scoring	1	62	1.0	2.0	Following Di Marco and Navigli (2013), we evaluated the systems' outputs in terms of the snippet clustering quality (Section 3.1) and the snippet diversification quality (Section 3.2).	0
1180	9111	9111	S13-11	Scoring	2	63	2.0	2.0	Given a query q ∈ Q and the corresponding set of 64 snippet results, let C be the clustering output by a given system and let G be the gold-standard clustering for those results.	0
1181	9112	9112	S13-11	Scoring	3	64	3.0	2.0	Each measure M (C, G) presented below is calculated for the query q using these two clusterings.	0
1182	9113	9113	S13-11	Scoring	4	65	4.0	2.0	The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of M (C, G) obtained for each single test query q ∈ Q.	0
1183	9114	9114	S13-11	Clustering Quality	1	66	1.0	3.0	The first evaluation concerned the quality of the clusters produced by the participating systems.	0
1184	9115	9115	S13-11	Clustering Quality	2	67	1.0	3.0	Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely:	0
1185	9116	9116	S13-11	Clustering Quality	3	68	1.0	3.0	• Rand Index (Rand, 1971);	0
1186	9117	9117	S13-11	Clustering Quality	4	69	1.0	3.0	• Adjusted Rand Index (Hubert and Arabie, 1985);	0
1187	9118	9118	S13-11	Clustering Quality	5	70	1.0	3.0	• Jaccard Index (Jaccard, 1901);	0
1188	9119	9119	S13-11	Clustering Quality	6	71	1.0	3.0	• F1 measure (van Rijsbergen, 1979).	0
1189	9120	9120	S13-11	Clustering Quality	7	72	2.0	3.0	The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows:	0
1190	9121	9121	S13-11	Clustering Quality	8	73	2.0	3.0	where TP is the number of true positives, i.e., snippet pairs which are in the same cluster both in C and G, TN is the number of true negatives, i.e., pairs which are in different clusters in both clusterings, and FP and FN are, respectively, the number of false positives and false negatives.	0
1191	9122	9122	S13-11	Clustering Quality	9	74	2.0	3.0	RI ranges between 0 and 1, where 1 indicates perfect correspondence.	0
1192	9123	9123	S13-11	Clustering Quality	10	75	2.0	3.0	Adjusted Rand Index (ARI) is a development of Rand Index which corrects the RI for chance agreement and makes it vary according to expectaction:	0
1193	9124	9124	S13-11	Clustering Quality	11	76	2.0	3.0	.	0
1194	9125	9125	S13-11	Clustering Quality	12	77	2.0	3.0	(3) where E(RI(C, G)) is the expected value of the RI.	0
1195	9126	9126	S13-11	Clustering Quality	13	78	2.0	3.0	Using the contingency table reported in Table 3 we can quantify the degree of overlap between C and G, where n ij denotes the number of snippets in common between G i and C j (namely, n ij = |G i ∩ C j |), a i and b j represent, respectively, the number of snippets in G i and C j , and N is the total number of snippets, i.e., N = 64.	0
1196	9127	9127	S13-11	Clustering Quality	14	79	3.0	3.0	Now, the above equation can be reformulated as:	0
1197	9128	9128	S13-11	Clustering Quality	15	80	3.0	3.0	.	0
1198	9129	9129	S13-11	Clustering Quality	16	81	3.0	3.0	(4)	0
1199	9130	9130	S13-11	Clustering Quality	17	82	3.0	3.0	The ARI ranges between −1 and +1 and is 0 when the index equals its expected value.	0
1200	9131	9131	S13-11	Clustering Quality	18	83	3.0	3.0	Jaccard Index (JI) is a measure which takes into account only the snippet pairs which are in the same cluster both in C and G, i.e., the true positives (TP), while neglecting true negatives (TN), which are the vast majority of cases.	0
1201	9132	9132	S13-11	Clustering Quality	19	84	3.0	3.0	JI is calculated as follows:	0
1202	9133	9133	S13-11	Clustering Quality	20	85	3.0	3.0	Finally, the F1 measure calculates the harmonic mean of precision (P) and recall (R).	0
1203	9134	9134	S13-11	Clustering Quality	21	86	4.0	3.0	Precision determines how accurately the clusters of C represent the query meanings in the gold standard G, whereas recall measures how accurately the different meanings in G are covered by the clusters in C. We follow Crabtree et al. (2005) and define the precision of a cluster C j ∈ C as follows:	0
1204	9135	9135	S13-11	Clustering Quality	22	87	4.0	3.0	where C s j is the intersection between C j ∈ C and the gold cluster G s ∈ G which maximizes the cardinality of the intersection.	0
1205	9136	9136	S13-11	Clustering Quality	23	88	4.0	3.0	The recall of a query sense s is instead calculated as:	0
1206	9137	9137	S13-11	Clustering Quality	24	89	4.0	3.0	where C s is the subset of clusters of C whose majority sense is s, and n s is the number of snippets tagged with query sense s in the gold standard.	0
1207	9138	9138	S13-11	Clustering Quality	25	90	4.0	3.0	The total precision and recall of the clustering C are then calculated as:	0
1208	9139	9139	S13-11	Clustering Quality	26	91	4.0	3.0	where S is the set of senses in the gold standard G for the given query (i.e., |S| = |G|).	0
1209	9140	9140	S13-11	Clustering Quality	27	92	4.0	3.0	The two values of P and R are then combined into their harmonic mean, namely the F1 measure:	0
1210	9141	9141	S13-11	Clustering Diversity	1	93	1.0	3.0	Our second evaluation is aimed at determining the impact of the output clustering on the diversification of the top results shown to a Web user.	0
1211	9142	9142	S13-11	Clustering Diversity	2	94	1.0	3.0	To this end, we applied an automatic procedure for flattening the clusterings produced by the participating systems to a list of search results.	0
1212	9143	9143	S13-11	Clustering Diversity	3	95	2.0	3.0	Given a clustering C = (C 1 , C 2 , . . . , C m ), we add to the initially empty list the first element of each cluster C j (j = 1, . . . , m); then we iterate the process by selecting the second element of each cluster C j such that |C j | ≥ 2, and so on.	0
1213	9144	9144	S13-11	Clustering Diversity	4	96	2.0	3.0	The remaining elements returned by the search engine, but not included in any cluster of C, are appended to the bottom of the list in their original order.	0
1214	9145	9145	S13-11	Clustering Diversity	5	97	2.0	3.0	Note that systems were asked to sort snippets within clusters, as well as clusters themselves, by relevance.	0
1215	9146	9146	S13-11	Clustering Diversity	6	98	3.0	3.0	Since our goal is to determine how many different meanings are covered by the top-ranking search results according to the output clustering, we used the measures of S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at recall r) (Zhai et al., 2003).	0
1216	9147	9147	S13-11	Clustering Diversity	7	99	3.0	4.0	S-recall@	0
1217	9148	9148	S13-11	Clustering Diversity	8	100	3.0	4.0	K determines the ratio of different meanings for a given query q in the top-K results returned:	0
1218	9149	9149	S13-11	Clustering Diversity	9	101	4.0	4.0	where sense(r i ) is the gold-standard sense associated with the i-th snippet returned by the system, and g is the total number of distinct senses for the query q in our gold standard.	0
1219	9150	9150	S13-11	Clustering Diversity	10	102	4.0	4.0	S-precision@r instead determines the ratio of different senses retrieved for query q in the first K r snippets, where K r is the minimum number of top results for which the system achieves recall r.	0
1220	9151	9151	S13-11	Clustering Diversity	11	103	4.0	4.0	The measure is defined as follows:	0
1221	9152	9152	S13-11	Baselines	1	104	1.0	4.0	We compared the participating systems with two simple baselines:	0
1222	9153	9153	S13-11	Baselines	2	105	1.0	4.0	• SINGLETONS: each snippet is clustered as a separate singleton cluster (i.e., |C| = 64).	0
1223	9154	9154	S13-11	Baselines	3	106	1.0	4.0	• ALL-IN-ONE: all snippets are clustered into a single cluster (i.e., |C| = 1).	0
1224	9155	9155	S13-11	Baselines	4	107	2.0	4.0	These baselines are important in that they make explicit the preference of certain quality measures towards clusterings made up with a small or large number of clusters.	0
1225	9156	9156	S13-11	Baselines	5	108	2.0	4.0	4 Systems 5 teams submitted 10 systems, out of which 9 were WSI systems, while 1 was a WSD system, i.e., using the Wikipedia sense inventory for performing the disambiguation task.	0
1226	9157	9157	S13-11	Baselines	6	109	2.0	4.0	All systems could exploit the information provided for each search result, i.e., URL, page title and result snippet.	0
1227	9158	9158	S13-11	Baselines	7	110	2.0	4.0	WSI systems were requested to use unannotated corpora only.	0
1228	9159	9159	S13-11	Baselines	8	111	3.0	4.0	We asked each team to provide information about their systems.	0
1229	9160	9160	S13-11	Baselines	9	112	3.0	4.0	In Table 4 we report the resources used by each system.	0
1230	9161	9161	S13-11	Baselines	10	113	3.0	4.0	The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-PK2 both use the snippets for inducing the query senses.	0
1231	9162	9162	S13-11	Baselines	11	114	4.0	4.0	Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes.	0
1232	9163	9163	S13-11	Baselines	12	115	4.0	4.0	They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection 6 (Biemann et al., 2007).	0
1233	9164	9164	S13-11	Baselines	13	116	4.0	4.0	SATTY-APPROACH1 just uses snippets.	0
1234	9165	9165	S13-11	Baselines	14	117	4.0	4.0	The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009).	0
1235	9166	9166	S13-11	Results	1	118	1.0	4.0	We show the results of RI and ARI in Table 5.	0
1236	9167	9167	S13-11	Results	2	119	1.0	4.0	The best performing systems are those from the HDP team, with considerably higher RI and ARI.	0
1237	9168	9168	S13-11	Results	3	120	1.0	4.0	The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH.	0
1238	9169	9169	S13-11	Results	4	121	2.0	4.0	SINGLETONS perform well with RI, but badly when chance agreement is taken into account.	0
1239	9170	9170	S13-11	Results	5	122	2.0	4.0	As for F1 and JI, whose values are shown in Table 6, the two HDP systems again perform best in terms of F1, and are on par with UKP-WSI-WACKY-LLR in terms of JI.	0
1240	9171	9171	S13-11	Results	6	123	2.0	4.0	The third best approach in terms of F1 is again SATTY-APPROACH1, which however per-	0
1241	9172	9172	S13-11	Results	7	124	2.0	4.0	To get more insights into the performance of the various systems, we calculated the average number of clusters per clustering produced by each system and compared it with the gold standard average.	0
1242	9173	9173	S13-11	Results	8	125	3.0	4.0	We also computed the average cluster size, i.e., the average number of snippets per cluster.	0
1243	9174	9174	S13-11	Results	9	126	3.0	4.0	The statistics are shown in Table 7.	0
1244	9175	9175	S13-11	Results	10	127	3.0	4.0	Interestingly, the best performing systems are those with the cluster number and average number of clusters closest to the gold standard ones.	0
1245	9176	9176	S13-11	Results	11	128	4.0	4.0	This finding is also confirmed by Figure 3, where we draw each system according to its average values regarding cluster number and size: again the distance from the gold standard is meaningful.	0
1246	9177	9177	S13-11	Results	12	129	4.0	4.0	We now move to the diversification perfor-    Our annotation experience showed that the Wikipedia sense inventory, augmented with our generic classes, is a good choice for semantically tagging search results, in that it covers most of the meanings a Web user might be interested in.	0
1247	9178	9178	S13-11	Results	13	130	4.0	4.0	In fact, only 20% of the snippets was annotated with the OTHER class.	0
1248	9179	9179	S13-11	Results	14	131	4.0	4.0	Future work might consider large-scale multilingual lexical resources, such as BabelNet (Navigli and Ponzetto, 2012), both as sense inventory and for performing the search result clustering and diversification task.	0
1249	9369	9369	S13-13	title	1	1	4.0	1.0	SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses	0
1250	9370	9370	S13-13	abstract	1	2	1.0	1.0	Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense.	0
1251	9371	9371	S13-13	abstract	2	3	2.0	1.0	However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage.	0
1252	9372	9372	S13-13	abstract	3	4	3.0	1.0	We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability.	0
1253	9373	9373	S13-13	abstract	4	5	4.0	1.0	Four teams submitted nine systems, which were evaluated in two settings.	0
1254	9374	9374	S13-13	Introduction	1	6	1.0	1.0	Word Sense Disambiguation (WSD) attempts to identify which of a word's meanings applies in a given context.	0
1255	9375	9375	S13-13	Introduction	2	7	1.0	1.0	A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009).	0
1256	9376	9376	S13-13	Introduction	3	8	1.0	1.0	Typically, each usage of a word is treated as expressing only a single sense.	0
1257	9377	9377	S13-13	Introduction	4	9	1.0	1.0	However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations.	0
1258	9378	9378	S13-13	Introduction	5	10	2.0	1.0	Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (Véronis, 1998;Murray and Green, 2004;	0
1259	9379	9379	S13-13	Introduction	6	11	2.0	1.0	Passonneau et al., 2012b;Jurgens, 2013;Navigli et al., 2013).	0
1260	9380	9380	S13-13	Introduction	7	12	2.0	1.0	Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled.	0
1261	9381	9381	S13-13	Introduction	8	13	2.0	1.0	Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability.	0
1262	9382	9382	S13-13	Introduction	9	14	3.0	1.0	WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD systems for specific types of text (e.g., social media), or the inventory may lack domain-specific senses.	0
1263	9383	9383	S13-13	Introduction	10	15	3.0	1.0	Word Sense Induction (WSI) has been proposed as a method for overcoming such limitations by learning the senses automatically from text.	0
1264	9384	9384	S13-13	Introduction	11	16	3.0	1.0	In essence, a WSI algorithm acts as a lexicographer by grouping word usages according to their shared meaning.	0
1265	9385	9385	S13-13	Introduction	12	17	3.0	1.0	The second goal of this task is to assess the performance of WSI algorithms when they are able to model multiple meanings of a usage with graded senses.	0
1266	9386	9386	S13-13	Introduction	13	18	4.0	1.0	Task 12 focuses on disambiguating senses for 50 target lemmas: 20 nouns, 20 verbs, and 10 adjectives (Sec. 2).	0
1267	9387	9387	S13-13	Introduction	14	19	4.0	1.0	Since the Task evaluates only unsupervised systems, no training data was provided; however, to enable more comparison, Unsupervised WSD systems were also allowed to participate.	0
1268	9388	9388	S13-13	Introduction	15	20	4.0	1.0	Participating systems were evaluated in two settings (Sec. 3), depending on whether they used induced senses or WordNet 3.1 senses for their annotations.	0
1269	9389	9389	S13-13	Introduction	16	21	4.0	1.0	The results (Sec. 5) demonstrate a substantial improvement over the competitive most frequent sense baseline.	0
1270	9390	9390	S13-13	Task Description	1	22	1.0	1.0	This task required participating systems to annotate instances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected due to its fine-grained senses.	1
1271	9391	9391	S13-13	Task Description	2	23	1.0	1.0	Participants could label each instance with one or more senses, weighting	1
1272	9392	9392	S13-13	Task Description	3	24	2.0	1.0	We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the dark centers of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition to encompass the thoughts of every entity we know.	0
1273	9393	9393	S13-13	Task Description	4	25	2.0	1.0	dark%3:00:01:: -devoid of or deficient in light or brightness; shadowed or black dark%3:00:00:: -secret I ask because my practice has always been to allow about five minutes grace, then remove it.	0
1274	9394	9394	S13-13	Task Description	5	26	2.0	1.0	ask%2:32:02:: -direct or put; seek an answer to ask%2:32:04:: -address a question to and expect an answer from Table 1: Example instances with multiple senses due to intended double meanings (top) or contextual ambiguity (bottom).	0
1275	9395	9395	S13-13	Task Description	6	27	3.0	1.0	Senses are specified using their WordNet 3.1 sense keys.	0
1276	9396	9396	S13-13	Task Description	7	28	3.0	1.0	each by their applicability.	0
1277	9397	9397	S13-13	Task Description	8	29	3.0	1.0	Table 1 highlights two example contexts where multiple senses apply.	0
1278	9398	9398	S13-13	Task Description	9	30	4.0	1.0	The first example shows a case of an intentional double meaning that evokes both the physical aspect of dark.	0
1279	9399	9399	S13-13	Task Description	10	31	4.0	1.0	a as being devoid of light and the causal result of being secret.	0
1280	9400	9400	S13-13	Task Description	11	32	4.0	1.0	"In contrast, the second example shows a case of multiple interpretations from ambiguity; a different preceding context could generate the alternate interpretations ""I ask [you] because"" (sense ask%2:32:04::) or ""I ask [the question] because"" (sense ask%2:32:02::)."	0
1281	9401	9401	S13-13	Data	1	33	1.0	1.0	Three datasets were provided with the task.	0
1282	9402	9402	S13-13	Data	2	34	1.0	1.0	The trial dataset provided weighted word sense annotations using the data gathered by .	0
1283	9403	9403	S13-13	Data	3	35	2.0	1.0	The trial dataset consisted of 50 contexts for eight words, where each context was labeled with WordNet 3.0 sense ratings from three untrained lexicographers.	0
1284	9404	9404	S13-13	Data	4	36	2.0	1.0	Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data.	0
1285	9405	9405	S13-13	Data	5	37	2.0	1.0	However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses.	0
1286	9406	9406	S13-13	Data	6	38	3.0	1.0	Previous SemEval WSI tasks had provided participants with corpora specific to the task's target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statistical associations.	0
1287	9407	9407	S13-13	Data	7	39	3.0	1.0	Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004, OANC) across a variety of genres and from both the spoken and written portions of the corpus, summarized in Table 2.	0
1288	9408	9408	S13-13	Data	8	40	4.0	1.0	All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that matched at least one WordNet 3.1 sense.	0
1289	9409	9409	S13-13	Data	9	41	4.0	1.0	This filtering also removed instances that were in a collocation, or had an idiomatic meaning.	0
1290	9410	9410	S13-13	Data	10	42	4.0	1.0	Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word.	0
1291	9411	9411	S13-13	Sense Annotation	1	43	1.0	1.0	Recent work proposes to gather sense annotations using crowdsourcing in order to reduce the time and cost of acquiring sense-annotated corpora (Biemann and Nygaard, 2010;	0
1292	9412	9412	S13-13	Sense Annotation	2	44	1.0	1.0	Passonneau et al., 2012b;Rumshisky et al., 2012;Jurgens, 2013).	0
1293	9413	9413	S13-13	Sense Annotation	3	45	1.0	1.0	Therefore, we initially annotated the Task's data using the method of Jurgens (2013), where workers on Amazon Mechanical Turk (AMT) rated all senses of a word on a Likert scale from one to five, indicating the sense does not apply at all or completely applies, respectively.	0
1294	9414	9414	S13-13	Sense Annotation	4	46	2.0	1.0	Twenty annotators were assigned per instance, with their ratings combined by selecting the most frequent rating.	0
1295	9415	9415	S13-13	Sense Annotation	5	47	2.0	1.0	However, we found that while the annotators achieved moderate inter-annotator agreement (IAA), the resulting annotations were not of high enough quality to use in the Task's evaluations.	0
1296	9416	9416	S13-13	Sense Annotation	6	48	2.0	2.0	Specifically, for some senses and contexts, AMT annotators required more information about sense distinctions than was feasible to integrate into the AMT setting, which led to consistent but incorrect sense assignments.	0
1297	9417	9417	S13-13	Sense Annotation	7	49	3.0	2.0	Therefore, the test data was annotated by the two authors, with the first author annotating all instances and the second author annotating a 10% sample of each lemma's instances in order to calculate IAA.	0
1298	9418	9418	S13-13	Sense Annotation	8	50	3.0	2.0	IAA was calculated using Krippendorff's α (Krippendorff, 1980;	0
1299	9419	9419	S13-13	Sense Annotation	9	51	3.0	2.0	Artstein and Poesio, 2008), which is an agreement measurement that adjusts for chance,   (Passonneau et al., 2006).	0
1300	9420	9420	S13-13	Sense Annotation	10	52	4.0	2.0	Table 2 summarizes the annotation statistics for the Task's data.	0
1301	9421	9421	S13-13	Sense Annotation	11	53	4.0	2.0	The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators.	0
1302	9422	9422	S13-13	Sense Annotation	12	54	4.0	2.0	An analysis across the corpora genres showed that the multiplesense annotation rates were similar.	0
1303	9423	9423	S13-13	Sense Annotation	13	55	4.0	2.0	Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses.	0
1304	9424	9424	S13-13	Evaluation	1	56	1.0	2.0	We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007;Manandhar et al., 2010).	0
1305	9425	9425	S13-13	Evaluation	2	57	2.0	2.0	The first evaluation uses a traditional WSD task that directly compares WordNet sense labels.	0
1306	9426	9426	S13-13	Evaluation	3	58	3.0	2.0	For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure.	0
1307	9427	9427	S13-13	Evaluation	4	59	4.0	2.0	The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons.	0
1308	9428	9428	S13-13	WSD Task	1	60	1.0	2.0	In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability ratings with human annotators.	0
1309	9429	9429	S13-13	WSD Task	2	61	2.0	2.0	Each objectives uses a specific measurement: (1) the Jaccard Index, (2) positionally-weighted Kendall's τ similarity, and (3) a weighted variant of Normalized Discounted Cumulative Gain, respectively.	0
1310	9430	9430	S13-13	WSD Task	3	62	3.0	2.0	Each measure is bounded in [0, 1], where 1 indicates complete agreement with the gold standard.	0
1311	9431	9431	S13-13	WSD Task	4	63	4.0	2.0	We generalize the traditional definition of WSD Recall such that it measures the average score for each measure across all instances, including those not labeled by the system.	0
1312	9432	9432	S13-13	WSD Task	5	64	4.0	2.0	Systems are ultimately scored using the F1 measure between each objective's measure and Recall.	0
1313	9433	9433	S13-13	Transforming Induced Sense Labels	1	65	1.0	2.0	In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., Word-Net 3.1) using a sense mapping procedure.	0
1314	9434	9434	S13-13	Transforming Induced Sense Labels	2	66	2.0	2.0	We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard.	0
1315	9435	9435	S13-13	Transforming Induced Sense Labels	3	67	3.0	2.0	This process is repeated so that each partition is tested once.	0
1316	9436	9436	S13-13	Transforming Induced Sense Labels	4	68	4.0	2.0	For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings.	0
1317	9437	9437	S13-13	Jaccard Index	1	69	2.0	2.0	Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the agreement:	0
1318	9438	9438	S13-13	Jaccard Index	2	70	4.0	2.0	The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint.	0
1319	9439	9439	S13-13	Positionally-Weighted Kendall's τ	1	71	1.0	2.0	Rank correlations have been proposed for evaluating a system's ability to order senses by applicability; in previous work, both  and Jurgens (2012) propose rank correlation coefficients that assume all positions in the ranking are equally important.	0
1320	9440	9440	S13-13	Positionally-Weighted Kendall's τ	2	72	1.0	2.0	However, in the case of graded sense evaluation, often only a few senses are applicable, with the applicability ratings of the remaining senses being relatively inconsequential.	0
1321	9441	9441	S13-13	Positionally-Weighted Kendall's τ	3	73	1.0	2.0	Therefore, we consider an alternate rank scoring based on Kumar and Vassilvitskii (2010), which weights the penalty of reordering the lower positions less than the penalty of reordering the first ranks.	0
1322	9442	9442	S13-13	Positionally-Weighted Kendall's τ	4	74	2.0	2.0	Kendall's τ distance, K, is a measure of the number of item position swaps required to make two sequences identical.	0
1323	9443	9443	S13-13	Positionally-Weighted Kendall's τ	5	75	2.0	2.0	Kumar and Vassilvitskii (2010) extend this distance definition using a variable penalty function δ for the cost of swapping two positions, which we denote K δ .	0
1324	9444	9444	S13-13	Positionally-Weighted Kendall's τ	6	76	2.0	2.0	By using an appropriate δ, K δ can be biased towards the correctness of higher ranks by assigning a smaller δ to lower ranks.	0
1325	9445	9445	S13-13	Positionally-Weighted Kendall's τ	7	77	2.0	2.0	Because K δ is a distance measure, its value range will be different depending on the number of ranks used.	0
1326	9446	9446	S13-13	Positionally-Weighted Kendall's τ	8	78	3.0	2.0	Therefore, to convert the measure to a similarity we normalize the distance to [0, 1] by dividing by the maximum K δ distance and then subtracting the distance from one.	0
1327	9447	9447	S13-13	Positionally-Weighted Kendall's τ	9	79	3.0	2.0	Given two rankings x and y where x is the reference by which y is to be measured, we may compute the normalized similarity using	0
1328	9448	9448	S13-13	Positionally-Weighted Kendall's τ	10	80	3.0	2.0	.	0
1329	9449	9449	S13-13	Positionally-Weighted Kendall's τ	11	81	3.0	2.0	(1) Equation 1 has its maximal value of one when ranking y is identical to ranking x, and its minimal value of zero when y is in the reverse order as x.	0
1330	9450	9450	S13-13	Positionally-Weighted Kendall's τ	12	82	4.0	2.0	We refer to this value as the positionally-weighted Kendall's τ similarity, K sim δ .	0
1331	9451	9451	S13-13	Positionally-Weighted Kendall's τ	13	83	4.0	2.0	As defined, K sim δ does not account for ties.	0
1332	9452	9452	S13-13	Positionally-Weighted Kendall's τ	14	84	4.0	2.0	Therefore, we arbitrarily break ties in a deterministic fashion for both rankings.	0
1333	9453	9453	S13-13	Positionally-Weighted Kendall's τ	15	85	4.0	2.0	Second, we define δ to assign higher cost to the first ranks: the cost to move an item into position i, δ i , is defined as n−(i+1) n , where n is the number of senses.	0
1334	9454	9454	S13-13	Weighted NDCG	1	86	1.0	2.0	To compare the applicability ratings for sense annotations, we recast the annotation process in an Information Retrieval setting: Given an example context acting as a query over a word's senses, the task is to retrieve all applicable senses, ranking and scoring them by their applicability.	0
1335	9455	9455	S13-13	Weighted NDCG	2	87	1.0	2.0	Moffat and Zobel (2008) propose using Discounted Cumulative Gain (DCG) as a method to compare a ranking against a baseline.	0
1336	9456	9456	S13-13	Weighted NDCG	3	88	2.0	2.0	Given (1) a gold standard weighting of the k senses applicable to a context, where w i denotes the applicability for sense i in the gold standard, and (2) a ranking of the k senses by some method, the DCG may be calculated as i+1) . DCG is commonly normalized to [0, 1] so that the value is comparable when computed on rankings with different k and weight values.	0
1337	9457	9457	S13-13	Weighted NDCG	4	89	2.0	2.0	To normalize, the maximum value is calculated by first computing the DCG on the ranking when the k items are sorted by their weights, referred as the Ideal DCG (IDCG), and then normalizing as N DCG = DCG IDCG .	0
1338	9458	9458	S13-13	Weighted NDCG	5	90	2.0	2.0	The DCG only considers the weights assigned in the gold standard, which potentially masks importance differences in the weights assigned to the senses.	0
1339	9459	9459	S13-13	Weighted NDCG	6	91	3.0	2.0	Therefore, we propose weighting the DCG by the relative difference in the two weights.	0
1340	9460	9460	S13-13	Weighted NDCG	7	92	3.0	2.0	Given an alternate weighting of the k items, denoted asŵ i ,	0
1341	9461	9461	S13-13	Weighted NDCG	8	93	3.0	2.0	The key impact in Equation 2 comes from weighting an item's contribution to the score by its relative deviation in absolute weight.	0
1342	9462	9462	S13-13	Weighted NDCG	9	94	4.0	2.0	A set of weights that achieves an equivalent ranking may have a low WDCG if the weights are significantly higher or lower than the reference.	0
1343	9463	9463	S13-13	Weighted NDCG	10	95	4.0	2.0	Equation 2 may be normalized in the same way as the DCG.	0
1344	9464	9464	S13-13	Weighted NDCG	11	96	4.0	3.0	We refer to this final normalized measure as the Weighted Normalized Discounted Cumulative Gain (WNDCG).	0
1345	9465	9465	S13-13	Sense Cluster Comparisons	1	97	1.0	3.0	Sense induction can be viewed as an unsupervised clustering task where usages of a word are grouped into clusters, each representing uses of the same meaning.	0
1346	9466	9466	S13-13	Sense Cluster Comparisons	2	98	1.0	3.0	In previous SemEval tasks on sense induction, instances were labeled with a single sense, which yields a partition over the instances into disjoint sets.	0
1347	9467	9467	S13-13	Sense Cluster Comparisons	3	99	1.0	3.0	The proposed partition can then be compared with a gold-standard partition using many existing clustering comparison methods, such as the V-Measure (Rosenberg and Hirschberg, 2007) or paired FScore .	0
1348	9468	9468	S13-13	Sense Cluster Comparisons	4	100	2.0	3.0	Such cluster comparison methods measure the degree of similarity between the sense boundaries created by lexicographers and those created by WSI methods.	0
1349	9469	9469	S13-13	Sense Cluster Comparisons	5	101	2.0	3.0	In the present task, instances are potentially labeled both with multiple senses and with weights reflecting the applicability.	0
1350	9470	9470	S13-13	Sense Cluster Comparisons	6	102	2.0	3.0	This type of sense labeling produces a fuzzy clustering:	0
1351	9471	9471	S13-13	Sense Cluster Comparisons	7	103	3.0	3.0	An instance may belong to one or more sense clusters with its cluster membership relative to its weight for that sense.	0
1352	9472	9472	S13-13	Sense Cluster Comparisons	8	104	3.0	3.0	Formally, we refer to (1) a solution where the sets of instances overlap as a cover and (2) a solution where the sets overlap and instances may have partial memberships in a set as fuzzy cover.	0
1353	9473	9473	S13-13	Sense Cluster Comparisons	9	105	3.0	3.0	We propose two new fuzzy measures for comparing fuzzy sense assignments: Fuzzy B-Cubed and Fuzzy Normalized Mutual Information.	0
1354	9474	9474	S13-13	Sense Cluster Comparisons	10	106	4.0	3.0	The two measures provide complementary information.	0
1355	9475	9475	S13-13	Sense Cluster Comparisons	11	107	4.0	3.0	B-Cubed summarizes the performance per instance and therefore provides an estimate of how well a system would perform on a new corpus with a similar sense distribution.	0
1356	9476	9476	S13-13	Sense Cluster Comparisons	12	108	4.0	3.0	In contrast, Fuzzy NMI is measured based on the clusters rather than the instances, thereby providing a performance analysis that is independent of the corpus sense distribution.	0
1357	9477	9477	S13-13	Fuzzy B-Cubed	1	109	1.0	3.0	Bagga and Baldwin (1998) proposed a clustering evaluation known as B-Cubed, which compares two partitions on a per-item basis.	0
1358	9478	9478	S13-13	Fuzzy B-Cubed	2	110	1.0	3.0	later extended the definition of B-Cubed to compare overlapping clusters (i.e., covers).	0
1359	9479	9479	S13-13	Fuzzy B-Cubed	3	111	1.0	3.0	We generalize B-Cubed further to handle the case of fuzzy covers.	0
1360	9480	9480	S13-13	Fuzzy B-Cubed	4	112	1.0	3.0	B-Cubed is based on precision and recall, which estimate the fit between two clusterings, X and Y at the item level.	0
1361	9481	9481	S13-13	Fuzzy B-Cubed	5	113	2.0	3.0	For an item i, precision reflects how many items sharing a cluster with i in X appear in its cluster in Y ; conversely, recall measures how many items sharing a cluster in Y with i also appear in its cluster in X.	0
1362	9482	9482	S13-13	Fuzzy B-Cubed	6	114	2.0	3.0	The final B-Cubed value is the harmonic mean of the two scores.	0
1363	9483	9483	S13-13	Fuzzy B-Cubed	7	115	2.0	3.0	To generalize B-Cubed to fuzzy covers, we adopt the formalization of , who define item-based precision and recall functions, P and R, in terms of a correctness function, C → {0, 1}.	0
1364	9484	9484	S13-13	Fuzzy B-Cubed	8	116	2.0	3.0	For notational brevity, let avg be a function that returns the mean value of a series, and µ x (i) denote the set of clusters in clustering X of which item i is a member.	0
1365	9485	9485	S13-13	Fuzzy B-Cubed	9	117	2.0	3.0	B-Cubed precision and recall may therefore calculated over all n items:	0
1366	9486	9486	S13-13	Fuzzy B-Cubed	10	118	3.0	3.0	When comparing partitions, P and R are defined as 1 if two items cluster labels are identical.	0
1367	9487	9487	S13-13	Fuzzy B-Cubed	11	119	3.0	3.0	To generalize B-Cubed for fuzzy covers, we redefine P and R to account for differences in the partial cluster membership of items.	0
1368	9488	9488	S13-13	Fuzzy B-Cubed	12	120	3.0	3.0	Let X (i) denote the set of clusters of which i is a member, and w k (i) denote the membership weight of item i in cluster k in X.	0
1369	9489	9489	S13-13	Fuzzy B-Cubed	13	121	3.0	3.0	We therefore define C with respect to X of two items as	0
1370	9490	9490	S13-13	Fuzzy B-Cubed	14	122	4.0	3.0	Equation 5 is maximized when i and j have identical membership weights in the clusters of which they are members.	0
1371	9491	9491	S13-13	Fuzzy B-Cubed	15	123	4.0	3.0	Importantly, Equation 5generalizes to the correctness operations both when comparing partitions and covers, as defined by .	0
1372	9492	9492	S13-13	Fuzzy B-Cubed	16	124	4.0	3.0	Item-based Precision and Recall are then defined using Equation 5as P (i, j, X) = Min(C(i,j,X),C(i,j,Y ))	0
1373	9493	9493	S13-13	Fuzzy B-Cubed	17	125	4.0	3.0	, respectively.	0
1374	9494	9494	S13-13	Fuzzy B-Cubed	18	126	4.0	3.0	These fuzzy generalizations are used in Equations 3 and 4.	0
1375	9495	9495	S13-13	Fuzzy Normalized Mutual Information	1	127	1.0	3.0	Mutual information measures the dependence between two random variables.	0
1376	9496	9496	S13-13	Fuzzy Normalized Mutual Information	2	128	1.0	3.0	In the context of clustering evaluation, mutual information treats the sense labels as random variables and measures the level of agreement in which instances are labeled with the same senses (Danon et al., 2005).	0
1377	9497	9497	S13-13	Fuzzy Normalized Mutual Information	3	129	1.0	3.0	Formally, mutual information is defined as I(X; Y ) = H(X)−(H(X|Y ) where H(X) denotes the entropy of the random variable X that represents a partition, i.e., the sets of instances assigned to each sense.	0
1378	9498	9498	S13-13	Fuzzy Normalized Mutual Information	4	130	1.0	3.0	Typically, mutual information is normalized to [0, 1] in order to facilitate comparisons between multiple clustering solutions on the same scale (Luo et al., 2009), with M ax(H(X), H(Y )) being the recommended normalizing factor (Vinh et al., 2010).	0
1379	9499	9499	S13-13	Fuzzy Normalized Mutual Information	5	131	1.0	3.0	In its original formulation Mutual information is defined only to compare non-overlapping cluster partitions.	0
1380	9500	9500	S13-13	Fuzzy Normalized Mutual Information	6	132	2.0	3.0	Therefore, we propose a new definition of mutual information between fuzzy covers using extension of Lancichinetti et al. (2009) for calculating the normalized mutual information between covers.	0
1381	9501	9501	S13-13	Fuzzy Normalized Mutual Information	7	133	2.0	3.0	In the case of partitions, a clustering is represented as a discrete random variable whose states denote the probability of being assigned to each cluster.	0
1382	9502	9502	S13-13	Fuzzy Normalized Mutual Information	8	134	2.0	3.0	In the fuzzy cover setting, each item may be assigned to multiple clusters and no longer has a binary assignment to a cluster, but takes on a value in [0, 1].	0
1383	9503	9503	S13-13	Fuzzy Normalized Mutual Information	9	135	2.0	3.0	Therefore, each cluster X i can be represented separately as a continuous random variable, with the entire fuzzy cover denoted as the variable X 1...k , where the ith entry of X is the continuous random variable for cluster i.	0
1384	9504	9504	S13-13	Fuzzy Normalized Mutual Information	10	136	2.0	3.0	However, by modeling clusters using continuous domain, differential entropy must be used for the continuous variables; importantly, differential entropy does not obey the same properties as discrete entropy and may be negative.	0
1385	9505	9505	S13-13	Fuzzy Normalized Mutual Information	11	137	2.0	3.0	To avoid calculating entropy in the continuous domain, we therefore propose an alternative method of computing mutual information based on discretizing the continuous values of X i in the fuzzy setting.	0
1386	9506	9506	S13-13	Fuzzy Normalized Mutual Information	12	138	3.0	3.0	For the continuous random variable X i , we discretize the value by dividing up probability mass into discrete bins.	0
1387	9507	9507	S13-13	Fuzzy Normalized Mutual Information	13	139	3.0	3.0	That is, the support of X i is partitioned into disjoint ranges, each of which represents a discrete outcome of X i .	0
1388	9508	9508	S13-13	Fuzzy Normalized Mutual Information	14	140	3.0	3.0	As a result, X i becomes a categorical distribution over a set of weights ranges {w 1 , . . . , w n } that denote the strength of membership in the fuzzy set.	0
1389	9509	9509	S13-13	Fuzzy Normalized Mutual Information	15	141	3.0	3.0	With respect to sense annotation, this discretization process is analogous to having an annotator rate the applicability of a sense for an instance using a Likert scale instead of using a rational number within a fixed bound.	0
1390	9510	9510	S13-13	Fuzzy Normalized Mutual Information	16	142	3.0	3.0	Discretizing the continuous cluster membership ratings into bins allows us to avoid the problematic interpretation of entropy in the continuous domain while still expanding the definition of mutual information from a binary cluster membership to one of degrees.	0
1391	9511	9511	S13-13	Fuzzy Normalized Mutual Information	17	143	3.0	3.0	Using the definition of X i and Y j as a categorical variables over discrete ratings, we may then estimate the entropy and joint entropy as follows.	0
1392	9512	9512	S13-13	Fuzzy Normalized Mutual Information	18	144	4.0	4.0	where p(w i ) is the probability of an instance being labeled with rating w i Similarly, we may define the joint entropy of two fuzzy clusters as	0
1393	9513	9513	S13-13	Fuzzy Normalized Mutual Information	19	145	4.0	4.0	where p(w i , w j ) is the probability of an instance being labeled with rating w i in cluster X k and w j in cluster Y l , and m denotes the number of bins for Y l .	0
1394	9514	9514	S13-13	Fuzzy Normalized Mutual Information	20	146	4.0	4.0	The conditional entropy between two clusters may then be calculated as	0
1395	9515	9515	S13-13	Fuzzy Normalized Mutual Information	21	147	4.0	4.0	Together, Equations 6 and 7 may be used to define I(X, Y ) as in the original definition.	0
1396	9516	9516	S13-13	Fuzzy Normalized Mutual Information	22	148	4.0	4.0	We then normalize using the method of McDaid et al. (2011).	0
1397	9517	9517	S13-13	Fuzzy Normalized Mutual Information	23	149	4.0	4.0	Based on the limited range of fuzzy memberships in [0, 1], we selected uniformly distributed bins in [0, 1] at 0.1 intervals when discretizing the membership weights for sense labelings.	0
1398	9518	9518	S13-13	Baselines	1	150	1.0	4.0	Task 12 included multiple baselines based on modeling different types of WSI and WSD systems.	0
1399	9519	9519	S13-13	Baselines	2	151	2.0	4.0	Due to space constraints, we include only the four most descriptive here: (1) Semcor MFS which labels each instance with the most frequent sense of that lemma in SemCor, (2) Semcor Ranked Senses baseline, which labels each instance with all of the target lemma's senses, ranked according to their frequency in SemCor, using weights n−i+1 n , where n is the number of senses and i is the rank, (3) 1c1inst which labels each instance with its own induced sense and (4)	0
1400	9520	9520	S13-13	Baselines	3	152	3.0	4.0	All-instances,	0
1401	9521	9521	S13-13	Baselines	4	153	4.0	4.0	One sense which labels all instances with the same induced sense.	0
1402	9522	9522	S13-13	Baselines	5	154	4.0	4.0	The first two baselines directly use WordNet 3.1 senses, while the last two use induced senses.	0
1403	9523	9523	S13-13	Participating Systems	1	155	1.0	4.0	Four teams submitted nine systems, seven of which used induced sense inventories.	0
1404	9524	9524	S13-13	Participating Systems	2	156	1.0	4.0	AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word's contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target.	0
1405	9525	9525	S13-13	Participating Systems	3	157	2.0	4.0	Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010).	0
1406	9526	9526	S13-13	Participating Systems	4	158	2.0	4.0	The resulting contextual distributions are then clustered using K-means to identify word senses.	0
1407	9527	9527	S13-13	Participating Systems	5	159	3.0	4.0	The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012).	0
1408	9528	9528	S13-13	Participating Systems	6	160	3.0	4.0	Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006)   like other teams, the Unimelb systems were trained on a Wikipedia corpus instead of the ukWaC corpus.	0
1409	9529	9529	S13-13	Participating Systems	7	161	4.0	4.0	The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013); the resulting fine-grained clusters are then combined based on their degree of separability.	0
1410	9530	9530	S13-13	Participating Systems	8	162	4.0	4.0	The La Sapienza team submitted two Unsupervised WSD systems based applying Personalized Page Rank (Agirre and Soroa, 2009) over a WordNet-based network to compare the similarity of each sense with the similarity of the context, ranking each sense according to its similarity.	0
1411	9531	9531	S13-13	Results and Discussion	1	163	1.0	4.0	Table 3 shows the main results for all instances.	0
1412	9532	9532	S13-13	Results and Discussion	2	164	1.0	4.0	Additionally, we report the number of induced clusters used to label each sense as #Cl and the number of resulting WordNet 3.1 senses for each sense with #S.	0
1413	9533	9533	S13-13	Results and Discussion	3	165	1.0	4.0	As in previous WSD tasks, the MFS baseline was quite competitive, outperforming all systems on detecting which senses were applicable, measured using the Jaccard Index.	0
1414	9534	9534	S13-13	Results and Discussion	4	166	1.0	4.0	However, most systems were able to outperform the MFS baseline on ranking senses and quantifying their applicability.	0
1415	9535	9535	S13-13	Results and Discussion	5	167	1.0	4.0	Previous cluster comparison evaluations often faced issues with the measures being biased either towards the 1c1inst baseline or labeling all instances with the same sense.	0
1416	9536	9536	S13-13	Results and Discussion	6	168	2.0	4.0	However, Table 3   systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline.	0
1417	9537	9537	S13-13	Results and Discussion	7	169	2.0	4.0	An analysis of the systems' results showed that many systems labeled instances with a high number of senses, which could have been influenced by the trial data having significantly more instances labeled with multiple senses than the test data.	0
1418	9538	9538	S13-13	Results and Discussion	8	170	2.0	4.0	Therefore, we performed a second analysis that partitioned the test set into two sets: those labeled with a single sense and those with multiple senses.	0
1419	9539	9539	S13-13	Results and Discussion	9	171	2.0	4.0	For single-sense set, we modified the test setting to have systems also label instances with a single sense:	0
1420	9540	9540	S13-13	Results and Discussion	10	172	2.0	4.0	(1) the sense mapping function for WSI systems (Sec. 3.1.1) was modified so that after the mapping,  only the highest-weighted WordNet 3.1 sense was used, and (2) the La Sapienza system output was modified to retain only the highest weighted sense.	0
1421	9541	9541	S13-13	Results and Discussion	11	173	2.0	4.0	In this single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall.	0
1422	9542	9542	S13-13	Results and Discussion	12	174	3.0	4.0	The remaining subset of instances annotated with multiple senses were evaluated separately.	0
1423	9543	9543	S13-13	Results and Discussion	13	175	3.0	4.0	Table 4 shows the systems' performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems.	0
1424	9544	9544	S13-13	Results and Discussion	14	176	3.0	4.0	Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002;	0
1425	9545	9545	S13-13	Results and Discussion	15	177	3.0	4.0	Mihalcea et al., 2004;Pradhan et al., 2007;	0
1426	9546	9546	S13-13	Results and Discussion	16	178	3.0	4.0	Agirre et al., 2010).	0
1427	9547	9547	S13-13	Results and Discussion	17	179	4.0	4.0	This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems.	0
1428	9548	9548	S13-13	Results and Discussion	18	180	4.0	4.0	Table 5 shows the performance on the subset of instances that were annotated with multiple senses.	0
1429	9549	9549	S13-13	Results and Discussion	19	181	4.0	4.0	We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus.	0
1430	9550	9550	S13-13	Results and Discussion	20	182	4.0	4.0	Notably, the La Sapienza systems sees a significant performance increase in this setting; their systems label each instance with all of the lemma's senses, which significantly de-grades performance in the most common case where only a single sense applies.	0
1431	9551	9551	S13-13	Results and Discussion	21	183	4.0	4.0	However, when multiple senses are known to be present, their method for quantifying sense applicability appears closest to the gold standard judgments.	0
1432	9552	9552	S13-13	Results and Discussion	22	184	4.0	4.0	Furthermore, the majority of WSI systems are able to surpass all four baselines on identifying which senses are present and quantifying their applicability.	0
1433	9553	9553	S13-13	Conclusion	1	185	1.0	4.0	We have introduced a new evaluation setting for WSI and WSD systems where systems are measured by their ability to detect and weight multiple applicable senses for a single context.	0
1434	9554	9554	S13-13	Conclusion	2	186	2.0	4.0	Four teams submitted nine systems, annotating a total of 4664 contexts for 50 words from the OANC.	0
1435	9555	9555	S13-13	Conclusion	3	187	2.0	4.0	Many systems were able to surpass the competitive MFS baseline.	0
1436	9556	9556	S13-13	Conclusion	4	188	3.0	4.0	Furthermore, when WSI systems were trained to produce only a single sense label, the performance of resulting semi-supervised WSD systems surpassed that of many supervised systems in previous WSD evaluations.	0
1437	9557	9557	S13-13	Conclusion	5	189	3.0	4.0	Future work may assess the impact of graded sense annotations in a task-based setting.	0
1438	9558	9558	S13-13	Conclusion	6	190	4.0	4.0	All materials have been released on the task website.	0
1439	9559	9559	S13-13	Conclusion	7	191	4.0	4.0	1	0
1440	10486	10486	S14-6	title	1	1	4.0	1.0	SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands	0
1441	10487	10487	S14-6	abstract	1	2	1.0	1.0	SemEval-2014	0
1442	10488	10488	S14-6	abstract	2	3	2.0	1.0	Task 6 aims to advance semantic parsing research by providing a high-quality annotated dataset to compare and evaluate approaches.	0
1443	10489	10489	S14-6	abstract	3	4	2.0	1.0	The task focuses on contextual parsing of robotic commands, in which the additional context of spatial scenes can be used to guide a parser to control a robot arm.	1
1444	10490	10490	S14-6	abstract	4	5	3.0	1.0	Six teams submitted systems using both rule-based and statistical methods.	0
1445	10491	10491	S14-6	abstract	5	6	4.0	1.0	The best performing (hybrid) system scored 92.5% and 90.5% for parsing with and without spatial context.	0
1446	10492	10492	S14-6	abstract	6	7	4.0	1.0	However, the best performing statistical system scored 87.35% and 60.84% respectively, indicating that generalized understanding of commands given to a robot remains challenging, despite the fixed domain used for the task.	0
1447	10493	10493	S14-6	Introduction	1	8	1.0	1.0	Semantic parsers analyze sentences to produce formal meaning representations that are used for the computational understanding of natural language.	0
1448	10494	10494	S14-6	Introduction	2	9	1.0	1.0	Recently, state-of-the-art semantic parsing methods have used for a variety of applications, including question answering (Kwiatkowski et al., 2013;	0
1449	10495	10495	S14-6	Introduction	3	10	1.0	1.0	Krishnamurthy and Mitchell, 2012), dialog systems (Artzi and Zettlemoyer, 2011), entity relation extraction (Kate and Mooney, 2010) and robotic control (Tellex, 2011;	0
1450	10496	10496	S14-6	Introduction	4	11	1.0	1.0	Kim and Mooney, 2012).	0
1451	10497	10497	S14-6	Introduction	5	12	1.0	1.0	Different parsers can be distinguished by the level of supervision they require during training.	0
1452	10498	10498	S14-6	Introduction	6	13	2.0	1.0	Fully supervised training typically requires an annotated dataset that maps natural language (NL) to a formal meaning representation such as logical form.	0
1453	10499	10499	S14-6	Introduction	7	14	2.0	1.0	However, because annotated data is often not available, a recent trend in semantic parsing research has been to eschew supervised training in favour of either unsupervised or weakly-supervised methods that utilize additional information.	0
1454	10500	10500	S14-6	Introduction	8	15	2.0	1.0	For example, Berant and Liang (2014) use a dataset of 5,810 questionanswer pairs without annotated logical forms to induce a parser for a question-answering system.	0
1455	10501	10501	S14-6	Introduction	9	16	2.0	1.0	In comparison, Poon (2013) converts NL questions into formal queries via indirect supervision through database interaction.	0
1456	10502	10502	S14-6	Introduction	10	17	2.0	1.0	In contrast to previous work, the shared task described in this paper uses the Robot Commands Treebank (Dukes, 2013a), a new dataset made available for supervised semantic parsing.	0
1457	10503	10503	S14-6	Introduction	11	18	2.0	1.0	The chosen domain is robotic control, in which NL commands are given to a robot arm used to manipulate shapes on an 8 x 8 game board.	0
1458	10504	10504	S14-6	Introduction	12	19	3.0	1.0	Despite the fixed domain, the task is challenging as correctly parsing commands requires understanding spatial context.	0
1459	10505	10505	S14-6	Introduction	13	20	3.0	1.0	For example, the command in Figure 1 may have several plausible interpretations, given different board configurations.	0
1460	10506	10506	S14-6	Introduction	14	21	3.0	1.0	'	0
1461	10507	10507	S14-6	Introduction	15	22	3.0	1.0	Move the pyramid on the blue cube on the gray one.'	0
1462	10508	10508	S14-6	Introduction	16	23	3.0	1.0	The task is inspired by the classic AI system SHRLDU, which responded to NL commands to control a robot for a similar game board (Winograd, 1972), although that system is reported to not have generalized well (Dreyfus, 2009;	0
1463	10509	10509	S14-6	Introduction	17	24	3.0	1.0	Mitkov, 1999).	0
1464	10510	10510	S14-6	Introduction	18	25	4.0	1.0	More recent research in command understanding has focused on parsing jointly with grounding, the process of mapping NL descriptions of entities within an environment to a semantic representation.	0
1465	10511	10511	S14-6	Introduction	19	26	4.0	1.0	Previous work includes Tellex et al. (2011), who develop a small corpus of commands for a simulated fork lift robot, with grounding performed using a factor graph.	0
1466	10512	10512	S14-6	Introduction	20	27	4.0	1.0	Similarly, Kim and Mooney (2012) perform joint parsing and grounding using a corpus of navigation commands.	0
1467	10513	10513	S14-6	Introduction	21	28	4.0	1.0	In contrast, this paper focuses on parsing using additional situational context for disambiguation and by using a larger NL dataset, in comparison to previous robotics research.	0
1468	10514	10514	S14-6	Introduction	22	29	4.0	1.0	In the remainder of this paper, we describe the task, the dataset and the metrics used for evaluation.	0
1469	10515	10515	S14-6	Introduction	23	30	4.0	1.0	We then compare the approaches used by participant systems and conclude with suggested improvements for future work.	0
1470	10516	10516	S14-6	Task Description	1	31	1.0	1.0	The long term research goal encouraged by the task is to develop a system that will robustly execute NL robotic commands.	0
1471	10517	10517	S14-6	Task Description	2	32	2.0	1.0	In general, this is a highly complex problem involving computational processing of language, spatial reasoning, contextual awareness and knowledge representation.	0
1472	10518	10518	S14-6	Task Description	3	33	2.0	1.0	To simplify the problem, participants were provided with additional tools and resources, allowing them to focus on developing a semantic parser for a fixed domain that would fit into an existing component architecture.	0
1473	10519	10519	S14-6	Task Description	4	34	3.0	1.0	Figure 2 shows how these components interact.	0
1474	10520	10520	S14-6	Task Description	5	35	4.0	1.0	Semantic parser: Systems submitted by participants are semantic parsers that accept an NL command as input, mapping this to a formal Robot Control Language (RCL), described further in section 3.3.	0
1475	10521	10521	S14-6	Task Description	6	36	4.0	1.0	The Robot Commands Treebank used for the both training and evaluation is an annotated corpus that pairs NL commands with contextual RCL statements.	0
1476	10522	10522	S14-6	Spatial planner:	1	37	1.0	1.0	A spatial planner is provided as an open Java API 1 . Commands in the treebank are specified in the context of spatial scenes.	0
1477	10523	10523	S14-6	Spatial planner:	2	38	2.0	1.0	By interfacing with the planner, participant systems	0
1478	10524	10524	S14-6	Spatial planner:	3	39	2.0	1.0	1 https://github.com/kaisdukes/train-robots have access to this additional information.	0
1479	10525	10525	S14-6	Spatial planner:	4	40	3.0	1.0	For example, given an RCL fragment for the expression 'the red cube on the blue block', the planner will ground the entity, returning a list of zero or more board coordinates corresponding to possible matches.	0
1480	10526	10526	S14-6	Spatial planner:	5	41	4.0	1.0	The planner also validates commands to determine if they are compatible with spatial context.	0
1481	10527	10527	S14-6	Spatial planner:	6	42	4.0	1.0	It can therefore be used to constrain the search space of possible parses, as well as enabling early resolution of attachment ambiguity during parsing.	0
1482	10528	10528	S14-6	Robotic simulator:	1	43	1.0	1.0	The simulated environment consists of an 8 x 8 board that can hold prisms and cubes which occur in eight different colors.	0
1483	10529	10529	S14-6	Robotic simulator:	2	44	2.0	1.0	The robot's gripper can move to any discrete position within an 8 x 8 x 8 space above the board.	0
1484	10530	10530	S14-6	Robotic simulator:	3	45	2.0	1.0	The planner uses the simulator to enforce physical laws within the game.	0
1485	10531	10531	S14-6	Robotic simulator:	4	46	3.0	1.0	For example, a block cannot remain unsupported in empty space due to gravity.	0
1486	10532	10532	S14-6	Robotic simulator:	5	47	4.0	1.0	Similarly, prisms cannot lie below other block types.	0
1487	10533	10533	S14-6	Robotic simulator:	6	48	4.0	1.0	In the integrated system, the parser uses the planner for context, then provides the final RCL statement to the simulator which executes the command by moving the robot arm to update the board.	0
1488	10534	10534	S14-6	Data	1	49	1.0	1.0	Data Collection	0
1489	10535	10535	S14-6	Data	2	50	2.0	2.0	For the shared task, 3,409 sentences were selected from the treebank.	0
1490	10536	10536	S14-6	Data	3	51	3.0	2.0	This data size compares with related corpora used for semantic parsing such as the ATIS (Zettlemoyer and Collins, 2007), GeoQuery (Kate et al., 2005), Jobs (Tang and Mooney, 2001) and RoboCup (Kuhlmann et al., 2004) datasets, consisting of 4,978; 880; 640 and 300 sentences respectively.	0
1491	10537	10537	S14-6	Data	4	52	4.0	2.0	The treebank was developed via a game with a purpose (www.TrainRobots.com), in which players were shown before and after configurations and asked to give a corresponding command to a hypothetical robot arm.	0
1492	10538	10538	S14-6	Data	5	53	4.0	2.0	To make the game more competitive and to promote data quality, players rated each other's sentences and were rewarded with points for accurate entries (Dukes, 2013b).	0
1493	10539	10539	S14-6	Annotation	1	54	1.0	2.0	In total, over 10,000 commands were collected through the game.	0
1494	10540	10540	S14-6	Annotation	2	55	2.0	2.0	During an offline annotation phase, sentences were manually mapped to RCL.	0
1495	10541	10541	S14-6	Annotation	3	56	2.0	2.0	However, due to the nature of the game, players were free to enter arbitrarily complex sentences to describe moves, not all of which could be represented by RCL.	0
1496	10542	10542	S14-6	Annotation	4	57	3.0	2.0	In addition, some commands were syntactically well-formed, but not compatible with the corresponding scenes.	0
1497	10543	10543	S14-6	Annotation	5	58	4.0	2.0	The 3,409 commands selected for the task had RCL statements that were both understood by the planner and when given to the robotic simulator resulted in the expected move being made between before and after board configurations.	0
1498	10544	10544	S14-6	Annotation	6	59	4.0	2.0	Due to this extra validation step, all RCL statements provided for the task were contextually well-formed.	0
1499	10545	10545	S14-6	Robot Control Language	1	60	1.0	2.0	RCL is a novel linguistically-oriented semantic representation.	0
1500	10546	10546	S14-6	Robot Control Language	2	61	1.0	2.0	An RCL statement is a semantic tree (Figure 3) where leaf nodes generally align to words in the corresponding sentence, and nonleaves are tagged using a pre-defined set of categories.	0
1501	10547	10547	S14-6	Robot Control Language	3	62	1.0	2.0	RCL is designed to annotate rich linguistic structure, including ellipsis (such as 'place [it] on'), anaphoric references ('it' and 'one'), multiword spatial expressions ('on top of') and lexical disambiguation ('one' and 'place').	0
1502	10548	10548	S14-6	Robot Control Language	4	63	1.0	2.0	Due to ellipsis, unaligned words and multi-word expressions, a leaf node may align to zero, one or more words in a sentence.	0
1503	10549	10549	S14-6	Robot Control Language	5	64	2.0	2.0	Figure 4 shows the RCL syntax for the tree in Figure 3, as accepted by the spatial planner and the simulator.	0
1504	10550	10550	S14-6	Robot Control Language	6	65	2.0	2.0	As these components do not require NL word alignment data, this additional information was made available to task participants for training via a separate Java API.	0
1505	10551	10551	S14-6	Robot Control Language	7	66	2.0	2.0	The tagset used to annotate RCL nodes can be divided into general tags (that are arguably applicable to other domains) and specific tags that were customized for the domain in the task (Tables 1 and 2 overleaf, respectively).	0
1506	10552	10552	S14-6	Robot Control Language	8	67	2.0	2.0	The general elements are typed entities (labelled with semantic features) that are connected using relations and events.	0
1507	10553	10553	S14-6	Robot Control Language	9	68	3.0	2.0	This universal formalism is not domain-specific, and is inspired by semantic frames (Fillmore and Baker, 2001), a practical representation used for NL understanding systems (Dzikovska, 2004;	0
1508	10554	10554	S14-6	Robot Control Language	10	69	3.0	2.0	UzZaman and Allen, 2010;	0
1509	10555	10555	S14-6	Robot Control Language	11	70	3.0	2.0	Coyne et al., 2010;Dukes, 2009).	0
1510	10556	10556	S14-6	Robot Control Language	12	71	3.0	2.0	In the remainder of this section we summarize aspects of RCL that are relevant to the task; a more detailed description is provided by Dukes (2013a;.	0
1511	10557	10557	S14-6	Robot Control Language	13	72	4.0	2.0	In an RCL statement such as Figure 4, a preterminal node together with its child leaf node correspond to a feature-value pair (such as the feature color and the constant blue).	0
1512	10558	10558	S14-6	Robot Control Language	14	73	4.0	2.0	Two special features which are distinguished by the planner are id and reference-id, which are used for co-referencing such as for annotating anaphora and their antecedents.	0
1513	10559	10559	S14-6	Robot Control Language	15	74	4.0	2.0	The remaining features model the simulated robotic domain.	0
1514	10560	10560	S14-6	Robot Control Language	16	75	4.0	2.0	For sequence Used to specify a sequence of events or statements.	0
1515	10561	10561	S14-6	spatial-relation	1	76	1.0	2.0	Used to specify a spatial relation between two entities or to describe a location.	0
1516	10562	10562	S14-6	spatial-relation	2	77	2.0	2.0	type Used to specify an entity type.	0
1517	10563	10563	S14-6	spatial-relation	3	78	2.0	2.0	example, the values of the action feature are the moves used to control the robotic arm, while values of the type and relation features are the entity and relation types understood by the spatial planner (Table 2).	0
1518	10564	10564	S14-6	spatial-relation	4	79	3.0	2.0	As well as qualitative relations (such as 'below' or 'above'), the planner also accepts spatial relations that include quantitative measurements, such as in 'two squares left of the red prism' (Figure 5).	0
1519	10565	10565	S14-6	spatial-relation	5	80	3.0	2.0	RCL distinguishes between relations which relate entities and indicators, which are attributes of entities (such as 'left' in 'the left cube').	0
1520	10566	10566	S14-6	spatial-relation	6	81	4.0	2.0	For the task, participants are asked to map NL sentences to well-formed RCL by identifying spatial relations and indicators, then parsing higher-level entities and events.	0
1521	10567	10567	S14-6	spatial-relation	7	82	4.0	2.0	Finally, a well-formed RCL tree with an event (or sequence of events) at toplevel is given the simulator for execution.	0
1522	10568	10568	S14-6	Evaluation Metrics	1	83	1.0	2.0	Out of the 3,400 sentences annotated for the task, 2,500 sentences were provided to participants for system training.	0
1523	10569	10569	S14-6	Evaluation Metrics	2	84	2.0	2.0	During evaluation, trained systems were presented with 909 previously unseen sentences and asked to generate corresponding RCL statements, with access to the spatial planner for additional context.	0
1524	10570	10570	S14-6	Evaluation Metrics	3	85	2.0	2.0	To keep the evaluation process as simple as possible, each parser's output for a sentence was scored as correct if it exactly matched the expected RCL statement in the treebank.	0
1525	10571	10571	S14-6	Evaluation Metrics	4	86	3.0	2.0	Participants were asked to calculate two metrics, P and NP, which are the proportion of exact matches with and without using the spatial planner respectively:  3: System results for supervised semantic parsing of the Robot Commands Treebank (P = parsing with integrated spatial planning, NP = parsing without integrated spatial planning, NP -P = drop in performance without integrated spatial planning, N/A = performance not available).	0
1526	10572	10572	S14-6	Evaluation Metrics	5	87	4.0	2.0	These metrics contrast with measures for partially correct parsed structures, such as Parseval (Black et al., 1991) or the leaf-ancestor metric (Sampson and Babarczy, 2003).	0
1527	10573	10573	S14-6	Evaluation Metrics	6	88	4.0	2.0	The rationale for using a strict match is that in the integrated system, a command will only be executed if it is completely understood, as both the spatial planner and the simulator require well-formed RCL.	0
1528	10574	10574	S14-6	Systems and Results	1	89	1.0	2.0	Six teams participated in the shared task using a variety of strategies (Table 3).	0
1529	10575	10575	S14-6	Systems and Results	2	90	1.0	2.0	The last measure in the table gives the performance drop without spatial context.	0
1530	10576	10576	S14-6	Systems and Results	3	91	2.0	2.0	The value NP -P = -2 for the best performing system suggests this as an upper bound for the task.	0
1531	10577	10577	S14-6	Systems and Results	4	92	2.0	2.0	The different values of this measure indicate the sensitivity to (or possibly reliance on) context to guide the parsing process.	0
1532	10578	10578	S14-6	Systems and Results	5	93	2.0	2.0	In the remainder of this section we compare the approaches and results of the six systems.	0
1533	10579	10579	S14-6	Systems and Results	6	94	3.0	2.0	Packard (2014) achieved the best score for parsing both with and without spatial context, at 92.5% and 90.5%, respectively, using a hybrid system that combines a rule-based grammar with the Berkeley parser (Petrov et al., 2006).	0
1534	10580	10580	S14-6	Systems and Results	7	95	3.0	2.0	The rule-based component uses the English Resource Grammar, a broad coverage handwritten HPSG grammar for English.	0
1535	10581	10581	S14-6	Systems and Results	8	96	3.0	2.0	The ERG produces a ranked list of Minimal Recursion Semantics (MRS) structures that encode predicate argument relations (Copestake et al., 2005).	0
1536	10582	10582	S14-6	Systems and Results	9	97	4.0	2.0	Approximately 80 rules were then used to convert MRS to RCL.	0
1537	10583	10583	S14-6	Systems and Results	10	98	4.0	2.0	The highest ranked result that is validated by the spatial planner was selected as the output of the rule-based system.	0
1538	10584	10584	S14-6	Systems and Results	11	99	4.0	2.0	Using this approach, Packard reports scores of P = 82.4% and NP = 80.3% for parsing the evaluation data.	0
1539	10585	10585	S14-6	UW-MRS:	1	100	1.0	3.0	To further boost performance, the Berkeley parser was used for back-off.	0
1540	10586	10586	S14-6	UW-MRS:	2	101	2.0	3.0	To train the parser, the RCL treebank was converted to phrase struc-ture by removing non-aligned nodes and inserting additional nodes to ensure one-to-one alignment with words in NL sentences.	0
1541	10587	10587	S14-6	UW-MRS:	3	102	2.0	3.0	Performance of the Berkeley parser alone was NP = 81.5% (no P-measure was available as spatial planning was not integrated).	0
1542	10588	10588	S14-6	UW-MRS:	4	103	3.0	3.0	To combine components, the ERG was used initially, with fall back to the Berkeley parser when no contextually compatible RCL statement was produced.	0
1543	10589	10589	S14-6	UW-MRS:	5	104	4.0	3.0	The hybrid approach improved accuracy considerably, with P = 92.5% and NP = 90.5%.	0
1544	10590	10590	S14-6	UW-MRS:	6	105	4.0	3.0	Interestingly, Packard also performs precision and recall analysis, and reports that the rule-based component had higher precision, while the statistical component had higher recall, with the combined system outperforming each separate component in both precision and recall.	0
1545	10591	10591	S14-6	AT&amp;	1	106	1.0	3.0	T Labs Research:	0
1546	10592	10592	S14-6	AT&amp;	2	107	1.0	3.0	The system by Stoyanchev et al. (2014) scored second best for contextual parsing and third best for parsing without using the spatial planner (P = 87.35% and NP = 60.84%).	0
1547	10593	10593	S14-6	AT&amp;	3	108	1.0	3.0	In contrast to Packard's UW-MRS submission, the AT&amp;	0
1548	10594	10594	S14-6	AT&amp;	4	109	1.0	3.0	T system is a combination of three statistical models for tagging, parsing and reference resolution.	0
1549	10595	10595	S14-6	AT&amp;	5	110	1.0	3.0	During the tagging phase, a two-stage sequence tagger first assigns a part-of-speech tag to each word in a sentence, followed by an RCL feature-value pair such as (type: cube) or (color: blue), with unaligned words tagged as 'O'.	0
1550	10596	10596	S14-6	AT&amp;	6	111	1.0	3.0	For parsing, a constituency parser was trained using non-lexical RCL trees.	0
1551	10597	10597	S14-6	AT&amp;	7	112	1.0	3.0	Finally, anaphoric references were resolved using a maximum entropy feature model.	0
1552	10598	10598	S14-6	AT&amp;	8	113	2.0	3.0	When combined, the three components generate a list of weighted RCL trees, which are filtered by the spatial planner.	0
1553	10599	10599	S14-6	AT&amp;	9	114	2.0	3.0	Without integrated planning, the most-probable parse tree is selected.	0
1554	10600	10600	S14-6	AT&amp;	10	115	2.0	3.0	In their evaluation, Stoyanchev et al. report accuracy scores for the separate phases as well as for the combined system.	0
1555	10601	10601	S14-6	AT&amp;	11	116	2.0	3.0	For the tagger, they report an accuracy score of 95.2%, using the standard split of 2,500 sentences for training and 909 for evaluation.	0
1556	10602	10602	S14-6	AT&amp;	12	117	2.0	3.0	To separately measure the joint accuracy of the parser together with reference resolution, gold-standard tags were used resulting in a performance of P = 94.83% and NP = 67.55%.	0
1557	10603	10603	S14-6	AT&amp;	13	118	2.0	3.0	However, using predicted tags, the system's final performance dropped to P = 87.35% and NP = 60.84%.	0
1558	10604	10604	S14-6	AT&amp;	14	119	2.0	3.0	To measure the effect of less supervision, the models were additionally trained on only 500 sentences.	0
1559	10605	10605	S14-6	AT&amp;	15	120	3.0	3.0	In this scenario, the tagging model degraded significantly, while the parsing and reference resolution models performed nearly as well.	0
1560	10606	10606	S14-6	AT&amp;	16	121	3.0	3.0	RoBox: Using Combinatory Categorial Grammar (CCG) as a semantic parsing framework has been previously shown to be suitable for translating NL into logical form.	0
1561	10607	10607	S14-6	AT&amp;	17	122	3.0	3.0	Inspired by previous work using a CCG parser in combination with a structured perceptron (Zettlemoyer and Collins, 2007), RoBox (Evang and Bos, 2014) was the best performing CCG system in the shared task scoring P = 86.8% and NP = 79.21%.	0
1562	10608	10608	S14-6	AT&amp;	18	123	3.0	3.0	Using a similar approach to UW-MRS for its statistical component, RCL trees were interpreted as phrase-structure and converted to CCG derivations for training.	0
1563	10609	10609	S14-6	AT&amp;	19	124	3.0	3.0	During decoding, RCL statements were generated directly by the CCG parser.	0
1564	10610	10610	S14-6	AT&amp;	20	125	3.0	3.0	However, in contrast to the approach used by the AT&amp;	0
1565	10611	10611	S14-6	AT&amp;	21	126	3.0	3.0	T system, RoBox interfaces with the planner during parsing instead of performing spatial validation a post-processing step.	0
1566	10612	10612	S14-6	AT&amp;	22	127	4.0	3.0	This enables early resolution of attachment ambiguity and helps constrain the search space.	0
1567	10613	10613	S14-6	AT&amp;	23	128	4.0	3.0	However, the planner is only used to validate entity elements, so that event and sequence elements were not validated.	0
1568	10614	10614	S14-6	AT&amp;	24	129	4.0	3.0	As a further difference to the AT&amp;	0
1569	10615	10615	S14-6	AT&amp;	25	130	4.0	3.0	T system, anaphora resolution was not performed using a statistical model.	0
1570	10616	10616	S14-6	AT&amp;	26	131	4.0	3.0	Instead, multiple RCL trees were generated with different candidate anaphoric references, which were filtered out contextually using the spatial planner.	0
1571	10617	10617	S14-6	AT&amp;	27	132	4.0	3.0	RoBox suffered only a 7.59% absolute drop in performance without using spatial planning, second only to UW-MRS at 2%.	0
1572	10618	10618	S14-6	AT&amp;	28	133	4.0	3.0	Evang and Bos perform error analysis on RoBox and report that most errors relate to ellipsis, the ambiguous word one, anaphora or attachment ambiguity.	0
1573	10619	10619	S14-6	AT&amp;	29	134	4.0	3.0	They suggest that the system could be improved with better feature selection or by integrating the CCG parser more closely with the spatial planner.	0
1574	10620	10620	S14-6	Shrdlite:	1	135	1.0	3.0	The Shrdlite system by Ljunglöf (2014), inspired by the Classic SHRDLU system by Winograd (1972), is a purely rule-based sys-tem that was shown to be effective for the task.	0
1575	10621	10621	S14-6	Shrdlite:	2	136	1.0	3.0	Scoring P = 86.1% and NP = 51.5%, Shrdlite ranked fourth for parsing with integrated planning, and fifth without using spatial context.	0
1576	10622	10622	S14-6	Shrdlite:	3	137	1.0	3.0	However, it suffered the largest absolute drop in performance without planning (34.6 points), indicating that integration with the planner is essential for the system's reported accuracy.	0
1577	10623	10623	S14-6	Shrdlite:	4	138	2.0	3.0	Shrdlite uses a hand-written compact unification grammar for the fragment of English appearing in the training data.	0
1578	10624	10624	S14-6	Shrdlite:	5	139	2.0	3.0	The grammar is small, consisting of only 25 grammatical rules and 60 lexical rules implemented as a recursive-descent parser in Prolog.	0
1579	10625	10625	S14-6	Shrdlite:	6	140	2.0	3.0	The lexicon consists of 150 words (and multi-word expressions) divided into 23 lexical categories, based on the RCL preterminal nodes found in the treebank.	0
1580	10626	10626	S14-6	Shrdlite:	7	141	3.0	3.0	In a postprocessing phase, the resulting parse trees are normalized to ensure that they are well-formed by using a small set of supplementary rules.	0
1581	10627	10627	S14-6	Shrdlite:	8	142	3.0	3.0	However, the grammar is highly ambiguous resulting in multiple parses for a given input sentence.	0
1582	10628	10628	S14-6	Shrdlite:	9	143	3.0	3.0	These are filtered by the spatial planner.	0
1583	10629	10629	S14-6	Shrdlite:	10	144	4.0	3.0	If multiple parse trees were found to be compatible with spatial context (or when not using the planner), the tree with the smallest number of nodes was selected as the parser's final output.	0
1584	10630	10630	S14-6	Shrdlite:	11	145	4.0	3.0	Additionally, because both the training and evaluation data were collected via crowdsourcing, sentences occasionally contain spelling errors, which were intentionally included in the task.	0
1585	10631	10631	S14-6	Shrdlite:	12	146	4.0	3.0	To handle misspelt words, Shrdlite uses Levenshtein edit distance with a penalty to reparse sentences when the parser initially fails to produce any analysis.	0
1586	10632	10632	S14-6	KUL-Eval:	1	147	1.0	3.0	The CCG system by Mattelaer et al. (2014) uses a different approach to the RoBox system described previously.	0
1587	10633	10633	S14-6	KUL-Eval:	2	148	1.0	3.0	KUL-Eval scored P = 71.29% and NP = 57.76% in comparison to the RoBox scores of P = 86.8% and NP = 79.21%.	0
1588	10634	10634	S14-6	KUL-Eval:	3	149	1.0	4.0	During training, the RCL treebank was converted to λ-expressions.	0
1589	10635	10635	S14-6	KUL-Eval:	4	150	2.0	4.0	This process is fully reversible, so that no information in an RCL tree is lost during conversion.	0
1590	10636	10636	S14-6	KUL-Eval:	5	151	2.0	4.0	In contrast to RoBox, but in common with the AT&amp;	0
1591	10637	10637	S14-6	KUL-Eval:	6	152	2.0	4.0	T parser, KUL-Eval performs spatial validation as a post-processing step and does not integrate the planner directly into the parsing process.	0
1592	10638	10638	S14-6	KUL-Eval:	7	153	3.0	4.0	A probabilistic CCG is used for parsing, so that multiple λ-expressions are returned (each with an associated confidence measure) that are translated into RCL.	0
1593	10639	10639	S14-6	KUL-Eval:	8	154	3.0	4.0	Finally, in the validation step, the spatial planner is used to discard RCL statements that are incompatible with spatial context and the remaining mostprobable parse is returned as the system's output.	0
1594	10640	10640	S14-6	KUL-Eval:	9	155	3.0	4.0	Mattelaer et al. note that in several cases the parser produced partially correct statements but that these outputs did not contribute to the final score, given the strictly matching measures used for the P and NP metrics.	0
1595	10641	10641	S14-6	KUL-Eval:	10	156	4.0	4.0	However, well-formed RCL statements are required by the spatial planner and robotic simulator for the integrated system to robustly execute the specified NL command.	0
1596	10642	10642	S14-6	KUL-Eval:	11	157	4.0	4.0	Partially correct structures included statements which almost matched the expected RCL tree with the exception of incorrect featurevalues, or the addition or deletion of nodes.	0
1597	10643	10643	S14-6	KUL-Eval:	12	158	4.0	4.0	The most common errors were feature-values with incorrect entity types (such as 'edge' and 'region') and mismatched spatial relations (such as confusing 'above ' and 'within' and confusing 'right', 'left' and 'front').	0
1598	10644	10644	S14-6	UWM:	1	159	1.0	4.0	The UWM system submitted by Kate (2014) uses an existing semantic parser, KRISP, for the shared task.	0
1599	10645	10645	S14-6	UWM:	2	160	1.0	4.0	KRISP (Kernel-based Robust Interpretation for Semantic Parsing) is a trainable semantic parser (Kate and Mooney, 2006) that uses Support Vector Machines (SVMs) as the machine learning method with a string subsequence kernel.	0
1600	10646	10646	S14-6	UWM:	3	161	1.0	4.0	As well as training data consisting of RCL paired with NL commands, KRISP required a context-free grammar for RCL, which was hand-written for UWM.	0
1601	10647	10647	S14-6	UWM:	4	162	2.0	4.0	During training, id nodes were removed from the RCL trees.	0
1602	10648	10648	S14-6	UWM:	5	163	2.0	4.0	These were recovered after parsing in a post-processing phase to resolve anaphora by matching to the nearest preceding antecedent.	0
1603	10649	10649	S14-6	UWM:	6	164	2.0	4.0	In contrast to other systems submitted for the task, UWM does not interface with the spatial planner and parses purely non-contextually.	0
1604	10650	10650	S14-6	UWM:	7	165	3.0	4.0	Because the planner was not used, the system's accuracy was negatively impacted by simple issues that may have been easily resolved using spatial context.	0
1605	10651	10651	S14-6	UWM:	8	166	3.0	4.0	For example, in RCL, the verb 'place' can map to either drop or move actions, depending on whether or not a block is held in the gripper in the corresponding spatial scene.	0
1606	10652	10652	S14-6	UWM:	9	167	3.0	4.0	Without using spatial context, it is hard to distinguish between these cases during parsing.	0
1607	10653	10653	S14-6	UWM:	10	168	4.0	4.0	The system scored a non-contextual measure of NP = 45.98%, with Kate reporting a 51.18% best F-measure (at 72.67% precision and 39.49% recall).	0
1608	10654	10654	S14-6	UWM:	11	169	4.0	4.0	No P-measure was reported as the spatial planner was not used.	0
1609	10655	10655	S14-6	UWM:	12	170	4.0	4.0	Due to memory constraints when training the SVM classifiers, only 1,500 out of 2,500 possible sentences were used from the treebank to build the parsing model.	0
1610	10656	10656	S14-6	UWM:	13	171	4.0	4.0	However, it may be possible to increasing the size of training data in future work through sampling.	0
1611	10657	10657	S14-6	Discussion	1	172	1.0	4.0	The six systems evaluated for the task employed a variety of semantic parsing strategies.	0
1612	10658	10658	S14-6	Discussion	2	173	1.0	4.0	With the exception of one submission, all systems interfaced with the spatial planner, either in a postprocessing phase, or directly during parsing to enable early disambiguation and to help constrain the search space.	0
1613	10659	10659	S14-6	Discussion	3	174	1.0	4.0	An open question that remains following the task is how applicable these methods would be to other domains.	0
1614	10660	10660	S14-6	Discussion	4	175	2.0	4.0	Systems that relied heavily on the planner to guide the parsing process could only be adapted to domains for a which a planner could conceivably exist.	0
1615	10661	10661	S14-6	Discussion	5	176	2.0	4.0	For example, nearly all robotic tasks such as such as navigation, object manipulation and task execution involve aspects of planning.	0
1616	10662	10662	S14-6	Discussion	6	177	2.0	4.0	NL question-answering interfaces to databases or knowledge stores are also good candidates for this approach, since parsing NL questions into a semantic representation within the context of a database schema or an ontology could be guided by a query planner.	0
1617	10663	10663	S14-6	Discussion	7	178	2.0	4.0	However, approaches with a more attractive NP -P measure (such as UW-MRS and RoBox) are arguably more easily generalized to other domains, as they are less reliant on a planner.	0
1618	10664	10664	S14-6	Discussion	8	179	3.0	4.0	Additionally, the usual arguments for rule-based systems verses supervised statistical systems apply to any discussion on domain adaptation: rulebased systems require human manual effort, while supervised statistical systems required annotated data for the new domain.	0
1619	10665	10665	S14-6	Discussion	9	180	3.0	4.0	In comparing the best two statistical systems (AT&amp;T and RoBox) it is interesting to note that these performed similarly with integrated planning (P = 87.35% and 86.80%, respectively), but differed considerably without planning (NP = 60.84% and 79.21%).	0
1620	10666	10666	S14-6	Discussion	10	181	3.0	4.0	As these two systems employed different parsers (a constituency parser and a CCG parser), it is difficult to perform a direct comparison to understand why the AT&amp;	0
1621	10667	10667	S14-6	Discussion	11	182	4.0	4.0	T system is more reliant on spatial context.	0
1622	10668	10668	S14-6	Discussion	12	183	4.0	4.0	It would also be interesting to understand, in further work, why the two CCG-based systems differed considerably in their P and NP scores.	0
1623	10669	10669	S14-6	Discussion	13	184	4.0	4.0	It is also surprising that the best performing system, UW-MRS, suffered only a 2% drop in performance without using the planner, demonstrating clearly that in the majority of sentences in the evaluation data, spatial context is not actually required to perform semantic parsing.	0
1624	10670	10670	S14-6	Discussion	14	185	4.0	4.0	Although as shown by the NP -P scores, spatial context can dramatically boost performance of certain approaches for the task when used.	0
1625	10671	10671	S14-6	Conclusion and Future Work	1	186	1.0	4.0	This paper described a new task for SemEval: Supervised Semantic Parsing of Robotic Spatial Commands.	0
1626	10672	10672	S14-6	Conclusion and Future Work	2	187	1.0	4.0	Despite its novel nature, the task attracted high-quality submissions from six teams, using a variety of semantic parsing strategies.	0
1627	10673	10673	S14-6	Conclusion and Future Work	3	188	1.0	4.0	It is hoped that this task will reappear at Se-mEval.	0
1628	10674	10674	S14-6	Conclusion and Future Work	4	189	2.0	4.0	Several lessons were learnt from this first version of the shared task which can be used to improve the task in future.	0
1629	10675	10675	S14-6	Conclusion and Future Work	5	190	2.0	4.0	One issue which several participants noted was the way in which the treebank was split into training and evaluation datasets.	0
1630	10676	10676	S14-6	Conclusion and Future Work	6	191	2.0	4.0	Out of the 3,409 sentences in the treebank, the first 2,500 sequential sentences were chosen for training.	0
1631	10677	10677	S14-6	Conclusion and Future Work	7	192	3.0	4.0	Because this data was not randomized, certain syntactic structures were only found during evaluation and were not present in the training data.	0
1632	10678	10678	S14-6	Conclusion and Future Work	8	193	3.0	4.0	Although this may have affected results, all participants evaluated their systems against the same datasets.	0
1633	10679	10679	S14-6	Conclusion and Future Work	9	194	3.0	4.0	Based on participant feedback, in addition to reporting P and NP-measures, it would also be illuminating to include a metric such as Parseval F1-scores to measure partial accuracy.	0
1634	10680	10680	S14-6	Conclusion and Future Work	10	195	4.0	4.0	An improved version of the task could also feature a better dataset by expanding the treebank, not only in terms of size but also in terms of linguistic structure.	0
1635	10681	10681	S14-6	Conclusion and Future Work	11	196	4.0	4.0	Many commands captured in the annotation game are not yet represented in RCL due to linguistic phenomena such as negation and conditional statements.	0
1636	10682	10682	S14-6	Conclusion and Future Work	12	197	4.0	4.0	Looking forward, a more promising approach to improving the spatial planner could be probabilistic planning, so that semantic parsers could interface with probabilistic facts with confidence measures.	0
1637	10683	10683	S14-6	Conclusion and Future Work	13	198	4.0	4.0	This approach is particularly suitable for robotics, where sensors often supply noisy signals about the robot's environment.	0
1638	11010	11010	S14-9	title	1	1	4.0	1.0	SemEval-2014 Task 9: Sentiment Analysis in Twitter	0
1639	11011	11011	S14-9	abstract	1	2	1.0	1.0	We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014.	0
1640	11012	11012	S14-9	abstract	2	3	2.0	1.0	It is a continuation of the last year's task that ran successfully as part of SemEval-2013.	0
1641	11013	11013	S14-9	abstract	3	4	2.0	1.0	As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams).	0
1642	11014	11014	S14-9	abstract	4	5	3.0	1.0	This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii) LiveJournal sentences.	0
1643	11015	11015	S14-9	abstract	5	6	4.0	1.0	We further tested on (iv) 2013 tweets, and (v) 2013 SMS messages.	0
1644	11016	11016	S14-9	abstract	6	7	4.0	1.0	The highest F1score on (i) was achieved by NRC-Canada at 86.63 for subtask A and by TeamX at 70.96 for subtask B.	0
1645	11017	11017	S14-9	Introduction	1	8	1.0	1.0	In the past decade, new forms of communication have emerged and have become ubiquitous through social media.	0
1646	11018	11018	S14-9	Introduction	2	9	1.0	1.0	Microblogs (e.g., Twitter), Weblogs (e.g., LiveJournal) and cell phone messages (SMS) are often used to share opinions and sentiments about the surrounding world, and the availability of social content generated on sites such as Twitter creates new opportunities to automatically study public opinion.	0
1647	11019	11019	S14-9	Introduction	3	10	1.0	1.0	Working with these informal text genres presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as newswire.	0
1648	11020	11020	S14-9	Introduction	4	11	1.0	1.0	The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, e.g., RT for re-tweet and #hashtags 1 .	0
1649	11021	11021	S14-9	Introduction	5	12	1.0	1.0	This work is licensed under a Creative Commons Attribution 4.0 International Licence.	0
1650	11022	11022	S14-9	Introduction	6	13	1.0	1.0	Page numbers and proceedings footer are added by the organisers.	0
1651	11023	11023	S14-9	Introduction	7	14	2.0	1.0	Licence details: http://creativecommons.org/licenses/by/4.0/	0
1652	11024	11024	S14-9	Introduction	8	15	2.0	1.0	1 Hashtags are a type of tagging for Twitter messages.	0
1653	11025	11025	S14-9	Introduction	9	16	2.0	1.0	Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document.	0
1654	11026	11026	S14-9	Introduction	10	17	2.0	1.0	How to handle such challenges so as to automatically mine and understand people's opinions and sentiments has only recently been the subject of research (Jansen et al., 2009;	0
1655	11027	11027	S14-9	Introduction	11	18	2.0	1.0	Barbosa and Feng, 2010;	0
1656	11028	11028	S14-9	Introduction	12	19	2.0	1.0	Bifet et al., 2011;Davidov et al., 2010;O'Connor et al., 2010;	0
1657	11029	11029	S14-9	Introduction	13	20	2.0	1.0	Pak and Paroubek, 2010;Tumasjan et al., 2010;Kouloumpis et al., 2011).	0
1658	11030	11030	S14-9	Introduction	14	21	3.0	1.0	Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year's SemEval Task 4 (Pontiki et al., 2014).	0
1659	11031	11031	S14-9	Introduction	15	22	3.0	1.0	These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets.	0
1660	11032	11032	S14-9	Introduction	16	23	3.0	1.0	While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment.	0
1661	11033	11033	S14-9	Introduction	17	24	3.0	1.0	Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media.	0
1662	11034	11034	S14-9	Introduction	18	25	3.0	1.0	Toward that goal, we created the Se-mEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013).	0
1663	11035	11035	S14-9	Introduction	19	26	3.0	1.0	It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity.	0
1664	11036	11036	S14-9	Introduction	20	27	4.0	1.0	This year, we extended the corpus by adding new tweets and LiveJournal sentences.	0
1665	11037	11037	S14-9	Introduction	21	28	4.0	1.0	Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (González-Ibáñez et al., 2011;	0
1666	11038	11038	S14-9	Introduction	22	29	4.0	1.0	Liebrecht et al., 2013).	0
1667	11039	11039	S14-9	Introduction	23	30	4.0	1.0	In fact, sarcasm indicates that the message polarity should be flipped.	0
1668	11040	11040	S14-9	Introduction	24	31	4.0	1.0	With this in mind, this year, we also evaluate on sarcastic tweets.	0
1669	11041	11041	S14-9	Introduction	25	32	4.0	1.0	In the remainder of this paper, we first describe the task, the dataset creation process and the evaluation methodology.	0
1670	11042	11042	S14-9	Introduction	26	33	4.0	1.0	We then summarize the characteristics of the approaches taken by the participating systems, and we discuss their scores.	0
1671	11043	11043	S14-9	Task Description	1	34	1.0	1.0	As SemEval-2013	0
1672	11044	11044	S14-9	Task Description	2	35	2.0	1.0	Task 2, we included two subtasks: an expression-level subtask and a messagelevel subtask.	0
1673	11045	11045	S14-9	Task Description	3	36	3.0	2.0	Participants could choose to participate in either or both.	0
1674	11046	11046	S14-9	Task Description	4	37	4.0	2.0	Below we provide short descriptions of the objectives of these two subtasks.	0
1675	11047	11047	S14-9	Subtask A: Contextual Polarity Disambiguation	1	38	2.0	2.0	Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context.	1
1676	11048	11048	S14-9	Subtask A: Contextual Polarity Disambiguation	2	39	4.0	2.0	The instance boundaries were provided: this was a classification task, not an entity recognition task.	0
1677	11049	11049	S14-9	Subtask B: Message Polarity Classification	1	40	1.0	2.0	Given a message, decide whether it is of positive, negative, or neutral sentiment.	0
1678	11050	11050	S14-9	Subtask B: Message Polarity Classification	2	41	1.0	2.0	For messages conveying both positive and negative sentiment, the stronger one is to be chosen.	0
1679	11051	11051	S14-9	Subtask B: Message Polarity Classification	3	42	2.0	2.0	Each participating team was allowed to submit results for two different systems per subtask: one constrained, and one unconstrained.	0
1680	11052	11052	S14-9	Subtask B: Message Polarity Classification	4	43	2.0	2.0	A constrained system could only use the provided data for training, but it could also use other resources such as lexicons obtained elsewhere.	0
1681	11053	11053	S14-9	Subtask B: Message Polarity Classification	5	44	3.0	2.0	An unconstrained system could use any additional data as part of the training process; this could be done in a supervised, semi-supervised, or unsupervised fashion.	0
1682	11054	11054	S14-9	Subtask B: Message Polarity Classification	6	45	3.0	2.0	Note that constrained/unconstrained refers to the data used to train a classifier.	0
1683	11055	11055	S14-9	Subtask B: Message Polarity Classification	7	46	4.0	2.0	For example, if other data (excluding the test data) was used to develop a sentiment lexicon, and the lexicon was used to generate features, the system would still be constrained.	0
1684	11056	11056	S14-9	Subtask B: Message Polarity Classification	8	47	4.0	2.0	However, if other data (excluding the test data) was used to develop a sentiment lexicon, and this lexicon was used to automatically label additional Tweet/SMS messages and then used with the original data to train the classifier, then such a system would be considered unconstrained.	0
1685	11057	11057	S14-9	Datasets	1	48	4.0	2.0	In this section, we describe the process of collecting and annotating the 2014 testing tweets, including the sarcastic ones, and LiveJournal sentences.	0
1686	11058	11058	S14-9	Datasets Used	1	49	1.0	2.0	For training and development, we released the Twitter train/dev/test datasets from SemEval-2013 task 2, as well as the SMS test set, which uses messages from the NUS SMS corpus (Chen and Kan, 2013), which we annotated for sentiment in 2013.	0
1687	11059	11059	S14-9	Datasets Used	2	50	2.0	2.0	We further added a new 2014 Twitter test set, as well as a small set of tweets that contained the #sarcasm hashtag to determine how sarcasm affects the tweet polarity.	0
1688	11060	11060	S14-9	Datasets Used	3	51	3.0	2.0	Finally, we included sentences from LiveJournal in order to determine how systems trained on Twitter perform on other sources.	0
1689	11061	11061	S14-9	Datasets Used	4	52	4.0	2.0	The statistics for each dataset and for each subtask are shown in Tables 1 and 2.	0
1690	11062	11062	S14-9	Annotation	1	53	1.0	2.0	We annotated the new tweets as in 2013: by identifying tweets from popular topics that contain sentiment-bearing words by using SentiWordNet (Baccianella et al., 2010) as a filter.	0
1691	11063	11063	S14-9	Annotation	2	54	2.0	2.0	We altered the annotation task for the sarcastic tweets, displaying them to the Mechanical Turk annotators without the #sarcasm hashtag; the Turkers had to determine whether the tweet is sarcastic on their own.	0
1692	11064	11064	S14-9	Annotation	3	55	3.0	2.0	Moreover, we asked Turkers to indicate the degree of sarcasm as (a) definitely sarcastic, (b) probably sarcastic, and (c) not sarcastic.	0
1693	11065	11065	S14-9	Annotation	4	56	4.0	2.0	As in 2013, we combined the annotations using intersection, where a word had to appear in 2/3 of the annotations to be accepted.	0
1694	11066	11066	S14-9	Annotation	5	57	4.0	2.0	An annotated example from each source is shown in Table 3.   [. . .], and are followed by their polarity; the sentence-level polarity is shown in the last column.	0
1695	11067	11067	S14-9	Tweets Delivery	1	58	1.0	2.0	We did not deliver the annotated tweets to the participants directly; instead, we released annotation indexes, a list of corresponding Twitter IDs, and a download script that extracts the corresponding tweets via the Twitter API.	0
1696	11068	11068	S14-9	Tweets Delivery	2	59	1.0	2.0	2	0
1697	11069	11069	S14-9	Tweets Delivery	3	60	2.0	2.0	We provided the tweets in this manner in order to ensure that Twitter's terms of service are not violated.	0
1698	11070	11070	S14-9	Tweets Delivery	4	61	2.0	2.0	Unfortunately, due to this restriction, the task participants had access to different number of training tweets depending on when they did the downloading.	0
1699	11071	11071	S14-9	Tweets Delivery	5	62	3.0	2.0	This varied between a minimum of 5,215 tweets and the full set of 10,882 tweets.	0
1700	11072	11072	S14-9	Tweets Delivery	6	63	3.0	2.0	On average the teams were able to collect close to 9,000 tweets; for teams that did not participate in 2013, this was about 8,500.	0
1701	11073	11073	S14-9	Tweets Delivery	7	64	4.0	2.0	The difference in training data size did not seem to have had a major impact.	0
1702	11074	11074	S14-9	Tweets Delivery	8	65	4.0	2.0	In fact, the top two teams in subtask B (coooolll and TeamX) trained on less than 8,500 tweets.	0
1703	11075	11075	S14-9	Scoring	1	66	1.0	2.0	The participating systems were required to perform a three-way classification for both subtasks.	0
1704	11076	11076	S14-9	Scoring	2	67	1.0	2.0	A particular marked phrase (for subtask A) or an entire message (for subtask B) was to be classified as positive, negative or objective/neutral.	0
1705	11077	11077	S14-9	Scoring	3	68	1.0	2.0	We scored the systems by computing a score for predicting positive/negative phrases/messages.	0
1706	11078	11078	S14-9	Scoring	4	69	2.0	2.0	For instance, to compute positive precision, p pos , we find the number of phrases/messages that a system correctly predicted to be positive, and we divide that number by the total number it predicted to be positive.	0
1707	11079	11079	S14-9	Scoring	5	70	2.0	2.0	To compute positive recall, r pos , we find the number of phrases/messages correctly predicted to be positive and we divide that number by the total number of positives in the gold standard.	0
1708	11080	11080	S14-9	Scoring	6	71	2.0	2.0	We then calculate F1-score for the positive class as follows F pos = 2(ppos+rpos) ppos * rpos .	0
1709	11081	11081	S14-9	Scoring	7	72	3.0	3.0	We carry out a similar computation for F neg , for the negative phrases/messages.	0
1710	11082	11082	S14-9	Scoring	8	73	3.0	3.0	The overall score is then F = (F pos + F neg )/2.	0
1711	11083	11083	S14-9	Scoring	9	74	3.0	3.0	We used the two test sets from 2013 and the three from 2014, which we combined into one test set and we shuffled to make it hard to guess which set a sentence came from.	0
1712	11084	11084	S14-9	Scoring	10	75	4.0	3.0	This guaranteed that participants would submit predictions for all five test sets.	0
1713	11085	11085	S14-9	Scoring	11	76	4.0	3.0	It also allowed us to test how well systems trained on standard tweets generalize to sarcastic tweets and to LiveJournal sentences, without the participants putting extra efforts into this.	0
1714	11086	11086	S14-9	Scoring	12	77	4.0	3.0	The participants were also not informed about the source the extra test sets come from.	0
1715	11087	11087	S14-9	Scoring	13	78	4.0	3.0	We provided the participants with a scorer that outputs the overall score F and a confusion matrix for each of the five test sets.	0
1716	11088	11088	S14-9	Participants and Results	1	79	1.0	3.0	The results are shown in Tables 4 and 5, and the team affiliations are shown in Table 6.	0
1717	11089	11089	S14-9	Participants and Results	2	80	1.0	3.0	Tables 4  and 5 contain results on the two progress test sets (tweets and SMS messages), which are the official test sets from the 2013 edition of the task, and on the three new official 2014 testsets (tweets, tweets with sarcasm, and LiveJournal).	0
1718	11090	11090	S14-9	Participants and Results	3	81	1.0	3.0	The tables further show macro-and micro-averaged results over the 2014 datasets.	0
1719	11091	11091	S14-9	Participants and Results	4	82	2.0	3.0	There is an index for each result showing the relative rank of that result within the respective column.	0
1720	11092	11092	S14-9	Participants and Results	5	83	2.0	3.0	The participating systems are ranked by their score on the Twitter-2014 testset, which is the official ranking for the task; all remaining rankings are secondary.	0
1721	11093	11093	S14-9	Participants and Results	6	84	2.0	3.0	As we mentioned above, the participants were not told that the 2013 test sets would be included in the big 2014 test set, so that they do not overtune their systems on them.	0
1722	11094	11094	S14-9	Participants and Results	7	85	3.0	3.0	However, the 2013 test sets were made available for development, but it was explicitly forbidden to use them for training.	0
1723	11095	11095	S14-9	Participants and Results	8	86	3.0	3.0	Still, some participants did not notice this restriction, which resulted in their unusually high scores on Twitter2013-test; we did our best to identify all such cases, and we asked the authors to submit corrected runs.	0
1724	11096	11096	S14-9	Participants and Results	9	87	3.0	3.0	The tables mark such resubmissions accordingly.	0
1725	11097	11097	S14-9	Participants and Results	10	88	4.0	3.0	Most of the submissions were constrained, with just a few unconstrained: 7 out of 27 for subtask A, and 8 out of 50 for subtask B.	0
1726	11098	11098	S14-9	Participants and Results	11	89	4.0	3.0	In any case, the best systems were constrained.	0
1727	11099	11099	S14-9	Participants and Results	12	90	4.0	3.0	Some teams participated with both a constrained and an unconstrained system, but the unconstrained system was not always better than the constrained one: sometimes it was worse, sometimes it performed the same.	0
1728	11100	11100	S14-9	Participants and Results	13	91	4.0	3.0	Thus, we decided to produce a single ranking, including both constrained and unconstrained systems, where we mark the latter accordingly.	0
1729	11101	11101	S14-9	Subtask A	1	92	1.0	3.0	Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams.	0
1730	11102	11102	S14-9	Subtask A	2	93	2.0	3.0	There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
1731	11103	11103	S14-9	Subtask A	3	94	3.0	3.0	The best systems were constrained.	0
1732	11104	11104	S14-9	Subtask A	4	95	4.0	3.0	All participating systems outperformed the majority class baseline by a sizable margin.	0
1733	11105	11105	S14-9	Subtask B	1	96	1.0	3.0	The results for subtask B are shown in Table 5.	0
1734	11106	11106	S14-9	Subtask B	2	97	2.0	3.0	The subtask attracted 50 submissions from 44 teams.	0
1735	11107	11107	S14-9	Subtask B	3	98	3.0	3.0	There were eight unconstrained submissions: six teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only.	0
1736	11108	11108	S14-9	Subtask B	4	99	4.0	3.0	As for subtask A, the best systems were constrained.	0
1737	11109	11109	S14-9	Subtask B	5	100	4.0	3.0	Again, all participating systems outperformed the majority class baseline; however, some systems were very close to it.	0
1738	11110	11110	S14-9	Discussion	1	101	1.0	3.0	Overall, we observed similar trends as in SemEval-2013 Task 2.	0
1739	11111	11111	S14-9	Discussion	2	102	1.0	3.0	Almost all systems used supervised learning.	0
1740	11112	11112	S14-9	Discussion	3	103	1.0	3.0	Most systems were constrained, including the best ones in all categories.	0
1741	11113	11113	S14-9	Discussion	4	104	1.0	3.0	As in 2013, we observed several cases of a team submitting a constrained and an unconstrained run and the constrained run performing better.	0
1742	11114	11114	S14-9	Discussion	5	105	1.0	3.0	It is unclear why unconstrained systems did not outperform constrained ones.	0
1743	11115	11115	S14-9	Discussion	6	106	1.0	3.0	It could be because participants did not use enough external data or because the data they used was too different from Twitter or from our annotation method.	0
1744	11116	11116	S14-9	Discussion	7	107	1.0	4.0	Or it could be due to our definition of unconstrained, which labels as unconstrained systems that use additional tweets directly, but considers unconstrained those that use additional tweets to build sentiment lexicons and then use these lexicons.	0
1745	11117	11117	S14-9	Discussion	8	108	1.0	4.0	As in 2013, the most popular classifiers were SVM, MaxEnt, and Naive Bayes.	0
1746	11118	11118	S14-9	Discussion	9	109	2.0	4.0	Moreover, two submissions used deep learning, coooolll (Harbin Institute of Technology) and ThinkPositive (IBM Research, Brazil), which were ranked second and tenth on subtask B, respectively.	0
1747	11119	11119	S14-9	Discussion	10	110	2.0	4.0	The features used were quite varied, including word-based (e.g., word and character ngrams, word shapes, and lemmata), syntactic, and Twitter-specific such as emoticons and abbreviations.	0
1748	11120	11120	S14-9	Discussion	11	111	2.0	4.0	The participants still relied heavily on lexicons of opinion words, the most popular ones being the same as in 2013: MPQA, SentiWord-Net and Bing Liu's opinion lexicon.	0
1749	11121	11121	S14-9	Discussion	12	112	2.0	4.0	Popular this year was also the NRC lexicon (Mohammad et al., 2013), created by the best-performing team in 2013, which is top-performing this year as well.	0
1750	11122	11122	S14-9	Discussion	13	113	2.0	4.0	Preprocessing of tweets was still a popular technique.	0
1751	11123	11123	S14-9	Discussion	14	114	2.0	4.0	In addition to standard NLP steps such as tokenization, stemming, lemmatization, stopword removal and POS tagging, most teams applied some kind of Twitter-specific processing such as substitution/removal of URLs, substitution of emoticons, word normalization, abbreviation lookup, and punctuation removal.	0
1752	11124	11124	S14-9	Discussion	15	115	2.0	4.0	Finally, several of the teams used Twitter-tuned NLP tools such as part of speech and named entity taggers (Gimpel et al., 2011;	0
1753	11125	11125	S14-9	Discussion	16	116	2.0	4.0	Ritter et al., 2011).	0
1754	11126	11126	S14-9	Discussion	17	117	3.0	4.0	The similarity of preprocessing techniques, NLP tools, classifiers and features used in 2013 and this year is probably partially due to many teams participating in both years.	0
1755	11127	11127	S14-9	Discussion	18	118	3.0	4.0	As Table 6 shows, 18 out of the 46 teams are returning teams.	0
1756	11128	11128	S14-9	Discussion	19	119	3.0	4.0	Comparing the results on the progress Twitter test in 2013 and 2014, we can see that NRC-Canada, the 2013 winner for subtask A, have now improved their F1 score from 88.93 to 90.14, which is the 2014 best score.	0
1757	11129	11129	S14-9	Discussion	20	120	3.0	4.0	The best score on the Progress SMS in 2014 of 89.31 belongs to ECNU; this is a big jump compared to their 2013 score of 76.69, but it is less compared to the 2013 best of 88.37 achieved by GU-MLT-LT.	0
1758	11130	11130	S14-9	Discussion	21	121	3.0	4.0	For subtask B, on the Twitter progress testset, the 2013 winner NRC-Canada improves their 2013 result from 69.02 to 70.75, which is the second best in 2014; the winner in 2014, TeamX, achieves 72.12.	0
1759	11131	11131	S14-9	Discussion	22	122	3.0	4.0	On the SMS progress test, the 2013 winner NRC-Canada improves its F1 score from 68.46 to 70.28.	0
1760	11132	11132	S14-9	Discussion	23	123	3.0	4.0	Overall, we see consistent improvements on the progress testset for both subtasks: 0-1 and 2-3 points absolute for subtasks A and B, respectively.	0
1761	11133	11133	S14-9	Discussion	24	124	3.0	4.0	Table 4: Results for subtask A.	0
1762	11134	11134	S14-9	Discussion	25	125	4.0	4.0	The * indicates system resubmissions (because they initially trained on Twitter2013-test), and the indicates a system that includes a task co-organizer as a team member.	0
1763	11135	11135	S14-9	Discussion	26	126	4.0	4.0	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
1764	11136	11136	S14-9	Discussion	27	127	4.0	4.0	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
1765	11137	11137	S14-9	Discussion	28	128	4.0	4.0	Finally, note that for both subtasks, the best systems on the Twitter-2014 dataset are those that performed best on the 2013 progress Twitter dataset: NRC-Canada for subtask A, and TeamX (Fuji Xerox Co., Ltd.) for subtask B.	0
1766	11138	11138	S14-9	Discussion	29	129	4.0	4.0	It is interesting to note that the best results for Twitter2014-test are lower than those for Twitter2013-test for both subtask A (86.63 vs. 90.14) and subtask B (70.96 vs 72.12).	0
1767	11139	11139	S14-9	Discussion	30	130	4.0	4.0	This is so despite the baselines for Twitter2014-test being higher than those for Twitter2013-test: 42.2 vs. 38.1 for subtask A, and 34.6 vs. 29.2 for subtask B. Most likely, having access to Twitter2013-test at development time, teams have overfitted on it.	0
1768	11140	11140	S14-9	Discussion	31	131	4.0	4.0	It could be also the case that some of the sentiment dictionaries that were built in 2013 have become somewhat outdated by 2014.	0
1769	11141	11141	S14-9	Discussion	32	132	4.0	4.0	Finally, note that while some teams such as NRC-Canada performed well across all test sets, other such as TeamX, which used a weighting scheme tuned specifically for class imbalances in tweets, were only strong on Twitter datasets.	0
1770	11142	11142	S14-9	Conclusion	1	133	1.0	4.0	We have described the data, the experimental setup and the results for SemEval-2014 Task 9.	0
1771	11143	11143	S14-9	Conclusion	2	134	1.0	4.0	As in 2013, our task was the most popular one at SemEval-2014, attracting 46 participating teams: 21 in subtask A (27 submissions) and 44 in subtask B (50 submissions).	0
1772	11144	11144	S14-9	Conclusion	3	135	2.0	4.0	We introduced three new test sets for 2014: an in-domain Twitter dataset, an out-of-domain Live-Journal test set, and a dataset of tweets containing sarcastic content.	0
1773	11145	11145	S14-9	Conclusion	4	136	2.0	4.0	While the performance on the LiveJournal test set was mostly comparable to the in-domain Twitter test set, for most teams there was a sharp drop in performance for sarcastic tweets, highlighting better handling of sarcastic language as one important direction for future work in Twitter sentiment analysis.	0
1774	11146	11146	S14-9	Conclusion	5	137	2.0	4.0	We plan to run the task again in 2015 with the inclusion of a new sub-evaluation on detecting sarcasm with the goal of stimulating research in this area; we further plan to add one more test domain.	0
1775	11147	11147	S14-9	Conclusion	6	138	3.0	4.0	-test), and the indicates a system that includes a task co-organizer as a team member.	0
1776	11148	11148	S14-9	Conclusion	7	139	3.0	4.0	The systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets are indicated with a subscript.	0
1777	11149	11149	S14-9	Conclusion	8	140	4.0	4.0	The last two columns show macro-and micro-averaged results across the three 2014 test datasets.	0
1778	11150	11150	S14-9	Conclusion	9	141	4.0	4.0	In the 2015 edition of the task, we might also remove the constrained/unconstrained distinction.	0
1779	11151	11151	S14-9	Conclusion	10	142	4.0	4.0	Finally, as there are multiple opinions about a topic in Twitter, we would like to focus on detecting the sentiment trend towards a topic.	0
1780	12453	12453	S15-6	title	1	1	4.0	1.0	SemEval-2015 Task 6: Clinical TempEval	0
1781	12454	12454	S15-6	abstract	1	2	1.0	1.0	Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp-Eval campaigns to the clinical domain.	0
1782	12455	12455	S15-6	abstract	2	3	2.0	1.0	Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification.	1
1783	12456	12456	S15-6	abstract	3	4	3.0	1.0	Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain.	0
1784	12457	12457	S15-6	abstract	4	5	4.0	1.0	Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.	0
1785	12458	12458	S15-6	Introduction	1	6	1.0	1.0	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
1786	12459	12459	S15-6	Introduction	2	7	1.0	1.0	Verhagen et al., 2010;UzZaman et al., 2013).	0
1787	12460	12460	S15-6	Introduction	3	8	1.0	1.0	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	0
1788	12461	12461	S15-6	Introduction	4	9	2.0	1.0	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
1789	12462	12462	S15-6	Introduction	5	10	2.0	1.0	Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic.	0
1790	12463	12463	S15-6	Introduction	6	11	2.0	1.0	This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understanding of the language of time beyond newswire expressions and structure.	0
1791	12464	12464	S15-6	Introduction	7	12	3.0	1.0	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
1792	12465	12465	S15-6	Introduction	8	13	3.0	1.0	Participating systems are expected to take as input raw text such as:	0
1793	12466	12466	S15-6	Introduction	9	14	3.0	1.0	April 23, 2014:	0
1794	12467	12467	S15-6	Introduction	10	15	4.0	1.0	The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
1795	12468	12468	S15-6	Introduction	11	16	4.0	1.0	And output annotations over the text that capture the following kinds of information:	0
1796	12469	12469	S15-6	Introduction	12	17	4.0	1.0	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
1797	12470	12470	S15-6	Data	1	18	1.0	1.0	The Clinical TempEval corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic.	0
1798	12471	12471	S15-6	Data	2	19	1.0	1.0	These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expressions were not altered.	0
1799	12472	12472	S15-6	Data	3	20	2.0	1.0	The notes were then manually annotated by the THYME project (thyme.healthnlp.org) using an extension of ISO-TimeML for the annotation of times, events and temporal relations in clinical notes (Styler et al., 2014b).	0
1800	12473	12473	S15-6	Data	4	21	2.0	1.0	This extension includes additions such as new time expression types (e.g., PREPOSTEXP for expressions like postoperative), new EVENT attributes (e.g., DE-GREE=LITTLE for expressions like slight nausea), and an increased focus on temporal relations of type CONTAINS (a.k.a. INCLUDES).	0
1801	12474	12474	S15-6	Data	5	22	3.0	1.0	The annotation procedure was as follows:	0
1802	12475	12475	S15-6	Data	6	23	3.0	1.0	1. Annotators identified time and event expressions, along with their attributes 2.	0
1803	12476	12476	S15-6	Data	7	24	4.0	1.0	Adjudicators revised and finalized the time and event expressions and their attributes 3.	0
1804	12477	12477	S15-6	Data	8	25	4.0	1.0	Annotators identified temporal relations between pairs of events and events and times	0
1805	12478	12478	S15-6	Adjudicators revised and finalized the temporal relations	1	26	1.0	1.0	More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a).	0
1806	12479	12479	S15-6	Adjudicators revised and finalized the temporal relations	2	27	1.0	1.0	Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.	0
1807	12480	12480	S15-6	Adjudicators revised and finalized the temporal relations	3	28	2.0	1.0	1	0
1808	12481	12481	S15-6	Adjudicators revised and finalized the temporal relations	4	29	2.0	1.0	The event, time and temporal relation annotations were distributed separately from the text, in an open source repository 2 using the Anafora standoff format (Chen and Styler, 2013).	0
1809	12482	12482	S15-6	Adjudicators revised and finalized the temporal relations	5	30	2.0	1.0	1	0
1810	12483	12483	S15-6	Adjudicators revised and finalized the temporal relations	6	31	3.0	1.0	The details of this process are described at http://thyme.	0
1811	12484	12484	S15-6	Adjudicators revised and finalized the temporal relations	7	32	3.0	1.0	The corpus was split into three portions: Train (50%), Dev (25%) and Test (25%).	0
1812	12485	12485	S15-6	Adjudicators revised and finalized the temporal relations	8	33	4.0	1.0	For Clinical TempEval 2015, the Train portion was used for training and the Dev portion was used for testing.	0
1813	12486	12486	S15-6	Adjudicators revised and finalized the temporal relations	9	34	4.0	1.0	The Test portion was not distributed, and was reserved as a test set for a future iteration of the shared task.	0
1814	12487	12487	S15-6	Adjudicators revised and finalized the temporal relations	10	35	4.0	2.0	Table 1 shows the number of documents, event expressions (EVENT annotations), time expressions (TIMEX3 annotations) and narrative container relations (TLINK annotations with TYPE=CONTAINS attributes) in the Train and Dev portions of the corpus.	0
1815	12488	12488	S15-6	Tasks	1	36	1.0	2.0	A total of nine tasks were included, grouped into three categories:  (Pustejovsky and Stubbs, 2011) between events and/or times, represented by TLINK annotations with TYPE=CONTAINS in the THYME corpus	0
1816	12489	12489	S15-6	Tasks	2	37	2.0	2.0	The evaluation was run in two phases:	0
1817	12490	12490	S15-6	Tasks	3	38	3.0	2.0	1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2.	0
1818	12491	12491	S15-6	Tasks	4	39	4.0	2.0	Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations	0
1819	12492	12492	S15-6	Evaluation Metrics	1	40	1.0	2.0	All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F 1 :	0
1820	12493	12493	S15-6	Evaluation Metrics	2	41	1.0	2.0	where S is the set of items predicted by the system and H is the set of items manually annotated by the humans.	0
1821	12494	12494	S15-6	Evaluation Metrics	3	42	1.0	2.0	"Applying these metrics to the tasks only requires a definition of what is considered an ""item"" for each task."	0
1822	12495	12495	S15-6	Evaluation Metrics	4	43	1.0	2.0	•	0
1823	12496	12496	S15-6	Evaluation Metrics	5	44	1.0	2.0	For evaluating the spans of event expressions or time expressions, items were tuples of (begin, end) character offsets.	0
1824	12497	12497	S15-6	Evaluation Metrics	6	45	2.0	2.0	Thus, systems only received credit for identifying events and times with exactly the same character offsets as the manually annotated ones.	0
1825	12498	12498	S15-6	Evaluation Metrics	7	46	2.0	2.0	•	0
1826	12499	12499	S15-6	Evaluation Metrics	8	47	2.0	2.0	For evaluating the attributes of event expressions or time expressions -Class, Contextual	0
1827	12500	12500	S15-6	Evaluation Metrics	9	48	2.0	2.0	Modality, Degree, Polarity and Type -items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute.	0
1828	12501	12501	S15-6	Evaluation Metrics	10	49	2.0	2.0	Thus, systems only received credit for an event (or time) attribute if they both found an event (or time) with the correct character offsets and then assigned the correct value for that attribute.	0
1829	12502	12502	S15-6	Evaluation Metrics	11	50	2.0	2.0	•	0
1830	12503	12503	S15-6	Evaluation Metrics	12	51	3.0	2.0	For relations between events and the document creation time, items were tuples of (begin, end, value), just as if it were an event attribute.	0
1831	12504	12504	S15-6	Evaluation Metrics	13	52	3.0	2.0	Thus, systems only received credit if they found a correct event and assigned the correct relation (BEFORE, OVERLAP, BEFORE-OVERLAP or AFTER) between that event and the document creation time.	0
1832	12505	12505	S15-6	Evaluation Metrics	14	53	3.0	2.0	Note that in the second phase of the evaluation, when manual event annotations were given as input, precision, recall and F 1 are all equivalent to standard accuracy.	0
1833	12506	12506	S15-6	Evaluation Metrics	15	54	3.0	2.0	•	0
1834	12507	12507	S15-6	Evaluation Metrics	16	55	3.0	2.0	For narrative container relations, items were tuples of ((begin 1 , end 1 ), (begin 2 , end 2 )), where the begins and ends corresponded to the character offsets of the events or times participating in the relation.	0
1835	12508	12508	S15-6	Evaluation Metrics	17	56	4.0	2.0	Thus, systems only received credit for a narrative container relation if they found both events/times and correctly assigned a CONTAINS relation between them.	0
1836	12509	12509	S15-6	Evaluation Metrics	18	57	4.0	2.0	For attributes, an additional metric measures how accurately a system predicts the attribute values on just those events or times that the system predicted.	0
1837	12510	12510	S15-6	Evaluation Metrics	19	58	4.0	2.0	The goal here is to allow a comparison across systems for assigning attribute values, even when different systems produce very different numbers of events and times.	0
1838	12511	12511	S15-6	Evaluation Metrics	20	59	4.0	2.0	This is calculated by dividing the F 1 on the attribute by the F 1 on identifying the spans:	0
1839	12512	12512	S15-6	Evaluation Metrics	21	60	4.0	2.0	For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A CON-TAINS B and B CONTAINS C, so A CONTAINS C):	0
1840	12513	12513	S15-6	Evaluation Metrics	22	61	4.0	2.0	These measures take the approach of prior work (Uz-Zaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure).	0
1841	12514	12514	S15-6	Baseline Systems	1	62	1.0	2.0	Two rule-based systems were used as baselines to compare the participating systems against.	0
1842	12515	12515	S15-6	Baseline Systems	2	63	1.0	2.0	memorize	0
1843	12516	12516	S15-6	Baseline Systems	3	64	2.0	2.0	For all tasks but the narrative container task, a memorization-based baseline was used.	0
1844	12517	12517	S15-6	Baseline Systems	4	65	2.0	2.0	To train the model, all phrases annotated as either events or times in the training data were collected.	0
1845	12518	12518	S15-6	Baseline Systems	5	66	2.0	2.0	All exact character matches for these phrases in the training data were then examined, and only phrases that were annotated as events or times greater than 50% of the time were retained.	0
1846	12519	12519	S15-6	Baseline Systems	6	67	3.0	2.0	For each phrase, the most frequently annotated type (event or time) and attribute values for instances of that phrase were determined.	0
1847	12520	12520	S15-6	Baseline Systems	7	68	3.0	2.0	To predict with the model, the raw text of the test data was searched for all exact character matches of any of the memorized phrases, preferring longer phrases when multiple matches overlapped.	0
1848	12521	12521	S15-6	Baseline Systems	8	69	3.0	2.0	Wherever a phrase match was found, an event or time with the memorized (most frequent) attribute values was predicted.	0
1849	12522	12522	S15-6	Baseline Systems	9	70	4.0	3.0	closest	0
1850	12523	12523	S15-6	Baseline Systems	10	71	4.0	3.0	For the narrative container task, a proximitybased baseline was used.	0
1851	12524	12524	S15-6	Baseline Systems	11	72	4.0	3.0	Each time expression was predicted to be a narrative container, containing only the closest event expression to it in the text.	0
1852	12525	12525	S15-6	Participating Systems	1	73	4.0	3.0	Three research teams submitted a total of 13 runs:	0
1853	12526	12526	S15-6	Human Agreement	1	74	1.0	3.0	We also give two types of human agreement on the task, measured with the same evaluation metrics as the systems:	0
1854	12527	12527	S15-6	Human Agreement	2	75	1.0	3.0	ann-ann Inter-annotator agreement between the two independent human annotators who annotated each document.	0
1855	12528	12528	S15-6	Human Agreement	3	76	2.0	3.0	This is the most commonly reported type of agreement, and often considered to be an upper bound on system performance.	0
1856	12529	12529	S15-6	Human Agreement	4	77	2.0	3.0	adj-ann Inter-annotator agreement between the adjudicator and the two independent annotators.	0
1857	12530	12530	S15-6	Human Agreement	5	78	3.0	3.0	This is usually a better bound on system performance in adjudicated corpora, since the models are trained on the adjudicated data, not on the individual annotator data.	0
1858	12531	12531	S15-6	Human Agreement	6	79	3.0	3.0	"Precision and recall are not reported in these scenarios since they depend on the arbitrary choice of one annotator as the ""human"" (H) and the other as the ""system"" (S)."	0
1859	12532	12532	S15-6	Human Agreement	7	80	4.0	3.0	Note that since temporal relations between events and the document creation time were annotated at the same time as the events themselves, agreement for this task is only reported in phase 1 of the evaluation.	0
1860	12533	12533	S15-6	Human Agreement	8	81	4.0	3.0	Similarly, since narrative container relations were only annotated after events and times had been adjudicated, agreement for this task is only reported in phase 2 of the evaluation.	0
1861	12534	12534	S15-6	Evaluation Results	1	82	1.0	3.0	Time Expressions	0
1862	12535	12535	S15-6	Evaluation Results	2	83	1.0	3.0	Table 2 shows results on the time expression tasks.	0
1863	12536	12536	S15-6	Evaluation Results	3	84	1.0	3.0	The BluLab system achieved the best F 1 at identifying time expressions, 0.725.	0
1864	12537	12537	S15-6	Evaluation Results	4	85	2.0	3.0	The other machine learning systems (KPSCMI run 2-3 and UFPRSheffield-SVM run 1-2) achieved F 1 in the 0.690-0.700 range.	0
1865	12538	12538	S15-6	Evaluation Results	5	86	2.0	3.0	The rule-based systems (KPSCMI run 1 and UFPRSheffield-Hynx run 1-5) all achieved higher recall than the machine learning systems, but at substantial costs to precision.	0
1866	12539	12539	S15-6	Evaluation Results	6	87	2.0	3.0	All systems outperformed the memorization baseline in terms of recall, and all machine-learning systems outperformed it in terms of F 1 , but only the BluLab system outperformed the baseline in terms of precision.	0
1867	12540	12540	S15-6	Evaluation Results	7	88	2.0	3.0	The BluLab system also achieved the best F 1 for predicting the classes of time expressions, though this is primarily due to achieving a higher F 1 at identifying time expressions in the first place.	0
1868	12541	12541	S15-6	Evaluation Results	8	89	3.0	3.0	UFPRSheffield-Hynx run 5 achieved the best accuracy on predicting classes for the time expressions it found, 0.978, though on this metric it only outperformed the memorization baseline by 0.004.	0
1869	12542	12542	S15-6	Evaluation Results	9	90	3.0	3.0	Across the time expression tasks, systems did not quite achieve performance at the level of human agreement.	0
1870	12543	12543	S15-6	Evaluation Results	10	91	3.0	3.0	For the spans of time expressions, the top system achieved 0.725 F 1, compared to 0.774 adjudicator-annotator F 1 , though almost half of the systems exceeded the lower annotator-annotator F 1 of 0.690.	0
1871	12544	12544	S15-6	Evaluation Results	11	92	4.0	3.0	For the classes of time expressions, the story was similar for F 1 , though several models exceeded the adjudicator-annotator accuracy of 0.965 on just the time expressions predicted by the system.	0
1872	12545	12545	S15-6	Evaluation Results	12	93	4.0	3.0	one exception was the semantic type of the event, where the memorization baseline had a better precision and also a better accuracy on the classes of the events that it identified.	0
1873	12546	12546	S15-6	Evaluation Results	13	94	4.0	3.0	The BluLab system got close to the level of adjudicator-annotator agreement on identifying the spans of event expressions (0.875 vs. 0.880 F 1 ), identifying the degree of events (0.870 vs. 0.877 F 1 ), and identifying the polarity of events (0.857 vs. 0.869 F 1 ), and it generally met or exceeded the lower annotator-annotator agreement on these tasks.	0
1874	12547	12547	S15-6	Evaluation Results	14	95	4.0	3.0	There is a larger gap (3+ points of F 1 ) between the system performance and adjudicator-annotator agreement for event modality and event type, though only a small gap (&lt;1 point of F 1 ) for the lower annotatorannotator agreement on these tasks.	0
1875	12548	12548	S15-6	Event Expressions	1	96	1.0	3.0	Temporal Relations	0
1876	12549	12549	S15-6	Event Expressions	2	97	1.0	3.0	Table 4 shows performance on the temporal relation tasks.	0
1877	12550	12550	S15-6	Event Expressions	3	98	2.0	3.0	In detecting the relations between events and the document creation time, the BluLab system substantially outperformed the memorization baseline, achieving F 1 of 0.702 on system-predicted events (phase 1) and F 1 of 0.791 on manually annotated events (phase 2).	0
1878	12551	12551	S15-6	Event Expressions	4	99	2.0	3.0	In identifying narrative container relations, the best BluLab system (run 2) outperformed the proximity-based baseline when using systempredicted events (F closure of 0.123 vs. 0.106) but not when using manually annotated events (F closure of 0.181 vs. 0.260).	0
1879	12552	12552	S15-6	Event Expressions	5	100	3.0	3.0	Across both phase 1 and phase 2 for narrative container relations, the top BluLab system always had the best recall, while the baseline system always had the best precision.	0
1880	12553	12553	S15-6	Event Expressions	6	101	3.0	3.0	Annotator agreement was higher than system performance on all temporal relation tasks.	0
1881	12554	12554	S15-6	Event Expressions	7	102	4.0	3.0	For relations between events and the document creation time, adjudicator-annotator agreement was 0.761 F 1 , compared to the best system's 0.702 F 1 , though this system did exceed the lower annotator-annotator agreement of 0.628 F 1 .	0
1882	12555	12555	S15-6	Event Expressions	8	103	4.0	3.0	For narrative container relations using manually annotated EVENTs and TIMEX3s, the gap was much greater, with adjudicator-annotator agreement at 0.672 F closure , and the top system (the baseline system) at 0.260 F closure .	0
1883	12556	12556	S15-6	Event Expressions	9	104	4.0	4.0	Even the lower annotator-annotator agreement of 0.475 F closure was much higher than the system performance.	0
1884	12557	12557	S15-6	Discussion	1	105	1.0	4.0	The results of Clinical TempEval 2015 suggest that a small number of temporal information extraction tasks are solved by current state-of-the-art systems, but for the majority of tasks, there is still room for improvement.	0
1885	12558	12558	S15-6	Discussion	2	106	1.0	4.0	Identifying events, their degrees and their polarities were the easiest tasks for the participants, with the best systems achieving within about 0.01 of human agreement on the tasks.	0
1886	12559	12559	S15-6	Discussion	3	107	1.0	4.0	Systems for identifying event modality and event type were not far behind, achieving within about 0.03 of human agree-  ment.	0
1887	12560	12560	S15-6	Discussion	4	108	1.0	4.0	Time expressions and relations to the document creation time were at the next level of difficulty, with a gap of about 0.05 from human agreement.	0
1888	12561	12561	S15-6	Discussion	5	109	1.0	4.0	Identifying narrative container relations was by far the most difficult task, with the best systems down by more than 0.40 from human agreement.	0
1889	12562	12562	S15-6	Discussion	6	110	1.0	4.0	In absolute terms, performance on narrative container relations was also quite low, with system F closure scores in the 0.10-0.12 range on system-generated events and times, and in the 0.12-0.26 range on manually-annotated events and times.	0
1890	12563	12563	S15-6	Discussion	7	111	1.0	4.0	For comparison, in TempEval 2013, which used newswire data, F closure scores were in the 0.24-0.36 range on systemgenerated events and times and in the 0.35-0.56 range on manually-annotated events and times (UzZaman et al., 2013).	0
1891	12564	12564	S15-6	Discussion	8	112	1.0	4.0	One major difference between the corpora is that the narrative container relations in the clinical domain often span many sentences, while almost all of the relations in TempEval 2013 were either within the same sentence or across adjacent sentences.	0
1892	12565	12565	S15-6	Discussion	9	113	2.0	4.0	Most past research systems have also focused on identifying within-sentence and adjacent-sentence relations.	0
1893	12566	12566	S15-6	Discussion	10	114	2.0	4.0	This focus on local relations might explain the poor performance on the more distant relations in the THYME corpus.	0
1894	12567	12567	S15-6	Discussion	11	115	2.0	4.0	But further investigation is needed to better understand the challenge here.	0
1895	12568	12568	S15-6	Discussion	12	116	2.0	4.0	In almost all tasks, the submitted systems substantially outperformed the baselines.	0
1896	12569	12569	S15-6	Discussion	13	117	2.0	4.0	The one exception to this was the narrative containers task.	0
1897	12570	12570	S15-6	Discussion	14	118	2.0	4.0	The baseline there -which simply predicted that each time expression contained the nearest event expression to it in the text -achieved 4 times the precision of the best submitted system and consequently achieved the best F 1 by a large margin.	0
1898	12571	12571	S15-6	Discussion	15	119	2.0	4.0	This suggests that future systems may want to incorporate better measures of proximity that can capture some of what the baseline is finding.	0
1899	12572	12572	S15-6	Discussion	16	120	2.0	4.0	While machine learning methods were overall the most successful, for time expression identification, the submitted rule-based systems achieved the best recall.	0
1900	12573	12573	S15-6	Discussion	17	121	2.0	4.0	This is counter to the usual assumption that rule-based systems will be more precise, and that machine learning systems will sacrifice precision to increase recall.	0
1901	12574	12574	S15-6	Discussion	18	122	3.0	4.0	The difference is likely that the rulebased systems were aiming for good coverage, trying to find all potential time expressions, but had too few constraints to discard such phrases in inappropriate contexts.	0
1902	12575	12575	S15-6	Discussion	19	123	3.0	4.0	The baseline system is suggestive of this possibility: it has a constraint to only memorize phrases that corresponded with time expressions more than 50% of the time, and it has high precision (0.743) and low recall (0.372) as is typically expected of a rule-based system, but if the constraint is removed, it has low precision (0.126) and high recall (0.521) like the participant rule-based systems.	0
1903	12576	12576	S15-6	Discussion	20	124	3.0	4.0	Clinical TempEval was the first TempEval exercise to use narrative containers, a significant shift from prior exercises.	0
1904	12577	12577	S15-6	Discussion	21	125	3.0	4.0	Annotator agreement in the dataset is moderate, but needs to be further improved.	0
1905	12578	12578	S15-6	Discussion	22	126	3.0	4.0	Similar agreement scores were found when annotating temporal relations in prior corpora (for TempEval or using TimeML), although these typically involved the application of more complex temporal relation ontologies.	0
1906	12579	12579	S15-6	Discussion	23	127	3.0	4.0	The narrative container approach is comparatively simple.	0
1907	12580	12580	S15-6	Discussion	24	128	3.0	4.0	The low annotator-adjudicator scores (i.e. below 0.90, a score generally recognized to indicate a production-quality resource) suggests that annotation is difficult independent of the number of potential temporal relation types.	0
1908	12581	12581	S15-6	Discussion	25	129	3.0	4.0	Difficulty may lie in the comprehension and reification of the potentially complex temporal structures described in natural language text.	0
1909	12582	12582	S15-6	Discussion	26	130	4.0	4.0	Nevertheless, systems did well on the DCT task, achieving high scores -similar to the pattern seen in Task D of TempEval-2, which had a comparable scoring metric.	0
1910	12583	12583	S15-6	Discussion	27	131	4.0	4.0	Though the results of Clinical TempEval 2015 are encouraging, they were limited somewhat by the small number of participants in the task.	0
1911	12584	12584	S15-6	Discussion	28	132	4.0	4.0	There are two likely reasons for this.	0
1912	12585	12585	S15-6	Discussion	29	133	4.0	4.0	First, there were many different sub-tasks for Clinical TempEval, meaning that to compete in all sub-tasks, a large number of sub-systems had to be developed in a limited amount of time (six months or less).	0
1913	12586	12586	S15-6	Discussion	30	134	4.0	4.0	This relatively high barrier for entry meant that of the 15 research groups that managed to sign a data use agreement and obtain the data before the competition, only 3 submitted systems to compete.	0
1914	12587	12587	S15-6	Discussion	31	135	4.0	4.0	Second, the data use agreement process was time consuming, and more than 10 research groups who began the data use agreement process were unable to complete it before the evaluation.	0
1915	12588	12588	S15-6	Discussion	32	136	4.0	4.0	In future iterations of Clinical TempEval, we expect these issues to be reduced.	0
1916	12589	12589	S15-6	Discussion	33	137	4.0	4.0	The next Clinical TempEval will use the current Train and Dev data as the training set, and as these data are already available, this leaves research teams with a year or more to develop systems.	0
1917	12590	12590	S15-6	Discussion	34	138	4.0	4.0	Furthermore, arrangements with the Mayo Clinic have been made to further expedite the data use agreement process, which should significantly reduce the wait time for new participants.	0
1918	13717	13717	S15-13	title	1	1	4.0	1.0	SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking	1
1919	13718	13718	S15-13	abstract	1	2	1.0	1.0	In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.	1
1920	13719	13719	S15-13	abstract	2	3	2.0	1.0	Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.	0
1921	13720	13720	S15-13	abstract	3	4	3.0	1.0	Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.	0
1922	13721	13721	S15-13	abstract	4	5	4.0	1.0	Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1) might enable WSD and EL to be solved by means of similar (even, the same) methods.	0
1923	13722	13722	S15-13	abstract	5	6	4.0	1.0	Moreover, we investigate this task in a multilingual setting and for some specific domains.	0
1924	13723	13723	S15-13	Introduction	1	7	1.0	1.0	The Senseval and SemEval evaluation series represent key moments in the community of computational linguistics and related areas.	0
1925	13724	13724	S15-13	Introduction	2	8	1.0	1.0	Their focus has been to provide objective evaluations of methods within the wide spectrum of semantic techniques for tasks mainly related to automatic text understanding.	0
1926	13725	13725	S15-13	Introduction	3	9	1.0	1.0	Through SemEval-2015 task 13 we both continue and renew the longstanding tradition of disambiguation tasks, by addressing multilingual WSD and EL in a joint manner.	0
1927	13726	13726	S15-13	Introduction	4	10	1.0	1.0	WSD (Navigli, 2009;	0
1928	13727	13727	S15-13	Introduction	5	11	1.0	1.0	Navigli, 2012) is a historical task aimed at explicitly assigning meanings to single-word and multi-word occurrences within text, a task which today is more alive than ever in the research community.	0
1929	13728	13728	S15-13	Introduction	6	12	2.0	1.0	EL (Erbs et al., 2011;	0
1930	13729	13729	S15-13	Introduction	7	13	2.0	1.0	Cornolti et al., 2013;	0
1931	13730	13730	S15-13	Introduction	8	14	2.0	1.0	Rao et al., 2013) is a more recent task which aims at discovering mentions of entities within a text and linking them to the most suitable entry in a knowledge base.	0
1932	13731	13731	S15-13	Introduction	9	15	2.0	1.0	Both these tasks aim at handling the inherent ambiguity of natural language, however WSD tackles it from a lexicographic perspective, while EL tackles it from an encyclopedic one.	0
1933	13732	13732	S15-13	Introduction	10	16	2.0	1.0	Specifically, the main difference between the two tasks lies in the kind of inventory they use.	0
1934	13733	13733	S15-13	Introduction	11	17	2.0	1.0	For instance, WordNet (Miller et al., 1990), a manually curated semantic network for the English language, has become the main reference inventory for English WSD systems thanks to its wide coverage of verbs, adverbs, adjectives and common nouns.	0
1935	13734	13734	S15-13	Introduction	12	18	3.0	1.0	More recently, Wikipedia has been shown to be an optimal resource for recovering named entities, and has consequently become -together with all its semi-automatic derivations such as DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2008) -the main reference inventory for EL systems.	0
1936	13735	13735	S15-13	Introduction	13	19	3.0	1.0	Over the years, the research community has typically focused on each of these tasks separately.	0
1937	13736	13736	S15-13	Introduction	14	20	3.0	1.0	Recently, however, joint approaches have been proposed (Moro et al., 2014b).	0
1938	13737	13737	S15-13	Introduction	15	21	3.0	1.0	One of the reasons for pursuing the unification of these tasks derives from the current trend in knowledge acquisition which consists of the seamless integration of encyclopedic and lexicographic knowledge within structured language resources (Hovy et al., 2013).	0
1939	13738	13738	S15-13	Introduction	16	22	3.0	1.0	A case in point here is BabelNet 1 , a multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012).	0
1940	13739	13739	S15-13	Introduction	17	23	4.0	1.0	Resources like BabelNet provide a common ground for the tasks of WSD and EL.	0
1941	13740	13740	S15-13	Introduction	18	24	4.0	1.0	In this task our goal is to promote research in the direction of joint word sense and named entity disambiguation, so as to concentrate research efforts on the aspects that differentiate these two tasks without duplicating research on common problems such as identifying the right meaning in context.	0
1942	13741	13741	S15-13	Introduction	19	25	4.0	1.0	However, we are also interested in systems that perform only one of the two tasks, and even systems which tackle one particular setting of WSD, such as allwords sense disambiguation vs. any subset of partof-speech tags.	0
1943	13742	13742	S15-13	Introduction	20	26	4.0	1.0	Moreover, given the recent upsurge of interest in multilingual approaches, we developed the task dataset in three different languages (English, Italian and Spanish) on parallel texts which have been independently and manually annotated by different native/fluent speakers.	0
1944	13743	13743	S15-13	Introduction	21	27	4.0	1.0	In contrast to the SemEval-2013 task 12 on Multilingual Word Sense Disambiguation , our focus in task 13 is to present a dataset containing both kinds of inventories (i.e., named entities and word senses) in different specific domains (biomedical domain, maths and computer domain, and a broader domain about social issues).	0
1945	13744	13744	S15-13	Introduction	22	28	4.0	1.0	Our goal is to further investigate the distance between research efforts regarding the dichotomy EL vs. WSD and those regarding the dichotomy open domain vs. closed domain.	0
1946	13745	13745	S15-13	Task Setup	1	29	1.0	1.0	The task setup consists of annotating four tokenized and part-of-speech tagged documents for which parallel versions in three languages (English, Italian and Spanish) have been provided.	0
1947	13746	13746	S15-13	Task Setup	2	30	2.0	1.0	Differently from previous editions Lefever and Hoste, 2013;Manandhar et al., 2010;	0
1948	13747	13747	S15-13	Task Setup	3	31	3.0	1.0	Lefever and Hoste, 2010;Pradhan et al., 2007;	0
1949	13748	13748	S15-13	Task Setup	4	32	4.0	1.0	Navigli et al., 2007;	0
1950	13749	13749	S15-13	Task Setup	5	33	4.0	1.0	Snyder and Palmer, 2004;Palmer et al., 2001), in this task we do not make explicit to the participating systems which fragments of the input text should be disambiguated, so as to have, on the one hand, a more realistic scenario, and, on the other hand, to follow the recent trend in EL challenges such as TAC KBP (Ji et al., 2014), MicroPost (Basave et al., 2013 and ERD (Carmel et al., 2014).	0
1951	13750	13750	S15-13	Corpora	1	34	1.0	1.0	"The documents considered in this task are taken from the OPUS project (http://opus.lingfil.uu.se/), more specifically from the EMEA (European Medicines Agency documents), KDEdoc (the KDE manual corpus) and ""The EU bookshop corpus"", which make available parallel and POS-tagged documents."	0
1952	13751	13751	S15-13	Corpora	2	35	2.0	1.0	We took four documents from these repositories.	0
1953	13752	13752	S15-13	Corpora	3	36	3.0	1.0	Two documents contain medical information about drugs.	0
1954	13753	13753	S15-13	Corpora	4	37	4.0	1.0	One document consists of the manual of a mathematical graph calculator (i.e., KAlgebra).	0
1955	13754	13754	S15-13	Corpora	5	38	4.0	1.0	The remaining document contains a formal discussion about social issues, like supporting elderly workers and, more in general, about issues and solutions to unemployment discussed by the members of the European Commission.	0
1956	13755	13755	S15-13	Sense Inventory	1	39	1.0	1.0	As our sense inventory we use the BabelNet 2.5.1 (http://babelnet.org) multilingual semantic network and encyclopedic dictionary (Navigli and Ponzetto, 2012), which is the result of the automatic integration of multiple language resources: Princeton WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata, Open Multi WordNet and automatic translations.	0
1957	13756	13756	S15-13	Sense Inventory	2	40	2.0	1.0	The meanings contained within this resource are organized in Babel synsets.	0
1958	13757	13757	S15-13	Sense Inventory	3	41	3.0	1.0	Each of these synsets can contain Wikipedia pages, Word-Net synsets and items from the other integrated resources.	0
1959	13758	13758	S15-13	Sense Inventory	4	42	4.0	1.0	"For instance, in BabelNet it is possible to find the concept ""medicine"" (bn:00054128n), which is represented by both the second word sense of medicine in WordNet and the Wikipedia page Pharmaceutical drug, among others, together with synonyms such as drug and medication in English and lexicalizations in other languages, such as farmaco in Italian and medicamento in Spanish."	0
1960	13759	13759	S15-13	Dataset Creation	1	43	1.0	2.0	The manual annotation of documents was performed in a language-specific manner, i.e., different taggers worked on the various translated versions of the input documents.	0
1961	13760	13760	S15-13	Dataset Creation	2	44	1.0	2.0	More precisely, we had two taggers for each language, who annotated each fragment of text recognized as linkable with all the senses deemed appropriate.	0
1962	13761	13761	S15-13	Dataset Creation	3	45	2.0	2.0	During the annotation procedure, for all languages, each tagger was shown an HTML page containing the sentence within which the target fragment was boldfaced.	0
1963	13762	13762	S15-13	Dataset Creation	4	46	2.0	2.0	Then a table of checkable meanings identified by their glosses (in English or, if not available, in Spanish or Italian), to-  gether with the available synonyms and hypernyms (as found in WordNet and the Wikipedia Bitaxonomy (Flati et al., 2014)).	0
1964	13763	13763	S15-13	Dataset Creation	5	47	2.0	2.0	The taggers agreed on at least one meaning for 68% of the instances.	0
1965	13764	13764	S15-13	Dataset Creation	6	48	3.0	2.0	A third tagger acted as judge by going through all the items and discarding overly general or irrelevant annotations, especially in the case of disagreement between the two taggers.	0
1966	13765	13765	S15-13	Dataset Creation	7	49	3.0	2.0	To enforce coherence and spot missing annotations, we projected the English annotations to the other two languages.	0
1967	13766	13766	S15-13	Dataset Creation	8	50	3.0	2.0	Finally, the third tagger determined if the projected English annotations that were missing in one of the other two languages were either correctly not included, or if the taggers had actually missed a correct annotation.	0
1968	13767	13767	S15-13	Dataset Creation	9	51	4.0	2.0	As a result of this procedure we obtained a dataset with around 1.2k items, but with only around 80 named entity mentions per language.	0
1969	13768	13768	S15-13	Dataset Creation	10	52	4.0	2.0	Please refer to Table 1 for general statistics about the dataset: we show the number of annotated instances per language and domain, together with their classification as single-or multi-word expressions and named entities.	0
1970	13769	13769	S15-13	Dataset Creation	11	53	4.0	2.0	We then show the degree of ambiguity both per POS and per instance and lemma (i.e., multiple instances with the same lemma count as a single instance) and, finally, we show how many of the instances have Wikipedia pages or WordNet keys as annotations 2 .	0
1971	13770	13770	S15-13	Evaluation Measures	1	54	2.0	2.0	To evaluate the performance of the participating systems we used the classical precision, recall and F1 measures:	0
1972	13771	13771	S15-13	Evaluation Measures	2	55	4.0	2.0	2 Please note that the sum of Wikipedia pages and WordNet keys does not amount to the number of instances, as BabelNet can have integrated synsets that contain both WordNet keys and Wikipedia pages.	0
1973	13772	13772	S15-13	P recision = T P T P + F P	1	56	2.0	2.0	(1)	0
1974	13773	13773	S15-13	P recision = T P T P + F P	2	57	3.0	2.0	To handle systems that output multiple answers for a single instance we followed the standard scorer of previous Senseval and SemEval challenges in uniformly weighting the multiple answers when computing the TP counts.	0
1975	13774	13774	S15-13	P recision = T P T P + F P	3	58	4.0	2.0	Moreover, we decided not to take into account fragments annotated by the systems which were not contained in the gold standard, similarly to the D2KB setting of the GERBIL evaluation framework for EL (Usbeck et al., 2015).	0
1976	13775	13775	S15-13	Baseline	1	59	1.0	2.0	As baseline we considered the performance of a simple heuristic (called Babel	0
1977	13776	13776	S15-13	Baseline	2	60	1.0	2.0	Net first sense or BFS) that exploits the default comparator integrated within the BabelNet 2.5.1 API (i.e., the Babel-SynsetComparator Java class).	0
1978	13777	13777	S15-13	Baseline	3	61	1.0	2.0	Babel synsets in Ba-belNet can be viewed as nodes of a semantic network and each of them can contain Wikipedia pages, WordNet synsets and items from the other integrated resources.	0
1979	13778	13778	S15-13	Baseline	4	62	1.0	2.0	The comparator takes as input the lemma of the word for which we are ranking the Babel synsets.	0
1980	13779	13779	S15-13	Baseline	5	63	1.0	2.0	There are three main cases managed by the comparator.	0
1981	13780	13780	S15-13	Baseline	6	64	2.0	2.0	The first case is when both Babel synsets contain a WordNet synset for the considered word.	0
1982	13781	13781	S15-13	Baseline	7	65	2.0	2.0	If this is the case, then the WordNet sense numbers are used to rank the synsets.	0
1983	13782	13782	S15-13	Baseline	8	66	2.0	2.0	The second case is when only one of the Babel synsets contains a WordNet synset: in this case the Babel synset that contains the WordNet synset gets ranked first.	0
1984	13783	13783	S15-13	Baseline	9	67	2.0	2.0	The last case is when no WordNet synsets are contained within the two Babel synsets.	0
1985	13784	13784	S15-13	Baseline	10	68	2.0	2.0	In this case a lexicographic ordering of the Wikipedia pages contained within the Babel synsets is taken into account.	0
1986	13785	13785	S15-13	Baseline	11	69	2.0	2.0	As is well known, the first sense heuristic based on Word-Net has always proved a really hard to beat baseline, outperforming all the developed systems for the English language over almost all settings and system combinations.	0
1987	13786	13786	S15-13	Baseline	12	70	3.0	2.0	In contrast, the BFS heuristic in the other languages shows itself to be weaker, achieving lower performances in almost all settings and system combinations.	0
1988	13787	13787	S15-13	Baseline	13	71	3.0	2.0	3 Participating Systems DFKI (Supervised).	0
1989	13788	13788	S15-13	Baseline	14	72	3.0	2.0	This system exploits Babel-Net as reference inventory and a CRF-based named entity recognizer.	0
1990	13789	13789	S15-13	Baseline	15	73	3.0	2.0	The disambiguation system is divided in two parts: one for nouns and another for verbs.	0
1991	13790	13790	S15-13	Baseline	16	74	3.0	2.0	For nouns the approach is based on the idea of maximizing multiple objectives at the same time.	0
1992	13791	13791	S15-13	Baseline	17	75	3.0	2.0	Similarly to (Hoffart et al., 2011), the disambiguation objectives consist of a global (coherence, unsupervised) part and a local (supervised) part.	0
1993	13792	13792	S15-13	Baseline	18	76	4.0	2.0	The global objective makes sure that disambiguation maximizes coherence of the selected synsets and it is based on the semantic signature graph (Moro et al., 2014b).	0
1994	13793	13793	S15-13	Baseline	19	77	4.0	2.0	The local objective ensures that the Word-Net synset type fits the local context of the noun to be disambiguated.	0
1995	13794	13794	S15-13	Baseline	20	78	4.0	2.0	One important aspect of this approach is that, unlike previous work (Hoffart et al., 2011;	0
1996	13795	13795	S15-13	Baseline	21	79	4.0	2.0	Moro et al., 2014b), it does not apply discrete optimization, but continuous optimization on the normalized sum of all objectives.	0
1997	13796	13796	S15-13	Baseline	22	80	4.0	2.0	The disambiguation procedure aims to optimize the objective function by iteratively updating the candidate probabilities for each fragment.	0
1998	13797	13797	S15-13	Baseline	23	81	4.0	2.0	As far as verbs are concerned, a feed-forward neural network is trained using local features such as arguments of the semantic roles of a verb in a sentence, context words, and the verb and its lemma.	0
1999	13798	13798	S15-13	EBL-Hope (Unsupervised + Sense relevance).	1	82	1.0	2.0	This approach uses a modified version of the Lesk algorithm and the Jiang &amp; Conrath similarity measure (Jiang and Conrath, 1997).	0
2000	13799	13799	S15-13	EBL-Hope (Unsupervised + Sense relevance).	2	83	1.0	2.0	It validates the output from both techniques for enhanced accuracy and exploits semantic relations and corpus (SemCor) in-formation available in BabelNet and WordNet in an unsupervised manner.	0
2001	13800	13800	S15-13	EBL-Hope (Unsupervised + Sense relevance).	3	84	1.0	2.0	el92 (Systems mix).	0
2002	13801	13801	S15-13	EBL-Hope (Unsupervised + Sense relevance).	4	85	1.0	2.0	This system is a generaldomain system for entity detection and linking.	0
2003	13802	13802	S15-13	EBL-Hope (Unsupervised + Sense relevance).	5	86	1.0	3.0	It does not perform WSD.	0
2004	13803	13803	S15-13	EBL-Hope (Unsupervised + Sense relevance).	6	87	1.0	3.0	The system combines, via a weighted voting, Entity Linking outputs from four publicly available services: Tagme (Ferragina and Scaiella, 2010), DBpedia Spotlight (Mendes et al., 2011), Wikipedia Miner (Milne and Witten, 2008) and Babelfy (Moro et al., 2014b;	0
2005	13804	13804	S15-13	EBL-Hope (Unsupervised + Sense relevance).	7	88	1.0	3.0	Moro et al., 2014a).	0
2006	13805	13805	S15-13	EBL-Hope (Unsupervised + Sense relevance).	8	89	1.0	3.0	The different runs correspond to different settings in the weighting formula (De La Clergerie et al., 2008;	0
2007	13806	13806	S15-13	EBL-Hope (Unsupervised + Sense relevance).	9	90	1.0	3.0	Fiscus, 1997).	0
2008	13807	13807	S15-13	EBL-Hope (Unsupervised + Sense relevance).	10	91	1.0	3.0	LIMSI (Unsupervised + Sense relevance).	0
2009	13808	13808	S15-13	EBL-Hope (Unsupervised + Sense relevance).	11	92	2.0	3.0	The system performs WSD by taking advantage of the parallelism of the test data, a feature that was not exploited by the systems that participated in the SemEval-2013 Multilingual Word Sense Disambiguation task 12 .	0
2010	13809	13809	S15-13	EBL-Hope (Unsupervised + Sense relevance).	12	93	2.0	3.0	The system needs no training and is applied directly to the test dataset, nor does it use distributional (context) information.	0
2011	13810	13810	S15-13	EBL-Hope (Unsupervised + Sense relevance).	13	94	2.0	3.0	The texts are sentence-and wordaligned pairwise, and content words are tagged by their translations in another language.	0
2012	13811	13811	S15-13	EBL-Hope (Unsupervised + Sense relevance).	14	95	2.0	3.0	The alignments serve to retrieve the BabelNet synsets that are relevant for each instance of a word in the texts (i.e., synsets that contain both the disambiguation target and its aligned translation).	0
2013	13812	13812	S15-13	EBL-Hope (Unsupervised + Sense relevance).	15	96	2.0	3.0	If a Babel synset is retained, this is used to annotate the instance of the word in the test set.	0
2014	13813	13813	S15-13	EBL-Hope (Unsupervised + Sense relevance).	16	97	2.0	3.0	If more than one synset is retained, these are ranked using the BabelSynset-Comparator Java class available in the BabelNet API (please refer to Section 2.5 for a detailed explanation).	0
2015	13814	13814	S15-13	EBL-Hope (Unsupervised + Sense relevance).	17	98	2.0	3.0	The highest ranked synset among the ones that contain the aligned translation is used to annotate the instance.	0
2016	13815	13815	S15-13	EBL-Hope (Unsupervised + Sense relevance).	18	99	2.0	3.0	The system falls back to the BabelNet first sense (BFS) provided by the BabelSynset	0
2017	13816	13816	S15-13	EBL-Hope (Unsupervised + Sense relevance).	19	100	2.0	3.0	Comparator for instances with no aligned translation, or in cases where the translation was not found in any of the synsets available for the word in BabelNet.	0
2018	13817	13817	S15-13	EBL-Hope (Unsupervised + Sense relevance).	20	101	2.0	3.0	SUDOKU (Unsupervised).	0
2019	13818	13818	S15-13	EBL-Hope (Unsupervised + Sense relevance).	21	102	2.0	3.0	"This deterministic constraint-based approach relies on a reasonable degree of ""document monosemy"" (percentage of unique monosemous lemmas in a document) and exploits Personalised PageRank (Agirre et al., 2014) to select the best candidate."	0
2020	13819	13819	S15-13	EBL-Hope (Unsupervised + Sense relevance).	22	103	3.0	3.0	The PPR is started with a surfing vector biased towards monosemous words (i.e., their respective sense).	0
2021	13820	13820	S15-13	EBL-Hope (Unsupervised + Sense relevance).	23	104	3.0	3.0	Each submission differs by its imposed constraints: Run1 is the plain approach (Manion and Sainudiin, 2014) applied at the document level; Run2 is the iterative version of the previous approach applied at the document level and with words disambiguated in order of increasing polysemy; Run3 is like Run2, but it is first applied to nouns and then to verbs, adjectives, and adverbs.	0
2022	13821	13821	S15-13	EBL-Hope (Unsupervised + Sense relevance).	24	105	3.0	3.0	TeamUFAL (Unsupervised).	0
2023	13822	13822	S15-13	EBL-Hope (Unsupervised + Sense relevance).	25	106	3.0	3.0	This system exploits Apache Lucene search engine to index Wikipedia documents, Wiktionary entries and WordNet senses.	0
2024	13823	13823	S15-13	EBL-Hope (Unsupervised + Sense relevance).	26	107	3.0	3.0	Then, to perform disambiguation, the Lucene ranking method is used to query the index with multiple queries (consisting of the text fragment and context words).	0
2025	13824	13824	S15-13	EBL-Hope (Unsupervised + Sense relevance).	27	108	3.0	3.0	Finally, all query results are merged and the disambiguated meaning is selected thanks to a simple threshold heuristic.	0
2026	13825	13825	S15-13	EBL-Hope (Unsupervised + Sense relevance).	28	109	3.0	3.0	UNIBA (Unsupervised + Sense relevance).	0
2027	13826	13826	S15-13	EBL-Hope (Unsupervised + Sense relevance).	29	110	3.0	3.0	This system 3 extends two well-known variations of the Lesk WSD method.	0
2028	13827	13827	S15-13	EBL-Hope (Unsupervised + Sense relevance).	30	111	3.0	3.0	The main contribution of the approach relies on the use of a word similarity function defined on a distributional semantic space (Word2vec tool (Mikolov et al., 2013)) to compute the gloss-context overlap.	0
2029	13828	13828	S15-13	EBL-Hope (Unsupervised + Sense relevance).	31	112	3.0	3.0	Entities are identified by exploiting a list of possible surface forms extracted from BabelNet synsets.	0
2030	13829	13829	S15-13	EBL-Hope (Unsupervised + Sense relevance).	32	113	3.0	3.0	Moreover, each synset has a prior probability computed over an annotated corpus.	0
2031	13830	13830	S15-13	EBL-Hope (Unsupervised + Sense relevance).	33	114	4.0	3.0	For Word	0
2032	13831	13831	S15-13	EBL-Hope (Unsupervised + Sense relevance).	34	115	4.0	3.0	Net synsets, SemCor is exploited, while for Wikipedia entities the number of citations in Wikipedia internal links is counted.	0
2033	13832	13832	S15-13	EBL-Hope (Unsupervised + Sense relevance).	35	116	4.0	3.0	vua-background (Partially supervised).	0
2034	13833	13833	S15-13	EBL-Hope (Unsupervised + Sense relevance).	36	117	4.0	3.0	This approach exploits the Named Entities contained in the test data to generate a background corpus.	0
2035	13834	13834	S15-13	EBL-Hope (Unsupervised + Sense relevance).	37	118	4.0	3.0	This is done by finding similar DBpedia entities for the entities in the input documents.	0
2036	13835	13835	S15-13	EBL-Hope (Unsupervised + Sense relevance).	38	119	4.0	3.0	Using this background corpus, the system tries to find the predominant sense of the words in the test data (McCarthy et al., 2004).	0
2037	13836	13836	S15-13	EBL-Hope (Unsupervised + Sense relevance).	39	120	4.0	3.0	"If a predominant sense is recognized for a specific lemma, then it is used, otherwise the system falls back to the ""It Makes Sense"" WSD system (Zhong and Ng, 2010)."	0
2038	13837	13837	S15-13	EBL-Hope (Unsupervised + Sense relevance).	40	121	4.0	3.0	WSD-games (Unsupervised).	0
2039	13838	13838	S15-13	EBL-Hope (Unsupervised + Sense relevance).	41	122	4.0	3.0	This approach is formulated in terms of Evolutionary Game Theory, where each word to be disambiguated is represented as a node in a graph and each sense as a class.	0
2040	13839	13839	S15-13	EBL-Hope (Unsupervised + Sense relevance).	42	123	4.0	3.0	The proposed algorithm performs a consistent class assignment of senses according to the similarity information of each word with the others, so that similar words are constrained to similar classes.	0
2041	13840	13840	S15-13	EBL-Hope (Unsupervised + Sense relevance).	43	124	4.0	3.0	The propagation of the information over the graph is formulated in terms of a non-cooperative multi-player game, where the players are the data points, in order to decide their class memberships, and equilibria correspond to consistent labeling of the data.	0
2042	13841	13841	S15-13	Results and Discussion	1	125	1.0	3.0	The results obtained by the participating systems are shown in Tables 2-6.	0
2043	13842	13842	S15-13	Results and Discussion	2	126	2.0	3.0	In Table 2 we show the precision, recall and F1 scores of the participating systems that annotated all classes of items (named entities, nouns, verbs, adverbs, adjectives) over the whole dataset.	0
2044	13843	13843	S15-13	Results and Discussion	3	127	3.0	3.0	Six out of the nine participating teams annotated the full set of items.	0
2045	13844	13844	S15-13	Results and Discussion	4	128	4.0	4.0	We also show the F1 performance on each considered domain independently and for different kinds of subsets of the item classes (i.e., we show the F1 score over all items, then only on named entities, all open-class word senses and individually).	0
2046	13845	13845	S15-13	Overall Performance	1	129	1.0	4.0	From Table 2 we can see that the best system for English (i.e., LIMSI) is able to obtain a performance more than five percentage points higher than the second ranked system.	0
2047	13846	13846	S15-13	Overall Performance	2	130	1.0	4.0	This is due to the goodquality indirect supervision provided by the alignments combined with the use of the BabelSynset-Comparator.	0
2048	13847	13847	S15-13	Overall Performance	3	131	2.0	4.0	However, on the other two languages this system obtains lower performance than the other competing systems.	0
2049	13848	13848	S15-13	Overall Performance	4	132	2.0	4.0	The performance of the SU-DOKU system is of a particular interest, as it obtains the second best scores on the English part of the dataset and the top scores overall on the other two languages.	0
2050	13849	13849	S15-13	Overall Performance	5	133	2.0	4.0	It exploits monosemous words within the input documents to run Personalized PageRank.	0
2051	13850	13850	S15-13	Overall Performance	6	134	3.0	4.0	The three runs differ mainly in respect of the order in which the words get disambiguated.	0
2052	13851	13851	S15-13	Overall Performance	7	135	3.0	4.0	In Table 3 we show the F1 scores of all the systems over the whole dataset for each class of the  manually annotated items and for each language.	0
2053	13852	13852	S15-13	Overall Performance	8	136	4.0	4.0	In the English part of the datasets the DFKI system performs best for verb, noun and named entity disambiguation, thanks to precomputed random walks called semantic signatures, along the lines of Babelfy (Moro et al., 2014b), and supervised techniques.	0
2054	13853	13853	S15-13	Overall Performance	9	137	4.0	4.0	The UNIBA system on the English dataset obtains the best result on adverbs.	0
2055	13854	13854	S15-13	Overall Performance	10	138	4.0	4.0	Finally, in the Spanish dataset the EBL-Hope system based on a combination of a Lesk-based measure together with the Jiang &amp; Conrath similarity measure shows the best performance for named entities.	0
2056	13855	13855	S15-13	Domain-based Evaluation	1	139	2.0	4.0	In Tables 4-6 we show the detailed performances of all the systems over different classes of items, and on different domains.	0
2057	13856	13856	S15-13	Domain-based Evaluation	2	140	3.0	4.0	One of the main goals of this task is to investigate the performance of disambiguation methods over different domains.	0
2058	13857	13857	S15-13	Domain-based Evaluation	3	141	4.0	4.0	Our documents derive from the biomedical domain, the maths and computer domain, and a broader domain (a document discussing social issues, especially for elderly workers and possible solutions).	0
2059	13858	13858	S15-13	Biomedical domain.	1	142	1.0	4.0	In Table 4 we show the performance of the systems on the biomedical documents.	0
2060	13859	13859	S15-13	Biomedical domain.	2	143	1.0	4.0	The first thing to notice is the much higher best score of the first ranked system (i.e., LIMSI), which attains an F1 score of 71.3%.	0
2061	13860	13860	S15-13	Biomedical domain.	3	144	1.0	4.0	This is due to the lower ambiguity of nouns and named entities (see Table 1) resulting from the greater numbers of domain-specific concepts used within this kind of documents.	0
2062	13861	13861	S15-13	Biomedical domain.	4	145	1.0	4.0	This can also be seen from the higher scores obtained by the BFS.	0
2063	13862	13862	S15-13	Biomedical domain.	5	146	1.0	4.0	Overall, all systems obtained a better performance than in the other domains, with a gain of more than four percentage points each.	0
2064	13863	13863	S15-13	Biomedical domain.	6	147	2.0	4.0	The second ranked system (i.e., SUDOKU) shows its ability to exploit monosemous words obtaining a 0.1 difference from the first ranked system and a 0.9 point distance from the BFS baseline.	0
2065	13864	13864	S15-13	Biomedical domain.	7	148	2.0	4.0	This is of particular interest as the system does not explicitly exploit any sense relevance information.	0
2066	13865	13865	S15-13	Biomedical domain.	8	149	2.0	4.0	Moreover, the DFKI system obtains the best scores for nouns and verbs, and is the only system able to obtain a 100% F1 score on NE disambiguation.	0
2067	13866	13866	S15-13	Biomedical domain.	9	150	2.0	4.0	However, several other systems performed above 90%, showing that in this particular set of documents named entities are easy to disambiguate.	0
2068	13867	13867	S15-13	Biomedical domain.	10	151	2.0	4.0	On the other two languages the performances are a little bit lower, but the SUDOKU system confirms its ability to exploit monosemous words at a quality comparable to the one obtained in the English dataset.	0
2069	13868	13868	S15-13	Biomedical domain.	11	152	3.0	4.0	The LIMSI system, instead, obtains a reduction of around 20% due to its exploitation of the BabelSynsetComparator, which performs badly in these languages (see the BFS scores).	0
2070	13869	13869	S15-13	Biomedical domain.	12	153	3.0	4.0	Maths and computer domain.	0
2071	13870	13870	S15-13	Biomedical domain.	13	154	3.0	4.0	In Table 5 we show the results for the maths and computer domain.	0
2072	13871	13871	S15-13	Biomedical domain.	14	155	3.0	4.0	As can be seen in Table 1, this is the most ambiguous domain and the best systems obtain much lower performances than in the other domains.	0
2073	13872	13872	S15-13	Biomedical domain.	15	156	3.0	4.0	Interestingly, the DFKI system is not able to achieve the best performance on any of the considered item classes, while UNIBA and SUDOKU show the best results for nouns and verbs.	0
2074	13873	13873	S15-13	Biomedical domain.	16	157	4.0	4.0	As regards named en-  tities, the system EBL-Hope obtains the best results in all languages.	0
2075	13874	13874	S15-13	Biomedical domain.	17	158	4.0	4.0	This system, in addition to exploiting a Lesk-based measure combined with the Jiang &amp; Conrath similarity measure, uses the BabelNet semantic relations, which have already been shown to be useful for attaining state-of-the-art performances in EL (Moro et al., 2014b).	0
2076	13875	13875	S15-13	Biomedical domain.	18	159	4.0	4.0	Interestingly, in the Italian dataset the system UNIBA (which is based on an extended version of the Lesk measure and a semantic relatedness measure) obtains the same performance for NE as the EBL-Hope system.	0
2077	13876	13876	S15-13	Biomedical domain.	19	160	4.0	4.0	"the predominant sense algorithm (McCarthy et al., 2004) and, as a fallback routine, on the ""It Makes Sense"" supervised WSD system (Zhong and Ng, 2010)."	0
2078	13877	13877	S15-13	Biomedical domain.	20	161	4.0	4.0	For the other two languages the SUDOKU system obtains the best scores, with the exception of adverbs in the Italian dataset where the UNIBA system is able to reach an F1 score of 100%.	0
2079	13878	13878	S15-13	Social issues domain.	1	162	1.0	4.0	In	0
2080	13879	13879	S15-13	Social issues domain.	2	163	1.0	4.0	Conclusion and Future Directions	0
2081	13880	13880	S15-13	Social issues domain.	3	164	2.0	4.0	In  disambiguation, and Lesk-based measures for verb, adjective and adverb disambiguation.	0
2082	13881	13881	S15-13	Social issues domain.	4	165	2.0	4.0	Another interesting outcome that emerges from this task is that supervised approaches are difficult to generalize in a multilingual setting.	0
2083	13882	13882	S15-13	Social issues domain.	5	166	3.0	4.0	In fact, the supervised systems that participated in this task took into account only the English language.	0
2084	13883	13883	S15-13	Social issues domain.	6	167	3.0	4.0	Moreover, the task confirms yet again that the WordNet first sense heuristic is a hard baseline to beat.	0
2085	13884	13884	S15-13	Social issues domain.	7	168	4.0	4.0	Unfortunately, no domainspecific disambiguation system participated in the task.	0
2086	13885	13885	S15-13	Social issues domain.	8	169	4.0	4.0	However, in the biomedical domain, the participating systems show higher quality performances than in the other considered domains.	0
2087	13886	13886	S15-13	Social issues domain.	9	170	4.0	4.0	As future directions, we would like to continue to investigate the nature of this novel joint task, and to concentrate on the differences between named entity  disambiguation and word sense disambiguation with a special focus on non-European languages.	0
2088	23177	23177	S19-2	title	1	1	4.0	1.0	SemEval-2019 Task 2: Unsupervised Lexical Frame Induction	0
2089	23178	23178	S19-2	abstract	1	2	1.0	1.0	This paper presents Unsupervised Lexical	0
2090	23179	23179	S19-2	abstract	2	3	2.0	1.0	Frame Induction, Task 2 of the International Workshop on Semantic Evaluation in 2019.	0
2091	23180	23180	S19-2	abstract	3	4	2.0	1.0	Given a set of prespecified syntactic forms in context, the task requires that verbs and their arguments be clustered to resemble semantic frame structures.	1
2092	23181	23181	S19-2	abstract	4	5	3.0	1.0	Results are useful in identifying polysemous words, i.e., those whose frame structures are not easily distinguished, as well as discerning semantic relations of the arguments.	0
2093	23182	23182	S19-2	abstract	5	6	3.0	1.0	Evaluation of unsupervised frame induction methods fell into two tracks: Task A) Verb Clustering based on FrameNet 1.7; and B) Argument Clustering, with B.1) based on FrameNet's core frame elements, and B.2) on VerbNet 3.2 semantic roles.	0
2094	23183	23183	S19-2	abstract	6	7	4.0	1.0	The shared task attracted nine teams, of whom three reported promising results.	0
2095	23184	23184	S19-2	abstract	7	8	4.0	1.0	This paper describes the task and its data, reports on methods and resources that these systems used, and offers a comparison to human annotation.	0
2096	23185	23185	S19-2	Introduction	1	9	1.0	1.0	SemEval 2019	0
2097	23186	23186	S19-2	Introduction	2	10	1.0	1.0	Task 2 focused on the unsupervised semantic labeling of a set of prespecified (semantically) unlabeled structures (Figure 1).	0
2098	23187	23187	S19-2	Introduction	3	11	1.0	1.0	Unsupervised learning methods analyze these structures (Figure 1a) to augment them with semantic labels (Figure 1b).	0
2099	23188	23188	S19-2	Introduction	4	12	1.0	1.0	The shape of the manually labeled input frames is constrained to an acyclic connected tree of lexical items (words and multi-word units) of maximum depth 1, where just one root governs several arguments.	0
2100	23189	23189	S19-2	Introduction	5	13	2.0	1.0	The task used Berkeley FrameNet (FN) (Ruppenhofer et al., 2016) and Q. Zadeh and Petruck (2019), guidelines for this task, to determine the arguments and label them with semantic information.	0
2101	23190	23190	S19-2	Introduction	6	14	2.0	1.0	We compared the proposed system results for unsupervised semantic tagging with that of human annotated (or, gold-standard) data in three different subtasks (Figure 2).	0
2102	23191	23191	S19-2	Introduction	7	15	2.0	1.0	To evaluate the systems, we computed distributional similarities between  their generated unsupervised labeled data and human annotated reference data.	0
2103	23192	23192	S19-2	Introduction	8	16	2.0	1.0	For computing similarities we used general purpose numeral methods of text clustering, in particular BCUBED F-SCORE (Bagga and Baldwin, 1998) as the single figure of merit to rank the systems.	0
2104	23193	23193	S19-2	Introduction	9	17	3.0	1.0	The most important result of the shared task is the creation of a benchmark for a future complex task.	0
2105	23194	23194	S19-2	Introduction	10	18	3.0	1.0	This benchmark includes a moderately sized, manually annotated set of frames, where only the verbs of each were included, along with their core frame elements (which uniquely define a frame as Ruppenhofer et al. describe).	0
2106	23195	23195	S19-2	Introduction	11	19	3.0	1.0	To complement FN's core frame elements that have highly specific meanings, the benchmark also includes the annotated argument structures of the verbs based on the generic semantic roles proposed for verb classes in VerbNet 3.2 (Kipper et al., 2000;	0
2107	23196	23196	S19-2	Introduction	12	20	3.0	1.0	Palmer et al., 2017).	0
2108	23197	23197	S19-2	Introduction	13	21	4.0	1.0	The benchmark comes with simplified annotation guidelines and a modular annotation sys-tem with browsing and editing capabilities.	0
2109	23198	23198	S19-2	Introduction	14	22	4.0	1.0	1 Complementing the benchmarking are several state-ofthe-art competing baselines, from the participants, that serve as a point of departure for improvements in the future.	0
2110	23199	23199	S19-2	Introduction	15	23	4.0	1.0	2	0
2111	23200	23200	S19-2	Introduction	16	24	4.0	1.0	The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks.	0
2112	23201	23201	S19-2	Background	1	25	1.0	1.0	Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structures have developed in parallel over several decades in theoretical linguistic studies about the syntax-semantics interface, as well as in empirical corpus-driven applications in natural language processing.	0
2113	23202	23202	S19-2	Background	2	26	1.0	1.0	Building repositories of (lexical) semantic frames is a core component in all of these efforts.	0
2114	23203	23203	S19-2	Background	3	27	1.0	1.0	In formal studies, lexical semantic frame knowledge bases instantiate foundational theories with tangible examples, e.g., to provide supporting evidence for the theory.	0
2115	23204	23204	S19-2	Background	4	28	1.0	1.0	Practically, frame semantic repositories play a pivotal role in natural language understanding and semantic parsing, both as inspiration for a representation format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others.	0
2116	23205	23205	S19-2	Background	5	29	2.0	1.0	However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks.	0
2117	23206	23206	S19-2	Background	6	30	2.0	1.0	The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English.	0
2118	23207	23207	S19-2	Background	7	31	2.0	1.0	While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at best.	0
2119	23208	23208	S19-2	Background	8	32	2.0	1.0	This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017).	0
2120	23209	23209	S19-2	Background	9	33	2.0	1.0	Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies.	0
2121	23210	23210	S19-2	Background	10	34	3.0	1.0	Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet.	0
2122	23211	23211	S19-2	Background	11	35	3.0	1.0	Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information.	0
2123	23212	23212	S19-2	Background	12	36	3.0	1.0	Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments.	0
2124	23213	23213	S19-2	Background	13	37	3.0	1.0	Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)).	0
2125	23214	23214	S19-2	Background	14	38	4.0	1.0	This task aimed to benchmark a class of such unsupervised frame induction methods.	0
2126	23215	23215	S19-2	Background	15	39	4.0	1.0	The ambitious goal of this task was the unsupervised induction of frame semantic structures from tokenized and morphosyntacally labeled text corpora.	0
2127	23216	23216	S19-2	Background	16	40	4.0	1.0	We sought to achieve this goal by building an evaluation benchmark for three tasks.	0
2128	23217	23217	S19-2	Background	17	41	4.0	1.0	Task A dealt with unsupervised labeling of verb lemmas with their frame meaning.	0
2129	23218	23218	S19-2	Background	18	42	4.0	1.0	Task B involved unsupervised argument role labeling, where B.1 benchmarked unsupervised labeling of frame-specific frame elements (FEs) based on FN, and B.2 benchmarked unsupervised role labeling of arguments in Case Grammar terms (Fillmore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet.	0
2130	23219	23219	S19-2	Task Description	1	43	1.0	1.0	The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation).	0
2131	23220	23220	S19-2	Task Description	2	44	2.0	1.0	Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information.	0
2132	23221	23221	S19-2	Task Description	3	45	3.0	1.0	Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels.	0
2133	23222	23222	S19-2	Task Description	4	46	4.0	1.0	For development purposes, developers received a small labeled development set.	0
2134	23223	23223	S19-2	Task A: Clustering Verbs	1	47	1.0	1.0	The goal of this task was to identify verbs that evoke the same frame.	0
2135	23224	23224	S19-2	Task A: Clustering Verbs	2	48	1.0	1.0	The task involved labeling verb uses in context to resemble their categorization based on Frame Semantics (Figure 2a).	0
2136	23225	23225	S19-2	Task A: Clustering Verbs	3	49	1.0	1.0	Here, we used FN 1.7 as the reference for frame definitions.	0
2137	23226	23226	S19-2	Task A: Clustering Verbs	4	50	1.0	1.0	Hence, the task constituted the unsupervised induction of FN's lexical units, where a lexical unit (LU) is a pairing of a lemma and a frame.	0
2138	23227	23227	S19-2	Task A: Clustering Verbs	5	51	1.0	1.0	For example, we expected that the LUs auction.v, retail.v, sell.v, etc., which evoke the typed situation of COMMERCE SELL, be labeled with the same unsupervised tag.	0
2139	23228	23228	S19-2	Task A: Clustering Verbs	6	52	2.0	1.0	3	0
2140	23229	23229	S19-2	Task A: Clustering Verbs	7	53	2.0	1.0	The task resembles word sense induction in that it assigns a class (or sense) label to a verb.	0
2141	23230	23230	S19-2	Task A: Clustering Verbs	8	54	2.0	1.0	In word sense induction (WSI), labels are determined and evaluated on word forms (lemma + part-ofspeech e.g., sell.v or auction.n).	0
2142	23231	23231	S19-2	Task A: Clustering Verbs	9	55	2.0	1.0	WSI evaluations assume that the inventory of senses (set S i s) for different word forms f is devised independently.	0
2143	23232	23232	S19-2	Task A: Clustering Verbs	10	56	2.0	1.0	For instance, assuming f 1 is labeled with the set of senses S 1 and f 2 with S 2 , then S 1 ∩ S 2 = φ only if f 1 = f 2 ; and, if f 1 = f 2 then S 1 ∩ S 2 = φ (as in other SemEval benchmarks, including Agirre and Soroa (2007); Manandhar et al. (2010);	0
2144	23233	23233	S19-2	Task A: Clustering Verbs	11	57	3.0	1.0	3 Dark red small caps indicate FN frames.	0
2145	23234	23234	S19-2	Task A: Clustering Verbs	12	58	3.0	1.0	Jurgens and Klapaftis (2013); Navigli and Vannella (2013)).	0
2146	23235	23235	S19-2	Task A: Clustering Verbs	13	59	3.0	1.0	For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from S sell are assigned to the lemma sell.v, and one label s is assigned to auction.v, knowing that s / ∈ S sell .	0
2147	23236	23236	S19-2	Task A: Clustering Verbs	14	60	3.0	1.0	Typically, lexical semantic relationships among members of S i s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)).	0
2148	23237	23237	S19-2	Task A: Clustering Verbs	15	61	3.0	1.0	In contrast, this task assumes that the sense inventory is defined independent of word forms.	0
2149	23238	23238	S19-2	Task A: Clustering Verbs	16	62	4.0	1.0	This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., f i = f j ) can be mapped to the same meaning (label), and the same meaning (label) can be mapped to several word forms.	0
2150	23239	23239	S19-2	Task A: Clustering Verbs	17	63	4.0	1.0	We defined S with respect to FrameNet and assumed that its typed-situation frames are units of meaning.	0
2151	23240	23240	S19-2	Task A: Clustering Verbs	18	64	4.0	1.0	So, COMMERCE SELL captures the meaning associated with both sell.	0
2152	23241	23241	S19-2	Task A: Clustering Verbs	19	65	4.0	1.0	v and auction.v., as well as other selling-related words.	0
2153	23242	23242	S19-2	Task A: Clustering Verbs	20	66	4.0	1.0	Hence, in some sense, Task A goes beyond the ordinary WSI task as it also demands identifying (unspecified) lexical semantic relationships between verbs.	0
2154	23243	23243	S19-2	Task B.1: Unsupervised Frame Semantic	1	67	1.0	2.0	Argument Labeling	0
2155	23244	23244	S19-2	Task B.1: Unsupervised Frame Semantic	2	68	2.0	2.0	Taking the frames as primary and defining roles relative to each frame, the aim of Task B.1 was to cluster prespecified verb-headed argument structures according to the principles of Frame Semantics, where FrameNet served as the reference for evaluation.	0
2156	23245	23245	S19-2	Task B.1: Unsupervised Frame Semantic	3	69	2.0	2.0	This task amounted to unsupervised labeling of frames and core FEs (Figure 2b).	0
2157	23246	23246	S19-2	Task B.1: Unsupervised Frame Semantic	4	70	3.0	2.0	Because Frame	0
2158	23247	23247	S19-2	Task B.1: Unsupervised Frame Semantic	5	71	3.0	2.0	Net defines FEs frame-specifically, Task B.1 entails Task A. Given a set of semantically-unlabelled arguments as input (e.g., Figure 1a), the root nodes (i.e., verbs) are clustered and assigned to a set of unsupervised frame labels π i (1 ≤ i ≤ n, where n is the number of latent frames).	0
2159	23248	23248	S19-2	Task B.1: Unsupervised Frame Semantic	6	72	4.0	2.0	Then, the arguments are labeled with semantic role labels (FEs) interpreted locally given the frame.	0
2160	23249	23249	S19-2	Task B.1: Unsupervised Frame Semantic	7	73	4.0	2.0	That is, for any pair of π x and π y , the set of assigned roles R x to arguments under π x are assumed to be independent from R y labels for π y (R x ∩ R y = φ).	0
2161	23250	23250	S19-2	Task B.2: Unsupervised Case Role Labeling	1	74	1.0	2.0	We defined Subtask B.2 in parallel to Subtask B.1 and involved an idea from Case Grammar.	0
2162	23251	23251	S19-2	Task B.2: Unsupervised Case Role Labeling	2	75	2.0	2.0	The ar-guments of a verb in a set of prespecified subcategorization frames were clustered according to a common set of generic semantic roles (Figure 2c).	0
2163	23252	23252	S19-2	Task B.2: Unsupervised Case Role Labeling	3	76	2.0	2.0	Here, the task assumed that semantic roles are universal and generic (e.g., Agent, Patient).	0
2164	23253	23253	S19-2	Task B.2: Unsupervised Case Role Labeling	4	77	3.0	2.0	Their configuration determines the argument structure of verb-headed phrases.	0
2165	23254	23254	S19-2	Task B.2: Unsupervised Case Role Labeling	5	78	3.0	2.0	We evaluated this unsupervised labeling of arguments with semantic roles independently of the class, sense, and word form of a verb.	0
2166	23255	23255	S19-2	Task B.2: Unsupervised Case Role Labeling	6	79	4.0	2.0	We compared the role labels against a set of semantic roles from VerbNet 3.2 (Kipper et al., 2000).	0
2167	23256	23256	S19-2	Task B.2: Unsupervised Case Role Labeling	7	80	4.0	2.0	Given a verb instance, no guarantee exists that input argument structures for B.2 and B.1 would be the same.	0
2168	23257	23257	S19-2	Evaluation Dataset	1	81	1.0	2.0	The dataset consists of manual annotations for verb-headed frame structures anchored in tokenized sentences.	0
2169	23258	23258	S19-2	Evaluation Dataset	2	82	2.0	2.0	These frame structures were manually annotated using the guidelines for this task (Q. Zadeh and Petruck, 2019).	0
2170	23259	23259	S19-2	Evaluation Dataset	3	83	2.0	2.0	For example, as already illustrated, the verb come from.	0
2171	23260	23260	S19-2	Evaluation Dataset	4	84	3.0	2.0	v is annotated in terms of FN's ORIGIN frame and its core FEs, as Example 1 shows.	0
2172	23261	23261	S19-2	Evaluation Dataset	5	85	4.0	2.0	(1)	0
2173	23262	23262	S19-2	Evaluation Dataset	6	86	4.0	2.0	Criticism of futures COMES FROM Wall Street.	0
2174	23263	23263	S19-2	Criticism come from Wall Street	1	87	2.0	2.0	ORIGIN ENTITY ORIGIN	0
2175	23264	23264	S19-2	Criticism come from Wall Street	2	88	4.0	2.0	Also, using the set of 32 generic semantic role labels in VerbNet 3.2 and two additional roles, COG-NIZER and CONTENT, we annotated arguments of the verb as the following graphic shows.	0
2176	23265	23265	S19-2	Criticism come from Wall Street THEME SOURCE	1	89	1.0	2.0	We assumed unique identifiers for sentences, e.g., #s1 for Example 1.	0
2177	23266	23266	S19-2	Criticism come from Wall Street THEME SOURCE	2	90	2.0	2.0	The evaluation record for come from.v (Task A) appears below, where #s1 4 5 specifies the position of the verb in the sentence (Example 1).	0
2178	23267	23267	S19-2	Criticism come from Wall Street THEME SOURCE	3	91	3.0	2.0	We stripped off the manually asserted labels from the records and passed them to systems for assigning unsupervised labels.	0
2179	23268	23268	S19-2	Criticism come from Wall Street THEME SOURCE	4	92	4.0	2.0	Evidently, later a scorer program (Section 5) compared system-generated labels with the manually assigned labels.	0
2180	23269	23269	S19-2	Data Sampling	1	93	1.0	2.0	We sampled data from the Wall Street Journal (WSJ) corpus of the Penn Treebank.	0
2181	23270	23270	S19-2	Data Sampling	2	94	2.0	2.0	Kallmeyer et al. (2018) provided frame annotations similar to those in this task for a portion of WSJ sentences, using SemLink  and EngVallex (Cinková et al., 2014) to generate frame semantic annotations semi-automatically.	0
2182	23271	23271	S19-2	Data Sampling	3	95	3.0	2.0	That work was based on FrameNet and the Prague Dependency Treebank (PSD) (Hajič et al., 2012) from the Broad-coverage Semantic Dependency resource (Oepen et al., 2016).	0
2183	23272	23272	S19-2	Data Sampling	4	96	4.0	2.0	We started by annotating a portion of the records in Kallmeyer et al. (2018), and later deviated from this subset to create a more representative sample of the overall diversity and distribution of verbs in the WSJ corpus using a stratified random sampling method.	0
2184	23273	23273	S19-2	Guidelines	1	97	1.0	2.0	The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks.	0
2185	23274	23274	S19-2	Guidelines	2	98	1.0	2.0	In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013).	0
2186	23275	23275	S19-2	Guidelines	3	99	2.0	2.0	To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE ENTITY.	0
2187	23276	23276	S19-2	Guidelines	4	100	2.0	2.0	We only annotated Criticism, understanding it as the LU that evokes JUDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span.	0
2188	23277	23277	S19-2	Guidelines	5	101	2.0	2.0	Thus, we assumed that another frame f a fills an argument of a frame.	0
2189	23278	23278	S19-2	Guidelines	6	102	3.0	2.0	We annotated only the main content word(s) that evoke(s) f a ; these main words are the semantic heads.	0
2190	23279	23279	S19-2	Guidelines	7	103	3.0	2.0	4 Multi-word unit semantic heads (e.g., named entities, word form combinations) are annotated as if a single word form, such as Wall Street (# 1), excluding modifiers.	0
2191	23280	23280	S19-2	Guidelines	8	104	3.0	2.0	In contrast to semantic depen-dency structures (e.g., DELPH-IN MRS-Derived Semantic Dependencies, Enju Predicate	0
2192	23281	23281	S19-2	Guidelines	9	105	4.0	2.0	Argument Structures, and Tectogramatical Representation in PSD (Oepen et al., 2016)), we did not commit to the underlying syntactic structure of the sentence since we were not obliged to relabel only syntactic structures.	0
2193	23282	23282	S19-2	Guidelines	10	106	4.0	2.0	Rather, we annotated words and MWUs if the frame analysis permitted doing so.	0
2194	23283	23283	S19-2	Guidelines	11	107	4.0	2.0	5	0
2195	23284	23284	S19-2	Annotation Procedure	1	108	1.0	2.0	We annotated the data in a modular manner and in a semi-controlled environment using an annotation system developed for this purpose.	0
2196	23285	23285	S19-2	Annotation Procedure	2	109	1.0	2.0	The procedure consisted of four steps: 1) Reading and Comprehension; 2) Choosing a Frame; 3) Annotating Arguments; and 4) Rating, Commenting, or Revising.	0
2197	23286	23286	S19-2	Annotation Procedure	3	110	1.0	2.0	We tracked and logged all changes in the data as well as annotator interaction with the annotation system upon starting to annotate.	0
2198	23287	23287	S19-2	Annotation Procedure	4	111	1.0	2.0	The tool measured the time that annotators spent on each record and each annotation step, as well as how annotators moved between steps.	0
2199	23288	23288	S19-2	Annotation Procedure	5	112	1.0	2.0	In Step 1, annotators viewed a sentence with one highlighted verb, as in Example 2.	0
2200	23289	23289	S19-2	Annotation Procedure	6	113	1.0	2.0	(2) Criticism of futures COMES from Wall Street.	0
2201	23290	23290	S19-2	Annotation Procedure	7	114	1.0	2.0	The goal of this step was understanding the meaning of the verb and its semantic function, and identifying semantic heads of arguments and their associated words or MWUs.	0
2202	23291	23291	S19-2	Annotation Procedure	8	115	2.0	2.0	To continue, an annotator must confirm the understanding of the verb's meaning of the verb, and can identify its semantic arguments.	0
2203	23292	23292	S19-2	Annotation Procedure	9	116	2.0	2.0	Without confirmation, an annotator would terminate the annotation process for that input sentence and go to the next one.	0
2204	23293	23293	S19-2	Annotation Procedure	10	117	2.0	2.0	If confirmed, Step 2 required the annotator to choose the frame that the verb evoked.	0
2205	23294	23294	S19-2	Annotation Procedure	11	118	2.0	2.0	This step may have included annotating multi-word phrasal verbs, e.g., COMES+FROM (Example 2).	0
2206	23295	23295	S19-2	Annotation Procedure	12	119	2.0	2.0	The annotation system assisted by providing a list of likely frames for the verb, including a LU lookup function (as in FN), an extended set of LUs derived via statistical methods, and previously logged annotations.	0
2207	23296	23296	S19-2	Annotation Procedure	13	120	2.0	2.0	After reviewing the definitions of the proposed frames, annotators chose one, or annotated the verb form with a different existing FN frame.	0
2208	23297	23297	S19-2	Annotation Procedure	14	121	2.0	2.0	"Otherwise, the annotator terminated the process and the record moved to the list of ""skipped items""."	0
2209	23298	23298	S19-2	Annotation Procedure	15	122	2.0	2.0	The annotation of arguments, Step 3, required 5 Q. Zadeh and Petruck describe the issues in detail.	0
2210	23299	23299	S19-2	Annotation Procedure	16	123	3.0	2.0	that annotators label the core FEs of the chosen frame by first identifying their semantic head, which first may have required marking MWUs, e.g., Wall+Street in Example 3, below.	0
2211	23300	23300	S19-2	Annotation Procedure	17	124	3.0	2.0	(3) Criticism of futures comes from Wall Street.	0
2212	23301	23301	S19-2	Annotation Procedure	18	125	3.0	2.0	The tool lists the core FEs and their definitions, and checks the integrity of record annotations to ensure that each core FE is annotated only once.	0
2213	23302	23302	S19-2	Annotation Procedure	19	126	3.0	2.0	In parallel, annotators add the verb's subcategorization frame and its semantic role.	0
2214	23303	23303	S19-2	Annotation Procedure	20	127	3.0	2.0	We did not annotate null instantiated FEs (but FN does).	0
2215	23304	23304	S19-2	Annotation Procedure	21	128	3.0	2.0	During step 3, annotators could go back to the previous step and change their choice of frame type.	0
2216	23305	23305	S19-2	Annotation Procedure	22	129	3.0	2.0	For	0
2217	23306	23306	S19-2	Annotation Procedure	23	130	4.0	2.0	Step 4, annotators rated their annotation, stating their opinion on how well the annotated instance fit FrameNet's definition and how it compared to other annotated instances.	0
2218	23307	23307	S19-2	Annotation Procedure	24	131	4.0	2.0	In a sense, annotators measured their confidence in the assigned labels.	0
2219	23308	23308	S19-2	Annotation Procedure	25	132	4.0	2.0	They did so by selecting a number on a scale from 1 to 5, with 1 not confident at all and 5 the most confident, i.e., the annotation fit perfectly to the chosen FrameNet frame, its definition, and examples.	0
2220	23309	23309	S19-2	Annotation Procedure	26	133	4.0	2.0	Annotators had the option to add free text comments on each record.	0
2221	23310	23310	S19-2	Annotation Procedure	27	134	4.0	3.0	The annotation procedure was rarely straightforward.	0
2222	23311	23311	S19-2	Annotation Procedure	28	135	4.0	3.0	Given the interdependence of Steps 2 and 3, annotators usually moved back and forth between them.	0
2223	23312	23312	S19-2	Annotation Procedure	29	136	4.0	3.0	In Step 2 an annotator might believe that a target verb did not belong in any existing FN frame.	0
2224	23313	23313	S19-2	Annotation Procedure	30	137	4.0	3.0	Likewise, annotators could terminate the annotation process even upon reaching the last step.	0
2225	23314	23314	S19-2	Quality Control	1	138	1.0	3.0	At least two annotators verified all annotation used in the evaluation.	0
2226	23315	23315	S19-2	Quality Control	2	139	1.0	3.0	A main annotator annotated all records in the dataset; two other annotators verified or disputed those annotations.	0
2227	23316	23316	S19-2	Quality Control	3	140	2.0	3.0	If annotators could not reach an agreement, we removed the record from the SemEval dataset.	0
2228	23317	23317	S19-2	Quality Control	4	141	2.0	3.0	A full analysis of annotator disagreement goes beyond the scope of this work.	0
2229	23318	23318	S19-2	Quality Control	5	142	3.0	3.0	While the source of annotator disagreement may seem trivial and simple (e.g., only one annotator understood the sentence correctly), we believe that some sentences may have more than one interpretation, all of which are plausible.	0
2230	23319	23319	S19-2	Quality Control	6	143	3.0	3.0	Like the disagreement resulting from incorrect frame assignment, deciding what frame a verb evokes may be challenging; and resolving the dilemma is not always simple.	0
2231	23320	23320	S19-2	Quality Control	7	144	4.0	3.0	Choosing between two related frames (e.g., BUILDING vs. INTENTIONALLY CREATE, related via Inheritance in FN), or identifying metaphorical and non-metaphorical uses of a verb requires subtle and sophisticated understanding of the semantics of the language, and of Frame Semantics.	0
2232	23321	23321	S19-2	Quality Control	8	145	4.0	3.0	At times, disagreements pointed to more complex linguistic issues that remain in debate, e.g., choosing the semantic head of a syntactically complex argument, treating quantifiers, conjunctions, etc.	0
2233	23322	23322	S19-2	Summary statistics	1	146	1.0	3.0	Table 1 shows a statistical summary of the annotation task.	0
2234	23323	23323	S19-2	Summary statistics	2	147	1.0	3.0	The SemEval column reports the statistics for the final set of records, i.e., gold records with double-agreement between annotators, and which we used to evaluate the systems.	0
2235	23324	23324	S19-2	Summary statistics	3	148	1.0	3.0	Total reports the statistics of all analyzed records, from which we chose our SemEval data.	0
2236	23325	23325	S19-2	Summary statistics	4	149	1.0	3.0	Skipped and InProg show the statistics for discarded records and records without a final decision, respectively.	0
2237	23326	23326	S19-2	Summary statistics	5	150	1.0	3.0	Dev shows the statistics for the development set.	0
2238	23327	23327	S19-2	Summary statistics	6	151	2.0	3.0	Each of the rows reports a value of a component of the data or annotator interaction with the data.	0
2239	23328	23328	S19-2	Summary statistics	7	152	2.0	3.0	Records indicates the number of annotated verbs and their arguments.	0
2240	23329	23329	S19-2	Summary statistics	8	153	2.0	3.0	Sentences and Tokens indicate the size of the sub-corpus of the annotated records.	0
2241	23330	23330	S19-2	Summary statistics	9	154	2.0	3.0	VF is the number of distinct verb lemmas (273), mapped to the number of distinct frames that the Frames-Type row shows (149) (Figure 3   Confidence reports the average of annotatorassigned confidence scores for annotations per record.	0
2242	23331	23331	S19-2	Summary statistics	10	155	2.0	3.0	Although interpreting this measure demands more work, the averages appear to be as expected.	0
2243	23332	23332	S19-2	Summary statistics	11	156	2.0	3.0	Specifically, SemEval is higher in value than both InProg and Skipped, facts that we associate with double agreement and the choice reviewing process.	0
2244	23333	23333	S19-2	Summary statistics	12	157	3.0	3.0	Still, many records with high confidence scores remained as InProg given the lack of double agreement.	0
2245	23334	23334	S19-2	Summary statistics	13	158	3.0	3.0	Table 5 (Appendix A.1) lists the top 10 frames annotated with their respective highest and lowest confidence ratings averaged by their frequency in SemEval.	0
2246	23335	23335	S19-2	Summary statistics	14	159	3.0	3.0	The last two rows of Table 1 are meta-data on the annotation process.	0
2247	23336	23336	S19-2	Summary statistics	15	160	3.0	3.0	Time reports the total time annotators spent in active annotation, engaged in the steps described above (742 hours), excluding the reviewing process (Section 4.3.1) and including the time to annotate MWUs.	0
2248	23337	23337	S19-2	Summary statistics	16	161	3.0	3.0	Total-Move is the total number of logical moves for frame annotation between annotators and the annotation system, i.e., logged changes in the process of frame and core FE annotation.	0
2249	23338	23338	S19-2	Summary statistics	17	162	4.0	3.0	This number excludes annotation of verb subcategorization with generic semantic roles.	0
2250	23339	23339	S19-2	Summary statistics	18	163	4.0	3.0	6 In SemEval, annotated frames had an average of 2.15 arguments, requiring a minimum of five logical moves to annotate (MWU-less sentences).	0
2251	23340	23340	S19-2	Summary statistics	19	164	4.0	3.0	However, on average, each SemEval record required 14.8 moves.	0
2252	23341	23341	S19-2	Summary statistics	20	165	4.0	3.0	This number is even higher for InProg (18.2); we believe that it indicates the complexity of the annotation task.	0
2253	23342	23342	S19-2	Summary statistics	21	166	4.0	3.0	Table 4 (Appendix A.1) further details annotator activity, with time spent and moves per annotation step.	0
2254	23343	23343	S19-2	Summary statistics	22	167	4.0	3.0	As expected, frame annotation of verbs (Step 2), was the most time consuming part of the task.	0
2255	23344	23344	S19-2	Development Dataset	1	168	2.0	3.0	Shared task participants received a development set consisting of 600 records from a total of 4,620 records, where Table 4 shows the statistics.	0
2256	23345	23345	S19-2	Development Dataset	2	169	4.0	3.0	The development set contained gold annotations for all three subtasks.	0
2257	23346	23346	S19-2	Evaluation Metrics	1	170	1.0	3.0	For all subtasks, as figure of merit, here we report the performance of participating systems with measures for evaluating text clustering techniques, including the classic measures of Purity (PU), inverse-Purity (IPU), and their harmonic mean (PIF) (Steinbach et al., 2000), as well as the harmonic mean for BCubed precision and recall (i.e., BCP, BCR, and BCF, respectively) (Bagga and Baldwin, 1998).	0
2258	23347	23347	S19-2	Evaluation Metrics	2	171	1.0	3.0	To compute these measures for the pairing of reference-labeled data and unsupervised-labeled data (with each having an exact set of annotated items), we built a contingency table T with rows for gold labels and columns for unsupervised system labels.	0
2259	23348	23348	S19-2	Evaluation Metrics	3	172	2.0	3.0	We filled the table with the number of intersecting items, as done in cross-tabulation of results in classification tasks to compute precision and recall.	0
2260	23349	23349	S19-2	Evaluation Metrics	4	173	2.0	3.0	For Task A (Section 3), T tracks the unsupervised system labels and the gold reference labels assigned to verbs.	0
2261	23350	23350	S19-2	Evaluation Metrics	5	174	2.0	3.0	For Task B.1, we labeled the rows and columns of T with tuples (l v , l a ), where l v labels the frame evoking verb and l a labels the FE filler.	0
2262	23351	23351	S19-2	Evaluation Metrics	6	175	3.0	3.0	For Task B.2, the rows and columns in T track the unsupervised system labels and the gold reference labels (generic semantic roles) assigned to arguments.	0
2263	23352	23352	S19-2	Evaluation Metrics	7	176	3.0	3.0	These performance measures reflect a notion of similarity between the distribution of unsupervised labels and that of the gold reference labels, given certain criteria.	0
2264	23353	23353	S19-2	Evaluation Metrics	8	177	3.0	3.0	Specifically, they define the notions of consistency and completeness of automatically generated clusters based on the evaluation data.	0
2265	23354	23354	S19-2	Evaluation Metrics	9	178	4.0	3.0	Each method measures consistency and completeness in its own way, and alone may lack sufficient information for a clear understanding and analysis of system performance (Amigó et al., 2009).	0
2266	23355	23355	S19-2	Evaluation Metrics	10	179	4.0	3.0	But, as the single metric for system ranking, we used the BCF measure, given its satisfactory behavior in certain situations.	0
2267	23356	23356	S19-2	Evaluation Metrics	11	180	4.0	3.0	Note that we modeled the task and its evaluation as hard clustering, where a record receives only one label, without overlap in any generated category of items.	0
2268	23357	23357	S19-2	Baselines	1	181	1.0	3.0	Similar to other clustering tasks, we use baselines of random, all-in-one-cluster (AIN1), and one-cluster-per-instance (1CPI).	0
2269	23358	23358	S19-2	Baselines	2	182	1.0	3.0	Additionally, we adapted the baseline of the most frequent sense in WSI for these tasks by introducing the one-cluster-per-head (1CPH) baseline in Task A, and one-cluster-per-syntactic-category (1CPG) for verb argument clustering in Task B.2. 7 For Task B.1, we built a baseline, 1CPGH for labeling verbs with their lemmas (as in 1CPH) and FEs with grammatical relation to their heads (as in 1CPG).	0
2270	23359	23359	S19-2	Baselines	3	183	1.0	3.0	We included two more labels lcmpx and rcmpx for frame fillers with no direct syntactic relation to the head verb, if occurring left of or right of the verb, respectively.	0
2271	23360	23360	S19-2	Baselines	4	184	2.0	3.0	Both 1CPH and 1CPG (and their combination for Task B.1) are hard to beat because of the longtailed distribution of the frequency of our test data.	0
2272	23361	23361	S19-2	Baselines	5	185	2.0	3.0	E.g., most verbs frequently instantiate one particular frame and rarely other ones.	0
2273	23362	23362	S19-2	Baselines	6	186	2.0	3.0	Similarly, a particular role (FE) frequently is filled by words that have a particular grammatical relation to its governing verb; e.g., most subjects of most verb forms receive the agent label in their subcategorization frame (or, an agent-like element in their Frame Semantics representations).	0
2274	23363	23363	S19-2	Baselines	7	187	3.0	3.0	Evidently the chosen labels for grammatical relations influences 1CPG and 1CPHG scores.	0
2275	23364	23364	S19-2	Baselines	8	188	3.0	3.0	Values reported later (specifically, Tables 6 and 2) could be improved by employing heuristics, e.g., relabeling enhanced dependencies using a few rules.	0
2276	23365	23365	S19-2	Baselines	9	189	3.0	3.0	We also employed one unsupervised and a second supervised system baselines.	0
2277	23366	23366	S19-2	Baselines	10	190	4.0	3.0	For the unsupervised one, we trained the system with data from Kallmeyer et al. (2018).	0
2278	23367	23367	S19-2	Baselines	11	191	4.0	3.0	For the supervised one, we used OPEN-SESAME, a state-of-the-art supervised FrameNet tagger (Swayamdipta et al., 2018).	0
2279	23368	23368	S19-2	Baselines	12	192	4.0	3.0	After converting its output to the format of the present task, we evaluated it similar to other systems.	0
2280	23369	23369	S19-2	Baselines	13	193	4.0	3.0	Both systems were trained out-of-thebox with no additional tuning.	0
2281	23370	23370	S19-2	System Descriptions	1	194	1.0	3.0	We received submissions from nine teams (13 participants).	0
2282	23371	23371	S19-2	System Descriptions	2	195	1.0	3.0	Only three chose to submit system description papers.	0
2283	23372	23372	S19-2	System Descriptions	3	196	1.0	3.0	provided a solution for Task A and Task B.2, using both sets of these results to address Task B.1.	0
2284	23373	23373	S19-2	System Descriptions	4	197	1.0	3.0	Task A used language models and Hearst-like patterns to tune and obtain contextualized vector representations for the verbs in the test set.	0
2285	23374	23374	S19-2	System Descriptions	5	198	1.0	3.0	A hierarchical agglomerative clustering method followed, where hyperparameters were set with labeled and unlabeled records from the development and test sets.	0
2286	23375	23375	S19-2	System Descriptions	6	199	2.0	3.0	Task B.2 employed a logistic regression trained over the development set to identify only the most frequent labels.	0
2287	23376	23376	S19-2	System Descriptions	7	200	2.0	3.0	The classifier was based on features obtained from a language model and hand-crafted rules.	0
2288	23377	23377	S19-2	System Descriptions	8	201	2.0	4.0	Using logistic regression and training this algorithm with the development set remains an issue of concern, given the intended unsupervised scenario.	0
2289	23378	23378	S19-2	System Descriptions	9	202	2.0	4.0	While we objected to using the development set to train a supervised system for this subtask, we still report its scores.	0
2290	23379	23379	S19-2	System Descriptions	10	203	2.0	4.0	The differences between its results and those of the other systems may be informative.	0
2291	23380	23380	S19-2	System Descriptions	11	204	3.0	4.0	Still, we considered Arefyev et al.'s results for Task B only complementarily, not to rank the systems.	0
2292	23381	23381	S19-2	System Descriptions	12	205	3.0	4.0	Anwar et al. (2019) proposed a method that was similar to that of .	0
2293	23382	23382	S19-2	System Descriptions	13	206	3.0	4.0	Arefyev et al. used contextualized word embeddings from the BERT language modeling tool Devlin et al. (2018), whereas Anwar et al. used pre-trained embeddings.	0
2294	23383	23383	S19-2	System Descriptions	14	207	3.0	4.0	They merged the outputs of Tasks A and B.2 for Task B.1.	0
2295	23384	23384	S19-2	System Descriptions	15	208	3.0	4.0	Task A used agglomerative clustering of vectors with concatenated verb representation vectors and vectors that represent usage context.	0
2296	23385	23385	S19-2	System Descriptions	16	209	4.0	4.0	Task B.2 employed hand crafted features, a method to encode syntactic information, and again an agglomerative clustering method.	0
2297	23386	23386	S19-2	System Descriptions	17	210	4.0	4.0	Ribeiro et al. (2019) also reported results for all subtasks using similar techniques to those reported in the other two submitted papers.	0
2298	23387	23387	S19-2	System Descriptions	18	211	4.0	4.0	Ribeiro et al. (2019) used the bidirectional neural language model BERT, which  also used.	0
2299	23388	23388	S19-2	System Descriptions	19	212	4.0	4.0	Task A employed contextualized word representations proposed in (Ustalov et al., 2018), and Biemann's clustering algorithm (Biemann, 2006).	0
2300	23389	23389	S19-2	System Descriptions	20	213	4.0	4.0	Compared to the two other systems, Ribeiro et al. (2019) exploited input structures, weighted them, and used them elegantly in its algorithm.	0
2301	23390	23390	S19-2	System Descriptions	21	214	4.0	4.0	With the same method but different hyper-parameters for B.2 along with combining results from Task A, Ribeiro et al. (2019) offered a solution to B.1.	0
2302	23391	23391	S19-2	Results and Data Analysis	1	215	1.0	4.0	Table 2 reports the BCF scores for system submissions along with a baseline for each task.	0
2303	23392	23392	S19-2	Results and Data Analysis	2	216	1.0	4.0	8	0
2304	23393	23393	S19-2	Results and Data Analysis	3	217	1.0	4.0	As the table shows, each system performs best only in one of the tasks.	0
2305	23394	23394	S19-2	Results and Data Analysis	4	218	1.0	4.0	We report Arefyev et al.'s submission for Tasks B.1 and B.2 only to show the benefit of using a small amount of training data and a supervised method together with a clustering algorithm, provided that such training data is available.	0
2306	23395	23395	S19-2	Results and Data Analysis	5	219	1.0	4.0	As readers know, finding the optimal (actual) number of clusters is an open research area.	0
2307	23396	23396	S19-2	Results and Data Analysis	6	220	1.0	4.0	Participants knew the number of clusters: whereas Arefyev et al. and Anwar et al. used this information, Ribeiro et al. opted for a statistical method tuned with data that we provided.	0
2308	23397	23397	S19-2	Results and Data Analysis	7	221	1.0	4.0	The baseline systems, the unsupervised method of Kallmeyer et al. (2018)   of all systems regarding BCF.	0
2309	23398	23398	S19-2	Results and Data Analysis	8	222	1.0	4.0	This result is not surprising since that work did not effectively handle MWUs in the test, where only the head of the MWU was kept.	0
2310	23399	23399	S19-2	Results and Data Analysis	9	223	1.0	4.0	However, the output of Open-SESAME, and its low BCF was indeed surprising.	0
2311	23400	23400	S19-2	Results and Data Analysis	10	224	1.0	4.0	We fed Open-SESAME the sentences from the test set; it identified approximately 5k frames.	0
2312	23401	23401	S19-2	Results and Data Analysis	11	225	2.0	4.0	However, the overlap with the test set was only 1,216 records (identification problem in Open-SESAME).	0
2313	23402	23402	S19-2	Results and Data Analysis	12	226	2.0	4.0	These 1,216 records exhibit a mismatch between 536 of the arguments and their respective target verbs.	0
2314	23403	23403	S19-2	Results and Data Analysis	13	227	2.0	4.0	We ignored the system's extra or incorrectly generated arguments, and replaced the missing items with those of the 1CPHG baseline records.	0
2315	23404	23404	S19-2	Results and Data Analysis	14	228	2.0	4.0	We then used the resulting records for evaluation against the task's gold data as did the task's participants.	0
2316	23405	23405	S19-2	Results and Data Analysis	15	229	2.0	4.0	As Table 3 shows, the unsupervised method outperforms the supervised system for all tasks by a wide margin, i.e.,the unsupervised label set can carry more information than does the supervised label set.	0
2317	23406	23406	S19-2	Results and Data Analysis	16	230	2.0	4.0	We compared results for confidence measure that annotators assigned to records.	0
2318	23407	23407	S19-2	Results and Data Analysis	17	231	2.0	4.0	First, we split the evaluation records according to their assigned confidence value into five subsets E i , 1 ≤ i ≤ 5, such that subset E 1 contained only records with confidence value 1, E 2 contained only record with confidence value 2, etc..	0
2319	23408	23408	S19-2	Results and Data Analysis	18	232	2.0	4.0	Then we evaluated system outputs on each subset E i and logged that BCF.	0
2320	23409	23409	S19-2	Results and Data Analysis	19	233	2.0	4.0	Later, we performed this evaluation cumulatively using subsets E i s by adding records from all E j s to E i where i &lt; j. Interpreting the obtained values requires careful attention (e.g., changes in the prior probabilities of gold clusters and their cardinality must be taken into account), overall, we observed a similar trend for all systems: as expected, namely a positive correlation between the confidence value and BCF.	0
2321	23410	23410	S19-2	Results and Data Analysis	20	234	2.0	4.0	Thus, what human annotators usually found hard to annotate, automatic systems also found hard to cluster.	0
2322	23411	23411	S19-2	Results and Data Analysis	21	235	3.0	4.0	(The reverse relation does not hold).	0
2323	23412	23412	S19-2	Results and Data Analysis	22	236	3.0	4.0	Or, pessimistically, the level of noise in annotation increases as their associated confidence decreases.	0
2324	23413	23413	S19-2	Results and Data Analysis	23	237	3.0	4.0	(Table 7 in Appendix A.2 details the results.)	0
2325	23414	23414	S19-2	Results and Data Analysis	24	238	3.0	4.0	Finally, we wanted to identify the frames that machines found difficult to cluster.	0
2326	23415	23415	S19-2	Results and Data Analysis	25	239	3.0	4.0	To estimate difficulty we used the differences in BCF under the following conditions.	0
2327	23416	23416	S19-2	Results and Data Analysis	26	240	3.0	4.0	We repeated the evaluation process 1 ≤ i ≤ n times (where n is the number of gold labels for a task) for each system.	0
2328	23417	23417	S19-2	Results and Data Analysis	27	241	3.0	4.0	In each iteration i, we removed all data items of a gold category i.	0
2329	23418	23418	S19-2	Results and Data Analysis	28	242	3.0	4.0	We measured and noted the resulting BCF in the given iteration; we deduced the score from the system performance over the entire gold set.	0
2330	23419	23419	S19-2	Results and Data Analysis	29	243	3.0	4.0	To cancel frequency effects, we normalized the differences by the number of gold data instances.	0
2331	23420	23420	S19-2	Results and Data Analysis	30	244	3.0	4.0	We removed all records annotated as COMMERCE SELL from the evaluation set E to form E .	0
2332	23421	23421	S19-2	Results and Data Analysis	31	245	4.0	4.0	We computed the BCF of the systems over E (E ⊂ E), and measured d = E BCF − E BCF .	0
2333	23422	23422	S19-2	Results and Data Analysis	32	246	4.0	4.0	We interpreted a positive difference as an easy to cluster gold category i, and a negative difference as a hard to cluster gold category i.	0
2334	23423	23423	S19-2	Results and Data Analysis	33	247	4.0	4.0	The heat maps in Table 8 and Table 9 show a summary of the results for Task A and Task B.2, respectively.	0
2335	23424	23424	S19-2	Results and Data Analysis	34	248	4.0	4.0	All systems performed similarly for approximately 30% of the gold classes.	0
2336	23425	23425	S19-2	Results and Data Analysis	35	249	4.0	4.0	Comparing differences across systems and the baselines of 1CPH and 1CPG reveals (possibly) interesting information.	0
2337	23426	23426	S19-2	Results and Data Analysis	36	250	4.0	4.0	Thus, for example, in Task A, most systems found COMMERCE SELL hard and COMMERCE BUY easy to cluster.	0
2338	23427	23427	S19-2	Results and Data Analysis	37	251	4.0	4.0	Interestingly, a set of six verbs evokes each frame: buy, purchase, buy back, buy up, buy out, buy into for COMMERCE BUY; and sell, retail, auction, place, deal, resell for COMMERCE SELL.	0
2339	23428	23428	S19-2	Results and Data Analysis	38	252	4.0	4.0	From these two sets of verbs, three are polysemous: buy in the former, and place and deal in the latter.	0
2340	23429	23429	S19-2	Results and Data Analysis	39	253	4.0	4.0	Does the morphology of the verbs (e.g., buy-back, resell) make one easy to cluster?	0
2341	23430	23430	S19-2	Results and Data Analysis	40	254	4.0	4.0	Alternatively, are other factors at play, such as the number of verb instances?	0
2342	23431	23431	S19-2	Results and Data Analysis	41	255	4.0	4.0	How these factors might influence the proposed naive BCF-difference model is an open question.	0
2343	23432	23432	S19-2	Concluding Remarks	1	256	1.0	4.0	We have presented the SemEval 2019 task on unsupervised lexical frame induction.	0
2344	23433	23433	S19-2	Concluding Remarks	2	257	1.0	4.0	We described the task in detail, provided a summary of methods that participants developed, and compared the results.	0
2345	23434	23434	S19-2	Concluding Remarks	3	258	1.0	4.0	Although much room for improvement of the task remains, we consider it a step forward.	0
2346	23435	23435	S19-2	Concluding Remarks	4	259	2.0	4.0	It employed a well-motivated typology of lexical frames to distinguish lexical frame induction tasks.	0
2347	23436	23436	S19-2	Concluding Remarks	5	260	2.0	4.0	The evaluation data derived from annotations of a well-known resource, namely a portion of WSJ sentences, perhaps the most annotated corpus of English.	0
2348	23437	23437	S19-2	Concluding Remarks	6	261	2.0	4.0	These features provide opportunities for future investigation, in particular in studies related to reciprocal relations between syntactic and lexical semantic frame structures.	0
2349	23438	23438	S19-2	Concluding Remarks	7	262	3.0	4.0	One reason to promote using unsupervised methods is their inherent flexibility to embrace unknown data.	0
2350	23439	23439	S19-2	Concluding Remarks	8	263	3.0	4.0	These methods have a high margin of tolerance for noise, and perform better than supervised method with insufficient training data.	0
2351	23440	23440	S19-2	Concluding Remarks	9	264	3.0	4.0	For unsupervised data, obtaining or generating training data is easier than doing so with supervised methods because they simply do not require annotation.	0
2352	23441	23441	S19-2	Concluding Remarks	10	265	4.0	4.0	For example, all participant systems could collect similar unlabeled training data from only syntactically annotated corpora to generate more unlabeled records.	0
2353	23442	23442	S19-2	Concluding Remarks	11	266	4.0	4.0	Ultimately, such methods can achieve respectable performance, and produce clusters which are both more informative than the unlabeled input and supervised categories (under certain situations).	0
2354	23443	23443	S19-2	Concluding Remarks	12	267	4.0	4.0	As shown, unsupervised methods can even outperform a state-of-the-art Frame Semantics parser by a wide margin (Section 7), while a very large gap remains for improvements in future work.	0
2355	26004	26004	S20-4	title	1	1	4.0	1.0	SemEval-2020 Task 4: Commonsense Validation and Explanation	0
2356	26005	26005	S20-4	abstract	1	2	1.0	1.0	In this paper, we present SemEval-2020 Task 4, Commonsense Validation and Explanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish a natural language statement that makes sense to humans from one that does not, and provide the reasons.	1
2357	26006	26006	S20-4	abstract	2	3	1.0	1.0	Specifically, in our first subtask, the participating systems are required to choose from two natural language statements of similar wording the one that makes sense and the one does not.	0
2358	26007	26007	S20-4	abstract	3	4	2.0	1.0	The second subtask additionally asks a system to select the key reason from three options why a given statement does not make sense.	0
2359	26008	26008	S20-4	abstract	4	5	2.0	1.0	In the third subtask, a participating system needs to generate the reason.	0
2360	26009	26009	S20-4	abstract	5	6	3.0	1.0	We finally attracted 39 teams participating at least one of the three subtasks.	0
2361	26010	26010	S20-4	abstract	6	7	3.0	1.0	For Subtask A and Subtask B, the performances of top-ranked systems are close to that of humans.	0
2362	26011	26011	S20-4	abstract	7	8	4.0	1.0	However, for Subtask C, there is still a relatively large gap between systems and human performance.	0
2363	26012	26012	S20-4	abstract	8	9	4.0	1.0	The dataset used in our task can be found at https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation;	0
2364	26013	26013	S20-4	abstract	9	10	4.0	1.0	The leaderboard can be found at https://competitions.codalab.org/competitions/21080#results.	0
2365	26014	26014	S20-4	Introduction	1	11	1.0	1.0	In the past decades, computer' ability in processing natural language has significantly improved.	0
2366	26015	26015	S20-4	Introduction	2	12	1.0	1.0	However, its intelligence for understanding common sense expressed in language is still limited.	0
2367	26016	26016	S20-4	Introduction	3	13	1.0	1.0	"For example, it is straightforward for humans to judge that the following sentence is plausible, or makes sense: ""John put a turkey into a fridge"" while ""John put an elephant into the fridge"" does not, but it is non-trivial for a computer to tell the difference."	0
2368	26017	26017	S20-4	Introduction	4	14	1.0	1.0	Arguably, commonsense reasoning plays a central role in a natural language understanding system (Davis, 2017).	0
2369	26018	26018	S20-4	Introduction	5	15	1.0	1.0	It is essential to gauge how well computers can understand whether a given statement makes sense.	0
2370	26019	26019	S20-4	Introduction	6	16	2.0	1.0	In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world.	0
2371	26020	26020	S20-4	Introduction	7	17	2.0	1.0	1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012;Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016;	0
2372	26021	26021	S20-4	Introduction	8	18	2.0	1.0	Ostermann et al., 2018b;	0
2373	26022	26022	S20-4	Introduction	9	19	2.0	1.0	Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016;	0
2374	26023	26023	S20-4	Introduction	10	20	2.0	1.0	Talmor et al., 2018;Mihaylov et al., 2018).	0
2375	26024	26024	S20-4	Introduction	11	21	2.0	1.0	They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge.	0
2376	26025	26025	S20-4	Introduction	12	22	3.0	1.0	The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process.	0
2377	26026	26026	S20-4	Introduction	13	23	3.0	1.0	The SemEval-2020	0
2378	26027	26027	S20-4	Introduction	14	24	3.0	1.0	Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons.	0
2379	26028	26028	S20-4	Introduction	15	25	3.0	1.0	"In the first subtask, a system needs to choose the against-common-sense statement from two natural language statements of similar wordings, e.g., ""John put an elephant into the fridge"" and ""John put a turkey into the fridge"", respectively."	0
2380	26029	26029	S20-4	Introduction	16	26	3.0	1.0	The second task aims to find the key reason from three provided options why a given nonsensical statement does not make sense.	0
2381	26030	26030	S20-4	Introduction	17	27	3.0	1.0	"For example, for the nonsensical statement, ""John put an elephant into the fridge"", the three options are ""An elephant is much bigger than a fridge"", ""Elephants are usually white while fridges are usually white"", and ""An elephant cannot eat a fridge."""	0
2382	26031	26031	S20-4	Introduction	18	28	4.0	1.0	A system needs to identify the correct reason.	0
2383	26032	26032	S20-4	Introduction	19	29	4.0	1.0	In addition, the third task requires the participating systems to generate the reason automatically.	0
2384	26033	26033	S20-4	Introduction	20	30	4.0	1.0	We hope that the task and datasets can facilitate studies on commonsense validation, its interpretability, and the related natural language understanding and generation problems.	0
2385	26034	26034	S20-4	Introduction	21	31	4.0	1.0	There are 39 teams submitting valid systems to at least one subtask.	0
2386	26035	26035	S20-4	Introduction	22	32	4.0	1.0	In Subtask A and Subtask B, top-performing systems achieve performances closed to that of human subjects.	0
2387	26036	26036	S20-4	Introduction	23	33	4.0	1.0	However, for Subtask C, there is still a relatively large between system and human performances.	0
2388	26037	26037	S20-4	Task Setup	1	34	1.0	1.0	Task Definition	0
2389	26038	26038	S20-4	Task Setup	2	35	1.0	1.0	Formally, each instance in our dataset is composed of eight sentences:	0
2390	26039	26039	S20-4	Task Setup	3	36	1.0	1.0	and s 2 are two similar statements that differ by only a few words; one of them makes sense (i.e., conforms to common sense) while the other does not.	0
2391	26040	26040	S20-4	Task Setup	4	37	1.0	1.0	They are used in our Subtask A: the Validation subtask, which requires a model to identify which one makes sense.	0
2392	26041	26041	S20-4	Task Setup	5	38	1.0	1.0	For the statement that does not make sense, we have three candidate reasons, i.e., three options o 1 , o 2 , and o 3 ; one of them explains why the statement does not make sense.	0
2393	26042	26042	S20-4	Task Setup	6	39	1.0	1.0	So, in our Subtask B, the Explanation (Multi-Choice) subtask, a model is required to find the correct reason from the three options.	0
2394	26043	26043	S20-4	Task Setup	7	40	1.0	1.0	For the same nonsensical statement, in Subtask C, the Explanation (Generation) subtask, a participating system needs to generate the reason why it does not make sense.	0
2395	26044	26044	S20-4	Task Setup	8	41	2.0	1.0	Three references, r 1 , r 2 , and r 3 , are used for evaluating Subtask C. Below we give an example for each subtask, in which we introduce some notations we will use in the paper.	0
2396	26045	26045	S20-4	Task Setup	9	42	2.0	1.0	• Subtask A: Validation Task:	0
2397	26046	26046	S20-4	Task Setup	10	43	2.0	1.0	Select the statement of the two that does not make sense.	0
2398	26047	26047	S20-4	Task Setup	11	44	2.0	1.0	s 1 : John put a turkey into a fridge.	0
2399	26048	26048	S20-4	Task Setup	12	45	2.0	1.0	s 2 : John put an elephant into the fridge.	0
2400	26049	26049	S20-4	Task Setup	13	46	2.0	1.0	In this example, s 1 is a sensical statement, also denoted as s c , while s 2 is the nonsensical statement, which is also denoted as s n .	0
2401	26050	26050	S20-4	Task Setup	14	47	2.0	1.0	• Subtask B: Explanation (Multi-Choice)	0
2402	26051	26051	S20-4	Task Setup	15	48	3.0	1.0	Task:	0
2403	26052	26052	S20-4	Task Setup	16	49	3.0	1.0	Select the best reason that explains why the given statement does not make sense.	0
2404	26053	26053	S20-4	Task Setup	17	50	3.0	1.0	Nonsensical statement (s n ):	0
2405	26054	26054	S20-4	Task Setup	18	51	3.0	1.0	John put an elephant into the fridge.	0
2406	26055	26055	S20-4	Task Setup	19	52	3.0	1.0	o 1 : An elephant is much bigger than a fridge.	0
2407	26056	26056	S20-4	Task Setup	20	53	3.0	1.0	o 2 : Elephants are usually white while fridges are usually white.	0
2408	26057	26057	S20-4	Task Setup	21	54	3.0	1.0	o 3 : An elephant cannot eat a fridge.	0
2409	26058	26058	S20-4	Task Setup	22	55	4.0	1.0	In this example, the option o 1 is the correct reason, which is also denoted also as o c , while o 2 and o 3 are not the reason, which are also denoted as o n1 and o n2 .	0
2410	26059	26059	S20-4	Task Setup	23	56	4.0	1.0	• Subtask C: Explanation (Generation)	0
2411	26060	26060	S20-4	Task Setup	24	57	4.0	1.0	Task: Generate the reason why this statement does not make sense.	0
2412	26061	26061	S20-4	Task Setup	25	58	4.0	1.0	Nonsensical statement (s n ):	0
2413	26062	26062	S20-4	Task Setup	26	59	4.0	1.0	John put an elephant into the fridge.	0
2414	26063	26063	S20-4	Task Setup	27	60	4.0	1.0	Reference reasons (used for calculating the BLEU score): r 1 : An elephant is much bigger than a fridge.	0
2415	26064	26064	S20-4	Task Setup	28	61	4.0	1.0	r 2 : A fridge is much smaller than an elephant.	0
2416	26065	26065	S20-4	Task Setup	29	62	4.0	1.0	r 3 : Most of the fridges aren't large enough to contain an elephant.	0
2417	26066	26066	S20-4	Score Description	1	63	2.0	1.0	0	0
2418	26067	26067	S20-4	Score Description	2	64	4.0	1.0	The reason is not grammatically correct, or not comprehensible at all, or not related to the statement at all.	0
2419	26068	26068	S20-4	1	1	65	2.0	1.0	The reason is just the negation of the statement or a simple paraphrase.	0
2420	26069	26069	S20-4	1	2	66	4.0	1.0	Obviously, a better explanation can be made.	0
2421	26070	26070	S20-4	2	1	67	2.0	1.0	The reason is relevant and appropriate, though it may contain a few grammatical errors or unnecessary parts.	0
2422	26071	26071	S20-4	2	2	68	4.0	1.0	Or like case 1, but it's hard to write a proper reason.	0
2423	26072	26072	S20-4	3	1	69	2.0	1.0	The reason is appropriate and is a solid explanation of why the statement does not make sense.	0
2424	26073	26073	S20-4	3	2	70	4.0	1.0	Table 1: Rubrics used in human evaluation in Subtask C.	0
2425	26074	26074	S20-4	Evaluation Metrics	1	71	1.0	1.0	The Subtasks	0
2426	26075	26075	S20-4	Evaluation Metrics	2	72	1.0	2.0	A and B are evaluated using accuracy.	0
2427	26076	26076	S20-4	Evaluation Metrics	3	73	2.0	2.0	Subtask C is evaluated with the BLEU score (Papineni et al., 2002).	0
2428	26077	26077	S20-4	Evaluation Metrics	4	74	2.0	2.0	In addition, for Subtask C, we further perform human evaluation.	0
2429	26078	26078	S20-4	Evaluation Metrics	5	75	3.0	2.0	We randomly select 100 instances from the test set and evaluate system outputs on Amazon Mechanical Turk.	0
2430	26079	26079	S20-4	Evaluation Metrics	6	76	3.0	2.0	We ask three different crowd-sourcing workers to score each generated reason with a scale ranging from 0 to 3, inclusively, according the rubrics listed in Table 1.	0
2431	26080	26080	S20-4	Evaluation Metrics	7	77	4.0	2.0	Then we calculate the average score of the three scores as our final human evaluation score.	0
2432	26081	26081	S20-4	Evaluation Metrics	8	78	4.0	2.0	Formally, the human evaluation score of system k is	0
2433	26082	26082	S20-4	Evaluation Metrics	9	79	4.0	2.0	where score ijk means the score from the j th annotator for system k on the i th instance.	0
2434	26083	26083	S20-4	Data Construction	1	80	1.0	2.0	Our data construction is mainly performed on Amazon Mechanical Turk, which consists of two steps:	0
2435	26084	26084	S20-4	Data Construction	2	81	1.0	2.0	• Step 1: In this step, we construct datasets for Subtask A and Subtask B. Specifically, we ask a crowd-sourcing worker to write a sensical statement s c and a nonsensical statement s n .	0
2436	26085	26085	S20-4	Data Construction	3	82	1.0	2.0	For the nonsensical statement s n , the worker further writes three sentences, o 1 , o 2 , o 3 ; one of them, denoted as o c , explains why the nonsensical statement does not make sense; two of them, denoted as o n1 and o n2 , serve as the confusing choices.	0
2437	26086	26086	S20-4	Data Construction	4	83	2.0	2.0	(Refer to Section 3.1 for details.)	0
2438	26087	26087	S20-4	Data Construction	5	84	2.0	2.0	•	0
2439	26088	26088	S20-4	Data Construction	6	85	2.0	2.0	Step 2: We then make three reference reasons, r 1 , r 2 , r 3 for Subtask C.	0
2440	26089	26089	S20-4	Data Construction	7	86	3.0	2.0	We use o c as one of three references, and collect two more references in this step.	0
2441	26090	26090	S20-4	Data Construction	8	87	3.0	2.0	We ask two different crowd-sourcing workers to write each of them.	0
2442	26091	26091	S20-4	Data Construction	9	88	3.0	2.0	Note that instead of letting the same worker in step 1 to write these two references, we asked two more workers.	0
2443	26092	26092	S20-4	Data Construction	10	89	4.0	2.0	The reason is to encourage diversity of the reference.	0
2444	26093	26093	S20-4	Data Construction	11	90	4.0	2.0	(Refer to Section 3.2 for details.)	0
2445	26094	26094	S20-4	Data Construction	12	91	4.0	2.0	Finally, each instance of the dataset have 8 sentences:	0
2446	26095	26095	S20-4	Data Construction	13	92	4.0	2.0	, but for convenience of description, we denote it differently.	0
2447	26096	26096	S20-4	3.1	1	93	1.0	2.0	Step 1: Collecting Data for Subtask A and B Annotation Guidelines.	0
2448	26097	26097	S20-4	3.1	2	94	1.0	2.0	When writing instances, workers were asked to follow several principles: (1)	0
2449	26098	26098	S20-4	3.1	3	95	1.0	2.0	Try to avoid complex knowledge and focus on daily common sense.	0
2450	26099	26099	S20-4	3.1	4	96	1.0	2.0	Make the questions as understandable as possible, so that a literate person is able to give the right answers.	0
2451	26100	26100	S20-4	3.1	5	97	1.0	2.0	(2)	0
2452	26101	26101	S20-4	3.1	6	98	1.0	2.0	The confusing reason options, o n1 and o n2 , should better contain more content words or information such as entities and activities in the nonsensical statements s n .	0
2453	26102	26102	S20-4	3.1	7	99	2.0	2.0	"For example, the confusing reasons of ""John put an elephant into the fridge"" should better contain both ""elephant"" and ""fridge""."	0
2454	26103	26103	S20-4	3.1	8	100	2.0	2.0	(3)	0
2455	26104	26104	S20-4	3.1	9	101	2.0	2.0	The confusing reasons, o n1 and o n2 , should be related to the statements s n and the correct reason o c and not deviate from the context; otherwise it may be easily captured by pretrained models like BERT (Talmor et al., 2018).	0
2456	26105	26105	S20-4	3.1	10	102	2.0	2.0	( 4  and o 3 should only be related to the incorrect statements s n rather than the correct statements s c , because we want further studies to be able to estimate nonsensical statements s n without the correct statement s c . (5)	0
2457	26106	26106	S20-4	3.1	11	103	2.0	2.0	The confusing reasons, o n1 and o n2 , should make sense themselves.	0
2458	26107	26107	S20-4	3.1	12	104	2.0	2.0	Otherwise, the models may simply ignore the incorrect options o n1 , o n2 without considering the casual semantics.	0
2459	26108	26108	S20-4	3.1	13	105	3.0	2.0	This concern is raised from and motivated by the fact that models can achieve high performance in the ROC Story Cloze Task, when only looking at the alternative endings and ignoring the story content (Schwartz et al., 2017).	0
2460	26109	26109	S20-4	3.1	14	106	3.0	2.0	( 6)	0
2461	26110	26110	S20-4	3.1	15	107	3.0	2.0	We ask the annotators to make the nonsensical statement s n contain about the same number of words as the sensical statement s c , and the correct reason o c have similar length with other two options.	0
2462	26111	26111	S20-4	3.1	16	108	3.0	2.0	We drop the instances which do not meet such requirements.	0
2463	26112	26112	S20-4	3.1	17	109	3.0	2.0	Use of Inspirational Materials.	0
2464	26113	26113	S20-4	3.1	18	110	3.0	2.0	It is not easy for all crowd-sourcing workers to write instances from scratch.	0
2465	26114	26114	S20-4	3.1	19	111	4.0	2.0	To address this issue, we also provide them with external reading materials to stimulate inspiration, such as the sentences of the Open Mind Common Sense (OMCS) project (Havasi et al., 2010).	0
2466	26115	26115	S20-4	3.1	20	112	4.0	2.0	"For example, ""he was sent to a (restaurant)/(hospital) for treatment after a car crash"" can be inspired by the two sentences ""restaurants provide food"" and ""hospitals provide medical care""."	0
2467	26116	26116	S20-4	3.1	21	113	4.0	2.0	Quality Control.	0
2468	26117	26117	S20-4	3.1	22	114	4.0	2.0	To ensure the quality of the data, we manually check the instances and drop or request a rewriting of the low-quality ones.	0
2469	26118	26118	S20-4	3.1	23	115	4.0	2.0	If one worker writes too many low-quality instances, we will remove her or him from our annotator pool.	0
2470	26119	26119	S20-4	3.1	24	116	4.0	2.0	With such process, we finally accept around 30% submitted instances.	0
2471	26120	26120	S20-4	3.2	1	117	1.0	2.0	Step 2: Collecting Data for Subtask C Annotation Guidelines.	0
2472	26121	26121	S20-4	3.2	2	118	1.0	2.0	To collect data for Subtask C, each worker is given a nonsensical statement s n and a sensical statement s c and asked to write a reason to explain why the nonsensical statement s n does not make sense.	0
2473	26122	26122	S20-4	3.2	3	119	1.0	2.0	They shall follow the following rules: (1)	0
2474	26123	26123	S20-4	3.2	4	120	1.0	2.0	Do not explain why the sensical statement s c makes sense.	0
2475	26124	26124	S20-4	3.2	5	121	2.0	2.0	(2)	0
2476	26125	26125	S20-4	3.2	6	122	2.0	2.0	Avoid mentioning the sensical statement s c . (3)	0
2477	26126	26126	S20-4	3.2	7	123	2.0	2.0	"Write the reason, rather than simply add the word ""not"" or ""can't"" to the nonsensical statement s n to form an explanation."	0
2478	26127	26127	S20-4	3.2	8	124	2.0	2.0	(4)	0
2479	26128	26128	S20-4	3.2	9	125	2.0	2.0	"Write the reason, don't use patterns like ""XXX is not for YYY"" to create an explanation."	0
2480	26129	26129	S20-4	3.2	10	126	3.0	2.0	(5)	0
2481	26130	26130	S20-4	3.2	11	127	3.0	2.0	Do not try to justify why the nonsensical statement s n makes sense. (6)	0
2482	26131	26131	S20-4	3.2	12	128	3.0	2.0	Write only one sentence, do not be overly formal.	0
2483	26132	26132	S20-4	3.2	13	129	3.0	2.0	(7)	0
2484	26133	26133	S20-4	3.2	14	130	4.0	2.0	"Refrain from using ""because"" at the beginning of a sentence. (8)"	0
2485	26134	26134	S20-4	3.2	15	131	4.0	2.0	Do not try to correct the statement s n , but just give the reason.	0
2486	26135	26135	S20-4	3.2	16	132	4.0	2.0	Quality Control.	0
2487	26136	26136	S20-4	3.2	17	133	4.0	2.0	As the same as in Step 1, after the annotators write the reasons in Step 2, the first two authors of the paper perform the check process again.	0
2488	26137	26137	S20-4	3.2	18	134	4.0	2.0	We reject low-quality reasons (that violate the rules significantly) and low-quality annotators (who write many low-quality reasons with the number above a threshold).	0
2489	26138	26138	S20-4	Data Summary and Analysis	1	135	1.0	2.0	For SemEval-2020, we created 11,997 instances (i.e., 11,997 8-sentence tuples).	0
2490	26139	26139	S20-4	Data Summary and Analysis	2	136	1.0	2.0	We further split the instances into three subsets with 10,000 (the training set), 997 (the development set), and 1,000 (the test set) instances, respectively.	0
2491	26140	26140	S20-4	Data Summary and Analysis	3	137	1.0	2.0	We randomly assign the label of the correct options in subtask A and B to avoid unbalanced correct labels.	0
2492	26141	26141	S20-4	Data Summary and Analysis	4	138	1.0	2.0	We conduct three more data analysis experiments to evaluate data quality, including sentence length, common words and repetition.	0
2493	26142	26142	S20-4	Data Summary and Analysis	5	139	1.0	2.0	Average Length.	0
2494	26143	26143	S20-4	Data Summary and Analysis	6	140	2.0	2.0	In  sensical statements and nonsensical statements almost have the same average lengths in the three sets (the differences are equal or smaller than 1%), which is balanced.	0
2495	26144	26144	S20-4	Data Summary and Analysis	7	141	2.0	2.0	However, there is an obvious gap between the correct reasons and confusing reasons in terms of the average lengths (roughly 4% in the training set and 10% in the dev/test set).	0
2496	26145	26145	S20-4	Data Summary and Analysis	8	142	2.0	2.0	Common Word Analysis.	0
2497	26146	26146	S20-4	Data Summary and Analysis	9	143	2.0	2.0	The most common words are important for showing the differences between sentences.	0
2498	26147	26147	S20-4	Data Summary and Analysis	10	144	2.0	3.0	We only present those words which have obvious different frequencies between sensical statements and nonsensical statements or between correct/referential reasons and confusing reasons.	0
2499	26148	26148	S20-4	Data Summary and Analysis	11	145	3.0	3.0	So, we skip most uninformative words, including 'a', 'an', 'the', 'to', 'in', 'on', 'of', 'for', 'and', 'is', 'are' and 'be'.	0
2500	26149	26149	S20-4	Data Summary and Analysis	12	146	3.0	3.0	After removing those words, we can list the top-5 common words in each type of sentence in the training/dev+test sets.	0
2501	26150	26150	S20-4	Data Summary and Analysis	13	147	3.0	3.0	For sensical statements s c and nonsensical statements s n , there are no significant differences between the training, dev, and test set.	0
2502	26151	26151	S20-4	Data Summary and Analysis	14	148	3.0	3.0	"However, there is an obvious gap in the correct reasons o c and confusing reasons o n in negative words such as ""not"", ""no"", and ""cannot""."	0
2503	26152	26152	S20-4	Data Summary and Analysis	15	149	3.0	3.0	In the training data, negative words are about 3 times more common in the correct option o c than in the confusing options o n .	0
2504	26153	26153	S20-4	Data Summary and Analysis	16	150	4.0	3.0	In the dev+test data, the gap is about 40%, which indicates that the dev+test data has a higher quality than the training data.	0
2505	26154	26154	S20-4	Data Summary and Analysis	17	151	4.0	3.0	However, as discussed in (Niven and Kao, 2019), spurious statistical cues can affect BERT's results.	0
2506	26155	26155	S20-4	Data Summary and Analysis	18	152	4.0	3.0	We conjure that the negative words are also spurious effective clues, which make the Subtask B potentially easier.	0
2507	26156	26156	S20-4	Data Summary and Analysis	19	153	4.0	3.0	Repetition.	0
2508	26157	26157	S20-4	Data Summary and Analysis	20	154	4.0	3.0	The dev+test set have 12 instances (0.6%) that repeat the same nonsensical statements in the training data and 36 instances (1.8%) that repeat the same correct reasons with the training data.	0
2509	26158	26158	S20-4	Cautions of using the data	1	155	1.0	3.0	The following advice is given to all task participants and future users: (1)	0
2510	26159	26159	S20-4	Cautions of using the data	2	156	2.0	3.0	Feel free to use whatever additional data they deem appropriate for the tasks to train their model. (2)	0
2511	26160	26160	S20-4	Cautions of using the data	3	157	3.0	3.0	Do not use the input of Subtask B/C to help Subtask A and do not use the option o of Subtask B to help Subtask C. Otherwise the task will be artificially easy.	0
2512	26161	26161	S20-4	Cautions of using the data	4	158	4.0	3.0	This is because of two reasons: a)	0
2513	26162	26162	S20-4	Cautions of using the data	5	159	4.0	3.0	The nonsensical statements s n of Subtask B and Subtask C is exactly the nonsensical statements s c of Subtask A and, participants can use the input of the Subtask B/C to directly obtain the answer of Subtask A and the option answers o of Subtask B will also reduce the difficulty of Subtask A; b) the correct reason o c of Subtask B is also one of the reference reason o c in Subtask C.	0
2514	26163	26163	S20-4	Systems and Results	1	160	2.0	3.0	In this section, we show the evaluation results of all the submitted systems for the three subtasks.	0
2515	26164	26164	S20-4	Systems and Results	2	161	4.0	3.0	Since most systems share similar model architecture for subtasks A and B, we discuss the two subtasks together.	0
2516	26165	26165	S20-4	Subtask A and Subtask B	1	162	1.0	3.0	The formal evaluation results of Subtask A and B are shown in Table 4 and 5.	0
2517	26166	26166	S20-4	Subtask A and Subtask B	2	163	1.0	3.0	There are in total 39 valid submissions for Subtask A and 27 valid submissions for Subtask B. Most top-performing submissions Figure 1: The most commonly used model architectures used in the three subtasks.	0
2518	26167	26167	S20-4	Subtask A and Subtask B	3	164	1.0	3.0	This figure is mostly based on Team Solomon's system.	0
2519	26168	26168	S20-4	Subtask A and Subtask B	4	165	1.0	3.0	"For Subtask B and C, the connector can be simply ""No, "", to help in constraining the model to learn a choice that explains the unreasonability of the statement."	0
2520	26169	26169	S20-4	Subtask A and Subtask B	5	166	1.0	3.0	For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input.	0
2521	26170	26170	S20-4	Subtask A and Subtask B	6	167	1.0	3.0	For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference.	0
2522	26171	26171	S20-4	Subtask A and Subtask B	7	168	2.0	3.0	adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task.	0
2523	26172	26172	S20-4	Subtask A and Subtask B	8	169	2.0	3.0	See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge.	0
2524	26173	26173	S20-4	Subtask A and Subtask B	9	170	2.0	3.0	Below we introduce in detail several top-performing systems and their main features.	0
2525	26174	26174	S20-4	Subtask A and Subtask B	10	171	2.0	3.0	• CN-HIT-IT.NLP  ranks top in Subtask A.	0
2526	26175	26175	S20-4	Subtask A and Subtask B	11	172	2.0	3.0	They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs.	0
2527	26176	26176	S20-4	Subtask A and Subtask B	12	173	2.0	3.0	K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity.	0
2528	26177	26177	S20-4	Subtask A and Subtask B	13	174	2.0	3.0	They use ConceptNet as the commonsense repository to extract the triples for the statements.	0
2529	26178	26178	S20-4	Subtask A and Subtask B	14	175	3.0	3.0	• ECNU-Sense	0
2530	26179	26179	S20-4	Subtask A and Subtask B	15	176	3.0	3.0	Maker (Zhao et al., 2020)    • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa.	0
2531	26180	26180	S20-4	Subtask A and Subtask B	16	177	3.0	3.0	• BUT-FIT (Jon et al., 2020), LMVE , Lijunyi  use ALBERT as the encoder.	0
2532	26181	26181	S20-4	Subtask A and Subtask B	17	178	3.0	3.0	BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system.	0
2533	26182	26182	S20-4	Subtask A and Subtask B	18	179	3.0	3.0	• UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020)	0
2534	26183	26183	S20-4	Subtask A and Subtask B	19	180	3.0	3.0	Table 6: Subtask C results of all the submitted systems.	0
2535	26184	26184	S20-4	Subtask A and Subtask B	20	181	4.0	3.0	Those marked with * did not submit a system description paper, and those marked with + means they do not include Subtask C in their system description paper.	0
2536	26185	26185	S20-4	Subtask A and Subtask B	21	182	4.0	3.0	It can be seen from the results that pretrained language models such as RoBERTa can achieve rather high performance, e.g., the team Solomon achieves 96.0% and 94.0% on Subtask A and Subtask B, respectively, without using further resources.	0
2537	26186	26186	S20-4	Subtask A and Subtask B	22	183	4.0	3.0	This shows that large-scale pretrained language models do contain commonsense knowledge to deal with the Subtask A and the Subtask B in this challenge.	0
2538	26187	26187	S20-4	Subtask A and Subtask B	23	184	4.0	3.0	Additionally finetuning the pretrained language models on commonsense-related text such as OMCS, which we use as inspirational materials, can push the results even higher, close to human performance.	0
2539	26188	26188	S20-4	Subtask A and Subtask B	24	185	4.0	3.0	The best-performing teams on Subtask A and Subtask B both adopt K-BERT, which incorporates the external knowledge base (i.e. ConceptNet) to complement the pretrained language models with knowledge triples.	0
2540	26189	26189	S20-4	Subtask A and Subtask B	25	186	4.0	3.0	This shows that knowledge-graph-enhanced approaches, such as K-BERT can effectively incorporate external knowledge.	0
2541	26190	26190	S20-4	Subtask A and Subtask B	26	187	4.0	3.0	However, the high number may also indicate data leaking to some extent, since in the data creation stage, both ConceptNet and OMCS are used as references for the annotator to write the data instances.	0
2542	26191	26191	S20-4	Subtask C	1	188	1.0	3.0	The results for Subtask C are shown in Table 6.	0
2543	26192	26192	S20-4	Subtask C	2	189	1.0	3.0	There are in total 17 valid submissions for Subtask C.	0
2544	26193	26193	S20-4	Subtask C	3	190	1.0	3.0	There are generally two approaches: (1) sequence-to-sequence approach, where the source side is the non-sensical statement, and the reason is the target sequence. (2) language model generation approach, which uses large-scale pretrained auto-regressive language models such as GPT-2 (Radford et al., 2019) for reason generation, where the non-sensical sentence acts as prompt.	0
2545	26194	26194	S20-4	Subtask C	4	191	1.0	3.0	An example of the language model generation approach is shown in Figure 1, which is most commonly used and achieves relatively good results.	0
2546	26195	26195	S20-4	Subtask C	5	192	1.0	3.0	Below we describe in detail the systems and their main features.	0
2547	26196	26196	S20-4	Subtask C	6	193	1.0	3.0	• BUT-FIT (Jon et al., 2020) experiments with both the sequence-to-sequence approach and the language generation approach.	0
2548	26197	26197	S20-4	Subtask C	7	194	2.0	3.0	For the sequence-to-sequence approach, they use BART (Lewis et al., 2019) with beam-search decoding to achieves the highest BLEU among all the teams.	0
2549	26198	26198	S20-4	Subtask C	8	195	2.0	3.0	For the language generation approach, the nonsensical statement is used as a prompt.	0
2550	26199	26199	S20-4	Subtask C	9	196	2.0	3.0	At the training stage, the statement and the explanation are concatenated together, and a GPT-2 is trained on these sequences with a next token prediction objective.	0
2551	26200	26200	S20-4	Subtask C	10	197	2.0	3.0	At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated.	0
2552	26201	26201	S20-4	Subtask C	11	198	2.0	3.0	• KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART.	0
2553	26202	26202	S20-4	Subtask C	12	199	2.0	3.0	To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary.	0
2554	26203	26203	S20-4	Subtask C	13	200	2.0	3.0	2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation.	0
2555	26204	26204	S20-4	Subtask C	14	201	3.0	3.0	This approach proves effective and makes their system second-best for human evaluations.	0
2556	26205	26205	S20-4	Subtask C	15	202	3.0	3.0	• ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework.	0
2557	26206	26206	S20-4	Subtask C	16	203	3.0	3.0	Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train the model with two self-attention heads: one for language models and another for classification.	0
2558	26207	26207	S20-4	Subtask C	17	204	3.0	3.0	They then use data from both task B and task C to calculate language model loss and classification loss.	0
2559	26208	26208	S20-4	Subtask C	18	205	3.0	3.0	Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage.	0
2560	26209	26209	S20-4	Subtask C	19	206	3.0	3.0	• Solomon (Srivastava et al., 2020), JUSTers (Fadel et al., 2020), SWAGex (Rim and Okazaki, 2020), UI (Doxolodeo and Mahendra, 2020)	0
2561	26210	26210	S20-4	Subtask C	20	207	4.0	3.0	Large-scale pretrained language models such as BART and GPT-2 dominates the submissions.	0
2562	26211	26211	S20-4	Subtask C	21	208	4.0	3.0	The two systems with the highest human evaluations, namely ANA and KaLM, use additional resources such as Wiktionary, OMCS, and other commonsense datasets.	0
2563	26212	26212	S20-4	Subtask C	22	209	4.0	3.0	This again shows that additional knowledge from structured databases can help with the generation of the reasons.	0
2564	26213	26213	S20-4	Subtask C	23	210	4.0	3.0	From Table 6 we can see that BLEU does not correlate well with Human Evaluation, especially for the top-performing systems.	0
2565	26214	26214	S20-4	Subtask C	24	211	4.0	3.0	"According to a further experiment of BUT-FIT, the naive baseline of ""copying source sentence as the reason"" can give a BLEU of 17.23, which can rank No. 4 among all the submissions."	0
2566	26215	26215	S20-4	Subtask C	25	212	4.0	3.0	This indicates that BLEU, which focuses on the surface token overlap, has difficulty in evaluating the generated text reliably.	0
2567	26216	26216	S20-4	Subtask C	26	213	4.0	3.0	The top-performed system achieves the human evaluation score of 2.10, showing the power of pretrained language models, but considering the human performance of 2.58, we still have a long way to go to generate human acceptable reasons.	0
2568	26217	26217	S20-4	Related Work	1	214	1.0	3.0	Commonsense reasoning in natural language has been studied in different forms of tasks and has recently attracted extensive attention.	0
2569	26218	26218	S20-4	Related Work	2	215	1.0	3.0	In the Winograd Schema Challenge (WSC) (Levesque et al., 2012;	0
2570	26219	26219	S20-4	Related Work	3	216	1.0	4.0	Morgenstern and Ortiz, 2015), a model needs to solve hard co-reference resolution problems based on commonsense knowledge.	0
2571	26220	26220	S20-4	Related Work	4	217	1.0	4.0	"For example, ""The trophy would not fit in the brown suitcase because it was too big."	0
2572	26221	26221	S20-4	Related Work	5	218	1.0	4.0	"What was too big (trophy or suitcase)?"""	0
2573	26222	26222	S20-4	Related Work	6	219	1.0	4.0	The Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) emphasizes on events and consequences.	0
2574	26223	26223	S20-4	Related Work	7	220	1.0	4.0	Each question in COPA aims to find the suitable cause or result of the premise from two given alternatives.	0
2575	26224	26224	S20-4	Related Work	8	221	1.0	4.0	All premises and alternatives are simple sentences.	0
2576	26225	26225	S20-4	Related Work	9	222	1.0	4.0	"For example, the premise can be ""The man broke his toe."	0
2577	26226	26226	S20-4	Related Work	10	223	1.0	4.0	"What was the CAUSE of this?"" and the two candidate answers are ""(1)"	0
2578	26227	26227	S20-4	Related Work	11	224	1.0	4.0	"He got a hole in his sock."" and ""(2)"	0
2579	26228	26228	S20-4	Related Work	12	225	1.0	4.0	"He dropped a hammer on his foot."""	0
2580	26229	26229	S20-4	Related Work	13	226	1.0	4.0	Several subsequent datasets are inspired by COPA.	0
2581	26230	26230	S20-4	Related Work	14	227	1.0	4.0	The JHU Ordinal Common-sense Inference (JOCI) (Zhang et al., 2017) aims to label the plausibility from 5 (very likely) to 1 (impossible) of human response after a particular situation.	0
2582	26231	26231	S20-4	Related Work	15	228	2.0	4.0	Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation.	0
2583	26232	26232	S20-4	Related Work	16	229	2.0	4.0	Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused.	0
2584	26233	26233	S20-4	Related Work	17	230	2.0	4.0	Besides, our dataset is not limited to events or situations.	0
2585	26234	26234	S20-4	Related Work	18	231	2.0	4.0	It concerns a broader commonsense setting, which includes events, descriptions, assertion etc.	0
2586	26235	26235	S20-4	Related Work	19	232	2.0	4.0	Some datasets are inspired by reading comprehension.	0
2587	26236	26236	S20-4	Related Work	20	233	2.0	4.0	The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016;	0
2588	26237	26237	S20-4	Related Work	21	234	2.0	4.0	Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story.	0
2589	26238	26238	S20-4	Related Work	22	235	2.0	4.0	For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question.	0
2590	26239	26239	S20-4	Related Work	23	236	2.0	4.0	Most questions require knowledge beyond the facts mentioned in the text.	0
2591	26240	26240	S20-4	Related Work	24	237	2.0	4.0	Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want.	0
2592	26241	26241	S20-4	Related Work	25	238	2.0	4.0	Some other datasets evolve from QA problems and care more about factual commonsense knowledge.	0
2593	26242	26242	S20-4	Related Work	26	239	2.0	4.0	SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions.	0
2594	26243	26243	S20-4	Related Work	27	240	2.0	4.0	Commonsense	0
2595	26244	26244	S20-4	Related Work	28	241	2.0	4.0	QA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet. OpenBook	0
2596	26245	26245	S20-4	Related Work	29	242	3.0	4.0	QA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions.	0
2597	26246	26246	S20-4	Related Work	30	243	3.0	4.0	The AI2 Reasoning Challenge (ARC)  gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences.	0
2598	26247	26247	S20-4	Related Work	31	244	3.0	4.0	MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020).	0
2599	26248	26248	S20-4	Related Work	32	245	3.0	4.0	Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense.	0
2600	26249	26249	S20-4	Related Work	33	246	3.0	4.0	Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018;	0
2601	26250	26250	S20-4	Related Work	34	247	3.0	4.0	"Porada et al., 2019), such as ""gorilla-ride-camel""."	0
2602	26251	26251	S20-4	Related Work	35	248	3.0	4.0	"In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as ""China's territory is larger than Japan's""."	0
2603	26252	26252	S20-4	Related Work	36	249	3.0	4.0	And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017).	0
2604	26253	26253	S20-4	Related Work	37	250	3.0	4.0	"Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task ""Tom's mom become (happy)/(upset) when Tom gets high grades in the exam"" is about social and emotional common sense."	0
2605	26254	26254	S20-4	Related Work	38	251	3.0	4.0	For our first task, those statements that conforms to commonsense can also be phrased as being plausible.	0
2606	26255	26255	S20-4	Related Work	39	252	3.0	4.0	Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only.	0
2607	26256	26256	S20-4	Related Work	40	253	3.0	4.0	More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions.	0
2608	26257	26257	S20-4	Related Work	41	254	3.0	4.0	In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks.	0
2609	26258	26258	S20-4	Related Work	42	255	3.0	4.0	Atomic  presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on.	0
2610	26259	26259	S20-4	Related Work	43	256	4.0	4.0	Event2	0
2611	26260	26260	S20-4	Related Work	44	257	4.0	4.0	Mind  proposes a new corpus and task, aiming to find out the mentioned/unmentioned people's intents and reactions under various daily circumstances.	0
2612	26261	26261	S20-4	Related Work	45	258	4.0	4.0	These datasets are not directly useful for our benchmark since they focus only on a small domain.	0
2613	26262	26262	S20-4	Related Work	46	259	4.0	4.0	Concept	0
2614	26263	26263	S20-4	Related Work	47	260	4.0	4.0	Net is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004;	0
2615	26264	26264	S20-4	Related Work	48	261	4.0	4.0	Havasi et al., 2007;	0
2616	26265	26265	S20-4	Related Work	49	262	4.0	4.0	Speer and Havasi, 2013;	0
2617	26266	26266	S20-4	Related Work	50	263	4.0	4.0	Speer et al., 2017).	0
2618	26267	26267	S20-4	Related Work	51	264	4.0	4.0	Concept	0
2619	26268	26268	S20-4	Related Work	52	265	4.0	4.0	Net constructs triples using labeled edges as relations and various words and/or phrases as entities.	0
2620	26269	26269	S20-4	Related Work	53	266	4.0	4.0	It also has the sentences describing the corresponding triples.	0
2621	26270	26270	S20-4	Related Work	54	267	4.0	4.0	In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource.	0
2622	26271	26271	S20-4	Related Work	55	268	4.0	4.0	Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences.	0
2623	26272	26272	S20-4	Related Work	56	269	4.0	4.0	In our task here, we also provide training data with human annotations.	0
2624	26273	26273	S20-4	Summary	1	270	1.0	4.0	This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation.	0
2625	26274	26274	S20-4	Summary	2	271	1.0	4.0	In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences.	0
2626	26275	26275	S20-4	Summary	3	272	1.0	4.0	The task attracted around 40 participating teams, out of which 31 teams submit their system papers.	0
2627	26276	26276	S20-4	Summary	4	273	1.0	4.0	The pretrained models are shown to be very effective in Subtask A and Subtask B, but there is still a large room to improve system performances in Subtask C. Contextualized embedding such as RoBERTa and BART play a central role in the success of the top-performing models, demonstrating that such methods contain commonsense information to a good extent.	0
2628	26277	26277	S20-4	Summary	5	274	2.0	4.0	We attribute the high performance on Subtask A and B to several main reasons: 1) Subtask A is a relatively easy question by definition: a model needs only to detect a relatively less plausible content among the two candidate sentences.	0
2629	26278	26278	S20-4	Summary	6	275	2.0	4.0	2) Pretrained models are obtained on billion-words large corpora such as Wikipedia data, which help obtain commonsense knowledge (Zhou et al., 2019), which helps achieve considerably better performance.	0
2630	26279	26279	S20-4	Summary	7	276	2.0	4.0	3) As described in the annotation process, we use the sentences from OMCS to inspire crowd-sourcing workers.	0
2631	26280	26280	S20-4	Summary	8	277	2.0	4.0	The top-3 systems also use OMCS, which potentially help them to attain better performances.	0
2632	26281	26281	S20-4	Summary	9	278	2.0	4.0	4) For Subtask B, as discussed in our data analysis section, the data has some flaws in the average length and common words, which reduces the difficulty.	0
2633	26282	26282	S20-4	Summary	10	279	3.0	4.0	5) Some instances have obvious patterns.	0
2634	26283	26283	S20-4	Summary	11	280	3.0	4.0	"For example, there are tens of instances that contain ""put XXX into YYY"", and ""XXX is bigger than YYY"", making the problems simpler."	0
2635	26284	26284	S20-4	Summary	12	281	3.0	4.0	6) Hundreds of crowd-sourcing workers write instances.	0
2636	26285	26285	S20-4	Summary	13	282	3.0	4.0	"It is likely for workers to think about the shared commonsense knowledge, such as ""XXX is bigger/shorter/quicker/slower than YYY""."	0
2637	26286	26286	S20-4	Summary	14	283	4.0	4.0	We consider future works in four directions: 1) We observe that there is still a gap between machine performance and human performance in Subtask C, and the reason generation task still needs further investigation.	0
2638	26287	26287	S20-4	Summary	15	284	4.0	4.0	2) The artifacts or spurious correlations in the datasets can be further removed, e.g., by making different candidate sentences in subtask B be the same, removing instances with shared commonsense knowledge, removing artifacts in common words, and filtering out common patterns.	0
2639	26288	26288	S20-4	Summary	16	285	4.0	4.0	3) Subtask A can be turned into a more difficult form.	0
2640	26289	26289	S20-4	Summary	17	286	4.0	4.0	Instead of comparing which statement makes more sense, we can form it into a classification task, validating if one statement makes sense or not.	0
2641	26290	26290	S20-4	Summary	18	287	4.0	4.0	4) We notice that the BLEU score does not closely align with human evaluation for systems with high performances, and it is desirable to develop an auto-metric for comparing the semantic correlation between two reasons.	0
2642	26291	26291	S20-5	title	1	1	4.0	1.0	SemEval-2020 Task 5: Counterfactual Recognition	0
2643	26292	26292	S20-5	abstract	1	2	1.0	1.0	We present a counterfactual recognition (CR) task, the shared Task 5 of SemEval-2020.	0
2644	26293	26293	S20-5	abstract	2	3	1.0	1.0	Counterfactuals describe potential outcomes (consequents) produced by actions or circumstances that did not happen or cannot happen and are counter to the facts (antecedent).	0
2645	26294	26294	S20-5	abstract	3	4	2.0	1.0	Counterfactual thinking is an important characteristic of the human cognitive system; it connects antecedents and consequents with causal relations.	0
2646	26295	26295	S20-5	abstract	4	5	2.0	1.0	Our task provides a benchmark for counterfactual recognition in natural language with two subtasks.	0
2647	26296	26296	S20-5	abstract	5	6	3.0	1.0	Subtask-1 aims to determine whether a given sentence is a counterfactual statement or not.	0
2648	26297	26297	S20-5	abstract	6	7	3.0	1.0	Subtask-2 requires the participating systems to extract the antecedent and consequent in a given counterfactual statement.	0
2649	26298	26298	S20-5	abstract	7	8	4.0	1.0	During the SemEval-2020 official evaluation period, we received 27 submissions to Subtask-1 and 11 to Subtask-2.	0
2650	26299	26299	S20-5	abstract	8	9	4.0	1.0	The data, baseline code, and leaderboard can be found	0
2651	26300	26300	S20-5	Introduction	1	10	1.0	1.0	"Counterfactual statements describe events that did not happen or cannot happen, and the possible consequences had those events happened, e.g., ""if kangaroos had no tails, they would topple over"" (Lewis, 2013)."	0
2652	26301	26301	S20-5	Introduction	2	11	1.0	1.0	"By developing a connection between the antecedent (e.g., ""kangaroos had no tails"") and consequent (e.g., ""they would topple over""), based on the imagination of possible worlds, humans can naturally form some causal judgments; e.g., having tails can prevent kangaroos from toppling over."	0
2653	26302	26302	S20-5	Introduction	3	12	1.0	1.0	One can understand counterfactuals using knowledge and explore the relationship between causes and effects.	0
2654	26303	26303	S20-5	Introduction	4	13	1.0	1.0	Although we may not be able to rollback the events which have happened or make impossible events occur in the real world, we can still think of potential outcomes of alternatives.	0
2655	26304	26304	S20-5	Introduction	5	14	1.0	1.0	Counterfactual thinking is a remarkable ability of human beings and is considered by many researchers, to act as the highest level of causation in the ladder of causal reasoning.	0
2656	26305	26305	S20-5	Introduction	6	15	1.0	1.0	Even the most advanced artificial intelligence system may still be far from achieving human-like counterfactual reasoning.	0
2657	26306	26306	S20-5	Introduction	7	16	1.0	1.0	Counterfactual reasoning is an important component for AI systems in obtaining stronger capability in generalization (Pearl and Mackenzie, 2018).	0
2658	26307	26307	S20-5	Introduction	8	17	1.0	1.0	Modeling counterfactuals has been studied in many different disciplines.	0
2659	26308	26308	S20-5	Introduction	9	18	1.0	1.0	For example, research in psychology has shown that counterfactual thinking can affect human cognition and behaviors (Epstude and Roese, 2008;	0
2660	26309	26309	S20-5	Introduction	10	19	2.0	1.0	Kray et al., 2010).	0
2661	26310	26310	S20-5	Introduction	11	20	2.0	1.0	The landmark paper of (Goodman, 1947) gives a detailed analysis of counterfactual conditionals in philosophy and logistics.	0
2662	26311	26311	S20-5	Introduction	12	21	2.0	1.0	As another example, counterfactuals have also been investigated in epidemiology to reveal the relationship between certain diseases and potential risk factors for those diseases (Vandenbroucke et al., 2016;	0
2663	26312	26312	S20-5	Introduction	13	22	2.0	1.0	Krieger and Davey Smith, 2016).	0
2664	26313	26313	S20-5	Introduction	14	23	2.0	1.0	We present a counterfactual recognition (CR) task, the task of determining whether a given statement conveys counterfactual thinking or not, and further analyzing the causal relations indicated by counterfactual statements.	0
2665	26314	26314	S20-5	Introduction	15	24	2.0	1.0	In our counterfactual recognition task, we aim to model counterfactual semantics and reasoning in natural language.	1
2666	26315	26315	S20-5	Introduction	16	25	2.0	1.0	Specifically, we provide a benchmark for counterfactual recognition with two subtasks.	0
2667	26316	26316	S20-5	Introduction	17	26	2.0	1.0	Subtask-1 requires systems to determine whether a given statement is counterfactual or not.	0
2668	26317	26317	S20-5	Introduction	18	27	2.0	1.0	The counterfactual detection task can serve as a foundation for downstream counterfactual analysis.	0
2669	26318	26318	S20-5	Introduction	19	28	3.0	1.0	Subtask-2 requires systems to further locate the antecedent and consequent text spans in a given counterfactual statement, as the connection between an antecedent and consequent can reveal core causal inference clues.	0
2670	26319	26319	S20-5	Introduction	20	29	3.0	1.0	To build the dataset for counterfactual recognition, we extract over 60,000 candidate counterfactual statements by scanning through news reports in three domains: finance, politics, and healthcare.	0
2671	26320	26320	S20-5	Introduction	21	30	3.0	1.0	The first round of annotation focuses on labeling each sample as true or false, where true denotes a sample is counterfactual and false otherwise in Subtask-1.	0
2672	26321	26321	S20-5	Introduction	22	31	3.0	1.0	A portion of samples labeled as true will be further used in Subtask-2 to detect the text spans that describe the antecedent and consequent.	0
2673	26322	26322	S20-5	Introduction	23	32	3.0	1.0	Specifically, we carefully select 20,000 high-quality samples from the 60,000 statements and use them in Subtask-1, with 13,000 (65%) as the training set and the rest for testing.	0
2674	26323	26323	S20-5	Introduction	24	33	3.0	1.0	The dataset for Subtask-2 contains 5,501 samples, among which we use 3,551 (65%) for training and the rest for testing.	0
2675	26324	26324	S20-5	Introduction	25	34	3.0	1.0	To achieve a decent performance in our shared task, we expect the systems should have a certain level of language understanding capacity in both semantics and syntax, together with a certain level of commonsense reasoning ability.	0
2676	26325	26325	S20-5	Introduction	26	35	3.0	1.0	In Subtask-1, the top-ranked submissions all use pre-trained neural models, which appear to be an effective way to integrate knowledge learned from large corpus.	0
2677	26326	26326	S20-5	Introduction	27	36	3.0	1.0	All of these models use neural networks, which further confirms the effectiveness of distributed representation and subsymbolic approaches for this task.	0
2678	26327	26327	S20-5	Introduction	28	37	4.0	1.0	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural networks with symbolic approaches.	0
2679	26328	26328	S20-5	Introduction	29	38	4.0	1.0	The first-place model also utilizes data augmentation to further improve system performance.	0
2680	26329	26329	S20-5	Introduction	30	39	4.0	1.0	In Subtask-2, top systems take two main approaches: sequence labelling or question answering.	0
2681	26330	26330	S20-5	Introduction	31	40	4.0	1.0	Same as systems in Subtask-1, all of them benefit from pre-training.	0
2682	26331	26331	S20-5	Introduction	32	41	4.0	1.0	We will provide a more detailed analysis in the system and result section.	0
2683	26332	26332	S20-5	Introduction	33	42	4.0	1.0	We built a dataset for this shared task from scratch.	0
2684	26333	26333	S20-5	Introduction	34	43	4.0	1.0	Our data, baseline code, and leaderboard can be found at https://competitions.codalab.org/competitions/21691.	0
2685	26334	26334	S20-5	Introduction	35	44	4.0	1.0	The data and baseline code are also available at https://zenodo.org/record/3932442.	0
2686	26335	26335	S20-5	Introduction	36	45	4.0	1.0	In general, our task here is a relatively basic one in counterfactual analysis in natural language.	0
2687	26336	26336	S20-5	Introduction	37	46	4.0	1.0	We hope it will intrigue and facilitate further research on counterfactual analysis and can benefit other related downstream tasks.	0
2688	26337	26337	S20-5	Task Setup	1	47	2.0	1.0	In this section, we detail the two counterfactual recognition subtasks and the metrics used to evaluate the performance.	0
2689	26338	26338	S20-5	Task Setup	2	48	4.0	1.0	During the evaluation, participants can work on both subtasks or any one of them.	0
2690	26339	26339	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	1	49	1.0	1.0	We formulate the Subtask-1 as a binary classification problem which asks the participating systems to detect whether a particular sentence is counterfactual or not.	0
2691	26340	26340	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	2	50	2.0	1.0	Below are two examples of counterfactual statements that need to be recognized:	0
2692	26341	26341	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	3	51	3.0	1.0	• Example-1: Officials say if they had authority to shut non-bank firms, the collapse of Lehman Brothers, which touched off the most virulent phase of the credit crisis, could have been avoided.	0
2693	26342	26342	S20-5	Subtask-1: Recognizing Counterfactual Statements (RCS)	4	52	4.0	1.0	• Example-2: The delivery numbers would have been high had it not been for the restrictions imposed by the military for security reasons.	0
2694	26343	26343	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	1	53	1.0	1.0	Indicating causal relationships is an inherent characteristic of counterfactuals.	0
2695	26344	26344	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	2	54	1.0	1.0	To further detect the causal knowledge conveyed in counterfactual statements, Subtask-2 aims to extract the antecedents and consequents.	0
2696	26345	26345	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	3	55	2.0	1.0	Specifically, given a counterfactual statement, systems for Subtask-2 need to identify the indices of the characters which indicate the start and end positions for antecedent and consequent, in terms of character indices: antecedent start ind, antecedent end ind, consequent start ind, consequent end ind.	0
2697	26346	26346	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	4	56	2.0	1.0	For some statements, the consequents may not be expressed in the statements, then the corresponding consequent start ind and consequent end ind will be set as −1.	0
2698	26347	26347	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	5	57	2.0	1.0	• Example-3: The delivery numbers would have been high had it not been for the restrictions imposed by the military for security reasons.	0
2699	26348	26348	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	6	58	3.0	2.0	Antecedent: had it not been for the restrictions imposed by the military for security reasons Consequent: the delivery numbers would have been high Label: 42, 122, 0, 40 A counterfactual statement can be converted to a contrapositive with a true antecedent and consequent, by assuming the antecedent and consequent in the original counterfactual statement is inalterably false (Goodman, 1947).	0
2700	26349	26349	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	7	59	3.0	2.0	Consider the Example-3 above.	0
2701	26350	26350	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	8	60	4.0	2.0	"It can be transposed into ""since the restrictions imposed by the military for security reasons, the delivery numbers were not high""."	0
2702	26351	26351	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	9	61	4.0	2.0	After extracting the antecedent and the corresponding consequent from a counterfactual statement, we may derive a contrapositive by performing an appropriate transformation, which can naturally reveal a causal relationship between the two parts or even further indicate the properties of each part.	0
2703	26352	26352	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	10	62	4.0	2.0	In this way, it is possible to extract causal knowledge across corpora.	0
2704	26353	26353	S20-5	Evaluation Metrics	1	63	1.0	2.0	Subtask-1 is a binary classification problem evaluated with Precision, Recall, and F1 score 1 .	0
2705	26354	26354	S20-5	Evaluation Metrics	2	64	2.0	2.0	In Subtask-2, we utilize two metrics: (i) Exact match is used to evaluate the percentage of predictions that exactly match the ground truth boundaries of the antecedents and consequents. (ii) F1 score is used to measure the overlap between the predictions and ground truth spans.	0
2706	26355	26355	S20-5	Evaluation Metrics	3	65	2.0	2.0	For each sample, we calculate the number of tokens in the overlapped intervals by comparing the predictions and ground truth indices of antecedent and consequent boundaries.	0
2707	26356	26356	S20-5	Evaluation Metrics	4	66	3.0	2.0	Then we can compute precision, recall, and F1 score for each sample.	0
2708	26357	26357	S20-5	Evaluation Metrics	5	67	3.0	2.0	We take the average F1 score across all the samples in the test set.	0
2709	26358	26358	S20-5	Evaluation Metrics	6	68	4.0	2.0	Note the F1 score used in both subtasks is calculated as: F 1 = 2 * P recision * Recall P recision+	0
2710	26359	26359	S20-5	Evaluation Metrics	7	69	4.0	2.0	Recall .	0
2711	26360	26360	S20-5	Data Development	1	70	2.0	2.0	We develop our dataset from news articles in the finance, politics, or healthcare domain.	0
2712	26361	26361	S20-5	Data Development	2	71	3.0	2.0	The data development consists of data collection and annotation.	0
2713	26362	26362	S20-5	Data Development	3	72	4.0	2.0	We use different approaches to ensure the quality of the data.	0
2714	26363	26363	S20-5	Data Collection	1	73	1.0	2.0	There are two major challenges in our data construction process.	0
2715	26364	26364	S20-5	Data Collection	2	74	1.0	2.0	First, due to the relative sparsity of counterfactual statements in the text, manually annotating each sentence in the original text is not of time and financial efficiency.	0
2716	26365	26365	S20-5	Data Collection	3	75	1.0	2.0	Accordingly, we perform a filtering step to narrow down candidates.	0
2717	26366	26366	S20-5	Data Collection	4	76	2.0	2.0	The second challenge is rooted in the flexibility and complexity of counterfactual expressions.	0
2718	26367	26367	S20-5	Data Collection	5	77	2.0	2.0	"Not all counterfactual statements follow certain patterns, e.g., the ""if + past perfect"" pattern (although this is a good pattern which can indicate a conditional relationship between the antecedent and the potential consequent)."	0
2719	26368	26368	S20-5	Data Collection	6	78	2.0	2.0	To solve these problems, we create a set of templates considering the trade-off between the effectiveness of filtering and its diversity in finding candidate counterfactuals, without making the filtering stage too rigorous.	0
2720	26369	26369	S20-5	Data Collection	7	79	3.0	2.0	Token-based Filtering	0
2721	26370	26370	S20-5	Data Collection	8	80	3.0	2.0	The template set consists of two subsets that jointly work to find candidate potential counterfactual statements when they are used to search through news articles.	0
2722	26371	26371	S20-5	Data Collection	9	81	3.0	2.0	The first subset focuses on word token patterns and the second subset leverages POS tag-based patterns.	0
2723	26372	26372	S20-5	Data Collection	10	82	4.0	2.0	The full list of token-based patterns are listed in Appendix A. Some of the patterns are based on the previous research which revealed common counterfactual constructions (Hobbs, 2005;	0
2724	26373	26373	S20-5	Data Collection	11	83	4.0	2.0	Son et al., 2017;	0
2725	26374	26374	S20-5	Data Collection	12	84	4.0	2.0	Rouvoli et al., 2019).	0
2726	26375	26375	S20-5	POS-based Filtering	1	85	1.0	2.0	The second subset of templates utilize patterns based on part-of-speech tags.	0
2727	26376	26376	S20-5	POS-based Filtering	2	86	2.0	2.0	We identified five counterfactual forms based on (Janocko et al., 2016) and coverted them into POS-based patterns to increase the chances of identifying true counterfactual statements.	0
2728	26377	26377	S20-5	POS-based Filtering	3	87	2.0	2.0	The details of the POSbased rules are presented in Appendix B.	0
2729	26378	26378	S20-5	POS-based Filtering	4	88	3.0	2.0	To apply the rules, we tokenize each sentence and conduct POS tagging with the NLTK library (Bird et al., 2009).	0
2730	26379	26379	S20-5	POS-based Filtering	5	89	4.0	2.0	Then we extract the sentences which match one of the pre-defined patterns.	0
2731	26380	26380	S20-5	POS-based Filtering	6	90	4.0	2.0	By applying both the token-based and POS-based rules, we obtain the candidate statements for further human annotation.	0
2732	26381	26381	S20-5	Annotation	1	91	1.0	2.0	As described above, each sample in Subtask-1 is labeled either as true (counterfactual) or false (noncounterfactual).	0
2733	26382	26382	S20-5	Annotation	2	92	1.0	2.0	We employ a two-step annotation strategy.	0
2734	26383	26383	S20-5	Annotation	3	93	2.0	2.0	First, each sample in the candidate statement set is annotated by five annotators to determine whether it is a counterfactual statement or not.	0
2735	26384	26384	S20-5	Annotation	4	94	2.0	2.0	We include those annotated as true (counterfactuals) by all five annotators, i.e., with an agreement rate of 100%.	0
2736	26385	26385	S20-5	Annotation	5	95	3.0	2.0	For negative samples (non-counterfactual statements), we take all of those labeled as false with 100% agreement and some sentences with 80% agreement, which 4 out of the 5 annotators label as false.	0
2737	26386	26386	S20-5	Annotation	6	96	3.0	2.0	We use the Amazon Mechanical Turk (AMT) platform for our annotation, by splitting the samples into HITs (Human Intelligent Task, where each HIT contains 20 to 30 samples) and distributing these HITs to qualified annotators along with thorough instructions and examples.	0
2738	26387	26387	S20-5	Annotation	7	97	4.0	2.0	In subtask 2, a portion of counterfactual statements (labeled as true in Subtask-1) are further annotated, in which the text spans of antecedents and consequents in counterfactual statements are obtained.	0
2739	26388	26388	S20-5	Annotation	8	98	4.0	2.0	In this stage, each sample was annotated by a single annotator on Amazon Mechanical Turk, and the annotators were asked to double-check whether the sample is a counterfactual statement before underlining specific spans.	0
2740	26389	26389	S20-5	Annotation	9	99	4.0	2.0	All the samples are further manually checked by ourselves to ensure the antecedent and consequent spans are appropriately labelled by a consistent standard.	0
2741	26390	26390	S20-5	Quality Control	1	100	1.0	2.0	We further make the following efforts to control the quality of our datasets.	0
2742	26391	26391	S20-5	Quality Control	2	101	1.0	2.0	First we set additional requirements when inviting workers to perform annotation.	0
2743	26392	26392	S20-5	Quality Control	3	102	2.0	2.0	We only invite workers from English-speaking countries and only if the approval rates of their previous HITs are above 82%.	0
2744	26393	26393	S20-5	Quality Control	4	103	2.0	2.0	In addition, workers take a qualification test before starting their annotation work.	0
2745	26394	26394	S20-5	Quality Control	5	104	3.0	2.0	The test provides detailed instructions and examples and includes 40 samples for workers to label which of the samples are counterfactuals.	0
2746	26395	26395	S20-5	Quality Control	6	105	3.0	2.0	Using this method, we have over 70 qualified workers for our data annotation task.	0
2747	26396	26396	S20-5	Quality Control	7	106	4.0	2.0	Having a stable pool of trained workers is beneficial for ensuring the quality of annotation.	0
2748	26397	26397	S20-5	Quality Control	8	107	4.0	2.0	In the entire process of annotation, we randomly select some HITs to evaluate the accuracy and the performance of workers to justify whether to accept them or not.	0
2749	26398	26398	S20-5	Quality Control	9	108	4.0	2.0	For subtask 2, we manually check all of the samples to ensure: (i) the samples are counterfactual statements (any incorrectly labelled statements are further removed from both Subtask-1 and Subtask-2 datasets); (ii) for a very small number of statements, if the antecedent and consequent spans labelled by the Turkers are not full constituent phrases, we manually adjust the span to make them full phrases.	0
2750	26399	26399	S20-5	Data Statistics	1	109	1.0	2.0	Table 1 shows the statistics of the data used in Subtask-1.	0
2751	26400	26400	S20-5	Data Statistics	2	110	2.0	2.0	We obtain 20,000 statements in total and we randomly split them into the training (65%) and test set (35%    2 shows the size of data used in Subtask-2.	0
2752	26401	26401	S20-5	Data Statistics	3	111	2.0	2.0	In total, we have 5,501 samples and randomly split them into the training and test set.	0
2753	26402	26402	S20-5	Data Statistics	4	112	3.0	2.0	Specifically, 3,551 samples are for training and the rest for testing.	0
2754	26403	26403	S20-5	Data Statistics	5	113	3.0	2.0	Not all counterfactuals have both an antecedent and a consequent, so we also provide statistics for samples that have only antecedents, and those that have both antecedents and consequents.	0
2755	26404	26404	S20-5	Data Statistics	6	114	4.0	2.0	Figure 1 in Appendix C shows statistics for the frequency of the number of words in both the Subtask-1 and Subtask-2 training and test datasets.	0
2756	26405	26405	S20-5	Data Statistics	7	115	4.0	3.0	4 Systems and Results	0
2757	26406	26406	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	1	116	1.0	3.0	The baseline used for Subtask-1 is a simple SVM classifier with the linear kernel function.	0
2758	26407	26407	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	2	117	1.0	3.0	In the baseline model, we first take some basic preprocessing steps starting with lemmatization; we then extract term frequency and inverse document frequency (tf-idf) features for training the SVM model to perform binary classification.	0
2759	26408	26408	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	3	118	1.0	3.0	"The motivation behind using this simple SVM as a baseline is to create a simple model that can identify counterfactuals by learning and searching for the presence keywords and phrases like ""had"" or ""should have been"", that tend to mark the presence of a counterfactual in a number of counterfactual grammatical forms."	0
2760	26409	26409	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	4	119	1.0	3.0	The baseline has poor performance, signalling that most counterfactuals cannot be determined based on the presence of certain words and that reasoning is necessary.	0
2761	26410	26410	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	5	120	1.0	3.0	The baseline is not shown on the official leaderboard but in Table 3.	0
2762	26411	26411	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	6	121	1.0	3.0	We received 27 submissions to Subtask-1.	0
2763	26412	26412	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	7	122	1.0	3.0	Table 3 shows all the official submission results and nearly all of them exceed the performance of the provided baseline model.	0
2764	26413	26413	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	8	123	1.0	3.0	The top-ranked submissions all use pre-trained neural models, which have achieved the state-of-the-art results across many natural language processing (NLP) tasks (Devlin et al., 2018;	0
2765	26414	26414	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	9	124	1.0	3.0	Radford et al., 2018;Radford et al., 2019;	0
2766	26415	26415	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	10	125	1.0	3.0	Yang et al., 2019;	0
2767	26416	26416	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	11	126	1.0	3.0	Liu et al., 2019;	0
2768	26417	26417	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	12	127	1.0	3.0	Lan et al., 2019), which we believe is an effective way to integrate additional external knowledge that does not exist in the training data, including common sense.	0
2769	26418	26418	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	13	128	1.0	3.0	Some participants like shngt experimented with classic machine learning methods like SVM and gradient boosted random forests, and found that model performance plateaued at an F1 score of around 60 percent (Anil Ojha et al., 2020), showing that these methods cannot capture counterfactual reasoning as well as the pre-trained models.	0
2770	26419	26419	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	14	129	2.0	3.0	One team, Serena, use a non-transformer approach, basing their system on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and a Pooling operation is done for dimensionality reduction (Ou et al., 2020).	0
2771	26420	26420	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	15	130	2.0	3.0	Their system struggles with data imbalance and they conclude that transformer networks can improve their performance.	0
2772	26421	26421	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	16	131	2.0	3.0	Among the top 8 competitors, BERT and RoBERTa based systems are most popular, being used as the primary models or as a part of their final ensemble in 5 and 4 of the top 8 participants' systems respectively.	0
2773	26422	26422	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	17	132	2.0	3.0	XLNet and ALBERT are less popular choices, but they are also used in the first and second of the top 8 participating systems respectively.	0
2774	26423	26423	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	18	133	2.0	3.0	In addition, the top models adopt ensemble strategies, and in most cases, achieve better performance than that of individual classifiers.	0
2775	26424	26424	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	19	134	2.0	3.0	One of the teams, shgnt, also found using a convolutional neural network model with GloVe embeddings in their ensemble helped to enhance it (Anil Ojha et al., 2020), showing that non-transformer network based methods can be useful.	0
2776	26425	26425	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	20	135	2.0	3.0	Despite the commonality of using pre-trained models, many of the participating systems differ in the structures they add above the pre-trained models, to capture additional information.	0
2777	26426	26426	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	21	136	2.0	3.0	Rather than adding a fully connected layer on top, some competitors reconstruct the top structure of the pre-trained models or add a neural network on top.	0
2778	26427	26427	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	22	137	2.0	3.0	For example, to capture local patterns in counterfactual statements, Roger (Lu et al., 2020) and shngt (Anil Ojha et al., 2020) add a CNN before classification in some of their systems.	0
2779	26428	26428	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	23	138	2.0	3.0	Similarly, Baiyang2581 experiments with this upper structure and use a bidirectional GRU and bidirectional LSTM in some of their systems after the transformer network (Bai and Zhou, 2020).	0
2780	26429	26429	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	24	139	2.0	3.0	In contrast to modifying the upper structure of transformer networks, the fifth-place team, lenyabloko, uses rule-based specialist modules by combining fine-tuned pre-trained models with constituency and dependency parsers to compensate for deficiencies in deep learning methods for causal inference in language to great effect (Yabloko, 2020).	0
2781	26430	26430	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	25	140	2.0	3.0	The dataset is highly imbalanced in favour of non-counterfactuals, and many participants use techniques to deal with this imbalance.	0
2782	26431	26431	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	26	141	2.0	3.0	A range of techniques like pseudo-labelling used by haodingkui (Xiao et al., 2020), multi sample dropout used by Ferryman (Chen et al., 2020), and oversampling and undersampling used by some other teams, notably ad6398 who found that undersampling non-counterfactuals optimized the performance of their models . changshivek experimented with 2 novel forms of data augmentation to increase the number of counterfactual samples (Liu and Yu, 2020).	0
2783	26432	26432	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	27	142	3.0	3.0	The first was back translation, which involves taking counterfactual samples and translating them into another language and then translating them back in English and adding them to the dataset.	0
2784	26433	26433	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	28	143	3.0	3.0	"The second technique was Easy Data Augmentation (EDA), namely synonym replacement of words in the counterfactual samples while making sure to preserve words relating to counterfactuals like ""should""."	0
2785	26434	26434	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	29	144	3.0	3.0	Back translation yielded poor performance, but decent improvement was seen when using EDA showing that this could be a viable method.	0
2786	26435	26435	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	30	145	3.0	3.0	These methods are all done to combat overfitting.	0
2787	26436	26436	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	31	146	3.0	3.0	K-fold cross validation is also a common strategy utilized by many of the participating systems to deal with the relatively small, highly imbalanced dataset to reduce some of the bias.	0
2788	26437	26437	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	32	147	3.0	3.0	Lastly, many teams experimented with pre-processing the data.	0
2789	26438	26438	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	33	148	3.0	3.0	Baiyang251 notes that minimal preprocessing (e.g. deleting punctuation, making sentences all lower-case) on the data yields the best results as they theorize removing these results in the loss of information useful for prediction (Bai and Zhou, 2020).	0
2790	26439	26439	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	34	149	3.0	3.0	Other groups note that extensive pre-processing does not yield notable performance improvements either and can even slightly hurt performance.	0
2791	26440	26440	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	35	150	3.0	3.0	The best F1 score in Subtask-1, 90.9%, was achieved by haodingkui (Xiao et al., 2020).	0
2792	26441	26441	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	36	151	3.0	3.0	In their approach, a pseudo-labelling strategy is used to generate more data to alleviate overfitting during training.	0
2793	26442	26442	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	37	152	3.0	3.0	In this strategy, if all classifiers agree on the labels of certain samples in the test set, then those samples will also be used for training.	0
2794	26443	26443	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	38	153	3.0	3.0	For model ensembling, they incorporate BERT, RoBERTa, and XLNet.	0
2795	26444	26444	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	39	154	3.0	3.0	The second-place system from josefjon concludes that using an ensemble of RoBERTa large models performs better than any other pre-trained model (Fajcik et al., 2020).	0
2796	26445	26445	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	40	155	4.0	3.0	The third-place system, proposed by Roger, incorporates convolutional neural networks to capture strong local context information in addition to fine-tuning pre-trained models.	0
2797	26446	26446	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	41	156	4.0	3.0	Furthermore, it theorizes about the effectiveness of using knowledge-enriched transformers to improve performance on the task (Lu et al., 2020).	0
2798	26447	26447	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	42	157	4.0	3.0	Some top systems also successfully incorporate rules to further improve the performance, suggesting the benefits of combining neural nets with symbolic approaches.	0
2799	26448	26448	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	43	158	4.0	3.0	Further Analysis and Challenges	0
2800	26449	26449	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	44	159	4.0	3.0	In general, one of the main challenges of Subtask-1 is that identifying counterfactuals requires inference and reasoning based on common sense and knowledge.	0
2801	26450	26450	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	45	160	4.0	3.0	Particularly, the fact that counterfactuals often do not follow specific grammatical rules makes such an ability important for some statements.	0
2802	26451	26451	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	46	161	4.0	3.0	The imbalanced nature of the dataset in Subtask-1 is another challenge; therefore different methods have been proposed to address this issue such as over-sampling and under-sampling.	0
2803	26452	26452	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	47	162	4.0	3.0	Some of the top models also try different methods of data augmentation so that they can have more positive examples to tackle the imbalance issue.	0
2804	26453	26453	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	48	163	4.0	3.0	By inspecting more details of submitted predictions of top systems, we found most of the wrongly classified samples require systems to understand the statement better while the existing models often lean toward memorizing and overweighting token level features to make predictions.	0
2805	26454	26454	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	49	164	4.0	3.0	"Take a counterfactual sentence as an example, ""if I were asked to, I would be happy to talk to anyone""."	0
2806	26455	26455	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	50	165	4.0	3.0	"This is misclassified likely because of including ""were...to"" in the antecedent, which is highly correlated to non-counterfactual statements, suggesting a major flaw of existing methods."	0
2807	26456	26456	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	51	166	4.0	3.0	Similarly, some non-counterfactual sentences are incorrectly labelled for they include some token-level counterfactual features while not indicating counterfactuals.	0
2808	26457	26457	S20-5	Subtask 1: Recognizing Counterfactual Statements (RCS)	52	167	4.0	3.0	"For example, the sentence ""under the current alignment, he said, American multinational corporations like Pfizer might invest more money in the United States, not less, if they had their tax domiciles abroad"" is a non-counterfactual sentence for it is not assuming anything counter to the facts, while ""if had"" part along with the modal verb (""might"" in this case) in the same sentence is usually correlated to a counterfactual."	0
2809	26458	26458	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	1	168	1.0	3.0	We build a conditional random field (CRF) model for sequence labeling as the baseline model for Subtask-2.	0
2810	26459	26459	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	2	169	1.0	3.0	This model can assign labels to each token in the input sequence by taking advantage of all input tokens and previous predictions.	0
2811	26460	26460	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	3	170	1.0	3.0	Specifically, same as in many name entity recognition systems, this baseline model annotates the antecedent and consequent using the B/I/O scheme, marking whether a word is at the Beginning, Inside or Outside either the antecedent or consequent.	0
2812	26461	26461	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	4	171	1.0	3.0	A common set of features for each word are extracted and used to train this model, including POS tags, features of nearby words, and whether the word has an uppercase/lowercase/title flag.	0
2813	26462	26462	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	5	172	1.0	4.0	The performance of the CRF baseline can be found in Table 4.	0
2814	26463	26463	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	6	173	1.0	4.0	The performance of the baseline model and submitted systems are shown in Table 4.	0
2815	26464	26464	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	7	174	2.0	4.0	We received 11 official submissions to Subtask-2, and most of the submissions outperform the provided baseline model.	0
2816	26465	26465	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	8	175	2.0	4.0	In subtask 2, top systems take two main approaches: sequence labelling or question answering, and nearly all of them benefit from pretraining.	0
2817	26466	26466	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	9	176	2.0	4.0	An exception is the 6th ranked team, Anderson Sung, that use a multi-stack, birdirectional LSTM architecture to some success (Sung et al., 2020), showing that non-transformer approaches are viable for the task.	0
2818	26467	26467	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	10	177	2.0	4.0	Similarly, habi-akl experiments with a BiLSTM Conditional Random Fields (CRF) model for Subtask-2, but find that a BERT based model with a multilayer perceptron classifier outperforms the LSTM and conclude that the semi-supervised systems show a better level of understanding of challenging counterfactual forms (Abi Akl et al., 2020).	0
2819	26468	26468	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	11	178	2.0	4.0	One approach among the top models formulates the problem as an extractive question answering (QA) task, with the target being extracting the answer from the given context towards a specific question.	0
2820	26469	26469	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	12	179	2.0	4.0	The others formulate the task as a sequence labeling task.	0
2821	26470	26470	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	13	180	3.0	4.0	In the top 4 systems, half of the teams took the QA approach, and the other half took the sequence labelling approach.	0
2822	26471	26471	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	14	181	3.0	4.0	The choice between BERT and RoBERTa has split almost evenly amongst most of the participants.	0
2823	26472	26472	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	15	182	3.0	4.0	As is the case for Subtask-1, many teams sought to build on top of the pre-trained models and add additional upper layer structures to handle the task better.	0
2824	26473	26473	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	16	183	3.0	4.0	The best results are achieved by team Martin, with an F1 score of 88.2 and an exact match score of 57.5 (Fajcik et al., 2020).	0
2825	26474	26474	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	17	184	3.0	4.0	To predict the start and ending positions of antecedents and consequents, the model utilizes an ensemble of RoBERTa models and extend it in the same manner as how BERT was extended for the SQuAD dataset (Rajpurkar et al., 2016).	0
2826	26475	26475	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	18	185	3.0	4.0	The second-place system pouria babvey uses a sequence labelling approach: the authors develop the model on top of BERT with a multi-head attention layer and label masking to capture mutual information between nearby labels (Babvey et al., 2020).	0
2827	26476	26476	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	19	186	4.0	4.0	Label masking, in which only part of the labels is fed during training and the rest have to be predicted, has shown to be particularly effective for improving accuracy, which can be seen as a form of regularization.	0
2828	26477	26477	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	20	187	4.0	4.0	In addition, a multi-stage algorithm is used to gradually improve certainty in predictions after each step.	0
2829	26478	26478	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	21	188	4.0	4.0	The third-place system, Roger, formulates the problem as a query-based question answering problem, where antecedents and consequents are extracted after an antecedent and consequent query are supplied along with the original statement into BERT.	0
2830	26479	26479	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	22	189	4.0	4.0	Pointer networks are further used to predict the start and ending positions (Lu et al., 2020) .	0
2831	26480	26480	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	23	190	4.0	4.0	A unique approach for Subtask-2 is used by 7th placed team, rajaswa patil, where they use a base architecture for both subtasks.	0
2832	26481	26481	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	24	191	4.0	4.0	They first train with a binary-classification module for Subtask-1, then replace it with a regression-module and further fine-tune the system for Subtask-2 (Patil and Baths, 2020), leveraging the commonality between the two tasks.	0
2833	26482	26482	S20-5	Subtask-2: Detecting Antecedent and Consequent (DAC)	25	192	4.0	4.0	We can observe that there is still a gap between the performance of exact match and F1, which is mainly due to the fact that Exact Match is sensitive to non-essential phrases in predictions even the core parts are identified correctly.	0
2834	26483	26483	S20-5	Related Work	1	193	1.0	4.0	Modelling counterfactual thinking has started to attract more interest.	0
2835	26484	26484	S20-5	Related Work	2	194	1.0	4.0	One of the previous works closest to ours is (Son et al., 2017), in which a small-scale counterfactual tweet dataset is collected from social media.	0
2836	26485	26485	S20-5	Related Work	3	195	1.0	4.0	There are three main differences between that dataset and ours.	0
2837	26486	26486	S20-5	Related Work	4	196	1.0	4.0	First, there are only 2,000 samples in the tweet dataset (including the supplement data mentioned in the paper), while our dataset for counterfactual detection in Subtask-1 is ten times larger, which we believe is important for training deep learning based models.	0
2838	26487	26487	S20-5	Related Work	5	197	2.0	4.0	Second, our benchmark provides evaluation for antecedents and consequents extraction, which are essential components of counterfactual analysis.	0
2839	26488	26488	S20-5	Related Work	6	198	2.0	4.0	Third, our dataset includes statements from three different domains (finance, politics, healthcare).	0
2840	26489	26489	S20-5	Related Work	7	199	2.0	4.0	In contrast to the statements collected from tweets, which have a very large portion that are open-ended, vague thoughts, the counterfactuals in our dataset are more meaningful domain-related statements.	0
2841	26490	26490	S20-5	Related Work	8	200	2.0	4.0	There is another dataset TIMETRAVEL proposed in (Qin et al., 2019)  in which given a short story and an alternative counterfactual event context, the story needs to be minimally revised to keep compatible with the intervening counterfactual event.	0
2842	26491	26491	S20-5	Related Work	9	201	3.0	4.0	The empirical results show that it is still challenging for current neural language models to perform well on the counterfactual story rewriting task due to the lack of counterfactual reasoning capabilities.	0
2843	26492	26492	S20-5	Related Work	10	202	3.0	4.0	In a broader viewpoint, counterfatuals are an important form of causal reasoning.	0
2844	26493	26493	S20-5	Related Work	11	203	3.0	4.0	Researchers argue that the notion of counterfactuals is essential for causal reasoning, in which causal modeling is proposed to interpret counterfactual conditionals in natural language, and such work has been discussed since the possible worlds semantics developed in the 1970s (Lewis, 2013;	0
2845	26494	26494	S20-5	Related Work	12	204	3.0	4.0	Lewis, 1986).	0
2846	26495	26495	S20-5	Related Work	13	205	4.0	4.0	The more recent work renders useful insights by formulating causal inference as a three-level hierarchy, which are association, intervention, and counterfactual, respectively (Pearl and Mackenzie, 2018;Pearl, 2019).	0
2847	26496	26496	S20-5	Related Work	14	206	4.0	4.0	"The top of the hierarchy is counterfactual-if a model can correctly answer counterfactual queries like ""what would happen if we had acted differently"", it should also be able to answer association and intervention queries."	0
2848	26497	26497	S20-5	Related Work	15	207	4.0	4.0	The research in (Pearl, 1995;	0
2849	26498	26498	S20-5	Related Work	16	208	4.0	4.0	Pearl, 2010) also made contributions to a general theory of causal inference, which is based on the Structural Causal Model (SCM), and counterfactual analysis is provided with a formal mathematical formalism.	0
2850	26499	26499	S20-5	Summary and Future Work	1	209	1.0	4.0	We present a counterfactual recognition task that includes two basic subtasks.	0
2851	26500	26500	S20-5	Summary and Future Work	2	210	1.0	4.0	Subtask-1 evaluates whether a given statement is counterfactual or not with 20,000 training and test statements.	0
2852	26501	26501	S20-5	Summary and Future Work	3	211	2.0	4.0	Subtask-2 aims at recognizing antecedents and consequents in counterfactual statements.	0
2853	26502	26502	S20-5	Summary and Future Work	4	212	2.0	4.0	The official task received 27 submissions to Subtask-1 and 11 submissions for Subtask-2.	0
2854	26503	26503	S20-5	Summary and Future Work	5	213	3.0	4.0	The state-of-the-art performances achieved a 90% F1 score in Subtask 1, as well as an 88.2% F1 and 57.5% Exact Match score in Subtask-2.	0
2855	26504	26504	S20-5	Summary and Future Work	6	214	3.0	4.0	We hope this task and dataset will intrigue and facilitate further research on counterfactual analysis in natural language.	0
2856	26505	26505	S20-5	Summary and Future Work	7	215	4.0	4.0	4. Wish/Should Implied: The Wish/Should Implied counterfactual form only explicitly contains an antecedent in the sentence, with the consequent being implied, and it must contain an independent clause following a wish or should (Janocko et al., 2016).	0
2857	26506	26506	S20-5	Summary and Future Work	8	216	4.0	4.0	To capture this form, sentences that contain the token 'wish' and that have a word after this with a past tense verb or a past participle verb tag.	0
2858	26507	26507	S20-5	Verb Inversion:	1	217	1.0	4.0	The category has two specific forms that differ in if the antecedent presents before or after the consequent.	0
2859	26508	26508	S20-5	Verb Inversion:	2	218	1.0	4.0	In either case, according to (Janocko et al., 2016), the antecedent contains a had or were inversion along with a past tense verb, and the consequent has a modal verb and a past or present tense verb.	0
2860	26509	26509	S20-5	Verb Inversion:	3	219	1.0	4.0	(a)	0
2861	26510	26510	S20-5	Verb Inversion:	4	220	2.0	4.0	The antecedent presents first in this case.	0
2862	26511	26511	S20-5	Verb Inversion:	5	221	2.0	4.0	Thus, for this form the sentence first has to contain had or were as the first token.	0
2863	26512	26512	S20-5	Verb Inversion:	6	222	2.0	4.0	After this, a token with a past tense or past participle verb token must be present, but only if the first token is had.	0
2864	26513	26513	S20-5	Verb Inversion:	7	223	3.0	4.0	In either case, a word with a modal verb tag has to follow, and then further followed by a base verb, present non third person singular verb, present third person singular verb, or a past tense verb tag.	0
2865	26514	26514	S20-5	Verb Inversion:	8	224	3.0	4.0	(b)	0
2866	26515	26515	S20-5	Verb Inversion:	9	225	3.0	4.0	In this form a consequent presents first.	0
2867	26516	26516	S20-5	Verb Inversion:	10	226	4.0	4.0	As a result, a word with a modal verb tag must follow, and is in turn followed by a word with base verb, present non third person singular verb, present third person singular verb, or a past tense verb tag.	0
2868	26517	26517	S20-5	Verb Inversion:	11	227	4.0	4.0	After this the sentence had to contain a had or were token.	0
2869	26518	26518	S20-5	Verb Inversion:	12	228	4.0	4.0	If it does contain a had token, an additional past tense or past participle verb token word has to also follow it.	0
2870	27902	27902	S20-12	title	1	1	2.0	1.0	SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (Offens	0
2871	27903	27903	S20-12	title	2	2	4.0	1.0	Eval 2020)	0
2872	27904	27904	S20-12	abstract	1	3	1.0	1.0	We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020).	0
2873	27905	27905	S20-12	abstract	2	4	2.0	1.0	The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish.	0
2874	27906	27906	S20-12	abstract	3	5	3.0	1.0	Offens	0
2875	27907	27907	S20-12	abstract	4	6	4.0	1.0	Eval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.	0
2876	27908	27908	S20-12	Introduction	1	7	1.0	1.0	Offensive language is ubiquitous in social media platforms such as Facebook, Twitter, and Reddit, and it comes in many forms.	0
2877	27909	27909	S20-12	Introduction	2	8	1.0	1.0	Given the multitude of terms and definitions related to offensive language used in the literature, several recent studies have investigated the common aspects of different abusive language detection tasks (Waseem et al., 2017;	0
2878	27910	27910	S20-12	Introduction	3	9	1.0	1.0	Wiegand et al., 2018).	0
2879	27911	27911	S20-12	Introduction	4	10	1.0	1.0	One such example is SemEval-2019 Task 6: Offens	0
2880	27912	27912	S20-12	Introduction	5	11	1.0	1.0	Eval 1 (Zampieri et al., 2019b), which is the precursor to the present shared task.	0
2881	27913	27913	S20-12	Introduction	6	12	1.0	1.0	Offens	0
2882	27914	27914	S20-12	Introduction	7	13	1.0	1.0	Eval-2019 used the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets annotated using a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account (Zampieri et al., 2019a).	0
2883	27915	27915	S20-12	Introduction	8	14	1.0	1.0	The assumption behind this annotation schema is that the target of offensive messages is an important variable that allows us to discriminate between, e.g., hate speech, which often consists of insults targeted toward a group, and cyberbullying, which typically targets individuals.	0
2884	27916	27916	S20-12	Introduction	9	15	1.0	1.0	A number of recently organized related shared tasks followed similar hierarchical models.	0
2885	27917	27917	S20-12	Introduction	10	16	2.0	1.0	Examples include HASOC-2019 (Mandl et al., 2019) for English, German, and Hindi, HatEval-2019 (Basile et al., 2019) for English and Spanish, GermEval-2019 for German (Struß et al., 2019), and TRAC-2020 (Kumar et al., 2020) for English, Bengali, and Hindi.	0
2886	27918	27918	S20-12	Introduction	11	17	2.0	1.0	Offens	0
2887	27919	27919	S20-12	Introduction	12	18	2.0	1.0	Eval-2019 attracted nearly 800 team registrations and received 115 official submissions, which demonstrates the interest of the research community in this topic.	0
2888	27920	27920	S20-12	Introduction	13	19	2.0	1.0	Therefore, we organized a follow-up, OffensEval-2020 2 (SemEval-2020 Task 12), which is described in this report, building on the success of OffensEval-2019 with several improvements.	0
2889	27921	27921	S20-12	Introduction	14	20	2.0	1.0	In particular, we used the same three-level taxonomy to annotate new datasets in five languages, where each level in this taxonomy corresponds to a subtask in the competition:	0
2890	27922	27922	S20-12	Introduction	15	21	2.0	1.0	• Subtask A: Offensive language identification;	0
2891	27923	27923	S20-12	Introduction	16	22	2.0	1.0	• Subtask B: Automatic categorization of offense types;	0
2892	27924	27924	S20-12	Introduction	17	23	2.0	1.0	• Subtask C: Offense target identification.	0
2893	27925	27925	S20-12	Introduction	18	24	2.0	1.0	This work is licensed under a Creative Commons Attribution 4.0 International License.	0
2894	27926	27926	S20-12	Introduction	19	25	3.0	1.0	License details: http: //creativecommons.org/licenses/by/4.0/.	0
2895	27927	27927	S20-12	Introduction	20	26	3.0	1.0	1 http://sites.google.com/site/offensevalsharedtask/offenseval2019 2 http://sites.google.com/site/offensevalsharedtask/home	0
2896	27928	27928	S20-12	Introduction	21	27	3.0	1.0	The contributions of OffensEval-2020 can be summarized as follows:	0
2897	27929	27929	S20-12	Introduction	22	28	3.0	1.0	•	0
2898	27930	27930	S20-12	Introduction	23	29	3.0	1.0	We provided the participants with a new, large-scale semi-supervised training dataset containing over nine million English tweets (Rosenthal et al., 2020).	0
2899	27931	27931	S20-12	Introduction	24	30	3.0	1.0	•	0
2900	27932	27932	S20-12	Introduction	25	31	3.0	1.0	We introduced multilingual datasets, and we expanded the task to four new languages: Arabic (Mubarak et al., 2020b), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Turkish (Çöltekin, 2020).	0
2901	27933	27933	S20-12	Introduction	26	32	3.0	1.0	This opens the possibility for cross-lingual training and analysis, which several participants indeed explored.	0
2902	27934	27934	S20-12	Introduction	27	33	3.0	1.0	• Compared to OffensEval-2019, we used larger test datasets for all subtasks.	0
2903	27935	27935	S20-12	Introduction	28	34	4.0	1.0	Overall, Offens	0
2904	27936	27936	S20-12	Introduction	29	35	4.0	1.0	Eval-2020 was a very successful task.	0
2905	27937	27937	S20-12	Introduction	30	36	4.0	1.0	The huge interest demonstrated last year continued this year, with 528 teams signing up to participate in the task, and 145 of them submitting official runs on the test dataset.	0
2906	27938	27938	S20-12	Introduction	31	37	4.0	1.0	Furthermore, Offens	0
2907	27939	27939	S20-12	Introduction	32	38	4.0	1.0	Eval-2020 received 70 system description papers, which is an all-time record for a SemEval task.	0
2908	27940	27940	S20-12	Introduction	33	39	4.0	1.0	The remainder of this paper is organized as follows: Section 2 describes the annotation schema.	0
2909	27941	27941	S20-12	Introduction	34	40	4.0	1.0	Section 3 presents the five datasets that we used in the competition.	0
2910	27942	27942	S20-12	Introduction	35	41	4.0	1.0	Sections 4-9 present the results and discuss the approaches taken by the participating systems for each of the five languages.	0
2911	27943	27943	S20-12	Introduction	36	42	4.0	1.0	Finally, Section 10 concludes and suggests some possible directions for future work.	0
2912	27944	27944	S20-12	Annotation Schema	1	43	2.0	1.0	OLID's annotation schema proposes a hierarchical modeling of offensive language.	0
2913	27945	27945	S20-12	Annotation Schema	2	44	4.0	1.0	It classifies each example using the following three-level hierarchy:	0
2914	27946	27946	S20-12	Level A -Offensive Language Detection	1	45	2.0	1.0	Is the text offensive (OFF) or not offesive (NOT)?	0
2915	27947	27947	S20-12	Level A -Offensive Language Detection	2	46	3.0	1.0	NOT: text that is neither offensive, nor profane;	0
2916	27948	27948	S20-12	Level A -Offensive Language Detection	3	47	4.0	1.0	OFF: text containing inappropriate language, insults, or threats.	0
2917	27949	27949	S20-12	Level B -Categorization of Offensive Language	1	48	2.0	1.0	Is the offensive text targeted (TIN) or untargeted (UNT)?	0
2918	27950	27950	S20-12	Level B -Categorization of Offensive Language	2	49	4.0	1.0	TIN: targeted insults or threats towards a group or an individual; UNT: untargeted profanity or swearing.	0
2919	27951	27951	S20-12	Level C -Offensive Language Target Identification	1	50	2.0	1.0	Who or what is the target of the offensive content?	0
2920	27952	27952	S20-12	Level C -Offensive Language Target Identification	2	51	3.0	1.0	IND: the target is an individual, which can be explicitly mentioned or it can be implicit; GRP: the target is a group of people based on ethnicity, gender, sexual orientation, religious belief, or other common characteristic;	0
2921	27953	27953	S20-12	Level C -Offensive Language Target Identification	3	52	4.0	1.0	OTH: the target does not fall into any of the previous categories, e.g., organizations, events, and issues.	0
2922	27954	27954	S20-12	Data	1	53	1.0	1.0	In this section, we describe the datasets for all five languages: Arabic, Danish, English, Greek, and Turkish.	0
2923	27955	27955	S20-12	Data	2	54	1.0	1.0	All of the languages follow the OLID annotation schema and all datasets were pre-processed in the same way, e.g., all user mentions were substituted by @USER for anonymization.	0
2924	27956	27956	S20-12	Data	3	55	1.0	1.0	The introduction of new languages using a standardized schema with the purpose of detecting offensive and targeted speech should improve dataset consistency.	0
2925	27957	27957	S20-12	Data	4	56	1.0	1.0	This strategy is in line with current best practices in abusive language data collection (Vidgen and Derczynski, 2020).	0
2926	27958	27958	S20-12	Data	5	57	1.0	2.0	All languages contain data for subtask A, and only English contains data for subtasks B and C.	0
2927	27959	27959	S20-12	Data	6	58	1.0	2.0	The distribution of the data across categories for all languages for subtask A is shown in     English For English, we provided two datasets: OLID from OffensEval-2019 (Zampieri et al., 2019a), and SOLID, which is a new dataset we created for the task (Rosenthal et al., 2020).	0
2928	27960	27960	S20-12	Data	7	59	1.0	2.0	SOLID is an abbreviation for Semi-Supervised Offensive Language Identification Dataset, and it contains 9,089,140 English tweets, which makes it the largest dataset of its kind.	0
2929	27961	27961	S20-12	Data	8	60	1.0	2.0	For SOLID, we collected random tweets using the 20 most common English stopwords such as the, of, and, to, etc.	0
2930	27962	27962	S20-12	Data	9	61	1.0	2.0	Then, we labeled the collected tweets in a semi-supervised manner using democratic co-training, with OLID as a seed dataset.	0
2931	27963	27963	S20-12	Data	10	62	1.0	2.0	For the co-training, we used four models with different inductive biases: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019).	0
2932	27964	27964	S20-12	Data	11	63	2.0	2.0	We selected the OFF tweets for the test set using this semi-supervised process and we then annotated them manually for all subtasks.	0
2933	27965	27965	S20-12	Data	12	64	2.0	2.0	We further added 2,500 NOT tweets using this process without further annotation.	0
2934	27966	27966	S20-12	Data	13	65	2.0	2.0	We computed a Fleiss' κ Inter-Annotator Agreement (IAA) on a small subset of instances that were predicted to be OFF, and obtained 0.988 for Level A (almost perfect agreement), 0.818 for Level B (substantial agreement), and 0.630 for Level C (moderate agreement).	0
2935	27967	27967	S20-12	Data	14	66	2.0	2.0	The annotation for Level C was more challenging as it is 3-way and also as sometimes there could be different types of targets mentioned in the offensive tweet, but the annotators were forced to choose only one label.	0
2936	27968	27968	S20-12	Data	15	67	2.0	2.0	Arabic	0
2937	27969	27969	S20-12	Data	16	68	2.0	2.0	The Arabic dataset consists of 10,000 tweets collected in April-May 2019 using the Twitter API with the language filter set to Arabic: lang:ar.	0
2938	27970	27970	S20-12	Data	17	69	2.0	2.0	In order to increase the chance of having offensive content, only tweets with two or more vocative particles (yA in Arabic) were considered for annotation; the vocative particle is used mainly to direct the speech to a person or to a group, and it is widely observed in offensive communications in almost all Arabic dialects.	0
2939	27971	27971	S20-12	Data	18	70	2.0	2.0	This yielded 20% offensive tweets in the final dataset.	0
2940	27972	27972	S20-12	Data	19	71	2.0	2.0	The tweets were manually annotated (for Level A only) by a native speaker familiar with several Arabic dialects.	0
2941	27973	27973	S20-12	Data	20	72	2.0	2.0	A random subsample of offensive and non-offensive tweets were doubly annotated and the Fleiss κ IAA was found to be 0.92.	0
2942	27974	27974	S20-12	Data	21	73	2.0	2.0	More details can be found in (Mubarak et al., 2020b).	0
2943	27975	27975	S20-12	Data	22	74	3.0	2.0	Danish	0
2944	27976	27976	S20-12	Data	23	75	3.0	2.0	The Danish dataset consists of 3,600 comments drawn from Facebook, Reddit, and a local newspaper, Ekstra Bladet 3 .	0
2945	27977	27977	S20-12	Data	24	76	3.0	2.0	The selection of the comments was partially seeded using abusive terms gathered during a crowd-sourced lexicon compilation; in order to ensure sufficient data diversity, this seeding was limited to half the data only.	0
2946	27978	27978	S20-12	Data	25	77	3.0	2.0	The training data was not divided into distinct training/development splits, and participants were encouraged to perform cross-validation, as we wanted to avoid issues that fixed splits can cause (Gorman and Bedrick, 2019).	0
2947	27979	27979	S20-12	Data	26	78	3.0	2.0	The annotation (for Level A only) was performed at the individual comment level by males aged 25-40.	0
2948	27980	27980	S20-12	Data	27	79	3.0	2.0	A full description of the dataset and an accompanying data statement (Bender and Friedman, 2018) can be found in (Sigurbergsson and Derczynski, 2020).	0
2949	27981	27981	S20-12	Data	28	80	3.0	2.0	Greek The Offensive Greek Twitter Dataset (OGTD) used in this task is a compilation of 10,287 tweets.	0
2950	27982	27982	S20-12	Data	29	81	3.0	2.0	These tweets were sampled using popular and trending hashtags, including television programs such as series, reality and entertainment shows, along with some politically related tweets.	0
2951	27983	27983	S20-12	Data	30	82	3.0	2.0	"Another portion of the dataset was fetched using pejorative terms and ""you are"" as keywords."	0
2952	27984	27984	S20-12	Data	31	83	3.0	2.0	This particular strategy was adopted with the hypothesis that TV and politics would gather a handful of offensive posts, along with tweets containing vulgar language for further investigation.	0
2953	27985	27985	S20-12	Data	32	84	3.0	2.0	A team of volunteer annotators participated in the annotation process (for Level A only), with each tweet being judged by three annotators.	0
2954	27986	27986	S20-12	Data	33	85	4.0	2.0	In cases of disagreement, labels with majority agreement above 66% were selected as the actual tweet labels.	0
2955	27987	27987	S20-12	Data	34	86	4.0	2.0	The IAA was 0.78 (using Fleiss' κ coefficient).	0
2956	27988	27988	S20-12	Data	35	87	4.0	2.0	A full description of the dataset collection and annotation is detailed in (Pitenis et al., 2020).	0
2957	27989	27989	S20-12	Data	36	88	4.0	2.0	Turkish	0
2958	27990	27990	S20-12	Data	37	89	4.0	2.0	The Turkish dataset consists of over 35,000 tweets sampled uniformly from the Twitter stream and filtered using a list of the most frequent words in Turkish, as identified by Twitter.	0
2959	27991	27991	S20-12	Data	38	90	4.0	2.0	The tweets were annotated by volunteers (for Level A only).	0
2960	27992	27992	S20-12	Data	39	91	4.0	2.0	Most tweets were annotated by a single annotator.	0
2961	27993	27993	S20-12	Data	40	92	4.0	2.0	The Cohen's κ IAA calculated on 5,000 doubly-annotated tweets was 0.761.	0
2962	27994	27994	S20-12	Data	41	93	4.0	2.0	Note that we did not include any specific method for spotting offensive language, e.g., filtering by offensive words, or following usual targets of offensive language.	0
2963	27995	27995	S20-12	Data	42	94	4.0	2.0	As a result, the distribution closely resembles the actual offensive language use on Twitter, with more non-offensive tweets than offensive tweets.	0
2964	27996	27996	S20-12	Data	43	95	4.0	2.0	More details about the sampling and the annotation process can be found in (Çöltekin, 2020).	0
2965	27997	27997	S20-12	Task Participation	1	96	1.0	2.0	A total of 528 teams signed up to participate in the task, and 145 of them submitted results: 6 teams made submissions for all five languages, 19 did so for four languages, 11 worked on three languages, 13 on two languages, and 96 focused on just one language.	0
2966	27998	27998	S20-12	Task Participation	2	97	1.0	2.0	Tables 13, 14, and 15 show a summary of which team participated in which task.	0
2967	27999	27999	S20-12	Task Participation	3	98	1.0	2.0	A total of 70 teams submitted system description papers, which are listed in Table 12.	0
2968	28000	28000	S20-12	Task Participation	4	99	2.0	2.0	Below, we analyze the representation and the models used for all language tracks.	0
2969	28001	28001	S20-12	Task Participation	5	100	2.0	2.0	Representation	0
2970	28002	28002	S20-12	Task Participation	6	101	2.0	2.0	The vast majority of teams used some kind of pre-trained embeddings such as contextualized Transformers (Vaswani et al., 2017) and ELMo (Peters et al., 2018) embeddings.	0
2971	28003	28003	S20-12	Task Participation	7	102	2.0	2.0	The most popular Transformers were BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and the multi-lingual mBERT (Devlin et al., 2019).	0
2972	28004	28004	S20-12	Task Participation	8	103	3.0	2.0	4 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic.	0
2973	28005	28005	S20-12	Task Participation	9	104	3.0	2.0	Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words.	0
2974	28006	28006	S20-12	Task Participation	10	105	3.0	2.0	Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018).	0
2975	28007	28007	S20-12	Task Participation	11	106	4.0	2.0	Machine learning models	0
2976	28008	28008	S20-12	Task Participation	12	107	4.0	2.0	In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), AL-BERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular.	0
2977	28009	28009	S20-12	Task Participation	13	108	4.0	2.0	Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014).	0
2978	28010	28010	S20-12	Task Participation	14	109	4.0	2.0	Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles.	0
2979	28011	28011	S20-12	English Track	1	110	1.0	2.0	A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of the task): 27 teams participated in all three English subtasks, 18 teams participated in two English subtasks, and 42 focused on one English subtask only.	0
2980	28012	28012	S20-12	English Track	2	111	1.0	2.0	Pre-processing and normalization	0
2981	28013	28013	S20-12	English Track	3	112	2.0	2.0	Most teams performed some kind of pre-processing (67 teams) or text normalization (26 teams), which are typical steps when working with tweets.	0
2982	28014	28014	S20-12	English Track	4	113	2.0	3.0	Text normalization included various text transformations such as converting emojis to plain text, 5 segmenting hashtags, 6 general tweet text normalization (Satapathy et al., 2019), abbreviation expansion, bad word replacement, error correction, lowercasing, stemming, and/or lemmatization.	0
2983	28015	28015	S20-12	English Track	5	114	2.0	3.0	Other techniques included the removal of @user mentions, URLs, hashtags, emojis, emails, dates, numbers, punctuation, consecutive character repetitions, offensive words, and/or stop words.	0
2984	28016	28016	S20-12	English Track	6	115	3.0	3.0	Additional data	0
2985	28017	28017	S20-12	English Track	7	116	3.0	3.0	Most teams found the weakly supervised SOLID dataset useful, and 58 teams ended up using it in their systems.	0
2986	28018	28018	S20-12	English Track	8	117	3.0	3.0	Another six teams gave it a try, but could not benefit from it, and the remaining teams only used the manually annotated training data.	0
2987	28019	28019	S20-12	English Track	9	118	4.0	3.0	Some teams used additional datasets from HASOC-2019 (Mandl et al., 2019), the Kaggle competitions on Detecting Insults in Social Commentary 7 and Toxic Comment Classification 8 , the TRAC-2018 shared task on Aggression Identification (Kumar et al., 2018a;	0
2988	28020	28020	S20-12	English Track	10	119	4.0	3.0	Kumar et al., 2018b), the Wikipedia Detox dataset (Wulczyn et al., 2017), and the datasets from  and (Wulczyn et al., 2017), as well as some lexicons such as HurtLex (Bassignana et al., 2018) and Hatebase.	0
2989	28021	28021	S20-12	English Track	11	120	4.0	3.0	9 Finally, one team created their own dataset.	0
2990	28022	28022	S20-12	Subtask A	1	121	1.0	3.0	A total of 82 teams made submissions for subtask A, and the results can be seen in Table 5.	0
2991	28023	28023	S20-12	Subtask A	2	122	1.0	3.0	This was the most popular subtask among all subtasks and across all languages.	0
2992	28024	28024	S20-12	Subtask A	3	123	2.0	3.0	The best team UHH-LT achieved an F1 score of 0.9204 using an ensemble of ALBERT models of different sizes.	0
2993	28025	28025	S20-12	Subtask A	4	124	2.0	3.0	The team ranked second was UHH-LT with an F1 score of 0.9204, and it used RoBERTa-large that was fine-tuned on the SOLID dataset in an unsupervised way, i.e., using the MLM objective.	0
2994	28026	28026	S20-12	Subtask A	5	125	3.0	3.0	The third team, Galileo, achieved an F1 score of 0.9198, using an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large trained on the subtask A data for all languages.	0
2995	28027	28027	S20-12	Subtask A	6	126	3.0	3.0	The top-10 teams used BERT, RoBERTa or XLM-RoBERTa, sometimes as part of ensembles that also included CNNs and LSTMs (Hochreiter and Schmidhuber, 1997).	0
2996	28028	28028	S20-12	Subtask A	7	127	4.0	3.0	Overall, the competition for this subtask was very strong, and the scores are very close: the teams ranked 2-16 are within one point in the third decimal place, and those ranked 2-59 are within two absolute points in the second decimal place from the best team.	0
2997	28029	28029	S20-12	Subtask A	8	128	4.0	3.0	All but one team beat the majority class baseline (we suspect that team might have accidentally flipped their predicted labels).	0
2998	28030	28030	S20-12	Subtask B	1	129	1.0	3.0	A total of 41 teams made submissions for subtask B, and the results can be seen in Table 6.	0
2999	28031	28031	S20-12	Subtask B	2	130	1.0	3.0	The best team is Galileo (which were third on subtask A), whose ensemble model achieved an F1 score of 0.7462.	0
3000	28032	28032	S20-12	Subtask B	3	131	2.0	3.0	The second-place team, PGSG, used a complex teacher-student architecture built on top of a BERT-LSTM model, which was fine-tuned on the SOLID dataset in an unsupervised way, i.e., optimizing for the MLM objective.	0
3001	28033	28033	S20-12	Subtask B	4	132	2.0	3.0	NTU NLP was ranked third with an F1 score of 0.6906.	0
3002	28034	28034	S20-12	Subtask B	5	133	3.0	3.0	They tackled subtasks A, B, and C as part of a multi-task BERT-based model.	0
3003	28035	28035	S20-12	Subtask B	6	134	3.0	3.0	Overall, the differences in the scores for subtask B are much larger than for subtask A.	0
3004	28036	28036	S20-12	Subtask B	7	135	4.0	3.0	For example, the 4th team is two points behind the third one and seven points behind the first one.	0
3005	28037	28037	S20-12	Subtask B	8	136	4.0	3.0	The top-ranking teams used BERT-based Transformer models, and all but four teams could improve over the majority class baseline.	0
3006	28038	28038	S20-12	Subtask C	1	137	1.0	3.0	A total of 37 teams made submissions for subtask C and the results are shown in Table 7.	0
3007	28039	28039	S20-12	Subtask C	2	138	1.0	3.0	The best team was once again Galileo, with an F1 score of 0.7145.	0
3008	28040	28040	S20-12	Subtask C	3	139	2.0	3.0	LT@Helsinki was ranked second with an F1 score of 0.6700.	0
3009	28041	28041	S20-12	Subtask C	4	140	2.0	3.0	They used fine-tuned BERT with oversampling to improve class imbalance.	0
3010	28042	28042	S20-12	Subtask C	5	141	3.0	3.0	The third best system was PRHLT-UPV with an F1 score of 0.6692, which combines BERT with hand-crafted features; it is followed very closely by UHH-LT at rank 4, which achieved an F1 score of 0.6683.	0
3011	28043	28043	S20-12	Subtask C	6	142	3.0	3.0	This subtask is also dominated by BERT-based models, and all teams outperformed the majority class baseline.	0
3012	28044	28044	S20-12	Subtask C	7	143	4.0	3.0	Note that the absolute F1-scores obtained by the best teams in the English subtasks A and C are substantially higher than the scores obtained by the best teams in OffensEval-2019: 0.9223 vs. 0.8290 for subtask A and 0.7145 vs. 0.6600 for subtask C. This suggests that the much larger SOLID dataset made available in OffensEval-2020 helped the models make more accurate predictions.	0
3013	28045	28045	S20-12	Subtask C	8	144	4.0	3.0	Furthermore, it suggests that the weakly supervised method used to compile and annotate SOLID is a viable alternative to popular purely manual annotation approaches.	0
3014	28046	28046	S20-12	Subtask C	9	145	4.0	3.0	A more detailed analysis of the systems' performances will be carried out in order to determine the contribution of the SOLID dataset for the results.	0
3015	28047	28047	S20-12	Best Systems	1	146	1.0	3.0	We provide some more details about the approaches used by the top teams for each subtask.	0
3016	28048	28048	S20-12	Best Systems	2	147	1.0	3.0	We use subindices to show their rank for each subtask.	0
3017	28049	28049	S20-12	Best Systems	3	148	1.0	3.0	Additional summaries for some of the best teams can be found in Appendix A.	0
3018	28050	28050	S20-12	Best Systems	4	149	2.0	3.0	Galileo (A:3,B:1,C:1)	0
3019	28051	28051	S20-12	Best Systems	5	150	2.0	3.0	This team was ranked 3rd, 1st, and 1st on the English subtasks A, B, and C, respectively.	0
3020	28052	28052	S20-12	Best Systems	6	151	2.0	3.0	This is also the only team ranked among the top-3 across all languages.	0
3021	28053	28053	S20-12	Best Systems	7	152	2.0	3.0	For subtask A, they used multi-lingual pre-trained Transformers based on XLM-RoBERTa, followed by multi-lingual fine-tuning using the OffensEval data.	0
3022	28054	28054	S20-12	Best Systems	8	153	3.0	3.0	Ultimately, they submitted an ensemble that combined XLM-RoBERTa-base and XLM-RoBERTa-large, achieving an F1 score of 0.9198.	0
3023	28055	28055	S20-12	Best Systems	9	154	3.0	3.0	For subtasks B and C, they used knowledge distillation in a teacher-student framework, using Transformers such as ALBERT and ERNIE 2.0  as teacher models, achieving an F1 score of 0.7462 and 0.7145, for subtasks B and C respectively.	0
3024	28056	28056	S20-12	Best Systems	10	155	3.0	3.0	UHH-LT (A:1)	0
3025	28057	28057	S20-12	Best Systems	11	156	3.0	3.0	This team was ranked 1st on subtask A with an F1 score of 0.9223.	0
3026	28058	28058	S20-12	Best Systems	12	157	4.0	3.0	They fine-tuned different Transformer models on the OLID training data, and then combined them into an ensemble.	0
3027	28059	28059	S20-12	Best Systems	13	158	4.0	3.0	They experimented with BERT-base and BERT-large (uncased), RoBERTa-base and RoBERTa-large, XLM-RoBERTa, and four different ALBERT models (large-v1, large-v2, xxlarge-v1, and xxlarge-v2).	0
3028	28060	28060	S20-12	Best Systems	14	159	4.0	3.0	In their official submission, they used an ensemble combining different ALBERT models.	0
3029	28061	28061	S20-12	Best Systems	15	160	4.0	3.0	They did not use the labels of the SOLID dataset, but found the tweets it contained nevertheless useful for unsupervised fine-tuning (i.e., using the MLM objective) of the pre-trained Transformers.	0
3030	28062	28062	S20-12	Arabic Track	1	161	1.0	3.0	A total of 108 teams registered to participate in the Arabic track, and ultimately 53 teams entered the competition with at least one valid submission.	0
3031	28063	28063	S20-12	Arabic Track	2	162	2.0	3.0	Among them, ten teams participated in the Arabic track only, while the rest participated in other languages in addition to Arabic.	0
3032	28064	28064	S20-12	Arabic Track	3	163	3.0	3.0	This was the second shared task for Arabic after the one at the 4th workshop on Open-Source Arabic Corpora and Processing Tools (Mubarak et al., 2020a), which had different settings and less participating teams.	0
3033	28065	28065	S20-12	Arabic Track	4	164	4.0	3.0	Pre-processing and normalization	0
3034	28066	28066	S20-12	Arabic Track	5	165	4.0	3.0	Most teams performed some kind of pre-processing or text normalization, e.g., Hamza shapes, Alif Maqsoura, Taa Marbouta, diacritics, non-Arabic characters, etc., and only one team replaced emojis with their textual counter-parts.	0
3035	28067	28067	S20-12	Results	1	166	1.0	3.0	Table 8 shows the teams and the F1 scores they achieved for the Arabic subtask A.	0
3036	28068	28068	S20-12	Results	2	167	1.0	3.0	The majority class baseline had an F1 score of 0.4441, and several teams achieved results that doubled that baseline score.	0
3037	28069	28069	S20-12	Results	3	168	2.0	3.0	The best-performing team was ALAMIHamza with an F1 score of 0.9017.	0
3038	28070	28070	S20-12	Results	4	169	2.0	4.0	The second-best team, ALT, was almost tied with the winner, with an F1 score of 0.9016.	0
3039	28071	28071	S20-12	Results	5	170	3.0	4.0	The Galileo team was third with an F1 score of 0.8989.	0
3040	28072	28072	S20-12	Results	6	171	3.0	4.0	A summary of the approaches taken by the top-performing teams can be found in Appendix A; here we briefly describe the winning system:	0
3041	28073	28073	S20-12	Results	7	172	4.0	4.0	ALAMIHamza(A:1)	0
3042	28074	28074	S20-12	Results	8	173	4.0	4.0	The winning team achieved the highest F1-score using BERT to encode Arabic tweets, followed by a sigmoid classifier.	0
3043	28075	28075	S20-12	Results	9	174	4.0	4.0	They further performed translation of the meaning of emojis.	0
3044	28076	28076	S20-12	Danish Track	1	175	1.0	4.0	A total of 72 teams registered to participate in the Danish track, and 39 of them actually made official submissions on the test dataset.	0
3045	28077	28077	S20-12	Danish Track	2	176	2.0	4.0	This is the first shared task on offensive language identification to include Danish, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Sigurbergsson and Derczynski, 2020).	0
3046	28078	28078	S20-12	Danish Track	3	177	3.0	4.0	Pre-processing and normalization	0
3047	28079	28079	S20-12	Danish Track	4	178	4.0	4.0	Many teams used the pre-processing included in the relevant embedding model, e.g., BPE (Heinzerling and Strube, 2018) and WordPiece.	0
3048	28080	28080	S20-12	Danish Track	5	179	4.0	4.0	Other pre-processing techniques included emoji normalization, spelling correction, sentiment tagging, lexical and regex-based term and phrase flagging, and hashtag segmentation.	0
3049	28081	28081	S20-12	Results	1	180	1.0	4.0	The results are shown in Table 9.	0
3050	28082	28082	S20-12	Results	2	181	1.0	4.0	We can see that all teams managed to outperform the majority class baseline.	0
3051	28083	28083	S20-12	Results	3	182	2.0	4.0	Moreover, all but one team improved over a FastText baseline (F1 = 0.5148), and most teams achieved an F1 score of 0.7 or higher.	0
3052	28084	28084	S20-12	Results	4	183	2.0	4.0	Interestingly, one of the top-ranked teams, JCT, was entirely non-neural.	0
3053	28085	28085	S20-12	Results	5	184	3.0	4.0	LT@Helsinki (A:1)	0
3054	28086	28086	S20-12	Results	6	185	3.0	4.0	The winning team LT@Helsinki used NordicBERT for representation, as provided by BotXO. 10 NordicBERT is customized to Danish, and avoids some of the pre-processing noise and ambiguity introduced by other popular BERT implementations.	0
3055	28087	28087	S20-12	Results	7	186	4.0	4.0	The team further reduced orthographic lengthening to maximum two repeated characters, converted emojis to sentiment scores, and used cooccurrences of hashtags and references to usernames.	0
3056	28088	28088	S20-12	Results	8	187	4.0	4.0	They tuned the hyper-parameters of their model using 10-fold cross validation.	0
3057	28089	28089	S20-12	Greek Track	1	188	1.0	4.0	A total of 71 teams registered to participate in the Greek track, and ultimately 37 of them made an official submission on the test dataset.	0
3058	28090	28090	S20-12	Greek Track	2	189	2.0	4.0	This is the first shared task on offensive language identification to include Greek, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Pitenis et al., 2020).	0
3059	28091	28091	S20-12	Greek Track	3	190	3.0	4.0	Pre-processing and normalization	0
3060	28092	28092	S20-12	Greek Track	4	191	4.0	4.0	The participants experimented with various pre-processing and text normalization techniques, similarly to what was done for the other languages above.	0
3061	28093	28093	S20-12	Greek Track	5	192	4.0	4.0	One team further reported replacement of emojis with their textual equivalent.	0
3062	28094	28094	S20-12	Results	1	193	1.0	4.0	The evaluation results are shown in Table 10.	0
3063	28095	28095	S20-12	Results	2	194	1.0	4.0	The top team, NLPDove, achieved an F1 score of 0.852, with Galileo coming close at the second place with an F1 score of 0.851.	0
3064	28096	28096	S20-12	Results	3	195	2.0	4.0	The KS@LTH team was ranked third with an F1 score of 0.848.	0
3065	28097	28097	S20-12	Results	4	196	2.0	4.0	It is no surprise that the majority of the high-ranking submissions and participants used large-scale pre-trained Transformers, with BERT being the most prominent among them, along with wordwvec-style non-contextualized pre-trained word embeddings.	0
3066	28098	28098	S20-12	Results	5	197	3.0	4.0	NLP	0
3067	28099	28099	S20-12	Results	6	198	3.0	4.0	Dove (A:1)	0
3068	28100	28100	S20-12	Results	7	199	4.0	4.0	The winning team NLPDove used pre-trained word embeddings from mBERT, which they fine-tuned using the training data.	0
3069	28101	28101	S20-12	Results	8	200	4.0	4.0	A domain-specific vocabulary was generated by running the WordPiece algorithm (Schuster and Nakajima, 2012) and using embeddings for extended vocabulary to pre-train and fine-tune the model.	0
3070	28102	28102	S20-12	Turkish Track	1	201	2.0	4.0	A total of 86 teams registered to participate in the Turkish track, and ultimately 46 of them made an official submission on the test dataset.	0
3071	28103	28103	S20-12	Turkish Track	2	202	3.0	4.0	All teams except for one participated in at least one other track.	0
3072	28104	28104	S20-12	Turkish Track	3	203	4.0	4.0	This is the first shared task on offensive language identification to include Turkish, and the dataset provided to the OffensEval-2020 participants is an extended version of the one from (Çöltekin, 2020).	0
3073	28105	28105	S20-12	Results	1	204	1.0	4.0	The results are shown in Table 11.	0
3074	28106	28106	S20-12	Results	2	205	2.0	4.0	We can see that team Galileo achieved the highest macro-averaged F1 score of 0.8258, followed by SU-NLP and KUI-SAIL with F1 scores of 0.8167 and 0.8141, respectively.	0
3075	28107	28107	S20-12	Results	3	206	2.0	4.0	Note that the latter two teams are from Turkey, and they used some language-specific resources and tuning.	0
3076	28108	28108	S20-12	Results	4	207	3.0	4.0	Most results were in the interval 0.7-0.8, and almost all teams managed to outperform the majority class baseline, which had an F1 score of 0.4435.	0
3077	28109	28109	S20-12	Results	5	208	3.0	4.0	Galileo (A:1)	0
3078	28110	28110	S20-12	Results	6	209	4.0	4.0	The best team in the Turkish subtask A was Galileo, which achieved top results in several other tracks.	0
3079	28111	28111	S20-12	Results	7	210	4.0	4.0	Unlike the systems ranked second and third, Galileo's system is language-agnostic, and it used data for all five languages in a multi-lingual training setup.	0
3080	28112	28112	S20-12	Conclusion and Future Work	1	211	1.0	4.0	We presented the results of OffensEval-2020, which featured datasets in five languages: Arabic, Danish, English, Greek, and Turkish.	0
3081	28113	28113	S20-12	Conclusion and Future Work	2	212	1.0	4.0	For English, we had three subtasks, representing the three levels of the OLID hierarchy.	0
3082	28114	28114	S20-12	Conclusion and Future Work	3	213	1.0	4.0	For the other four languages, we had a subtask for the top-level of the OLID hierarchy only.	0
3083	28115	28115	S20-12	Conclusion and Future Work	4	214	2.0	4.0	A total of 528 teams signed up to participate in OffensEval-2020, and 145 of them actually submitted results across all languages and subtasks.	0
3084	28116	28116	S20-12	Conclusion and Future Work	5	215	2.0	4.0	Out of the 145 participating teams, 96 teams participated in one language only, 13 teams participated in two languages, 11 in three languages, 19 in four languages, and 6 teams submitted systems for all five languages.	0
3085	28117	28117	S20-12	Conclusion and Future Work	6	216	2.0	4.0	The official submissions per language ranged from 37 (for Greek) to 81 (for English).	0
3086	28118	28118	S20-12	Conclusion and Future Work	7	217	2.0	4.0	Finally, 70 of the 145 participating teams submitted system description papers, which is an all-time record.	0
3087	28119	28119	S20-12	Conclusion and Future Work	8	218	3.0	4.0	The wide participation in the task allowed us to compare a number of approaches across different languages and datasets.	0
3088	28120	28120	S20-12	Conclusion and Future Work	9	219	3.0	4.0	Similarly to OffensEval-2019, we observed that the best systems for all languages and subtasks used large-scale BERT-style pre-trained Transformers such as BERT, RoBERTa, and mBERT.	0
3089	28121	28121	S20-12	Conclusion and Future Work	10	220	3.0	4.0	Unlike 2019, however, the multi-lingual nature of this year's data enabled cross-language approaches, which proved quite effective and were used by some of the top-ranked systems.	0
3090	28122	28122	S20-12	Conclusion and Future Work	11	221	3.0	4.0	In future work, we plan to extend the task in several ways.	0
3091	28123	28123	S20-12	Conclusion and Future Work	12	222	4.0	4.0	First, we want to offer subtasks B and C for all five languages from OffensEval-2020.	0
3092	28124	28124	S20-12	Conclusion and Future Work	13	223	4.0	4.0	We further plan to add some additional languages, especially under-represented ones.	0
3093	28125	28125	S20-12	Conclusion and Future Work	14	224	4.0	4.0	Other interesting aspects to explore are code-mixing, e.g., mixing Arabic script and Latin alphabet in the same Arabic message, and code-switching, e.g., mixing Arabic and English words and phrases in the same message.	0
3094	28126	28126	S20-12	Conclusion and Future Work	15	225	4.0	4.0	Last but not least, we plan to cover a wider variety of social media platforms.	0
3095	30743	30743	2020.nlptea-1.4	title	1	1	4.0	1.0	Overview of NLPTEA-2020 Shared Task for Chinese Grammatical Error Diagnosis	0
3096	30744	30744	2020.nlptea-1.4	abstract	1	2	1.0	1.0	This paper presents the NLPTEA 2020 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as a foreign language.	1
3097	30745	30745	2020.nlptea-1.4	abstract	2	3	2.0	1.0	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
3098	30746	30746	2020.nlptea-1.4	abstract	3	4	3.0	1.0	Of the 30 teams registered for this shared task, 17 teams developed the system and submitted a total of 43 runs.	0
3099	30747	30747	2020.nlptea-1.4	abstract	4	5	4.0	1.0	System performances achieved a significant progress, reaching F1 of 91% in detection level, 40% in position level and 28% in correction level.	0
3100	30748	30748	2020.nlptea-1.4	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
3101	30749	30749	2020.nlptea-1.4	Introduction	1	7	1.0	1.0	Automated grammar checking for learners of English as a foreign language has achieved obvious progress.	0
3102	30750	30750	2020.nlptea-1.4	Introduction	2	8	1.0	1.0	Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011;	0
3103	30751	30751	2020.nlptea-1.4	Introduction	3	9	1.0	1.0	Dale et al., 2012).	0
3104	30752	30752	2020.nlptea-1.4	Introduction	4	10	1.0	1.0	The shared tasks at CoNLL 2013 and 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013;.	0
3105	30753	30753	2020.nlptea-1.4	Introduction	5	11	1.0	1.0	Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language (CFL) learners.	0
3106	30754	30754	2020.nlptea-1.4	Introduction	6	12	2.0	1.0	Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012;	0
3107	30755	30755	2020.nlptea-1.4	Introduction	7	13	2.0	1.0	Wu et al, 2010;	0
3108	30756	30756	2020.nlptea-1.4	Introduction	8	14	2.0	1.0	Yu and Chen, 2012), rule-based analysis (Lee et al., 2013), neuro network modelling (Zheng et al., 2016;	0
3109	30757	30757	2020.nlptea-1.4	Introduction	9	15	2.0	1.0	Fu et al., 2018) and hybrid methods Zhou et al., 2017).	0
3110	30758	30758	2020.nlptea-1.4	Introduction	10	16	2.0	1.0	In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL .	0
3111	30759	30759	2020.nlptea-1.4	Introduction	11	17	2.0	1.0	A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015), COLING-2016.	0
3112	30760	30760	2020.nlptea-1.4	Introduction	12	18	3.0	1.0	Its name was fixed from then on: Chinese Grammatical Error Diagnosis (CGED).	0
3113	30761	30761	2020.nlptea-1.4	Introduction	13	19	3.0	1.0	As a part of IJCNLP 2017, the shared task was organized (Rao et al., 2017).	0
3114	30762	30762	2020.nlptea-1.4	Introduction	14	20	3.0	1.0	In conjunction with NLP-TEA workshop in ACL 2018, CGED was organized again (Rao et al., 2018).	0
3115	30763	30763	2020.nlptea-1.4	Introduction	15	21	3.0	1.0	The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results.	0
3116	30764	30764	2020.nlptea-1.4	Introduction	16	22	3.0	1.0	Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task.	0
3117	30765	30765	2020.nlptea-1.4	Introduction	17	23	4.0	1.0	The rest of this paper is organized as follows.	0
3118	30766	30766	2020.nlptea-1.4	Introduction	18	24	4.0	1.0	Section 2 describes the task in detail.	0
3119	30767	30767	2020.nlptea-1.4	Introduction	19	25	4.0	1.0	Section 3 introduces the constructed data sets.	0
3120	30768	30768	2020.nlptea-1.4	Introduction	20	26	4.0	1.0	Section 4 proposes evaluation metrics.	0
3121	30769	30769	2020.nlptea-1.4	Introduction	21	27	4.0	1.0	Section 5 reports the results of the participants' approaches.	0
3122	30770	30770	2020.nlptea-1.4	Introduction	22	28	4.0	1.0	Conclusions are finally drawn in Section 6.	0
3123	30771	30771	2020.nlptea-1.4	Task Description	1	29	1.0	1.0	The goal of this shared task is to develop NLP techniques to automatically diagnose (and furtherly correct) grammatical errors in Chinese sentences written by CFL learners.	0
3124	30772	30772	2020.nlptea-1.4	Task Description	2	30	1.0	1.0	"Such errors are defined as PADS: redundant words (denoted as a capital ""R""), missing words (""M""), word selection errors (""S""), and word ordering errors (""W"")."	0
3125	30773	30773	2020.nlptea-1.4	Task Description	3	31	2.0	2.0	The input sentence may contain one or more such errors.	0
3126	30774	30774	2020.nlptea-1.4	Task Description	4	32	2.0	2.0	The developed system should indicate which error types are embedded in the given unit (containing 1 to 5 sentences) and the position at which they occur.	0
3127	30775	30775	2020.nlptea-1.4	Task Description	5	33	2.0	2.0	"Each input unit is given a unique number ""sid""."	0
3128	30776	30776	2020.nlptea-1.4	Task Description	6	34	3.0	2.0	"If the inputs contain no grammatical errors, the system should return: ""sid, correct""."	0
3129	30777	30777	2020.nlptea-1.4	Task Description	7	35	3.0	2.0	"If an input unit contains the grammatical errors, the output format should include four items ""sid, start_off, end_off, error_type"", where start_off and end_off respectively denote the positions of starting and ending character at which the grammatical error occurs, and error_type should be one of the defined errors: ""R"", ""M"", ""S"", and ""W""."	0
3130	30778	30778	2020.nlptea-1.4	Task Description	8	36	4.0	2.0	Each character or punctuation mark occupies 1 space for counting positions.	0
3131	30779	30779	2020.nlptea-1.4	Task Description	9	37	4.0	2.0	Example sentences and corresponding notes are shown as Table 1 shows.	0
3132	30780	30780	2020.nlptea-1.4	Task Description	10	38	4.0	2.0	This year, we only have one track of HSK.	0
3133	30781	30781	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	1	39	1.0	2.0	"Example 1 Input: (sid=00038800481) 我根本不能了解这妇女辞职回家的现象。在这个时代，为什么放弃自己的工作，就 回家当家庭主妇？ Output: 00038800481, 6, 7, S 00038800481, 8, 8, R (Notes: ""了解""should be ""理解""."	0
3134	30782	30782	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	2	40	2.0	2.0	"In addition, ""这"" is a redundant word.)"	0
3135	30783	30783	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	3	41	2.0	2.0	"Example 2 Input: (sid=00038800464)我真不明白。她们可能是追求一些前代的浪漫。 Output: 00038800464, correct Example 3 Input: (sid=00038801261)人战胜了饥饿，才努力为了下一代作更好的、更健康的东西。 Output: 00038801261, 9, 9, M 00038801261, 16, 16, S (Notes: ""能"" is missing."	0
3136	30784	30784	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	4	42	3.0	2.0	"The word ""作""should be ""做""."	0
3137	30785	30785	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	5	43	4.0	2.0	"The correct sentence is ""才能努力为了下一代做更好的"")"	0
3138	30786	30786	2020.nlptea-1.4	Hanyu Shuiping Kaoshi (HSK)	6	44	4.0	2.0	"Example 4 Input: (sid=00038801320)饥饿的问题也是应该解决的。世界上每天由于饥饿很多人死亡。 Output: 00038801320, 19, 25, W (Notes: ""由于饥饿很多人"" should be ""很多人由于饥饿"")"	0
3139	30787	30787	2020.nlptea-1.4	Data Sets	1	45	1.0	2.0	The learner corpora used in our shared task were taken from the writing section of the HSK (Pinyin of Hanyu Shuiping Kaoshi, Test of Chinese Level) (Cui et al, 2011;	0
3140	30788	30788	2020.nlptea-1.4	Data Sets	2	46	1.0	2.0	Zhang et al, 2013).	0
3141	30789	30789	2020.nlptea-1.4	Data Sets	3	47	1.0	2.0	Native Chinese speakers were trained to manually annotate grammatical errors and provide corrections corresponding to each error.	0
3142	30790	30790	2020.nlptea-1.4	Data Sets	4	48	1.0	2.0	The data were then split into two mutually exclusive sets as follows.	0
3143	30791	30791	2020.nlptea-1.4	Data Sets	5	49	2.0	2.0	(1) Training Set: All units in this set were used to train the grammatical error diagnostic systems.	0
3144	30792	30792	2020.nlptea-1.4	Data Sets	6	50	2.0	2.0	Each unit contains 1 to 5 sentences with annotated grammatical errors and their corresponding corrections.	0
3145	30793	30793	2020.nlptea-1.4	Data Sets	7	51	2.0	2.0	All units are represented in SGML format, as shown in Table 2.	0
3146	30794	30794	2020.nlptea-1.4	Data Sets	8	52	2.0	2.0	We provide 1129 training units with a total of 2,909 grammatical errors, categorized as redundant (678 instances), missing (801), word selection (1228) and word ordering (201).	0
3147	30795	30795	2020.nlptea-1.4	Data Sets	9	53	3.0	2.0	In addition to the data sets provided, participating research teams were allowed to use other public data for system development and implementation.	0
3148	30796	30796	2020.nlptea-1.4	Data Sets	10	54	3.0	2.0	Use of other data should be specified in the final system report.	0
3149	30797	30797	2020.nlptea-1.4	Data Sets	11	55	3.0	2.0	Test Set:	0
3150	30798	30798	2020.nlptea-1.4	Data Sets	12	56	3.0	2.0	This set consists of testing units used for evaluating system performance.	0
3151	30799	30799	2020.nlptea-1.4	Data Sets	13	57	4.0	2.0	Table 3 shows statistics for the testing set for this year.	0
3152	30800	30800	2020.nlptea-1.4	Data Sets	14	58	4.0	2.0	According to the sampling in the writing sessions in HSK, over 40% of the sentences contain no error.	0
3153	30801	30801	2020.nlptea-1.4	Data Sets	15	59	4.0	2.0	This was simulated in the test set, in order to test the performance of the systems in false positive identification.	0
3154	30802	30802	2020.nlptea-1.4	Data Sets	16	60	4.0	2.0	The distributions of error types (Table 4) are similar with that of the training set.	0
3155	30803	30803	2020.nlptea-1.4	Data Sets	17	61	4.0	3.0	The proportion of the correct sentences is sampled from data of the online Dynamic Corpus of HSK 1 .	0
3156	30804	30804	2020.nlptea-1.4	Error Type	1	62	1.0	3.0	Performance Metrics	0
3157	30805	30805	2020.nlptea-1.4	Error Type	2	63	1.0	3.0	Table 5 shows the confusion matrix used for evaluating system performance.	0
3158	30806	30806	2020.nlptea-1.4	Error Type	3	64	1.0	3.0	In this matrix, TP (True Positive) is the number of sentences with grammatical errors are correctly identified by the developed system; FP (False Positive) is the number of sentences in which non-existent grammatical errors are identified as errors; TN (True Negative) is the number of sentences without grammatical errors that are correctly identified as such; FN (False Negative) is the number of sentences with grammatical errors which the system incorrectly identifies as being correct.	0
3159	30807	30807	2020.nlptea-1.4	Error Type	4	65	1.0	3.0	The criteria for judging correctness are determined at three levels as follows.	0
3160	30808	30808	2020.nlptea-1.4	Error Type	5	66	1.0	3.0	(1) Detection-level: Binary classification of a given sentence, that is, correct or incorrect, should be completely identical with the gold standard.	0
3161	30809	30809	2020.nlptea-1.4	Error Type	6	67	1.0	3.0	All error types will be regarded as incorrect.	0
3162	30810	30810	2020.nlptea-1.4	Error Type	7	68	2.0	3.0	(2) Identification-level:	0
3163	30811	30811	2020.nlptea-1.4	Error Type	8	69	2.0	3.0	This level could be considered as a multi-class categorization problem.	0
3164	30812	30812	2020.nlptea-1.4	Error Type	9	70	2.0	3.0	All error types should be clearly identified.	0
3165	30813	30813	2020.nlptea-1.4	Error Type	10	71	2.0	3.0	A 1 http://bcc.blcu.edu.cn/hsk correct case should be completely identical with the gold standard of the given error type.	0
3166	30814	30814	2020.nlptea-1.4	Error Type	11	72	2.0	3.0	(3) Position-level:	0
3167	30815	30815	2020.nlptea-1.4	Error Type	12	73	2.0	3.0	In addition to identifying the error types, this level also judges the occurrence range of the grammatical error.	0
3168	30816	30816	2020.nlptea-1.4	Error Type	13	74	3.0	3.0	That is to say, the system results should be perfectly identical with the quadruples of the gold standard.	0
3169	30817	30817	2020.nlptea-1.4	Error Type	14	75	3.0	3.0	Besides the traditional criteria in the past share tasks, Correction-level was introduced to CGED since 2018.	0
3170	30818	30818	2020.nlptea-1.4	Error Type	15	76	3.0	3.0	(4) Correction-level:	0
3171	30819	30819	2020.nlptea-1.4	Error Type	16	77	3.0	3.0	For the error types of Selection and Missing, recommended corrections are required.	0
3172	30820	30820	2020.nlptea-1.4	Error Type	17	78	3.0	3.0	At most 3 recommended corrections are allowed for each S and M type error.	0
3173	30821	30821	2020.nlptea-1.4	Error Type	18	79	3.0	3.0	In this level the amount of the corrections recommended would influence the precision and F1 in this level.	0
3174	30822	30822	2020.nlptea-1.4	Error Type	19	80	4.0	3.0	The trust of the recommendation would be test.	0
3175	30823	30823	2020.nlptea-1.4	Error Type	20	81	4.0	3.0	The sub-track TOP1 count only one recommended correction, while TOP3 count one hit, if one correction in three hits the golden standard, ignoring its ranking.	0
3176	30824	30824	2020.nlptea-1.4	Error Type	21	82	4.0	3.0	The following metrics are measured at all levels with the help of the confusion matrix.	0
3177	30825	30825	2020.nlptea-1.4	Error Type	22	83	4.0	3.0	 False Positive Rate = FP / (FP+TN)  Accuracy = (TP+TN) / (TP+FP+TN+FN)  Precision = TP / (TP+FP)  Recall = TP / (TP+FN)  F1 =2*Precision*Recall / (Precision + Recall)	0
3178	30826	30826	2020.nlptea-1.4	Error Type	23	84	4.0	3.0	"For example, for 4 testing inputs with gold standards shown as ""00038800481, 6, 7, S"", ""00038800481, 8, 8, R"", ""00038800464, correct"", ""00038801261, 9, 9, M"", ""00038801261, 16, 16, S"" and ""00038801320, 19, 25, W"", the system may output the result as ""00038800481, 2, 3, S"", ""00038800481, 4, 5, S"", ""00038800481, 8, 8, R"", ""00038800464, correct"", ""00038801261, 9, 9, M"", ""00038801261, 16, 19, S"" and ""00038801320, 19, 25, M""."	0
3179	30827	30827	2020.nlptea-1.4	Error Type	24	85	4.0	3.0	The scoring script will yield the following performance.	0
3180	30828	30828	2020.nlptea-1.4	Error Type	25	86	4.0	3.0	False Positive Rate (FPR) = 0 (=0/1) Detection-level:	0
3181	30829	30829	2020.nlptea-1.4	Evaluation Results	1	87	1.0	3.0	Table 6 summarizes the submission statistics for the 17 participating teams.	0
3182	30830	30830	2020.nlptea-1.4	Evaluation Results	2	88	1.0	3.0	In the official testing phase, each participating team was allowed to submit at most three runs.	0
3183	30831	30831	2020.nlptea-1.4	Evaluation Results	3	89	1.0	3.0	Of the 17 teams, 11 teams submitted their testing results in Correction-level, for a total of 43 runs.	0
3184	30832	30832	2020.nlptea-1.4	Evaluation Results	4	90	1.0	3.0	Table 7 to 11 show the testing results of the CGED2020 in 6 tracks: false positive rate (FPR), detection level, identification level, position level and correction level (in two settings: top1 and top3).	0
3185	30833	30833	2020.nlptea-1.4	Evaluation Results	5	91	2.0	4.0	All runs of top F1 score are highlighted in the tables.	0
3186	30834	30834	2020.nlptea-1.4	Evaluation Results	6	92	2.0	4.0	The CYUT achieved the lowest FPR of 0.0163, about one third of the lowest FPR in the CGED 2018.	0
3187	30835	30835	2020.nlptea-1.4	Evaluation Results	7	93	2.0	4.0	Detection-level evaluations are designed to detect whether a sentence contains grammatical errors or not.	0
3188	30836	30836	2020.nlptea-1.4	Evaluation Results	8	94	2.0	4.0	A neutral baseline can be easily achieved by reporting all testing sentences containing errors.	0
3189	30837	30837	2020.nlptea-1.4	Evaluation Results	9	95	2.0	4.0	According to the test data distribution, the baseline system can achieve an accuracy of 0.7893.	0
3190	30838	30838	2020.nlptea-1.4	Evaluation Results	10	96	3.0	4.0	However, not all systems performed above the baseline.	0
3191	30839	30839	2020.nlptea-1.4	Evaluation Results	11	97	3.0	4.0	The system result submitted by NJU-NLP achieved the best detection F1 of 0.9122, beating the 0.9 mark for the first time.	0
3192	30840	30840	2020.nlptea-1.4	Evaluation Results	12	98	3.0	4.0	For identification-level evaluations, the systems need to identify the error types in a given unit.	0
3193	30841	30841	2020.nlptea-1.4	Evaluation Results	13	99	3.0	4.0	The system developed by Flying and Orange	0
3194	30842	30842	2020.nlptea-1.4	Evaluation Results	14	100	3.0	4.0	Plus provided the highest F1 score of 0.6736 and 0.6726 for grammatical error identification.	0
3195	30843	30843	2020.nlptea-1.4	Evaluation Results	15	101	4.0	4.0	For position-level, Flying achieved the best F1 score of 0.4041, crossing the 0.4 mark for the first time.	0
3196	30844	30844	2020.nlptea-1.4	Evaluation Results	16	102	4.0	4.0	Orange	0
3197	30845	30845	2020.nlptea-1.4	Evaluation Results	17	103	4.0	4.0	Plus reached 0.394.	0
3198	30846	30846	2020.nlptea-1.4	Evaluation Results	18	104	4.0	4.0	Perfectly identifying the error types and their corresponding positions is difficult because the error propagation is serious.	0
3199	30847	30847	2020.nlptea-1.4	Evaluation Results	19	105	4.0	4.0	In correction-level, UNIPUS-Flaubert achieved best F1 of 0.1891 in top1 setting and YD_NLP of 0.1885 top3 setting.	0
3200	30848	30848	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	1	106	1.0	4.0	In CGED 2020, the implementation of pre-trained model like BERT achieved significant improvement in many tracks.	0
3201	30849	30849	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	2	107	1.0	4.0	"The ""standard pipe-line"" biLSTM+CRF in CGED2017 and 2018 is replaced."	0
3202	30850	30850	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	3	108	1.0	4.0	Hybrid methods based on pre-trained model were proposed by most of the teams.	0
3203	30851	30851	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	4	109	2.0	4.0	ResNet, graph convolution network and data argumentation appeared for the first time in the solutions.	0
3204	30852	30852	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	5	110	2.0	4.0	The rethinking the data construction (including pseudo data generation) and feature selection did not attract the attention of the participants.	0
3205	30853	30853	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	6	111	2.0	4.0	However, the balance of the FPR and other track did not progress a lot.	0
3206	30854	30854	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	7	112	3.0	4.0	The rough merging strategies implemented in hybrid methods and the over generation of generation models may lead the drop in FPR.	0
3207	30855	30855	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	8	113	3.0	4.0	From organizers' perspectives, a good system should have a high F1 score and a low false positive rate.	0
3208	30856	30856	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	9	114	3.0	4.0	In summary, none of the submitted systems provided a comprehensive superior performance using different metrics, indicating the difficulty of developing systems for effective grammatical error diagnosis, especially in CFL contexts.	0
3209	30857	30857	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	10	115	4.0	4.0	It is worth noting that in the track of detection, the performance over 0.9 is close to the application of actual scene.	0
3210	30858	30858	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	11	116	4.0	4.0	In the highly focused track of position and correction, variant teams lead the ranks, unlike the past CGEDs.	0
3211	30859	30859	2020.nlptea-1.4	Participant (Ordered by names) #Runs Correction-level	12	117	4.0	4.0	It's a very exciting phenomena indicating the attraction the task increased quickly.	0
3212	30860	30860	2020.nlptea-1.4	Conclusion	1	118	1.0	4.0	This study describes the NLP-TEA 2020 shared task for Chinese grammatical error diagnosis, including task design, data preparation, performance metrics, and evaluation results.	0
3213	30861	30861	2020.nlptea-1.4	Conclusion	2	119	2.0	4.0	Regardless of actual performance, all submissions contribute to the common effort to develop Chinese grammatical error diagnosis system, and the individual reports in the proceedings provide useful insights into computer-assisted language learning for CFL learners.	0
3214	30862	30862	2020.nlptea-1.4	Conclusion	3	120	3.0	4.0	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
3215	30863	30863	2020.nlptea-1.4	Conclusion	4	121	4.0	4.0	Therefore, all data sets with gold standards and scoring scripts are publicly available online at http://www.cged.science.	0
3216	31208	31208	W18-3601	title	1	1	4.0	1.0	The First Multilingual Surface Realisation Shared Task (SR&apos;18): Overview and Evaluation Results	0
3217	31209	31209	W18-3601	abstract	1	2	1.0	1.0	We report results from the SR'18 Shared Task, a new multilingual surface realisation task organised as part of the ACL'18 Workshop on Multilingual Surface Realisation.	0
3218	31210	31210	W18-3601	abstract	2	3	2.0	1.0	As in its English-only predecessor task SR'11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed.	0
3219	31211	31211	W18-3601	abstract	3	4	2.0	1.0	The shallow track was offered in ten, and the deep track in three languages.	0
3220	31212	31212	W18-3601	abstract	4	5	3.0	1.0	Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity.	0
3221	31213	31213	W18-3601	abstract	5	6	4.0	1.0	This report presents the evaluation results, along with descriptions of the SR'18 tracks, data and evaluation methods.	0
3222	31214	31214	W18-3601	abstract	6	7	4.0	1.0	For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.	0
3223	31215	31215	W18-3601	Introduction and Task Overview	1	8	1.0	1.0	Natural Language Generation (NLG) is attracting growing interest both in the form of end-toend tasks (e.g. data-to-text and text-to-text generation), and as embedded component tasks (e.g. in abstractive summarisation, dialogue-based interaction and question answering).	0
3224	31216	31216	W18-3601	Introduction and Task Overview	2	9	1.0	1.0	NLG research has been given a boost by two recent developments: the rapid spread of neural language generation techniques, and the growing availability of multilingual treebanks annotated with Universal Dependencies 1 (UD), to the point 1 http://universaldependencies.org/ where as many as 70 treebanks covering about 50 languages can now be downloaded freely.	0
3225	31217	31217	W18-3601	Introduction and Task Overview	3	10	1.0	1.0	2 UD treebanks facilitate the development of applications that work potentially across all languages for which UD treebanks are available in a uniform fashion, which is a big advantage for system developers.	0
3226	31218	31218	W18-3601	Introduction and Task Overview	4	11	1.0	1.0	As has already been seen in parsing, UD treebanks are also a good basis for multilingual shared tasks: a method that works for some languages may also work for others.	0
3227	31219	31219	W18-3601	Introduction and Task Overview	5	12	1.0	1.0	The SR'18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation.	1
3228	31220	31220	W18-3601	Introduction and Task Overview	6	13	2.0	1.0	SR'18 also addresses questions about just how suitable and useful the notion of universal dependencies-which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular-is for NLG.	0
3229	31221	31221	W18-3601	Introduction and Task Overview	7	14	2.0	1.0	SR'18 follows the SR'11 pilot surface realisation task for English  which was part of Generation Challenges 2011 (GenChal'11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks.	0
3230	31222	31222	W18-3601	Introduction and Task Overview	8	15	2.0	1.0	Outside of the SR tasks, just three 'deep' NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG 3 (Gardent et al., 2017), Se-mEval Task 9 4 (May and Priyadarshi, 2017), and E2E 5 (Novikova et al., 2017).	0
3231	31223	31223	W18-3601	Introduction and Task Overview	9	16	2.0	1.0	What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016):	0
3232	31224	31224	W18-3601	Introduction and Task Overview	10	17	2.0	1.0	http:// universaldependencies.org/conll17/.	0
3233	31225	31225	W18-3601	Introduction and Task Overview	11	18	2.0	1.0	3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ Interaction	0
3234	31226	31226	W18-3601	Introduction and Task Overview	12	19	3.0	1.0	Lab/E2E/ tasks have only been offered for English.	0
3235	31227	31227	W18-3601	Introduction and Task Overview	13	20	3.0	1.0	As in SR'11, the Multilingual Surface Realisation shared task (SR'18) comprises two tracks with different levels of difficulty:	0
3236	31228	31228	W18-3601	Introduction and Task Overview	14	21	3.0	1.0	Shallow Track:	0
3237	31229	31229	W18-3601	Introduction and Task Overview	15	22	3.0	1.0	This track starts from genuine UD structures in which word order information has been removed and tokens have been lemmatised.	0
3238	31230	31230	W18-3601	Introduction and Task Overview	16	23	3.0	1.0	In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations.	0
3239	31231	31231	W18-3601	Introduction and Task Overview	17	24	4.0	1.0	The task amounts to determining the word order and inflecting words.	0
3240	31232	31232	W18-3601	Introduction and Task Overview	18	25	4.0	1.0	Deep Track:	0
3241	31233	31233	W18-3601	Introduction and Task Overview	19	26	4.0	1.0	This track starts from UD structures from which functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological and syntactic information have been removed.	0
3242	31234	31234	W18-3601	Introduction and Task Overview	20	27	4.0	1.0	In addition to what is required for the Shallow Track, the task in the Deep Track thus also requires reintroduction of the removed functional words and morphological features.	0
3243	31235	31235	W18-3601	Introduction and Task Overview	21	28	4.0	1.0	In the remainder of this paper, we describe the data we used in the two tracks (Section 2), and the evaluation methods we used to evaluate submitted systems (Sections 3.1 and 3.2).	0
3244	31236	31236	W18-3601	Introduction and Task Overview	22	29	4.0	1.0	We then briefly introduce the participating systems (Section 4), report and discuss evaluation results (Section 5), and conclude with some discussion and a look to the future (Section 6).	0
3245	31237	31237	W18-3601	Data	1	30	1.0	1.0	To create the SR'18 training and testing data, we used as data sources ten UD treebanks for which annotations of reasonable quality were available, providing PoS tags and morphologically relevant markup (number, tense, verbal finiteness, etc.):	0
3246	31238	31238	W18-3601	Data	2	31	1.0	1.0	UD Arabic, UD Czech, UD Dutch, UD English, UD Finnish, UD French, UD Italian, UD Portuguese, UD Russian-SynTag	0
3247	31239	31239	W18-3601	Data	3	32	1.0	1.0	Rus and UD Spanish-AnCora.	0
3248	31240	31240	W18-3601	Data	4	33	1.0	1.0	6	0
3249	31241	31241	W18-3601	Data	5	34	2.0	1.0	We created training and test data for all ten languages for the Shallow Track, and for three of the languages, namely English, French and Spanish, for the Deep Track.	0
3250	31242	31242	W18-3601	Data	6	35	2.0	1.0	Inputs in both Shallow and Deep Tracks are trees, and are released in CoNLL-U format, with no meta-information.	0
3251	31243	31243	W18-3601	Data	7	36	2.0	1.0	7 Figures 1, 2 and 3 show a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it.	0
3252	31244	31244	W18-3601	Data	8	37	2.0	1.0	To create inputs to the Shallow Track, the UD structures were processed as follows:	0
3253	31245	31245	W18-3601	Data	9	38	3.0	1.0	1	0
3254	31246	31246	W18-3601	Data	10	39	3.0	1.0	Word order information was removed by randomised scrambling;	0
3255	31247	31247	W18-3601	Data	11	40	3.0	1.0	2. Words were replaced by their lemmas.	0
3256	31248	31248	W18-3601	Data	12	41	3.0	1.0	For the Deep Track, the following steps were additionally carried out:	0
3257	31249	31249	W18-3601	Data	13	42	4.0	1.0	3	0
3258	31250	31250	W18-3601	Data	14	43	4.0	1.0	Edge labels were generalised into predicate/argument labels, in the Prop-Bank/NomBank (Palmer et al., 2005;	0
3259	31251	31251	W18-3601	Data	15	44	4.0	1.0	Meyers et al., 2004) fashion.	0
3260	31252	31252	W18-3601	Data	16	45	4.0	1.0	That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1.	0
3261	31253	31253	W18-3601	Functional prepositions and conjunctions in	1	46	1.0	1.0	argument position (i.e. prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure) are removed (e.g. by and of in Figure 2    node for the subject is added if an originally finite verb has no first argument and no available argument to build a passive; for a prodrop language such as Spanish, a dummy pronoun is added if the first argument is missing.	0
3262	31254	31254	W18-3601	Functional prepositions and conjunctions in	2	47	1.0	1.0	7. Surface-level morphologically relevant information as prescribed by syntactic structure or agreement (such as verbal finiteness or verbal number) is removed, whereas semantic-level information such as nominal number and verbal tense is retained.	0
3263	31255	31255	W18-3601	Functional prepositions and conjunctions in	3	48	1.0	2.0	8. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) are removed, and only coarse-grained ones are retained (column 4 in Figures 2 and 3).	0
3264	31256	31256	W18-3601	Functional prepositions and conjunctions in	4	49	2.0	2.0	Shallow Track inputs were generated with the aid of a simple Python script from the original UD structures.	0
3265	31257	31257	W18-3601	Functional prepositions and conjunctions in	5	50	2.0	2.0	During the conversion, we filtered out sentences that contained dependencies that only make sense in an analysis context (e.g. reparandum, or orphan).	0
3266	31258	31258	W18-3601	Functional prepositions and conjunctions in	6	51	2.0	2.0	This amounted to around 1.5% of sentences for the different languages on average; see Table 2 for an overview of the final sizes of the datasets.	0
3267	31259	31259	W18-3601	Functional prepositions and conjunctions in	7	52	2.0	2.0	Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of graphtransduction grammars that cover steps 3-8 above (in a similar fashion as Mille et al. (2017)).	0
3268	31260	31260	W18-3601	Functional prepositions and conjunctions in	8	53	3.0	2.0	There is a node-to-node correspondence between the deep and shallow input structures.	0
3269	31261	31261	W18-3601	Functional prepositions and conjunctions in	9	54	3.0	2.0	The Deep Track inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information (see, e.g. the inputs in the SemEval, WebNLG, E2E shared tasks).	0
3270	31262	31262	W18-3601	Functional prepositions and conjunctions in	10	55	3.0	2.0	At the same time, we used only information found in the UD syntactic structures to create the deep inputs, and tried to keep their structure simple.	0
3271	31263	31263	W18-3601	Functional prepositions and conjunctions in	11	56	4.0	2.0	It can be argued that not all the information necessary to reconstruct the original sentences is available in the Deep Track inputs.	0
3272	31264	31264	W18-3601	Functional prepositions and conjunctions in	12	57	4.0	2.0	Task definitions specifically designed for NLG, as used e.g. in Se-mEval Task 9, tend to use abstract meaning representations (AMRs) as inputs that contain additional information such as OntoNotes labelling or typed circumstantials, which make the generation task easier.	0
3273	31265	31265	W18-3601	Functional prepositions and conjunctions in	13	58	4.0	2.0	In the SR'18 Deep Track inputs, words are not disambiguated, full prepositions may be missing, and some argument relations may be underspecified or missing.	0
3274	31266	31266	W18-3601	Functional prepositions and conjunctions in	14	59	4.0	2.0	train 6,016 66,485 12,375 14,289 12,030 14,529 12,796 12,318 8,325 48  3 Evaluation Methods	0
3275	31267	31267	W18-3601	Automatic methods	1	60	1.0	2.0	We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems.	0
3276	31268	31268	W18-3601	Automatic methods	2	61	1.0	2.0	BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences.	0
3277	31269	31269	W18-3601	Automatic methods	3	62	1.0	2.0	We use the smoothed version and report results for n = 4.	0
3278	31270	31270	W18-3601	Automatic methods	4	63	2.0	2.0	NIST 9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative.	0
3279	31271	31271	W18-3601	Automatic methods	5	64	2.0	2.0	Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text.	0
3280	31272	31272	W18-3601	Automatic methods	6	65	2.0	2.0	The resulting number is then divided by the number of characters in the reference text, and finally subtracted from 1, in order to align with the other metrics.	0
3281	31273	31273	W18-3601	Automatic methods	7	66	3.0	2.0	Spaces and punctuation marks count as characters; output texts were otherwise normalised as for all metrics (see below).	0
3282	31274	31274	W18-3601	Automatic methods	8	67	3.0	2.0	The figures in the tables below are the systemlevel scores for BLEU and NIST, and the mean sentence-level scores for DIST.	0
3283	31275	31275	W18-3601	Automatic methods	9	68	3.0	2.0	Text normalisation:	0
3284	31276	31276	W18-3601	Automatic methods	10	69	4.0	2.0	Output texts were normalised prior to computing metrics by lowercasing all tokens, removing any extraneous whitespace characters.	0
3285	31277	31277	W18-3601	Automatic methods	11	70	4.0	2.0	Missing outputs: Missing outputs were scored 0.	0
3286	31278	31278	W18-3601	Automatic methods	12	71	4.0	2.0	Since coverage was 100% for all systems except one, we only report results for all sentences (incorporating the missing-output penalty), rather than also separately reporting scores for just the in-coverage items.	0
3287	31279	31279	W18-3601	Human-assessed methods	1	72	1.0	2.0	We assessed two quality criteria in the human evaluations, in separate evaluation experiments: Readability and Meaning Similarity.	0
3288	31280	31280	W18-3601	Human-assessed methods	2	73	1.0	2.0	As in SR'11 , we used continuous sliders as rating tools, because raters tend to prefer them .	0
3289	31281	31281	W18-3601	Human-assessed methods	3	74	1.0	2.0	Slider positions were mapped to values from 0 to 100 (best).	0
3290	31282	31282	W18-3601	Human-assessed methods	4	75	1.0	2.0	Raters were first given brief instructions, including instructions to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation.	0
3291	31283	31283	W18-3601	Human-assessed methods	5	76	1.0	2.0	"The part of the instructions used only in the Readability assessment experiments was: ""The quality criterion you need to assess is Readability."	0
3292	31284	31284	W18-3601	Human-assessed methods	6	77	2.0	2.0	This is sometimes called fluency, and your task is to decide how well the given text reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc.	0
3293	31285	31285	W18-3601	Human-assessed methods	7	78	2.0	2.0	"Please rate the text by moving the slider to the position that corresponds to your rating, where 0 is the worst, and 100 is the best rating."""	0
3294	31286	31286	W18-3601	Human-assessed methods	8	79	2.0	2.0	The corresponding instructions for Meaning Similarity assessment, in which system outputs were compared to reference sentences, were as follows:	0
3295	31287	31287	W18-3601	Human-assessed methods	9	80	2.0	2.0	"""The quality criterion you need to assess is Meaning Similarity."	0
3296	31288	31288	W18-3601	Human-assessed methods	10	81	2.0	2.0	You need to read both texts, and then decide how close in meaning the second text (in black) is to the first (in grey).	0
3297	31289	31289	W18-3601	Human-assessed methods	11	82	2.0	2.0	Please use the slider at the bottom of the page to express your rating.	0
3298	31290	31290	W18-3601	Human-assessed methods	12	83	3.0	2.0	The closer in meaning the second text clipping is to the first, the further to the right (towards 100) you need to place the slider.	0
3299	31291	31291	W18-3601	Human-assessed methods	13	84	3.0	2.0	"In other words, a rating of 100% would mean that the meaning of the two text clippings is exactly identical."""	0
3300	31292	31292	W18-3601	Human-assessed methods	14	85	3.0	2.0	Slider design:	0
3301	31293	31293	W18-3601	Human-assessed methods	15	86	3.0	2.0	In SR'11, a slider design was used, which had a smiley face at the 100 end and a frowning face at the 0 end, with the pointer starting out at 50.	0
3302	31294	31294	W18-3601	Human-assessed methods	16	87	3.0	2.0	For conformity with what has emerged as a new affordable human evaluation standard over the past two years in the main machine translation shared tasks held at WMT (Bojar et al., 2017a), we changed this design to look as follows, with the pointer starting at 0: Test data sets for human evaluations:	0
3303	31295	31295	W18-3601	Human-assessed methods	17	88	4.0	2.0	Test set sizes out of the box varied considerably for the different languages.	0
3304	31296	31296	W18-3601	Human-assessed methods	18	89	4.0	2.0	For the human test sets we selected either the entire set or a subset of 1,000, whichever was the smaller number, for a given language.	0
3305	31297	31297	W18-3601	Human-assessed methods	19	90	4.0	2.0	For subsets, test set items were selected randomly but ensuring a similar sentence length distribution as in the whole set.	0
3306	31298	31298	W18-3601	Human-assessed methods	20	91	4.0	2.0	Reported scores: Again in keeping with the WMT approach, we report both average raw scores and average standardised scores per system.	0
3307	31299	31299	W18-3601	Human-assessed methods	21	92	4.0	2.0	In order to produce standardised scores we simply map each individual evaluator's scores to their standard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator's mean and standard deviation.	0
3308	31300	31300	W18-3601	Human-assessed methods	22	93	4.0	2.0	For both raw and standard scores, we compute the mean of sentence-level scores.	0
3309	31301	31301	W18-3601	Mechanical	1	94	1.0	2.0	Turk evaluations	0
3310	31302	31302	W18-3601	Mechanical	2	95	1.0	3.0	For three of the languages in the shallow track (English, Spanish and French), we replicated the human evaluation method from WMT'17, known as Direct Assessment (DA) (Graham et al., 2016), exactly, except that we also ran (separate) experiments to assess the Readability criterion, using the same method.	0
3311	31303	31303	W18-3601	Mechanical	3	96	2.0	3.0	Quality assurance: System outputs are randomly assigned to HITs (following Mechanical Turk terminology) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a 'damaged' version and (iii) some are replaced by their corresponding reference texts.	0
3312	31304	31304	W18-3601	Mechanical	4	97	2.0	3.0	In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high.	0
3313	31305	31305	W18-3601	Mechanical	5	98	3.0	3.0	For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a).	0
3314	31306	31306	W18-3601	Mechanical	6	99	3.0	3.0	Below we report QA figures for the MTurk evaluations (Section 3.2.1).	0
3315	31307	31307	W18-3601	Mechanical	7	100	4.0	3.0	Code:	0
3316	31308	31308	W18-3601	Mechanical	8	101	4.0	3.0	We were able to reuse, with minor adaptations, the code produced for the WMT'17 evaluations.	0
3317	31309	31309	W18-3601	Mechanical	9	102	4.0	3.0	10	0
3318	31310	31310	W18-3601	Google Data Compute Evaluation	1	103	1.0	3.0	In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google's internal 'Data Compute' system evaluation service, where experienced evaluators carefully assess each system output.	0
3319	31311	31311	W18-3601	Google Data Compute Evaluation	2	104	1.0	3.0	We used an interface that matches the WMT'17 interface above, as closely as was possible within the constraints of the Data Compute platform.	0
3320	31312	31312	W18-3601	Google Data Compute Evaluation	3	105	2.0	3.0	Everything stated at the beginning of Section 3.2 also holds for the expert annotator evaluations with Google Data Compute.	0
3321	31313	31313	W18-3601	Google Data Compute Evaluation	4	106	2.0	3.0	Quality assurance: Because in the Google Data Compute version of the evaluation experiment we were using expert evaluators from a pool of workers routinely employed to perform such tasks, we did not replicate the WMT'17 QA techniques precisely, opting for a simpler test of self-consistency, or intra-evaluator agreement (IEA) instead.	0
3322	31314	31314	W18-3601	Google Data Compute Evaluation	5	107	3.0	3.0	Test set items were randomly grouped into sets of 100 (which we are also calling HITs here for uniformity) and order was again randomised before presentation to evaluators.	0
3323	31315	31315	W18-3601	Google Data Compute Evaluation	6	108	3.0	3.0	Each evaluator did at least one HIT.	0
3324	31316	31316	W18-3601	Google Data Compute Evaluation	7	109	4.0	3.0	Each HIT contained 5 items which were duplicated to test for IEA which we computed as the average Pearson correlation coefficient per HIT.	0
3325	31317	31317	W18-3601	Google Data Compute Evaluation	8	110	4.0	3.0	The average IEA for English was 0.75 on the raw scores for Meaning Similarity, and 0.66 for Readability.	0
3326	31318	31318	W18-3601	Overview of Submitted Systems	1	111	1.0	3.0	Eight different teams (out of twenty-one registered) submitted outputs to SR'18: the ADAPT Centre (ADAPT, Ireland), AX Semantics (AX, Germany), IIT-BHU Varanasi (IIT-BHU, India), Ohio State University (OSU, USA), University of São Paulo (NILC, Brazil), Tilburg University (Tilburg, The Netherlands), Università degli Studi di Torino (DipInfo-UniTo, Italy), and Technische Universität Darmstadt (BinLin, Germany).	0
3327	31319	31319	W18-3601	Overview of Submitted Systems	2	112	1.0	3.0	All teams submitted outputs for at least the English Shallow Track; one team participated in the Deep Track (ADAPT, English), and three teams submitted outputs for all ten languages of the Shallow Track (AX, OSU, and BinLin).	0
3328	31320	31320	W18-3601	Overview of Submitted Systems	3	113	1.0	3.0	Most submitted systems are based on neural components, and break down the surface realisation task into two subtasks: linearisation, and word inflection.	0
3329	31321	31321	W18-3601	Overview of Submitted Systems	4	114	2.0	3.0	Details of each approach are provided in the teams' reports elsewhere in this volume; here, we briefly summarise each approach:	0
3330	31322	31322	W18-3601	Overview of Submitted Systems	5	115	2.0	3.0	ADAPT uses linearised parse tree inputs to train a sequence-to-sequence LSTM model with copy attention, augmenting the training set with additional synthetic data.	0
3331	31323	31323	W18-3601	Overview of Submitted Systems	6	116	2.0	3.0	AX is trained on word pairs for ordering and is combined with a rule-based morphology component.	0
3332	31324	31324	W18-3601	Overview of Submitted Systems	7	117	2.0	3.0	IIT-BHU uses an LSTM-based encoderdecoder model for word re-inflection, and a Language Model-based approach for word reordering.	0
3333	31325	31325	W18-3601	Overview of Submitted Systems	8	118	3.0	3.0	OSU first generates inflected wordforms with a neural sequence-to-sequence model, and then incrementally linearises them using a global linear model over features that take into account the dependency structure and dependency location.	0
3334	31326	31326	W18-3601	Overview of Submitted Systems	9	119	3.0	3.0	11 NILC is a neural-based system that uses a bottom-up approach to build the sentence using the dependency relations together with a language model, and language-specific lexicons to produce the word forms of each lemma in the sentence.	0
3335	31327	31327	W18-3601	Overview of Submitted Systems	10	120	3.0	3.0	Tilburg works by first preprocessing an input dependency tree into an ordered linearised string, 11 Some of OSU's outputs were submitted after the start of the human evaluations and are not included in this report; outputs submitted late, but before the human evaluation started, are included and marked with asterisks in the results tables.	0
3336	31328	31328	W18-3601	Overview of Submitted Systems	11	121	4.0	3.0	which is then realised using a statistical machine translation model.	0
3337	31329	31329	W18-3601	Overview of Submitted Systems	12	122	4.0	3.0	DipInfo-Uni	0
3338	31330	31330	W18-3601	Overview of Submitted Systems	13	123	4.0	3.0	To employs two separate neural networks with different architectures to predict the word ordering and the morphological inflection independently; outputs are combined to produce the final sentence.	0
3339	31331	31331	W18-3601	Overview of Submitted Systems	14	124	4.0	3.0	BinLin uses one neural module as a binary classifier in a sequential process of ordering token lemmas, and another for character-level morphology generation where the words are inflected to finish the surface realisation.	0
3340	31332	31332	W18-3601	Evaluation results	1	125	1.0	3.0	Results from metric evaluations	0
3341	31333	31333	W18-3601	Evaluation results	2	126	1.0	3.0	Tables 3-5 show BLEU-4, NIST, and DIST results for both the Shallow and Deep tracks, for all submitted systems; results are listed in order of number of languages submitted for.	0
3342	31334	31334	W18-3601	Evaluation results	3	127	2.0	3.0	Best results for each language are shown in boldface.	0
3343	31335	31335	W18-3601	Evaluation results	4	128	2.0	3.0	In terms of BLEU-4, in the Shallow Track, Tilburg obtained the best scores for four languages (French, Italian, Dutch, Portuguese), OSU for three (Arabic, Spanish, Finnish), BinLin for two (Czech, Russian), and ADAPT for one (English).	0
3344	31336	31336	W18-3601	Evaluation results	5	129	3.0	3.0	The highest BLEU-4 scores across languages were obtained on the English and Spanish datasets, with BLEU-4 scores of 69.14 (ADAPT) and 65.31 (OSU) respectively.	0
3345	31337	31337	W18-3601	Evaluation results	6	130	3.0	3.0	Results are identical for DIST, except that AX, rather than BinLin, has the highest score for Czech.	0
3346	31338	31338	W18-3601	Evaluation results	7	131	4.0	3.0	The picture for NIST is also very similar to that for BLEU-4, except that ADAPT and OSU are tied for best NIST score for English, and Bin-Lin (rather than Tilburg) has the best NIST score for Dutch.	0
3347	31339	31339	W18-3601	Evaluation results	8	132	4.0	3.0	In the Deep Track, only ADAPT submitted system outputs (English), and as expected, the scores are much lower than for the Shallow Track, across all metrics.	0
3348	31340	31340	W18-3601	Results from human evaluations	1	133	1.0	3.0	Given the small number of submissions in the Deep Track, we conducted human evaluations for the Shallow Track only.	0
3349	31341	31341	W18-3601	Results from human evaluations	2	134	2.0	3.0	We used Mechanical Turk for the three languages for which this is feasible (English, Spanish and French), and our aim was to also conduct evaluations via Google's Data Compute service for three additional languages which had the next highest numbers of submissions, as    well as for English in order to enable us to compare results obtained with the two different methods.	0
3350	31342	31342	W18-3601	Results from human evaluations	3	135	3.0	3.0	However, most of the latter evaluations are still ongoing and will be reported separately in a future paper.	0
3351	31343	31343	W18-3601	Results from human evaluations	4	136	4.0	3.0	Below, we report Google Data Compute results and comparisons with Mechanical Turk results, for English only.	0
3352	31344	31344	W18-3601	Mechanical	1	137	1.0	3.0	Turk results	0
3353	31345	31345	W18-3601	Mechanical	2	138	1.0	3.0	Tables 6, 7 and 8 show the results of the human evaluation carried out via Mechanical Turk with Direct Assessment (MTurk DA), for English, French and Spanish, respectively.	0
3354	31346	31346	W18-3601	Mechanical	3	139	1.0	3.0	See Section 3.2 for details of the evaluation method.	0
3355	31347	31347	W18-3601	Mechanical	4	140	1.0	3.0	'DA' refers to the specific way in which scores are collected in the WMT approach which differs from what we did for SR'11, and here in the Google Data Compute experiments.	0
3356	31348	31348	W18-3601	Mechanical	5	141	1.0	3.0	English: Average Meaning Similarity DA scores for English systems range from 86.9% to 67% with OSU achieving the highest overall score in terms of both average raw DA scores and corresponding z-scores.	0
3357	31349	31349	W18-3601	Mechanical	6	142	1.0	4.0	Readability scores for the same set of systems range from 78.7% to 41.3%, revealing that MTurk workers rate the Meaning Similarity between generated texts and corresponding reference sentences higher in general than Readability.	0
3358	31350	31350	W18-3601	Mechanical	7	143	1.0	4.0	In order to investigate how Readability of system outputs compare to human-produced text, we included the original test sentences as a system in the Readability evaluation (for Meaning Similarity the notional score is 100%).	0
3359	31351	31351	W18-3601	Mechanical	8	144	1.0	4.0	Unsurprisingly, human text achieves the highest score in terms of Readability (78.7%) but is quite closely followed by the best performing system in terms of Readability, ADAPT (73.9%).	0
3360	31352	31352	W18-3601	Mechanical	9	145	1.0	4.0	Overall in the English Shallow Track, average DA scores for systems are close.	0
3361	31353	31353	W18-3601	Mechanical	10	146	2.0	4.0	We tested for statistical significance of differences between average DA scores using a Wilcoxon rank sum test.	0
3362	31354	31354	W18-3601	Mechanical	11	147	2.0	4.0	Figure 4 shows significance test results for each pair of systems participating in the English evaluation in the form of heatmaps where a green cell denotes a significantly higher average score for the system in that row over the system in that column, with a darker shade of green denoting a conclusion drawn with more certainty.	0
3363	31355	31355	W18-3601	Mechanical	12	148	2.0	4.0	Results show that two entries are tied for first place in terms of Meaning Similarity, OSU and ADAPT, with the small difference in average scores proving not statistically significant.	0
3364	31356	31356	W18-3601	Mechanical	13	149	2.0	4.0	In terms of Readability, however, the ADAPT sentences achieve a significantly higher readability score compared to OSU.	0
3365	31357	31357	W18-3601	Mechanical	14	150	2.0	4.0	French: Table 7 shows average DA scores for systems participating in the French Shallow Track.	0
3366	31358	31358	W18-3601	Mechanical	15	151	2.0	4.0	Meaning Similarity scores for French systems range from 72.9% to 48.6% with the Tilburg system achieving the highest overall score.	0
3367	31359	31359	W18-3601	Mechanical	16	152	2.0	4.0	In terms of Readability, again Tilburg achieves the highest average score of 65.4%, with a considerable gap to the next best entry, OSU.	0
3368	31360	31360	W18-3601	Mechanical	17	153	2.0	4.0	Compared to the human results, there is a larger gap than we saw for English outputs.	0
3369	31361	31361	W18-3601	Mechanical	18	154	2.0	4.0	Figure 5 shows results of tests for statistical significance between average DA scores for systems in the French Shallow Track.	0
3370	31362	31362	W18-3601	Mechanical	19	155	2.0	4.0	Tilburg achieves a significantly higher average DA score compared to all other systems in terms of both Meaning Similarity and Readability.	0
3371	31363	31363	W18-3601	Mechanical	20	156	3.0	4.0	All systems are significantly worse in terms of Readability than the human authored texts.	0
3372	31364	31364	W18-3601	Mechanical	21	157	3.0	4.0	Spanish: Table 8 shows average DA scores for systems participating in the Shallow Track for Spanish.	0
3373	31365	31365	W18-3601	Mechanical	22	158	3.0	4.0	Meaning Similarity scores range from 77.3% to 43.9%, with OSU achieving the highest score.	0
3374	31366	31366	W18-3601	Mechanical	23	159	3.0	4.0	In terms of Readability, the text produced by the systems ranges from 77.0% to 33.0%, and again OSU achieves the highest score.	0
3375	31367	31367	W18-3601	Mechanical	24	160	3.0	4.0	Figure 6 shows results of the corresponding significance tests: OSU significantly outperforms all other participating systems with respect to both evaluation criteria.	0
3376	31368	31368	W18-3601	Mechanical	25	161	3.0	4.0	Human-generated texts are significantly more readable than all system outputs.	0
3377	31369	31369	W18-3601	Mechanical	26	162	3.0	4.0	MTurk DA quality control: Only 31% of workers passed quality control (being able to replicate scores for same sentences and scoring damaged sentences lower, for full details see Bojar et al., 2017a), highlighting the danger of crowdsourcing without good quality control measures.	0
3378	31370	31370	W18-3601	Mechanical	27	163	3.0	4.0	The remaining 69%, who did not meet this criterion, were omitted from computation of the of-ficial DA results above.	0
3379	31371	31371	W18-3601	Mechanical	28	164	3.0	4.0	Of those 31% included in the evaluation, a very high proportion, 97%, showed no significant difference in scores collected in repeated assessment of the same sentences; these high levels of agreement are consistent with what we have seen in DA used for Machine Translation (Graham et al., 2016) and Video Captioning evaluation (Graham et al., 2017).	0
3380	31372	31372	W18-3601	Mechanical	29	165	4.0	4.0	Agreement with automatic metrics: Table 9 shows Pearson correlations between MTurk DA scores and automatic metric scores in the English, French and Spanish shallow tracks.	0
3381	31373	31373	W18-3601	Mechanical	30	166	4.0	4.0	Overall, BLEU agrees most consistently across the different tasks, achieving a correlation above 0.95 in all settings, whereas the correlation of NIST scores with human Meaning Similarity scores is just 0.854 for French, while DIST scores correlate with human Readability scores at just 0.831 for English.	0
3382	31374	31374	W18-3601	Mechanical	31	167	4.0	4.0	Conclusions from metric correlations should be drawn with a degree of caution, since in all cases the sample size from which we compute correlations is small, 8 systems for English, 5 for French, and 6 for Spanish.	0
3383	31375	31375	W18-3601	Mechanical	32	168	4.0	4.0	We carried out significance tests to investigate to what degree differences in correlations are likely to occur by chance.	0
3384	31376	31376	W18-3601	Mechanical	33	169	4.0	4.0	In order to take into account the fact that we are comparing correlations between human assessment and competing pairs of metrics (where metric scores themselves correlate with each other), we apply a Williams test for significance of differences in dependent correlations, as done in evaluation of Machine Translation metrics (Graham and Baldwin, 2014;	0
3385	31377	31377	W18-3601	Mechanical	34	170	4.0	4.0	Bojar et al., 2017b).	0
3386	31378	31378	W18-3601	Mechanical	35	171	4.0	4.0	Results are shown in Table 9.	0
3387	31379	31379	W18-3601	Mechanical	36	172	4.0	4.0	Correlations between metrics and human assessment in bold are not significantly lower than any other metric.	0
3388	31380	31380	W18-3601	Mechanical	37	173	4.0	4.0	As can be seen from Table 9, there is no significant difference between any of the three metrics in terms of correlation with human assessment in both the French and Spanish tracks.	0
3389	31381	31381	W18-3601	Mechanical	38	174	4.0	4.0	In the English track, however, the correlation of BLEU and NIST scores with human assessment are significantly higher than that of DIST.	0
3390	31382	31382	W18-3601	Google Data Compute results	1	175	1.0	4.0	Table 10 shows the results for the English assessment conducted via the Google Data Compute (GDC) evaluation service with expert evaluators.	0
3391	31383	31383	W18-3601	Google Data Compute results	2	176	1.0	4.0	One difference between the MTurk and the Google results is the range of scores, which for     Meaning Similarity range from 67 to 86.9 for MTurk, compared to 52 to 86.1 for GDC.	0
3392	31384	31384	W18-3601	Google Data Compute results	3	177	2.0	4.0	The latter is a wider range of scores, and expert evaluators' scores distinguish between systems more clearly than the crowdsourced scores which place the top four systems very close together.	0
3393	31385	31385	W18-3601	Google Data Compute results	4	178	2.0	4.0	Readability scores range from 41.3 to 78.7 for MTurk, and from 60.2 to 88.2 for GDC.	0
3394	31386	31386	W18-3601	Google Data Compute results	5	179	3.0	4.0	The expert evaluators tended to assign higher scores overall, but their range and the way they distinguish between systems is similar.	0
3395	31387	31387	W18-3601	Google Data Compute results	6	180	3.0	4.0	For example, neither evaluation found much difference for the bottom two systems.	0
3396	31388	31388	W18-3601	Google Data Compute results	7	181	4.0	4.0	The rank order of systems in the two separate evaluations is identical.	0
3397	31389	31389	W18-3601	Google Data Compute results	8	182	4.0	4.0	Table 11 shows the Pearson correlation of scores for systems in the evaluations, where meaning similarity scores correlate almost perfectly at 0.997 (raw %) and 0.993 (z) and readability at 0.986 (raw %) and 0.985 (z).	0
3398	31390	31390	W18-3601	Conclusion	1	183	1.0	4.0	SR'18 was the second surface realisation shared task, and followed an earlier pilot task for English, SR'11.	0
3399	31391	31391	W18-3601	Conclusion	2	184	2.0	4.0	Participation was high for a first instance   of a shared task, at least in the Shallow Track, indicating that interest is high enough to continue running it again next year to enable more teams to participate.	0
3400	31392	31392	W18-3601	Conclusion	3	185	2.0	4.0	One important question that needs to be addressed is to what extent UDs are suitable inputs for NLG systems.	0
3401	31393	31393	W18-3601	Conclusion	4	186	3.0	4.0	More specifically, can they reasonably be expected to be generated by other, content-determining, modules in an NLG system, do they provide all the information necessary to generate surface realisations, and if not, how can they be augmented to provide it.	0
3402	31394	31394	W18-3601	Conclusion	5	187	3.0	4.0	We hope to discuss these and related issues with the research community as we prepare the next instance of the SR Task.	0
3403	31395	31395	W18-3601	Conclusion	6	188	4.0	4.0	A goal to aim for may be to make it possible for different NLG components to be connected via standard interface representations, to increase re-usability for NLG components.	0
3404	31396	31396	W18-3601	Conclusion	7	189	4.0	4.0	However, what may constitute a good interface representation for surface realisation remains far from clear.	0
3405	31515	31515	W19-3203	title	1	1	2.0	1.0	Overview of the Fourth Social Media Mining for Health (#SMM4H)	0
3406	31516	31516	W19-3203	title	2	2	4.0	1.0	Shared Task at ACL 2019	0
3407	31517	31517	W19-3203	abstract	1	3	1.0	1.0	The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking on a regular basis 1 . Advances in automated data processing and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media.	0
3408	31518	31518	W19-3203	abstract	2	4	1.0	1.0	We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users.	0
3409	31519	31519	W19-3203	abstract	3	5	2.0	1.0	For the fourth execution of this challenge, we proposed four different tasks.	0
3410	31520	31520	W19-3203	abstract	4	6	2.0	1.0	Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not.	1
3411	31521	31521	W19-3203	abstract	5	7	3.0	1.0	Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs.	1
3412	31522	31522	W19-3203	abstract	6	8	3.0	1.0	Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary.	1
3413	31523	31523	W19-3203	abstract	7	9	4.0	1.0	Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one's health, a more general discussion of the health issue, or is an unrelated mention.	1
3414	31524	31524	W19-3203	abstract	8	10	4.0	1.0	A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run.	0
3415	31525	31525	W19-3203	abstract	9	11	4.0	1.0	We summarize here the corpora for this challenge which are freely available at https://competitions.codalab. org/competitions/22521, and present an overview of the methods and the results of the competing systems.	0
3416	31526	31526	W19-3203	Introduction	1	12	1.0	1.0	The intent of the #SMM4H shared tasks series is to challenge the community with Natural Language Processing tasks for mining relevant data for health monitoring and surveillance in social media.	0
3417	31527	31527	W19-3203	Introduction	2	13	1.0	1.0	Such challenges require processing imbalanced, noisy, real-world, and substantially creative language expressions from social media.	0
3418	31528	31528	W19-3203	Introduction	3	14	1.0	1.0	The competing systems should be able to deal with many linguistic variations and semantic complexities in the various ways people express medication-related concepts and outcomes.	0
3419	31529	31529	W19-3203	Introduction	4	15	1.0	1.0	It has been shown in past research (Liu et al., 2011;	0
3420	31530	31530	W19-3203	Introduction	5	16	1.0	1.0	Giuseppe et al., 2017) that automated systems frequently under-perform when exposed to social media text because of the presence of novel/creative phrases, misspellings and frequent use of idiomatic, ambiguous and sarcastic expressions.	0
3421	31531	31531	W19-3203	Introduction	6	17	1.0	1.0	The tasks act as a discovery and verification process of what approaches work best for social media data.	0
3422	31532	31532	W19-3203	Introduction	7	18	2.0	1.0	As in previous years, our tasks focused on mining health information from Twitter.	0
3423	31533	31533	W19-3203	Introduction	8	19	2.0	1.0	This year we challenged the community with two different problems.	0
3424	31534	31534	W19-3203	Introduction	9	20	2.0	1.0	The first problem focuses on performing pharmacovigilance from social media data.	0
3425	31535	31535	W19-3203	Introduction	10	21	2.0	1.0	It is now well understood that social media data may contain reports of adverse drug reactions (ADRs) and these reports may complement traditional adverse event reporting systems, such as the FDA adverse event reporting system (FAERS).	0
3426	31536	31536	W19-3203	Introduction	11	22	2.0	1.0	However, automatically curating reports from adverse reactions from Twitter requires the application of a series of NLP methods in an end-to-end pipeline .	0
3427	31537	31537	W19-3203	Introduction	12	23	2.0	1.0	The first three tasks of this year's challenge represent three key NLP problems in a social media based pharmacovigilance pipeline -(i) automatic classification of ADRs, (ii) extraction of spans of ADRs and (iii) normal-ization of the extracted ADRs to standardized IDs.	0
3428	31538	31538	W19-3203	Introduction	13	24	2.0	1.0	The second problem explores the generalizability of predictive models.	0
3429	31539	31539	W19-3203	Introduction	14	25	3.0	1.0	In health research using social media, it is often necessary for researchers to build individual classifiers to identify health mentions of a particular disease in a particular context.	0
3430	31540	31540	W19-3203	Introduction	15	26	3.0	1.0	Classification models that can generalize to different health contexts would be greatly beneficial to researchers in these fields (e.g., (Payam and Eugene, 2018)), as this would allow researchers to more easily apply existing tools and resources to new problems.	0
3431	31541	31541	W19-3203	Introduction	16	27	3.0	1.0	Motivated by these ideas, Task 4 was testing tweet classification methods across diverse health contexts, so the test data included a very different health context than the training data.	0
3432	31542	31542	W19-3203	Introduction	17	28	3.0	1.0	This setting measures the ability of tweet classifiers to generalize across health contexts.	0
3433	31543	31543	W19-3203	Introduction	18	29	3.0	1.0	The fourth iteration of our series follows the same organization as previous iterations.	0
3434	31544	31544	W19-3203	Introduction	19	30	3.0	1.0	We collected posts from Twitter, annotated the data for the four tasks proposed and released the posts to the registered teams.	0
3435	31545	31545	W19-3203	Introduction	20	31	3.0	1.0	This year, we conducted the evaluation of all participating systems using Codalab, an open source platform facilitating data science competitions.	0
3436	31546	31546	W19-3203	Introduction	21	32	4.0	1.0	The performances of the systems were compared on a blind evaluations sets for each task.	0
3437	31547	31547	W19-3203	Introduction	22	33	4.0	2.0	All teams registered were allowed to participate to one or multiple tasks.	0
3438	31548	31548	W19-3203	Introduction	23	34	4.0	2.0	We provided the participants with two sets of data for each task, a training and a test set.	0
3439	31549	31549	W19-3203	Introduction	24	35	4.0	2.0	Participants had a period of six weeks, from March 5 th to April 15 th , for training their systems on our training sets, and 4 days, from the 16 th to 20 th of April, for calibrating their systems on our test sets and submitting their predictions.	0
3440	31550	31550	W19-3203	Introduction	25	36	4.0	2.0	In total 34 teams registered and 19 teams submitted at least one run (each team was allowed to submit, at most, three runs per task).	0
3441	31551	31551	W19-3203	Introduction	26	37	4.0	2.0	In detail, we received 43 runs for task 1, 24 for task 2, 10 for task 3 and 15 for task 4.	0
3442	31552	31552	W19-3203	Introduction	27	38	4.0	2.0	We briefly describe each task and their data in section 2, before discussing the results obtained in section 3.	0
3443	31553	31553	W19-3203	Task Descriptions	1	39	1.0	2.0	Tasks	0
3444	31554	31554	W19-3203	Task Descriptions	2	40	1.0	2.0	Task 1: Automatic classification of tweets mentioning an ADR.	0
3445	31555	31555	W19-3203	Task Descriptions	3	41	1.0	2.0	This is a binary classification task for which systems are required to predict if a tweet mentions an ADR or not.	0
3446	31556	31556	W19-3203	Task Descriptions	4	42	1.0	2.0	In an end-to-end social media based pharmacovigilance pipeline, such a system is needed after data collection to filter out the large volume of medication-related chatter that is not a mention of an ADR.	0
3447	31557	31557	W19-3203	Task Descriptions	5	43	2.0	2.0	This task is a rerun of the popular classification task organized in past years.	0
3448	31558	31558	W19-3203	Task Descriptions	6	44	2.0	2.0	Task 2: Automatic extraction of ADR mentions from tweets.	0
3449	31559	31559	W19-3203	Task Descriptions	7	45	2.0	2.0	This is a named entity recognition (NER) task that typically follows the ADR classification step (Task 1) in an ADR extraction pipeline.	0
3450	31560	31560	W19-3203	Task Descriptions	8	46	2.0	2.0	Given a set of tweets containing drug mentions and potentially containing ADRs, the objective was to determine the span of the ADR mention, if any.	0
3451	31561	31561	W19-3203	Task Descriptions	9	47	2.0	2.0	ADRs are rare events making ADR classification a challenging task with an F1score in the vicinity of 0.5 (based on previous shared task results (Weissenbacher et al., 2018)) for the ADR class.	0
3452	31562	31562	W19-3203	Task Descriptions	10	48	3.0	2.0	The dataset for the ADR extraction task contains tweets that are both positive and negative for the presence of ADRs.	0
3453	31563	31563	W19-3203	Task Descriptions	11	49	3.0	2.0	This allowed participants to choose to train their systems on either the set of tweets containing ADRs or include tweets that were negative for the presence of ADRs.	0
3454	31564	31564	W19-3203	Task Descriptions	12	50	3.0	2.0	Task 3: Automatic extraction of ADR mentions and normalization of extracted ADRs to Med-DRA preferred term identifiers.	0
3455	31565	31565	W19-3203	Task Descriptions	13	51	3.0	2.0	This is an extension of Task 2 consisting of the combination of NER and entity normalization tasks: a named entity resolution task.	0
3456	31566	31566	W19-3203	Task Descriptions	14	52	4.0	2.0	In this task, given the same set of tweets as in Task 2, the objective was to extract the span of an ADR mention and to normalize it to MedDRA identifiers 2 . MedDRA (Medical Dictionary for Regulatory Activities), which is the standard nomenclature for monitoring medical products, and includes diseases, disorders, signs, symptoms, adverse events or adverse drug reactions.	0
3457	31567	31567	W19-3203	Task Descriptions	15	53	4.0	2.0	For the normalization task, MedDRA version 21.1 was used, containing 79,507 lower level terms (LLTs) and 23,389 respective preferred terms (PTs).	0
3458	31568	31568	W19-3203	Task Descriptions	16	54	4.0	2.0	Task 4: Automatic classification of personal mentions of health.	0
3459	31569	31569	W19-3203	Task Descriptions	17	55	4.0	2.0	In this binary classification task, the systems were required to distinguish tweets of personal health status or opinions across different health domains.	0
3460	31570	31570	W19-3203	Task Descriptions	18	56	4.0	2.0	The proposed task was intended to provide a baseline understanding of the ability to identify personal health mentions in a generalized context.	0
3461	31571	31571	W19-3203	Data	1	57	1.0	2.0	All corpora were composed of public tweets downloaded using the official streaming API provided by Twitter and made available to the participants in accordance with Twitter's data use policy.	0
3462	31572	31572	W19-3203	Data	2	58	1.0	2.0	This study received an exempt determination by the Institutional Review Board of the University of Pennsylvania.	0
3463	31573	31573	W19-3203	Data	3	59	1.0	2.0	Task 1.	0
3464	31574	31574	W19-3203	Data	4	60	1.0	2.0	For training, participants were provided with all the tweets from the #SMM4H 2017 shared tasks , which are publicly available at: https://data.mendeley. com/datasets/rxwfb3tysd/2. A total of 25,678 tweets were made available for training.	0
3465	31575	31575	W19-3203	Data	5	61	1.0	2.0	The test set consisted of 4575 tweets with 626 (13.7%) tweets representing ADRs.	0
3466	31576	31576	W19-3203	Data	6	62	1.0	2.0	The evaluation metric for this task was micro-averaged F1score for the ADR class.	0
3467	31577	31577	W19-3203	Data	7	63	1.0	2.0	Task 2. Participants of Task 2 were provided with a training set containing 2276 tweets which mentioned at least one drug name.	0
3468	31578	31578	W19-3203	Data	8	64	2.0	2.0	The dataset contained 1300 tweets that were positive for the presence of ADRs and 976 tweets that were negative.	0
3469	31579	31579	W19-3203	Data	9	65	2.0	2.0	Participants were allowed to include additional negative instances from Task 1 for training purposes.	0
3470	31580	31580	W19-3203	Data	10	66	2.0	3.0	Positive tweets were annotated with the start and end indices of the ADRs and the corresponding span text in the tweets.	0
3471	31581	31581	W19-3203	Data	11	67	2.0	3.0	The evaluation set contained 1573 tweets, 785 and 788 tweets were positive and negative for the presence of ADRs respectively.	0
3472	31582	31582	W19-3203	Data	12	68	2.0	3.0	The participants were asked to submit outputs from their systems that contained the predicted start and end indices of ADRs.	0
3473	31583	31583	W19-3203	Data	13	69	2.0	3.0	The participants' submissions were evaluated using standard strict and overlapping F1-scores for extracted ADRs.	0
3474	31584	31584	W19-3203	Data	14	70	2.0	3.0	Under strict mode of evaluation, ADR spans were considered correct only if both start and end indices matched with the indices in our gold standard annotations.	0
3475	31585	31585	W19-3203	Data	15	71	3.0	3.0	Under overlapping mode of evaluation, ADR spans were considered correct only if spans in predicted annotations overlapped with our gold standard annotations.	0
3476	31586	31586	W19-3203	Data	16	72	3.0	3.0	Task 3.	0
3477	31587	31587	W19-3203	Data	17	73	3.0	3.0	Participants were provided with the same training and evaluation datasets as in Task 2.	0
3478	31588	31588	W19-3203	Data	18	74	3.0	3.0	However, the datasets contained additional columns for the MedDRA annotated LLT and PT identifiers for each ADR mention.	0
3479	31589	31589	W19-3203	Data	19	75	3.0	3.0	In total, of the 79,507 LLT and 23,389 PT identifiers available in MedDRA, the training set of 2276 tweets and 1832 annotated ADRs contained 490 unique LLT iden-tifiers and 327 unique PT identifiers.	0
3480	31590	31590	W19-3203	Data	20	76	3.0	3.0	The evaluation set contained 112 PT identifiers that were not present as part of the training set.	0
3481	31591	31591	W19-3203	Data	21	77	3.0	3.0	The participants were asked to submit outputs containing the predicted start and end indices of ADRs and respective PT identifiers.	0
3482	31592	31592	W19-3203	Data	22	78	4.0	3.0	Although the training dataset contained annotations at the LLT level, the performance was only evaluated at the higher PT level.	0
3483	31593	31593	W19-3203	Data	23	79	4.0	3.0	The participants' submissions were evaluated using standard strict and overlapping F-scores for extracted ADRs and respective MedDRA identifiers.	0
3484	31594	31594	W19-3203	Data	24	80	4.0	3.0	Under strict mode of evaluation, ADR spans were considered correct only if both start and end indices matched along with matching MedDRA PT identifiers.	0
3485	31595	31595	W19-3203	Data	25	81	4.0	3.0	Under overlapping mode of evaluation, ADR spans were considered correct only if spans in predicted ADRs overlapped with gold standard ADR spans in addition to matching MedDRA PT identifiers.	0
3486	31596	31596	W19-3203	Data	26	82	4.0	3.0	Task 4 Data.	0
3487	31597	31597	W19-3203	Data	27	83	4.0	3.0	Participants were provided training data from one disease domain, influenza, across two contexts, being sick and getting vaccinated, both annotated for personal mentions: the user is personally sick or the user has been personally vaccinated.	0
3488	31598	31598	W19-3203	Data	28	84	4.0	3.0	Test data included new tweets of personal health mentions about influenza and tweets from an additional disease domain, Zika virus, with two different contexts, the user is changing their travel plans in response to Zika concerns, or the user is minimizing potential mosquito exposure due to Zika concerns.	0
3489	31599	31599	W19-3203	Annotation and Inter-Annotator Agreements	1	85	1.0	3.0	Two annotators with biomedical education and both experienced in Social Media research tasks manually annotated the corpora for tasks 1, 2 and 3.	0
3490	31600	31600	W19-3203	Annotation and Inter-Annotator Agreements	2	86	1.0	3.0	Our annotators independently dual-annotated each test sets to insure the quality of our annotations.	0
3491	31601	31601	W19-3203	Annotation and Inter-Annotator Agreements	3	87	1.0	3.0	Disagreement were resolved after an adjudication phase between our two annotators.	0
3492	31602	31602	W19-3203	Annotation and Inter-Annotator Agreements	4	88	1.0	3.0	On task 1, the classification task, the inter annotatoragreement (IAA) was high with a Cohens Kappa = 0.82.	0
3493	31603	31603	W19-3203	Annotation and Inter-Annotator Agreements	5	89	2.0	3.0	On task 2, the information extraction task, IAAs were good with and an F1-score of 0.73 for strict agreement, and 0.85 for overlapping agreement 3 .	0
3494	31604	31604	W19-3203	Annotation and Inter-Annotator Agreements	6	90	2.0	3.0	On task 3, our annotators double annotated 535 of the extracted ADR terms and normalized them to MedDRA lower lever terms (LLT).	0
3495	31605	31605	W19-3203	Annotation and Inter-Annotator Agreements	7	91	2.0	3.0	They achieved an agreement accuracy of 82.6%.	0
3496	31606	31606	W19-3203	Annotation and Inter-Annotator Agreements	8	92	2.0	3.0	After converting the LLT to their corresponding preferred term (PT) in MedDRA, which is the coding the task was scored against, accuracy improved to 87.7% 4 . The annotation process followed for task 4 was slightly different due to the nature of the task.	0
3497	31607	31607	W19-3203	Annotation and Inter-Annotator Agreements	9	93	2.0	3.0	We obtained the two datasets of our training set, focusing on flu vaccination and flu infection, from (Huang et al., 2017) and (Lamb et al., 2013) respectively.	0
3498	31608	31608	W19-3203	Annotation and Inter-Annotator Agreements	10	94	3.0	3.0	Huang et al.	0
3499	31609	31609	W19-3203	Annotation and Inter-Annotator Agreements	11	95	3.0	3.0	(Huang et al., 2017) used mechanical turk to crowdsource labels (Fleiss' kappa = 0.793).	0
3500	31610	31610	W19-3203	Annotation and Inter-Annotator Agreements	12	96	3.0	3.0	Lamb et al.	0
3501	31611	31611	W19-3203	Annotation and Inter-Annotator Agreements	13	97	3.0	3.0	(Lamb et al., 2013) did not report their labeling procedure or annotator agreement metrics, but do report annotation guidelines 5 . A few of the tweets released by Lamb et al. appeared to be mislabeled and were corrected in accordance with the annotation guidelines defined by the authors.	0
3502	31612	31612	W19-3203	Annotation and Inter-Annotator Agreements	14	98	3.0	4.0	We obtained the test data for task 4 by compiling three datasets.	0
3503	31613	31613	W19-3203	Annotation and Inter-Annotator Agreements	15	99	4.0	4.0	For the dataset related to travel changes due to Zika concerns, we selected a subset of data already available from (Daughton and Paul, 2019).	0
3504	31614	31614	W19-3203	Annotation and Inter-Annotator Agreements	16	100	4.0	4.0	Initial labeling of these tweets was performed by two annotators with a public health background (Cohen's kappa = 0.66).	0
3505	31615	31615	W19-3203	Annotation and Inter-Annotator Agreements	17	101	4.0	4.0	We reuse the original annotations for this dataset without changes.	0
3506	31616	31616	W19-3203	Annotation and Inter-Annotator Agreements	18	102	4.0	4.0	For the mosquito exposure dataset, tweets were labeled by one annotator with public health knowledge and experienced with social media, and then verified by a second annotator with similar experience.	0
3507	31617	31617	W19-3203	Annotation and Inter-Annotator Agreements	19	103	4.0	4.0	The additional set of data on personal exposure to Influenza were obtained from a separate group, who used an independent labeling procedure.	0
3508	31618	31618	W19-3203	Results	1	104	1.0	4.0	The challenge received a solid response with 19 teams from 12 countries (7 from North America, 1 from South America, 6 from Asia and 5 from Europe) submitting 92 runs in total in one or more tasks.	0
3509	31619	31619	W19-3203	Results	2	105	1.0	4.0	We present an overview of all architectures competing in the different tasks in Table 1, 2, 3, 4.	0
3510	31620	31620	W19-3203	Results	3	106	1.0	4.0	We also list in these tables the external resources competitors integrated for improving the pre-training of their systems or for embedding high-level features to help decision-making.	0
3511	31621	31621	W19-3203	Results	4	107	1.0	4.0	The overview of all architectures is interesting in two ways.	0
3512	31622	31622	W19-3203	Results	5	108	1.0	4.0	First, this challenge confirms the tendency of the community to abandon traditional Machine Learning systems based on handcrafted features for deep learning architectures capable of discovering the features relevant for the task at hand from pre-trained embeddings.	0
3513	31623	31623	W19-3203	Results	6	109	2.0	4.0	During the challenge, when participants implemented traditional systems, such as SVM or CRF, they used such systems as baselines and, observing significant differences of performances with systems based on deep learning on their validation sets, most of them did not submit their predictions as official runs.	0
3514	31624	31624	W19-3203	Results	7	110	2.0	4.0	"Second, while last year convolutional or recurrent neural networks ""fed"" with pretrained word embeddings learned on local windows of words (e.g. word2vec, GloVe) were the most popular architectures, this year we can see a clear dominance of neural architectures using word embeddings pre-trained with the Bidirectional Encoder Representations from Transformers (BERT) proposed by (Devlin et al., 2018), or fine-tuning these words embeddings on our training corpora."	0
3515	31625	31625	W19-3203	Results	8	111	2.0	4.0	BERT allows to compute words embeddings based on the full context of sentences and not only on local windows.	0
3516	31626	31626	W19-3203	Results	9	112	2.0	4.0	A notable result from task 1-3 is that, despite an improvement in performances for the detection of ADRs, their resolution remains challenging and will require further research.	0
3517	31627	31627	W19-3203	Results	10	113	2.0	4.0	The participants largely adopted contextual word-embeddings during this challenge, a choice rewarded by new records in performances during the task 1, the only task reran from last years.	0
3518	31628	31628	W19-3203	Results	11	114	3.0	4.0	The performances increased from .522 F1-score (.442 P, .636 R) (Weissenbacher et al., 2018) to .646 F1-score (0.608 P, 0.689 R) for the best systems of each years.	0
3519	31629	31629	W19-3203	Results	12	115	3.0	4.0	However, with a strict matching F1-score of .432 (.362 P, .535 R) for the best system, the performances obtained in task 3 for ADRs resolution are still low and human inspection is still required to make use of the data extracted automatically.	0
3520	31630	31630	W19-3203	Results	13	116	3.0	4.0	As shown by the best score of .887 Accuracy obtained on the ADR normalization in task 3 ran during #SMM4H in 2017 (Sarker et al., 2018) 6 , once ADRs are extracted, the normalization of the ADRs can be per-formed with a good reliability.	0
3521	31631	31631	W19-3203	Results	14	117	3.0	4.0	However errors are made during all steps of the resolution -detection, extraction, normalization -and their overall accumulation render current automatic systems inefficient.	0
3522	31632	31632	W19-3203	Results	15	118	3.0	4.0	Note that bulk of the errors are made during the extraction of the ADRs, as shown by the low strict F1-score of the best system in task 2, .464 F1-score (.389P, .576 R).	0
3523	31633	31633	W19-3203	Results	16	119	4.0	4.0	For task 4, we were especially interested in the generalizability of first person health classifiers to a domain separate from that of the training data.	0
3524	31634	31634	W19-3203	Results	17	120	4.0	4.0	We find that, on average, teams do reasonably well across the full test dataset (average F1-score: 0.70, range: 0.41-0.87).	0
3525	31635	31635	W19-3203	Results	18	121	4.0	4.0	Unsurprisingly, classifiers tended to do better on a test set in the same domain as the training dataset (context 1, average F1-score: 0.82) and more modestly on the Zika travel and mosquito datasets (average F1-score: 0.40 and 0.52, respectively).	0
3526	31636	31636	W19-3203	Results	19	122	4.0	4.0	Interestingly, in all contexts, precision was higher than recall.	0
3527	31637	31637	W19-3203	Results	20	123	4.0	4.0	We note that both the training and the testing data were limited in quantity, and that classifiers would likely improve with more data.	0
3528	31638	31638	W19-3203	Results	21	124	4.0	4.0	However, in general, it is encouraging that classifiers trained in one health domain can be applied to separate health domains.	0
3529	31639	31639	W19-3203	Conclusion	1	125	1.0	4.0	In this paper we presented an overview of the results of #SMM4H 2019 which focuses on a) the resolution of adverse drug reaction (ADR) mentioned in Twitter and b) the distinction between tweets reporting personal health status form opinions across different health domains.	0
3530	31640	31640	W19-3203	Conclusion	2	126	2.0	4.0	With a total of 92 runs submitted by 19 teams, the challenge was well attended.	0
3531	31641	31641	W19-3203	Conclusion	3	127	2.0	4.0	The participants, in large part, opted for neural architectures and integrated pretrained word-embedding sensitive to their contexts based on the recent Bidirectional Encoder Representations from Transformers.	0
3532	31642	31642	W19-3203	Conclusion	4	128	3.0	4.0	Such architectures were the most efficient on our four tasks.	0
3533	31643	31643	W19-3203	Conclusion	5	129	4.0	4.0	Results on tasks 1-3 show that, despite a continuous improvement of performances in the detection of tweets mentioning ADRs over the past years, their end-to-end resolution still remain a major challenge for the community and an opportunity for further research.	0
3534	31644	31644	W19-3203	Conclusion	6	130	4.0	4.0	Results of task 4 were more encouraging, with systems able to generalized their predictions over domains not present in their training data.	0
3535	32732	32732	2020.sigmorphon-1.1	title	1	1	4.0	1.0	SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection	0
3536	32733	32733	2020.sigmorphon-1.1	abstract	1	2	1.0	1.0	A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language.	0
3537	32734	32734	2020.sigmorphon-1.1	abstract	2	3	1.0	1.0	Most systems, however, are developed using data from just one language such as English.	0
3538	32735	32735	2020.sigmorphon-1.1	abstract	3	4	2.0	1.0	The SIG-MORPHON 2020 shared task on morphological reinflection aims to investigate systems' ability to generalize across typologically distinct languages, many of which are low resource.	0
3539	32736	32736	2020.sigmorphon-1.1	abstract	4	5	2.0	1.0	Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages.	0
3540	32737	32737	2020.sigmorphon-1.1	abstract	5	6	3.0	1.0	A total of 22 systems (19 neural) from 10 teams were submitted to the task.	0
3541	32738	32738	2020.sigmorphon-1.1	abstract	6	7	3.0	1.0	All four winning systems were neural (two monolingual transformers and two massively multilingual RNNbased models with gated attention).	0
3542	32739	32739	2020.sigmorphon-1.1	abstract	7	8	4.0	1.0	Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages.	0
3543	32740	32740	2020.sigmorphon-1.1	abstract	8	9	4.0	1.0	Nonneural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data.	0
3544	32741	32741	2020.sigmorphon-1.1	abstract	9	10	4.0	1.0	Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.	0
3545	32742	32742	2020.sigmorphon-1.1	Introduction	1	11	1.0	1.0	Human language is marked by considerable diversity around the world.	0
3546	32743	32743	2020.sigmorphon-1.1	Introduction	2	12	1.0	1.0	Though the world's languages share many basic attributes (e.g., Swadesh, 1950 and more recently, List et al., 2016), grammatical features, and even abstract implications (proposed in Greenberg, 1963), each language nevertheless has a unique evolutionary trajectory that is affected by geographic, social, cultural, and other factors.	0
3547	32744	32744	2020.sigmorphon-1.1	Introduction	3	13	1.0	1.0	As a result, the surface form of languages varies substantially.	0
3548	32745	32745	2020.sigmorphon-1.1	Introduction	4	14	1.0	1.0	The morphology of languages can differ in many ways:	0
3549	32746	32746	2020.sigmorphon-1.1	Introduction	5	15	1.0	1.0	"Some exhibit rich grammatical case systems (e.g., 12 in Erzya and 24 in Veps) and mark possessiveness, others might have complex verbal morphology (e.g., Oto-Manguean languages; Palancar and Léonard, 2016) or even ""decline"" nouns for tense (e.g., Tupi-Guarani languages)."	0
3550	32747	32747	2020.sigmorphon-1.1	Introduction	6	16	1.0	1.0	Linguistic typology is the discipline that studies these variations by means of a systematic comparison of languages (Croft, 2002;	0
3551	32748	32748	2020.sigmorphon-1.1	Introduction	7	17	1.0	1.0	Comrie, 1989).	0
3552	32749	32749	2020.sigmorphon-1.1	Introduction	8	18	1.0	1.0	Typologists have defined several dimensions of morphological variation to classify and quantify the degree of crosslinguistic variation.	0
3553	32750	32750	2020.sigmorphon-1.1	Introduction	9	19	2.0	1.0	This comparison can be challenging as the categories are based on studies of known languages and are progressively refined with documentation of new languages (Haspelmath, 2007).	0
3554	32751	32751	2020.sigmorphon-1.1	Introduction	10	20	2.0	1.0	Nevertheless, to understand the potential range of morphological variation, we take a closer look at three dimensions here: fusion, inflectional synthesis, and position of case affixes (Dryer and Haspelmath, 2013).	0
3555	32752	32752	2020.sigmorphon-1.1	Introduction	11	21	2.0	1.0	Fusion, our first dimension of variation, refers to the degree to which morphemes bind to one another in a phonological word (Bickel and Nichols, 2013b).	0
3556	32753	32753	2020.sigmorphon-1.1	Introduction	12	22	2.0	1.0	Languages range from strictly isolating (i.e., each morpheme is its own phonological word) to concatenative (i.e., morphemes bind together within a phonological word); nonlinearities such as ablaut or tonal morphology can also be present.	0
3557	32754	32754	2020.sigmorphon-1.1	Introduction	13	23	2.0	1.0	From a geographic perspective, isolating languages are found in the Sahel Belt in West Africa, Southeast Asia and the Pacific.	0
3558	32755	32755	2020.sigmorphon-1.1	Introduction	14	24	2.0	1.0	Ablaut-concatenative morphology and tonal morphology can be found in African languages.	0
3559	32756	32756	2020.sigmorphon-1.1	Introduction	15	25	2.0	1.0	Tonal-concatenative morphology can be found in Mesoamerican languages (e.g., Oto-Manguean).	0
3560	32757	32757	2020.sigmorphon-1.1	Introduction	16	26	2.0	1.0	Concatenative morphology is the most common system and can be found around the world.	0
3561	32758	32758	2020.sigmorphon-1.1	Introduction	17	27	3.0	1.0	Inflectional synthesis, the second dimension considered, refers to whether grammatical categories like tense, voice or agreement are expressed as affixes (synthetic) or individual words (analytic) (Bickel and Nichols, 2013c).	0
3562	32759	32759	2020.sigmorphon-1.1	Introduction	18	28	3.0	1.0	Analytic expressions are common in Eurasia (except the Pacific Rim, and the Himalaya and Caucasus mountain ranges), whereas synthetic expressions are used to a high degree in the Americas.	0
3563	32760	32760	2020.sigmorphon-1.1	Introduction	19	29	3.0	1.0	Finally, affixes can variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013).	0
3564	32761	32761	2020.sigmorphon-1.1	Introduction	20	30	3.0	1.0	Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013).	0
3565	32762	32762	2020.sigmorphon-1.1	Introduction	21	31	3.0	1.0	In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead.	0
3566	32763	32763	2020.sigmorphon-1.1	Introduction	22	32	3.0	1.0	These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable.	0
3567	32764	32764	2020.sigmorphon-1.1	Introduction	23	33	3.0	1.0	Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging.	0
3568	32765	32765	2020.sigmorphon-1.1	Introduction	24	34	3.0	1.0	As Bender (2009	0
3569	32766	32766	2020.sigmorphon-1.1	Introduction	25	35	4.0	1.0	Bender ( , 2016 notes, many current architectures and training and tuning algorithms still present language-specific biases.	0
3570	32767	32767	2020.sigmorphon-1.1	Introduction	26	36	4.0	1.0	The most commonly used language for developing NLP applications is English.	0
3571	32768	32768	2020.sigmorphon-1.1	Introduction	27	37	4.0	1.0	Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology.	0
3572	32769	32769	2020.sigmorphon-1.1	Introduction	28	38	4.0	1.0	With respect to languages that exhibit inflectional morphology, English is relatively impoverished.	0
3573	32770	32770	2020.sigmorphon-1.1	Introduction	29	39	4.0	1.0	1 Importantly, English is just one morphological system among many.	0
3574	32771	32771	2020.sigmorphon-1.1	Introduction	30	40	4.0	1.0	A larger goal of natural language processing is that the system work for any presented language.	0
3575	32772	32772	2020.sigmorphon-1.1	Introduction	31	41	4.0	1.0	If an NLP system is trained on just one language, it could be missing important flexibility in its ability to account for cross-linguistic morphological variation.	0
3576	32773	32773	2020.sigmorphon-1.1	Introduction	32	42	4.0	1.0	In this year's iteration of the SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems' ability to generalize across typologically distinct languages many of which are low-resource.	0
3577	32774	32774	2020.sigmorphon-1.1	Introduction	33	43	4.0	1.0	"For example, if a neural network architecture works well for a sample of Indo-European languages, should the same architecture also work well for Tupi-Guarani languages (where nouns are ""declined"" for tense) or Austronesian languages (where verbal morphology is frequently prefixing)?"	0
3578	32775	32775	2020.sigmorphon-1.1	Task Description	1	44	1.0	1.0	The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form.	1
3579	32776	32776	2020.sigmorphon-1.1	Task Description	2	45	1.0	1.0	For each language we provide a separate training, development, and test set.	0
3580	32777	32777	2020.sigmorphon-1.1	Task Description	3	46	1.0	1.0	"More historically, all of these tasks resemble the classic ""wug""-test that Berko (1958) developed to test child and human knowledge of English nominal morphology."	0
3581	32778	32778	2020.sigmorphon-1.1	Task Description	4	47	2.0	1.0	Unlike the task from earlier years, this year's task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces previously unseen data.	0
3582	32779	32779	2020.sigmorphon-1.1	Task Description	5	48	2.0	1.0	The task starts with the Development Phase, which was an elongated period of time (about two months), during which participants develop a model of morphological inflection.	0
3583	32780	32780	2020.sigmorphon-1.1	Task Description	6	49	2.0	1.0	In this phase, we provide training and development splits for 45 languages representing the Austronesian, Niger-Congo, Oto-Manguean, Uralic and Indo-European language families.	0
3584	32781	32781	2020.sigmorphon-1.1	Task Description	7	50	2.0	1.0	Table 1 provides details on the languages.	0
3585	32782	32782	2020.sigmorphon-1.1	Task Description	8	51	3.0	1.0	The Generalization Phase is a short period of time (it started about a week before the Evaluation Phase) during which participants fine-tune their models on new data.	0
3586	32783	32783	2020.sigmorphon-1.1	Task Description	9	52	3.0	1.0	At the start of the phase, we provide training and development splits for 45 new languages where approximately half are genetically related (belong to the same family) and half are genetically unrelated (are isolates or belong to a different family) to the languages presented in the Development Phase.	0
3587	32784	32784	2020.sigmorphon-1.1	Task Description	10	53	3.0	1.0	More specifically, we introduce (surprise) languages from Afro-Asiatic, Algic, Dravidian, Indo-European, Niger-Congo, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan families.	0
3588	32785	32785	2020.sigmorphon-1.1	Task Description	11	54	3.0	1.0	See Table 2 for more details.	0
3589	32786	32786	2020.sigmorphon-1.1	Task Description	12	55	4.0	1.0	Finally, test splits for all 90 languages are released in the Evaluation Phase.	0
3590	32787	32787	2020.sigmorphon-1.1	Task Description	13	56	4.0	1.0	During this phase, the models are evaluated on held-out forms.	0
3591	32788	32788	2020.sigmorphon-1.1	Task Description	14	57	4.0	1.0	Importantly, the languages from both previous phases are evaluated simultaneously.	0
3592	32789	32789	2020.sigmorphon-1.1	Task Description	15	58	4.0	1.0	This way, we evaluate the extent to which models (especially those with shared parameters) overfit to the development data: a model based on the morphological patterning of the Indo-European languages may end up with a bias towards suffixing and will struggle to learn prefixing or infixation.	0
3593	32790	32790	2020.sigmorphon-1.1	Meet our Languages	1	59	1.0	1.0	In the 2020 shared task we cover 15 language families: Afro-Asiatic, Algic, Austronesian, Dravidian, Indo-European, Niger-Congo, Oto-Manguean, Sino-Tibetan, Siouan, Songhay, Southern Daly, Tungusic, Turkic, Uralic, and Uto-Aztecan.	0
3594	32791	32791	2020.sigmorphon-1.1	Meet our Languages	2	60	2.0	1.0	2 Five language families were used for the Development phase while ten were held out for the Generalization phase.	0
3595	32792	32792	2020.sigmorphon-1.1	Meet our Languages	3	61	3.0	1.0	Tab. 1 and Tab. 2 provide information on the languages, their families, and sources of data.	0
3596	32793	32793	2020.sigmorphon-1.1	Meet our Languages	4	62	4.0	1.0	In the following section, we provide an overview of each language family's morphological system.	0
3597	32794	32794	2020.sigmorphon-1.1	Afro-Asiatic	1	63	1.0	1.0	The Afro-Asiatic language family, consisting of six branches and over 300 languages, is among the largest language families in the world.	0
3598	32795	32795	2020.sigmorphon-1.1	Afro-Asiatic	2	64	1.0	1.0	It is mainly spoken in Northern, Western and Central Africa as well as West Asia and spans large modern languages such as Arabic, in addition to ancient languages like Biblical Hebrew.	0
3599	32796	32796	2020.sigmorphon-1.1	Afro-Asiatic	3	65	2.0	1.0	Similarly, some of its languages have a long tradition of written form, while others have yet to incorporate a writing system.	0
3600	32797	32797	2020.sigmorphon-1.1	Afro-Asiatic	4	66	2.0	1.0	The six branches differ most notably in typology and syntax, with the Chadic language being the main source of differences, which has sparked discussion of the division of the family (Frajzyngier, 2018).	0
3601	32798	32798	2020.sigmorphon-1.1	Afro-Asiatic	5	67	3.0	1.0	For example, in the Egyptian and Semitic branches, the root of a verb may not contain vowels, while this is allowed in Chadic.	0
3602	32799	32799	2020.sigmorphon-1.1	Afro-Asiatic	6	68	3.0	1.0	Although only four of the six branches, excluding Chadic and Omotic, use a prefix and suffix in conjugation when adding a subject to a verb, it is con-sidered an important characteristic of the family.	0
3603	32800	32800	2020.sigmorphon-1.1	Afro-Asiatic	7	69	4.0	1.0	In addition, some of the families in the phylum use tone to encode tense, modality and number among others.	0
3604	32801	32801	2020.sigmorphon-1.1	Afro-Asiatic	8	70	4.0	1.0	However, all branches use objective and passive suffixes.	0
3605	32802	32802	2020.sigmorphon-1.1	Afro-Asiatic	9	71	4.0	1.0	Markers of tense are generally simple, whereas aspect is typically distinguished with more elaborate systems.	0
3606	32803	32803	2020.sigmorphon-1.1	Algic	1	72	1.0	1.0	The Algic family embraces languages native to North America-more specifically the United States and Canada-and contain three branches.	0
3607	32804	32804	2020.sigmorphon-1.1	Algic	2	73	2.0	1.0	Of these, our sample contains Cree, the language from the largest genus, Algonquian, from which most languages are now extinct.	0
3608	32805	32805	2020.sigmorphon-1.1	Algic	3	74	3.0	1.0	The Algonquian genus is characterized by its concatenative morphology.	0
3609	32806	32806	2020.sigmorphon-1.1	Algic	4	75	4.0	1.0	Cree morphology is also concatenative and suffixing.	0
3610	32807	32807	2020.sigmorphon-1.1	Algic	5	76	4.0	1.0	It distinguishes between impersonal and non-impersonal verbs and presents four apparent declension classes among non-impersonal verbs.	0
3611	32808	32808	2020.sigmorphon-1.1	Austronesian	1	77	1.0	1.0	The Austronesian family of languages is largely comprised of languages from the Greater Central Philippine and Oceanic regions.	0
3612	32809	32809	2020.sigmorphon-1.1	Austronesian	2	78	2.0	1.0	They are characterized by limited morphology, mostly prefixing in nature.	0
3613	32810	32810	2020.sigmorphon-1.1	Austronesian	3	79	2.0	1.0	Additionally, tense-aspect affixes are predominantly seen as prefixes, though some suffixes are used.	0
3614	32811	32811	2020.sigmorphon-1.1	Austronesian	4	80	3.0	1.0	In the general case, verbs do not mark number, person, or gender.	0
3615	32812	32812	2020.sigmorphon-1.1	Austronesian	5	81	3.0	1.0	In Māori, verbs may be suffixed with a marker indicating the passive voice.	0
3616	32813	32813	2020.sigmorphon-1.1	Austronesian	6	82	4.0	1.0	This marker takes the form of one of twelve endings.	0
3617	32814	32814	2020.sigmorphon-1.1	Austronesian	7	83	4.0	1.0	These endings are difficult to predict as the language has undergone a loss of word-final consonants and there is no clear link between a stem and the passive suffix that it employs (Harlow, 2007).	0
3618	32815	32815	2020.sigmorphon-1.1	Dravidian	1	84	2.0	1.0	Indo-European	0
3619	32816	32816	2020.sigmorphon-1.1	Dravidian	2	85	3.0	1.0	Languages in the Indo-European family are native to most of Europe and a large part of Asia-with our sample including languages from the genera: Germanic, Indic, Iranian, and Romance.	0
3620	32817	32817	2020.sigmorphon-1.1	Dravidian	3	86	4.0	1.0	This is (arguably) the most well studied language family, containing a few of the highest-resource languages in the world.	0
3621	32818	32818	2020.sigmorphon-1.1	Romance	1	87	1.0	1.0	The Romance genus comprises of a set of fusional languages evolved from Latin.	0
3622	32819	32819	2020.sigmorphon-1.1	Romance	2	88	2.0	1.0	They traditionally originated in Southern and Southeastern Europe, though they are presently spoken in other continents such Africa and the Americas.	0
3623	32820	32820	2020.sigmorphon-1.1	Romance	3	89	3.0	1.0	Romance languages mark tense, person, number and mood in verbs, and gender and number in nouns.	0
3624	32821	32821	2020.sigmorphon-1.1	Romance	4	90	4.0	1.0	Inflection is primarily achieved through suffixes, with some verbal person syncretism and suppletion for high-frequency verbs.	0
3625	32822	32822	2020.sigmorphon-1.1	Romance	5	91	4.0	1.0	There is some morphological variation within the genus, such as French, which exhibits comparatively less inflection, and Romanian has comparatively more-it still marks case.	0
3626	32823	32823	2020.sigmorphon-1.1	Germanic	1	92	1.0	2.0	The Germanic genus comprises several languages which originated in Northern and Northwestern Europe, and today are spoken in many parts of the world.	0
3627	32824	32824	2020.sigmorphon-1.1	Germanic	2	93	2.0	2.0	Verbs in Germanic languages mark tense and mood, in many languages person and number are also marked, predominantly through suffixation.	0
3628	32825	32825	2020.sigmorphon-1.1	Germanic	3	94	2.0	2.0	Some Germanic languages exhibit widespread Indo-European ablaut.	0
3629	32826	32826	2020.sigmorphon-1.1	Germanic	4	95	3.0	2.0	The gendering of nouns differs between Germanic languages: German nouns can be masculine, feminine or neuter, while English nouns are not marked for gender.	0
3630	32827	32827	2020.sigmorphon-1.1	Germanic	5	96	4.0	2.0	In Danish and Swedish, historically masculine and feminine nouns have merged to form one common gender, so nouns are either common or neuter.	0
3631	32828	32828	2020.sigmorphon-1.1	Germanic	6	97	4.0	2.0	Marking of case also differs between the languages: German nouns have one of four cases and this case is marked in articles and adjectives as well as nouns and pronouns, while English does not mark noun case (although Old English, which also appears in our language sample, does).   no grammatical evidentials.	0
3632	32829	32829	2020.sigmorphon-1.1	Indo-Iranian	1	98	1.0	2.0	Oto-Manguean	0
3633	32830	32830	2020.sigmorphon-1.1	Indo-Iranian	2	99	1.0	2.0	The Oto-Manguean languages are a diverse family of tonal languages spoken in central and southern Mexico.	0
3634	32831	32831	2020.sigmorphon-1.1	Indo-Iranian	3	100	2.0	2.0	Even though all of these languages are tonal, the tonal system within each language varies widely.	0
3635	32832	32832	2020.sigmorphon-1.1	Indo-Iranian	4	101	2.0	2.0	Some have an inventory of two tones (e.g., Chichimec and Pame) others have ten tones (e.g., the Eastern Chatino languages of the Zapotecan branch, Palancar and Léonard (2016)).	0
3636	32833	32833	2020.sigmorphon-1.1	Indo-Iranian	5	102	2.0	2.0	Oto-Manguean languages are also rich in tonal morphology.	0
3637	32834	32834	2020.sigmorphon-1.1	Indo-Iranian	6	103	3.0	2.0	The inflectional system marks person-number and aspect in verbs and personnumber in adjectives and noun possessions, relying heavily on tonal contrasts.	0
3638	32835	32835	2020.sigmorphon-1.1	Indo-Iranian	7	104	3.0	2.0	Other interesting as-pects of Oto-Manguean languages include the fact that pronominal inflections use a system of enclitics, and first and second person plural has a distinction between exclusive and inclusive (Campbell, 2016).	0
3639	32836	32836	2020.sigmorphon-1.1	Indo-Iranian	8	105	3.0	2.0	Tone marking schemes in the writing systems also vary greatly.	0
3640	32837	32837	2020.sigmorphon-1.1	Indo-Iranian	9	106	4.0	2.0	Some writing systems do not represent tone, others use diacritics, and others represent tones with numbers.	0
3641	32838	32838	2020.sigmorphon-1.1	Indo-Iranian	10	107	4.0	2.0	In languages that use numbers, single digits represent level tones and double digits represent contour tones.	0
3642	32839	32839	2020.sigmorphon-1.1	Indo-Iranian	11	108	4.0	2.0	For example, in San Juan Quiahije of Eastern Chatino number 1 represents high tone, number 4 represents low tone, and numbers 14 represent a descending tone contour and numbers 42 represent an ascending tone contour Cruz (2014).	0
3643	32840	32840	2020.sigmorphon-1.1	Sino-Tibetan	1	109	1.0	2.0	The Sino-Tibetan family is represented by the Tibetan language.	0
3644	32841	32841	2020.sigmorphon-1.1	Sino-Tibetan	2	110	2.0	2.0	Tibetan uses an abugida script and contains complex syllabic components in which vowel marks can be added above and below the base consonant.	0
3645	32842	32842	2020.sigmorphon-1.1	Sino-Tibetan	3	111	3.0	2.0	Tibetan verbs are inflected for tense and mood.	0
3646	32843	32843	2020.sigmorphon-1.1	Sino-Tibetan	4	112	4.0	2.0	Previous studies on Tibetan morphology (Di et al., 2019) indicate that the majority of mispredictions produced by neural models are due to allomorphy.	0
3647	32844	32844	2020.sigmorphon-1.1	Sino-Tibetan	5	113	4.0	2.0	This is followed by generation of nonce words (impossible combinations of vowel and consonant components).	0
3648	32845	32845	2020.sigmorphon-1.1	Siouan	1	114	1.0	2.0	The Siouan languages are located in North America, predominantly along the Mississippi and Missouri Rivers and in the Ohio Valley.	0
3649	32846	32846	2020.sigmorphon-1.1	Siouan	2	115	2.0	2.0	The family is represented in our task by Dakota, a critically endangered language spoken in North and South Dakota, Minnesota, and Saskatchewan.	0
3650	32847	32847	2020.sigmorphon-1.1	Siouan	3	116	2.0	2.0	The Dakota language is largely agglutinating in its derivational morphology and fusional in its inflectional morphology with a mixed affixation system (Rankin et al., 2003).	0
3651	32848	32848	2020.sigmorphon-1.1	Siouan	4	117	3.0	2.0	The present task includes verbs, which are marked for first and second person, number, and duality.	0
3652	32849	32849	2020.sigmorphon-1.1	Siouan	5	118	4.0	2.0	All three affixation types are found: person was generally marked by an infix, but could also appear as a prefix, and plurality was marked by a suffix.	0
3653	32850	32850	2020.sigmorphon-1.1	Siouan	6	119	4.0	2.0	Morphophonological processes of fortition and vowel lowering are also present.	0
3654	32851	32851	2020.sigmorphon-1.1	Songhay	1	120	2.0	2.0	The Songhay family consists of around eleven or twelve languages spoken in Mali, Niger, Benin, Burkina Faso and Nigeria.	0
3655	32852	32852	2020.sigmorphon-1.1	Songhay	2	121	3.0	2.0	In the shared task we use Zarma, the most widely spoken Songhay language.	0
3656	32853	32853	2020.sigmorphon-1.1	Songhay	3	122	4.0	2.0	Most of the Songhay languages are predominantly SOV with medium-sized consonant inventories (with implosives), five phonemic vowels, vowel length distinctions, and word level tones, which also are used to distinguish nouns, verbs, and adjectives (Heath, 2014).	0
3657	32854	32854	2020.sigmorphon-1.1	Southern Daly	1	123	1.0	2.0	The Southern Daly is a small language family of the Northern Territory in Australia that consists of two distantly related languages.	0
3658	32855	32855	2020.sigmorphon-1.1	Southern Daly	2	124	2.0	2.0	In the current task we only have one of the languages, Murrinh-patha (which was initially thought to be a language isolate).	0
3659	32856	32856	2020.sigmorphon-1.1	Southern Daly	3	125	3.0	2.0	Murrinh-patha is classified as polysynthetic with highly complex verbal morphology.	0
3660	32857	32857	2020.sigmorphon-1.1	Southern Daly	4	126	4.0	2.0	Verbal roots are surrounded by prefixes and suffixes that indicate tense, mood, object, subject.	0
3661	32858	32858	2020.sigmorphon-1.1	Southern Daly	5	127	4.0	2.0	As Mansfield ( 2019) notes, Murrinh-patha verbs have 39 conjugation classes.	0
3662	32859	32859	2020.sigmorphon-1.1	Tungusic	1	128	1.0	2.0	Tungusic languages are spoken principally in Russia, China and Mongolia.	0
3663	32860	32860	2020.sigmorphon-1.1	Tungusic	2	129	2.0	2.0	In Russia they are concentrated in north and eastern Siberia and in China in the east, in Manchuria.	0
3664	32861	32861	2020.sigmorphon-1.1	Tungusic	3	130	2.0	2.0	The largest languages in the family are Xibe, Evenki and Even; we use Evenki in the shared task.	0
3665	32862	32862	2020.sigmorphon-1.1	Tungusic	4	131	3.0	2.0	The languages are of the agglutinating morphological type with a moderate number of cases, 7 for Xibe and 13 for Evenki.	0
3666	32863	32863	2020.sigmorphon-1.1	Tungusic	5	132	4.0	2.0	In addition to case markers, Evenki marks possession in nominals (including reflexive possession) and distinguishes between alienable and inalienable possession.	0
3667	32864	32864	2020.sigmorphon-1.1	Tungusic	6	133	4.0	2.0	In terms of morphophonological processes, the languages exhibit vowel harmony, consonant alternations and phonological vowel length.	0
3668	32865	32865	2020.sigmorphon-1.1	Turkic	1	134	1.0	2.0	Languages of the Turkic family are primarily spoken in Central Asia.	0
3669	32866	32866	2020.sigmorphon-1.1	Turkic	2	135	2.0	2.0	The family is morphologically concatenative, fusional, and suffixing.	0
3670	32867	32867	2020.sigmorphon-1.1	Turkic	3	136	2.0	2.0	Turkic languages generally exhibit back vowel harmony, with the notable exception of Uzbek.	0
3671	32868	32868	2020.sigmorphon-1.1	Turkic	4	137	3.0	2.0	In addition to harmony in backness, several languages also have labial vowel harmony (e.g., Kyrgyz, Turkmen, among others).	0
3672	32869	32869	2020.sigmorphon-1.1	Turkic	5	138	3.0	2.0	In addition, most of the languages have dorsal consonant allophony that accompanies back vowel harmony.	0
3673	32870	32870	2020.sigmorphon-1.1	Turkic	6	139	4.0	2.0	Additional morphophonological processes include vowel epenthesis and voicing assimilation.	0
3674	32871	32871	2020.sigmorphon-1.1	Turkic	7	140	4.0	2.0	Selection of the inflectional allomorph can frequently be determined from the infinitive morpheme (which frequently reveals vowel backness and roundedness) and also the final segment of the stem.	0
3675	32872	32872	2020.sigmorphon-1.1	Uralic	1	141	1.0	2.0	The Uralic languages are spoken in Russia from the north of Siberia to Scandinavia and Hungary in Europe.	0
3676	32873	32873	2020.sigmorphon-1.1	Uralic	2	142	1.0	2.0	They are agglutinating with some subgroups displaying fusional characteristics (e.g., the Sámi languages).	0
3677	32874	32874	2020.sigmorphon-1.1	Uralic	3	143	2.0	2.0	Many of the languages have vowel harmony.	0
3678	32875	32875	2020.sigmorphon-1.1	Uralic	4	144	2.0	2.0	The languages have almost complete suffixal morphology and a medium-sized case inventory, ranging from 5-6 cases to numbers in the high teens.	0
3679	32876	32876	2020.sigmorphon-1.1	Uralic	5	145	2.0	2.0	Many of the larger case paradigms are made up of spatial cases, sometimes with distinctions for direction and position.	0
3680	32877	32877	2020.sigmorphon-1.1	Uralic	6	146	3.0	2.0	Most of the languages have possessive suffixes, which can express possession, or agreement in non-finite clauses.	0
3681	32878	32878	2020.sigmorphon-1.1	Uralic	7	147	3.0	2.0	The paradigms are largely regular, with few, if any, irregular forms.	0
3682	32879	32879	2020.sigmorphon-1.1	Uralic	8	148	4.0	2.0	Many exhibit complex patterns of consonant gradation-consonant mutations that occur in specific morphological forms in some stems.	0
3683	32880	32880	2020.sigmorphon-1.1	Uralic	9	149	4.0	2.0	Which gradation category a stem belongs to in often unpredictable.	0
3684	32881	32881	2020.sigmorphon-1.1	Uralic	10	150	4.0	2.0	The languages spoken in Russia are typically SOV, while those in Europe have SVO order.	0
3685	32882	32882	2020.sigmorphon-1.1	Uto-Aztecan	1	151	1.0	2.0	The Uto-Aztecan family is represented by the Tohono O'odham (Papago-Pima) language spoken along the US-Mexico border in southern Arizona and northern Sonora.	0
3686	32883	32883	2020.sigmorphon-1.1	Uto-Aztecan	2	152	2.0	2.0	O'odham is agglutinative with a mixed prefixing and suffixing system.	0
3687	32884	32884	2020.sigmorphon-1.1	Uto-Aztecan	3	153	3.0	2.0	Nominal and verbal pluralization is frequently realized by partial reduplication of the initial consonant and/or vowel, and occasionally by final consonant deletion or null affixation.	0
3688	32885	32885	2020.sigmorphon-1.1	Uto-Aztecan	4	154	4.0	2.0	Processes targeting vowel length (shortening or lengthening) are also present.	0
3689	32886	32886	2020.sigmorphon-1.1	Uto-Aztecan	5	155	4.0	2.0	A small number of verbs exhibit suppletion in the past tense.	0
3690	32887	32887	2020.sigmorphon-1.1	Data Preparation	1	156	1.0	2.0	Data Format	0
3691	32888	32888	2020.sigmorphon-1.1	Data Preparation	2	157	2.0	2.0	Similar to previous years, training and development sets contain triples consisting of a lemma, a target form, and morphosyntactic descriptions (MSDs, or morphological tags).	0
3692	32889	32889	2020.sigmorphon-1.1	Data Preparation	3	158	3.0	2.0	3 Test sets only contain two fields, i.e., target forms are omitted.	0
3693	32890	32890	2020.sigmorphon-1.1	Data Preparation	4	159	4.0	2.0	All data follows UTF-8 encoding.	0
3694	32891	32891	2020.sigmorphon-1.1	Conversion and Canonicalization	1	160	1.0	2.0	A significant amount of data for this task was extracted from corresponding (language-specific) grammars.	0
3695	32892	32892	2020.sigmorphon-1.1	Conversion and Canonicalization	2	161	2.0	2.0	In order to allow cross-lingual comparison, we manually converted their features (tags) into the UniMorph format (Sylak-Glassman, 2016).	0
3696	32893	32893	2020.sigmorphon-1.1	Conversion and Canonicalization	3	162	3.0	2.0	"We then canonicalized the converted language data 4 to make sure all tags are consistently ordered and no category (e.g., ""Number"") is assigned two tags (e.g., singular and plural)."	0
3697	32894	32894	2020.sigmorphon-1.1	Conversion and Canonicalization	4	163	4.0	2.0	5	0
3698	32895	32895	2020.sigmorphon-1.1	Splitting	1	164	1.0	2.0	We use only noun, verb, and adjective forms to construct training, development, and evaluation sets.	0
3699	32896	32896	2020.sigmorphon-1.1	Splitting	2	165	2.0	2.0	We de-duplicate annotations such that there are no multiple examples of exact lemma-formtag matches.	0
3700	32897	32897	2020.sigmorphon-1.1	Splitting	3	166	2.0	2.0	To create splits, we randomly sample 70%, 10%, and 20% for train, development, and test, respectively.	0
3701	32898	32898	2020.sigmorphon-1.1	Splitting	4	167	3.0	2.0	We cap the training set size to 100k examples for each language; where languages exceed this (e.g., Finnish), we subsample to this point, balancing lemmas such that all forms for a given lemma are either included or discarded.	0
3702	32899	32899	2020.sigmorphon-1.1	Splitting	5	168	4.0	2.0	Some languages such as Zarma (dje), Tajik (tgk), Lingala (lin), Ludian* (lud), Māori (mao), Sotho (sot), Võro (vro), Anglo-Norman (xno), and Zulu (zul) contain less than 400 training samples and are extremely low-resource.	0
3703	32900	32900	2020.sigmorphon-1.1	Splitting	6	169	4.0	2.0	6 Tab. 6 and Tab. 7 in the Appendix provide the number of samples for every language in each split, the number of samples per lemma, and statistics on inconsistencies in the data.	0
3704	32901	32901	2020.sigmorphon-1.1	Baseline Systems	1	170	2.0	2.0	The organizers provided two types of pre-trained baselines.	0
3705	32902	32902	2020.sigmorphon-1.1	Baseline Systems	2	171	4.0	2.0	Their use was optional.	0
3706	32903	32903	2020.sigmorphon-1.1	Non-neural	1	172	1.0	2.0	The first baseline was a non-neural system that had been used as a baseline in earlier shared tasks on morphological reinflection (Cotterell et al., 2017(Cotterell et al., , 2018.	0
3707	32904	32904	2020.sigmorphon-1.1	Non-neural	2	173	2.0	2.0	The system first heuristically extracts lemma-to-form transformations; it assumes that these transformations are suffix-or prefix-based.	0
3708	32905	32905	2020.sigmorphon-1.1	Non-neural	3	174	2.0	2.0	4 Using the UniMorph schema canonicalization script https://github.com/unimorph/umcanonicalize	0
3709	32906	32906	2020.sigmorphon-1.1	Non-neural	4	175	3.0	2.0	5 Conversion schemes and canonicalization scripts are available at https://github.com/ sigmorphon2020/task0-data	0
3710	32907	32907	2020.sigmorphon-1.1	Non-neural	5	176	4.0	2.0	A simple majority classifier is used to apply the most frequent suitable transformation to an input lemma, given the morphological tag, yielding the output form.	0
3711	32908	32908	2020.sigmorphon-1.1	Non-neural	6	177	4.0	2.0	See Cotterell et al. (2017) for further details.	0
3712	32909	32909	2020.sigmorphon-1.1	Neural	1	178	1.0	2.0	Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*).	0
3713	32910	32910	2020.sigmorphon-1.1	Neural	2	179	2.0	2.0	The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIG-MORPHON shared task data (Wu et al., 2020, trm-*).	0
3714	32911	32911	2020.sigmorphon-1.1	Neural	3	180	2.0	2.0	Both models take the lemma and morphological tags as input and output the target inflection.	0
3715	32912	32912	2020.sigmorphon-1.1	Neural	4	181	3.0	2.0	The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. ( 2017)).	0
3716	32913	32913	2020.sigmorphon-1.1	Neural	5	182	4.0	2.0	Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language's alphabet, producing hallucinated lemma-tag-form triples.	0
3717	32914	32914	2020.sigmorphon-1.1	Neural	6	183	4.0	2.0	Both neural baselines were trained in mono-(*-single) and multilingual (shared parameters among the same family, *-shared) settings.	0
3718	32915	32915	2020.sigmorphon-1.1	Competing Systems	1	184	1.0	3.0	As Tab. 3 shows, 10 teams submitted 22 systems in total, out of which 19 were neural.	0
3719	32916	32916	2020.sigmorphon-1.1	Competing Systems	2	185	1.0	3.0	Some teams such as ETH Zurich and UIUC built their models on top of the proposed baselines.	0
3720	32917	32917	2020.sigmorphon-1.1	Competing Systems	3	186	1.0	3.0	In particular, ETH Zurich enriched each of the (multilingual) neural baseline models with exact decoding strategy that uses Dijkstra's search algorithm.	0
3721	32918	32918	2020.sigmorphon-1.1	Competing Systems	4	187	1.0	3.0	UIUC enriched the transformer model with synchronous bidirectional decoding technique (Zhou et al., 2019) in order to condition the prediction of an affix character on its environment from both sides.	0
3722	32919	32919	2020.sigmorphon-1.1	Competing Systems	5	188	1.0	3.0	(The authors demonstrate positive effects in Oto-Manguean, Turkic, and some Austronesian languages.)	0
3723	32920	32920	2020.sigmorphon-1.1	Competing Systems	6	189	1.0	3.0	A few teams further improved models that were among top performers in previous shared tasks.	0
3724	32921	32921	2020.sigmorphon-1.1	Competing Systems	7	190	2.0	3.0	IMS and Flexica re-used the hard monotonic attention model from (Aharoni and Goldberg, 2017).	0
3725	32922	32922	2020.sigmorphon-1.1	Competing Systems	8	191	2.0	3.0	IMS developed an ensemble of two models (with left-to-right and right-to-left generation or-der) with a genetic algorithm for ensemble search (Haque et al., 2016) and iteratively provided hallucinated data.	0
3726	32923	32923	2020.sigmorphon-1.1	Competing Systems	9	192	2.0	3.0	Flexica submitted two neural systems.	0
3727	32924	32924	2020.sigmorphon-1.1	Competing Systems	10	193	2.0	3.0	The first model (flexica-02-1) was multilingual (family-wise) hard monotonic attention model with improved alignment strategy.	0
3728	32925	32925	2020.sigmorphon-1.1	Competing Systems	11	194	2.0	3.0	This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016).	0
3729	32926	32926	2020.sigmorphon-1.1	Competing Systems	12	195	2.0	3.0	LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder-decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages.	0
3730	32927	32927	2020.sigmorphon-1.1	Competing Systems	13	196	3.0	3.0	DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins ( 2019) and trained it on all languages at once (massively multilingual).	0
3731	32928	32928	2020.sigmorphon-1.1	Competing Systems	14	197	3.0	3.0	The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax , deepspin-01-1).	0
3732	32929	32929	2020.sigmorphon-1.1	Competing Systems	15	198	3.0	3.0	Many teams based their models on the transformer architecture.	0
3733	32930	32930	2020.sigmorphon-1.1	Competing Systems	16	199	3.0	3.0	NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and ensembles of three (NYU-CUBoulder-01-0) and five (NYU-CUBoulder-03-0) pointer-generator transformers.	0
3734	32931	32931	2020.sigmorphon-1.1	Competing Systems	17	200	3.0	3.0	For languages with less than 1,000 training samples, they also generate hallucinated data.	0
3735	32932	32932	2020.sigmorphon-1.1	Competing Systems	18	201	3.0	3.0	CULing developed an ensemble of three (monolingual) transformers with identical architecture but different input data format.	0
3736	32933	32933	2020.sigmorphon-1.1	Competing Systems	19	202	4.0	3.0	The first model was trained on the initial data format (lemma, target tags, target form).	0
3737	32934	32934	2020.sigmorphon-1.1	Competing Systems	20	203	4.0	3.0	For the other two models the team used the idea of lexeme's principal parts (Finkel and Stump, 2007)    were neural, some teams experimented with nonneural approaches showing that in certain scenarios they might surpass neural systems.	0
3738	32935	32935	2020.sigmorphon-1.1	Competing Systems	21	204	4.0	3.0	A large group of researchers from CU7565 manually developed finite-state grammars for 25 languages (CU7565-01-0).	0
3739	32936	32936	2020.sigmorphon-1.1	Competing Systems	22	205	4.0	3.0	They additionally developed a non-neural learner for all languages (CU7565-02-0) that uses hierarchical paradigm clustering (based on similarity of string transformation rules between inflectional slots).	0
3740	32937	32937	2020.sigmorphon-1.1	Competing Systems	23	206	4.0	3.0	Another team, Flexica, proposed a model (flexica-01-0) conceptually similar to Hulden et al. (2014), although they did not attempt to reconstruct the paradigm itself and treated transformation rules independently assigning each of them a score based on its frequency and specificity as well as diversity of the characters surrounding the pattern.	0
3741	32938	32938	2020.sigmorphon-1.1	Competing Systems	24	207	4.0	3.0	7	0
3742	32939	32939	2020.sigmorphon-1.1	Evaluation	1	208	1.0	3.0	This year, we instituted a slightly different evaluation regimen than in previous years, which takes into account the statistical significance of differences between systems and allows for an informed comparison across languages and families better than a simple macro-average.	0
3743	32940	32940	2020.sigmorphon-1.1	Evaluation	2	209	1.0	3.0	The process works as follows:	0
3744	32941	32941	2020.sigmorphon-1.1	Evaluation	3	210	2.0	3.0	1	0
3745	32942	32942	2020.sigmorphon-1.1	Evaluation	4	211	2.0	3.0	For each language, we rank the systems according to their accuracy (or Levenshtein distance).	0
3746	32943	32943	2020.sigmorphon-1.1	Evaluation	5	212	3.0	3.0	To do so, we use paired bootstrap resampling (Koehn, 2004) 8 to only take statistically significant differences into account.	0
3747	32944	32944	2020.sigmorphon-1.1	Evaluation	6	213	3.0	3.0	That way, any system which is the same (as assessed via statistical significance) as the best performing one is also ranked 1 st for that language.	0
3748	32945	32945	2020.sigmorphon-1.1	Evaluation	7	214	4.0	3.0	re-rank them based on the amount of times they ranked 1 st , 2 nd , 3 rd , etc.	0
3749	32946	32946	2020.sigmorphon-1.1	Evaluation	8	215	4.0	3.0	Table 4 illustrates an example of this process using four Zapotecan languages and six systems.	0
3750	32947	32947	2020.sigmorphon-1.1	Results	1	216	1.0	3.0	This year we had four winning systems (i.e., ones that outperform the best baseline): CULing-01-0, deepspin-02-1, uiuc-01-0, and deepspin-01-1, all neural.	0
3751	32948	32948	2020.sigmorphon-1.1	Results	2	217	1.0	3.0	As Tab. 5 shows, they achieve over 90% accuracy.	0
3752	32949	32949	2020.sigmorphon-1.1	Results	3	218	1.0	3.0	Although CULing-01-0 and uiuc-01-0 are both monolingual transformers that do not use any hallucinated data, they follow different strategies to improve performance.	0
3753	32950	32950	2020.sigmorphon-1.1	Results	4	219	2.0	3.0	The strategy proposed by CULing-01-0 of enriching the input data with extra entries that included non-lemma forms and their tags as a source form, enabled their system to be among top performers on all language families; uiuc-01-0, on the other hand, did not modify the data but rather changed the decoder to be bidirectional and made family-wise fine-tuning of each (monolingual) model.	0
3754	32951	32951	2020.sigmorphon-1.1	Results	5	220	2.0	3.0	The system is also among the top performers on all language families except Iranian.	0
3755	32952	32952	2020.sigmorphon-1.1	Results	6	221	2.0	3.0	The third team, DeepSpin, trained and fine-tuned their models on all language data.	0
3756	32953	32953	2020.sigmorphon-1.1	Results	7	222	3.0	3.0	Both models are ranked high (although the sparsemax model, deepspin-02-1, performs better overall) on most language groups with exception of Algic.	0
3757	32954	32954	2020.sigmorphon-1.1	Results	8	223	3.0	3.0	Sparsemax was also found useful by CMU-Tartan.	0
3758	32955	32955	2020.sigmorphon-1.1	Results	9	224	3.0	3.0	The neural ensemble model with data augmentation from IMS team shows superior performance on languages with smaller data sizes (under 10,000 samples).	0
3759	32956	32956	2020.sigmorphon-1.1	Results	10	225	4.0	3.0	LTI and Flexica teams also observed positive effects of multilingual training and data hallucination on low-resource languages.	0
3760	32957	32957	2020.sigmorphon-1.1	Results	11	226	4.0	3.0	The latter was also found useful in the ablation study made by NYU-CUBoulder team.	0
3761	32958	32958	2020.sigmorphon-1.1	Results	12	227	4.0	3.0	Several teams aimed to address particular research questions; we will further summarize their results.	0
3762	32959	32959	2020.sigmorphon-1.1	System	1	228	1.0	3.0	Rank Acc	0
3763	32960	32960	2020.sigmorphon-1.1	System	2	229	1.0	3.0	Has morphological inflection become a solved problem in certain scenarios?	0
3764	32961	32961	2020.sigmorphon-1.1	System	3	230	2.0	3.0	The results shown in Fig. 2 suggest that for some of the development language families, such as Austronesian and Niger-Congo, the task was relatively easy, with most systems achieving high accuracy, whereas the task was more difficult for Uralic and Oto-Manguean languages, which showed greater variability in level of performance across submitted systems.	0
3765	32962	32962	2020.sigmorphon-1.1	System	4	231	2.0	3.0	Languages such as Ludic (lud), Norwegian Nynorsk (nno), Middle Low German 1 3 6 7 1 5 0 9 2 3 3 9 1 0 8 1 3 4 6 4 5 9 0 1 5 8 5 7 3 9 9 8 8 7 0 1 6 5 1 1 7 9 9 1 9 6 2 4 8 2 3 8 9 5 3 9 7 4 5 4 4 4 2 9 3 3 3 1 3 5 9 6 2 1 1 5 2 0 2 2 6 0 2 7 0 1 7 5 8 4 6 6 3 1 1 3 4 9 1 2 3 5 0 9 8 0 2 4 8 4 4 7 1 6 7 0 4 1 4 9 1 6 3 7 2 1 3 3 9 7 0.00  (gml), Evenki (evn), and O'odham (ood) seem to be the most challenging languages based on simple accuracy.	0
3766	32963	32963	2020.sigmorphon-1.1	System	5	232	2.0	3.0	"For a more fine-grained study, we have classified test examples into four categories: ""very easy"", ""easy"", ""hard"", and ""very hard""."	0
3767	32964	32964	2020.sigmorphon-1.1	System	6	233	3.0	3.0	"""Very easy"" examples are ones that all submitted systems got correct, while ""very hard"" examples are ones that no submitted system got correct."	0
3768	32965	32965	2020.sigmorphon-1.1	System	7	234	3.0	3.0	"""Easy"" examples were predicted correctly for 80% of systems, and ""hard"" were only correct in 20% of systems."	0
3769	32966	32966	2020.sigmorphon-1.1	System	8	235	4.0	3.0	Fig. 3, Fig. 4, and Fig. 5 represent percentage of noun, verb, and adjective samples that fall into each category and illustrate that most language samples are correctly predicted by majority of the systems.	0
3770	32967	32967	2020.sigmorphon-1.1	System	9	236	4.0	3.0	For noun declension, Old English (ang), Middle Low German (gml), Evenki (evn), O'odham (ood), Võro (vro) are the most difficult (some of this difficulty comes from language data inconsistency, as described in the following section).	0
3771	32968	32968	2020.sigmorphon-1.1	System	10	237	4.0	3.0	For adjective declension, Classic Syriac presents the highest difficulty (likely due to its limited data).	0
3772	32969	32969	2020.sigmorphon-1.1	Error Analysis	1	238	1.0	3.0	In our error analysis we follow the error type taxonomy proposed in Gorman et al. (2019).	0
3773	32970	32970	2020.sigmorphon-1.1	Error Analysis	2	239	1.0	3.0	First, we evaluate systematic errors due to inconsistencies in the data, followed by an analysis of whether having seen the language or its family improved accuracy.	0
3774	32971	32971	2020.sigmorphon-1.1	Error Analysis	3	240	1.0	3.0	We then proceed with an overview of accuracy for each of the language families.	0
3775	32972	32972	2020.sigmorphon-1.1	Error Analysis	4	241	1.0	3.0	For a select number of families, we provide a more detailed analysis of the error patterns.	0
3776	32973	32973	2020.sigmorphon-1.1	Error Analysis	5	242	2.0	3.0	Tab. 6 and Tab. 7 provide the number of samples in the training, development, and test sets, percentage of inconsistent entries (the same lemma-tag pair has multiple infected forms) in them, percentage of contradicting entries (same lemma-tag pair occurring in train and development or test sets but assigned to different inflected forms), and percentage of entries in the development or test sets containing a lemma observed in the training set.	0
3777	32974	32974	2020.sigmorphon-1.1	Error Analysis	6	243	2.0	3.0	The train, development and test sets contain 2%, 0.3%, and 0.6% inconsistent entries, respectively.	0
3778	32975	32975	2020.sigmorphon-1.1	Error Analysis	7	244	2.0	3.0	Azerbaijani (aze), Old English (ang), Cree (cre), Danish (dan), Middle Low German (gml), Kannada (kan), Norwegian Bokmål (nob), Chichimec (pei), and Veps (vep) had the highest rates of inconsistency.	0
3779	32976	32976	2020.sigmorphon-1.1	Error Analysis	8	245	2.0	3.0	These languages also exhibit the highest percentage of contradicting entries.	0
3780	32977	32977	2020.sigmorphon-1.1	Error Analysis	9	246	3.0	3.0	The inconsistencies in some Finno-Ugric languages (such as Veps and Ludic) are due to dialectal variations.	0
3781	32978	32978	2020.sigmorphon-1.1	Error Analysis	10	247	3.0	3.0	The overall accuracy of system and language pairings appeared to improve with an increase in the size of the dataset (Fig. 6; see also Fig. 7 for accuracy trends by language family and Fig. 8 for accuracy trends by system).	0
3782	32979	32979	2020.sigmorphon-1.1	Error Analysis	11	248	3.0	3.0	Overall, the variance was considerable regardless of whether the language family or even the language itself had been observed during the Development Phase.	0
3783	32980	32980	2020.sigmorphon-1.1	Error Analysis	12	249	3.0	3.0	A linear mixed-effects regression was used to assess variation in accuracy using fixed effects of language category, the size of the training dataset (log count), and their interactions, as well as random intercepts for system and language family accuracy.	0
3784	32981	32981	2020.sigmorphon-1.1	Error Analysis	13	250	4.0	3.0	10 Language category was sum-coded with three levels: development language-development family, surprise language-development family, or surprise language-surprise family.	0
3785	32982	32982	2020.sigmorphon-1.1	Error Analysis	14	251	4.0	3.0	A significant effect of dataset size was observed, such that a one unit increase in log count corresponded to a 2% increase in accuracy (β = 0.019, p &lt; 0.001).	0
3786	32983	32983	2020.sigmorphon-1.1	Error Analysis	15	252	4.0	3.0	Language category type also significantly influenced accuracy: both development languages and surprise languages from development families were less accurate on average (β dev−dev = -0.145, β sur−dev = -0.167, each p &lt; 0.001).	0
3787	32984	32984	2020.sigmorphon-1.1	Error Analysis	16	253	4.0	3.0	These main effects were, however, significantly modulated by interactions with dataset size: on top of the main effect of dataset size, accuracy for development languages increased an additional ≈ 1.7% (β dev−dev×size = 0.017, p &lt; 0.001) and accuracy for surprise languages from development families increased an additional ≈ 2.9% (β sur−dev×size = 0.029, p &lt; 0.001).	0
3788	32985	32985	2020.sigmorphon-1.1	Afro-Asiatic:	1	254	1.0	3.0	This family was represented by three languages.	0
3789	32986	32986	2020.sigmorphon-1.1	Afro-Asiatic:	2	255	1.0	3.0	Mean accuracy across systems was above average at 91.7%.	0
3790	32987	32987	2020.sigmorphon-1.1	Afro-Asiatic:	3	256	1.0	3.0	Relative to other families, variance in accuracy was low, but nevertheless ranged from 41.1% to 99.0%.	0
3791	32988	32988	2020.sigmorphon-1.1	Afro-Asiatic:	4	257	1.0	3.0	Algic:	0
3792	32989	32989	2020.sigmorphon-1.1	Afro-Asiatic:	5	258	1.0	3.0	This family was represented by one language, Cree.	0
3793	32990	32990	2020.sigmorphon-1.1	Afro-Asiatic:	6	259	1.0	3.0	Mean accuracy across systems was below average at 65.1%.	0
3794	32991	32991	2020.sigmorphon-1.1	Afro-Asiatic:	7	260	2.0	3.0	Relative to other families, variance in accuracy was low, ranging from 41.5% to 73%.	0
3795	32992	32992	2020.sigmorphon-1.1	Afro-Asiatic:	8	261	2.0	3.0	All systems appeared to struggle with the choice of preverbal auxiliary.	0
3796	32993	32993	2020.sigmorphon-1.1	Afro-Asiatic:	9	262	2.0	3.0	Some auxiliaries were overloaded: 'kitta' could refer to future, imperfective, or imperative.	0
3797	32994	32994	2020.sigmorphon-1.1	Afro-Asiatic:	10	263	2.0	3.0	The morphological features for mood and tense were also frequently combined, such as SBJV+OPT (subjunctive plus optative mood).	0
3798	32995	32995	2020.sigmorphon-1.1	Afro-Asiatic:	11	264	2.0	3.0	While the paradigms were very large, there were very few lemmas (28 impersonal verbs and 14 transitive verbs), which may have contributed to the lower accuracy.	0
3799	32996	32996	2020.sigmorphon-1.1	Afro-Asiatic:	12	265	2.0	3.0	Interestingly, the inflections could largely be generated by rules.	0
3800	32997	32997	2020.sigmorphon-1.1	Afro-Asiatic:	13	266	3.0	3.0	11 Austronesian:	0
3801	32998	32998	2020.sigmorphon-1.1	Afro-Asiatic:	14	267	3.0	3.0	This family was represented by five languages.	0
3802	32999	32999	2020.sigmorphon-1.1	Afro-Asiatic:	15	268	3.0	3.0	Mean accuracy across systems was around average at 80.5%.	0
3803	33000	33000	2020.sigmorphon-1.1	Afro-Asiatic:	16	269	3.0	3.0	Relative to other families, variance in accuracy was high, with accuracy ranging from 39.5% to 100%.	0
3804	33001	33001	2020.sigmorphon-1.1	Afro-Asiatic:	17	270	3.0	3.0	One may notice a discrepancy among the difficulty in processing different Austronesian languages.	0
3805	33002	33002	2020.sigmorphon-1.1	Afro-Asiatic:	18	271	3.0	3.0	For instance, we see a difference of over 10% in the baseline performance of Cebuano (84%) and Hiligaynon (96%).	0
3806	33003	33003	2020.sigmorphon-1.1	Afro-Asiatic:	19	272	4.0	3.0	12 This could come from the fact that Cebuano only has partial reduplication while Hiligaynon has full reduplication.	0
3807	33004	33004	2020.sigmorphon-1.1	Afro-Asiatic:	20	273	4.0	3.0	Furthermore, the prefix choice for Cebuano is more irregular, making it more difficult to predict the correct conjugation of the verb.	0
3808	33005	33005	2020.sigmorphon-1.1	Afro-Asiatic:	21	274	4.0	3.0	Dravidian:	0
3809	33006	33006	2020.sigmorphon-1.1	Afro-Asiatic:	22	275	4.0	3.0	This family was represented by two languages: Kannada and Telugu.	0
3810	33007	33007	2020.sigmorphon-1.1	Afro-Asiatic:	23	276	4.0	4.0	Mean accuracy across systems was around average at 82.2%.	0
3811	33008	33008	2020.sigmorphon-1.1	Afro-Asiatic:	24	277	4.0	4.0	Relative to other families, variance in accuracy was high: system accuracy ranged from 44.6% to 96.0%.	0
3812	33009	33009	2020.sigmorphon-1.1	Afro-Asiatic:	25	278	4.0	4.0	Accuracy for Telugu was systematically higher than accuracy for Kannada.	0
3813	33010	33010	2020.sigmorphon-1.1	Indo-European:	1	279	1.0	4.0	This family was represented by 29 languages and four main branches.	0
3814	33011	33011	2020.sigmorphon-1.1	Indo-European:	2	280	1.0	4.0	Mean accuracy across systems was slightly above average at 86.9%.	0
3815	33012	33012	2020.sigmorphon-1.1	Indo-European:	3	281	1.0	4.0	Relative to other families, variance in accuracy was very high: system accuracy ranged from 0.02% to 100%.	0
3816	33013	33013	2020.sigmorphon-1.1	Indo-European:	4	282	1.0	4.0	For Indo-Aryan, mean accuracy was high (96.0%) with low variance; for Germanic, mean accuracy was slightly below average (79.0%) but with very high variance (ranging from 0.02% to 99.5%), for Romance, mean accuracy was high (93.4%) but also had a high variance (ranging from 23.5% to 99.8%), and for Iranian, mean accuracy was high (89.2%), but again with a high variance (ranging from 25.0% to 100%).	0
3817	33014	33014	2020.sigmorphon-1.1	Indo-European:	5	283	1.0	4.0	Languages from the Germanic branch of the Indo-European family were included in the Development Phase.	0
3818	33015	33015	2020.sigmorphon-1.1	Indo-European:	6	284	1.0	4.0	Niger-Congo:	0
3819	33016	33016	2020.sigmorphon-1.1	Indo-European:	7	285	1.0	4.0	This family was represented by ten languages.	0
3820	33017	33017	2020.sigmorphon-1.1	Indo-European:	8	286	1.0	4.0	Mean accuracy across systems was very good at 96.4%.	0
3821	33018	33018	2020.sigmorphon-1.1	Indo-European:	9	287	1.0	4.0	Relative to other families, variance in accuracy was low, with accuracy ranging from 62.8% to 100%.	0
3822	33019	33019	2020.sigmorphon-1.1	Indo-European:	10	288	1.0	4.0	"Most languages in this family are considered low resource, and the resources used for data gathering may have been biased towards the languages' regular forms, as such this high accuracy may not be representative of the ""easiness"" of the task in this family."	0
3823	33020	33020	2020.sigmorphon-1.1	Indo-European:	11	289	1.0	4.0	Languages from the Niger-Congo family was included in the Development Phase.	0
3824	33021	33021	2020.sigmorphon-1.1	Indo-European:	12	290	1.0	4.0	Oto-Manguean:	0
3825	33022	33022	2020.sigmorphon-1.1	Indo-European:	13	291	1.0	4.0	This family was represented by nine languages.	0
3826	33023	33023	2020.sigmorphon-1.1	Indo-European:	14	292	1.0	4.0	Mean accuracy across systems was slightly below average at 78.5%.	0
3827	33024	33024	2020.sigmorphon-1.1	Indo-European:	15	293	1.0	4.0	Relative to other families, variance in accuracy was high, with accuracy ranging from 18.7% to 99.1%.	0
3828	33025	33025	2020.sigmorphon-1.1	Indo-European:	16	294	1.0	4.0	Languages from the Oto-Manguean family were included in the Development Phase.	0
3829	33026	33026	2020.sigmorphon-1.1	Indo-European:	17	295	1.0	4.0	Sino-Tibetan:	0
3830	33027	33027	2020.sigmorphon-1.1	Indo-European:	18	296	1.0	4.0	This family was represented by one language, Bodic.	0
3831	33028	33028	2020.sigmorphon-1.1	Indo-European:	19	297	1.0	4.0	Mean accuracy across systems was average at 82.1%, and variance across systems was also very low.	0
3832	33029	33029	2020.sigmorphon-1.1	Indo-European:	20	298	2.0	4.0	Accuracy ranged from 67.9% to 85.1%.	0
3833	33030	33030	2020.sigmorphon-1.1	Indo-European:	21	299	2.0	4.0	The results are similar to those in Di et al. (2019) where majority of errors relate to allomorphy and impossible combinations of Tibetan unit components.	0
3834	33031	33031	2020.sigmorphon-1.1	Indo-European:	22	300	2.0	4.0	above average at 89.4%, and variance across systems was also low, despite the range from 0% to 95.7%.	0
3835	33032	33032	2020.sigmorphon-1.1	Indo-European:	23	301	2.0	4.0	Dakota presented variable prefixing and infixing of person morphemes, along some complexities related to fortition processes.	0
3836	33033	33033	2020.sigmorphon-1.1	Indo-European:	24	302	2.0	4.0	Determining the factor(s) that governed variation in affix position was difficult from a linguist's perspective, though many systems were largely successful.	0
3837	33034	33034	2020.sigmorphon-1.1	Indo-European:	25	303	2.0	4.0	Success varied in the choice of the first or second person singular allomorphs which had increasing degrees of consonant strengthening (e.g., /wa/, /ma/, /mi/ /bde/, /bdu/ for the first person singular and /ya/, /na/, /ni/, /de/, or /du/ for the second person singular).	0
3838	33035	33035	2020.sigmorphon-1.1	Indo-European:	26	304	2.0	4.0	In some cases, these fortition processes were overapplied, and in some cases, entirely missed.	0
3839	33036	33036	2020.sigmorphon-1.1	Indo-European:	27	305	2.0	4.0	Songhay:	0
3840	33037	33037	2020.sigmorphon-1.1	Indo-European:	28	306	2.0	4.0	This family was represented by one language, Zarma.	0
3841	33038	33038	2020.sigmorphon-1.1	Indo-European:	29	307	2.0	4.0	Mean accuracy across systems was above average at 88.6%, and variance across systems was relatively high.	0
3842	33039	33039	2020.sigmorphon-1.1	Indo-European:	30	308	2.0	4.0	Accuracy ranged from 0% to 100%.	0
3843	33040	33040	2020.sigmorphon-1.1	Indo-European:	31	309	2.0	4.0	Southern Daly:	0
3844	33041	33041	2020.sigmorphon-1.1	Indo-European:	32	310	2.0	4.0	This family was represented by one language, Murrinh-Patha.	0
3845	33042	33042	2020.sigmorphon-1.1	Indo-European:	33	311	2.0	4.0	Mean accuracy across systems was below average at 73.2%, and variance across systems was relatively high.	0
3846	33043	33043	2020.sigmorphon-1.1	Indo-European:	34	312	2.0	4.0	Accuracy ranged from 21.2% to 91.9%.	0
3847	33044	33044	2020.sigmorphon-1.1	Indo-European:	35	313	2.0	4.0	Tungusic:	0
3848	33045	33045	2020.sigmorphon-1.1	Indo-European:	36	314	2.0	4.0	This family was represented by one language, Evenki.	0
3849	33046	33046	2020.sigmorphon-1.1	Indo-European:	37	315	2.0	4.0	The overall accuracy was the lowest across families.	0
3850	33047	33047	2020.sigmorphon-1.1	Indo-European:	38	316	2.0	4.0	Mean accuracy was 53.8% with very low variance across systems.	0
3851	33048	33048	2020.sigmorphon-1.1	Indo-European:	39	317	2.0	4.0	Accuracy ranged from 43.5% to 59.0%.	0
3852	33049	33049	2020.sigmorphon-1.1	Indo-European:	40	318	3.0	4.0	The low accuracy is due to several factors.	0
3853	33050	33050	2020.sigmorphon-1.1	Indo-European:	41	319	3.0	4.0	Firstly and primarily, the dataset was created from oral speech samples in various dialects of the language.	0
3854	33051	33051	2020.sigmorphon-1.1	Indo-European:	42	320	3.0	4.0	The Evenki language is known to have rich dialectal variation.	0
3855	33052	33052	2020.sigmorphon-1.1	Indo-European:	43	321	3.0	4.0	Moreover, there was little attempt at any standardization in the oral speech transcription.	0
3856	33053	33053	2020.sigmorphon-1.1	Indo-European:	44	322	3.0	4.0	These peculiarities led to a high number of errors.	0
3857	33054	33054	2020.sigmorphon-1.1	Indo-European:	45	323	3.0	4.0	For instance, some of the systems synthesized a wrong plural form for a noun ending in /-n/.	0
3858	33055	33055	2020.sigmorphon-1.1	Indo-European:	46	324	3.0	4.0	Depending on the dialect, it can be /-r/ or /-l/, and there is a trend to have /-hVl/ for borrowed nouns.	0
3859	33056	33056	2020.sigmorphon-1.1	Indo-European:	47	325	3.0	4.0	Deducing such a rule as well as the fact that the noun is a loanword is a hard task.	0
3860	33057	33057	2020.sigmorphon-1.1	Indo-European:	48	326	3.0	4.0	Other suffixes may also have variable forms (such as /-k	0
3861	33058	33058	2020.sigmorphon-1.1	Indo-European:	49	327	3.0	4.0	Vllu/ vs /-k	0
3862	33059	33059	2020.sigmorphon-1.1	Indo-European:	50	328	3.0	4.0	Vldu/ depending on the dialect for the 2PL imperative.	0
3863	33060	33060	2020.sigmorphon-1.1	Indo-European:	51	329	3.0	4.0	Some verbs have irregular past tense forms depending on the dialect and the meaning of the verb (e. g. /o:-/ 'to make' and 'to become').	0
3864	33061	33061	2020.sigmorphon-1.1	Indo-European:	52	330	3.0	4.0	Next, various dialects exhibit various vowel and consonant changes in suffixes.	0
3865	33062	33062	2020.sigmorphon-1.1	Indo-European:	53	331	3.0	4.0	For example, some dialects (but not all of them) change /w/ to /b/ after /l/, and the systems sometimes synthesized a wrong form.	0
3866	33063	33063	2020.sigmorphon-1.1	Indo-European:	54	332	3.0	4.0	The vowel harmony is complex: not all suffixes obey it, and it is also dialect-dependent.	0
3867	33064	33064	2020.sigmorphon-1.1	Indo-European:	55	333	3.0	4.0	Some suffixes have variants (e. g., /-sin/ and /-s/ for SEMEL (semelfactive)), and the choice between them might be hard to understand.	0
3868	33065	33065	2020.sigmorphon-1.1	Indo-European:	56	334	3.0	4.0	Finally, some of the mistakes are due to the markup scheme scarcity.	0
3869	33066	33066	2020.sigmorphon-1.1	Indo-European:	57	335	3.0	4.0	For example, various past tense forms are all annotated as PST, or there are several comitative suffixes all annotated as COM.	0
3870	33067	33067	2020.sigmorphon-1.1	Indo-European:	58	336	3.0	4.0	Moreover, some features are present in the word form but they receive no annotation at all.	0
3871	33068	33068	2020.sigmorphon-1.1	Indo-European:	59	337	4.0	4.0	It is worth mentioning that some of the predictions could theoretically be possible.	0
3872	33069	33069	2020.sigmorphon-1.1	Indo-European:	60	338	4.0	4.0	To sum up, the Evenki case presents the chal-lenges of oral non-standardized speech.	0
3873	33070	33070	2020.sigmorphon-1.1	Indo-European:	61	339	4.0	4.0	Turkic:	0
3874	33071	33071	2020.sigmorphon-1.1	Indo-European:	62	340	4.0	4.0	This family was represented by nine languages.	0
3875	33072	33072	2020.sigmorphon-1.1	Indo-European:	63	341	4.0	4.0	Mean accuracy across systems was relatively high at 93%, and relative to other families, variance across systems was low.	0
3876	33073	33073	2020.sigmorphon-1.1	Indo-European:	64	342	4.0	4.0	Accuracy ranged from 51.5% to 100%.	0
3877	33074	33074	2020.sigmorphon-1.1	Indo-European:	65	343	4.0	4.0	Accuracy was lower for Azerbaijani and Turkmen, which after closer inspection revealed some slight contamination in the 'gold' files.	0
3878	33075	33075	2020.sigmorphon-1.1	Indo-European:	66	344	4.0	4.0	There was very marginal variation in the accuracy for these languages across systems.	0
3879	33076	33076	2020.sigmorphon-1.1	Indo-European:	67	345	4.0	4.0	Besides these two, accuracies were predominantly above 98%.	0
3880	33077	33077	2020.sigmorphon-1.1	Indo-European:	68	346	4.0	4.0	A few systems struggled with the choice and inflection of the postverbal auxiliary in various languages (e.g., Kyrgyz, Kazakh, and Uzbek).	0
3881	33078	33078	2020.sigmorphon-1.1	Indo-European:	69	347	4.0	4.0	Uralic:	0
3882	33079	33079	2020.sigmorphon-1.1	Indo-European:	70	348	4.0	4.0	This family was represented by 16 languages.	0
3883	33080	33080	2020.sigmorphon-1.1	Indo-European:	71	349	4.0	4.0	Mean accuracy across systems was average at 81.5%, but the variance across systems and languages was very high.	0
3884	33081	33081	2020.sigmorphon-1.1	Indo-European:	72	350	4.0	4.0	Accuracy ranged from 0% to 99.8%.	0
3885	33082	33082	2020.sigmorphon-1.1	Indo-European:	73	351	4.0	4.0	Languages from the Uralic family were included in the Development Phase.	0
3886	33083	33083	2020.sigmorphon-1.1	Indo-European:	74	352	4.0	4.0	Uto-Aztecan:	0
3887	33084	33084	2020.sigmorphon-1.1	Indo-European:	75	353	4.0	4.0	This family was represented by one language, O'odham.	0
3888	33085	33085	2020.sigmorphon-1.1	Indo-European:	76	354	4.0	4.0	Mean accuracy across systems was slightly below average at 76.4%, but the variance across systems and languages was fairly low.	0
3889	33086	33086	2020.sigmorphon-1.1	Indo-European:	77	355	4.0	4.0	Accuracy ranged from 54.8% to 82.5%.	0
3890	33087	33087	2020.sigmorphon-1.1	Indo-European:	78	356	4.0	4.0	The systems with higher accuracy may have benefited from better recall of suppletive forms relative to lower accuracy systems.	0
3891	33088	33088	2020.sigmorphon-1.1	Conclusion	1	357	1.0	4.0	This years's shared task on morphological reinflection focused on building models that could generalize across an extremely typologically diverse set of languages, many from understudied language families and with limited available text resources.	0
3892	33089	33089	2020.sigmorphon-1.1	Conclusion	2	358	1.0	4.0	As in previous years, neural models performed well, even in relatively low-resource cases.	0
3893	33090	33090	2020.sigmorphon-1.1	Conclusion	3	359	2.0	4.0	Submissions were able to make productive use of multilingual training to take advantage of commonalities across languages in the dataset.	0
3894	33091	33091	2020.sigmorphon-1.1	Conclusion	4	360	2.0	4.0	Data augmentation techniques such as hallucination helped fill in the gaps and allowed networks to generalize to unseen inputs.	0
3895	33092	33092	2020.sigmorphon-1.1	Conclusion	5	361	2.0	4.0	These techniques, combined with architecture tweaks like sparsemax, resulted in excellent overall performance on many languages (over 90% accuracy on average).	0
3896	33093	33093	2020.sigmorphon-1.1	Conclusion	6	362	3.0	4.0	However, the task's focus on typological diversity revealed that some morphology types and language families (Tungusic, Oto-Manguean, South-ern Daly) remain a challenge for even the best systems.	0
3897	33094	33094	2020.sigmorphon-1.1	Conclusion	7	363	3.0	4.0	These families are extremely low-resource, represented in this dataset by few or a single language.	0
3898	33095	33095	2020.sigmorphon-1.1	Conclusion	8	364	3.0	4.0	This makes cross-linguistic transfer of similarities by multilanguage training less viable.	0
3899	33096	33096	2020.sigmorphon-1.1	Conclusion	9	365	4.0	4.0	They may also have morphological properties and rules (e.g., Evenki is agglutinating with many possible forms for each lemma) that are particularly difficult for machine learners to induce automatically from sparse data.	0
3900	33097	33097	2020.sigmorphon-1.1	Conclusion	10	366	4.0	4.0	For some languages (Ingrian, Tajik, Tagalog, Zarma, and Lingala), optimal performance was only achieved in this shared task by hand-encoding linguist knowledge in finite state grammars.	0
3901	33098	33098	2020.sigmorphon-1.1	Conclusion	11	367	4.0	4.0	It is up to future research to imbue models with the right kinds of linguistic inductive biases to overcome these challenges.	0
3902	38129	38129	K15-2001	title	1	1	4.0	1.0	The CoNLL-2015 Shared Task on Shallow Discourse Parsing	0
3903	38130	38130	K15-2001	abstract	1	2	1.0	1.0	The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text.	0
3904	38131	38131	K15-2001	abstract	2	3	1.0	1.0	A discourse relation can be expressed explicitly or implicitly, and takes two arguments realized as sentences, clauses, or in some rare cases, phrases.	0
3905	38132	38132	K15-2001	abstract	3	4	2.0	1.0	Sixteen teams from three continents participated in this task.	0
3906	38133	38133	K15-2001	abstract	4	5	2.0	1.0	For the first time in the history of the CoNLL shared tasks, participating teams, instead of running their systems on the test set and submitting the output, were asked to deploy their systems on a remote virtual machine and use a web-based evaluation platform to run their systems on the test set.	0
3907	38134	38134	K15-2001	abstract	5	6	3.0	1.0	This meant they were unable to actually see the data set, thus preserving its integrity and ensuring its replicability.	0
3908	38135	38135	K15-2001	abstract	6	7	3.0	1.0	In this paper, we present the task definition, the training and test sets, and the evaluation protocol and metric used during this shared task.	0
3909	38136	38136	K15-2001	abstract	7	8	4.0	1.0	We also summarize the different approaches adopted by the participating teams, and present the evaluation results.	0
3910	38137	38137	K15-2001	abstract	8	9	4.0	1.0	The evaluation data sets and the scorer will serve as a benchmark for future research on shallow discourse parsing.	0
3911	38138	38138	K15-2001	Introduction	1	10	1.0	1.0	The shared task for the Nineteenth Conference on Computational Natural Language Learning (CoNLL-2015) is on Shallow Discourse Parsing (SDP).	0
3912	38139	38139	K15-2001	Introduction	2	11	1.0	1.0	In the course of the sixteen CoNLL shared tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011;Pradhan et al., 2012).	0
3913	38140	38140	K15-2001	Introduction	3	12	1.0	1.0	The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012).	0
3914	38141	38141	K15-2001	Introduction	4	13	1.0	1.0	Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text.	0
3915	38142	38142	K15-2001	Introduction	5	14	1.0	1.0	Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012;	0
3916	38143	38143	K15-2001	Introduction	6	15	1.0	1.0	Webber et al., 2012;Prasad and Bunt, 2015).	0
3917	38144	38144	K15-2001	Introduction	7	16	1.0	1.0	For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008;	0
3918	38145	38145	K15-2001	Introduction	8	17	1.0	1.0	Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them.	0
3919	38146	38146	K15-2001	Introduction	9	18	1.0	1.0	For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.	0
3920	38147	38147	K15-2001	Introduction	10	19	2.0	1.0	1	0
3921	38148	38148	K15-2001	Introduction	11	20	2.0	1.0	The necessary conditions are also in place for such a task.	0
3922	38149	38149	K15-2001	Introduction	12	21	2.0	1.0	The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008;	0
3923	38150	38150	K15-2001	Introduction	13	22	2.0	1.0	Duverle and Prendinger, 2009;	0
3924	38151	38151	K15-2001	Introduction	14	23	2.0	1.0	Lin et al., 2009;	0
3925	38152	38152	K15-2001	Introduction	15	24	2.0	1.0	Pitler et al., 2009;	0
3926	38153	38153	K15-2001	Introduction	16	25	2.0	1.0	Subba and Di Eugenio, 2009;	0
3927	38154	38154	K15-2001	Introduction	17	26	2.0	1.0	Zhou et al., 2010;	0
3928	38155	38155	K15-2001	Introduction	18	27	2.0	1.0	Feng and Hirst, 2012;	0
3929	38156	38156	K15-2001	Introduction	19	28	3.0	1.0	Ghosh et al., 2012;	0
3930	38157	38157	K15-2001	Introduction	20	29	3.0	1.0	Park and Cardie, 2012;	0
3931	38158	38158	K15-2001	Introduction	21	30	3.0	1.0	Wang et al., 2012;Biran and McKeown, 2013;	0
3932	38159	38159	K15-2001	Introduction	22	31	3.0	1.0	Lan et al., 2013;	0
3933	38160	38160	K15-2001	Introduction	23	32	3.0	1.0	Feng and Hirst, 2014;	0
3934	38161	38161	K15-2001	Introduction	24	33	3.0	1.0	Ji and Eisenstein, 2014;	0
3935	38162	38162	K15-2001	Introduction	25	34	3.0	1.0	Li and Nenkova, 2014;	0
3936	38163	38163	K15-2001	Introduction	26	35	3.0	1.0	Lin et al., 2014;	0
3937	38164	38164	K15-2001	Introduction	27	36	3.0	1.0	Rutherford and Xue, 2014), and the momentum is building.	0
3938	38165	38165	K15-2001	Introduction	28	37	4.0	1.0	Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference.	0
3939	38166	38166	K15-2001	Introduction	29	38	4.0	1.0	The resurgence of deep learning techniques opens the door for innovative approaches to this problem.	0
3940	38167	38167	K15-2001	Introduction	30	39	4.0	1.0	"A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of ""standard"" feature-based learning techniques and ""deep"" representation learning techniques."	0
3941	38168	38168	K15-2001	Introduction	31	40	4.0	1.0	The rest of this overview paper is structured as follows.	0
3942	38169	38169	K15-2001	Introduction	32	41	4.0	1.0	In Section 2, we provide a concise definition of the shared task.	0
3943	38170	38170	K15-2001	Introduction	33	42	4.0	1.0	We describe how the training and test data are prepared in Section 3.	0
3944	38171	38171	K15-2001	Introduction	34	43	4.0	1.0	In Section 4, we present the evaluation protocol, metric and scorer.	0
3945	38172	38172	K15-2001	Introduction	35	44	4.0	1.0	The different approaches that participants took in the shared task are summarized in Section 5.	0
3946	38173	38173	K15-2001	Introduction	36	45	4.0	1.0	In Section 6, we present the ranking of participating systems and analyze the evaluation results.	0
3947	38174	38174	K15-2001	Introduction	37	46	4.0	1.0	We present our conclusions in Section 7.	0
3948	38175	38175	K15-2001	Task Definition	1	47	1.0	1.0	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
3949	38176	38176	K15-2001	Task Definition	2	48	1.0	1.0	Specifically, given a newswire article as input, a participating system is asked to return a set of discourse relations contained in the text.	1
3950	38177	38177	K15-2001	Task Definition	3	49	2.0	1.0	A discourse relation, as defined in the PDTB, from which the training data for the shared task is drawn, is a relation taking two abstract objects (events, states, facts, or propositions) as arguments.	1
3951	38178	38178	K15-2001	Task Definition	4	50	2.0	1.0	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between abstract object units.	0
3952	38179	38179	K15-2001	Task Definition	5	51	2.0	1.0	In the current version of the PDTB, non-explicit relations are inferred only between adjacent units.	0
3953	38180	38180	K15-2001	Task Definition	6	52	3.0	1.0	Each discourse relation is labeled with a sense selected from a sense hierarchy, and its arguments are generally in the form of sentences, clauses, or in some rare cases, noun phrases.	0
3954	38181	38181	K15-2001	Task Definition	7	53	3.0	1.0	To detect a discourse relation, a participating system needs to:	0
3955	38182	38182	K15-2001	Task Definition	8	54	4.0	1.0	1. Identify the text span of an explicit discourse connective, if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments;	0
3956	38183	38183	K15-2001	Task Definition	9	55	4.0	1.0	"4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
3957	38184	38184	K15-2001	Task Definition	10	56	4.0	1.0	3 Data	0
3958	38185	38185	K15-2001	Training and Development	1	57	1.0	1.0	The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse Tree-Bank 2.0. (PDTB-2.0.)	0
3959	38186	38186	K15-2001	Training and Development	2	58	1.0	1.0	(Prasad et al., 2008;	0
3960	38187	38187	K15-2001	Training and Development	3	59	1.0	1.0	Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005).	0
3961	38188	38188	K15-2001	Training and Development	4	60	1.0	1.0	The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text.	0
3962	38189	38189	K15-2001	Training and Development	5	61	1.0	1.0	Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two.	0
3963	38190	38190	K15-2001	Training and Development	6	62	1.0	1.0	A limited but well-defined set of implicit relations are also annotated within sentences.	0
3964	38191	38191	K15-2001	Training and Development	7	63	2.0	1.0	Arguments of relations are annotated in each case, following the minimality principle for selecting all and only the material needed to interpret the relation.	0
3965	38192	38192	K15-2001	Training and Development	8	64	2.0	1.0	For explicit connectives, Arg2, which is defined as the argument with which the connective is syntactically associated, is in the same sentence as the connective (though not necessarily string adjacent), but Arg1, defined simply as the other argument, is unconstrained in terms of its distance from the connective and can be found anywhere in the text (Exs. 1-3).	0
3966	38193	38193	K15-2001	Training and Development	9	65	2.0	1.0	(All the following PDTB examples shown highlight Arg1 (in italics), Arg2 (in boldface), expressions realizing the relation (underlined), sense (in parentheses), and the WSJ file number for the text with the example (in square brackets)).	0
3967	38194	38194	K15-2001	Training and Development	10	66	2.0	1.0	( Between adjacent sentences unrelated by any explicit connective, four scenarios hold: (a) the sentences may be related by a discourse relation that has no lexical realization, in which case a connective (called an Implicit connective) is inserted to express the inferred relation (Ex. 4), (b) the sentences may be related by a discourse relation that is realized by some alternative non-connective expression (called AltLex), in which case these alternative lexicalizations are annotated as the carriers of the relation (Ex. 5), (c) the sentences may be related not by a discourse relation realizable by a connective or AltLex, but by an entity-based coherence relation, in which case the presence of such a relation is labeled EntRel (Ex 6), and (d) the sentences may not be related at all, in which case they are labeled NoRel.	0
3968	38195	38195	K15-2001	Training and Development	11	67	2.0	1.0	Relations annotated in these four scenarios are collectively referred to as Non-Explicit relations in this paper.	0
3969	38196	38196	K15-2001	Training and Development	12	68	2.0	1.0	(5)	0
3970	38197	38197	K15-2001	Training and Development	13	69	3.0	1.0	Now, GM appears to be stepping up the pace of its factory consolidation to get in shape for the 1990s.	0
3971	38198	38198	K15-2001	Training and Development	14	70	3.0	1.0	In addition to the argument structure of relations, the PDTB provides sense annotation for each discourse relation, capturing the polysemy of connectives.	0
3972	38199	38199	K15-2001	Training and Development	15	71	3.0	1.0	Senses are organized in a three-level hierarchy, with 4 top-level semantic classes.	0
3973	38200	38200	K15-2001	Training and Development	16	72	3.0	1.0	For each class, a second level of types is defined, and there are 16 such types.	0
3974	38201	38201	K15-2001	Training and Development	17	73	3.0	1.0	There is a third level of subtype which provides further refinement to the second level types.	0
3975	38202	38202	K15-2001	Training and Development	18	74	3.0	1.0	In the PDTB annotation, annotators are allowed back off to a higher level in the sense hierarchy if they are not certain about a lower level sense.	0
3976	38203	38203	K15-2001	Training and Development	19	75	4.0	1.0	That is, if they cannot distinguish between the subtypes under a type sense, they can just annotate the type level sense, and if there is further uncertainty in choosing among the types under a class sense, they can just annotate the class level sense.	0
3977	38204	38204	K15-2001	Training and Development	20	76	4.0	2.0	Most of the discourse relation instances in the PDTB are annotated with at least a type level sense, but there are also a small number annotated with only a class level sense.	0
3978	38205	38205	K15-2001	Training and Development	21	77	4.0	2.0	The PDTB also provides annotations of attribution over all discourse relations and each of their arguments, as well as of text spans considered as supplementary to arguments of relations.	0
3979	38206	38206	K15-2001	Training and Development	22	78	4.0	2.0	However, both of these annotation types are excluded from the shared task.	0
3980	38207	38207	K15-2001	Training and Development	23	79	4.0	2.0	PDTB-2.0. contains annotations of 40,600 discourse relations, distributed into the following five types: 18,459 Explicit relations, 16,053 Implicit relations, 624 AltLex relations, 5,210 EntRel relations, and 254 NoRel relations.	0
3981	38208	38208	K15-2001	Training and Development	24	80	4.0	2.0	We provide Sections 2-21 of the PDTB 2.0 release as the training set, and Section 22 as the development set.	0
3982	38209	38209	K15-2001	Test Data	1	81	2.0	2.0	We provide two test sets for the shared task: Section 23 of the PDTB, and a blind test set we prepared especially for the shared task.	0
3983	38210	38210	K15-2001	Test Data	2	82	3.0	2.0	The official ranking of the systems is based on their performance on the blind test set.	0
3984	38211	38211	K15-2001	Test Data	3	83	4.0	2.0	In this section, we provide a detailed description of how the blind test set was prepared.	0
3985	38212	38212	K15-2001	Data Selection and Post-processing	1	84	1.0	2.0	For the blind test data, 30,158 words of untokenized English newswire texts were selected from a dump of English Wikinews 2 , accessed 22nd October 2014, and annotated in accordance with PDTB 2.0 guidelines.	0
3986	38213	38213	K15-2001	Data Selection and Post-processing	2	85	1.0	2.0	The raw Wikinews data was pre-processed as follows:	0
3987	38214	38214	K15-2001	Data Selection and Post-processing	3	86	1.0	2.0	•	0
3988	38215	38215	K15-2001	Data Selection and Post-processing	4	87	1.0	2.0	News articles were extracted from the Wikinews XML dump 3 using the publicly available WikiExtractor.	0
3989	38216	38216	K15-2001	Data Selection and Post-processing	5	88	2.0	2.0	py script.	0
3990	38217	38217	K15-2001	Data Selection and Post-processing	6	89	2.0	2.0	4	0
3991	38218	38218	K15-2001	Data Selection and Post-processing	7	90	2.0	2.0	• Additional processing was done to remove any remaining XML information and produce a raw text version of each article (including its title).	0
3992	38219	38219	K15-2001	Data Selection and Post-processing	8	91	2.0	2.0	•	0
3993	38220	38220	K15-2001	Data Selection and Post-processing	9	92	3.0	2.0	All paragraphs were double spaced to ease paragraph boundary identification.	0
3994	38221	38221	K15-2001	Data Selection and Post-processing	10	93	3.0	2.0	•	0
3995	38222	38222	K15-2001	Data Selection and Post-processing	11	94	3.0	2.0	Each article was named according to its unique Wikinews ID such that it is accessible online at http://en.wikinews.org/ wiki?curid=ID.	0
3996	38223	38223	K15-2001	Data Selection and Post-processing	12	95	3.0	2.0	Initially, 30k words of text were selected from this processed data at random.	0
3997	38224	38224	K15-2001	Data Selection and Post-processing	13	96	4.0	2.0	However, it soon became apparent that some texts were too short for PDTB-style annotation or otherwise still contained remnant XML errors.	0
3998	38225	38225	K15-2001	Data Selection and Post-processing	14	97	4.0	2.0	Another issue was that since Wikinews texts are written by members of the public, rather than professionally trained journalists, some articles were considered as not up to the same standards of spelling and grammar as the WSJ texts in the PDTB.	0
3999	38226	38226	K15-2001	Data Selection and Post-processing	15	98	4.0	2.0	For these reasons, despite making the decision to allow the correction of extremely minor errors (such as obvious typos and occasional article or preposition errors), just under half of the original 30k word random selection was ultimately deemed unsuitable for annotation.	0
4000	38227	38227	K15-2001	Data Selection and Post-processing	16	99	4.0	2.0	Consequently, the remaining texts were selected manually from Wikinews, with a slight preference for longer articles with many multi-sentence paragraphs that are more consistent with WSJ-style texts.	0
4001	38228	38228	K15-2001	Annotations	1	100	1.0	2.0	Annotation of the blind test set was carried out by two of the shared task organizers, one of whom (fifth author) was the main annotator (MA) while the other (fourth author), a lead developer of the PDTB, acted as the reviewing annotator (RA), reviewing each relation annotated by the MA and recording agreement or disagreement.	0
4002	38229	38229	K15-2001	Annotations	2	101	1.0	2.0	Annotation involved marking the relation type (Explicit, Implicit, AltLex, EntRel, NoRel), relation realization (explicit connective, implicit connective, Al-tLex expression), arguments (Arg1 and Arg2), and sense of a discourse relation, using the PDTB annotation tool.	0
4003	38230	38230	K15-2001	Annotations	3	102	1.0	2.0	5 Unlike the PDTB guidelines, we did not allow back-off to the top class level during annotation.	0
4004	38231	38231	K15-2001	Annotations	4	103	2.0	2.0	Every relation was annotated with a sense chosen from at least the second type level.	0
4005	38232	38232	K15-2001	Annotations	5	104	2.0	2.0	5 https://www.seas.upenn.edu/˜pdtb/tools.shtml# annotator	0
4006	38233	38233	K15-2001	Annotations	6	105	2.0	2.0	Also different from the PDTB, attribution spans or attribution features were not annotated.	0
4007	38234	38234	K15-2001	Annotations	7	106	2.0	2.0	Before commencing official annotation, MA was trained in PDTB-2.0. style annotation by RA.	0
4008	38235	38235	K15-2001	Annotations	8	107	3.0	2.0	A review of the guidelines was followed by double blind annotation (by MA and RA) of a small number of WSJ texts not previously annotated in the PDTB, and differences were then compared and discussed.	0
4009	38236	38236	K15-2001	Annotations	9	108	3.0	2.0	MA then also underwent self-training by first annotating some WSJ texts that were already annotated in the PDTB, and then comparing these annotations, to further strengthen knowledge of the guidelines.	0
4010	38237	38237	K15-2001	Annotations	10	109	3.0	2.0	After the training period, the entire blind test data was annotated by MA over a period of a few weeks, and then reviewed by RA.	0
4011	38238	38238	K15-2001	Annotations	11	110	3.0	2.0	Disagreements during the review were manually recorded using a formal scheme addressing all aspects of the annotation, including relation type, explicit connective identification, senses, and each of the arguments.	0
4012	38239	38239	K15-2001	Annotations	12	111	4.0	2.0	This was done to verify the integrity of the blind test data and keep a record of any confusion or difficulty encountered during annotation.	0
4013	38240	38240	K15-2001	Annotations	13	112	4.0	2.0	Manual entry of disagreements was done within the tool interface, through its commenting feature.	0
4014	38241	38241	K15-2001	Annotations	14	113	4.0	2.0	A recorded comment in the tool is unique to a relation token and is recorded in a stand-off style.	0
4015	38242	38242	K15-2001	Annotations	15	114	4.0	2.0	Disagreements were later resolved by consensus between MA and RA.	0
4016	38243	38243	K15-2001	Inter-annotator Agreement	1	115	1.0	2.0	The record of disagreements was utilized to compute inter-annotator agreement between MA and RA.	0
4017	38244	38244	K15-2001	Inter-annotator Agreement	2	116	1.0	2.0	The overall agreement was 76.5%, which represents the percentage of relations on which there was complete agreement.	0
4018	38245	38245	K15-2001	Inter-annotator Agreement	3	117	1.0	2.0	Agreement on explicit connective identification was 96.0%, representing the percentage of explicit connectives that both MA and RA identified as discourse connectives.	0
4019	38246	38246	K15-2001	Inter-annotator Agreement	4	118	1.0	2.0	"We note here that if a connective was identified in the blind test data, but was not annotated in the PDTB despite its occurrence in the WSJ (e.g.,""after which time"", ""despite""), we did not consider it a potential connective and hence did not include it in the agreement calculation."	0
4020	38247	38247	K15-2001	Inter-annotator Agreement	5	119	1.0	2.0	When the textual context allowed it, such expressions were instead marked as AltLex.	0
4021	38248	38248	K15-2001	Inter-annotator Agreement	6	120	2.0	2.0	We also did a more fine-grained assessment to determine agreement on Arg1, Arg2, Arg1+Arg2 (i.e., the number of relations on which the annotators agreed on both Arg1 and Arg2), and senses.	0
4022	38249	38249	K15-2001	Inter-annotator Agreement	7	121	2.0	2.0	This was done for all the relation types considered together, as well as for Explicit and Non-Explicit relation types separately.	0
4023	38250	38250	K15-2001	Inter-annotator Agreement	8	122	2.0	2.0	Sense disagreement was computed using the CoNLL sense classification scheme (see Section 3.3), even though the annotation was done using the full PDTB sense classification scheme (see Table 2).	0
4024	38251	38251	K15-2001	Inter-annotator Agreement	9	123	2.0	2.0	The agreement percentages are shown in Table 1.	0
4025	38252	38252	K15-2001	Inter-annotator Agreement	10	124	2.0	2.0	When multiple senses were provided for a relation, a disagreement on any of the senses was counted as disagreement for the relation; disagreement on more than one of the senses was counted only once.	0
4026	38253	38253	K15-2001	Inter-annotator Agreement	11	125	3.0	2.0	Absence of a second sense by one annotator when the other did provide one was also counted as disagreement.	0
4027	38254	38254	K15-2001	Inter-annotator Agreement	12	126	3.0	2.0	As the table shows, agreement on senses was reasonably high overall (85.5%), with agreement for Explicit relations expectedly higher (91.0%) than for Non-Explicit relations (80.9%).	0
4028	38255	38255	K15-2001	Inter-annotator Agreement	13	127	3.0	2.0	Overall agreement on arguments was also high, but in contrast to the senses, agreement was generally higher for the Non-Explicit than for Explicit relations.	0
4029	38256	38256	K15-2001	Inter-annotator Agreement	14	128	3.0	2.0	Agreement on the Arg1 of Explicit relations (89.6%) is, not surprisingly, lower than for Arg2 (98.7%), because the Arg1 of Explicit relations can be non-adjacent to the connective's sentence or clause, and thus, harder to identify.	0
4030	38257	38257	K15-2001	Inter-annotator Agreement	15	129	3.0	2.0	For the Non-Explicit relations, in contrast, but again to be expected, because of the argument adjacency constraint for such relations, agreement on Arg1 (95.0%) and Arg2 (96.4%) shows minimal difference.	0
4031	38258	38258	K15-2001	Inter-annotator Agreement	16	130	4.0	2.0	Table 1 also provides the percentage of relations with agreement on both Arg1 and Arg2, showing this to be higher for Non-Explicit relations (92.4%) than for Explicit relations (88.7%).	0
4032	38259	38259	K15-2001	Inter-annotator Agreement	17	131	4.0	2.0	Compared to the agreement reported for the PDTB (Prasad et al., 2008;	0
4033	38260	38260	K15-2001	Inter-annotator Agreement	18	132	4.0	2.0	Miltsakaki et al., 2004), the results obtained here (See Table 1) are slightly better.	0
4034	38261	38261	K15-2001	Inter-annotator Agreement	19	133	4.0	2.0	PDTB agreement on Arg1 and Arg2 of Explicit relations is reported to be 86.3% and 94.1%, respectively, whereas overall agreement on arguments of Non-Explicit relations is 85.1%.	0
4035	38262	38262	K15-2001	Inter-annotator Agreement	20	134	4.0	2.0	For the senses, although the CoNLL senses do not exactly align with the PDTB senses, a rough correspondence can be assumed between the CoNLL classification as a whole and the type and subtype levels of the PDTB classification, for which PDTB reports 84% and 80%, respectively.	0
4036	38263	38263	K15-2001	Adapting the PDTB Annotation for the shared task	1	135	1.0	2.0	The discourse relations annotated in the PDTB have many different elements, and it is impracti-cal to predict all of them in the context of a shared task where participants have a relatively short time frame in which to complete the task.	0
4037	38264	38264	K15-2001	Adapting the PDTB Annotation for the shared task	2	136	1.0	2.0	As a result, we had to make a number of exclusions and simplifications, which we describe below.	0
4038	38265	38265	K15-2001	Adapting the PDTB Annotation for the shared task	3	137	1.0	2.0	The core elements of a discourse relation are the two abstract objects as its arguments.	0
4039	38266	38266	K15-2001	Adapting the PDTB Annotation for the shared task	4	138	1.0	2.0	In addition to this, some discourse relations include supplementary information that is relevant but not necessary (as per the minimality principle) to the interpretation of a discourse relation.	0
4040	38267	38267	K15-2001	Adapting the PDTB Annotation for the shared task	5	139	1.0	2.0	"Supplementary information is associated with arguments, and optionally marked with the labels ""Sup1"", for material supplementary to Arg1, and ""Sup2"", for material supplementary to Arg2."	0
4041	38268	38268	K15-2001	Adapting the PDTB Annotation for the shared task	6	140	1.0	2.0	An example of a Sup1 annotation is shown in (7).	0
4042	38269	38269	K15-2001	Adapting the PDTB Annotation for the shared task	7	141	1.0	2.0	In the shared task, supplementary information is excluded from evaluation when computing argument spans.	0
4043	38270	38270	K15-2001	Adapting the PDTB Annotation for the shared task	8	142	1.0	2.0	Also excluded from evaluation, to make the shared task manageable, are attribution relations annotated in PDTB.	0
4044	38271	38271	K15-2001	Adapting the PDTB Annotation for the shared task	9	143	1.0	2.0	"An example of an explicit attribution is ""he says"" in (8), marked over Arg1."	0
4045	38272	38272	K15-2001	Adapting the PDTB Annotation for the shared task	10	144	1.0	2.0	The PDTB senses form a hierarchical system of three levels, consisting of 4 classes, 16 types, and 23 subtypes.	0
4046	38273	38273	K15-2001	Adapting the PDTB Annotation for the shared task	11	145	1.0	2.0	While all classes are divided into multiple types, some types do not have subtypes.	0
4047	38274	38274	K15-2001	Adapting the PDTB Annotation for the shared task	12	146	1.0	2.0	Previous work on PDTB sense classification has mostly focused on classes (Pitler et al., 2009;	0
4048	38275	38275	K15-2001	Adapting the PDTB Annotation for the shared task	13	147	2.0	2.0	Zhou et al., 2010;	0
4049	38276	38276	K15-2001	Adapting the PDTB Annotation for the shared task	14	148	2.0	2.0	Park and Cardie, 2012;	0
4050	38277	38277	K15-2001	Adapting the PDTB Annotation for the shared task	15	149	2.0	2.0	Biran and McKeown, 2013;	0
4051	38278	38278	K15-2001	Adapting the PDTB Annotation for the shared task	16	150	2.0	2.0	Li and Nenkova, 2014;	0
4052	38279	38279	K15-2001	Adapting the PDTB Annotation for the shared task	17	151	2.0	3.0	Rutherford and Xue, 2014).	0
4053	38280	38280	K15-2001	Adapting the PDTB Annotation for the shared task	18	152	2.0	3.0	The senses that are the target of prediction in the CoNLL-2015 shared task are primarily based on the second-level types and a selected number of third-level subtypes.	0
4054	38281	38281	K15-2001	Adapting the PDTB Annotation for the shared task	19	153	2.0	3.0	We made a few modifications to make the distinctions clearer and their distributions more balanced, and these changes are presented in Table 2.	0
4055	38282	38282	K15-2001	Adapting the PDTB Annotation for the shared task	20	154	2.0	3.0	First, senses in the PDTB that have distinctions that are too subtle and thus too difficult to predict are collapsed.	0
4056	38283	38283	K15-2001	Adapting the PDTB Annotation for the shared task	21	155	2.0	3.0	Senses that involve a change from the PDTB senses are marked * .	0
4057	38284	38284	K15-2001	Adapting the PDTB Annotation for the shared task	22	156	2.0	3.0	"For example, ""Contingency."	0
4058	38285	38285	K15-2001	Adapting the PDTB Annotation for the shared task	23	157	2.0	3.0	"Pragmatic cause"" is merged into ""Contingency."	0
4059	38286	38286	K15-2001	Adapting the PDTB Annotation for the shared task	24	158	2.0	3.0	Cause.	0
4060	38287	38287	K15-2001	Adapting the PDTB Annotation for the shared task	25	159	3.0	3.0	"Reason"", and ""Contingency."	0
4061	38288	38288	K15-2001	Adapting the PDTB Annotation for the shared task	26	160	3.0	3.0	"Pragmatic condition"" is merged into ""Contingency."	0
4062	38289	38289	K15-2001	Adapting the PDTB Annotation for the shared task	27	161	3.0	3.0	"Condition""."	0
4063	38290	38290	K15-2001	Adapting the PDTB Annotation for the shared task	28	162	3.0	3.0	"Second, the distinction between ""Expansion."	0
4064	38291	38291	K15-2001	Adapting the PDTB Annotation for the shared task	29	163	3.0	3.0	"Conjunction"" and ""Expansion."	0
4065	38292	38292	K15-2001	Adapting the PDTB Annotation for the shared task	30	164	3.0	3.0	"List"" is not clear in the PDTB and in fact, they seem very similar for the most part, so the latter is merged into the former."	0
4066	38293	38293	K15-2001	Adapting the PDTB Annotation for the shared task	31	165	3.0	3.0	"Third, while ""Expansion."	0
4067	38294	38294	K15-2001	Adapting the PDTB Annotation for the shared task	32	166	3.0	3.0	Alternative.	0
4068	38295	38295	K15-2001	Adapting the PDTB Annotation for the shared task	33	167	3.0	3.0	"Conjunctive"" and ""Expansion."	0
4069	38296	38296	K15-2001	Adapting the PDTB Annotation for the shared task	34	168	3.0	3.0	Alternative.	0
4070	38297	38297	K15-2001	Adapting the PDTB Annotation for the shared task	35	169	3.0	3.0	"Disjunctive"" are merged into ""Expansion."	0
4071	38298	38298	K15-2001	Adapting the PDTB Annotation for the shared task	36	170	3.0	3.0	"Alternative"", a third subtype of ""Expansion."	0
4072	38299	38299	K15-2001	Adapting the PDTB Annotation for the shared task	37	171	4.0	3.0	"Alternative"", ""Expansion."	0
4073	38300	38300	K15-2001	Adapting the PDTB Annotation for the shared task	38	172	4.0	3.0	Alternative.	0
4074	38301	38301	K15-2001	Adapting the PDTB Annotation for the shared task	39	173	4.0	3.0	"Chosen Alternative"" is kept as a separate category as its meaning involves more than presentation of alternatives."	0
4075	38302	38302	K15-2001	Adapting the PDTB Annotation for the shared task	40	174	4.0	3.0	"Finally, while ""EntRel"" relations are not treated as discourse relations in the PDTB, we have included this category as a sense for sense classification since they are a kind of coherence relation and we require systems to label these relations in the shared task."	0
4076	38303	38303	K15-2001	Adapting the PDTB Annotation for the shared task	41	175	4.0	3.0	"In contrast, instances annotated with ""NoRel"" are not treated as discourse relations and are excluded from the training, development and test data sets."	0
4077	38304	38304	K15-2001	Adapting the PDTB Annotation for the shared task	42	176	4.0	3.0	This means that a system needs to treat them as negative samples and not identify them as discourse relations.	0
4078	38305	38305	K15-2001	Adapting the PDTB Annotation for the shared task	43	177	4.0	3.0	These changes have resulted in a flat list of 15 sense categories that need to be predicted in the shared task.	0
4079	38306	38306	K15-2001	Adapting the PDTB Annotation for the shared task	44	178	4.0	3.0	A comparison of the PDTB senses and the senses used in the CoNLL shared task is presented in Table 2.	0
4080	38307	38307	K15-2001	Adapting the PDTB Annotation for the shared task	45	179	4.0	3.0	Table 3: Distribution of senses across the four relation types in the WSJ PDTB data used for the shared task.	0
4081	38308	38308	K15-2001	Adapting the PDTB Annotation for the shared task	46	180	4.0	3.0	The total numbers of the relations here are less than in the complete PDTB release because some sections (00, 01, and 24) are excluded for the shared task, following standard split of WSJ data in the evaluation community.	0
4082	38309	38309	K15-2001	Adapting the PDTB Annotation for the shared task	47	181	4.0	3.0	We are intentionally withholding distribution over the blind test set in case there is a repeat of the SDP shared task using the same test set.	0
4083	38310	38310	K15-2001	Adapting the PDTB Annotation for the shared task	48	182	4.0	3.0	Table 3 shows the distribution of the senses across the four discourse relations within the WSJ PDTB data 6 .	0
4084	38311	38311	K15-2001	Adapting the PDTB Annotation for the shared task	49	183	4.0	3.0	We are intentionally withholding the sense distribution across the blind test set in case there is a repeat of the SDP shared task using the same test set.	0
4085	38312	38312	K15-2001	Evaluation	1	184	1.0	3.0	Closed and open tracks	0
4086	38313	38313	K15-2001	Evaluation	2	185	1.0	3.0	In keeping with the CoNLL shared task tradition, participating systems were evaluated in two tracks, a closed track and an open track.	0
4087	38314	38314	K15-2001	Evaluation	3	186	1.0	3.0	A participating system in the closed track could only use the provided PDTB training set but was allowed to process the data using any publicly available (i.e., non-proprietary) natural language processing tools such as syntactic parsers and semantic role labelers.	0
4088	38315	38315	K15-2001	Evaluation	4	187	1.0	3.0	In contrast, in the open track, a participating system could not only use any publicly available NLP tools to process the data, but also any publicly available (i.e., non-proprietary) data for training.	0
4089	38316	38316	K15-2001	Evaluation	5	188	2.0	3.0	A participating team could choose to participate in the closed track or the open track, or both.	0
4090	38317	38317	K15-2001	Evaluation	6	189	2.0	3.0	The motivation for having two tracks in CoNLL shared tasks was to isolate the contribution of algorithms and resources to a particular task.	0
4091	38318	38318	K15-2001	Evaluation	7	190	2.0	3.0	In the closed track, the resources are held constant so that the advantages of different algorithms and models can be more meaningfully compared.	0
4092	38319	38319	K15-2001	Evaluation	8	191	2.0	3.0	In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task.	0
4093	38320	38320	K15-2001	Evaluation	9	192	2.0	3.0	This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set.	0
4094	38321	38321	K15-2001	Evaluation	10	193	3.0	3.0	However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014).	0
4095	38322	38322	K15-2001	Evaluation	11	194	3.0	3.0	In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set.	0
4096	38323	38323	K15-2001	Evaluation	12	195	3.0	3.0	As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the train- 6 There is a small number of instances in the PDTB training set that are only annotated with the class level sense.	0
4097	38324	38324	K15-2001	Evaluation	13	196	3.0	3.0	We did not take them out of the training set for the sake of completeness.	0
4098	38325	38325	K15-2001	Evaluation	14	197	4.0	3.0	ing set:	0
4099	38326	38326	K15-2001	Evaluation	15	198	4.0	3.0	To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation processed with state-of-the-art NLP tools:	0
4100	38327	38327	K15-2001	Evaluation	16	199	4.0	3.0	•	0
4101	38328	38328	K15-2001	Evaluation	17	200	4.0	3.0	Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014))	0
4102	38329	38329	K15-2001	Evaluation	18	201	4.0	3.0	As it turned out, all of the teams this year chose to participate in the closed track.	0
4103	38330	38330	K15-2001	Evaluation Platform: TIRA	1	202	1.0	3.0	We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012;	0
4104	38331	38331	K15-2001	Evaluation Platform: TIRA	2	203	1.0	3.0	Potthast et al., 2014).	0
4105	38332	38332	K15-2001	Evaluation Platform: TIRA	3	204	1.0	3.0	Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation.	0
4106	38333	38333	K15-2001	Evaluation Platform: TIRA	4	205	2.0	3.0	This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their systems on the test sets without actually seeing the test sets.	0
4107	38334	38334	K15-2001	Evaluation Platform: TIRA	5	206	2.0	3.0	The organizers would then inspect the evaluation results, and verify that participating systems yielded acceptable output.	0
4108	38335	38335	K15-2001	Evaluation Platform: TIRA	6	207	2.0	3.0	This evaluation protocol allowed us to maintain the integrity of the blind test set and reduce the organizational overhead.	0
4109	38336	38336	K15-2001	Evaluation Platform: TIRA	7	208	2.0	3.0	On TIRA, the blind test set can only be accessed in the evaluation environment, and the evaluation results are automatically collected.	0
4110	38337	38337	K15-2001	Evaluation Platform: TIRA	8	209	3.0	3.0	Participants cannot see any part of the test sets and hence cannot do iterative development based on the test set performance, which preserves the integrity of the evaluation.	0
4111	38338	38338	K15-2001	Evaluation Platform: TIRA	9	210	3.0	3.0	Most importantly, this evaluation platform promotes replicability, which is very crucial for proper evaluation of scientific progress.	0
4112	38339	38339	K15-2001	Evaluation Platform: TIRA	10	211	3.0	3.0	Reproducing all of the results is just a matter of a button click on TIRA.	0
4113	38340	38340	K15-2001	Evaluation Platform: TIRA	11	212	3.0	3.0	All of the results presented in this paper, along with the trained models and the software, are archived and available for distribution upon request to the organizers and upon the permission of the participating team, who holds the copyrights to the software.	0
4114	38341	38341	K15-2001	Evaluation Platform: TIRA	12	213	4.0	3.0	Replicability also helps speed up the research and development in discourse parsing.	0
4115	38342	38342	K15-2001	Evaluation Platform: TIRA	13	214	4.0	3.0	Anyone wanting to extend or apply any of the approaches proposed by a shared task participant does not have to re-implement the model from scratch.	0
4116	38343	38343	K15-2001	Evaluation Platform: TIRA	14	215	4.0	3.0	They can request a clone of the virtual machine where the participating system is deployed, and then implement their extension based off the original source code.	0
4117	38344	38344	K15-2001	Evaluation Platform: TIRA	15	216	4.0	3.0	Any extension effort also benefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation.	0
4118	38345	38345	K15-2001	Evaluation metrics and scorer	1	217	1.0	3.0	A shallow discourse parser is evaluated based on the end-to-end F 1 score on a per-discourse relation basis.	0
4119	38346	38346	K15-2001	Evaluation metrics and scorer	2	218	1.0	3.0	The input to the system consists of documents with gold-standard word tokens along with their automatic parses.	0
4120	38347	38347	K15-2001	Evaluation metrics and scorer	3	219	1.0	3.0	We do not pre-identify the discourse connectives or any other elements of the discourse annotation.	0
4121	38348	38348	K15-2001	Evaluation metrics and scorer	4	220	1.0	3.0	The shallow discourse parser must output a list of discourse relations that consist of the argument spans and their labels, explicit discourse connectives where applicable, and the senses.	0
4122	38349	38349	K15-2001	Evaluation metrics and scorer	5	221	1.0	3.0	The F 1 score is computed based on the number of predicted relations that match a gold standard relation exactly.	0
4123	38350	38350	K15-2001	Evaluation metrics and scorer	6	222	1.0	3.0	A relation is correctly predicted if (a) the discourse connective is correctly detected (for Explicit discourse relations), (b) the sense of the discourse connective is correctly predicted, and (c) the text spans of its two arguments are correctly predicted (Arg1 and Arg2).	0
4124	38351	38351	K15-2001	Evaluation metrics and scorer	7	223	2.0	3.0	Although the submissions are ranked based on the relation F 1 score, the scorer also provides component-wise evaluation with error propagation.	0
4125	38352	38352	K15-2001	Evaluation metrics and scorer	8	224	2.0	3.0	The scorer computes the precision, recall, and F 1 for the following 7 :	0
4126	38353	38353	K15-2001	Evaluation metrics and scorer	9	225	2.0	3.0	• Arg1 and Arg2 identification.	0
4127	38354	38354	K15-2001	Evaluation metrics and scorer	10	226	2.0	4.0	•	0
4128	38355	38355	K15-2001	Evaluation metrics and scorer	11	227	2.0	4.0	Sense classification with error propagation from discourse connective and argument identification.	0
4129	38356	38356	K15-2001	Evaluation metrics and scorer	12	228	2.0	4.0	For purposes of evaluation, an explicit discourse connective predicted by the parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the predicted connective to be a subset of the tokens in the gold raw connective.	0
4130	38357	38357	K15-2001	Evaluation metrics and scorer	13	229	2.0	4.0	We provide a function that maps discourse connectives to their corresponding heads.	0
4131	38358	38358	K15-2001	Evaluation metrics and scorer	14	230	3.0	4.0	The notion of discourse connective head is not the same as its syntactic head.	0
4132	38359	38359	K15-2001	Evaluation metrics and scorer	15	231	3.0	4.0	Rather, it is thought of as the part of the connective conveying its core meaning.	0
4133	38360	38360	K15-2001	Evaluation metrics and scorer	16	232	3.0	4.0	"For example, the head of the discourse connective ""At least not when"" is ""when"", and the head of ""five minutes before"" is ""before""."	0
4134	38361	38361	K15-2001	Evaluation metrics and scorer	17	233	3.0	4.0	The non-head part of the connective serves to semantically restrict the interpretation of the connective.	0
4135	38362	38362	K15-2001	Evaluation metrics and scorer	18	234	3.0	4.0	Although Implicit discourse relations are annotated with an implicit connective inserted between adjacent sentences, participants are not required to provide the inserted connective.	0
4136	38363	38363	K15-2001	Evaluation metrics and scorer	19	235	3.0	4.0	They only need to output the sense of the discourse relation.	0
4137	38364	38364	K15-2001	Evaluation metrics and scorer	20	236	3.0	4.0	Similarly, for AltLex relations, which are also annotated between adjacent sentences, participants are not required to output the text span of the AltLex expression, but only the sense.	0
4138	38365	38365	K15-2001	Evaluation metrics and scorer	21	237	4.0	4.0	The EntRel relation is included as a sense in the shared task, and here, systems are required to correctly label the EntRel relation between adjacent sentence pairs.	0
4139	38366	38366	K15-2001	Evaluation metrics and scorer	22	238	4.0	4.0	An argument is considered correctly identified if and only if it matches the corresponding gold standard argument span exactly, and is also correctly labeled (Arg1 or Arg2).	0
4140	38367	38367	K15-2001	Evaluation metrics and scorer	23	239	4.0	4.0	Systems are not given any credit for partial match on argument spans.	0
4141	38368	38368	K15-2001	Evaluation metrics and scorer	24	240	4.0	4.0	Sense classification evaluation is less straightforward, since senses are sometimes annotated partially or annotated with two senses.	0
4142	38369	38369	K15-2001	Evaluation metrics and scorer	25	241	4.0	4.0	To be considered correct, the predicted sense for a relation must match one of the two senses if there is more than one sense.	0
4143	38370	38370	K15-2001	Evaluation metrics and scorer	26	242	4.0	4.0	If the gold standard is partially annotated, the sense must match with the partially annotated sense.	0
4144	38371	38371	K15-2001	Evaluation metrics and scorer	27	243	4.0	4.0	Additionally, the scorer provides a breakdown of the discourse parser performance for Explicit and Non-Explicit discourse relations.	0
4145	38372	38372	K15-2001	Approaches	1	244	1.0	4.0	The Shallow Discourse Parsing (SDP) task this year requires the development of an end-to-end system that potentially involves many components.	0
4146	38373	38373	K15-2001	Approaches	2	245	1.0	4.0	All participating systems adopt some variation of the pipeline architecture proposed by Lin et al (2014)  ing discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations.	0
4147	38374	38374	K15-2001	Approaches	3	246	1.0	4.0	Most participating systems cast discourse connective identification and argument extraction as token-level sequence labeling tasks, while a few systems use rule-based approaches to extract the arguments.	0
4148	38375	38375	K15-2001	Approaches	4	247	1.0	4.0	Sense determination is cast as a straightforward multi-category classification task.	0
4149	38376	38376	K15-2001	Approaches	5	248	1.0	4.0	Most systems use machine learning techniques to determine the senses, but there are also systems that, due to lack of time, adopt a simple baseline approach that detects the most frequent sense based on the training data.	0
4150	38377	38377	K15-2001	Approaches	6	249	1.0	4.0	"In terms of learning techniques, all participating systems except the two systems submitted by the Dublin team use standard ""shallow"" learning models that take binary features as input."	0
4151	38378	38378	K15-2001	Approaches	7	250	2.0	4.0	For sequence labeling subtasks such as discourse connective identification and argument extraction, the preferred learning method is Conditional Random Fields (CRF).	0
4152	38379	38379	K15-2001	Approaches	8	251	2.0	4.0	For sense determination, a variety of learning methods have been used, including Maximum Entropy, Support Vector Machines, and decision trees.	0
4153	38380	38380	K15-2001	Approaches	9	252	2.0	4.0	In the last couple of years, neural networks have experienced a resurgence and have been shown to be effective in many natural language processing tasks.	0
4154	38381	38381	K15-2001	Approaches	10	253	2.0	4.0	Neural network based models on discourse parsing have also started to appear (Ji and Eisenstein, 2014).	0
4155	38382	38382	K15-2001	Approaches	11	254	2.0	4.0	"The use of neural networks for the SDP task this year represents a minority, presumably because researchers are still less familiar with neural network based techniques, compared with standard ""shallow"" learning techniques, and it is difficult to use a new learning technique to good effect within a short time window."	0
4156	38383	38383	K15-2001	Approaches	12	255	2.0	4.0	In this shared task, only the Dublin University team attempted to use neural networks as a learning approach in their system components.	0
4157	38384	38384	K15-2001	Approaches	13	256	2.0	4.0	In their first submission (Dublin I), Recurrent Neural Networks (RNN) are used for token level sequence labeling in the argument extraction task.	0
4158	38385	38385	K15-2001	Approaches	14	257	3.0	4.0	In their second submission, paragraph embeddings are used in a neural network model to determine the senses of discourse relations.	0
4159	38386	38386	K15-2001	Approaches	15	258	3.0	4.0	The discussion of learning techniques cannot be entirely separated from the use of features and the linguistic resources that are used to extract them.	0
4160	38387	38387	K15-2001	Approaches	16	259	3.0	4.0	"Standard ""shallow"" architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings."	0
4161	38388	38388	K15-2001	Approaches	17	260	3.0	4.0	For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses.	0
4162	38389	38389	K15-2001	Approaches	18	261	3.0	4.0	Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015;	0
4163	38390	38390	K15-2001	Approaches	19	262	3.0	4.0	Devi et al., 2015;	0
4164	38391	38391	K15-2001	Approaches	20	263	3.0	4.0	Kong et al., 2015;	0
4165	38392	38392	K15-2001	Approaches	21	264	4.0	4.0	Song et al., 2015;	0
4166	38393	38393	K15-2001	Approaches	22	265	4.0	4.0	Stepanov et al., 2015;Wang and Lan, 2015;	0
4167	38394	38394	K15-2001	Approaches	23	266	4.0	4.0	Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015).	0
4168	38395	38395	K15-2001	Approaches	24	267	4.0	4.0	Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015;, Verb-Net classes (Devi et al., 2015;	0
4169	38396	38396	K15-2001	Approaches	25	268	4.0	4.0	Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015;	0
4170	38397	38397	K15-2001	Approaches	26	269	4.0	4.0	Kong et al., 2015;Wang and Lan, 2015).	0
4171	38398	38398	K15-2001	Approaches	27	270	4.0	4.0	Table 4 provides a summary of the different approaches.	0
4172	38399	38399	K15-2001	Results	1	271	1.0	4.0	Table 5 shows the performance of all participating systems across the three test evaluation sets: i) (Official)	0
4173	38400	38400	K15-2001	Results	2	272	1.0	4.0	Blind test set; ii) Standard WSJ test set; iii) Standard WSJ development set.	0
4174	38401	38401	K15-2001	Results	3	273	1.0	4.0	The official rankings are based on the blind test set annotated specifically for this shared task.	0
4175	38402	38402	K15-2001	Results	4	274	1.0	4.0	The top-ranked system is the submission by East China Normal University (Wang and Lan, 2015).	0
4176	38403	38403	K15-2001	Results	5	275	1.0	4.0	As discussed in Section 4, the evaluation metric is very strict, and is based on exact match for the extraction of argument spans.	0
4177	38404	38404	K15-2001	Results	6	276	2.0	4.0	For the detection of discourse connectives, only the head of a discourse connective has to be correctly detected.	0
4178	38405	38405	K15-2001	Results	7	277	2.0	4.0	Errors in the begin-ning of the pipeline will propagate to the end, and other than word tokenization, all input to the participating systems is automatically generated, so the overall accuracy reflects results in realistic situations.	0
4179	38406	38406	K15-2001	Results	8	278	2.0	4.0	The scores are very low, with the top system achieving an overall parsing score of 24.00% (F1) on the blind test set and 29.69% (F1) on the Wall Street Journal (WSJ) test set.	0
4180	38407	38407	K15-2001	Results	9	279	2.0	4.0	For comparison purposes, the National University of Singapore team re-implemented the state-of-the-art endto-end parser described in (Lin et al., 2014), and this system achieves an F1 of 19.98% on the WSJ test set.	0
4181	38408	38408	K15-2001	Results	10	280	2.0	4.0	This shows that a fair amount of progress has been made against the Lin et al baseline.	0
4182	38409	38409	K15-2001	Results	11	281	3.0	4.0	The rankings are generally consistent across the two test sets, with the largest change in ranking from the NTT team and the Goethe University team.	0
4183	38410	38410	K15-2001	Results	12	282	3.0	4.0	This is perhaps not a coincidence: both teams used rule-based approaches to extract arguments.	0
4184	38411	38411	K15-2001	Results	13	283	3.0	4.0	The rules worked well on the WSJ test set which draws from the same source as the development set, but might not adapt well to the blind test set, which is drawn from a different source.	0
4185	38412	38412	K15-2001	Results	14	284	3.0	4.0	Machine-learning based approaches generally can better adapt to new data sets.	0
4186	38413	38413	K15-2001	Results	15	285	3.0	4.0	Due to the short time frame participants had to complete an end-to-end task, teams chose to focus on either argument extraction components or the sense classification components, or in the case of sense classification, either focus on the classification of senses for Explicit relations or senses for Non-Explicit relations.	0
4187	38414	38414	K15-2001	Results	16	286	4.0	4.0	A detailed breakdown of the performance for Explicit versus Non-Explicit discourse relations is presented in Table 6.	0
4188	38415	38415	K15-2001	Results	17	287	4.0	4.0	In general, parser performance for Explicit discourse relations is much higher than that of Non-Explicit discourse relations.	0
4189	38416	38416	K15-2001	Results	18	288	4.0	4.0	The difficulty for Non-Explicit discourse relations mostly stems from Non-Explicit sense classification.	0
4190	38417	38417	K15-2001	Results	19	289	4.0	4.0	This is evidenced by the fact that even for systems that achieve higher argument extraction accuracy for Non-Explicit discourse relations than Explicit discourse relations, the overall parser accuracy is still lower for Non-Explicit relations.	0
4191	38418	38418	K15-2001	Results	20	290	4.0	4.0	The lower accuracy in sense classification thus drags down the overall parser accuracy for Non-Explicit discourse relations.	0
4192	38419	38419	K15-2001	Conclusions	1	291	1.0	4.0	Sixteen teams from three continents participated in the CoNLL-2015 Shared Task on shallow dis-	0
4193	38420	38420	K15-2001	Conclusions	2	292	1.0	4.0	The rows are sorted by the parser performance of the participating systems on the Explicit task.	0
4194	38421	38421	K15-2001	Conclusions	3	293	2.0	4.0	The Column O, E, I refer to official, Explicit and Non-Explicit task ranks respectively.	0
4195	38422	38422	K15-2001	Conclusions	4	294	2.0	4.0	The blue highlighted rows indicate participants that did not attempt the Non-Explicit relation subtask.	0
4196	38423	38423	K15-2001	Conclusions	5	295	2.0	4.0	The green highlighted row shows a team that probably overfitted the development set.	0
4197	38424	38424	K15-2001	Conclusions	6	296	3.0	4.0	Finally, the red highlighted row indicates a team that possibly focused on the Explicit relations task and even though their overall rank was lower, they did very well on the Explicit relations subtask.	0
4198	38425	38425	K15-2001	Conclusions	7	297	3.0	4.0	This is also the system that did not submit a paper, so we do not know more details.	0
4199	38426	38426	K15-2001	Conclusions	8	298	3.0	4.0	course parsing.	0
4200	38427	38427	K15-2001	Conclusions	9	299	4.0	4.0	The shared task required the development of an end-to-end system, and the best system achieved an F1 score of 24.0% on the blind test set, reflecting the serious error propagation problem in such a system.	0
4201	38428	38428	K15-2001	Conclusions	10	300	4.0	4.0	The shared task exposed the most challenging aspect of shallow discourse parsing as a research problem, helping future research better calibrate their efforts.	0
4202	38429	38429	K15-2001	Conclusions	11	301	4.0	4.0	The evaluation data sets and the scorer we prepared for the shared task will be a useful benchmark for future research on shallow discourse parsing.	0
4203	42557	42557	D19-5719	title	1	1	4.0	1.0	Bacteria Biotope at BioNLP Open Shared Tasks 2019	0
4204	42558	42558	D19-5719	abstract	1	2	1.0	1.0	This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019.	0
4205	42559	42559	D19-5719	abstract	2	3	2.0	1.0	The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and fulltext excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology).	1
4206	42560	42560	D19-5719	abstract	3	4	3.0	1.0	The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology.	0
4207	42561	42561	D19-5719	abstract	4	5	4.0	1.0	The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization.	0
4208	42562	42562	D19-5719	abstract	5	6	4.0	1.0	We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.	0
4209	42563	42563	D19-5719	Introduction	1	7	1.0	1.0	In this paper, we present the fourth edition 1 of the Bacteria Biotope (BB) task.	0
4210	42564	42564	D19-5719	Introduction	2	8	1.0	1.0	The task was introduced in 2011.	0
4211	42565	42565	D19-5719	Introduction	3	9	1.0	1.0	It has the ambition of promoting large-scale information extraction (IE) from scientific documents in order to automatically fill knowledge bases in the microbial diversity field (Bossy et al., 2012).	0
4212	42566	42566	D19-5719	Introduction	4	10	1.0	1.0	BB 2019 is part of BioNLP Open Shared Tasks 2019 2 . BioNLP-OST is a community-wide effort for the comparison and evaluation of biomedical text mining technologies on manually curated benchmarks.	0
4213	42567	42567	D19-5719	Introduction	5	11	2.0	1.0	A large amount of information about microbes and their properties that is critical for microbiology research and development is scattered among millions of publications and databases .	0
4214	42568	42568	D19-5719	Introduction	6	12	2.0	1.0	Information extraction as framed by the Bacteria Biotope task identifies relevant entities and interrelationships in the text and map them to reference categories from existing knowledge resources.	0
4215	42569	42569	D19-5719	Introduction	7	13	2.0	1.0	This information can thus be combined with information from other sources referring to the same knowledge resources.	0
4216	42570	42570	D19-5719	Introduction	8	14	2.0	1.0	The knowledge resources used in the BB task are the NCBI taxonomy 3  (Federhen, 2011) for microbial taxa and the OntoBiotope ontology 4 (Nédellec et al., 2018) for microbial habitats and phenotypes.	0
4217	42571	42571	D19-5719	Introduction	9	15	3.0	1.0	The large size of these resources relative to the small number of training examples reflects the real conditions of IE application development, whilst it challenges current IE methods.	0
4218	42572	42572	D19-5719	Introduction	10	16	3.0	1.0	The lexical richness of the two resources partially offsets the difficulty.	0
4219	42573	42573	D19-5719	Introduction	11	17	3.0	1.0	Compared to the 2016 corpus that contained only scientific paper abstracts from the PubMed database (Deléger et al., 2016), the 2019 corpus is enriched with extracts from full-text articles.	0
4220	42574	42574	D19-5719	Introduction	12	18	3.0	1.0	We introduced a new entity type (phenotype) and a new relation type (linking microorganisms and phenotypes).	0
4221	42575	42575	D19-5719	Introduction	13	19	4.0	1.0	Phenotypes are observable characteristics such as morphology, or environment requirement (e.g. acidity, oxygen).	0
4222	42576	42576	D19-5719	Introduction	14	20	4.0	1.0	It is very valuable information for studying the ability of a given microbe to adapt to an environment (Brbić et al., 2016).	0
4223	42577	42577	D19-5719	Introduction	15	21	4.0	1.0	The definition of microorganism phenotype in the OntoBiotope ontology includes host interaction characteristics (e.g. symbiont) and community behavior and growth habit (e.g. epilithic).	0
4224	42578	42578	D19-5719	Introduction	16	22	4.0	1.0	The task organization and the evaluation metrics remain unchanged.	0
4225	42579	42579	D19-5719	Task Description	1	23	1.0	1.0	The representation scheme of the Bacteria Biotope task contains four entity types:	0
4226	42580	42580	D19-5719	Task Description	2	24	1.0	1.0	• Microorganism: names denoting microorganism taxa.	0
4227	42581	42581	D19-5719	Task Description	3	25	1.0	1.0	These taxa correspond to microorganism branches of the NCBI taxon-omy.	0
4228	42582	42582	D19-5719	Task Description	4	26	1.0	1.0	The set of relevant taxa is given on the BB task website.	0
4229	42583	42583	D19-5719	Task Description	5	27	1.0	1.0	• Habitat: phrases denoting physical places where microorganisms may be observed;	0
4230	42584	42584	D19-5719	Task Description	6	28	2.0	1.0	• Geographical: names of geographical places;	0
4231	42585	42585	D19-5719	Task Description	7	29	2.0	1.0	• Phenotype: expressions describing microbial characteristics.	0
4232	42586	42586	D19-5719	Task Description	8	30	2.0	1.0	The scheme defines two relation types:	0
4233	42587	42587	D19-5719	Task Description	9	31	2.0	1.0	•	0
4234	42588	42588	D19-5719	Task Description	10	32	2.0	1.0	Lives in relations which link a microorganism entity to its location (either a habitat or a geographical entity, or in few rare cases a microorganism entity);	0
4235	42589	42589	D19-5719	Task Description	11	33	3.0	1.0	•	0
4236	42590	42590	D19-5719	Task Description	12	34	3.0	1.0	Exhibits relations which link a microorganism entity to a phenotype entity.	0
4237	42591	42591	D19-5719	Task Description	13	35	3.0	1.0	Arguments of relations may occur in different sentences.	0
4238	42592	42592	D19-5719	Task Description	14	36	3.0	1.0	In addition, microorganisms are normalized to taxa from the NCBI taxonomy.	0
4239	42593	42593	D19-5719	Task Description	15	37	3.0	1.0	Habitat and phenotype entities are normalized to concepts from the OntoBiotope ontology.	0
4240	42594	42594	D19-5719	Task Description	16	38	4.0	1.0	We used the BioNLP-OST-2019 version of OntoBiotope available on AgroPortal 5 .	0
4241	42595	42595	D19-5719	Task Description	17	39	4.0	1.0	We used the NCBI Taxonomy version as available on February 2, 2019 from NCBI website 6 . Copies of both resources can be downloaded from the task website.	0
4242	42596	42596	D19-5719	Task Description	18	40	4.0	1.0	The microorganism part of the taxonomy contains 903,191 taxa plus synonyms, while the OntoBiotope ontology includes 3,601 concepts plus synonyms (3,172 for the Habitat branch and 429 for the Phenotype branch of the ontology).	0
4243	42597	42597	D19-5719	Task Description	19	41	4.0	1.0	Geographical entities are not normalized.	0
4244	42598	42598	D19-5719	Task Description	20	42	4.0	1.0	Figure 1 shows an example of a sentence annotated with normalized entities and relations.	0
4245	42599	42599	D19-5719	Task Description	21	43	4.0	1.0	As in the 2016 edition, we designed three tasks, each including two modalities, one where entity annotations are provided and one where they are not and have to be predicted.	0
4246	42600	42600	D19-5719	Entity Normalization	1	44	2.0	1.0	The first task focused on entity normalization.	0
4247	42601	42601	D19-5719	Entity Normalization	2	45	3.0	1.0	In the BB-norm modality of this task, participant systems had to normalize textual entity mentions according to the NCBI taxonomy for microorganisms and to the OntoBiotope ontology for habitats and phenotypes.	0
4248	42602	42602	D19-5719	Entity Normalization	3	46	4.0	1.0	In the BB-norm+ner modality, systems had to recognize the mentions before normalizing them.	0
4249	42603	42603	D19-5719	Relation Extraction	1	47	2.0	1.0	The second task focused on the extraction of the two types of relations-Lives in relations among microorganism, habitat and geographical entities, and Exhibits relations between microorganism and phenotype entities.	0
4250	42604	42604	D19-5719	Relation Extraction	2	48	4.0	1.0	In the BB-rel modality, participant systems only had to extract the relations, while in the BB-rel+ner modality they had to perform entity recognition in addition to relation extraction.	0
4251	42605	42605	D19-5719	Knowledge Base Extraction	1	49	1.0	1.0	The goal of the third task is to build a knowledge base using the entities and relations extracted from the corpus.	0
4252	42606	42606	D19-5719	Knowledge Base Extraction	2	50	2.0	2.0	It can be viewed as the combination of the previous tasks, followed by a merging step.	0
4253	42607	42607	D19-5719	Knowledge Base Extraction	3	51	2.0	2.0	Participant systems must normalize entities and extract relations.	0
4254	42608	42608	D19-5719	Knowledge Base Extraction	4	52	3.0	2.0	In the BB-kb modality, participant systems had to perform normalization and relation extraction with entity mentions being provided.	0
4255	42609	42609	D19-5719	Knowledge Base Extraction	5	53	4.0	2.0	In the BB-kb+ner modality, they had to perform entity recognition as well.	0
4256	42610	42610	D19-5719	Knowledge Base Extraction	6	54	4.0	2.0	3 Corpus Description	0
4257	42611	42611	D19-5719	Document Selection	1	55	1.0	2.0	The BB task corpus consists of two types of documents: PubMed references (titles and abstracts) related to microorganisms, and extracts from fulltext articles related to beneficial microorganisms living in food products.	0
4258	42612	42612	D19-5719	Document Selection	2	56	1.0	2.0	The PubMed references are the same as the 215 references of the Bacteria Biotope 2016 corpus.	0
4259	42613	42613	D19-5719	Document Selection	3	57	2.0	2.0	They were sampled from all PubMed entries indexed with a term from the Organisms/Bacteria subtree of the MeSH thesaurus.	0
4260	42614	42614	D19-5719	Document Selection	4	58	2.0	2.0	The full selection process is described in Deléger et al. (2016).	0
4261	42615	42615	D19-5719	Document Selection	5	59	3.0	2.0	Full-text extracts were selected from scientific articles about microorganisms of food interest and annotated by microbiologist experts in the context of the Florilege project (Falentin et al., 2017).	0
4262	42616	42616	D19-5719	Document Selection	6	60	3.0	2.0	We reused and complemented this corpus for the BB task.	0
4263	42617	42617	D19-5719	Document Selection	7	61	4.0	2.0	Because manual annotation is time-consuming and experts have limited time to dedicate to this task, they did not annotate the full articles.	0
4264	42618	42618	D19-5719	Document Selection	8	62	4.0	2.0	Instead, they chose the paragraphs and sentences they found the most informative in the articles.	0
4265	42619	42619	D19-5719	Document Selection	9	63	4.0	2.0	Thus, this part of the BB corpus is composed of 177 extracts of variable lengths (from one single	0
4266	42620	42620	D19-5719	Annotation	1	64	1.0	2.0	The PubMed references were already annotated as part of the 2016 edition.	0
4267	42621	42621	D19-5719	Annotation	2	65	1.0	2.0	We revised these annotations to add phenotype entities with their concept normalization and Exhibits relations.	0
4268	42622	42622	D19-5719	Annotation	3	66	2.0	2.0	Habitat annotations were also revised to take into account the new and enriched version of the OntoBiotope ontology (compared to the 2016 version 7 ).	0
4269	42623	42623	D19-5719	Annotation	4	67	2.0	2.0	We also extended the existing annotations of the full-text extracts of the Florilege project by assigning normalized concepts to the entities.	0
4270	42624	42624	D19-5719	Annotation	5	68	2.0	2.0	Annotation revision was performed by six annotators with backgrounds in biology, computer science and natural language processing.	0
4271	42625	42625	D19-5719	Annotation	6	69	3.0	2.0	All documents were annotated independently by two annotators and disagreements were resolved through an adjudication phase.	0
4272	42626	42626	D19-5719	Annotation	7	70	3.0	2.0	Detailed annotation guidelines  were provided to the annotators and were regularly updated following issues raised during the annotation or adjudication phases.	0
4273	42627	42627	D19-5719	Annotation	8	71	4.0	2.0	The inter-annotator agreement was computed by evaluating one of the two annotations before adjudication against the other.	0
4274	42628	42628	D19-5719	Annotation	9	72	4.0	2.0	Table 1 summarizes the inter-annotator agreement for named entities, normalization and relations.	0
4275	42629	42629	D19-5719	Annotation	10	73	4.0	2.0	The metrics used for inter-agreement are the same as for the evaluation of predictions and thus are described below (5.1).	0
4276	42630	42630	D19-5719	Descriptive Statistics	1	74	2.0	2.0	Table 2 gives the size of the corpus, in terms of documents, words, sentences and annotated ele-	0
4277	42631	42631	D19-5719	Descriptive Statistics	2	75	4.0	2.0	In the following, we present more detailed statistics and highlight corpus characteristics that may be challenging for the participants.	0
4278	42632	42632	D19-5719	Entities and Concepts	1	76	1.0	2.0	Table 3 shows the number of mentions, unique (lemmatized) mentions, concepts and average number of mentions per concept for each entity type.	0
4279	42633	42633	D19-5719	Entities and Concepts	2	77	1.0	2.0	Habitat entities are the most frequent, followed by Microorganism entities.	0
4280	42634	42634	D19-5719	Entities and Concepts	3	78	1.0	2.0	Geographical entities are very scarce.	0
4281	42635	42635	D19-5719	Entities and Concepts	4	79	1.0	2.0	There is much more variation in the expression of habitats and phenotypes than in that of microorganisms.	0
4282	42636	42636	D19-5719	Entities and Concepts	5	80	1.0	2.0	There is an average of respectively 4 and 3.5 unique mentions per habitat and phenotype concept while microorganisms only have 1.9.	0
4283	42637	42637	D19-5719	Entities and Concepts	6	81	2.0	2.0	Their proportion of unique entities out of all mentions is also higher (respectively 50.6% and 45.2% vs. 38.2% for microorganisms).	0
4284	42638	42638	D19-5719	Entities and Concepts	7	82	2.0	2.0	The proportion of direct mappings (i.e., exact string matches, taking into account lemmatization) between entity mentions and labels of concepts (from the NCBI taxonomy or the Onto-Biotope ontology) is displayed on Figure 2.	0
4285	42639	42639	D19-5719	Entities and Concepts	8	83	2.0	2.0	It emphasizes once more the variability of Habitat and Phenotype entity expressions, with respectively 72.5% and 91.2% mentions that do not exactly match a concept label or synonym.	0
4286	42640	42640	D19-5719	Entities and Concepts	9	84	2.0	2.0	Among exact matches, a small proportion of mentions are not actually normalized with the concept whose label they match.	0
4287	42641	42641	D19-5719	Entities and Concepts	10	85	2.0	2.0	"These are ""contextual normalization"" cases, i.e. entities are normalized with a more specific concept which can be inferred from the context."	0
4288	42642	42642	D19-5719	Entities and Concepts	11	86	3.0	2.0	These often correspond to lexical coreference cases.	0
4289	42643	42643	D19-5719	Entities and Concepts	12	87	3.0	2.0	A distinctive feature of the BB task is that multiple concepts may be assigned to a given entity mention.	0
4290	42644	42644	D19-5719	Entities and Concepts	13	88	3.0	2.0	Multiple normalization happens when two (or more) concepts can describe an entity and are all deemed necessary because each concept corresponds to a different aspect of the entity.	0
4291	42645	42645	D19-5719	Entities and Concepts	14	89	3.0	2.0	"An example of such a case is the Habitat entity ""diseased cow"" which is normalized by both the &lt;cow&gt; and &lt;animal with disease&gt; concepts."	0
4292	42646	42646	D19-5719	Entities and Concepts	15	90	3.0	2.0	This is the case mainly for Habitat entities (8.7%), and rarely happens for Phenotype entities (0.6%) and Microorganism entities (only one occurrence).	0
4293	42647	42647	D19-5719	Entities and Concepts	16	91	4.0	2.0	Another characteristic of the corpus is the presence of nested entities (entities embedded in another larger entity) and discontinuous entities (entities split in several fragments).	0
4294	42648	42648	D19-5719	Entities and Concepts	17	92	4.0	2.0	Both phenomena can be challenging for machine-learning methods and are often ignored.	0
4295	42649	42649	D19-5719	Entities and Concepts	18	93	4.0	2.0	The proportion of discontinuous entities in the corpus is limited, with a total of 3.7%.	0
4296	42650	42650	D19-5719	Entities and Concepts	19	94	4.0	2.0	Nested entities are more frequent (17.8% in total), especially for habitats.	0
4297	42651	42651	D19-5719	Entities and Concepts	20	95	4.0	2.0	"For instance, the Habitat entity ""cheese making factory"" also contains the smaller Habitat entity ""cheese""."	0
4298	42652	42652	D19-5719	Relations	1	96	1.0	2.0	Table 4 shows the number of relations for both Lives in and Exhibits types, including intrasentence and inter-sentence relations.	0
4299	42653	42653	D19-5719	Relations	2	97	1.0	2.0	Intrasentence relations involve entities occurring in the same sentence while inter-sentence relations involve entities occurring in different sentences, not necessarily contiguous.	0
4300	42654	42654	D19-5719	Relations	3	98	2.0	2.0	Inter-sentence relations are known to be challenging for automatic methods.	0
4301	42655	42655	D19-5719	Relations	4	99	2.0	2.0	Their proportion in the corpus is not negligible (17.5% in total).	0
4302	42656	42656	D19-5719	Relations	5	100	3.0	3.0	An example can be seen in the following extract: Vibrios [. . . ] are ubiquitous to oceans, coastal waters, and estuaries.	0
4303	42657	42657	D19-5719	Relations	6	101	3.0	3.0	[. . . ]	0
4304	42658	42658	D19-5719	Relations	7	102	4.0	3.0	The bacterial pathogen is a growing concern in North America.	0
4305	42659	42659	D19-5719	Relations	8	103	4.0	3.0	There is an inter-sentence relation between the two underlined entities.	0
4306	42660	42660	D19-5719	Training, Development and Test Sets	1	104	1.0	3.0	The BB corpus is split into training, development and test sets.	0
4307	42661	42661	D19-5719	Training, Development and Test Sets	2	105	1.0	3.0	"In practice, there are two test sets, one for the modalities involving entity recognition (the ""+ner"" sub-tasks) and one for the modalities where entity annotations are given."	0
4308	42662	42662	D19-5719	Training, Development and Test Sets	3	106	2.0	3.0	We kept the corpus division of the 2016 edition for the PubMed references.	0
4309	42663	42663	D19-5719	Training, Development and Test Sets	4	107	2.0	3.0	This was possible because the gold annotations of the test set were never released to the public.	0
4310	42664	42664	D19-5719	Training, Development and Test Sets	5	108	3.0	3.0	Then we split the Florilege full-text extracts using the same proportions as for   The proportion of concepts seen in the training set out of all concepts present in the knowledge resources is low for all entity types, which means that there is a large number of unseen examples (0.02% for microorganisms, 7.3% for habitats, and 15.6% for phenotypes).	0
4311	42665	42665	D19-5719	Training, Development and Test Sets	6	109	3.0	3.0	It emphasizes the need for methods that handle few-shot and zeroshot learning.	0
4312	42666	42666	D19-5719	Training, Development and Test Sets	7	110	4.0	3.0	Microorganisms have the lowest proportion, due to the large size of the microorganism taxonomies.	0
4313	42667	42667	D19-5719	Training, Development and Test Sets	8	111	4.0	3.0	However, the names of the	0
4314	42668	42668	D19-5719	Supporting Resources	1	112	1.0	3.0	Supporting resources were made available to participants.	0
4315	42669	42669	D19-5719	Supporting Resources	2	113	2.0	3.0	They consist of outputs from state-ofthe-art tools applied to the BB data sets (e.g., POS tagging, syntactic parsing, NER, word embeddings).	0
4316	42670	42670	D19-5719	Supporting Resources	3	114	3.0	3.0	We proposed in-house embeddings trained on selected relevant PubMed abstracts, and links to external embeddings (Pyysalo et al., 2013;	0
4317	42671	42671	D19-5719	Supporting Resources	4	115	4.0	3.0	Li et al., 2017) trained on PubMed and Wikipedia.	0
4318	42672	42672	D19-5719	Supporting Resources	5	116	4.0	3.0	The full list of tools and resources is available on the website.	0
4319	42673	42673	D19-5719	Evaluation	1	117	1.0	3.0	Metrics	0
4320	42674	42674	D19-5719	Evaluation	2	118	1.0	3.0	We used the same evaluation metrics as in the 2016 edition.	0
4321	42675	42675	D19-5719	Evaluation	3	119	1.0	3.0	The underlying rationale and formula of each score is detailed in Deléger et al. (2016);	0
4322	42676	42676	D19-5719	Evaluation	4	120	2.0	3.0	Bossy et al. (2013).	0
4323	42677	42677	D19-5719	Evaluation	5	121	2.0	3.0	Additionally we compute a variety of alternate scorings in order to distinguish the strengths of each submission.	0
4324	42678	42678	D19-5719	Evaluation	6	122	2.0	3.0	The evaluation tool was provided to participants 9 .	0
4325	42679	42679	D19-5719	Evaluation	7	123	3.0	3.0	Normalization accuracy is measured through a semantic similarity metric, and micro-averaging across entities.	0
4326	42680	42680	D19-5719	Evaluation	8	124	3.0	3.0	Relation extraction is measured with Recall, Precision, and F 1 .	0
4327	42681	42681	D19-5719	Evaluation	9	125	3.0	3.0	However for tasks where systems must recognize entities, we used the Slot Error Rate (SER) instead of F 1 in order to avoid sanctioning twice the inaccuracy of boundaries.	0
4328	42682	42682	D19-5719	Evaluation	10	126	4.0	3.0	The SER measures the amount of errors according to three types: insertions (false positives), deletions (false negatives), and substitutions (partial matches).	0
4329	42683	42683	D19-5719	Evaluation	11	127	4.0	3.0	The SER is normalized by the number of reference items.	0
4330	42684	42684	D19-5719	Evaluation	12	128	4.0	3.0	The higher the value the worse is the prediction, and there is no upper bound since insertions can exceed the number of items in the reference.	0
4331	42685	42685	D19-5719	Evaluation	13	129	4.0	3.0	Confidence intervals were computed for each metric with the bootstrap resampling method (90%, n=100).	0
4332	42686	42686	D19-5719	Baseline	1	130	1.0	3.0	We designed simple baselines for each sub-task in order to provide a comparison reference.	0
4333	42687	42687	D19-5719	Baseline	2	131	1.0	3.0	We preprocessed the corpus with the AlvisNLP 10 engine, that performs tokenization, sentence splitting, and lemmatization using the GENIA tagger (Tsuruoka et al., 2005).	0
4334	42688	42688	D19-5719	Baseline	3	132	1.0	3.0	• BB-norm: we performed exact matching between lemmatized entities and the knowledge resources.	0
4335	42689	42689	D19-5719	Baseline	4	133	2.0	3.0	When no match was found, we normalized habitats and phenotypes with the top-level concept of the Habitat and Phenotype ontology branches, and microorganisms with the high-level &lt;Bacteria&gt; taxon.	0
4336	42690	42690	D19-5719	Baseline	5	134	2.0	3.0	• BB-norm+ner: we used our exact matching approach on the lemmatized text of the documents instead of on given entity mentions.	0
4337	42691	42691	D19-5719	Baseline	6	135	2.0	3.0	• BB-rel: we used a simple co-occurrence approach, linking pairs of entities occurring in the same sentences.	0
4338	42692	42692	D19-5719	Baseline	7	136	3.0	3.0	• BB-rel+ner: we first detected entities using our exact matching strategy for microorganisms, habitats and phenotypes.	0
4339	42693	42693	D19-5719	Baseline	8	137	3.0	3.0	For geographical entities, we used the Stanford Named Entity Recognition tool (Finkel et al., 2005).	0
4340	42694	42694	D19-5719	Baseline	9	138	3.0	3.0	Then we linked entities occurring in the same sentences, as for the BB-rel task.	0
4341	42695	42695	D19-5719	Baseline	10	139	4.0	3.0	• BB-kb: we combined the BB-norm and BBrel approaches.	0
4342	42696	42696	D19-5719	Baseline	11	140	4.0	3.0	• BB-kb+ner: we combined our BB-norm+ner method with our co-occurrence approach.	0
4343	42697	42697	D19-5719	Baseline	12	141	4.0	3.0	6 Outcome	0
4344	42698	42698	D19-5719	Participation	1	142	2.0	3.0	The blind test data was released on the 22 nd of July 2019 and participants were given until the 31 st of July to submit their predictions.	0
4345	42699	42699	D19-5719	Participation	2	143	3.0	3.0	Each team was allowed two submissions to each sub-task.	0
4346	42700	42700	D19-5719	Participation	3	144	4.0	3.0	Ten teams participated to all six sub-tasks and submitted a total of 31 runs.	0
4347	42701	42701	D19-5719	Participants' Methods and Resources	1	145	1.0	3.0	As in 2016, most methods are based on Machine Learning algorithms.	0
4348	42702	42702	D19-5719	Participants' Methods and Resources	2	146	1.0	3.0	For named entity recognition, the CRF algorithm is still the most used (BLAIR GMU), though sometimes combined with a neural network (MIC-CIS).	0
4349	42703	42703	D19-5719	Participants' Methods and Resources	3	147	1.0	3.0	In 2016, the majority of participants used SVMs for relation extraction.	0
4350	42704	42704	D19-5719	Participants' Methods and Resources	4	148	2.0	3.0	In this edition nearly all participants used neural networks in a diversity of architectures: multi-layer perceptron (Yuhang Wu), bi-LSTM (whunlp), AGCNN (whunlp).	0
4351	42705	42705	D19-5719	Participants' Methods and Resources	5	149	2.0	3.0	One participant predicted relations through filtered co-occurrences (BOUN-ISIK), and another by bagging SVM and Logistic Regression (BLAIR GMU).	0
4352	42706	42706	D19-5719	Participants' Methods and Resources	6	150	2.0	4.0	Note that AliAI employed a multi-task architecture similar to BERT (Devlin et al., 2019) to perform both named-entity recognition and relation extraction.	0
4353	42707	42707	D19-5719	Participants' Methods and Resources	7	151	2.0	4.0	The normalization task was addressed in a more diverse manner.	0
4354	42708	42708	D19-5719	Participants' Methods and Resources	8	152	3.0	4.0	On one hand several distinct ML algorithms were used to discriminate entity categories: ensemble CNNs (PADIA BacReader), kNN with reranking (BOUN-ISIK), or Linear Regression (BLAIR GMU).	0
4355	42709	42709	D19-5719	Participants' Methods and Resources	9	153	3.0	4.0	On the other hand MIC-CIS employed an exact and an approximate matching algorithm.	0
4356	42710	42710	D19-5719	Participants' Methods and Resources	10	154	3.0	4.0	Word embeddings trained with Word2Vec (Mikolov et al., 2013) on a domain-specific corpus (PubMed abstract, PMC articles) seem to be an universal resource since all but one submissions for any task used them.	0
4357	42711	42711	D19-5719	Participants' Methods and Resources	11	155	4.0	4.0	BLAIR GMU used contextual embeddings based on BERT and XLNet (Yang et al., 2019).	0
4358	42712	42712	D19-5719	Participants' Methods and Resources	12	156	4.0	4.0	Dependency parsing was used in every relation extraction submission, and also for normalization (BOUN-ISIK).	0
4359	42713	42713	D19-5719	Participants' Methods and Resources	13	157	4.0	4.0	The most popular NLP tool libraries are Stanford CoreNLP (Manning et al., 2014) and NLTK (Bird et al., 2009).	0
4360	42714	42714	D19-5719	Participants' Methods and Resources	14	158	4.0	4.0	We also note that the Word-Piece segmentation is used even in systems that do not use BERT.	0
4361	42715	42715	D19-5719	Results	1	159	1.0	4.0	In this section we report the results for all subtasks, and highlight notable results as well as a comparison with results obtained in 2016 in the third edition of the Bacteria Biotope task in BioNLP-ST 2016.	0
4362	42716	42716	D19-5719	Results	2	160	2.0	4.0	The task site presents detailed results, including main and alternate metrics, as well as confidence intervals.	0
4363	42717	42717	D19-5719	Results	3	161	3.0	4.0	However comparison with 2016 is limited by the evolution of the task.	0
4364	42718	42718	D19-5719	Results	4	162	4.0	4.0	On one hand the data set has increased approximately by 50%, and the annotations were revised and their quality improved.	0
4365	42719	42719	D19-5719	Results	5	163	4.0	4.0	On the other hand the tasks were made harder because the schema was enriched with an entity type and a relation type, and the target taxa have been extended from Bacteria only to all microorganisms.	0
4366	42720	42720	D19-5719	BB-norm and BB-norm+ner	1	164	1.0	4.0	The main results as well as the results for each entity type are shown in Tables 6 and 7. BOUN-ISIK and BLAIR GMU obtained the best overall results for BB-norm, and MIC-CIS for BB-norm+ner.	0
4367	42721	42721	D19-5719	BB-norm and BB-norm+ner	2	165	2.0	4.0	The results for each entity type highlight different profiles.	0
4368	42722	42722	D19-5719	BB-norm and BB-norm+ner	3	166	2.0	4.0	While BOUN-ISIK predicts accurate normalizations for habitat entities for BB-norm, BLAIR GMU predicts better normalizations for microorganism entities.	0
4369	42723	42723	D19-5719	BB-norm and BB-norm+ner	4	167	3.0	4.0	PADIA BacReader's predictions for habitats is on par with BOUN-ISIK, and their normalization of phenotype entities is outstanding.	0
4370	42724	42724	D19-5719	BB-norm and BB-norm+ner	5	168	3.0	4.0	As for BB-norm+ner, MIC-CIS consistently predicts the best entity boundaries and normalizations for all types.	0
4371	42725	42725	D19-5719	BB-norm and BB-norm+ner	6	169	4.0	4.0	In comparison to 2016, the state of the art for multi-word entity recognition and normalization, like habitats and phenotypes, has improved.	0
4372	42726	42726	D19-5719	BB-norm and BB-norm+ner	7	170	4.0	4.0	We note that with the introduction of new taxa the recognition and normalization of taxa may have been rendered more difficult than anticipated since the results are lower than obtained in 2016.	0
4373	42727	42727	D19-5719	BB-rel and BB-rel+ner	1	171	1.0	4.0	The results of BB-rel and BB-rel+ner are given in Tables 8 and 9 respectively.	0
4374	42728	42728	D19-5719	BB-rel and BB-rel+ner	2	172	1.0	4.0	The table includes the scores obtained for each relation type, as well as the best results obtained in 2016.	0
4375	42729	42729	D19-5719	BB-rel and BB-rel+ner	3	173	2.0	4.0	The highest F-score for BB-rel was obtained by the whunlp submission, with AliAI as a very close contender.	0
4376	42730	42730	D19-5719	BB-rel and BB-rel+ner	4	174	2.0	4.0	UTU, and very closely behind AliAI, obtained the highest Precision, whereas BOUN-ISIK the highest Recall.	0
4377	42731	42731	D19-5719	BB-rel and BB-rel+ner	5	175	3.0	4.0	The Recall of the baseline prediction indicates the highest recall possible for relations contained in a single sentence.	0
4378	42732	42732	D19-5719	BB-rel and BB-rel+ner	6	176	3.0	4.0	No participating system addresses cross-sentence relations, which appears to be the most productive lead to increase performance.	0
4379	42733	42733	D19-5719	BB-rel and BB-rel+ner	7	177	4.0	4.0	Most submissions outperform the best predictions of 2016 in at least one score, and five of the eleven submissions obtain a significantly higher Fscore.	0
4380	42734	42734	D19-5719	BB-rel and BB-rel+ner	8	178	4.0	4.0	For BB-rel+ner, AliAI obtains the highest recall and precision, consistently for Lives In and Exhibits relations.	0
4381	42735	42735	D19-5719	BB-rel and BB-rel+ner	9	179	4.0	4.0	This submission also outperforms significantly the state of the art set in 2016.	0
4382	42736	42736	D19-5719	BB-kb and BB-kb+ner	1	180	1.0	4.0	BLAIR GMU is the only team to submit to the BB-kb and BB-kb+ner tasks, their results are shown in Table 10.	0
4383	42737	42737	D19-5719	BB-kb and BB-kb+ner	2	181	2.0	4.0	The knowledge-base task and evaluation necessarily require end-to-end prediction systems that must perform named-entity recognition, entity normalization, relation extraction, as well as contributory tasks like POStagging, or coreference resolution.	0
4384	42738	42738	D19-5719	BB-kb and BB-kb+ner	3	182	2.0	4.0	The limited scores obtained might be explained by the accumulation of errors by successive prediction steps.	0
4385	42739	42739	D19-5719	BB-kb and BB-kb+ner	4	183	3.0	4.0	Since the data of all sub-tasks comes from the       10: Results for the BB-kb and BB-kb+ner subtasks.	0
4386	42740	42740	D19-5719	BB-kb and BB-kb+ner	5	184	4.0	4.0	The metric is the average of the semantic similarity between the reference and the predicted normalizations for all relation arguments after removing duplicates at the corpus level.	0
4387	42741	42741	D19-5719	BB-kb and BB-kb+ner	6	185	4.0	4.0	Best scores are in bold font, several scores are in bold if their difference is not significant.	0
4388	42742	42742	D19-5719	Conclusion	1	186	1.0	4.0	The Bacteria Biotope	0
4389	42743	42743	D19-5719	Conclusion	2	187	1.0	4.0	Task arouses sustained interest with a total of 10 teams participating in the fourth edition.	0
4390	42744	42744	D19-5719	Conclusion	3	188	1.0	4.0	As usual, the relation extraction sub-tasks (BB-rel and BB-rel+ner) were the most popular, demonstrating that this task is still a scientific and technical challenge.	0
4391	42745	42745	D19-5719	Conclusion	4	189	2.0	4.0	The most notable evolution of participating systems since the last edition is the pervasiveness of methods based on neural networks and word embeddings.	0
4392	42746	42746	D19-5719	Conclusion	5	190	2.0	4.0	These systems yielded superior predictions compared to those in 2016.	0
4393	42747	42747	D19-5719	Conclusion	6	191	2.0	4.0	As mentioned previously, there is still much room for improvement in addressing cross-sentence relation extraction.	0
4394	42748	42748	D19-5719	Conclusion	7	192	2.0	4.0	We also note a growing interest in the normalization sub-tasks (BB-norm and BB-norm+ner).	0
4395	42749	42749	D19-5719	Conclusion	8	193	3.0	4.0	The predictions improved for habitat entities, and are very promising for phenotype entities.	0
4396	42750	42750	D19-5719	Conclusion	9	194	3.0	4.0	However the generalization from bacteria-only taxa in 2016 to all microorganisms in this edition proved to pose an unexpected challenge.	0
4397	42751	42751	D19-5719	Conclusion	10	195	3.0	4.0	Knowledge base population (BB-kb and BB-kb+ner) is the most challenging task, since it requires a wider set of capabilities.	0
4398	42752	42752	D19-5719	Conclusion	11	196	4.0	4.0	Nevertheless we demonstrated that the combination of other subtask predictions allows to produce better quality knowledge bases.	0
4399	42753	42753	D19-5719	Conclusion	12	197	4.0	4.0	To help participants, supporting resources were provided.	0
4400	42754	42754	D19-5719	Conclusion	13	198	4.0	4.0	The most used resources were pretrained word embeddings, and general-domain named entities.	0
4401	42755	42755	D19-5719	Conclusion	14	199	4.0	4.0	The evaluation on the test set will be maintained online 11 in order for future experiments to compare with the current state of the art.	0
4402	42958	42958	D19-6007	title	1	1	4.0	1.0	Commonsense Inference in Natural Language Processing (COIN) -Shared Task Report	0
4403	42959	42959	D19-6007	abstract	1	2	1.0	1.0	This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019.	0
4404	42960	42960	D19-6007	abstract	2	3	2.0	1.0	The tasks consisted of two machine comprehension evaluations, each of which tested a system's ability to answer questions/queries about a text.	0
4405	42961	42961	D19-6007	abstract	3	4	3.0	1.0	Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text.	0
4406	42962	42962	D19-6007	abstract	4	5	4.0	1.0	A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
4407	42963	42963	D19-6007	Introduction	1	6	1.0	1.0	Due to the rise of powerful pre-trained word and sentence representations, automated text processing has come a long way in recent years, with systems that perform even better than humans on some datasets (Rajpurkar et al., 2016a).	0
4408	42964	42964	D19-6007	Introduction	2	7	1.0	1.0	However, natural language understanding also involves complex challenges.	0
4409	42965	42965	D19-6007	Introduction	3	8	1.0	1.0	One important difference between human and machine text understanding lies in the fact that humans can access commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that are assumed to be common ground.	0
4410	42966	42966	D19-6007	Introduction	4	9	1.0	1.0	"(1) Max: ""It's 1 pm already, I think we should get lunch."""	0
4411	42967	42967	D19-6007	Introduction	5	10	1.0	1.0	"Dustin: ""Let me get my wallet."""	0
4412	42968	42968	D19-6007	Introduction	6	11	2.0	1.0	Consider the conversation in Example 1: Max will not be surprised that Dustin needs to get his wallet, since she knows that paying is a part of getting lunch.	0
4413	42969	42969	D19-6007	Introduction	7	12	2.0	1.0	Also, she knows that a wallet is needed for paying, so Dustin needs to get a wallet for lunch.	0
4414	42970	42970	D19-6007	Introduction	8	13	2.0	1.0	This is part of the commonsense knowledge about getting lunch and should be known by both persons.	0
4415	42971	42971	D19-6007	Introduction	9	14	2.0	1.0	For a computer system, inferring such unmentioned facts is a non-trivial challenge.	0
4416	42972	42972	D19-6007	Introduction	10	15	2.0	1.0	The workshop on Commonsense Inference in NLP (COIN) is focused on such phenomena, looking at models, data, and evaluation methods for commonsense inference.	0
4417	42973	42973	D19-6007	Introduction	11	16	3.0	1.0	This report summarizes the results of the COIN shared tasks, an unofficial extension of the Sem-Eval 2018 shared task 11, Machine Comprehension using Commonsense Knowledge (Ostermann et al., 2018b).	0
4418	42974	42974	D19-6007	Introduction	12	17	3.0	1.0	The tasks aim to evaluate the commonsense inference capabilities of text understanding systems in two settings: Commonsense inference in everyday narrations (task 1) and commonsense inference in news texts (task 2).	1
4419	42975	42975	D19-6007	Introduction	13	18	3.0	1.0	Framed as machine comprehension evaluations, the datasets used for both tasks contain challenging reading comprehension questions asking for facts that are not explicitly mentioned in the given reading texts.	0
4420	42976	42976	D19-6007	Introduction	14	19	3.0	1.0	Several teams participated in the shared tasks and submitted system description papers.	0
4421	42977	42977	D19-6007	Introduction	15	20	3.0	1.0	All systems are based on Transformer architectures (Vaswani et al., 2017), some of them explicitly incorporating commonsense knowledge resources, whereas others only use pretraining on other machine comprehension data sets.	0
4422	42978	42978	D19-6007	Introduction	16	21	4.0	1.0	The best submitted system achieves 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	0
4423	42979	42979	D19-6007	Introduction	17	22	4.0	1.0	Still, there are cases that remain elusive: Humans outperform this system by a margin of 7% (task 1) and 8% (task 2).	0
4424	42980	42980	D19-6007	Introduction	18	23	4.0	1.0	Our results indicate that while Transformer models are able to perform extremely well on the data used in our shared task, there are still some remaining cases demonstrating that human level is not achieved yet.	0
4425	42981	42981	D19-6007	Introduction	19	24	4.0	1.0	Still, we believe that our results also imply the need for more challenging data sets.	0
4426	42982	42982	D19-6007	Introduction	20	25	4.0	1.0	In particular, we need data sets that make it harder to benefit from redundancy in the training data or large-scale pretraining on similar domains.	0
4427	42983	42983	D19-6007	Introduction	21	26	4.0	1.0	In the following, we briefly describe the data sets ( §2), baselines and evaluation metrics of the shared tasks ( §3) and we present a summary of the participating systems ( §4), their results ( §5) as well as a discussion thereof ( §6).	0
4428	42984	42984	D19-6007	Data and Tasks	1	27	1.0	1.0	Text understanding systems are often evaluated by means of a reading comprehension task, which is also referred to as machine (reading) comprehension (MC).	0
4429	42985	42985	D19-6007	Data and Tasks	2	28	1.0	1.0	The central idea is that a system has to process a text and then find a correct answer to a question that is asked on the text.	0
4430	42986	42986	D19-6007	Data and Tasks	3	29	1.0	1.0	Our shared tasks follow this paradigm and use machine comprehension settings to evaluate a model's capability to perform commonsense inferences.	0
4431	42987	42987	D19-6007	Data and Tasks	4	30	1.0	1.0	In contrast to most existing MC datasets, the two datasets that are used for our shared tasks, MCScript2.0 (Ostermann et al., 2019) and ReCoRD (Zhang et al., 2018), are focused on questions that cannot be answered from the text alone, but that require a model to draw inference over unmentioned facts.	0
4432	42988	42988	D19-6007	Data and Tasks	5	31	2.0	1.0	(2) Text: Camping is one of my favorite summer vacations. (...)	0
4433	42989	42989	D19-6007	Data and Tasks	6	32	2.0	1.0	Once I have all my gear and clothing I'll pack it into my car, making sure to leave room for myself, my dog and anything my friends want to bring.	0
4434	42990	42990	D19-6007	Data and Tasks	7	33	2.0	1.0	And then we are ready for our camping vacation.	0
4435	42991	42991	D19-6007	Data and Tasks	8	34	2.0	1.0	Question:	0
4436	42992	42992	D19-6007	Data and Tasks	9	35	3.0	1.0	What do they put the drinks in?	0
4437	42993	42993	D19-6007	Data and Tasks	10	36	3.0	1.0	a. Cooler b. Sleeping bag Example 2 illustrates the main idea of the shared tasks.	0
4438	42994	42994	D19-6007	Data and Tasks	11	37	3.0	1.0	It shows a reading text from MC-Script2.0, together with a question and two candidate answers.	0
4439	42995	42995	D19-6007	Data and Tasks	12	38	3.0	1.0	For a human, it is trivial to find that the drinks are put into a cooler rather than the sleeping bag.	0
4440	42996	42996	D19-6007	Data and Tasks	13	39	4.0	1.0	This information is however not mentioned in the text, so a machine needs to have the capability to infer this fact from commonsense knowledge.	0
4441	42997	42997	D19-6007	Data and Tasks	14	40	4.0	1.0	The reading texts of MCScript2.0 are narrations about everyday activities (task 1).	0
4442	42998	42998	D19-6007	Data and Tasks	15	41	4.0	1.0	Due to its domain, MCScript2.0 has a focus on evaluating script knowledge, i.e. knowledge about the events and participants of such everyday activities (Schank and Abelson, 1975).	0
4443	42999	42999	D19-6007	Data and Tasks	16	42	4.0	1.0	Task 2 utilizes the ReCoRD corpus (Zhang et al., 2018), which contains news texts, a more open domain.	0
4444	43000	43000	D19-6007	Data and Tasks	17	43	4.0	1.0	The inferences that are required for finding answers to the questions in ReCoRD are thus of a more general type.	0
4445	43001	43001	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	1	44	1.0	1.0	MCScript2.0 is a reading comprehension data set comprising 19,821 questions on 3,487 texts.	0
4446	43002	43002	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	2	45	1.0	1.0	Each of the questions has two answer candidates, one of which is correct.	0
4447	43003	43003	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	3	46	1.0	2.0	Questions in the data were annotated for reasoning types, i.e. according to whether the answer to a question can be found in the text or needs to be inferred from commonsense knowledge.	0
4448	43004	43004	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	4	47	2.0	2.0	Roughly half of the questions do require inferences over commonsense knowledge.	0
4449	43005	43005	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	5	48	2.0	2.0	The texts in MCScript2.0 are short narrations (164.4 tokens on average) on a total of 200 different everyday activities.	0
4450	43006	43006	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	6	49	2.0	2.0	All texts were crowdsourced on Amazon Mechanical Turk 1 , by asking crowd workers to tell a story about one of the 200 scenarios as if talking to a child (Modi et al., 2016;	0
4451	43007	43007	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	7	50	3.0	2.0	Ostermann et al., 2018a), resulting in simple texts which explicitly mention many details of a scenario.	0
4452	43008	43008	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	8	51	3.0	2.0	In the question collection, which was also conducted via crowdsourcing, turkers were then asked to write questions about noun or verb phrases that were highlighted in the texts.	0
4453	43009	43009	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	9	52	3.0	2.0	After collecting questions, the sentences containing the noun or verb phrases were deleted from the texts.	0
4454	43010	43010	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	10	53	4.0	2.0	During the answer collection, crowd workers thus had to infer the information required for finding an answer from background knowledge.	0
4455	43011	43011	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	11	54	4.0	2.0	Five turkers wrote correct and incorrect answer candidates for each question, and the most difficult incorrect candidates were selected via adversarial filtering (Zellers et al., 2018).	0
4456	43012	43012	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	12	55	4.0	2.0	For our shared task, we use the same data split as Ostermann et al. (2019): 14,191 questions on 2,500 texts for the training set, 2,020 questions on 355 texts for the development set and 3,610 questions on 632 texts for the test set.	0
4457	43013	43013	D19-6007	Task 1: Commonsense Inference in Everyday Narrations	13	56	4.0	2.0	All texts for five scenarios were reserved for the test set only to increase difficulty.	0
4458	43014	43014	D19-6007	Task 2: Commonsense Inference in News Articles	1	57	1.0	2.0	ReCoRD is a large-scale dataset for reading comprehension, which consists of over 120,000 ex-  amples, most of which require commonsense reasoning.	0
4459	43015	43015	D19-6007	Task 2: Commonsense Inference in News Articles	2	58	2.0	2.0	ReCoRD was collected in a fourstage process (Figure 1): (1) curating CNN/Daily	0
4460	43016	43016	D19-6007	Task 2: Commonsense Inference in News Articles	3	59	3.0	2.0	Mail news articles, (2) generating passage-queryanswers triples based on the news articles, (3) filtering out the queries that can be easily answered by state-of-the-art machine comprehension (MC) models, and (4) filtering out the queries ambiguous to human readers.	0
4461	43017	43017	D19-6007	Task 2: Commonsense Inference in News Articles	4	60	4.0	2.0	All named entities in the passages are possible answers to the queries.	0
4462	43018	43018	D19-6007	Task 2: Commonsense Inference in News Articles	5	61	4.0	2.0	Table 1 summarizes the data statistics.	0
4463	43019	43019	D19-6007	Shared Task Setup	1	62	4.0	2.0	The baselines for our shared tasks were adapted from Ostermann et al. (2019) and Zhang et al. (2018), respectively.	0
4464	43020	43020	D19-6007	Task 1 Baselines	1	63	1.0	2.0	Following Ostermann et al. (2019), we present results of three baseline models.	0
4465	43021	43021	D19-6007	Task 1 Baselines	2	64	1.0	2.0	Logistic Regression Model.	0
4466	43022	43022	D19-6007	Task 1 Baselines	3	65	1.0	2.0	Merkhofer et al. (2018) presented a logistic regression classifier for the SemEval 2018 shared task 11, which used simple overlap features and word patterns on MC-Script, a predecessor of the dataset used for this task.	0
4467	43023	43023	D19-6007	Task 1 Baselines	4	66	2.0	2.0	Their model outperformed many neural networks in spite of its simplicity.	0
4468	43024	43024	D19-6007	Task 1 Baselines	5	67	2.0	2.0	Attentive Reader.	0
4469	43025	43025	D19-6007	Task 1 Baselines	6	68	2.0	2.0	The second baseline model is an attentive reader network (Hermann et al., 2015).	0
4470	43026	43026	D19-6007	Task 1 Baselines	7	69	2.0	2.0	GRU units (Cho et al., 2014) are used to process text, question and answer.	0
4471	43027	43027	D19-6007	Task 1 Baselines	8	70	3.0	2.0	A questionaware text representation is computed based on a bilinear attention function, which is then combined with a GRU-based answer representation for prediction.	0
4472	43028	43028	D19-6007	Task 1 Baselines	9	71	3.0	2.0	For details, we refer to Ostermann et al. (2019), Ostermann et al. (2018a) and Chen et al. (2016) TriAN.	0
4473	43029	43029	D19-6007	Task 1 Baselines	10	72	3.0	2.0	As last model, we use the three-way attentive network (TriAN) (Wang et al., 2018), a recurrent neural network that scored the first place in the SemEval 2018 task.	0
4474	43030	43030	D19-6007	Task 1 Baselines	11	73	4.0	2.0	They use LSTM units (Hochreiter and Schmidhuber, 1997), several attention functions, and self attention to compute representations for text, question and answer.	0
4475	43031	43031	D19-6007	Task 1 Baselines	12	74	4.0	2.0	Concept	0
4476	43032	43032	D19-6007	Task 1 Baselines	13	75	4.0	2.0	Net (Speer et al., 2017), a large commonsense knowledge base containing thousands of entities and commonsense relations between them, is used to enhance text representations with commonsense information, by computing relation embeddings and appending them to the text representations.	0
4477	43033	43033	D19-6007	Task 1 Baselines	14	76	4.0	2.0	For more information we refer to Wang et al. (2018).	0
4478	43034	43034	D19-6007	Task 2 Baselines	1	77	1.0	2.0	We present five baselines for ReCoRD:	0
4479	43035	43035	D19-6007	Task 2 Baselines	2	78	1.0	2.0	BERT	0
4480	43036	43036	D19-6007	Task 2 Baselines	3	79	2.0	2.0	(Devlin et al., 2019) is a new language representation model.	0
4481	43037	43037	D19-6007	Task 2 Baselines	4	80	2.0	2.0	Recently fine-tuning the pre-trained BERT with an additional output layer has created state-of-the-art models on a wide range of NLP tasks.	0
4482	43038	43038	D19-6007	Task 2 Baselines	5	81	2.0	2.0	We formalized ReCoRD as an extractive QA task like SQuAD, and then reused the fine-tuning script for SQuAD to fine-tune BERT for ReCoRD.	0
4483	43039	43039	D19-6007	Task 2 Baselines	6	82	3.0	2.0	KT-NET	0
4484	43040	43040	D19-6007	Task 2 Baselines	7	83	3.0	2.0	(Yang et al., 2019a) employs an attention mechanism to adaptively select desired knowledge from knowledge bases, and then fuses selected knowledge with BERT to enable contextand knowledge-aware predictions for machine reading comprehension.	0
4485	43041	43041	D19-6007	Task 2 Baselines	8	84	3.0	2.0	(Seo et al., 2016) and self-attention, both of which are widely used in MC models.	0
4486	43042	43042	D19-6007	Task 2 Baselines	9	85	4.0	2.0	We also evaluated a variant of DocQA with ELMo (Peters et al., 2018) to analyze the impact of ELMo on this task.	0
4487	43043	43043	D19-6007	Task 2 Baselines	10	86	4.0	2.0	Random Guess acts as the lower bound of the evaluated models, which randomly picks a named entity from the passage as the answer.	0
4488	43044	43044	D19-6007	Task 2 Baselines	11	87	4.0	2.0	The reported results are averaged over 5 runs.	0
4489	43045	43045	D19-6007	Evaluation	1	88	1.0	2.0	Task 1.	0
4490	43046	43046	D19-6007	Evaluation	2	89	1.0	2.0	The evaluation measure for task 1 is accuracy, computed as the number of correctly answered questions divided by the number of all questions.	0
4491	43047	43047	D19-6007	Evaluation	3	90	2.0	2.0	We also report accuracy values on questions that crowd workers explicitly annotated as requiring commonsense as well as performance on the five held-out scenarios.	0
4492	43048	43048	D19-6007	Evaluation	4	91	2.0	3.0	Task 2.	0
4493	43049	43049	D19-6007	Evaluation	5	92	3.0	3.0	We use two evaluation metrics, EM and F1, similar to those used by SQuAD (Rajpurkar et al., 2016b).	0
4494	43050	43050	D19-6007	Evaluation	6	93	3.0	3.0	Exact Match (EM) measures the percentage of predictions that match a reference answer exactly.	0
4495	43051	43051	D19-6007	Evaluation	7	94	4.0	3.0	(Macro-averaged) F 1 measures the average overlap between model predictions and reference answers.	0
4496	43052	43052	D19-6007	Evaluation	8	95	4.0	3.0	For computing F 1 , we treat prediction and reference answers as bags of tokens.	0
4497	43053	43053	D19-6007	Evaluation	9	96	4.0	3.0	We take the maximum F 1 over all reference answers for a given query, and then average over all queries.	0
4498	43054	43054	D19-6007	Participants	1	97	1.0	3.0	In total, five teams submitted systems in task 1, and one team participated in task 2.	0
4499	43055	43055	D19-6007	Participants	2	98	1.0	3.0	All submitted models were neural networks, and all made use of pretrained Transformer language models such as BERT (Devlin et al., 2019).	0
4500	43056	43056	D19-6007	Participants	3	99	1.0	3.0	The participants used a wide range of external corpora and resources to augment their models, ranging from other machine comprehension data sets such as RACE (Lai et al., 2017) or MCScript (Ostermann et al., 2018a), up to commonsense knowledge databases such as ConceptNet (Speer et al., 2017), WebChild (Tandon et al., 2017) or ATOMIC (Sap et al., 2019).	0
4501	43057	43057	D19-6007	Participants	4	100	1.0	3.0	Table 2 gives a summary of the participating systems.	0
4502	43058	43058	D19-6007	Participants	5	101	2.0	3.0	• PSH-SJTU  participated in both tasks with a Transformer model based on XLNet (Yang et al., 2019b).	0
4503	43059	43059	D19-6007	Participants	6	102	2.0	3.0	For task 1, they pretrain the model in several steps, first on the RACE data (Lai et al., 2017) and then on SWAG (Zellers et al., 2018).	0
4504	43060	43060	D19-6007	Participants	7	103	2.0	3.0	For task 2, they do not conduct specific pretraining steps, but implement a range of simple rulebased answer verification strategies to verify the output of the model.	0
4505	43061	43061	D19-6007	Participants	8	104	2.0	3.0	• IIT-KGP (Sharma and Roychowdhury, 2019) present an ensemble of different pretrained language models, namely BERT and XLNet.	0
4506	43062	43062	D19-6007	Participants	9	105	3.0	3.0	Both models are pretrained on the RACE data (Lai et al., 2017), and their output is averaged for a final prediction.	0
4507	43063	43063	D19-6007	Participants	10	106	3.0	3.0	• BLCU-NLP	0
4508	43064	43064	D19-6007	Participants	11	107	3.0	3.0	(Liu et al., 2019) use a Transformer model based on BERT, which is finetuned in two stages: they first tune the BERTbased language model on the RACE and ReCoRD datasets and then (further) train the model for the actual machine comprehension task.	0
4509	43065	43065	D19-6007	Participants	12	108	3.0	3.0	• JDA (Da, 2019) use three different knowledge bases, namely ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019) and Web-Child (Tandon et al., 2017).	0
4510	43066	43066	D19-6007	Participants	13	109	4.0	3.0	They extract relevant edges from the knowledge bases and compute relation embeddings, which are combined with BERT-based word representations with a diadic multiplication operation.	0
4511	43067	43067	D19-6007	Participants	14	110	4.0	3.0	• KARNA (Jain and Singh, 2019) use a BERT model, but they enhance the text representation with edges that are extracted from Con-ceptNet.	0
4512	43068	43068	D19-6007	Participants	15	111	4.0	3.0	Following Wang et al. (2018), they extract relations between words in the text and the question/answer, and append them to the text representation.	0
4513	43069	43069	D19-6007	Participants	16	112	4.0	3.0	Instead of computing relational embeddings, they append a specific string that describes the relation.	0
4514	43070	43070	D19-6007	Results	1	113	1.0	3.0	Table 3 shows the performance of the participating systems and the baselines on the task 1 data.	0
4515	43071	43071	D19-6007	Results	2	114	1.0	3.0	We tested for significance using a pairwise approximate randomization test (Yeh, 2000) over questions.	0
4516	43072	43072	D19-6007	Results	3	115	1.0	3.0	Except for the two top scoring systems, each system performs significantly better than the next in rank.	0
4517	43073	43073	D19-6007	Results	4	116	1.0	3.0	All systems significantly outperform the baselines.	0
4518	43074	43074	D19-6007	Results	5	117	2.0	3.0	All systems show a lower performance on commonsense-based questions as compared to the average on all questions, with the difference for the two top-scoring systems being smallest.	0
4519	43075	43075	D19-6007	Results	6	118	2.0	3.0	Surprisingly, all models are able to perform better on the questions from held-out scenarios as compared to their performance on all questions.	0
4520	43076	43076	D19-6007	Results	7	119	2.0	3.0	This indicates that all models are able to generalize well from the training material.	0
4521	43077	43077	D19-6007	Results	8	120	2.0	3.0	Table 5 shows the systems' performance on single question types for task 1.	0
4522	43078	43078	D19-6007	Results	9	121	3.0	3.0	Question types are determined automatically, as described in (Ostermann et al., 2019).	0
4523	43079	43079	D19-6007	Results	10	122	3.0	3.0	As can be seen, both topscoring systems perform well over all different question types, indicating that both systems are able to model a wide range of phenomena.	0
4524	43080	43080	D19-6007	Results	11	123	3.0	3.0	Interestingly, when questions seem to be the most challenging question type for all systems, indicating difficulties when it comes to model event ordering information.	0
4525	43081	43081	D19-6007	Results	12	124	3.0	3.0	Also, where questions seem to be challenging, at least for some systems.	0
4526	43082	43082	D19-6007	Results	13	125	4.0	3.0	Table 4 shows EM (%) and F 1 (%) of human performance, the PSH-SJTU system as well as baselines on the development and test sets of task 2.	0
4527	43083	43083	D19-6007	Results	14	126	4.0	3.0	Compared with the best baseline, KT-NET (Yang et al., 2019a), PSH-SJTU achieves significantly better scores.	0
4528	43084	43084	D19-6007	Results	15	127	4.0	3.0	On the hidden test set, they improve EM by 10.08%, and F 1 by 8.98%.	0
4529	43085	43085	D19-6007	Results	16	128	4.0	3.0	Consequently, PSH-SJTU has reduced the gap between human and machine performance, with human performance being only 8% higher than PSH-SJTU.	0
4530	43086	43086	D19-6007	Discussion	1	129	1.0	3.0	Pretrained Transformer language models.	0
4531	43087	43087	D19-6007	Discussion	2	130	1.0	3.0	A main finding of our shared tasks is that large pretrained Transformer language models such as BERT or XLNet perform well even on challenging commonsense inference data.	0
4532	43088	43088	D19-6007	Discussion	3	131	1.0	3.0	Strikingly, all models generalize well, as can be seen from the good performance on held-out scenarios.	0
4533	43089	43089	D19-6007	Discussion	4	132	1.0	3.0	On task 1, XLNet-based systems perform best.	0
4534	43090	43090	D19-6007	Discussion	5	133	1.0	3.0	The difference to the models purely based on BERT  can mostly be attributed to the performance on commonsense-based questions:	0
4535	43091	43091	D19-6007	Discussion	6	134	2.0	3.0	While the performance of XLNet-based models on such questions is almost on par with their average performance, models based on BERT underperform on commonsense questions.	0
4536	43092	43092	D19-6007	Discussion	7	135	2.0	3.0	An interesting observation was made by , who found that including WordNet into a BERT model boosts performance, while there is no such boost for an XL-Net model.	0
4537	43093	43093	D19-6007	Discussion	8	136	2.0	4.0	This seems to indicate that XLNet is able to cover (at least partially) some form of lexical background knowledge, as encoded in Word-Net, without explicitly requiring access to such a resource.	0
4538	43094	43094	D19-6007	Discussion	9	137	2.0	4.0	Still, when inspecting questions that were not answered correctly by the best scoring model, we found a large number of commonsense-based when questions that ask for the typical order of events.	0
4539	43095	43095	D19-6007	Discussion	10	138	2.0	4.0	This indicates that XLNet-based models are only to a certain extent able to model complex phenomena such as temporal order.	0
4540	43096	43096	D19-6007	Discussion	11	139	3.0	4.0	Commonsense knowledge databases.	0
4541	43097	43097	D19-6007	Discussion	12	140	3.0	4.0	Only two participants made use of commonsense knowledge, in the form of knowledge graphs such as ConceptNet.	0
4542	43098	43098	D19-6007	Discussion	13	141	3.0	4.0	Both participants conducted ablation tests indicating the importance of including commonsense knowledge.	0
4543	43099	43099	D19-6007	Discussion	14	142	3.0	4.0	In comparison to ATOMIC and WebChild, Da (2019) report that ConceptNet is most beneficial for performance on the task 1 data, which can be explained with its domain:	0
4544	43100	43100	D19-6007	Discussion	15	143	3.0	4.0	The OMCS (Singh et al., 2002) data are part of the ConceptNet database, and OMCS scenarios were also used to collect texts for the task 1 data.	0
4545	43101	43101	D19-6007	Discussion	16	144	4.0	4.0	All in all, powerful pretrained models such as XLNet still outperform approaches that make use of structured knowledge bases, which indicates that they are (at least to some extent) capable of performing commonsense inference without explicit representations of commonsense knowledge.	0
4546	43102	43102	D19-6007	Discussion	17	145	4.0	4.0	Pretraining and finetuning on other data.	0
4547	43103	43103	D19-6007	Discussion	18	146	4.0	4.0	Several participants reported effects of pretraining/finetuning their models on related tasks.	0
4548	43104	43104	D19-6007	Discussion	19	147	4.0	4.0	For instance, Liu et al. (2019) experimented with different pretraining corpora and found results to be best when pretraining the encoder of their BERT model on RACE and ReCoRD.	0
4549	43105	43105	D19-6007	Discussion	20	148	4.0	4.0	Similarly,  report improved results when using larger data sets from other reading comprehension (RACE) and commonsense inference tasks (SWAG) for training before fine-tuning the model with the actual training data from the shared task.	0
4550	43106	43106	D19-6007	Related Work	1	149	1.0	4.0	Evaluating commonsense inference via machine comprehension has recently moved into the focus of interest.	0
4551	43107	43107	D19-6007	Related Work	2	150	1.0	4.0	Existing datasets cover various domains:	0
4552	43108	43108	D19-6007	Related Work	3	151	1.0	4.0	Web texts.	0
4553	43109	43109	D19-6007	Related Work	4	152	1.0	4.0	Trivia	0
4554	43110	43110	D19-6007	Related Work	5	153	1.0	4.0	QA (Joshi et al., 2017) is a corpus of webcrawled trivia and quiz-league websites together with evidence documents from the web.	0
4555	43111	43111	D19-6007	Related Work	6	154	2.0	4.0	A large part of questions requires a system to make use of factual commonsense knowledge for finding an answer.	0
4556	43112	43112	D19-6007	Related Work	7	155	2.0	4.0	Commonsense	0
4557	43113	43113	D19-6007	Related Work	8	156	2.0	4.0	QA (Talmor et al., 2018) consists of 9,000 crowdsourced multiplechoice questions with a focus on relations between entities that appear in ConceptNet (Speer et al., 2017).	0
4558	43114	43114	D19-6007	Related Work	9	157	2.0	4.0	Evidence documents were webcrawled based on the question and added after the crowdsourcing step.	0
4559	43115	43115	D19-6007	Related Work	10	158	2.0	4.0	Fictive texts.	0
4560	43116	43116	D19-6007	Related Work	11	159	2.0	4.0	Narrative	0
4561	43117	43117	D19-6007	Related Work	12	160	3.0	4.0	QA (Kočiský et al., 2018) provides full novels and other long texts as evidence documents and contains approx.	0
4562	43118	43118	D19-6007	Related Work	13	161	3.0	4.0	30 crowdsourced questions per text.	0
4563	43119	43119	D19-6007	Related Work	14	162	3.0	4.0	The questions require a system to understand the whole plot of the text and to conduct many successive complicated inference steps, under the use of various types of background knowledge.	0
4564	43120	43120	D19-6007	Related Work	15	163	3.0	4.0	News texts.	0
4565	43121	43121	D19-6007	Related Work	16	164	3.0	4.0	News	0
4566	43122	43122	D19-6007	Related Work	17	165	4.0	4.0	QA (Trischler et al., 2017) provides news texts with crowdsourced questions and answers, which are spans of the evidence documents.	0
4567	43123	43123	D19-6007	Related Work	18	166	4.0	4.0	The question collection procedure for NewsQA resulted in a large number of questions that require factual commonsense knowledge for finding an answer.	0
4568	43124	43124	D19-6007	Related Work	19	167	4.0	4.0	Other tasks.	0
4569	43125	43125	D19-6007	Related Work	20	168	4.0	4.0	There have been other attempts at evaluating commonsense inference apart from machine comprehension.	0
4570	43126	43126	D19-6007	Related Work	21	169	4.0	4.0	One example is the Story cloze test and the ROC dataset (Mostafazadeh et al., 2016), where systems have to find the correct ending to a 5-sentence story, using different types of commonsense knowledge.	0
4571	43127	43127	D19-6007	Related Work	22	170	4.0	4.0	SWAG (Zellers et al., 2018) is a natural language inference dataset with a focus on difficult commonsense inferences.	0
4572	43128	43128	D19-6007	Conclusion	1	171	1.0	4.0	This report presented the results of the shared tasks at the Workshop for Commonsense Inference in NLP (COIN).	0
4573	43129	43129	D19-6007	Conclusion	2	172	1.0	4.0	The tasks aimed at evaluating the capability of systems to make use of commonsense knowledge for challenging inference questions in a machine comprehension setting, on everyday narrations (task 1) and news texts (task 2).	0
4574	43130	43130	D19-6007	Conclusion	3	173	2.0	4.0	In total, 5 systems participated in task 1, and one system participated in task 2.	0
4575	43131	43131	D19-6007	Conclusion	4	174	2.0	4.0	All submitted models were Transformer models, pretrained with a language modeling objective on large amounts of textual data.	0
4576	43132	43132	D19-6007	Conclusion	5	175	2.0	4.0	The best system achieved 90.6% accuracy and 83.7% F1-score on task 1 and 2, respectively, leaving a gap of 7% and 8% to human performance.	0
4577	43133	43133	D19-6007	Conclusion	6	176	3.0	4.0	The results of our shared tasks suggest that existing models cover a large part of the commonsense knowledge required for our data sets in the domains of narrations and news texts.	0
4578	43134	43134	D19-6007	Conclusion	7	177	3.0	4.0	This does however not mean that commonsense inference is solved:	0
4579	43135	43135	D19-6007	Conclusion	8	178	4.0	4.0	We found a range of examples in our data that are not successfully covered.	0
4580	43136	43136	D19-6007	Conclusion	9	179	4.0	4.0	Furthermore, data sets such as HellaSWAG (Zellers et al., 2019) show that commonsense inference tasks can be specifically tailored to be hard for Transformer models.	0
4581	43137	43137	D19-6007	Conclusion	10	180	4.0	4.0	We believe that modeling true language understanding requires a shift towards text types and tasks that test commonsense knowledge go-ing beyond information that can be obtained by exploiting the redundancy of large-scale corpora and/or pretraining on related tasks.	0
4582	45218	45218	I17-4002	title	1	1	4.0	1.0	IJCNLP-2017 Task 2: Dimensional Sentiment Analysis for Chinese Phrases	0
4583	45219	45219	I17-4002	abstract	1	2	1.0	1.0	This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions.	0
4584	45220	45220	I17-4002	abstract	2	3	2.0	1.0	Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm.	0
4585	45221	45221	I17-4002	abstract	3	4	3.0	1.0	Of the 19 teams registered for this shared task for twodimensional sentiment analysis, 13 submitted results.	0
4586	45222	45222	I17-4002	abstract	4	5	4.0	1.0	We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing.	0
4587	45223	45223	I17-4002	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring script are made publicly available to researchers.	0
4588	45224	45224	I17-4002	Introduction	1	7	1.0	1.0	Sentiment analysis has emerged as a leading technique to automatically identify affective information within texts.	0
4589	45225	45225	I17-4002	Introduction	2	8	1.0	1.0	In sentiment analysis, affective states are generally represented using either categorical or dimensional approaches (Calvo and Kim, 2013).	0
4590	45226	45226	I17-4002	Introduction	3	9	1.0	1.0	The categorical approach represents affective states as several discrete classes (e.g., positive, negative, neutral), while the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1.	0
4591	45227	45227	I17-4002	Introduction	4	10	1.0	1.0	The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm.	0
4592	45228	45228	I17-4002	Introduction	5	11	2.0	1.0	Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011;	0
4593	45229	45229	I17-4002	Introduction	6	12	2.0	1.0	Malandrakis et al., 2011; or texts (Kim et al., 2010;	0
4594	45230	45230	I17-4002	Introduction	7	13	2.0	1.0	Paltoglou et al, 2013;	0
4595	45231	45231	I17-4002	Introduction	8	14	2.0	1.0	Wang et al., 2016b).	0
4596	45232	45232	I17-4002	Introduction	9	15	2.0	1.0	Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014)	0
4597	45233	45233	I17-4002	Introduction	10	16	3.0	1.0	The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing.	0
4598	45234	45234	I17-4002	Introduction	11	17	3.0	1.0	Sentiment lexicons with valence-arousal ratings are useful resources for the development of dimensional sentiment applications.	0
4599	45235	45235	I17-4002	Introduction	12	18	3.0	1.0	Due to the limited availability of such VA lexicons, especially for Chinese, the objective of the task is to automatically acquire the valence-arousal ratings of Chinese affective words and phrases.	0
4600	45236	45236	I17-4002	Introduction	13	19	3.0	1.0	The rest of this paper is organized as follows.	0
4601	45237	45237	I17-4002	Introduction	14	20	4.0	2.0	Section II describes the task in detail.	0
4602	45238	45238	I17-4002	Introduction	15	21	4.0	2.0	Section III introduces the constructed datasets.	0
4603	45239	45239	I17-4002	Introduction	16	22	4.0	2.0	Section IV proposes evaluation metrics.	0
4604	45240	45240	I17-4002	Introduction	17	23	4.0	2.0	Section V reports the results of the participants' approaches.	0
4605	45241	45241	I17-4002	Introduction	18	24	4.0	2.0	Conclusions are finally drawn in Section VI.	0
4606	45242	45242	I17-4002	Task Description	1	25	1.0	2.0	This task seeks to evaluate the capability of systems for predicting dimensional sentiments of Chinese words and phrases.	0
4607	45243	45243	I17-4002	Task Description	2	26	1.0	2.0	For a given word or phrase, participants were asked to provide a realvalued score from 1 to 9 for both the valence and arousal dimensions, respectively indicating the degree from most negative to most positive for valence, and from most calm to most excited for arousal.	1
4608	45244	45244	I17-4002	Task Description	3	27	1.0	2.0	"The input format is ""term_id, term"", and the output format is ""term_id, valence_rating, arousal_rating""."	0
4609	45245	45245	I17-4002	Task Description	4	28	1.0	2.0	"Below are the input/output formats of the example words ""好"" (good), ""非常好"" (very good), ""滿意"" (satisfy) and ""不滿意"" (not satisfy)."	0
4610	45246	45246	I17-4002	Task Description	5	29	2.0	2.0	with valence-arousal ratings.	0
4611	45247	45247	I17-4002	Task Description	6	30	2.0	2.0	For multi-word phrases, we first selected a set of modifiers such as negators (e.g., not), degree adverbs (e.g., very) and modals (e.g., would).	0
4612	45248	45248	I17-4002	Task Description	7	31	2.0	2.0	These modifiers were combined with the affective words in CVAW to form multi-word phrases.	0
4613	45249	45249	I17-4002	Task Description	8	32	2.0	2.0	The frequency of each phrase was then retrieved from a large web-based corpus.	0
4614	45250	45250	I17-4002	Task Description	9	33	2.0	2.0	Only phrases with a frequency greater than or equal to 3 were retained as candidates.	0
4615	45251	45251	I17-4002	Task Description	10	34	3.0	2.0	To avoid several modifiers dominating the whole dataset, each modifier (or modifier combination) can have at most 50 phrases.	0
4616	45252	45252	I17-4002	Task Description	11	35	3.0	2.0	In addition, the phrases were selected to maximize the balance between positive and negative words.	0
4617	45253	45253	I17-4002	Task Description	12	36	3.0	2.0	Finally, a total of 3,000 phrases were collected by excluding unusual and semantically incomplete candidate phrases, of which 2,250 phrases were randomly selected as the training set according to the proportions of each modifier (or modifier combination) in the original set, and the remaining 750 phrases were used as the test set.	0
4618	45254	45254	I17-4002	Task Description	13	37	3.0	2.0	Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words .	0
4619	45255	45255	I17-4002	Task Description	14	38	3.0	2.0	Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth.	0
4620	45256	45256	I17-4002	Task Description	15	39	4.0	3.0	Each multi-word phrase was rated by at least 10 different annotators.	0
4621	45257	45257	I17-4002	Task Description	16	40	4.0	3.0	Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations.	0
4622	45258	45258	I17-4002	Task Description	17	41	4.0	3.0	They were then excluded from the calculation of the average ratings for each phrase.	0
4623	45259	45259	I17-4002	Task Description	18	42	4.0	3.0	The policy of this shared task was implemented as is an open test.	0
4624	45260	45260	I17-4002	Task Description	19	43	4.0	3.0	That is, in addition to the above official datasets, participating teams were allowed to use other publicly available data for system development, but such sources should be specified in the final technical report.	0
4625	45261	45261	I17-4002	Evaluation Metrics	1	44	1.0	3.0	Prediction performance is evaluated by examining the difference between machine-predicted ratings and human-annotated ratings, in which valence and arousal are treated independently.	0
4626	45262	45262	I17-4002	Evaluation Metrics	2	45	2.0	3.0	The evaluation metrics include Mean Absolute Error (MAE)	0
4627	45263	45263	I17-4002	Evaluation Metrics	3	46	2.0	3.0	where	0
4628	45264	45264	I17-4002	Evaluation Metrics	4	47	3.0	3.0	Ai is the actual value, Pi is the predicted value, n is the number of test samples, A and P respectively denote the arithmetic mean of A and P, and σ is the standard deviation.	0
4629	45265	45265	I17-4002	Evaluation Metrics	5	48	4.0	3.0	The MAE measures the error rate and the PCC measures the linear correlation between the actual values and the predicted values.	0
4630	45266	45266	I17-4002	Evaluation Metrics	6	49	4.0	3.0	A lower MAE and a higher PCC indicate more accurate prediction performance.	0
4631	45267	45267	I17-4002	Evaluation Results	1	50	1.0	3.0	Participants	0
4632	45268	45268	I17-4002	Evaluation Results	2	51	1.0	3.0	Baseline	0
4633	45269	45269	I17-4002	Evaluation Results	3	52	2.0	3.0	We implemented a baseline by training a linear regression model using word vectors as the only features.	0
4634	45270	45270	I17-4002	Evaluation Results	4	53	2.0	3.0	For single words, the regression was implemented by directly training word vectors to determine VA scores.	0
4635	45271	45271	I17-4002	Evaluation Results	5	54	2.0	3.0	Given a word wi, the baseline regression model is defined as ( ) ( )	0
4636	45272	45272	I17-4002	Evaluation Results	6	55	3.0	3.0	where Valwi and Arowi respectively denote the valence and arousal ratings of wi.	0
4637	45273	45273	I17-4002	Evaluation Results	7	56	3.0	3.0	W and b respec-tively denote the weights and bias.	0
4638	45274	45274	I17-4002	Evaluation Results	8	57	3.0	3.0	For phrases, we first calculate the mean vector of the constituent words in the phrase, considering each modifier word can also obtain its word vector.	0
4639	45275	45275	I17-4002	Evaluation Results	9	58	4.0	4.0	Give a phrase pj, its representation can be obtained by, where wi∈pj is the word in phrase pj.	0
4640	45276	45276	I17-4002	Evaluation Results	10	59	4.0	4.0	The regression was then trained using vec(pj) as a feature, defined as ( ) ( )	0
4641	45277	45277	I17-4002	Evaluation Results	11	60	4.0	4.0	The word vectors were trained on the Chinese Wiki Corpus 2 using the CBOW model of word2vec 3 (Mikolov et al., 2013a;2013b) (di-mensionality=300 and window size=5).	0
4642	45278	45278	I17-4002	Results	1	61	1.0	4.0	Tables 2 shows the results of valence-arousal prediction for single words.	0
4643	45279	45279	I17-4002	Results	2	62	1.0	4.0	The three best performing systems are summarized as follows.	0
4644	45280	45280	I17-4002	Results	3	63	1.0	4.0	Tables 3 shows the results of valence-arousal prediction for multi-word phrases.	0
4645	45281	45281	I17-4002	Results	4	64	2.0	4.0	The three best performing systems are summarized as follows.	0
4646	45282	45282	I17-4002	Results	5	65	2.0	4.0	Table 4 shows the overall results for both single words and multi-word phrases.	0
4647	45283	45283	I17-4002	Results	6	66	2.0	4.0	We rank the MAE and PCC independently and calculate the mean rank (average of MAE rank and PCC rank) for ordering system performance.	0
4648	45284	45284	I17-4002	Results	7	67	3.0	4.0	The three best performing systems are THU_NGN, AL_I_NLP and CKIP.	0
4649	45285	45285	I17-4002	Results	8	68	3.0	4.0	Table 5 summarizes the approaches for each participating system.	0
4650	45286	45286	I17-4002	Results	9	69	3.0	4.0	CASIA, SAM and XMUT did not submit reports on their developed methods.	0
4651	45287	45287	I17-4002	Results	10	70	4.0	4.0	Nearly all teams used word embeddings.	0
4652	45288	45288	I17-4002	Results	11	71	4.0	4.0	The most commonly used word embeddings were word2vec (Mikolov et al., 2013a;2013b) and GloVe (Pennington et al., 2014).	0
4653	45289	45289	I17-4002	Results	12	72	4.0	4.0	Others included FastText 4 (Bojanowski et al., 2017), characterenhanced word embedding (Chen et al., 2015) and Cw2vec (Cao et al., 2017	0
4654	45290	45290	I17-4002	Conclusions	1	73	1.0	4.0	This study describes an overview of the IJCNLP 2017 shared task on dimensional sentiment analysis for Chinese phrases, including task design, data preparation, performance metrics, and evaluation results.	0
4655	45291	45291	I17-4002	Conclusions	2	74	2.0	4.0	Regardless of actual performance, all submissions contribute to the common effort to develop dimensional approaches for affective computing, and the individual report in the proceedings provide useful insights into Chinese sentiment analysis.	0
4656	45292	45292	I17-4002	Conclusions	3	75	3.0	4.0	We hope the data sets collected and annotated for this shared task can facilitate and expedite future development in this research area.	0
4657	45293	45293	I17-4002	Conclusions	4	76	4.0	4.0	Therefore, all data sets with gold standard and scoring script are publicly available 5 .	0
4658	46115	46115	W11-1802	title	1	1	1.0	1.0	Overview of Genia Event Task in BioNLP Shared Task 2011	0
4659	46116	46116	W11-1802	abstract	1	2	2.0	1.0	The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.	1
4660	46117	46117	W11-1802	abstract	2	3	3.0	1.0	As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.	0
4661	46118	46118	W11-1802	abstract	3	4	4.0	1.0	After a 3-month system development period, 15 teams submitted their performance results on test cases.	0
4662	46119	46119	W11-1802	abstract	4	5	1.0	1.0	The results show the community has made a significant advancement in terms of both performance improvement and generalization.	0
4663	46120	46120	W11-1802	Introduction	1	6	1.0	1.0	The BioNLP Shared Task (BioNLP-ST, hereafter) is a series of efforts to promote a communitywide collaboration towards fine-grained information extraction (IE) in biomedical domain.	0
4664	46121	46121	W11-1802	Introduction	2	7	1.0	1.0	The first event, BioNLP-ST 2009, introducing a biomolecular event (bio-event) extraction task to the community, attracted a wide attention, with 42 teams being registered for participation and 24 teams submitting final results (Kim et al., 2009).	0
4665	46122	46122	W11-1802	Introduction	3	8	1.0	1.0	To establish a community effort, the organizers provided the task definition, benchmark data, and evaluations, and the participants competed in developing systems to perform the task.	0
4666	46123	46123	W11-1802	Introduction	4	9	2.0	1.0	Meanwhile, participants and organizers communicated to develop a better setup of evaluation, and some provided their tools and resources for other participants, making it a collaborative competition.	0
4667	46124	46124	W11-1802	Introduction	5	10	2.0	1.0	The final results enabled to observe the state-ofthe-art performance of the community on the bioevent extraction task, which showed that the automatic extraction of simple events -those with unary arguments, e.g. gene expression, localization, phosphorylation -could be achieved at the performance level of 70% in F-score, but the extraction of complex events, e.g. binding and regulation, was a lot more challenging, having achieved 40% of performance level.	0
4668	46125	46125	W11-1802	Introduction	6	11	2.0	1.0	After BioNLP-ST 2009, all the resources from the event were released to the public, to encourage continuous efforts for further advancement.	0
4669	46126	46126	W11-1802	Introduction	7	12	2.0	1.0	Since then, several improvements have been reported (Miwa et al., 2010b;	0
4670	46127	46127	W11-1802	Introduction	8	13	2.0	1.0	Poon and Vanderwende, 2010;Vlachos, 2010;Miwa et al., 2010a;	0
4671	46128	46128	W11-1802	Introduction	9	14	3.0	1.0	Björne et al., 2010).	0
4672	46129	46129	W11-1802	Introduction	10	15	3.0	1.0	For example, Miwa et al.	0
4673	46130	46130	W11-1802	Introduction	11	16	3.0	1.0	(Miwa et al., 2010b) reported a significant improvement with binding events, achieving 50% of performance level.	0
4674	46131	46131	W11-1802	Introduction	12	17	3.0	1.0	The task introduced in BioNLP-ST 2009 was renamed to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al., 2011).	0
4675	46132	46132	W11-1802	Introduction	13	18	3.0	1.0	As the sole task that was repeated in the two events, the GE task was referenced during the development of other tasks, and took the role of connecting the results of the 2009 event to the main tasks of 2011.	0
4676	46133	46133	W11-1802	Introduction	14	19	4.0	1.0	The GE task in 2011 received final submissions from 15 teams.	0
4677	46134	46134	W11-1802	Introduction	15	20	4.0	1.0	The results show the community made a significant progress with the task, and also show the technology can be generalized to full papers at moderate cost of performance.	0
4678	46135	46135	W11-1802	Introduction	16	21	4.0	1.0	This paper presents the task setup, preparation, and discusses the results.	0
4679	46136	46136	W11-1802	Introduction	17	22	4.0	1.0	1: Event types and their arguments for Genia event task.	0
4680	46137	46137	W11-1802	Introduction	18	23	4.0	1.0	The type of each filler entity is specified in parenthesis.	0
4681	46138	46138	W11-1802	Introduction	19	24	1.0	1.0	"Arguments that may be filled more than once per event are marked with ""+""."	0
4682	46139	46139	W11-1802	Task Definition	1	25	1.0	1.0	The GE task follows the task definition of BioNLP-ST 2009, which is briefly described in this section.	0
4683	46140	46140	W11-1802	Task Definition	2	26	1.0	2.0	For more detail, please refer to (Kim et al., 2009).	0
4684	46141	46141	W11-1802	Task Definition	3	27	1.0	2.0	Table 1 shows the event types to be addressed in the task.	0
4685	46142	46142	W11-1802	Task Definition	4	28	1.0	2.0	For each event type, the primary and secondary arguments to be extracted with an event are defined.	0
4686	46143	46143	W11-1802	Task Definition	5	29	2.0	2.0	For example, a Phosphorylation event is primarily extracted with the protein to be phosphorylated.	0
4687	46144	46144	W11-1802	Task Definition	6	30	2.0	2.0	As secondary information, the specific site to be phosphorylated may be extracted.	0
4688	46145	46145	W11-1802	Task Definition	7	31	2.0	2.0	From a computational point of view, the event types represent different levels of complexity.	0
4689	46146	46146	W11-1802	Task Definition	8	32	2.0	2.0	When only primary arguments are considered, the first five event types in Table 1 are classified as simple event types, requiring only unary arguments.	0
4690	46147	46147	W11-1802	Task Definition	9	33	2.0	2.0	The Binding and Regulation types are more complex: Binding requires detection of an arbitrary number of arguments, and Regulation requires detection of recursive event structure.	0
4691	46148	46148	W11-1802	Task Definition	10	34	2.0	2.0	Based on the definition of event types, the entire task is divided to three sub-tasks addressing event extraction at different levels of specificity: Task 1.	0
4692	46149	46149	W11-1802	Task Definition	11	35	3.0	2.0	Core event extraction addresses the extraction of typed events together with their primary arguments.	0
4693	46150	46150	W11-1802	Task Definition	12	36	3.0	2.0	Task 2. Event enrichment addresses the extraction of secondary arguments that further specify the events extracted in Task 1.	0
4694	46151	46151	W11-1802	Task Definition	13	37	3.0	2.0	Task 3. Negation/Speculation detection addresses the detection of negations and speculations over the extracted events.	0
4695	46152	46152	W11-1802	Task Definition	14	38	3.0	2.0	Task 1 serves as the backbone of the GE task and is mandatory for all participants, while the other two are optional.	0
4696	46153	46153	W11-1802	Task Definition	15	39	3.0	2.0	The annotation T1 identifies the entity referred to by the string (p65) between the character offsets, 15 and 18 to be a Protein.	0
4697	46154	46154	W11-1802	Task Definition	16	40	3.0	2.0	T2 identifies the string, translocation, to refer to a Localization event.	0
4698	46155	46155	W11-1802	Task Definition	17	41	4.0	2.0	Entities other than proteins or event type references are classified into a default class Entity, as in T3. E1 then represents the event defined by the three entities, as defined in Table 1.	0
4699	46156	46156	W11-1802	Task Definition	18	42	4.0	2.0	Note that for Task 1, the entity, T3, does not need to be identified, and the event, E1, may be identified without specification of the secondary argument, ToLoc:T1: E1' Localization:	0
4700	46157	46157	W11-1802	Task Definition	19	43	4.0	2.0	T2 Theme:	0
4701	46158	46158	W11-1802	Task Definition	20	44	4.0	2.0	T1	0
4702	46159	46159	W11-1802	Task Definition	21	45	4.0	2.0	Finding the full representation of E1 is the goal of Task 2.	0
4703	46160	46160	W11-1802	Task Definition	22	46	4.0	2.0	In the example, the localization event, E1, is negated as expressed in the failure of .	0
4704	46161	46161	W11-1802	Task Definition	23	47	1.0	2.0	Finding the negation, M1 is the goal of Task 3.	0
4705	46162	46162	W11-1802	Data preparation	1	48	1.0	2.0	The data sets are prepared in two collections: the abstract and the full text collections.	0
4706	46163	46163	W11-1802	Data preparation	2	49	2.0	2.0	The abstract collection includes the same data used for BioNLP-ST 2009, and is meant to be used to measure the progress of the community.	0
4707	46164	46164	W11-1802	Data preparation	3	50	2.0	2.0	The full text collection includes full papers which are newly annotated, and is meant to be used to measure the generalization of the technology to full papers.	0
4708	46165	46165	W11-1802	Data preparation	4	51	3.0	3.0	Table 2 shows the statistics of the annotations in the GE task data sets.	0
4709	46166	46166	W11-1802	Data preparation	5	52	3.0	3.0	Since the training data from the full text collection is relatively small despite of the expected rich variety of expressions in full text, it is expected that 'generalization' of a model from the abstract collection to full papers would be a key technique to get a reasonable performance.	0
4710	46167	46167	W11-1802	Data preparation	6	53	4.0	3.0	A full paper consists of several sections including the title, abstract, introduction, results, conclusion, methods, and so on.	0
4711	46168	46168	W11-1802	Data preparation	7	54	4.0	3.0	Different sections would be written with different purposes, which may affect the type of information that are found in the sections.	0
4712	46169	46169	W11-1802	Data preparation	8	55	1.0	3.0	Table 3	0
4713	46170	46170	W11-1802	Participation	1	56	1.0	3.0	In total, 15 teams submitted final results.	0
4714	46171	46171	W11-1802	Participation	2	57	1.0	3.0	All 15 teams participated in the mandatory Task 1, four teams in Task 2, and two teams in Task 3.	0
4715	46172	46172	W11-1802	Participation	3	58	2.0	3.0	Only one team, UTurku, completed all the three tasks.	0
4716	46173	46173	W11-1802	Participation	4	59	2.0	3.0	Table 4 shows the profile of the teams, excepting three who chose to remain anonymous.	0
4717	46174	46174	W11-1802	Participation	5	60	2.0	3.0	A brief examination on the team organization (the People column) suggests the importance of a computer science background, C and BI, to perform the GE task, which agrees with the same observation made in 2009.	0
4718	46175	46175	W11-1802	Participation	6	61	3.0	3.0	It is interpreted as follows: the role of computer scientists may be emphasized in part due to the fact that the task requires complex computational modeling, demanding particular efforts in framework design and implementation and computational resources.	0
4719	46176	46176	W11-1802	Participation	7	62	3.0	3.0	The '09 column suggests that previous experience in the task may have affected to the performance of the teams, especially in a complex task like the GE task.	0
4720	46177	46177	W11-1802	Participation	8	63	3.0	3.0	Table 5 shows the profile of the systems.	0
4721	46178	46178	W11-1802	Participation	9	64	4.0	3.0	A notable observation is that four teams developed their systems based on the model of UTurku09 (Björne et al., 2009) which was the winning sys-tem of BioNLP-ST 2009.	0
4722	46179	46179	W11-1802	Participation	10	65	4.0	3.0	It may show an influence of the BioNLP-ST series in the task.	0
4723	46180	46180	W11-1802	Participation	11	66	4.0	3.0	For syntactic analyses, the prevailing use of Charniak Johnson re-ranking parser (Charniak and Johnson, 2005) using the self-trained biomedical model from Mc-Closky (2008) (McCCJ) which is converted to Stanford Dependency (de Marneffe et al., 2006) is notable, which may also be an influence from the results of BioNLP-ST 2009.	0
4724	46181	46181	W11-1802	Participation	12	67	1.0	3.0	The last two teams, XABioNLP and HCMUS, who did not use syntactic analyses could not get a performance comparable to the others, which may suggest the importance of using syntactic analyses for a complex IE task like GE task.	0
4725	46182	46182	W11-1802	Results	1	68	1.0	3.0	Task 1	0
4726	46183	46183	W11-1802	Results	2	69	1.0	3.0	Table 6 shows the final evaluation results of Task 1.	0
4727	46184	46184	W11-1802	Results	3	70	1.0	3.0	For reference, the reported performance of the two systems, UTurku09 and Miwa10 is listed in the top.	0
4728	46185	46185	W11-1802	Results	4	71	2.0	3.0	UTurku09 was the winning system of Task 1 in 2009 (Björne et al., 2009), and Miwa10 was the best system reported after BioNLP-ST 2009 (Miwa et al., 2010b	0
4729	46186	46186	W11-1802	Results	5	72	2.0	3.0	The best performance in Task 1 this time is achieved by the FAUST system, which adopts a combination model of UMass and Stanford.	0
4730	46187	46187	W11-1802	Results	6	73	2.0	3.0	Its performance on the abstract collection, 56.04%, demonstrates a significant improvement of the community in the repeated GE task, when compared to both UTurku09, 51.95% and Miwa10, 53.29%.	0
4731	46188	46188	W11-1802	Results	7	74	2.0	3.0	The biggest improvement is made to the Regulation events (40.11%→46.97%) which requires a complex modeling for recursive event structure -an event may become an argument of another event.	0
4732	46189	46189	W11-1802	Results	8	75	2.0	3.0	The second ranked system, UMass, shows the best performance on the full paper collection.	0
4733	46190	46190	W11-1802	Results	9	76	3.0	4.0	It suggests that what FAUST obtained from the model combi-nation might be a better optimization to abstracts.	0
4734	46191	46191	W11-1802	Results	10	77	3.0	4.0	The Concord	0
4735	46192	46192	W11-1802	Results	11	78	3.0	4.0	U system is notable as it is the sole rule-based system that is ranked above the average.	0
4736	46193	46193	W11-1802	Results	12	79	3.0	4.0	It shows a performance optimized for precision with relatively low recall.	0
4737	46194	46194	W11-1802	Results	13	80	3.0	4.0	The same tendency is roughly replicated by other rule-based systems, CCP-BTMG, TM-SCS, XABioNLP, and HCMUS.	0
4738	46195	46195	W11-1802	Results	14	81	4.0	4.0	It suggests that a rule-based system might not be a good choice if a high coverage is desired.	0
4739	46196	46196	W11-1802	Results	15	82	4.0	4.0	However, the performance of Concord	0
4740	46197	46197	W11-1802	Results	16	83	4.0	4.0	U for simple events suggests that a high precision can be achieved by a rule based system with a modest loss of recall.	0
4741	46198	46198	W11-1802	Results	17	84	4.0	4.0	It might be more true when the task is less complex.	0
4742	46199	46199	W11-1802	Results	18	85	4.0	4.0	This time, three teams achieved better results than Miwa10, which indicates some role of focused efforts like BioNLP-ST.	0
4743	46200	46200	W11-1802	Results	19	86	1.0	4.0	The comparison between the performance on abstract and full paper collections shows that generalization to full papers is feasible with very modest loss in performance.	0
4744	46201	46201	W11-1802	Task 2	1	87	1.0	4.0	Tables 7 shows final evaluation results of Task 2.	0
4745	46202	46202	W11-1802	Task 2	2	88	2.0	4.0	For reference, the reported performance of the taskwinning system in 2009, UT+DBCLS09 (Riedel et al., 2009), is shown in the top.	0
4746	46203	46203	W11-1802	Task 2	3	89	2.0	4.0	The first and second ranked system, FAUST and UMass, which share a same author with Riedel09, made a significant improvement over Riedel09 in the abstract collection.	0
4747	46204	46204	W11-1802	Task 2	4	90	3.0	4.0	UTurku achieved the best performance in finding sites arguments but did not produce location arguments.	0
4748	46205	46205	W11-1802	Task 2	5	91	3.0	4.0	In table 7, the performance of all the systems in full text collection suggests that finding secondary arguments in full text is much more challenging.	0
4749	46206	46206	W11-1802	Task 2	6	92	4.0	4.0	In detail, a significant improvement was made for Location arguments (36.59%→50.00%).	0
4750	46207	46207	W11-1802	Task 2	7	93	4.0	4.0	A further breakdown of the results of site extraction, shown in table 8, shows that finding site arguments for Phosphorylation, Binding and Regulation events are all significantly improved, but in different ways.	0
4751	46208	46208	W11-1802	Task 2	8	94	1.0	4.0	The extraction of protein sites to be phosphorylated is approaching a practical level of performance (84.21%), while protein sites to be bound or to be regulated remains challenging to be extracted.	0
4752	46209	46209	W11-1802	Task 3	1	95	2.0	4.0	Table 9 shows final evaluation results of Task 3.	0
4753	46210	46210	W11-1802	Task 3	2	96	3.0	4.0	For reference, the reported performance of the taskwinning system in 2009, Kilicoglu09 (Kilicoglu and Bergler, 2009), is shown in the top.	0
4754	46211	46211	W11-1802	Task 3	3	97	4.0	4.0	Among the two teams participated in the task, UTurku showed a better performance in extracting negated events, while Concord	0
4755	46212	46212	W11-1802	Task 3	4	98	2.0	4.0	U showed a better performance in extracting speculated events.	0
4756	46213	46213	W11-1802	Conclusions	1	99	3.0	4.0	The Genia event task which was repeated for BioNLP-ST 2009 and 2011 took a role of measuring the progress of the community and generalization IE technology to full papers.	0
4757	46214	46214	W11-1802	Conclusions	2	100	4.0	4.0	The results from 15 teams who made their final submissions to the task show that a clear advance of the community in terms of the performance on a focused domain and also generalization to full papers.	0
4758	46215	46215	W11-1802	Conclusions	3	101	4.0	4.0	To our disappointment, however, an effective use of supporting task results was not observed, which thus remains as future work for further improvement.	0
