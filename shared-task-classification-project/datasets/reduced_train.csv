	Unnamed: 0	id	paper_id	headers	local_pos	global_pos	local_pct	global_pct	sentences	labels
0	123	123	S01-dutch	title	1	1	4.0	1.0	Dutch Word Sense Disambiguation: Data and Preliminary Results	1
1	124	124	S01-dutch	abstract	1	2	4.0	1.0	We describe the Dutch word sense disambiguation data submitted to SENSEVAL-2, and give preliminary results on the data using a WSD system based on memory-based learning and statistical keyword selection.	0
2	218	218	S01-english-allwords	title	1	1	4.0	1.0	English Tasks: All-Words and Verb Lexical Sample	1
3	219	219	S01-english-allwords	abstract	1	2	4.0	1.0	We describe our experience in preparing the lexicon and sense-tagged corpora used in the English all-words and lexical sample tasks of SENSEVAL-2.	0
4	544	544	S01-japanese-dictionary	Introduction	1	8	1.0	1.0	In SENSEVAL-2, there are two Japanese tasks, a translation task and a dictionary task.	0
5	545	545	S01-japanese-dictionary	Introduction	2	9	1.0	1.0	This paper describes the details of the dictionary task.	0
6	546	546	S01-japanese-dictionary	Introduction	3	10	1.0	1.0	First of all, let me introduce an overview of the Japanese dictionary task.	0
7	547	547	S01-japanese-dictionary	Introduction	4	11	2.0	1.0	This task is a lexical sample task.	0
8	548	548	S01-japanese-dictionary	Introduction	5	12	2.0	1.0	Word senses were defined according to the Iwanami Kokugo Jiten (Nishio et aL, 1994), a Japanese dictionary published by Iwanami Shoten.	0
9	549	549	S01-japanese-dictionary	Introduction	6	13	2.0	1.0	It was distributed to all participants as a sense inventory.	0
10	550	550	S01-japanese-dictionary	Introduction	7	14	2.0	1.0	Training data, a corpus consisting of 3,000 newspaper articles and manually annotated with sense IDs, was also distributed to participants.	0
11	551	551	S01-japanese-dictionary	Introduction	8	15	3.0	1.0	For evaluation, we distributed newspaper articles with marked target words as test documents.	0
12	552	552	S01-japanese-dictionary	Introduction	9	16	3.0	1.0	Participants were required to assign one or more sense IDs to each target word, optionally with associated probabilities.	1
13	553	553	S01-japanese-dictionary	Introduction	10	17	3.0	1.0	The number of target words was 100, 50 nouns and 50 verbs.	0
14	554	554	S01-japanese-dictionary	Introduction	11	18	3.0	1.0	One hundred instances of each target word were provided, making for a total of 10,000 instances.	0
15	555	555	S01-japanese-dictionary	Introduction	12	19	4.0	1.0	In what follows, Section 2 describes details of data used in the Japanese dictionary task.	0
16	556	556	S01-japanese-dictionary	Introduction	13	20	4.0	1.0	Section 3 describes the process to construct the 33 gold standard data, including the analysis of inter-tagger agreement.	0
17	557	557	S01-japanese-dictionary	Introduction	14	21	4.0	1.0	Section 4 briefly introduces participating systems and their results.	0
18	558	558	S01-japanese-dictionary	Introduction	15	22	4.0	1.0	Finally, Section 5 concludes this paper.	0
19	664	664	S01-japanese-translation	Introduction	1	8	1.0	1.0	In written texts, words which have multiple senses can be classified into two categories; homonyms and polysemous words.	0
20	665	665	S01-japanese-translation	Introduction	2	9	1.0	1.0	Generally speaking, while homonymy sense distinction is quite clear, polysemy sense distinction is very subtle and hard.	0
21	666	666	S01-japanese-translation	Introduction	3	10	1.0	1.0	English texts contain many homonyms.	0
22	667	667	S01-japanese-translation	Introduction	4	11	2.0	1.0	On the other hand, Japanese texts in which most content words are written by ideograms rarely contain homonyms.	0
23	668	668	S01-japanese-translation	Introduction	5	12	2.0	1.0	That is, the main target in Japanese WSD is polysemy, which makes Japanese WSD task setup very hard.	0
24	669	669	S01-japanese-translation	Introduction	6	13	2.0	1.0	What sense distinction of polysemous words is reasonable and effective heavily depends on how to use it, that is, an application ofWSD.	0
25	670	670	S01-japanese-translation	Introduction	7	14	2.0	1.0	Considering such a situation, in addition to the ordinary dictionary task we organized another task for Japanese, a translation task, in which word sense is defined according to translation distinction.	0
26	671	671	S01-japanese-translation	Introduction	8	15	3.0	1.0	Here, we set up the task assuming the example-based machine translation paradigm (Nagao, 1981).	0
27	672	672	S01-japanese-translation	Introduction	9	16	3.0	1.0	That is, first, a translation memory (TM) is constructed which contains, for each Japanese head word, a list of typical Japanese expressions (phrases/sentences) involving the head word and an English translation for each (Figure 1).	0
28	673	673	S01-japanese-translation	Introduction	10	17	3.0	1.0	We call a pair of Japanese and English expressions in the TM as a TM record.	0
29	674	674	S01-japanese-translation	Introduction	11	18	3.0	1.0	Given an evaluation document containing a target word, participants have to submit the TM record best approximating that usage.	1
30	675	675	S01-japanese-translation	Introduction	12	19	4.0	1.0	Alternatively, submissions can take the form of actual target word translations, or translations of phrases or sentences including each target word.	0
31	676	676	S01-japanese-translation	Introduction	13	20	4.0	1.0	This allows existing rule-based machine translation (MT) systems to participate in the task, and we can compare TM based systems with existing MT systems.	0
32	677	677	S01-japanese-translation	Introduction	14	21	4.0	1.0	For evaluation, we distributed newspaper articles.	0
33	678	678	S01-japanese-translation	Introduction	15	22	4.0	2.0	The number of target words was 40, and 30 instances of each target word were provided, making for a total of 1,200 instances.	0
34	1014	1014	S04-catalan	title	1	1	4.0	1.0	Senseval-3: The Catalan Lexical Sample Task	1
35	1015	1015	S04-catalan	abstract	1	2	1.0	1.0	Introduction	0
36	1016	1016	S04-catalan	abstract	2	3	2.0	1.0	In this paper we describe the Catalan Lexical Sample task.	0
37	1017	1017	S04-catalan	abstract	3	4	2.0	1.0	This task was initially devised for evaluating the role of unlabeled examples in supervised and semi-supervised learning systems for WSD and it is the counterpart of the Spanish Lexical Sample task.	0
38	1018	1018	S04-catalan	abstract	4	5	3.0	1.0	It was coordinated also with other lexical sample tasks (Basque, English, Italian, Rumanian, and Spanish) in order to share part of the target words.	0
39	1019	1019	S04-catalan	abstract	5	6	3.0	1.0	Firstly, we describe the methodology followed for developing the specific linguistic resources necessary for the task: the MiniDir-Cat lexicon and the MiniCors-Cat corpus.	0
40	1020	1020	S04-catalan	abstract	6	7	4.0	1.0	Secondly, we briefly describe the seven participant systems, the results obtained, and a comparative evaluation between them.	0
41	1021	1021	S04-catalan	abstract	7	8	4.0	1.0	All participant teams applied only pure supervised learning algorithms.	0
42	1138	1138	S04-english	Conclusion	1	44	2.0	4.0	The English lexical sample task in SENSEVAL-3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.	1
43	1139	1139	S04-english	Conclusion	2	45	3.0	4.0	The objective of this task was to: (1)	0
44	1140	1140	S04-english	Conclusion	3	46	4.0	4.0	Determine feasibility of reliably finding the	0
45	1196	1196	S04-english-hindi	abstract	1	2	2.0	1.0	This paper describes the English-Hindi Multilingual lexical sample task in SENSEVAL-3.	0
46	1197	1197	S04-english-hindi	abstract	2	3	3.0	1.0	Rather than tagging an English word with a sense from an English dictionary, this task seeks to assign the most appropriate Hindi translation to an ambiguous target word.	1
47	1198	1198	S04-english-hindi	abstract	3	4	4.0	1.0	Training data was solicited via the Open Mind Word Expert (OMWE) from Web users who are fluent in English and Hindi.	0
48	1289	1289	S04-italian	Introduction	1	6	1.0	1.0	The task consisted in automatically determining the correct meaning of a word within a given context (i.e. a short text snippet).	1
49	1290	1290	S04-italian	Introduction	2	7	2.0	1.0	Systems' results were compared on the one hand to those achieved by human annotators (upper bound), and on the other hand to those returned by a basic algorithm (baseline).	0
50	1291	1291	S04-italian	Introduction	3	8	3.0	1.0	In the second section of this paper an overview of the task preparation is given and in the following one the main features of the participating systems are briefly outlined and the results of the evaluation exercise are presented.	0
51	1292	1292	S04-italian	Introduction	4	9	4.0	1.0	In the conclusions we give an overall judgement of the outcome of the task, suggesting possible improvements for the next campaign.	0
52	1363	1363	S04-italian-allwords	abstract	1	2	2.0	1.0	This paper describes the Italian all-words sense disambiguation task for Senseval-3.	1
53	1364	1364	S04-italian-allwords	abstract	2	3	4.0	1.0	The annotation procedure and criteria together with the encoding of multiwords are presented.	0
54	1365	1365	S04-italian-allwords	Introduction	1	4	1.0	1.0	This paper describes the Italian all-words sense disambiguation task for Senseval-3: about 5000 words were manually disambiguated according to the ItalWordNet (IWN) word senses.	1
55	1366	1366	S04-italian-allwords	Introduction	2	5	2.0	1.0	The first section briefly describes of the corpus and the lexical reference resource.	0
56	1367	1367	S04-italian-allwords	Introduction	3	6	2.0	1.0	The second section contains some general criteria adopted for the annotation of the corpus and illustrated by a series of examples.	0
57	1368	1368	S04-italian-allwords	Introduction	4	7	3.0	1.0	Issues connected to the treatment of phenomena typically found in corpora, e.g. abbreviations, foreign words, jargon, locutions are discussed.	0
58	1369	1369	S04-italian-allwords	Introduction	5	8	3.0	1.0	Furthermore, the encoding of compounds, metaphorical usages, and multiword units is described.	0
59	1370	1370	S04-italian-allwords	Introduction	6	9	4.0	1.0	Problems connected with i) the high granularity of sense distinctions in the lexical resource and ii) unsolvable ambiguities of the contexts are dealt with.	0
60	1371	1371	S04-italian-allwords	Introduction	7	10	4.0	1.0	Finally, it is evidenced how the annotation exercise can be of help in updating or tuning IWN, by adding missing senses and/or entries.	0
61	1455	1455	S04-logic-form-id	Introduction	1	7	1.0	1.0	The goal of a Logic Form Identification (LFi) task is to evaluate the performance of different methods addressing the issue of LFi.	0
62	1456	1456	S04-logic-form-id	Introduction	2	8	2.0	1.0	The Logic Form (LF) that we use is a flat, scope-free first order logic representation that embeds lexical and syntactic information.	0
63	1457	1457	S04-logic-form-id	Introduction	3	9	3.0	1.0	Given a set of English sentences, participating systems were supposed to return the sentences in Logic Form as in the example below.	1
64	1458	1458	S04-logic-form-id	Introduction	4	10	4.0	1.0	The general approach adopted for evaluation was a gold standard approach in which the test data is first correctly mapped onto its corresponding LF by a team of experts and then this correct LF is automatically compared against outputs provided by participating sytems.	0
65	1531	1531	S04-romanian	title	1	1	4.0	1.0	An Evaluation Exercise for Romanian Word Sense Disambiguation	1
66	1532	1532	S04-romanian	abstract	1	2	2.0	1.0	This paper presents the task definition, resources, participating systems, and comparative results for a Romanian Word Sense Disambiguation task, which was organized as part of the SENSEVAL-3 evaluation exercise.	1
67	1533	1533	S04-romanian	abstract	2	3	4.0	1.0	Five teams with a total of seven systems were drawn to this task.	0
68	1607	1607	S04-semantic-roles	The Senseval-3 Task	1	18	1.0	1.0	This Senseval-3 task calls for the development of systems to meet the same objectives as the Gildea and Jurafsky study.	0
69	1608	1608	S04-semantic-roles	The Senseval-3 Task	2	19	1.0	1.0	The data for this task is a sample of the FrameNet hand-annotated data.	0
70	1609	1609	S04-semantic-roles	The Senseval-3 Task	3	20	1.0	1.0	Evaluation of systems is measured using precision and recall of frame elements and overlap of a system's frame element sentence positions with those identified in the FrameNet data.	0
71	1610	1610	S04-semantic-roles	The Senseval-3 Task	4	21	1.0	1.0	The basic task for Senseval-3 is: Given a sentence, a target word and its frame, identify the frame elements within that sentence and tag them with the appropriate frame element name.	1
72	1611	1611	S04-semantic-roles	The Senseval-3 Task	5	22	1.0	1.0	The Frame	0
73	1612	1612	S04-semantic-roles	The Senseval-3 Task	6	23	1.0	1.0	Net project has just released a major revision (FrameNet 1.1) to its database, with 487 frames using 696 distinctly-named frame elements (although it is not guaranteed that frame elements with the same name have the same meaning).	0
74	1613	1613	S04-semantic-roles	The Senseval-3 Task	7	24	1.0	1.0	This release includes 132,968 annotated sentences (mostly taken from the British National Corpus).	0
75	1614	1614	S04-semantic-roles	The Senseval-3 Task	8	25	1.0	1.0	The Senseval-3 task used 8,002 of these sentences selected randomly from 40 frames (also selected randomly) having at least 370 annotations (out of the 100 frames having the most annotations).	0
76	1615	1615	S04-semantic-roles	The Senseval-3 Task	9	26	1.0	2.0	1 Participants were provided with a training set that identified, for each of the 40 frames, the lexical unit identification number (which equates to a file name) and a sentence identification name.	0
77	1616	1616	S04-semantic-roles	The Senseval-3 Task	10	27	1.0	2.0	They were also provided with the answers, i.e., the frame element names and their beginning and ending positions.	0
78	1617	1617	S04-semantic-roles	The Senseval-3 Task	11	28	1.0	2.0	Since the training set was much larger than the test set, participants were required to use the FrameNet 1.1 dataset to obtain the full sentence, its target word, and the tagged frame elements.	0
79	1618	1618	S04-semantic-roles	The Senseval-3 Task	12	29	1.0	2.0	For the test data, participants were provided, for each frame, with sentence instances that identified the lexical unit, the lexical unit identification number, the sentence identification number, the full sentence, and a specification of the target along with its start and end positions.	0
80	1619	1619	S04-semantic-roles	The Senseval-3 Task	13	30	1.0	2.0	Participants were required to submit their answers in a text file, with one answer per line.	0
81	1620	1620	S04-semantic-roles	The Senseval-3 Task	14	31	2.0	2.0	Each line was to identify the frame name and the sentence identifier and then all the frame elements with their start and end positions that their systems were able to identify.	0
82	1621	1621	S04-semantic-roles	The Senseval-3 Task	15	32	2.0	2.0	For example, for the sentence However, its task is made much more difficult by the fact that derogations granted to the Welsh water authority allow &lt;	0
83	1622	1622	S04-semantic-roles	The Senseval-3 Task	16	33	2.0	2.0	Agent&gt;it&lt;/&gt; to &lt;Target&gt;pump&lt;/&gt; &lt;Fluid&gt;raw sewage&lt;/&gt; &lt;Goal&gt;into both those rivers&lt;/&gt;. the correct answer would appear as follows:	0
84	1623	1623	S04-semantic-roles	The Senseval-3 Task	17	34	2.0	2.0	Cause_fluidic_motion.256263 Agent (119,120) Fluid (130,139) Goal (141,162)	0
85	1624	1624	S04-semantic-roles	The Senseval-3 Task	18	35	2.0	2.0	The sentences provided to participants were not presegmented (as defined in the Gildea and Jurafsky study); this was left to the participants' systems.	0
86	1625	1625	S04-semantic-roles	The Senseval-3 Task	19	36	2.0	2.0	The FrameNet dataset contains considerable information that was tagged by the FrameNet lexicographers.	0
87	1626	1626	S04-semantic-roles	The Senseval-3 Task	20	37	2.0	2.0	Participants could use (and were strongly encouraged to use) any and all of the FrameNet data in developing and training their systems.	0
88	1627	1627	S04-semantic-roles	The Senseval-3 Task	21	38	2.0	2.0	In the test, participants could use any of this data, but were strongly encouraged to use only data available in the sentence itself and in the frame that is identified.	0
89	1628	1628	S04-semantic-roles	The Senseval-3 Task	22	39	2.0	2.0	"(This corresponds to the ""more difficult task"" identified by Gildea and Jurafsky.)"	0
90	1629	1629	S04-semantic-roles	The Senseval-3 Task	23	40	2.0	2.0	Participants could submit two runs, one with (non-restrictive case) and one without (restrictive case) using the additional data; these were scored separately.	0
91	1630	1630	S04-semantic-roles	The Senseval-3 Task	24	41	2.0	2.0	Frame	0
92	1631	1631	S04-semantic-roles	The Senseval-3 Task	25	42	2.0	2.0	"Net recognizes the permissibility of ""conceptually salient"" frame elements that have not been instantiated in a sentence; these are called null instantiations (see Johnson et al. for a fuller description)."	0
93	1632	1632	S04-semantic-roles	The Senseval-3 Task	26	43	2.0	2.0	"An example occurs in the following sentence (sentID=""1087911"") from the Motion frame: ""I went and stood in the sitting room doorway, but I couldn't get any further --my legs wouldn't move."""	0
94	1633	1633	S04-semantic-roles	The Senseval-3 Task	27	44	2.0	2.0	In this case, the FrameNet taggers considered the Path frame element to be an indefinite null instantiation (INI).	0
95	1634	1634	S04-semantic-roles	The Senseval-3 Task	28	45	3.0	2.0	Frame elements that have been so designated for a particular sentence appear to be Core frame elements, but not all core frame elements missing from a sentence have designated as null instantiations.	0
96	1635	1635	S04-semantic-roles	The Senseval-3 Task	29	46	3.0	2.0	The correct answer for this case, based on the tagging, is as follows:	0
97	1636	1636	S04-semantic-roles	The Senseval-3 Task	30	47	3.0	2.0	Motion.1087911	0
98	1637	1637	S04-semantic-roles	The Senseval-3 Task	31	48	3.0	2.0	Theme (82,88) Path (0,0)	0
99	1638	1638	S04-semantic-roles	The Senseval-3 Task	32	49	3.0	2.0	Participants were instructed to identify null instantiations in submissions by giving a (0,0) value for the frame element's position.	0
100	1639	1639	S04-semantic-roles	The Senseval-3 Task	33	50	3.0	2.0	2 Participants were told in the task description that null instantiations would be analyzed separately.	0
101	1640	1640	S04-semantic-roles	The Senseval-3 Task	34	51	3.0	3.0	3 For this Senseval task, participants were allowed to download the training data at any time; the 21-day 1 The test set was generated with the Windows-based program FrameNet Explorer, available at http://www.clres.com/SensSemRoles.html.	0
102	1641	1641	S04-semantic-roles	The Senseval-3 Task	35	52	3.0	3.0	FrameNet Explorer provides several facilities for examining the FrameNet data: by frame, frame element, and lexical units.	0
103	1642	1642	S04-semantic-roles	The Senseval-3 Task	36	53	3.0	3.0	For each unit, a user can explore a frame's elements, associated lexical units, frame-to-frame relations, frame and frame element definitions, lexical units and their definitions, and all sentences.	0
104	1643	1643	S04-semantic-roles	The Senseval-3 Task	37	54	3.0	3.0	restriction on submission of results after downloading the training data was waived since this is a new Senseval task and the dataset is very complex.	0
105	1644	1644	S04-semantic-roles	The Senseval-3 Task	38	55	3.0	3.0	Participants could work with the training data as long as they wished.	0
106	1645	1645	S04-semantic-roles	The Senseval-3 Task	39	56	3.0	3.0	The 7-day restriction of submitting results after downloading the test data still applied.	0
107	1646	1646	S04-semantic-roles	The Senseval-3 Task	40	57	3.0	3.0	In general, FrameNet frames contain many frame elements (perhaps an average of 10), most of which are not instantiated in a given sentence.	0
108	1647	1647	S04-semantic-roles	The Senseval-3 Task	41	58	4.0	3.0	Systems were not penalized if they returned more frame elements than those identified by the FrameNet taggers.	0
109	1648	1648	S04-semantic-roles	The Senseval-3 Task	42	59	4.0	3.0	For the 8002 sentences in the test set, only 16212 frame elements constituted the answer set.	0
110	1649	1649	S04-semantic-roles	The Senseval-3 Task	43	60	4.0	3.0	In scoring the runs, each frame element (not a null instantiation) returned by a system was counted as an item attempted.	0
111	1650	1650	S04-semantic-roles	The Senseval-3 Task	44	61	4.0	3.0	If the frame element was one that had been identified by the FrameNet taggers, the answer was scored as correct.	0
112	1651	1651	S04-semantic-roles	The Senseval-3 Task	45	62	4.0	3.0	In addition, however, the scoring program required that the frame boundaries identified by the system's answer had to overlap with the boundaries identified by FrameNet.	0
113	1652	1652	S04-semantic-roles	The Senseval-3 Task	46	63	4.0	3.0	An additional measure of system performance was the degree of overlap.	0
114	1653	1653	S04-semantic-roles	The Senseval-3 Task	47	64	4.0	3.0	If a system's answer coincided exactly to FrameNet's start and end position, the system received an overlap score of 1.0.	0
115	1654	1654	S04-semantic-roles	The Senseval-3 Task	48	65	4.0	3.0	If not, the overlap score was the number of characters overlapping divided by the length of the FrameNet start and end positions (i.e., end-start+1) 4	0
116	1655	1655	S04-semantic-roles	The Senseval-3 Task	49	66	4.0	3.0	The number attempted was the number of nonnull frame elements generated by a system.	0
117	1656	1656	S04-semantic-roles	The Senseval-3 Task	50	67	4.0	3.0	Precision was computed as the number of correct answers divided by the number attempted.	0
118	1657	1657	S04-semantic-roles	The Senseval-3 Task	51	68	4.0	3.0	Recall was computed as the number of correct answers divided by the number of frame elements in the test set.	0
119	1658	1658	S04-semantic-roles	The Senseval-3 Task	52	69	4.0	3.0	Overlap was the average overlap of all correct answers.	0
120	1659	1659	S04-semantic-roles	The Senseval-3 Task	53	70	4.0	3.0	The percent Attempted was the number of frame elements generated divided by the number of frame elements in the test set, multiplied by 100.	0
121	1660	1660	S04-semantic-roles	The Senseval-3 Task	54	71	4.0	3.0	If a system returned frame elements not identified in the test set, its precision would be lower.	0
122	1761	1761	S04-wordnet-glosses	abstract	1	2	1.0	1.0	The SENSEVAL-3 task to perform word-sense disambiguation of WordNet glosses was designed to encourage development of technology to make use of standard lexical resources.	1
123	1762	1762	S04-wordnet-glosses	abstract	2	3	2.0	1.0	The task was based on the availability of sensedisambiguated hand-tagged glosses created in the eXtended WordNet project.	0
124	1763	1763	S04-wordnet-glosses	abstract	3	4	2.0	1.0	"The hand-tagged glosses provided a ""gold standard"" for judging the performance of automated disambiguation systems."	0
125	1764	1764	S04-wordnet-glosses	abstract	4	5	3.0	1.0	Seven teams participated in the task, with a total of 10 runs.	0
126	1765	1765	S04-wordnet-glosses	abstract	5	6	4.0	1.0	"Scoring these runs as an ""all-words"" task, along with considerable discussions among participants, provided more insights than just the underlying technology."	0
127	1766	1766	S04-wordnet-glosses	abstract	6	7	4.0	1.0	The task identified several issues about the nature of the WordNet sense inventory and the underlying use of wordnet design principles, particularly the significance of WordNet-style relations.	0
128	1868	1868	S04-wsd-subcategorization-acq	Introduction	1	5	1.0	1.0	Gold standard evaluation approaches to evaluating word sense disambiguation (WSD) systems often suffer due to the choice of inventory.	0
129	1869	1869	S04-wsd-subcategorization-acq	Introduction	2	6	1.0	1.0	Fundamentally, the sense distinctions (e.g., in WordNet) tend to be very fine-grained which makes the disambiguation task highly difficult.	0
130	1870	1870	S04-wsd-subcategorization-acq	Introduction	3	7	1.0	1.0	Because the best level of sense granularity is likely to be application-dependent, a good alternative is to evaluate WSD systems in a task-based environment.	0
131	1871	1871	S04-wsd-subcategorization-acq	Introduction	4	8	2.0	1.0	We propose task-based evaluation in the context of automatic subcategorization frame (SCF) acquisition where the optimal sense granularity is fairly coarse.	1
132	1872	1872	S04-wsd-subcategorization-acq	Introduction	5	9	2.0	1.0	Automatic subcategorization acquisition is an important NLP task since access to a comprehensive anda ccurate subcategorization lexicon, acquired via automatic means, is vital e.g. for the development of successful parsing technology.	0
133	1873	1873	S04-wsd-subcategorization-acq	Introduction	6	10	2.0	1.0	It is also a suitable task for evaluation of WSD systems because SCF frequencies are known to vary with word senses from one corpus / text type to another.	0
134	1874	1874	S04-wsd-subcategorization-acq	Introduction	7	11	3.0	1.0	While most current systems for subcategorization acquisition are purely syntax-driven and do not employ WSD, Korhonun and Preiss (2003) have recently proposed a method which makes use of word sense.	0
135	1875	1875	S04-wsd-subcategorization-acq	Introduction	8	12	3.0	1.0	This method guides the acquisition process using back-off (i.e., probability) estimates based on verbs different senses in corpora.	0
136	1876	1876	S04-wsd-subcategorization-acq	Introduction	9	13	3.0	1.0	Where the senses are detected correctly, the method improves system performance considerably as the estimates help to correct the acquired SCF distribution and deal with sparse data.	0
137	1877	1877	S04-wsd-subcategorization-acq	Introduction	10	14	4.0	1.0	Our WSD evaluation makes use of this method.	0
138	1878	1878	S04-wsd-subcategorization-acq	Introduction	11	15	4.0	1.0	The paper is structured as follows: Section 2 describes the SCF acquisition system, and shows how the WSD answers can improve performance.	0
139	1879	1879	S04-wsd-subcategorization-acq	Introduction	12	16	4.0	2.0	The evaluation corpus and the evaluation method are introduced in Section 3.	0
140	1880	1880	S04-wsd-subcategorization-acq	Introduction	13	17	4.0	2.0	We present the effect of WSD accuracy on the performance of SCF acquisition in Section 4, and draw our conclusions in Section 5.	0
141	1959	1959	S07-1	Description of the task	1	34	1.0	1.0	This is an application-driven task, where the application is a fixed CLIR system.	0
142	1960	1960	S07-1	Description of the task	2	35	1.0	1.0	Participants disambiguate text by assigning WordNet 1.6 synsets and the system will do the expansion to other languages, index the expanded documents and run the retrieval for all the languages in batch.	1
143	1961	1961	S07-1	Description of the task	3	36	1.0	1.0	The retrieval results are taken as the measure for fitness of the disambiguation.	0
144	1962	1962	S07-1	Description of the task	4	37	1.0	1.0	The modules and rules for the expansion and the retrieval will be exactly the same for all participants.	0
145	1963	1963	S07-1	Description of the task	5	38	1.0	1.0	We proposed two specific subtasks:	0
146	1964	1964	S07-1	Description of the task	6	39	2.0	1.0	1. Participants disambiguate the corpus, the corpus is expanded to synonyms/translations and we measure the effects on IR/CLIR.	0
147	1965	1965	S07-1	Description of the task	7	40	2.0	1.0	Topics 2 are not processed.	0
148	1966	1966	S07-1	Description of the task	8	41	2.0	1.0	2. Participants disambiguate the topics per language, we expand the queries to synonyms/translations and we measure the effects on IR/CLIR.	0
149	1967	1967	S07-1	Description of the task	9	42	2.0	2.0	Documents are not processed	0
150	1968	1968	S07-1	Description of the task	10	43	2.0	2.0	The corpora and topics were obtained from the ad-hoc CLEF tasks.	0
151	1969	1969	S07-1	Description of the task	11	44	2.0	2.0	The supported languages in the topics are English and Spanish, but in order to limit the scope of the exercise we decided to only use English documents.	0
152	1970	1970	S07-1	Description of the task	12	45	3.0	2.0	The participants only had to disambiguate the English topics and documents.	0
153	1971	1971	S07-1	Description of the task	13	46	3.0	2.0	Note that most WSD systems only run on English text.	0
154	1972	1972	S07-1	Description of the task	14	47	3.0	2.0	Due to these limitations, we had the following evaluation settings: IR with WSD of topics , where the participants disambiguate the documents, the disambiguated documents are expanded to synonyms, and the original topics are used for querying.	0
155	1973	1973	S07-1	Description of the task	15	48	3.0	2.0	All documents and topics are in English.	0
156	1974	1974	S07-1	Description of the task	16	49	3.0	2.0	IR with WSD of documents , where the participants disambiguate the topics, the disambiguated topics are expanded and used for querying the original documents.	0
157	1975	1975	S07-1	Description of the task	17	50	4.0	2.0	All documents and topics are in English.	0
158	1976	1976	S07-1	Description of the task	18	51	4.0	2.0	CLIR with WSD of documents , where the participants disambiguate the documents, the disambiguated documents are translated, and the original topics in Spanish are used for querying.	0
159	1977	1977	S07-1	Description of the task	19	52	4.0	2.0	The documents are in English and the topics are in Spanish.	0
160	1978	1978	S07-1	Description of the task	20	53	4.0	2.0	We decided to focus on CLIR for evaluation, given the difficulty of improving IR.	0
161	1979	1979	S07-1	Description of the task	21	54	4.0	2.0	The IR results are given as illustration, and as an upperbound of the CLIR task.	0
162	1980	1980	S07-1	Description of the task	22	55	4.0	2.0	This use of IR results as a reference for CLIR systems is customary in the CLIR community (Harman, 2005).	0
163	2262	2262	S07-4	Task Description and Related Work	1	7	1.0	1.0	The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities -honey bee, for example, shows an instance of the Product-Producer relation.	1
164	2263	2263	S07-4	Task Description and Related Work	2	8	1.0	1.0	The classification occurs in the context of a sentence in a written English text.	0
165	2264	2264	S07-4	Task Description and Related Work	3	9	1.0	1.0	Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on.	0
166	2265	2265	S07-4	Task Description and Related Work	4	10	1.0	1.0	The recognition of textual entailment  is an example of successful use of this type of deeper analysis in high-end NLP applications.	0
167	2266	2266	S07-4	Task Description and Related Work	5	11	2.0	1.0	The literature shows a wide variety of methods of nominal relation classification.	0
168	2267	2267	S07-4	Task Description and Related Work	6	12	2.0	1.0	They depend as much on the training data as on the domain of application and the available resources.	0
169	2268	2268	S07-4	Task Description and Related Work	7	13	2.0	1.0	Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound.	0
170	2269	2269	S07-4	Task Description and Related Work	8	14	2.0	1.0	Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level.	0
171	2270	2270	S07-4	Task Description and Related Work	9	15	2.0	1.0	Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005;	0
172	2271	2271	S07-4	Task Description and Related Work	10	16	3.0	1.0	Turney, 2005;Nastase et al., 2006) have used their class scheme and data set.	0
173	2272	2272	S07-4	Task Description and Related Work	11	17	3.0	1.0	Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005).	0
174	2273	2273	S07-4	Task Description and Related Work	12	18	3.0	1.0	Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations.	0
175	2274	2274	S07-4	Task Description and Related Work	13	19	3.0	1.0	Stephens et al. (2001) propose 17 classes targeted to relations between genes.	0
176	2275	2275	S07-4	Task Description and Related Work	14	20	4.0	1.0	Lapata (2002) presents a binary classification of relations in nominalizations.	0
177	2276	2276	S07-4	Task Description and Related Work	15	21	4.0	1.0	There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications.	0
178	2277	2277	S07-4	Task Description and Related Work	16	22	4.0	1.0	For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text.	0
179	2278	2278	S07-4	Task Description and Related Work	17	23	4.0	1.0	We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms.	0
180	2279	2279	S07-4	Task Description and Related Work	18	24	4.0	1.0	We do not presume to propose a single classification scheme, however alluring it would	0
181	2362	2362	S07-5	abstract	1	2	2.0	1.0	The Multilingual Chinese-English lexical sample task at SemEval-2007 provides a framework to evaluate Chinese word sense disambiguation and to promote research.	1
182	2363	2363	S07-5	abstract	2	3	4.0	1.0	This paper reports on the task preparation and the results of six participants.	0
183	2443	2443	S07-5	Conclusion	1	83	1.0	4.0	The goal of this task is to create a framework to evaluate Chinese word sense disambiguation and to promote research.	1
184	2444	2444	S07-5	Conclusion	2	84	1.0	4.0	Together six teams participate in this WSD task, four of them adopt supervised learning methods and two of them used unsupervised algorithms.	0
185	2445	2445	S07-5	Conclusion	3	85	2.0	4.0	All of the four supervised learning systems exceed obviously the baseline obtained by the most frequent sense.	0
186	2446	2446	S07-5	Conclusion	4	86	2.0	4.0	It is noted that the performances of the first three systems are very close.	0
187	2447	2447	S07-5	Conclusion	5	87	2.0	4.0	Two unsupervised methods' scores are below the baseline.	0
188	2448	2448	S07-5	Conclusion	6	88	3.0	4.0	More unlabeled data maybe improve their performance.	0
189	2449	2449	S07-5	Conclusion	7	89	3.0	4.0	Although the SRCB-WSD system got the highest scores among the six participants, it does not perform always better than other system from table 2 and table 3.	0
190	2450	2450	S07-5	Conclusion	8	90	3.0	4.0	But to each word, the four supervised systems always predict correctly more instances than the two un-supervised systems.	0
191	2451	2451	S07-5	Conclusion	9	91	4.0	4.0	Besides the corpus, we provide a specification of the PoS tag set.	0
192	2452	2452	S07-5	Conclusion	10	92	4.0	4.0	Only SRCB-WSD system utilized this knowledge in feature selection.	0
193	2453	2453	S07-5	Conclusion	11	93	4.0	4.0	We will provide more instances in the next campaign.	0
194	2454	2454	S07-6	title	1	1	4.0	1.0	SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions	1
195	2455	2455	S07-6	abstract	1	2	1.0	1.0	The SemEval-2007 task to disambiguate prepositions was designed as a lexical sample task.	0
196	2456	2456	S07-6	abstract	2	3	2.0	1.0	A set of over 25,000 instances was developed, covering 34 of the most frequent English prepositions, with two-thirds of the instances for training and one-third as the test set.	0
197	2457	2457	S07-6	abstract	3	4	2.0	1.0	Each instance identified a preposition to be tagged in a full sentence taken from the FrameNet corpus (mostly from the British National Corpus).	0
198	2458	2458	S07-6	abstract	4	5	3.0	1.0	Definitions from the Oxford Dictionary of English formed the sense inventories.	0
199	2459	2459	S07-6	abstract	5	6	3.0	1.0	Three teams participated, with all achieving supervised results significantly better than baselines, with a high fine-grained precision of 0.693.	0
200	2460	2460	S07-6	abstract	6	7	4.0	1.0	This level is somewhat similar to results on lexical sample tasks with open class words, indicating that significant progress has been made.	0
201	2461	2461	S07-6	abstract	7	8	4.0	1.0	The data generated in the task provides ample opportunitites for further investigations of preposition behavior.	0
202	2613	2613	S07-7	Task Setup	1	18	4.0	1.0	The task required participating systems to annotate open-class words (i.e. nouns, verbs, adjectives, and adverbs) in a test corpus with the most appropriate sense from a coarse-grained version of the WordNet sense inventory.	1
203	2692	2692	S07-8	Introduction	1	5	1.0	1.0	Both word sense disambiguation and named entity recognition have benefited enormously from shared task evaluations, for example in the Senseval, MUC and CoNLL frameworks.	0
204	2693	2693	S07-8	Introduction	2	6	1.0	1.0	Similar campaigns have not been developed for the resolution of figurative language, such as metaphor, metonymy, idioms and irony.	0
205	2694	2694	S07-8	Introduction	3	7	1.0	1.0	However, resolution of figurative language is an important complement to and extension of word sense disambiguation as it often deals with word senses that are not listed in the lexicon.	0
206	2695	2695	S07-8	Introduction	4	8	1.0	1.0	For example, the meaning of stopover in the sentence	0
207	2696	2696	S07-8	Introduction	5	9	1.0	1.0	"He saw teaching as a stopover on his way to bigger things is a metaphorical sense of the sense ""stopping place in a physical journey"", with the literal sense listed in WordNet 2.0 but the metaphorical one not being listed."	0
208	2697	2697	S07-8	Introduction	6	10	1.0	1.0	1	0
209	2698	2698	S07-8	Introduction	7	11	1.0	1.0	The same holds for the metonymic reading of rattlesnake (for the animal's meat) in Roast rattlesnake tastes like chicken.	0
210	2699	2699	S07-8	Introduction	8	12	1.0	1.0	2 Again, the meat read-ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is.	0
211	2700	2700	S07-8	Introduction	9	13	2.0	1.0	As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997;	0
212	2701	2701	S07-8	Introduction	10	14	2.0	1.0	Hobbs et al., 1993;	0
213	2702	2702	S07-8	Introduction	11	15	2.0	1.0	Barnden et al., 2003, among others) carry out only smallscale evaluations.	0
214	2703	2703	S07-8	Introduction	12	16	2.0	1.0	In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994;	0
215	2704	2704	S07-8	Introduction	13	17	2.0	1.0	Nissim and Markert, 2003;Mason, 2004;Peirsman, 2006;	0
216	2705	2705	S07-8	Introduction	14	18	2.0	1.0	Birke and Sarkaar, 2006;Krishnakamuran and Zhu, 2007).	0
217	2706	2706	S07-8	Introduction	15	19	2.0	1.0	Still, apart from (Nissim and Markert, 2003;	0
218	2707	2707	S07-8	Introduction	16	20	2.0	1.0	Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks.	0
219	2708	2708	S07-8	Introduction	17	21	3.0	1.0	This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy.	0
220	2709	2709	S07-8	Introduction	18	22	3.0	1.0	In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat.	0
221	2710	2710	S07-8	Introduction	19	23	3.0	1.0	Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there.	0
222	2711	2711	S07-8	Introduction	20	24	3.0	1.0	(1) Sex, drugs, and Vietnam have haunted Bill Clinton's campaign.	0
223	2712	2712	S07-8	Introduction	21	25	3.0	1.0	In Ex. 2 and 3, BMW, the name of a company, stands for its index on the stock market, or a vehicle manufactured by BMW, respectively.	0
224	2713	2713	S07-8	Introduction	22	26	3.0	1.0	(2) BMW slipped 4p to 31p	0
225	2714	2714	S07-8	Introduction	23	27	3.0	1.0	(3) His BMW went on to race at Le Mans	0
226	2715	2715	S07-8	Introduction	24	28	3.0	1.0	The importance of resolving metonymies has been shown for a variety of NLP tasks, such as ma-chine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993), anaphora resolution (Harabagiu, 1998;	0
227	2716	2716	S07-8	Introduction	25	29	4.0	1.0	Markert and Hahn, 2002) and geographical information retrieval (Leveling and Hartrumpf, 2006).	0
228	2717	2717	S07-8	Introduction	26	30	4.0	1.0	Although metonymic readings are, like all figurative readings, potentially open ended and can be innovative, the regularity of usage for word groups helps in establishing a common evaluation framework.	0
229	2718	2718	S07-8	Introduction	27	31	4.0	1.0	Many other location names, for instance, can be used in the same fashion as Vietnam in Ex. 1.	0
230	2719	2719	S07-8	Introduction	28	32	4.0	2.0	Thus, given a semantic class (e.g. location), one can specify several regular metonymic patterns (e.g. place-for-event) that instances of the class are likely to undergo.	0
231	2720	2720	S07-8	Introduction	29	33	4.0	2.0	In addition to literal readings, regular metonymic patterns and innovative metonymic readings, there can also be so-called mixed readings, similar to zeugma, where both a literal and a metonymic reading are evoked (Nunberg, 1995).	0
232	2721	2721	S07-8	Introduction	30	34	4.0	2.0	The metonymy task is a lexical sample task for English, consisting of two subtasks, one concentrating on the semantic class location, exemplified by country names, and another one concentrating on organisation, exemplified by company names.	0
233	2722	2722	S07-8	Introduction	31	35	4.0	2.0	Participants had to automatically classify preselected country/company names as having a literal or non-literal meaning, given a four-sentence context.	1
234	2723	2723	S07-8	Introduction	32	36	4.0	2.0	Additionally, participants could attempt finer-grained interpretations, further specifying readings into prespecified metonymic patterns (such as place-for-event) and recognising innovative readings.	0
235	2946	2946	S07-10	abstract	1	2	1.0	1.0	In this paper we describe the English Lexical Substitution task for SemEval.	0
236	2947	2947	S07-10	abstract	2	3	2.0	1.0	In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.	1
237	2948	2948	S07-10	abstract	3	4	3.0	1.0	The task involves both finding the synonyms and disambiguating the context.	1
238	2949	2949	S07-10	abstract	4	5	4.0	1.0	Participating systems are free to use any lexical resource.	0
239	2950	2950	S07-10	abstract	5	6	4.0	1.0	There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is.	0
240	3076	3076	S07-11	Introduction	1	6	1.0	1.0	As part of the SemEval-2007 evaluation exercise, we organized an English lexical sample task for word sense disambiguation (WSD), where the senseannotated examples were semi-automatically gathered from word-aligned English-Chinese parallel texts.	1
241	3077	3077	S07-11	Introduction	2	7	1.0	1.0	Two tracks were organized for this task, each gathering data from a different corpus.	0
242	3078	3078	S07-11	Introduction	3	8	1.0	1.0	In this paper, we describe our motivation for organizing the task, our task framework, and the results of participants.	0
243	3079	3079	S07-11	Introduction	4	9	1.0	1.0	Past research has shown that supervised learning is one of the most successful approaches to WSD.	0
244	3080	3080	S07-11	Introduction	5	10	1.0	1.0	However, this approach involves the collection of a large text corpus in which each ambiguous word has been annotated with the correct sense to serve as training data.	0
245	3081	3081	S07-11	Introduction	6	11	2.0	1.0	Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available.	0
246	3082	3082	S07-11	Introduction	7	12	2.0	1.0	An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users.	0
247	3083	3083	S07-11	Introduction	8	13	2.0	1.0	Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task.	0
248	3084	3084	S07-11	Introduction	9	14	2.0	1.0	In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmyth 1 was used as the sense inventory for verbs.	0
249	3085	3085	S07-11	Introduction	10	15	2.0	1.0	Another source of potential training data is parallel texts.	0
250	3086	3086	S07-11	Introduction	11	16	3.0	1.0	Our past research in (Ng et al., 2003;	0
251	3087	3087	S07-11	Introduction	12	17	3.0	1.0	Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD.	0
252	3088	3088	S07-11	Introduction	13	18	3.0	1.0	"Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English side of a word-aligned parallel text can then serve as the training data, as they are considered to have been disambiguated and ""sense-tagged"" by the appropriate Chinese translations."	0
253	3089	3089	S07-11	Introduction	14	19	3.0	2.0	Using the above approach, we gathered the training and test examples for our task from parallel texts.	0
254	3090	3090	S07-11	Introduction	15	20	3.0	2.0	Note that our examples are collected without manually annotating each individual ambiguous word occurrence, allowing us to gather our examples in a much shorter time.	0
255	3091	3091	S07-11	Introduction	16	21	4.0	2.0	This contrasts with the setting of the English lexical sample task in previous SENSE-VAL evaluations.	0
256	3092	3092	S07-11	Introduction	17	22	4.0	2.0	In the English lexical sample task of SENSEVAL-2, the sense tagged data were created through manual annotation by trained lexicographers.	0
257	3093	3093	S07-11	Introduction	18	23	4.0	2.0	In SENSEVAL-3, the data were gathered through manual sense annotation by Internet users.	0
258	3094	3094	S07-11	Introduction	19	24	4.0	2.0	In the next section, we describe in more detail the process of gathering examples from parallel texts and the two different parallel corpora we used.	0
259	3095	3095	S07-11	Introduction	20	25	4.0	2.0	We then give a brief description of each of the partici-pating systems.	0
260	3096	3096	S07-11	Introduction	21	26	4.0	2.0	In Section 4, we present the results obtained by the participants, before concluding in Section 5.	0
261	3267	3267	S07-13	Introduction	1	4	1.0	1.0	Finding information about people in the World Wide Web is one of the most common activities of Internet users.	0
262	3268	3268	S07-13	Introduction	2	5	1.0	1.0	Person names, however, are highly ambiguous.	0
263	3269	3269	S07-13	Introduction	3	6	1.0	1.0	In most cases, the results for a person name search are a mix of pages about different people sharing the same name.	0
264	3270	3270	S07-13	Introduction	4	7	1.0	1.0	The user is then forced either to add terms to the query (probably losing recall and focusing on one single aspect of the person), or to browse every document in order to filter the information about the person he is actually looking for.	0
265	3271	3271	S07-13	Introduction	5	8	1.0	1.0	In an ideal system the user would simply type a person name, and receive search results clustered according to the different people sharing that name.	0
266	3272	3272	S07-13	Introduction	6	9	2.0	1.0	And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants: systems receive a set of web pages (which are the result of a web search for a person name), and they have to cluster them in as many sets as entities sharing the name.	1
267	3273	3273	S07-13	Introduction	7	10	2.0	1.0	This task has close links with Word Sense Disambiguation (WSD), which is generally formulated as the task of deciding which sense a word has in a given con-text.	0
268	3274	3274	S07-13	Introduction	8	11	2.0	1.0	In both cases, the problem addressed is the resolution of the ambiguity in a natural language expression.	0
269	3275	3275	S07-13	Introduction	9	12	2.0	1.0	A couple of differences make our problem different.	0
270	3276	3276	S07-13	Introduction	10	13	2.0	1.0	WSD is usually focused on openclass words (common nouns, adjectives, verbs and adverbs).	0
271	3277	3277	S07-13	Introduction	11	14	2.0	1.0	The first difference is that boundaries between word senses in a dictionary are often subtle or even conflicting, making binary decisions harder and sometimes even useless depending on the application.	0
272	3278	3278	S07-13	Introduction	12	15	3.0	1.0	In contrast, distinctions between people should be easier to establish.	0
273	3279	3279	S07-13	Introduction	13	16	3.0	1.0	The second difference is that WSD usually operates with a dictionary containing a relatively small number of senses that can be assigned to each word.	0
274	3280	3280	S07-13	Introduction	14	17	3.0	1.0	"Our task is rather a case of Word Sense Discrimination, because the number of ""senses"" (actual people) is unknown a priori, and it is in average much higher than in the WSD task (there are 90,000 different names shared by 100 million people according to the U.S. Census Bureau)."	0
275	3281	3281	S07-13	Introduction	15	18	3.0	1.0	There is also a strong relation of our proposed task with the Co-reference Resolution problem, focused on linking mentions (including pronouns) in a text.	0
276	3282	3282	S07-13	Introduction	16	19	3.0	1.0	Our task can be seen as a co-reference resolution problem where the focus is on solving interdocument co-reference, disregarding the linking of all the mentions of an entity inside each document.	0
277	3283	3283	S07-13	Introduction	17	20	3.0	1.0	"An early work in name disambiguation (Bagga and Baldwin, 1998) uses the similarity between documents in a Vector Space using a ""bag of words"" representation."	0
278	3284	3284	S07-13	Introduction	18	21	4.0	1.0	An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information.	0
279	3285	3285	S07-13	Introduction	19	22	4.0	1.0	Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that two names refer to the same individual 1 .	0
280	3286	3286	S07-13	Introduction	20	23	4.0	1.0	The paper is organized as follows.	0
281	3287	3287	S07-13	Introduction	21	24	4.0	1.0	Section 2 provides a description of the experimental methodology, the training and test data provided to the participants, the evaluation measures, baseline systems and the campaign design.	0
282	3288	3288	S07-13	Introduction	22	25	4.0	1.0	Section 3 gives a description of the participant systems and provides the evaluation results.	0
283	3289	3289	S07-13	Introduction	23	26	4.0	1.0	Finally, Section 4 presents some conclusions.	0
284	3397	3397	S07-14	abstract	1	2	2.0	1.0	"The ""Affective Text"" task focuses on the classification of emotions and valence (positive/negative polarity) in news headlines, and is meant as an exploration of the connection between emotions and lexical semantics."	1
285	3398	3398	S07-14	abstract	2	3	4.0	1.0	In this paper, we describe the data set used in the evaluation and the results obtained by the participating systems.	0
286	3511	3511	S07-15	abstract	1	2	2.0	1.0	The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations.	1
287	3512	3512	S07-15	abstract	2	3	3.0	1.0	It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise evaluation of temporal relations.	0
288	3513	3513	S07-15	abstract	3	4	4.0	1.0	The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing.	0
289	3660	3660	S07-16	abstract	1	2	1.0	1.0	This task tries to establish the relative quality of available semantic resources (derived by manual or automatic means).	0
290	3661	3661	S07-16	abstract	2	3	2.0	1.0	The quality of each large-scale knowledge resource is indirectly evaluated on a Word Sense Disambiguation task.	1
291	3662	3662	S07-16	abstract	3	4	3.0	1.0	In particular, we use Senseval-3 and SemEval-2007 English Lexical Sample tasks as evaluation bechmarks to evaluate the relative quality of each resource.	0
292	3663	3663	S07-16	abstract	4	5	4.0	1.0	Furthermore, trying to be as neutral as possible with respect the knowledge bases studied, we apply systematically the same disambiguation method to all the resources.	0
293	3664	3664	S07-16	abstract	5	6	4.0	1.0	A completely different behaviour is observed on both lexical data sets (Senseval-3 and SemEval-2007).	0
294	3792	3792	S07-17	Introduction	1	4	2.0	1.0	Correctly disambiguating words (WSD), and correctly identifying the semantic relationships between those words (SRL), is an important step for building successful natural language processing applications, such as text summarization, question answering, and machine translation.	1
295	3793	3793	S07-17	Introduction	2	5	3.0	1.0	SemEval-2007 Task-17 (English Lexical Sample, SRL and All-Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.	0
296	3794	3794	S07-17	Introduction	3	6	4.0	1.0	It includes three subtasks: i) the traditional All-Words task comprising fine-grained word sense disambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grained word sense disambiguation on a selected set of lexemes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.	0
297	3897	3897	S07-18	abstract	1	2	1.0	1.0	In this paper, we present the details of the Arabic Semantic Labeling task.	0
298	3898	3898	S07-18	abstract	2	3	2.0	1.0	We describe some of the features of Arabic that are relevant for the task.	0
299	3899	3899	S07-18	abstract	3	4	3.0	1.0	The task comprises two subtasks: Arabic word sense disambiguation and Arabic semantic role labeling.	1
300	3900	3900	S07-18	abstract	4	5	4.0	1.0	The task focuses on modern standard Arabic.	0
301	4080	4080	S07-19	abstract	1	2	1.0	1.0	This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects).	1
302	4081	4081	S07-19	abstract	2	3	2.0	1.0	The training data was FN annotated sentences.	0
303	4082	4082	S07-19	abstract	3	4	3.0	1.0	In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles.	0
304	4083	4083	S07-19	abstract	4	5	4.0	1.0	Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.	0
305	4187	4187	S10-1	Introduction	1	5	1.0	1.0	The task of coreference resolution, defined as the identification of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community.	0
306	4188	4188	S10-1	Introduction	2	6	1.0	1.0	(1)	0
307	4189	4189	S10-1	Introduction	3	7	1.0	1.0	Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months.	0
308	4190	4190	S10-1	Introduction	4	8	1.0	1.0	The league is reviewing security at all ballparks to crack down on spectator violence.	0
309	4191	4191	S10-1	Introduction	5	9	1.0	1.0	Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation.	0
310	4192	4192	S10-1	Introduction	6	10	1.0	1.0	There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open:	0
311	4193	4193	S10-1	Introduction	7	11	2.0	1.0		0
312	4194	4194	S10-1	Introduction	8	12	2.0	1.0	To what extent is it possible to implement a general coreference resolution system portable to different languages?	0
313	4195	4195	S10-1	Introduction	9	13	2.0	1.0	How much language-specific tuning is necessary?	0
314	4196	4196	S10-1	Introduction	10	14	2.0	1.0		0
315	4197	4197	S10-1	Introduction	11	15	2.0	1.0	How helpful are morphology, syntax and semantics for solving coreference relations?	0
316	4198	4198	S10-1	Introduction	12	16	2.0	1.0	How much preprocessing is needed?	0
317	4199	4199	S10-1	Introduction	13	17	2.0	1.0	Does its quality (perfect linguistic input versus noisy automatic input) really matter?	0
318	4200	4200	S10-1	Introduction	14	18	3.0	1.0		0
319	4201	4201	S10-1	Introduction	15	19	3.0	1.0	How (dis)similar are different coreference evaluation metrics-MUC, B-CUBED, CEAF and BLANC?	0
320	4202	4202	S10-1	Introduction	16	20	3.0	1.0	Do they all provide the same ranking?	0
321	4203	4203	S10-1	Introduction	17	21	3.0	1.0	Are they correlated?	0
322	4204	4204	S10-1	Introduction	18	22	3.0	1.0	Our goal was to address these questions in a shared task.	0
323	4205	4205	S10-1	Introduction	19	23	3.0	1.0	Given six datasets in Catalan, Dutch, English, German, Italian, and Spanish, the task we present involved automatically detecting full coreference chains-composed of named entities (NEs), pronouns, and full noun phrases-in four different scenarios.	1
324	4206	4206	S10-1	Introduction	20	24	4.0	1.0	For more information, the reader is referred to the task website.	0
325	4207	4207	S10-1	Introduction	21	25	4.0	1.0	1	0
326	4208	4208	S10-1	Introduction	22	26	4.0	1.0	The rest of the paper is organized as follows.	0
327	4209	4209	S10-1	Introduction	23	27	4.0	1.0	Section 2 presents the corpora from which the task datasets were extracted, and the automatic tools used to preprocess them.	0
328	4210	4210	S10-1	Introduction	24	28	4.0	1.0	In Section 3, we describe the task by providing information about the data format, evaluation settings, and evaluation metrics.	0
329	4211	4211	S10-1	Introduction	25	29	4.0	1.0	Participating systems are described in Section 4, and their results are analyzed and compared in Section 5.	0
330	4212	4212	S10-1	Introduction	26	30	4.0	1.0	Finally, Section 6 concludes.	0
331	4317	4317	S10-2	abstract	1	2	2.0	1.0	In this paper we describe the SemEval-2010 Cross-Lingual Lexical Substitution task, where given an English target word in context, participating systems had to find an alternative substitute word or phrase in Spanish.	1
332	4318	4318	S10-2	abstract	2	3	3.0	1.0	The task is based on the English Lexical Substitution task run at SemEval-2007.	0
333	4319	4319	S10-2	abstract	3	4	4.0	1.0	In this paper we provide background and motivation for the task, we describe the data annotation process and the scoring system, and present the results of the participating systems.	0
334	4583	4583	S10-5	abstract	1	2	1.0	1.0	This paper describes Task 5 of the Workshop on Semantic Evaluation 2010 (SemEval-2010).	0
335	4584	4584	S10-5	abstract	2	3	2.0	1.0	Systems are to automatically assign keyphrases or keywords to given scientific articles.	1
336	4585	4585	S10-5	abstract	3	4	3.0	1.0	The participating systems were evaluated by matching their extracted keyphrases against manually assigned ones.	0
337	4586	4586	S10-5	abstract	4	5	4.0	1.0	We present the overall ranking of the submitted systems and discuss our findings to suggest future directions for this task.	0
338	4690	4690	S10-7	abstract	1	2	1.0	1.0	We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise.	0
339	4691	4691	S10-7	abstract	2	3	2.0	1.0	This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects.	0
340	4692	4692	S10-7	abstract	3	4	3.0	1.0	Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing.	1
341	4693	4693	S10-7	abstract	4	5	4.0	1.0	We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.	0
342	4828	4828	S10-8	Introduction	1	5	1.0	1.0	SemEval-2010	0
343	4829	4829	S10-8	Introduction	2	6	1.0	1.0	Task 8 focused on semantic relations between pairs of nominals.	0
344	4830	4830	S10-8	Introduction	3	7	1.0	1.0	"For example, tea and ginseng are in an ENTITY-ORIGIN relation in ""The cup contained tea from dried ginseng.""."	0
345	4831	4831	S10-8	Introduction	4	8	2.0	1.0	The automatic recognition of semantic relations has many applications, such as information extraction, document summarization, machine translation, or construction of thesauri and semantic networks.	0
346	4832	4832	S10-8	Introduction	5	9	2.0	1.0	It can also facilitate auxiliary tasks such as word sense disambiguation, language modeling, paraphrasing, and recognizing textual entailment.	0
347	4833	4833	S10-8	Introduction	6	10	2.0	1.0	Our goal was to create a testbed for automatic classification of semantic relations.	0
348	4834	4834	S10-8	Introduction	7	11	3.0	1.0	In developing the task we met several challenges: selecting a suitable set of relations, specifying the annotation procedure, and deciding on the details of the task itself.	0
349	4835	4835	S10-8	Introduction	8	12	3.0	1.0	They are discussed briefly in Section 2; see also Hendrickx et al. (2009), which includes a survey of related work.	0
350	4836	4836	S10-8	Introduction	9	13	3.0	1.0	The direct predecessor of Task 8 was Classification of semantic relations between nominals, Task 4 at SemEval-1 (Girju et al., 2009), which had a separate binary-labeled dataset for each of seven relations.	0
351	4837	4837	S10-8	Introduction	10	14	4.0	1.0	We have defined SemEval-2010	0
352	4838	4838	S10-8	Introduction	11	15	4.0	1.0	Task 8 as a multi-way classification task in which the label for each example must be chosen from the complete set of ten relations and the mapping from nouns to argument slots is not provided in advance.	1
353	4839	4839	S10-8	Introduction	12	16	4.0	1.0	We also provide more data: 10,717 annotated examples, compared to 1,529 in SemEval-1 Task 4.	0
354	4994	4994	S10-9	Task Description	1	32	1.0	2.0	The Objective	0
355	4995	4995	S10-9	Task Description	2	33	1.0	2.0	For the purpose of the task, we focused on twoword NCs which are modifier-head pairs of nouns, such as apple pie or malaria mosquito.	0
356	4996	4996	S10-9	Task Description	3	34	2.0	2.0	"There are several ways to ""attack"" the paraphrase-based semantics of such NCs."	0
357	4997	4997	S10-9	Task Description	4	35	2.0	2.0	We have proposed a rather simple problem: assume that many paraphrases can be found -perhaps via clever Web search -but their relevance is up in the air.	0
358	4998	4998	S10-9	Task Description	5	36	2.0	2.0	Given sufficient training data, we seek to estimate the quality of candidate paraphrases in a test set.	0
359	4999	4999	S10-9	Task Description	6	37	3.0	2.0	Each NC in the training set comes with a long list of verbs in the infinitive (often with a preposition) which may paraphrase the NC adequately.	0
360	5000	5000	S10-9	Task Description	7	38	3.0	2.0	Examples of apt paraphrasing verbs: olive oilbe extracted from, drug death -be caused by, flu shot -prevent.	0
361	5001	5001	S10-9	Task Description	8	39	3.0	2.0	These lists have been constructed from human-proposed paraphrases.	0
362	5002	5002	S10-9	Task Description	9	40	4.0	2.0	For the training data, we also provide the participants with a quality score for each paraphrase, which is a simple count of the number of human subjects who proposed that paraphrase.	0
363	5003	5003	S10-9	Task Description	10	41	4.0	2.0	At test time, given a noun compound and a list of paraphrasing verbs, a participating system needs to produce aptness scores that correlate well (in terms of relative ranking) with the held out human judgments.	1
364	5004	5004	S10-9	Task Description	11	42	4.0	2.0	There may be a diverse range of paraphrases for a given compound, some of them in fact might be inappropriate, but it can be expected that the distribution over paraphrases estimated from a large number of subjects will indeed be representative of the compound's meaning.	0
365	5085	5085	S10-10	abstract	1	2	1.0	1.0	"We describe the SemEval-2010 shared task on ""Linking Events and Their Participants in Discourse""."	0
366	5086	5086	S10-10	abstract	2	3	2.0	1.0	This task is an extension to the classical semantic role labeling task.	0
367	5087	5087	S10-10	abstract	3	4	3.0	1.0	While semantic role labeling is traditionally viewed as a sentence-internal task, local semantic argument structures clearly interact with each other in a larger context, e.g., by sharing references to specific discourse entities or events.	0
368	5088	5088	S10-10	abstract	4	5	4.0	1.0	In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist).	1
369	5089	5089	S10-10	abstract	5	6	4.0	1.0	This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization.	0
370	5245	5245	S10-11	abstract	1	2	1.0	1.0	table .	0
371	5246	5246	S10-11	abstract	2	3	1.0	1.0	The goal of the task is to detect and analyze the event contents in real world Chinese news texts.	1
372	5247	5247	S10-11	abstract	3	4	1.0	1.0	It consists of finding key verbs or verb phrases to describe these events in the Chinese sentences after word segmentation and part-of-speech tagging, selecting suitable situation descriptions for them, and anchoring different situation arguments with suitable syntactic chunks in the sentence.	1
373	5248	5248	S10-11	abstract	4	5	1.0	1.0	Three main sub-tasks are as follows: (1) Target verb WSD; (2) Sentence SRL; (3) Event detection.	0
374	5249	5249	S10-11	abstract	5	6	1.0	1.0	We will select 100 high-frequency Chinese target verbs for this task.	0
375	5250	5250	S10-11	abstract	6	7	1.0	1.0	Among them, 30 verbs have multiple senses and 70 verbs have single sense.	0
376	5251	5251	S10-11	abstract	7	8	2.0	2.0	Each target verb will be assigned more than 50 annotated sentences to consist of training and test sets.	0
377	5252	5252	S10-11	abstract	8	9	2.0	2.0	Each annotated sentence will have following event information: (1) word segmentation and POS tags; (2) the target verb (or verb phrase) and its position in the sentence; (3) the event description (situation description formula or natural explanation text) of the target verb (or verb phrase) in the context of the sentences; (4) the chunks annotated with suitable syntactic constituent tags, functional tags and event argument role tags.	0
378	5253	5253	S10-11	abstract	9	10	2.0	2.0	The training and test set will be extracted from the data set with ratio 8:2.	0
379	5254	5254	S10-11	abstract	10	11	2.0	2.0	For the WSD subtask, we give two evaluation measures: WSD-Micro-Accuracy and WSD-Macro-Accuracy.	0
380	5255	5255	S10-11	abstract	11	12	2.0	2.0	The correct conditions are: the selected situation description formula and natural explanation text of the target verbs will be same with the gold-standard codes.	0
381	5256	5256	S10-11	abstract	12	13	2.0	2.0	We evaluated 27 multiple-sense target verbs in the test set.	0
382	5257	5257	S10-11	abstract	13	14	2.0	2.0	For the SRL subtask, we give three evaluation measures: Chunk-Precision, Chunk-Recall, and Chunk-F-measure.	0
383	5258	5258	S10-11	abstract	14	15	3.0	3.0	The correct conditions are: the recognized chunks should have the same boundaries, syntactic constituent and functional tags, and situation argument tags with the gold-standard argument chunks of the key verbs or verb phrases.	0
384	5259	5259	S10-11	abstract	15	16	3.0	3.0	We only select the key argument chunks (with semantic role tags: x, y, z, L or O) for evaluation.	0
385	5260	5260	S10-11	abstract	16	17	3.0	3.0	For the event detection subtask, we give two evaluation measures: Event-Micro-Accuracy and Event-Macro-Accuracy.	0
386	5261	5261	S10-11	abstract	17	18	3.0	3.0	The correct conditions are: (1)	0
387	5262	5262	S10-11	abstract	18	19	3.0	3.0	The event situation description formula and natural explanation text of the target verb should be same with the gold-standard ones; (2)	0
388	5263	5263	S10-11	abstract	19	20	3.0	3.0	All the argument chunks of the event descriptions should be same with the gold-standard ones; (3)	0
389	5264	5264	S10-11	abstract	20	21	3.0	3.0	The number of the recognized argument chunks should be same with the gold-standard one.	0
390	5265	5265	S10-11	abstract	21	22	4.0	4.0	8 participants downloaded the training and test data.	0
391	5266	5266	S10-11	abstract	22	23	4.0	4.0	Only 3 participants uploaded the final results.	0
392	5267	5267	S10-11	abstract	23	24	4.0	4.0	Among them, 1 participant (User ID = 156) submitted 4 results and 1 participant (User ID = 485) submitted 2 results.	0
393	5268	5268	S10-11	abstract	24	25	4.0	4.0	So we received 7 uploaded results for evaluation.	0
394	5269	5269	S10-11	abstract	25	26	4.0	4.0	The mean elaboration time of the test data is about 30 hours.	0
395	5270	5270	S10-11	abstract	26	27	4.0	4.0	The following is the evaluation result	0
396	5271	5271	S10-11	abstract	27	28	4.0	4.0	The results show the event detection task is still an open problem for exploring in the Chinese language.	0
397	5419	5419	S10-13	abstract	1	2	2.0	1.0	Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.	1
398	5420	5420	S10-13	abstract	2	3	4.0	1.0	Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish.	0
399	5529	5529	S10-14	abstract	1	2	2.0	1.0	This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction &amp; Disambiguation task, as well as the evaluation results of 26 participating systems.	0
400	5530	5530	S10-14	abstract	2	3	3.0	1.0	In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.	1
401	5531	5531	S10-14	abstract	3	4	4.0	1.0	Systems' answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.	0
402	5651	5651	S10-15	abstract	1	2	1.0	1.0	Introduction	0
403	5652	5652	S10-15	abstract	2	3	1.0	1.0	There are seven cases of grapheme to phoneme in a text to speech system (Yarowsky, 1997).	0
404	5653	5653	S10-15	abstract	3	4	1.0	1.0	Among them, the most difficult task is disambiguating the homograph word, which has the same POS but different pronunciation.	1
405	5654	5654	S10-15	abstract	4	5	2.0	1.0	In this case, different pronunciations of the same word always correspond to different word senses.	0
406	5655	5655	S10-15	abstract	5	6	2.0	1.0	Once the word senses are disambiguated, the problem of GTP is resolved.	0
407	5656	5656	S10-15	abstract	6	7	2.0	2.0	There is a little different from traditional WSD, in this task two or more senses may correspond to one pronunciation.	0
408	5657	5657	S10-15	abstract	7	8	2.0	2.0	That is, the sense granularity is coarser than WSD.	0
409	5658	5658	S10-15	abstract	8	9	3.0	2.0	"For example, the preposition """" has three senses: sense1 and sense2 have the same pronunciation {wei 4}, while sense3 corresponds to {wei 2}."	0
410	5659	5659	S10-15	abstract	9	10	3.0	2.0	In this task, to the target word, not only the pronunciations but also the sense labels are provided for training; but for test, only the pronunciations are evaluated.	0
411	5660	5660	S10-15	abstract	10	11	3.0	2.0	The challenge of this task is the much skewed distribution in real text: the most frequent pronunciation occupies usually over 80%.	0
412	5661	5661	S10-15	abstract	11	12	3.0	2.0	In this task, we will provide a large volume of training data (each homograph word has at least 300 instances) accordance with the truly distribution in real text.	0
413	5662	5662	S10-15	abstract	12	13	4.0	3.0	In the test data, we will provide at least 100 instances for each target word.	0
414	5663	5663	S10-15	abstract	13	14	4.0	3.0	The senses distribution in test data is the same as in training data.	0
415	5664	5664	S10-15	abstract	14	15	4.0	3.0	All instances come from People Daily newspaper (the most popular newspaper in Mandarin).	0
416	5665	5665	S10-15	abstract	15	16	4.0	3.0	Double blind annotations are executed manually, and a third annotator checks the annotation.	0
417	5800	5800	S10-17	Conclusions	1	127	1.0	4.0	Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges.	0
418	5801	5801	S10-17	Conclusions	2	128	1.0	4.0	The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems.	0
419	5802	5802	S10-17	Conclusions	3	129	2.0	4.0	With this paper we have motivated the creation of an all-words test dataset for WSD on the environment domain in several languages, and presented the overall design of this SemEval task.	1
420	5803	5803	S10-17	Conclusions	4	130	2.0	4.0	One of the goals of the exercise was to show that WSD systems could make use of unannotated background corpora to adapt to the domain and improve their results.	0
421	5804	5804	S10-17	Conclusions	5	131	3.0	4.0	Although it's early to reach hard conclusions, the results show that in each of the datasets, knowledge-based systems are able to improve their results using background text, and in two datasets the adaptation of knowledge-based systems leads to results over the MFS baseline.	0
422	5805	5805	S10-17	Conclusions	6	132	3.0	4.0	The evidence of domain adaptation of supervised systems is weaker, as only one team tried, and the differences with respect to MFS are very small.	0
423	5806	5806	S10-17	Conclusions	7	133	4.0	4.0	The best results for English are obtained by a system that combines a knowledge-based system with some targeted hand-tagging.	0
424	5807	5807	S10-17	Conclusions	8	134	4.0	4.0	Regarding the techniques used, graph-based methods over WordNet and distributional thesaurus acquisition methods have been used by several teams.	0
425	5808	5808	S10-17	Conclusions	9	135	4.0	4.0	All datasets and related information are publicly available from the task websites 6 .	0
426	5975	5975	S10-19	Introduction	1	8	1.0	1.0	This paper reports an overview of the SemEval-2 Japanese Word Sense Disambiguation (WSD) task.	0
427	5976	5976	S10-19	Introduction	2	9	1.0	1.0	It can be considered an extension of the SENSEVAL-2 Japanese monolingual dictionarybased task (Shirai, 2001), so it is a lexical sample task.	0
428	5977	5977	S10-19	Introduction	3	10	1.0	1.0	Word senses are defined according to the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary published by Iwanami Shoten.	0
429	5978	5978	S10-19	Introduction	4	11	1.0	1.0	It was distributed to participants as a sense inventory.	0
430	5979	5979	S10-19	Introduction	5	12	1.0	1.0	Our task has the following two new characteristics:	0
431	5980	5980	S10-19	Introduction	6	13	1.0	1.0	1	0
432	5981	5981	S10-19	Introduction	7	14	1.0	1.0	All previous Japanese sense-tagged corpora were from newspaper articles, while sensetagged corpora were constructed in English on balanced corpora, such as Brown corpus and BNC corpus.	0
433	5982	5982	S10-19	Introduction	8	15	2.0	1.0	The first balanced corpus of contemporary written Japanese (BCCWJ corpus) is now being constructed as part of a national project in Japan (Maekawa, 2008), and we are now constructing a sense-tagged corpus based on it.	0
434	5983	5983	S10-19	Introduction	9	16	2.0	1.0	Therefore, the task will use the first balanced Japanese sense-tagged corpus.	0
435	5984	5984	S10-19	Introduction	10	17	2.0	1.0	Because a balanced corpus consists of documents from multiple genres, the corpus can be divided into multiple sub-corpora of a genre.	0
436	5985	5985	S10-19	Introduction	11	18	2.0	1.0	In supervised learning approaches on word sense disambiguation, because word sense distribution might vary across different sub-corpora, we need to take into account the genres of training and test corpora.	0
437	5986	5986	S10-19	Introduction	12	19	2.0	1.0	Therefore, word sense disambiguation on a balanced corpus requires tackling a kind of domain (genre) adaptation problem (Chang and Ng, 2006;	0
438	5987	5987	S10-19	Introduction	13	20	2.0	1.0	Agirre and de Lacalle, 2008).	0
439	5988	5988	S10-19	Introduction	14	21	2.0	1.0	2. In previous WSD tasks, systems have been required to select a sense from a given set of senses in a dictionary for a word in one context (an instance).	1
440	5989	5989	S10-19	Introduction	15	22	3.0	1.0	However, the set of senses in the dictionary is not always complete.	0
441	5990	5990	S10-19	Introduction	16	23	3.0	1.0	New word senses sometimes appear after the dictionary has been compiled.	0
442	5991	5991	S10-19	Introduction	17	24	3.0	1.0	Therefore, some instances might have a sense that cannot be found in the dictionary's set.	0
443	5992	5992	S10-19	Introduction	18	25	3.0	1.0	The task will take into account not only the instances that have a sense in the given set but also the instances that have a sense that cannot be found in the set.	0
444	5993	5993	S10-19	Introduction	19	26	3.0	1.0	In the latter case, systems should output that the instances have a sense that is not in the set.	0
445	5994	5994	S10-19	Introduction	20	27	3.0	1.0	Training data, a corpus that consists of three genres (books, newspaper articles, and white papers) and is manually annotated with sense IDs, was also distributed to participants.	0
446	5995	5995	S10-19	Introduction	21	28	3.0	1.0	For the evaluation, we distributed a corpus that consists of four genres (books, newspaper articles, white papers, and documents from a Q&amp;	0
447	5996	5996	S10-19	Introduction	22	29	4.0	1.0	A site on the WWW) with marked target words as test data.	0
448	5997	5997	S10-19	Introduction	23	30	4.0	1.0	Participants were requested to assign one or more sense IDs to each target word, optionally with associated probabilities.	0
449	5998	5998	S10-19	Introduction	24	31	4.0	1.0	The number of target words was 50, with 22 nouns, 23 verbs, and 5 adjectives.	0
450	5999	5999	S10-19	Introduction	25	32	4.0	1.0	Fifty instances of each target word were provided, con-sisting of a total of 2,500 instances for the evaluation.	0
451	6000	6000	S10-19	Introduction	26	33	4.0	1.0	In what follows, section two describes the details of the data used in the Japanese WSD task.	0
452	6001	6001	S10-19	Introduction	27	34	4.0	1.0	Section three describes the process to construct the sense tagged data, including the analysis of an inter-annotator agreement.	0
453	6002	6002	S10-19	Introduction	28	35	4.0	1.0	Section four briefly introduces participating systems and section five describes their results.	0
454	6003	6003	S10-19	Introduction	29	36	4.0	1.0	Finally, section six concludes the paper.	0
455	6144	6144	S12-1	Task definition	1	27	1.0	1.0	"Given a short context, a target word in English, and several substitutes for the target word that are deemed adequate for that context, the goal of the English Simplification task at SemEval-2012 is to rank these substitutes according to how ""simple"" they are, allowing ties."	1
456	6145	6145	S12-1	Task definition	2	28	2.0	1.0	Simple words/phrases are loosely defined as those which can be understood by a wide range of people, including those with low literacy levels or some cognitive disability, children, and non-native speakers of English.	0
457	6146	6146	S12-1	Task definition	3	29	2.0	1.0	In particular, the data provided as part of the task is annotated by fluent but non-native speakers of English.	0
458	6147	6147	S12-1	Task definition	4	30	3.0	1.0	The task thus essentially involves comparing words or phrases and determining their order of complexity.	0
459	6148	6148	S12-1	Task definition	5	31	3.0	1.0	By ranking the candidates, as opposed to categorizing them into specific labels (simple, moderate, complex, etc.), we avoid the need for a fixed number of categories and for more subjective judgments.	0
460	6149	6149	S12-1	Task definition	6	32	4.0	1.0	Also ranking enables a more natural and intuitive way for humans (and systems) to perform annotations by preventing them from treating each individual case in isolation, as opposed to relative to each other.	0
461	6150	6150	S12-1	Task definition	7	33	4.0	1.0	However, the inherent subjectivity introduced by ranking entails higher disagreement among human annotators, and more complexity for systems to tackle.	0
462	6351	6351	S12-2	Objective	1	42	1.0	1.0	Our task is to rate word pairs by the degree to which they are prototypical members of a given relation class.	1
463	6352	6352	S12-2	Objective	2	43	2.0	1.0	The relation class is specified by a few paradigmatic (highly prototypical) examples of word pairs that belong to the class and also by a schematic representation of the relation class.	0
464	6353	6353	S12-2	Objective	3	44	2.0	1.0	The task requires comparing a word pair to the paradigmatic examples and/or the schematic representation.	0
465	6354	6354	S12-2	Objective	4	45	3.0	1.0	For example, suppose the relation class is REVERSE.	0
466	6355	6355	S12-2	Objective	5	46	3.0	1.0	"We may specify this class by the paradigmatic examples attack:defend, buy:sell, love:hate, and the schematic representation ""X is the reverse act of Y "" or ""X may be undone by Y ."""	0
467	6356	6356	S12-2	Objective	6	47	4.0	1.0	Given a pair such as repair:break, we compare this pair to the paradigmatic examples and/or the schematic representation, in order to estimate its degree of prototypicality.	0
468	6357	6357	S12-2	Objective	7	48	4.0	1.0	The challenges are (1) to infer the relation from the paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples.	0
469	6532	6532	S12-3	abstract	1	2	1.0	1.0	This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling.	0
470	6533	6533	S12-3	abstract	2	3	2.0	1.0	The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators.	1
471	6534	6534	S12-3	abstract	3	4	2.0	1.0	In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted.	1
472	6535	6535	S12-3	abstract	4	5	3.0	1.0	The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark.	0
473	6536	6536	S12-3	abstract	5	6	4.0	1.0	We have one participant system with two runs.	0
474	6537	6537	S12-3	abstract	6	7	4.0	1.0	The participant's runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers.	0
475	6766	6766	S12-4	abstract	1	2	1.0	1.0	This task focuses on evaluating word similarity computation in Chinese.	1
476	6767	6767	S12-4	abstract	2	3	1.0	1.0	We follow the way of Finkelstein et al. (2002) to select word pairs.	0
477	6768	6768	S12-4	abstract	3	4	2.0	1.0	Then we organize twenty undergraduates who are major in Chinese linguistics to annotate the data.	0
478	6769	6769	S12-4	abstract	4	5	2.0	1.0	Each pair is assigned a similarity score by each annotator.	0
479	6770	6770	S12-4	abstract	5	6	3.0	1.0	We rank the word pairs by the average value of similar scores among the twenty annotators.	0
480	6771	6771	S12-4	abstract	6	7	3.0	1.0	This data is used as gold standard.	0
481	6772	6772	S12-4	abstract	7	8	4.0	1.0	Four systems participating in this task return their results.	0
482	6773	6773	S12-4	abstract	8	9	4.0	1.0	We evaluate their results on gold standard data in term of Kendall's tau value, and the results show three of them have a positive correlation with the rank manually created while the taus' value is very small.	0
483	6856	6856	S12-5	abstract	1	2	1.0	1.0	The paper presents the SemEval-2012 Shared Task 5: Chinese Semantic Dependency Parsing.	0
484	6857	6857	S12-5	abstract	2	3	2.0	1.0	The goal of this task is to identify the dependency structure of Chinese sentences from the semantic view.	1
485	6858	6858	S12-5	abstract	3	4	3.0	1.0	We firstly introduce the motivation of providing Chinese semantic dependency parsing task, and then describe the task in detail including data preparation, data format, task evaluation, and so on.	0
486	6859	6859	S12-5	abstract	4	5	4.0	1.0	Over ten thousand sentences were labeled for participants to train and evaluate their systems.	0
487	6860	6860	S12-5	abstract	5	6	4.0	1.0	At last, we briefly describe the submitted systems and analyze these results.	0
488	7175	7175	S12-7	abstract	1	2	1.0	1.0	SemEval-2012	0
489	7176	7176	S12-7	abstract	2	3	2.0	1.0	Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise.	1
490	7177	7177	S12-7	abstract	3	4	3.0	1.0	In this paper, we describe the development of this task and its motivation.	0
491	7178	7178	S12-7	abstract	4	5	4.0	1.0	We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research.	0
492	7179	7179	S12-7	abstract	5	6	4.0	1.0	We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future.	0
493	7432	7432	S13-1	Introduction	1	5	1.0	1.0	The TempEval task (Verhagen et al., 2009) was added as a new task in SemEval-2007.	0
494	7433	7433	S13-1	Introduction	2	6	1.0	1.0	The ultimate aim of research in this area is the automatic identification of temporal expressions (timexes), events, and temporal relations within a text as specified in TimeML annotation (Pustejovsky et al., 2005).	1
495	7434	7434	S13-1	Introduction	3	7	1.0	1.0	However, since addressing this aim in a first evaluation challenge was deemed too difficult a staged approach was suggested.	0
496	7435	7435	S13-1	Introduction	4	8	1.0	1.0	TempEval (henceforth TempEval-1) was an initial evaluation exercise focusing only on the categorization of temporal relations and only in English.	0
497	7436	7436	S13-1	Introduction	5	9	1.0	1.0	It included three relation types: event-timex, event-dct, 1 and relations between main events in consecutive sentences.	0
498	7437	7437	S13-1	Introduction	6	10	2.0	1.0	TempEval-2 (Verhagen et al., 2010) extended TempEval-1, growing into a multilingual task, and consisting of six subtasks rather than three.	0
499	7438	7438	S13-1	Introduction	7	11	2.0	1.0	This included event and timex extraction, as well as the three relation tasks from TempEval-1, with the addition of a relation task where one event subordinates another.	0
500	7439	7439	S13-1	Introduction	8	12	2.0	1.0	TempEval-3 (UzZaman et al., 2012b) is a follow-up to TempEval 1 and 2, covering English and Spanish.	0
501	7440	7440	S13-1	Introduction	9	13	2.0	1.0	Temp	0
502	7441	7441	S13-1	Introduction	10	14	2.0	1.0	Eval-3 is different from its predecessors in a few respects:	0
503	7442	7442	S13-1	Introduction	11	15	3.0	1.0	1 DCT stands for document creation time Size of the corpus: the dataset used has about 600K word silver standard data and about 100K word gold standard data for training, compared to around 50K word corpus used in TempEval 1 and 2.	0
504	7443	7443	S13-1	Introduction	12	16	3.0	1.0	Temporal annotation is a time-consuming task for humans, which has limited the size of annotated data in previous TempEval exercises.	0
505	7444	7444	S13-1	Introduction	13	17	3.0	1.0	Current systems, however, are performing close to the inter-annotator reliability, which suggests that larger corpora could be built from automatically annotated data with minor human reviews.	0
506	7445	7445	S13-1	Introduction	14	18	3.0	1.0	We want to explore whether there is value in adding a large automatically created silver standard to a hand-crafted gold standard.	0
507	7446	7446	S13-1	Introduction	15	19	3.0	1.0	End-to-end temporal relation processing task: the temporal relation classification tasks are performed from raw text, i.e. participants need to extract their own events and temporal expressions first, determine which ones to link and then obtain the relation types.	0
508	7447	7447	S13-1	Introduction	16	20	4.0	1.0	In previous Tem-pEvals, gold timexes, events, and relations (without category) were given to participants.	0
509	7448	7448	S13-1	Introduction	17	21	4.0	1.0	Temporal relation types: the full set of temporal relations in TimeML are used, rather than the reduced set used in earlier TempEvals.	0
510	7449	7449	S13-1	Introduction	18	22	4.0	1.0	Platinum test set: A new test dataset has been developed for this edition.	0
511	7450	7450	S13-1	Introduction	19	23	4.0	1.0	It is based on manual annotations by experts over new text (unseen in previous editions).	0
512	7451	7451	S13-1	Introduction	20	24	4.0	1.0	Evaluation: we report a temporal awareness score for evaluating temporal relations, which helps to rank systems with a single score.	0
513	7674	7674	S13-2	Subtask A: Contextual Polarity Disambiguation	1	37	2.0	1.0	Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context.	1
514	7675	7675	S13-2	Subtask A: Contextual Polarity Disambiguation	2	38	3.0	1.0	The boundaries for the marked instance were provided: this was a classification task, not an entity recognition task.	0
515	7676	7676	S13-2	Subtask A: Contextual Polarity Disambiguation	3	39	4.0	1.0	2 http://www.daedalus.es/TASS/corpus.php	0
516	7819	7819	S13-3	abstract	1	2	1.0	1.0	Many NLP applications require information about locations of objects referenced in text, or relations between them in space.	0
517	7820	7820	S13-3	abstract	2	3	2.0	1.0	For example, the phrase a book on the desk contains information about the location of the object book, as trajector, with respect to another object desk, as landmark.	0
518	7821	7821	S13-3	abstract	3	4	3.0	1.0	Spatial Role Labeling (SpRL) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them.	1
519	7822	7822	S13-3	abstract	4	5	4.0	1.0	This paper describes the task in Semantic Evaluations 2013, annotation schema, corpora, participants, methods and results obtained by the participants.	0
520	7966	7966	S13-3	Conclusion	1	149	1.0	4.0	In this paper we described an evaluation task on Spatial Role Labeling in the context of Semantic Evaluations 2013.	0
521	7967	7967	S13-3	Conclusion	2	150	2.0	4.0	The task sets a goal to automatically process text and identify objects of spatial scenes and relations between them.	1
522	7968	7968	S13-3	Conclusion	3	151	2.0	4.0	Building largely upon the previous evaluation campaign, SpRL-2012, in SpRL-2013 we introduced additional spatial roles and relations for capturing motions in text.	0
523	7969	7969	S13-3	Conclusion	4	152	3.0	4.0	In addition, a new annotated corpus for spatial roles (including annotated motions) was produced and released to the participants.	0
524	7970	7970	S13-3	Conclusion	5	153	3.0	4.0	It comprises a set of 117 files with about 40,000 tokens in total.	0
525	7971	7971	S13-3	Conclusion	6	154	4.0	4.0	With the registered number of 10 participants and the final number of submissions (only one) we can conclude that spatial role labeling is an interesting task within the research community, however sometimes underestimated in its complexity.	0
526	7972	7972	S13-3	Conclusion	7	155	4.0	4.0	Our further steps in promoting spatial role labeling will be a detailed description of the annotation scheme and annotation guidelines, analysis of the corpora and obtained results.	0
527	8033	8033	S13-4	Task description	1	61	1.0	2.0	This is an English NC interpretation task, which explores the idea of interpreting the semantics of NCs via free paraphrases.	0
528	8034	8034	S13-4	Task description	2	62	2.0	2.0	Given a noun-noun compound such as air filter, the participating systems are asked to produce an explicitly ranked list of free paraphrases, as in the following example:	1
529	8035	8035	S13-4	Task description	3	63	2.0	2.0	1 filter for air 2 filter of air 3 filter that cleans the air 4 filter which makes air healthier 5 a filter that removes impurities from the air . . .	0
530	8036	8036	S13-4	Task description	4	64	3.0	2.0	Such a list is then automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed via Amazon's Mechanical Turk.	0
531	8037	8037	S13-4	Task description	5	65	3.0	2.0	The comparison of raw paraphrases is sensitive to syntactic and morphological variation.	0
532	8038	8038	S13-4	Task description	6	66	4.0	2.0	The ranking of paraphrases is based on their relative popularity among different annotators.	0
533	8039	8039	S13-4	Task description	7	67	4.0	2.0	To make the ranking more reliable, highly similar paraphrases are grouped so as to downplay superficial differences in syntax and morphology.	0
534	8326	8326	S13-7	Introduction	1	7	1.0	1.0	One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring.	0
535	8327	8327	S13-7	Introduction	2	8	1.0	1.0	Much previous work has been devoted to the automated scoring of essays (Attali and Burstein, 2006;	0
536	8328	8328	S13-7	Introduction	3	9	1.0	1.0	Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009;	0
537	8329	8329	S13-7	Introduction	4	10	1.0	1.0	Sheehan et al., 2010;Nelson et al., 2012).	0
538	8330	8330	S13-7	Introduction	5	11	1.0	1.0	In these applications, NLP methods based on shallow features and supervised learning are often highly effective.	0
539	8331	8331	S13-7	Introduction	6	12	1.0	1.0	However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003;	0
540	8332	8332	S13-7	Introduction	7	13	1.0	1.0	Pulman and Sukkarieh, 2005;	0
541	8333	8333	S13-7	Introduction	8	14	1.0	1.0	Nielsen et al., 2008a;	0
542	8334	8334	S13-7	Introduction	9	15	1.0	1.0	Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999;	0
543	8335	8335	S13-7	Introduction	10	16	1.0	1.0	Glass, 2000;	0
544	8336	8336	S13-7	Introduction	11	17	1.0	1.0	Pon-Barry et al., 2004;	0
545	8337	8337	S13-7	Introduction	12	18	1.0	1.0	Jordan et al., 2006;	0
546	8338	8338	S13-7	Introduction	13	19	2.0	1.0	Van	0
547	8339	8339	S13-7	Introduction	14	20	2.0	1.0	Lehn et al., 2007;Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate.	0
548	8340	8340	S13-7	Introduction	15	21	2.0	1.0	Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community.	0
549	8341	8341	S13-7	Introduction	16	22	2.0	1.0	Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could Example 1 QUESTION	0
550	8342	8342	S13-7	Introduction	17	23	2.0	1.0	You used several methods to separate and identify the substances in mock rocks.	0
551	8343	8343	S13-7	Introduction	18	24	2.0	1.0	How did you separate the salt from the water?	0
552	8344	8344	S13-7	Introduction	19	25	2.0	1.0	REF.	0
553	8345	8345	S13-7	Introduction	20	26	2.0	1.0	ANS.	0
554	8346	8346	S13-7	Introduction	21	27	2.0	1.0	The water was evaporated, leaving the salt.	0
555	8347	8347	S13-7	Introduction	22	28	2.0	1.0	STUD.	0
556	8348	8348	S13-7	Introduction	23	29	2.0	1.0	ANS.	0
557	8349	8349	S13-7	Introduction	24	30	2.0	1.0	The water dried up and left the salt.	0
558	8350	8350	S13-7	Introduction	25	31	2.0	1.0	Example 2 QUESTION	0
559	8351	8351	S13-7	Introduction	26	32	3.0	1.0	Georgia found one brown mineral and one black mineral.	0
560	8352	8352	S13-7	Introduction	27	33	3.0	1.0	How will she know which one is harder?	0
561	8353	8353	S13-7	Introduction	28	34	3.0	1.0	REF.	0
562	8354	8354	S13-7	Introduction	29	35	3.0	1.0	ANS.	0
563	8355	8355	S13-7	Introduction	30	36	3.0	1.0	The harder mineral will leave a scratch on the less hard mineral.	0
564	8356	8356	S13-7	Introduction	31	37	3.0	1.0	If the black mineral is harder, the brown mineral will have a scratch.	0
565	8357	8357	S13-7	Introduction	32	38	3.0	1.0	STUD.	0
566	8358	8358	S13-7	Introduction	33	39	3.0	1.0	ANS.	0
567	8359	8359	S13-7	Introduction	34	40	3.0	1.0	The harder will leave a scratch on the other.	0
568	8360	8360	S13-7	Introduction	35	41	3.0	1.0	Figure 1: Example questions and answers help a full dialog system to generate appropriate and effective feedback on errors.	0
569	8361	8361	S13-7	Introduction	36	42	3.0	1.0	System designers typically create a repertoire of questions that the system can ask a student, together with reference answers (see Figure 1 for an example).	0
570	8362	8362	S13-7	Introduction	37	43	3.0	1.0	For each student answer, the system needs to decide on the appropriate tutorial feedback, either confirming that the answer was correct, or providing additional help to indicate how the answer is flawed and help the student improve.	1
571	8363	8363	S13-7	Introduction	38	44	4.0	1.0	This task requires semantic inference, for example, to detect when the student answers are explaining the same content but in different words, or when they are contradicting the reference answers.	0
572	8364	8364	S13-7	Introduction	39	45	4.0	1.0	Recognizing Textual Entailment (RTE) is a series of highly successful challenges used to evaluate tasks related to semantic inference, held annually since 2005.	0
573	8365	8365	S13-7	Introduction	40	46	4.0	1.0	Initial challenges used examples from information retrieval, question answering, machine translation and information extraction tasks (Dagan et al., 2006;	0
574	8366	8366	S13-7	Introduction	41	47	4.0	1.0	Giampiccolo et al., 2008).	0
575	8367	8367	S13-7	Introduction	42	48	4.0	1.0	Later challenges started to explore the applicability and impact of RTE technology on specific application settings such as Summarization and Knowledge Base Population (Bentivogli et al., 2009;	0
576	8368	8368	S13-7	Introduction	43	49	4.0	1.0	Bentivogli et al., 2010;Bentivogli et al., 2011).	0
577	8369	8369	S13-7	Introduction	44	50	4.0	1.0	The SRA Task offers a similar opportunity.	0
578	8370	8370	S13-7	Introduction	45	51	4.0	1.0	We therefore organized a joint challenge at SemEval-2013, aiming to bring together the educational NLP and the semantic inference communities.	0
579	8371	8371	S13-7	Introduction	46	52	4.0	1.0	The goal of the challenge is to compare approaches for student answer assessment and to evaluate the methods typically used in RTE on data from educational applications.	0
580	8372	8372	S13-7	Introduction	47	53	4.0	1.0	We present the corpus used in the task (Section 2) and describe the Main task, including educational NLP and textual entailment perspectives and data set creation (Section 3).	0
581	8373	8373	S13-7	Introduction	48	54	4.0	1.0	We discuss evaluation metrics and results in Section 4.	0
582	8374	8374	S13-7	Introduction	49	55	4.0	1.0	Section 5 describes the Pilot task, including data set creation and evaluation results.	0
583	8375	8375	S13-7	Introduction	50	56	4.0	1.0	Section 6 presents conclusions and future directions.	0
584	8404	8404	S13-7	RTE perspective and 2-and 3-way Tasks	1	85	1.0	2.0	According to the standard definition of Textual Entailment, given two text fragments called Text (T) and Hypothesis (H), it is said that T entails H if, typically, a human reading T would infer that H is most likely true (Dagan et al., 2006).	0
585	8405	8405	S13-7	RTE perspective and 2-and 3-way Tasks	2	86	1.0	2.0	In a typical answer assessment scenario, we expect that a correct student answer would entail the reference answer, while an incorrect answer would not.	0
586	8406	8406	S13-7	RTE perspective and 2-and 3-way Tasks	3	87	1.0	2.0	However, students often skip details that are mentioned in the question or may be inferred from it, while reference answers often repeat or make explicit information that appears in or is implied from the question, as in Example 2 in Figure 1.	0
587	8407	8407	S13-7	RTE perspective and 2-and 3-way Tasks	4	88	2.0	2.0	Hence, a more precise formulation of the task in this context considers the entailing text T as consisting of both the original question and the student answer, while H is the reference answer.	1
588	8408	8408	S13-7	RTE perspective and 2-and 3-way Tasks	5	89	2.0	2.0	We carried out a feasibility study to check how well the entailment judgments in this formulation align with the annotated response assessment, by annotating a sample of the data used in the SRA task with entailment judgments.	0
589	8409	8409	S13-7	RTE perspective and 2-and 3-way Tasks	6	90	2.0	2.0	"We found that some answers labeled as ""correct"" implied inferred or assumed pieces of information not present in the text."	0
590	8410	8410	S13-7	RTE perspective and 2-and 3-way Tasks	7	91	2.0	2.0	These reflected the teachers' assessment of student understanding but would not be considered entailed from the traditional RTE perspective.	0
591	8411	8411	S13-7	RTE perspective and 2-and 3-way Tasks	8	92	3.0	2.0	However, we observed that in most such cases, a substantial part of the hypothesis was still implied by the text.	0
592	8412	8412	S13-7	RTE perspective and 2-and 3-way Tasks	9	93	3.0	2.0	"Moreover, answers assigned labels other than ""correct"" were always judged as ""not entailed""."	0
593	8413	8413	S13-7	RTE perspective and 2-and 3-way Tasks	10	94	3.0	2.0	Overall, we concluded that the correlation between assessment judgments of the two types was sufficiently high to consider an RTE approach.	0
594	8414	8414	S13-7	RTE perspective and 2-and 3-way Tasks	11	95	3.0	2.0	The challenge for the textual entailment community was to address the answer assessment task at varying levels of granularity, using textual entailment techniques, and explore how well these techniques can help in this real-world educational setting.	0
595	8415	8415	S13-7	RTE perspective and 2-and 3-way Tasks	12	96	4.0	2.0	In order to make the setup more similar to pre-vious RTE tasks, we introduced 3-way and 2-way versions of the task.	0
596	8416	8416	S13-7	RTE perspective and 2-and 3-way Tasks	13	97	4.0	2.0	The data for those tasks were obtained by automatically collapsing the 5-way labels.	0
597	8417	8417	S13-7	RTE perspective and 2-and 3-way Tasks	14	98	4.0	2.0	In the 3-way task, the systems were required to classify the student answer as either (i) correct; (ii) contradictory; or (iii) incorrect (combining the categories partially correct but incomplete, irrelevant and not in the domain from the 5-way classification).	0
598	8418	8418	S13-7	RTE perspective and 2-and 3-way Tasks	15	99	4.0	2.0	In the two-way task, the systems were required to classify the student answer as either correct or incorrect (combining the categories contradictory and incorrect from the 3-way classification)	0
599	8723	8723	S13-9	abstract	1	2	1.0	1.0	The DDIExtraction 2013 task concerns the recognition of drugs and extraction of drugdrug interactions that appear in biomedical literature.	1
600	8724	8724	S13-9	abstract	2	3	2.0	1.0	We propose two subtasks for the DDIExtraction 2013 Shared Task challenge: 1) the recognition and classification of drug names and 2) the extraction and classification of their interactions.	0
601	8725	8725	S13-9	abstract	3	4	3.0	1.0	Both subtasks have been very successful in participation and results.	0
602	8726	8726	S13-9	abstract	4	5	4.0	1.0	There were 14 teams who submitted a total of 38 runs.	0
603	8727	8727	S13-9	abstract	5	6	4.0	1.0	The best result reported for the first subtask was F1 of 71.5% and 65.1% for the second one.	0
604	9204	9204	S13-12	Task Setup	1	25	2.0	1.0	The task required participating systems to annotate nouns in a test corpus with the most appropriate sense from the BabelNet sense inventory or, alternatively, from two main subsets of it, namely the WordNet or Wikipedia sense inventories.	1
605	9205	9205	S13-12	Task Setup	2	26	4.0	1.0	In contrast to previous all-words WSD tasks we did not focus on the other three open classes (i.e., verbs, adjectives and adverbs) since BabelNet does not currently provide non-English coverage for them.	0
606	9582	9582	S14-1	The Task	1	23	1.0	1.0	The Task involved two subtasks.	1
607	9583	9583	S14-1	The Task	2	24	1.0	1.0	(i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them (see below for the exact definition).	1
608	9584	9584	S14-1	The Task	3	25	1.0	1.0	Sentence relatedness scores provide a direct way to evaluate CDSMs, insofar as their outputs are able to quantify the degree of semantic similarity between sentences.	0
609	9585	9585	S14-1	The Task	4	26	2.0	1.0	On the other hand, starting from the assumption that understanding a sentence means knowing when it is true, being able to verify whether an entailment is valid is a crucial challenge for semantic systems.	0
610	9586	9586	S14-1	The Task	5	27	2.0	1.0	In the semantic relatedness subtask, given two sentences, systems were required to produce a relatedness score (on a continuous scale) indicating the extent to which the sentences were expressing a related meaning.	0
611	9587	9587	S14-1	The Task	6	28	2.0	1.0	Table 1 shows examples of sentence pairs with different degrees of semantic relatedness; gold relatedness scores are expressed on a 5-point rating scale.	0
612	9588	9588	S14-1	The Task	7	29	3.0	1.0	In the entailment subtask, given two sentences A and B, systems had to determine whether the meaning of B was entailed by A.	0
613	9589	9589	S14-1	The Task	8	30	3.0	1.0	In particular, systems were required to assign to each pair either the ENTAILMENT label (when A entails B, viz., B cannot be false when A is true), the CONTRA-DICTION label (when A contradicted B, viz.	0
614	9590	9590	S14-1	The Task	9	31	3.0	1.0	B is false whenever A is true), or the NEUTRAL label (when the truth of B could not be determined on the basis of A).	0
615	9591	9591	S14-1	The Task	10	32	4.0	1.0	Table 2 shows examples of sentence pairs holding different entailment relations.	0
616	9592	9592	S14-1	The Task	11	33	4.0	1.0	Participants were invited to submit up to five system runs for one or both subtasks.	0
617	9593	9593	S14-1	The Task	12	34	4.0	2.0	Developers of CDSMs were especially encouraged to participate, but developers of other systems that could tackle sentence relatedness or entailment tasks were also welcome.	0
618	9594	9594	S14-1	The Task	13	35	4.0	2.0	Besides being of intrinsic interest, the latter systems' performance will serve to situate CDSM performance within the broader landscape of computational semantics.	0
619	9694	9694	S14-2	abstract	1	2	1.0	1.0	In this paper we present the SemEval-2014	0
620	9695	9695	S14-2	abstract	2	3	1.0	1.0	Task 2 on spoken dialogue grammar induction.	0
621	9696	9696	S14-2	abstract	3	4	2.0	1.0	The task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems.	1
622	9697	9697	S14-2	abstract	4	5	2.0	1.0	We describe four subtasks covering two languages, English and Greek, and three speech application domains, travel reservation, tourism and finance.	0
623	9698	9698	S14-2	abstract	5	6	3.0	1.0	The classification results are compared against the groundtruth.	0
624	9699	9699	S14-2	abstract	6	7	3.0	1.0	Weighted and unweighted precision, recall and fmeasure are reported.	0
625	9700	9700	S14-2	abstract	7	8	4.0	1.0	Three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training.	0
626	9701	9701	S14-2	abstract	8	9	4.0	1.0	The submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the baseline, averaged across all subtasks.	0
627	9894	9894	S14-3	Introduction	1	7	1.0	1.0	Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning.	0
628	9895	9895	S14-3	Introduction	2	8	1.0	1.0	Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval (Hliaoutakis et al., 2006) or paraphrasing (Glickman and Dagan, 2003).	0
629	9896	9896	S14-3	Introduction	3	9	1.0	1.0	Semantic similarity evaluations have largely focused on comparing similar types of lexical items.	0
630	9897	9897	S14-3	Introduction	4	10	1.0	1.0	Most recently, tasks in SemEval (Agirre et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases.	0
631	9898	9898	S14-3	Introduction	5	11	1.0	1.0	Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License.	0
632	9899	9899	S14-3	Introduction	6	12	2.0	1.0	Page numbers and proceedings footer are added by the organizers.	0
633	9900	9900	S14-3	Introduction	7	13	2.0	1.0	License details: http: //creativecommons.org/licenses/by/4.0/ of Rubenstein and Goodenough (1965) measure similarity between word pairs, while the data sets of Navigli (2006) and Kilgarriff (2001) offer a binary similar-dissimilar distinction between senses.	0
634	9901	9901	S14-3	Introduction	8	14	2.0	1.0	Notably, all of these evaluations have focused on comparisons between a single type, in contrast to application-based evaluations such as summarization and compositionality which incorporate textual items of different sizes, e.g., measuring the quality of a paragraph's sentence summarization.	0
635	9902	9902	S14-3	Introduction	9	15	2.0	1.0	Task 3 introduces a new evaluation where similarity is measured between items of different types: paragraphs, sentences, phrases, words and senses.	0
636	9903	9903	S14-3	Introduction	10	16	2.0	1.0	Given an item of the lexically-larger type, a system measures the degree to which the meaning of the larger item is captured in the smaller type, e.g., comparing a paragraph to a sentence.	1
637	9904	9904	S14-3	Introduction	11	17	3.0	1.0	We refer to this task as Cross-Level Semantic Similarity (CLSS).	0
638	9905	9905	S14-3	Introduction	12	18	3.0	1.0	A major motivation of this task is to produce semantic similarity systems that are able to compare all types of text, thereby freeing downstream NLP applications from needing to consider the type of text being compared.	0
639	9906	9906	S14-3	Introduction	13	19	3.0	1.0	"Task 3 enables assessing the extent to which the meaning of the sentence ""do u know where i can watch free older movies online without download?"" is captured in the phrase ""streaming vintage movies for free"", or how similar is ""circumscribe"" to the phrase ""beating around the bush."""	0
640	9907	9907	S14-3	Introduction	14	20	3.0	1.0	Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality.	0
641	9908	9908	S14-3	Introduction	15	21	3.0	1.0	Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications.	0
642	9909	9909	S14-3	Introduction	16	22	4.0	1.0	Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy andNavigli, 2009), summariza-tion (Sprck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a).	0
643	9910	9910	S14-3	Introduction	17	23	4.0	1.0	Task 3 was designed with three main objectives.	0
644	9911	9911	S14-3	Introduction	18	24	4.0	1.0	First, the task should include multiple types of comparison in order to assess each type's difficulty and whether specialized resources are needed for each.	0
645	9912	9912	S14-3	Introduction	19	25	4.0	1.0	Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types.	0
646	9913	9913	S14-3	Introduction	20	26	4.0	1.0	Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text-and sense-based similarity methods within a single framework.	0
647	10103	10103	S14-4	abstract	1	2	1.0	1.0	Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint.	0
648	10104	10104	S14-4	abstract	2	3	2.0	1.0	The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen).	0
649	10105	10105	S14-4	abstract	3	4	2.0	1.0	SemEval-2014	0
650	10106	10106	S14-4	abstract	4	5	3.0	1.0	Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect.	1
651	10107	10107	S14-4	abstract	5	6	4.0	1.0	The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure.	0
652	10108	10108	S14-4	abstract	6	7	4.0	1.0	It attracted 163 submissions from 32 teams.	0
653	10305	10305	S14-5	Introduction	1	6	1.0	1.0	We present a new cross-lingual and applicationoriented task for SemEval that is situated in the area where Word Sense Disambiguation and Machine Translation meet.	0
654	10306	10306	S14-5	Introduction	2	7	1.0	1.0	Finding the proper translation of a word or phrase in a given context is much like the problem of disambiguating between multiple senses.	0
655	10307	10307	S14-5	Introduction	3	8	2.0	1.0	In this task participants are asked to build a translation/writing assistance system that translates specifically marked L1 fragments in an L2 context to their proper L2 translation.	1
656	10308	10308	S14-5	Introduction	4	9	2.0	1.0	This type of translation can be applied in writing assistance systems for language learners in which users write in a target language, but are allowed to occasionally back off to their native L1 when they are uncertain of the proper lexical or grammatical form in L2.	0
657	10309	10309	S14-5	Introduction	5	10	3.0	1.0	The task concerns the NLP back-end rather than any user interface.	0
658	10310	10310	S14-5	Introduction	6	11	3.0	1.0	Full-on machine translation typically concerns the translation of complete sentences or texts from L1 to L2.	0
659	10311	10311	S14-5	Introduction	7	12	4.0	1.0	This task, in contrast, focuses on smaller fragments, side-tracking the problem of full word reordering.	0
660	10312	10312	S14-5	Introduction	8	13	4.0	1.0	We focus on the following language combinations of L1 and L2 pairs: English-German, English-Spanish, French-English and Dutch-English.	0
661	10313	10313	S14-5	Introduction	9	14	4.0	1.0	Task participants could participate for all language pairs or any subset thereof.	0
662	10685	10685	S14-7	abstract	1	2	1.0	1.0	This paper describes the SemEval-2014, Task 7 on the Analysis of Clinical Text and presents the evaluation results.	0
663	10686	10686	S14-7	abstract	2	3	2.0	1.0	It focused on two subtasks: (i) identification (Task A) and (ii) normalization (Task B) of diseases and disorders in clinical reports as annotated in the Shared Annotated Resources (ShARe) 1 corpus.	1
664	10687	10687	S14-7	abstract	3	4	2.0	1.0	This task was a follow-up to the ShARe/CLEF eHealth 2013 shared task, subtasks 1a and 1b, 2 but using a larger test set.	0
665	10688	10688	S14-7	abstract	4	5	3.0	1.0	A total of 21 teams competed in Task A, and 18 of those also participated in Task B. For Task A, the best system had a strict F 1 -score of 81.3, with a precision of 84.3 and recall of 78.6.	0
666	10689	10689	S14-7	abstract	5	6	3.0	1.0	For Task B, the same group had the best strict accuracy of 74.1.	0
667	10690	10690	S14-7	abstract	6	7	4.0	1.0	The organizers have made the text corpora, annotations, and evaluation tools available for future research and development at the shared task website.	0
668	10691	10691	S14-7	abstract	7	8	4.0	1.0	3 evaluation 3	0
669	10843	10843	S14-8	abstract	1	2	2.0	1.0	Task 8 at SemEval 2014 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning.	1
670	10844	10844	S14-8	abstract	2	3	4.0	1.0	In this task description, we position the problem in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results.	0
671	11153	11153	S14-10	abstract	1	2	1.0	1.0	In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets.	1
672	11154	11154	S14-10	abstract	2	3	2.0	1.0	This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity.	0
673	11155	11155	S14-10	abstract	3	4	2.0	1.0	For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotes-Word	0
674	11156	11156	S14-10	abstract	4	5	3.0	1.0	Net sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings.	0
675	11157	11157	S14-10	abstract	5	6	3.0	1.0	For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire.	0
676	11158	11158	S14-10	abstract	6	7	4.0	1.0	The annotations for both tasks leveraged crowdsourcing.	0
677	11159	11159	S14-10	abstract	7	8	4.0	1.0	The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs.	0
678	11389	11389	S15-1	abstract	1	2	1.0	1.0	In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data.	0
679	11390	11390	S15-1	abstract	2	3	1.0	1.0	Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence.	1
680	11391	11391	S15-1	abstract	3	4	2.0	1.0	The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs.	0
681	11392	11392	S15-1	abstract	4	5	2.0	1.0	A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task.	0
682	11393	11393	S15-1	abstract	5	6	3.0	1.0	The evaluation shows encouraging results and open challenges for future research.	0
683	11394	11394	S15-1	abstract	6	7	3.0	1.0	The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach &gt;0.80 Pearson on well-formed text.	0
684	11395	11395	S15-1	abstract	7	8	4.0	1.0	This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together.	0
685	11396	11396	S15-1	abstract	8	9	4.0	1.0	We make all the data, baseline systems and evaluation scripts publicly available.	0
686	11397	11397	S15-1	abstract	9	10	4.0	1.0	1	0
687	11555	11555	S15-2	abstract	1	2	1.0	1.0	In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets.	1
688	11556	11556	S15-2	abstract	2	3	2.0	1.0	This year, the participants were challenged with new datasets in English and Spanish.	0
689	11557	11557	S15-2	abstract	3	4	2.0	1.0	The annotations for both subtasks leveraged crowdsourcing.	0
690	11558	11558	S15-2	abstract	4	5	3.0	1.0	The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs.	0
691	11559	11559	S15-2	abstract	5	6	3.0	1.0	In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair.	0
692	11560	11560	S15-2	abstract	6	7	4.0	1.0	The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years.	0
693	11561	11561	S15-2	abstract	7	8	4.0	1.0	7 teams participated with 29 runs.	0
694	11830	11830	S15-3	abstract	1	2	1.0	1.0	Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts.	0
695	11831	11831	S15-3	abstract	2	3	2.0	1.0	In this context, we organized SemEval-2015 Task 3 on Answer Selection in cQA, which included two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect to the question, and (b) answering a YES/NO question with yes, no, or unsure, based on the list of all answers.	1
696	11832	11832	S15-3	abstract	3	4	2.0	1.0	We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic.	0
697	11833	11833	S15-3	abstract	4	5	3.0	1.0	We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community.	0
698	11834	11834	S15-3	abstract	5	6	4.0	1.0	Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive.	0
699	11835	11835	S15-3	abstract	6	7	4.0	1.0	The best systems achieved an official score (macro-averaged F 1 ) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.	0
700	12062	12062	S15-4	abstract	1	2	1.0	1.0	This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015.	0
701	12063	12063	S15-4	abstract	2	3	2.0	1.0	Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity, by detecting, anchoring in time and ordering the events involving that entity.	1
702	12064	12064	S15-4	abstract	3	4	3.0	1.0	The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents.	0
703	12065	12065	S15-4	abstract	4	5	4.0	1.0	Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F 1 -score of 7.85 in the main track (timeline creation from raw text).	0
704	12270	12270	S15-5	Task Description	1	27	1.0	1.0	The task for participant systems is equivalent to TempEval-3 task ABC, see Figure 1.	0
705	12271	12271	S15-5	Task Description	2	28	1.0	1.0	Systems must annotate temporal expressions, events, and temporal relations between them 1 .	1
706	12272	12272	S15-5	Task Description	3	29	2.0	1.0	The input to participants is a set of unannotated text documents in TempEval-3 format.	0
707	12273	12273	S15-5	Task Description	4	30	2.0	1.0	Participating systems are required to annotate the plain documents following the TimeML scheme, divided into two types of elements:	0
708	12274	12274	S15-5	Task Description	5	31	3.0	1.0	" Temporal entities: These include events (EVENT tag, ""came"", ""attack"") and temporal expressions (timexes, TIMEX3 tag, e.g., ""yesterday"", ""8 p.m."") as well as their attributes like event class, timex type, and normalized values."	0
709	12275	12275	S15-5	Task Description	6	32	3.0	1.0	 (1) John was in the gym between 6:00 p.m and 7:00 p.m.	0
710	12276	12276	S15-5	Task Description	7	33	4.0	1.0	Each system's annotations represent its temporal knowledge of the documents.	0
711	12277	12277	S15-5	Task Description	8	34	4.0	1.0	These annotations are 1 http://alt.qcri.org/semeval2015/task5	0
712	12278	12278	S15-5	Task Description	9	35	4.0	1.0	Figure 1: Task -Equivalent to TempEval-3 task ABC then used as input to a temporal QA system (Uz-Zaman et al., 2012) that will answer questions on behalf of the systems, and the accuracy of their answers is compared across systems.	0
713	12592	12592	S15-7	abstract	1	2	1.0	1.0	In this paper we describe a novel task, namely the Diachronic Text Evaluation task.	0
714	12593	12593	S15-7	abstract	2	3	2.0	1.0	A corpus of snippets which contain relevant information for the time when the text was created is extracted from a large collection of newspapers published between 1700 and 2010.	0
715	12594	12594	S15-7	abstract	3	4	2.0	1.0	The task, subdivided in three subtasks, requires the automatic system to identify the time interval when the piece of news was written.	1
716	12595	12595	S15-7	abstract	4	5	3.0	1.0	The subtasks concern specific type of information that might be available in news.	0
717	12596	12596	S15-7	abstract	5	6	4.0	1.0	The intervals come in three grades: fine, medium and coarse according to their length.	0
718	12597	12597	S15-7	abstract	6	7	4.0	1.0	The systems participating in the tasks have proved that this a doable task with very interesting possible continuations.	0
719	12852	12852	S15-8	The Task	1	31	4.0	1.0	The goals of SpaceEval include identifying and classifying items from an inventory of spatial concepts:	1
720	12980	12980	S15-9	Introduction	1	7	1.0	1.0	Current research in sentiment analysis (SA, henceforth) is mostly focused on lexical resources that store polarity values.	0
721	12981	12981	S15-9	Introduction	2	8	1.0	1.0	For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items.	0
722	12982	12982	S15-9	Introduction	3	9	1.0	1.0	This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events -involving perspectives and points of view -are expressed.	0
723	12983	12983	S15-9	Introduction	4	10	2.0	1.0	In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions -mainly adjectives, adverbs and several nouns -leaving verbs on the foreground.	0
724	12984	12984	S15-9	Introduction	5	11	2.0	1.0	Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009;	0
725	12985	12985	S15-9	Introduction	6	12	2.0	1.0	Akkaya et al., 2009, Balhaur et al., 2011	0
726	12986	12986	S15-9	Introduction	7	13	3.0	1.0	Russo et al. 2011;Cambria et al., 2012, Deng et al., 2013.	0
727	12987	12987	S15-9	Introduction	8	14	3.0	1.0	One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity.	0
728	12988	12988	S15-9	Introduction	9	15	3.0	1.0	By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment.	0
729	12989	12989	S15-9	Introduction	10	16	4.0	1.0	Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions.	0
730	12990	12990	S15-9	Introduction	11	17	4.0	1.0	Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (Lewinsohn and Amenson, 1978;	1
731	12991	12991	S15-9	Introduction	12	18	4.0	1.0	MacPhillamy and Lewinsohn, 1982).	0
732	13357	13357	S15-10	Conclusion	1	252	1.0	4.0	We have described the five subtasks organized as part of SemEval-2015 Task 10 on Sentiment Analysis in Twitter: detecting sentiment of terms in context (subtask A), classifiying the sentiment of an entire tweet, SMS message or blog post (subtask B), predicting polarity towards a topic (subtask C), quantifying polarity towards a topic (subtask D), and proposing real-valued prior sentiment scores for Twitter terms (subtask E).	1
733	13358	13358	S15-10	Conclusion	2	253	1.0	4.0	Over 40 teams participated in these subtasks, using various techniques.	0
734	13359	13359	S15-10	Conclusion	3	254	2.0	4.0	We plan a new edition of the task as part of SemEval-2016, where we will focus on sentiment with respect to a topic, but this time on a fivepoint scale, which is used for human review ratings on popular websites such as Amazon, TripAdvisor, Yelp, etc.	0
735	13360	13360	S15-10	Conclusion	4	255	2.0	4.0	From a research perspective, moving to an ordered five-point scale means moving from binary classification to ordinal regression.	0
736	13361	13361	S15-10	Conclusion	5	256	3.0	4.0	We further plan to continue the trend detection subtask, which represents a move from classification to quantification, and is on par with what applications need.	0
737	13362	13362	S15-10	Conclusion	6	257	3.0	4.0	They are not interested in the sentiment of a particular tweet but rather in the percentage of tweets that are positive/negative.	0
738	13363	13363	S15-10	Conclusion	7	258	4.0	4.0	Finally, we plan a new subtask on trend detection, but using a five-point scale, which would get us even closer to what business (e.g. marketing studies), and researchers, (e.g. in political science or public policy), want nowadays.	0
739	13364	13364	S15-10	Conclusion	8	259	4.0	4.0	From a research perspective, this is a problem of ordinal quantification.	0
740	13400	13400	S15-11	Task Description	1	36	1.0	2.0	The task concerns itself with the classification of overall sentiment in micro-texts drawn from the micro-blogging service Twitter.	0
741	13401	13401	S15-11	Task Description	2	37	2.0	2.0	These texts, called tweets, are chosen so that the set as a whole contains a great deal of irony, sarcasm or metaphor, so no particular tweet is guaranteed to manifest a specific figurative phenomenon.	0
742	13402	13402	S15-11	Task Description	3	38	2.0	2.0	Since irony and sarcasm are typically used to criticize or to mock, and thus skew the perception of sentiment toward the negative, it is not enough for a system to simply determine whether the sentiment of a given tweet is positive or negative.	0
743	13403	13403	S15-11	Task Description	4	39	3.0	2.0	We thus use an 11point scale, ranging from -5 (very negative, for tweets with highly critical meanings) to +5 (very positive, for tweets with flattering or very upbeat meanings).	0
744	13404	13404	S15-11	Task Description	5	40	3.0	2.0	The point 0 on this scale is used for neutral tweets, or those whose positivity and negativity cancel each other out.	0
745	13405	13405	S15-11	Task Description	6	41	4.0	2.0	While the majority of tweets will have sentiments in the negative part of the scale, the challenge for participating systems is to decide just how negative or positive a tweet seems to be.	0
746	13406	13406	S15-11	Task Description	7	42	4.0	2.0	So, given a set of tweets that are rich in metaphor, sarcasm and irony, the goal is to determine whether a user has expressed a positive, negative or neutral sentiment in each, and the degree to which this sentiment has been communicated.	1
747	13508	13508	S15-12	abstract	1	2	1.0	1.0	SemEval-2015	0
748	13509	13509	S15-12	abstract	2	3	1.0	1.0	Task 12, a continuation of SemEval-2014	0
749	13510	13510	S15-12	abstract	3	4	2.0	1.0	Task 4, aimed to foster research beyond sentence-or text-level sentiment classification towards Aspect Based Sentiment Analysis.	0
750	13511	13511	S15-12	abstract	4	5	2.0	1.0	The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price).	1
751	13512	13512	S15-12	abstract	5	6	3.0	1.0	The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure.	0
752	13513	13513	S15-12	abstract	6	7	3.0	1.0	It attracted 93 submissions from 16 teams.	0
753	13514	13514	S15-12	abstract	7	8	4.0	1.0	1	0
754	13515	13515	S15-12	abstract	8	9	4.0	1.0	A subset of the datasets has been annotated with aspects at the sentence level.	0
755	13888	13888	S15-14	abstract	1	2	1.0	1.0	We describe two tasks-named entity recognition (Task 1) and template slot filling (Task 2)-for clinical texts.	1
756	13889	13889	S15-14	abstract	2	3	1.0	1.0	The tasks leverage annotations from the ShARe corpus, which consists of clinical notes with annotated mentions disorders, along with their normalization to a medical terminology and eight additional attributes.	0
757	13890	13890	S15-14	abstract	3	4	2.0	1.0	The purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling.	0
758	13891	13891	S15-14	abstract	4	5	2.0	1.0	Task 2 consisted of two subtasks: template slot filling given gold-standard disorder spans (Task 2a) and end-to-end disorder span identification together with template slot filling (Task 2b).	0
759	13892	13892	S15-14	abstract	5	6	2.0	1.0	For Task 1 (disorder span detection and normalization), 16 teams participated.	0
760	13893	13893	S15-14	abstract	6	7	3.0	1.0	The best system yielded a strict F1-score of 75.7, with a precision of 78.3 and recall of 73.2.	0
761	13894	13894	S15-14	abstract	7	8	3.0	1.0	For Task 2a (template slot filling given goldstandard disorder spans), six teams participated.	0
762	13895	13895	S15-14	abstract	8	9	4.0	1.0	The best system yielded a combined overall weighted accuracy for slot filling of 88.6.	0
763	13896	13896	S15-14	abstract	9	10	4.0	1.0	For Task 2b (disorder recognition and template slot filling), nine teams participated.	0
764	13897	13897	S15-14	abstract	10	11	4.0	1.0	The best system yielded a combined relaxed F (for span detection) and overall weighted accuracy of 80.8.	0
765	14227	14227	S15-15	Conclusion	1	184	1.0	4.0	This paper introduces a new SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis.	0
766	14228	14228	S15-15	Conclusion	2	185	1.0	4.0	Dictionary entry building is split into three subtasks: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically.	1
767	14229	14229	S15-15	Conclusion	3	186	2.0	4.0	Drawing from the Pattern Dictionary of English Verbs, we have produced a high-quality resource for the advancement of semantic processing: it contains 121 verbs connected to a corpus of 17,000 sentences.	0
768	14230	14230	S15-15	Conclusion	4	187	2.0	4.0	This resource will be made freely accessible from the task website for more in depth future research.	0
769	14231	14231	S15-15	Conclusion	5	188	2.0	4.0	Task 15 has attracted 5 participants, 3 on subtask 1 and 2 on subtask 2.	0
770	14232	14232	S15-15	Conclusion	6	189	3.0	4.0	Subtask 1 proved to be more difficult for participants than expected, since no system beat the baseline.	0
771	14233	14233	S15-15	Conclusion	7	190	3.0	4.0	We however show that the submissions possess interesting features that should be put to use in future experiments on the dataset.	0
772	14234	14234	S15-15	Conclusion	8	191	3.0	4.0	Subtask 2's baseline was beaten by one of the participants on a large margin, despite the fact that the baseline is very competitive.	0
773	14235	14235	S15-15	Conclusion	9	192	4.0	4.0	It seems that splitting the task into 3 subtasks has had the benefit of attracting different approaches (supervised and unsupervised) towards the common target of the task, which is to build a dictionary entry.	0
774	14236	14236	S15-15	Conclusion	10	193	4.0	4.0	Lexicography is such a complex task that it needs major efforts from the NLP community to support it.	0
775	14237	14237	S15-15	Conclusion	11	194	4.0	4.0	We hope that this task will stimulate more research and the development of new approaches to the automatic creation of lexical resources.	0
776	14239	14239	S15-17	abstract	1	2	1.0	1.0	This paper describes the first shared task on Taxonomy Extraction Evaluation organised as part of SemEval-2015.	0
777	14240	14240	S15-17	abstract	2	3	2.0	1.0	Participants were asked to find hypernym-hyponym relations between given terms.	1
778	14241	14241	S15-17	abstract	3	4	3.0	1.0	For each of the four selected target domains the participants were provided with two lists of domainspecific terms: a WordNet collection of terms and a well-known terminology extracted from an online publicly available taxonomy.	0
779	14242	14242	S15-17	abstract	4	5	4.0	1.0	A total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures, the structural similarity with a gold standard taxonomy, and through manual quality assessment of sampled novel relations.	0
780	14388	14388	S15-18	abstract	1	2	2.0	1.0	Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate-argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning.	1
781	14389	14389	S15-18	abstract	2	3	4.0	1.0	In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results.	0
782	14634	14634	S16-1	Task Overview	1	79	1.0	2.0	STS presents participating systems with paired text snippets of approximately one sentence in length.	0
783	14635	14635	S16-1	Task Overview	2	80	1.0	2.0	The systems are then asked to return a numerical score indicating the degree of semantic similarity between the two snippets.	1
784	14636	14636	S16-1	Task Overview	3	81	1.0	2.0	Canonical STS scores fall on an ordinal scale with 6 specifically defined degrees of semantic similarity (see Table 1).	0
785	14637	14637	S16-1	Task Overview	4	82	1.0	2.0	While the underlying labels and their interpretation are ordinal, systems can provide real valued scores to indicate their semantic similarity prediction.	0
786	14638	14638	S16-1	Task Overview	5	83	1.0	2.0	Participating systems are then evaluated based on the degree to which their predicted similarity scores correlate with STS human judgements.	0
787	14639	14639	S16-1	Task Overview	6	84	2.0	2.0	Algorithms are free to use any scale or range of values for the scores they return.	0
788	14640	14640	S16-1	Task Overview	7	85	2.0	2.0	They are not punished for outputting scores outside the range of the interpretable human annotated STS labels.	0
789	14641	14641	S16-1	Task Overview	8	86	2.0	2.0	This evaluation strategy is motivated by a desire to maximize the flexibility in the design of machine learning models and systems for STS.	0
790	14642	14642	S16-1	Task Overview	9	87	2.0	2.0	It reinforces the assumption that computing textual similarity is an enabling component for other natural language processing applications, rather than being an end in itself.	0
791	14643	14643	S16-1	Task Overview	10	88	2.0	2.0	Table 1 illustrates the ordinal similarity scale the shared task uses.	0
792	14644	14644	S16-1	Task Overview	11	89	2.0	2.0	Both the English and the crosslingual Spanish-English STS subtasks use a 6 point similarity scale.	0
793	14645	14645	S16-1	Task Overview	12	90	3.0	2.0	A similarity label of 0 means that two texts are completely dissimilar; this can be interpreted as two sentences with no overlap in their meanings.	0
794	14646	14646	S16-1	Task Overview	13	91	3.0	2.0	The next level up, a similarity label of 1, indicates that the two snippets are not equivalent but are topically related to each other.	0
795	14647	14647	S16-1	Task Overview	14	92	3.0	2.0	A label of 2 indicates that the two texts are still not equivalent but agree on some details of what is being said.	0
796	14648	14648	S16-1	Task Overview	15	93	3.0	2.0	The labels 3 and 4, both indicate that the two sentences are approximately equivalent.	0
797	14649	14649	S16-1	Task Overview	16	94	3.0	2.0	However, a score of 3 implies that there are some differences in important details, while a score of 4 indicates that the differing details are not important.	0
798	14650	14650	S16-1	Task Overview	17	95	3.0	2.0	The top score of 5, denotes that the two texts being evaluated have complete semantic equivalence.	0
799	14651	14651	S16-1	Task Overview	18	96	4.0	2.0	In the context of the STS task, meaning equivalence is defined operationally as two snippets of text that mean the same thing when interpreted by a reasonable human judge.	0
800	14652	14652	S16-1	Task Overview	19	97	4.0	2.0	The operational approach to sentence level semantics was popularized by the recognizing textual entailment task (Dagan et al., 2010).	0
801	14653	14653	S16-1	Task Overview	20	98	4.0	2.0	It has the advantage that it allows the labeling of sentence pairs by human annotators without any training in formal semantics, while also being more useful and intuitive to work with for downstream systems.	0
802	14654	14654	S16-1	Task Overview	21	99	4.0	2.0	Beyond just sentence level semantics, the operationally defined STS labels also reflect both world knowledge and pragmatic phenomena.	0
803	14655	14655	S16-1	Task Overview	22	100	4.0	2.0	As in prior years, 2016 shared task participants are allowed to make use of existing resources and tools (e.g., WordNet, Mikolov et al. (2013)'s word2vec).	0
804	14656	14656	S16-1	Task Overview	23	101	4.0	2.0	Participants are also allowed to make unsupervised use of arbitrary data sets, even if such data overlaps with the announced sources of the evaluation data.	0
805	14849	14849	S16-2	Introduction	1	8	1.0	1.0	Semantic Textual Similarity (STS) (Agirre et al., 2015) measures the degree of equivalence in the underlying semantics of paired snippets of text.	0
806	14850	14850	S16-2	Introduction	2	9	1.0	1.0	The idea of Interpretable STS (iSTS) is to explain why two sentences may be related/unrelated, by supplementing the STS similarity score with an explanatory layer.	0
807	14851	14851	S16-2	Introduction	3	10	1.0	1.0	Our final goal would be to enable interpretable systems, that is, systems that are able to explain which are the differences and commonalities between two sentences.	0
808	14852	14852	S16-2	Introduction	4	11	1.0	1.0	For instance, let's assume the following two sentences drawn from a corpus of news headlines: 12 killed in bus accident in Pakistan 10 killed in road accident in NW Pakistan * * Authors listed in alphabetical order 1 http://at.qrci.org/semeval2016/task2/	0
809	14853	14853	S16-2	Introduction	5	12	1.0	1.0	The output of such a system would be something like the following:	0
810	14854	14854	S16-2	Introduction	6	13	1.0	1.0	The two sentences talk about accidents with casualties in Pakistan, but they differ in the number of people killed (12 vs. 10) and level of detail: the first one specifies that it is a bus accident, and the second one specifies that the location is NW Pakistan.	0
811	14855	14855	S16-2	Introduction	7	14	1.0	1.0	While giving such explanations comes naturally to people, constructing algorithms and computational models that mimic human level performance represents a difficult Natural Language Understanding (NLU) problem, with applications in dialogue systems, interactive systems and educational systems.	0
812	14856	14856	S16-2	Introduction	8	15	1.0	1.0	In the iSTS 2015 pilot task (Agirre et al., 2015), we defined a first step of such an ambitious system, which we follow in 2016.	0
813	14857	14857	S16-2	Introduction	9	16	1.0	1.0	Given the input (a pair of sentences), participant systems need first to identify the chunks in each sentence, and then, align chunks across the two sentences, indicating the relation and similarity score of each alignment.	1
814	14858	14858	S16-2	Introduction	10	17	2.0	1.0	The relation can be one of equivalence, opposition, specificity, similarity or relatedness, and the similarity score can range from 1 to 5.	0
815	14859	14859	S16-2	Introduction	11	18	2.0	1.0	Unrelated chunks are left unaligned.	0
816	14860	14860	S16-2	Introduction	12	19	2.0	1.0	An optional tag can be added to alignments for the cases where there is a difference in factuality or polarity.	0
817	14861	14861	S16-2	Introduction	13	20	2.0	1.0	See Figure 1 for the manual alignment of the two sample sentences.	0
818	14862	14862	S16-2	Introduction	14	21	2.0	1.0	The alignments between chunks in Figure 1 can be used to produce the kind of explanations shown in the previous example.	0
819	14863	14863	S16-2	Introduction	15	22	2.0	1.0	"In previous work, Brockett (2007) and Rus et al. (2012) produced a dataset where corresponding Figure 1: Example of a manual alignment of two sentences: ""12 killed in bus accident in Pakistan"" and ""10 killed in road accident in NW Pakistan""."	0
820	14864	14864	S16-2	Introduction	16	23	2.0	1.0	Each aligned pair of chunks included information on the type of alignment, and the score of alignment.	0
821	14865	14865	S16-2	Introduction	17	24	2.0	1.0	words (including some multiword expressions like named-entities) were aligned.	0
822	14866	14866	S16-2	Introduction	18	25	2.0	1.0	Although this alignment is useful, we wanted to move forward to the alignment of segments, and decided to align chunks (Abney, 1991).	0
823	14867	14867	S16-2	Introduction	19	26	3.0	1.0	Brockett (2007) did not provide any label to alignments, while Rus et al. ( 2012) defined a basic typology.	0
824	14868	14868	S16-2	Introduction	20	27	3.0	1.0	In our task, we provided a more detailed typology for the aligned chunks as well as a similarity/relatedness score for each alignment.	0
825	14869	14869	S16-2	Introduction	21	28	3.0	1.0	Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them.	0
826	14870	14870	S16-2	Introduction	22	29	3.0	1.0	"In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the ""facets"" (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer."	0
827	14871	14871	S16-2	Introduction	23	30	3.0	1.0	The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment.	0
828	14872	14872	S16-2	Introduction	24	31	3.0	1.0	This model was later followed by Levy et al. (2013).	0
829	14873	14873	S16-2	Introduction	25	32	3.0	1.0	Our task was different in that we identified the corresponding chunks in both sentences.	0
830	14874	14874	S16-2	Introduction	26	33	3.0	1.0	We think that, in the future, the aligned facets could provide complementary information to chunks.	0
831	14875	14875	S16-2	Introduction	27	34	3.0	1.0	The SemEval Semantic Textual Similarity (STS) task in 2015 contained a subtask on Interpretable STS (Agirre et al., 2015), showing that the task is feasible, with high inter-annotator agreement and system scores well above baselines.	0
832	14876	14876	S16-2	Introduction	28	35	4.0	1.0	The datasets comprised news headlines and image captions.	0
833	14877	14877	S16-2	Introduction	29	36	4.0	1.0	For 2016, the pilot subtask has been updated into a standalone task.	0
834	14878	14878	S16-2	Introduction	30	37	4.0	1.0	The restriction from the iSTS 2015 task to allow only one-to-one alignments has been now lifted, and we thus allow any number of chunks to be aligned to any number of chunks.	0
835	14879	14879	S16-2	Introduction	31	38	4.0	1.0	Annotation guidelines have been revised accordingly, including an updated chunking criterium for subordinate clauses and a better explanation of the instruc-tions.	0
836	14880	14880	S16-2	Introduction	32	39	4.0	1.0	The 2015 datasets were re-annotated and released as training data.	0
837	14881	14881	S16-2	Introduction	33	40	4.0	1.0	New pairs from news headlines and image captions have been annotated and used for test.	0
838	14882	14882	S16-2	Introduction	34	41	4.0	1.0	In addition, a new dataset of sentence pairs from the education domain has been produced, including train and test data.	0
839	14883	14883	S16-2	Introduction	35	42	4.0	1.0	The paper is organized as follows.	0
840	14884	14884	S16-2	Introduction	36	43	4.0	1.0	We first provide the description of the task, followed by the evaluation metrics and the baseline system.	0
841	14885	14885	S16-2	Introduction	37	44	4.0	1.0	Section 5 describes the participation, Section 6 the results, and Section 7 comments on the systems, tools and resources used.	0
842	15053	15053	S16-3	Introduction	1	14	1.0	1.0	"Building on the success of SemEval-2015 Task 3 ""Answer Selection in Community Question Answering"" 1 , we run an extension in 2016, which covers a full task on Community Question Answering (CQA) and which is, therefore, closer to the real application needs."	0
843	15054	15054	S16-3	Introduction	2	15	1.0	1.0	All the information related to the task, data, participants, results and publications can be found on the SemEval-2016 Task 3 website.	0
844	15055	15055	S16-3	Introduction	3	16	1.0	1.0	2 1 http://alt.qcri.org/semeval2015/task3 2 http://alt.qcri.org/semeval2016/task3 CQA forums such as Stack Overflow 3 and Qatar Living 4 , are gaining popularity online.	0
845	15056	15056	S16-3	Introduction	4	17	1.0	1.0	These forums are seldom moderated, quite open, and thus they typically have little restrictions, if any, on who can post and who can answer a question.	0
846	15057	15057	S16-3	Introduction	5	18	1.0	1.0	On the positive side, this means that one can freely ask any question and can then expect some good, honest answers.	0
847	15058	15058	S16-3	Introduction	6	19	1.0	1.0	On the negative side, it takes effort to go through all possible answers and to make sense of them.	0
848	15059	15059	S16-3	Introduction	7	20	1.0	1.0	For example, it is not unusual for a question to have hundreds of answers, which makes it very time-consuming for the user to inspect and to winnow through them all.	0
849	15060	15060	S16-3	Introduction	8	21	1.0	1.0	The present task could help to automate the process of finding good answers to new questions in a community-created discussion forum, e.g., by retrieving similar questions in the forum and by identifying the posts in the comment threads of those similar questions that answer the original question well.	0
850	15061	15061	S16-3	Introduction	9	22	1.0	1.0	"In essence, the main CQA task can be defined as follows: ""given (i) a new question and (ii) a large collection of question-comment threads created by a user community, rank the comments that are most useful for answering the new question""."	1
851	15062	15062	S16-3	Introduction	10	23	2.0	1.0	The test question is new with respect to the collection, but it is expected to be related to one or several questions in the collection.	0
852	15063	15063	S16-3	Introduction	11	24	2.0	1.0	The best answers can come from different question-comment threads.	0
853	15064	15064	S16-3	Introduction	12	25	2.0	1.0	In the collection, the threads are independent of each other and the lists of comments are chronologically sorted and contain some meta information, e.g., date, user, topic, etc.	0
854	15065	15065	S16-3	Introduction	13	26	2.0	1.0	The comments in a particular thread are intended to answer the question initiating that thread, but since this is a resource created by a community of casual users, there is a lot of noise and irrelevant material, apart from informal language usage and lots of typos and grammatical mistakes.	0
855	15066	15066	S16-3	Introduction	14	27	2.0	1.0	Interestingly, the questions in the collection can be semantically related to each other, although not explicitly.	0
856	15067	15067	S16-3	Introduction	15	28	2.0	1.0	Our intention was not to run just another regular Question Answering task.	0
857	15068	15068	S16-3	Introduction	16	29	2.0	1.0	"Similarly to the 2015 edition, we had three objectives: (i) to focus on semantic-based solutions beyond simple ""bag-ofwords"" representations and ""word matching"" techniques; (ii) to study the new natural language processing (NLP) phenomena arising in the community question answering scenario, e.g., relations between the comments in a thread, relations between different threads and question-to-question similarity; and (iii) to facilitate the participation of non IR/QA experts to our challenge."	0
858	15069	15069	S16-3	Introduction	17	30	2.0	1.0	The third point was achieved by explicitly providing the set of potential answers-the search engine step was carried out by us-to be (re)ranked and by defining two optional subtasks apart from the main CQA task.	0
859	15070	15070	S16-3	Introduction	18	31	2.0	1.0	Subtask A (Question-Comment Similarity): given a question from a question-comment thread, rank the comments according to their relevance (similarity) with respect to the question; Subtask B (Question-Question Similarity): given the new question, rerank all similar questions retrieved by a search engine, assuming that the answers to the similar questions should be answering the new question too.	0
860	15071	15071	S16-3	Introduction	19	32	3.0	1.0	Subtasks	0
861	15072	15072	S16-3	Introduction	20	33	3.0	1.0	A and B should give participants enough tools to create a CQA system to solve the main task.	0
862	15073	15073	S16-3	Introduction	21	34	3.0	1.0	Nonetheless, one can approach CQA without necessarily solving the two tasks above.	0
863	15074	15074	S16-3	Introduction	22	35	3.0	1.0	Participants were free to use whatever approach they wanted, and the participation in the main task and/or the two subtasks was optional.	0
864	15075	15075	S16-3	Introduction	23	36	3.0	1.0	A more precise definition of all subtasks can be found in Section 3.	0
865	15076	15076	S16-3	Introduction	24	37	3.0	1.0	Keeping the multilinguality from 2015, we provided data for two languages: English and Arabic.	0
866	15077	15077	S16-3	Introduction	25	38	3.0	1.0	For English, we used real data from the communitycreated Qatar Living forum.	0
867	15078	15078	S16-3	Introduction	26	39	3.0	1.0	The Arabic data was collected from medical forums, with a slightly different procedure.	0
868	15079	15079	S16-3	Introduction	27	40	3.0	1.0	We only proposed the main ranking CQA task on this data, i.e., finding good answers for a given new question.	0
869	15080	15080	S16-3	Introduction	28	41	4.0	1.0	Finally, we provided training data for all languages and subtasks with human supervision.	0
870	15081	15081	S16-3	Introduction	29	42	4.0	1.0	All examples were manually labeled by a community of annotators in a crowdsourcing platform.	0
871	15082	15082	S16-3	Introduction	30	43	4.0	1.0	The datasets and the annotation procedure are described in Section 4, and some examples can be found in Figures 3 and 4.	0
872	15083	15083	S16-3	Introduction	31	44	4.0	1.0	The rest of the paper is organized as follows: Section 2 introduces some related work.	0
873	15084	15084	S16-3	Introduction	32	45	4.0	1.0	Section 3 gives a more detailed definition of the task.	0
874	15085	15085	S16-3	Introduction	33	46	4.0	1.0	Section 4 describes the datasets and the process of their creation.	0
875	15086	15086	S16-3	Introduction	34	47	4.0	1.0	Section 5 explains the evaluation measures.	0
876	15087	15087	S16-3	Introduction	35	48	4.0	1.0	Section 6 presents the results for all subtasks and for all participating systems.	0
877	15088	15088	S16-3	Introduction	36	49	4.0	1.0	Section 7 summarizes the main approaches and features used by these systems.	0
878	15089	15089	S16-3	Introduction	37	50	4.0	1.0	Finally, Section 8 offers some further discussion and presents the main conclusions.	0
879	15390	15390	S16-4	abstract	1	2	1.0	1.0	"This paper discusses the fourth year of the ""Sentiment Analysis in Twitter Task""."	0
880	15391	15391	S16-4	abstract	2	3	1.0	1.0	SemEval-2016	0
881	15392	15392	S16-4	abstract	3	4	2.0	1.0	Task 4 comprises five subtasks, three of which represent a significant departure from previous editions.	0
882	15393	15393	S16-4	abstract	4	5	2.0	1.0	The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet.	1
883	15394	15394	S16-4	abstract	5	6	3.0	1.0	"The three new subtasks focus on two variants of the basic ""sentiment classification in Twitter"" task."	0
884	15395	15395	S16-4	abstract	6	7	3.0	1.0	The first variant adopts a five-point scale, which confers an ordinal character to the classification task.	0
885	15396	15396	S16-4	abstract	7	8	4.0	1.0	The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature.	0
886	15397	15397	S16-4	abstract	8	9	4.0	1.0	The task continues to be very popular, attracting a total of 43 teams.	0
887	15398	15398	S16-4	abstract	9	10	4.0	1.0	* Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy.	0
888	15686	15686	S16-5	Introduction	1	6	1.0	1.0	Many consumers use the Web to share their experiences about products, services or travel destinations (Yoo and Gretzel, 2008).	0
889	15687	15687	S16-5	Introduction	2	7	1.0	1.0	Online opinionated texts (e.g., reviews, tweets) are important for consumer decision making (Chevalier and Mayzlin, 2006) and constitute a source of valuable customer feedback that can help companies to measure satisfaction and improve their products or services.	0
890	15688	15688	S16-5	Introduction	3	8	1.0	1.0	In this setting, Aspect Based Sentiment Analysis (ABSA) -i.e., mining opinions from text about specific entities and their aspects (Liu, 2012) -can provide valuable insights to both consumers and businesses.	1
891	15689	15689	S16-5	Introduction	4	9	1.0	1.0	An ABSA * *Corresponding author: mpontiki@ilsp.gr. method can analyze large amounts of unstructured texts and extract (coarse-or fine-grained) information not included in the user ratings that are available in some review sites (e.g., Fig. 1).	0
892	15690	15690	S16-5	Introduction	5	10	1.0	1.0	"Sentiment Analysis (SA) touches every aspect (e.g., entity recognition, coreference resolution, negation handling) of Natural Language Processing (Liu, 2012) and as Cambria et al. (2013) mention ""it requires a deep understanding of the explicit and implicit, regular and irregular, and syntactic and semantic language rules""."	0
893	15691	15691	S16-5	Introduction	6	11	1.0	1.0	Within the last few years several SA-related shared tasks have been organized in the context of workshops and conferences focus-ing on somewhat different research problems (Seki et al., 2007;	0
894	15692	15692	S16-5	Introduction	7	12	1.0	1.0	Seki et al., 2008;	0
895	15693	15693	S16-5	Introduction	8	13	1.0	1.0	Seki et al., 2010;	0
896	15694	15694	S16-5	Introduction	9	14	1.0	1.0	Mitchell, 2013;	0
897	15695	15695	S16-5	Introduction	10	15	2.0	1.0	Nakov et al., 2013;	0
898	15696	15696	S16-5	Introduction	11	16	2.0	1.0	Rosenthal et al., 2014;	0
899	15697	15697	S16-5	Introduction	12	17	2.0	1.0	Pontiki et al., 2014;	0
900	15698	15698	S16-5	Introduction	13	18	2.0	1.0	Rosenthal et al., 2015;	0
901	15699	15699	S16-5	Introduction	14	19	2.0	1.0	Ghosh et al., 2015;	0
902	15700	15700	S16-5	Introduction	15	20	2.0	1.0	Pontiki et al., 2015;Mohammad et al., 2016;	0
903	15701	15701	S16-5	Introduction	16	21	2.0	1.0	Recupero and Cambria, 2014;	0
904	15702	15702	S16-5	Introduction	17	22	2.0	1.0	Ruppenhofer et al., 2014;	0
905	15703	15703	S16-5	Introduction	18	23	2.0	1.0	Loukachevitch et al., 2015).	0
906	15704	15704	S16-5	Introduction	19	24	3.0	1.0	Such competitions provide training datasets and the opportunity for direct comparison of different approaches on common test sets.	0
907	15705	15705	S16-5	Introduction	20	25	3.0	1.0	Currently, most of the available SA-related datasets, whether released in the context of shared tasks or not (Socher et al., 2013;	0
908	15706	15706	S16-5	Introduction	21	26	3.0	1.0	Ganu et al., 2009), are monolingual and usually focus on English texts.	0
909	15707	15707	S16-5	Introduction	22	27	3.0	1.0	Multilingual datasets (Klinger and Cimiano, 2014;	0
910	15708	15708	S16-5	Introduction	23	28	3.0	1.0	Jimnez-Zafra et al., 2015) provide additional benefits enabling the development and testing of crosslingual methods (Lambert, 2015).	0
911	15709	15709	S16-5	Introduction	24	29	3.0	1.0	Following this direction, this year the SemEval ABSA task provided datasets in a variety of languages.	0
912	15710	15710	S16-5	Introduction	25	30	3.0	1.0	"ABSA was introduced as a shared task for the first time in the context of SemEval in 2014; SemEval-2014 Task 4 1 (SE-ABSA14) provided datasets of English reviews annotated at the sentence level with aspect terms (e.g., ""mouse"", ""pizza"") and their polarity for the laptop and restaurant domains, as well as coarser aspect categories (e.g., ""food"") and their polarity only for restaurants (Pontiki et al., 2014)."	0
913	15711	15711	S16-5	Introduction	26	31	3.0	1.0	SemEval-2015 Task 12 2 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks into a unified framework in which all the identified constituents of the expressed opinions (i.e., aspects, opinion target expressions and sentiment polarities) meet a set of guidelines and are linked to each other within sentence-level tuples (Pontiki et al., 2015).	0
914	15712	15712	S16-5	Introduction	27	32	3.0	1.0	These tuples are important since they indicate the part of text within which a specific opinion is expressed.	0
915	15713	15713	S16-5	Introduction	28	33	4.0	1.0	However, a user might also be interested in the overall rating of the text towards a particular aspect.	0
916	15714	15714	S16-5	Introduction	29	34	4.0	1.0	Such ratings can be used to estimate the mean sentiment per aspect from multiple reviews (McAuley et al., 2012).	0
917	15715	15715	S16-5	Introduction	30	35	4.0	1.0	Therefore, in addition to sentence-level annotations, SE-ABSA16 3 accommodated also text-level ABSA annotations and provided the respective training and testing data.	0
918	15716	15716	S16-5	Introduction	31	36	4.0	1.0	Fur-1 http://alt.qcri.org/semeval2014/task4/ 2 http://alt.qcri.org/semeval2015/task12/ 3 http://alt.qcri.org/semeval2016/task5/ thermore, the SE-ABSA15 annotation framework was extended to new domains and applied to languages other than English (Arabic, Chinese, Dutch, French, Russian, Spanish, and Turkish).	0
919	15717	15717	S16-5	Introduction	32	37	4.0	1.0	The remainder of this paper is organized as follows: the task set-up is described in Section 2.	0
920	15718	15718	S16-5	Introduction	33	38	4.0	1.0	Section 3 provides information about the datasets and the annotation process, while Section 4 presents the evaluation measures and the baselines.	0
921	15719	15719	S16-5	Introduction	34	39	4.0	1.0	General information about participation in the task is provided in Section 5.	0
922	15720	15720	S16-5	Introduction	35	40	4.0	1.0	The evaluation scores of the participating systems are presented and discussed in Section 6.	0
923	15721	15721	S16-5	Introduction	36	41	4.0	1.0	The paper concludes with an overall assessment of the task.	0
924	15888	15888	S16-6	abstract	1	2	1.0	1.0	Here for the first time we present a shared task on detecting stance from tweets: given a tweet and a target entity (person, organization, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely.	1
925	15889	15889	S16-6	abstract	2	3	2.0	1.0	The target of interest may or may not be referred to in the tweet, and it may or may not be the target of opinion.	0
926	15890	15890	S16-6	abstract	3	4	2.0	1.0	Two tasks are proposed.	0
927	15891	15891	S16-6	abstract	4	5	3.0	1.0	Task A is a traditional supervised classification task where 70% of the annotated data for a target is used as training and the rest for testing.	0
928	15892	15892	S16-6	abstract	5	6	3.0	1.0	For Task B, we use as test data all of the instances for a new target (not used in task A) and no training data is provided.	0
929	15893	15893	S16-6	abstract	6	7	4.0	1.0	Our shared task received submissions from 19 teams for Task A and from 9 teams for Task B.	0
930	15894	15894	S16-6	abstract	7	8	4.0	1.0	The highest classification F-score obtained was 67.82 for Task A and 56.28 for Task B. However, systems found it markedly more difficult to infer stance towards the target of interest from tweets that express opinion towards another entity.	0
931	16163	16163	S16-7	Task Description	1	50	1.0	2.0	The task is formulated as follows: given a list of terms (single words and multi-word phrases), an automatic system needs to provide a score between 0 and 1 that is indicative of the term's strength of association with positive sentiment.	1
932	16164	16164	S16-7	Task Description	2	51	1.0	2.0	A score of 1 indicates maximum association with positive sentiment (or least association with negative sentiment) and a score of 0 indicates least association with positive sentiment (or maximum association with negative sentiment).	0
933	16165	16165	S16-7	Task Description	3	52	1.0	2.0	If a term is more positive than another, then it should have a higher score than the other.	0
934	16166	16166	S16-7	Task Description	4	53	1.0	2.0	There are three tasks, one for each of the three domains:	0
935	16167	16167	S16-7	Task Description	5	54	1.0	2.0	 General English Sentiment Modifiers Set:	0
936	16168	16168	S16-7	Task Description	6	55	1.0	2.0	This dataset comprises English single words and multi-word phrases from the general domain.	0
937	16169	16169	S16-7	Task Description	7	56	2.0	2.0	The phrases are formed by combining a word and a modifier, where a modifier is a negator, an auxilary verb, a degree adverb, or a combination of those, for example, would be very easy, did not harm, and would have been nice.	0
938	16170	16170	S16-7	Task Description	8	57	2.0	2.0	The single word terms are chosen from the set of words that are part of the multi-word phrases, for example, easy, harm, and nice.	0
939	16171	16171	S16-7	Task Description	9	58	2.0	2.0	 English Twitter Mixed Polarity Set:	0
940	16172	16172	S16-7	Task Description	10	59	2.0	2.0	This dataset focuses on English phrases made up of opposing polarity terms, for example, phrases such as lazy sundays, best winter break, happy accident and couldn't stop smiling.	0
941	16173	16173	S16-7	Task Description	11	60	2.0	2.0	The dataset also includes single word terms (as separate entries).	0
942	16174	16174	S16-7	Task Description	12	61	2.0	2.0	These terms are chosen from the set of words that are part of the multi-word phrases.	0
943	16175	16175	S16-7	Task Description	13	62	2.0	2.0	The multi-word phrases and single-word terms are drawn from a corpus of tweets, and include a small number of hashtag words (e.g., #wantit) and creatively spelled words (e.g., plssss).	0
944	16176	16176	S16-7	Task Description	14	63	3.0	2.0	However, a majority of the terms are those that one would use in everyday English.	0
945	16177	16177	S16-7	Task Description	15	64	3.0	2.0	 Arabic Twitter Set:	0
946	16178	16178	S16-7	Task Description	16	65	3.0	2.0	This dataset includes single words and phrases commonly found in Arabic tweets.	0
947	16179	16179	S16-7	Task Description	17	66	3.0	2.0	The phrases in this set are formed only by combining a negator and a word.	0
948	16180	16180	S16-7	Task Description	18	67	3.0	2.0	Teams could participate in any one, two, or all three tasks; however, only one submission was al-  lowed per task.	0
949	16181	16181	S16-7	Task Description	19	68	3.0	2.0	For each task, the above description and a development set (200 terms) were provided to the participants in advance; there were no training sets.	0
950	16182	16182	S16-7	Task Description	20	69	4.0	2.0	The three test sets, one for each task, were released at the start of the evaluation period.	0
951	16183	16183	S16-7	Task Description	21	70	4.0	2.0	The test sets and the development sets have no terms in common.	0
952	16184	16184	S16-7	Task Description	22	71	4.0	2.0	The participants were allowed to use the development sets in any way (for example, for tuning or training), and they were allowed to use any additional manually or automatically generated resources.	0
953	16185	16185	S16-7	Task Description	23	72	4.0	2.0	In 2015, the task was set up similarly (Rosenthal et al., 2015).	0
954	16186	16186	S16-7	Task Description	24	73	4.0	2.0	Single words and multi-word phrases from English Twitter comprised the development and test sets (1,515 terms in total).	0
955	16187	16187	S16-7	Task Description	25	74	4.0	2.0	The phrases were simple two-word negated expressions (e.g., cant waitttt).	0
956	16188	16188	S16-7	Task Description	26	75	4.0	2.0	Participants were allowed to use these datasets for the development of their systems.	0
957	16314	16314	S16-8	abstract	1	2	1.0	1.0	In this report we summarize the results of the SemEval 2016 Task 8: Meaning Representation Parsing.	0
958	16315	16315	S16-8	abstract	2	3	1.0	1.0	Participants were asked to generate Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the news and discussion forum domains.	1
959	16316	16316	S16-8	abstract	3	4	2.0	1.0	Eleven sites submitted valid systems.	0
960	16317	16317	S16-8	abstract	4	5	2.0	1.0	The availability of state-of-the-art baseline systems was a key factor in lowering the bar to entry; many submissions relied on CAMR (Wang et al., 2015b;	0
961	16318	16318	S16-8	abstract	5	6	3.0	1.0	Wang et al., 2015a) as a baseline system and added extensions to it to improve scores.	0
962	16319	16319	S16-8	abstract	6	7	3.0	1.0	The evaluation set was quite difficult to parse, particularly due to creative approaches to word representation in the web forum portion.	0
963	16320	16320	S16-8	abstract	7	8	4.0	1.0	The top scoring systems scored 0.62 F1 according to the Smatch (Cai and Knight, 2013) evaluation heuristic.	0
964	16321	16321	S16-8	abstract	8	9	4.0	1.0	We show some sample sentences along with a comparison of system parses and perform quantitative ablative studies.	0
965	16483	16483	S16-9	abstract	1	2	1.0	1.0	This paper describes the SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing.	0
966	16484	16484	S16-9	abstract	2	3	2.0	1.0	We extend the traditional treestructured representation of Chinese sentence to directed acyclic graphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences.	1
967	16485	16485	S16-9	abstract	3	4	3.0	1.0	We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXT-BOOKS domain with 14,793 sentences respectively.	0
968	16486	16486	S16-9	abstract	4	5	4.0	1.0	We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on.	0
969	16487	16487	S16-9	abstract	5	6	4.0	1.0	At last, we briefly describe the submitted systems and analyze these results.	0
970	16623	16623	S16-10	Introduction	1	5	1.0	1.0	Grammatical analysis tasks, e.g., part-of-speech tagging, are rather successful applications of natural language processing (NLP).	0
971	16624	16624	S16-10	Introduction	2	6	1.0	1.0	They are comprehensive, i.e., they operate under the assumption that all grammatically-relevant parts of a sentence will be analyzed:	0
972	16625	16625	S16-10	Introduction	3	7	1.0	1.0	We do not expect a POS tagger to only know a subset of the tags in the language.	0
973	16626	16626	S16-10	Introduction	4	8	2.0	1.0	Most POS tags accommodate unseen words and adapt readily to new text genres.	0
974	16627	16627	S16-10	Introduction	5	9	2.0	1.0	Together, these factors indicate a representation which achieves broad coverage.	0
975	16628	16628	S16-10	Introduction	6	10	2.0	1.0	Explicit analysis of lexical semantics, by contrast, has been more difficult to scale to broad coverage owing to limited comprehensiveness and extensibility.	0
976	16629	16629	S16-10	Introduction	7	11	2.0	1.0	The dominant paradigm of fine-grained word sense disambiguation, WordNet (Fellbaum, 1998), is difficult to annotate in corpora, results in considerable data sparseness, and does not readily generalize to out-of-vocabulary words.	0
977	16630	16630	S16-10	Introduction	8	12	3.0	1.0	While the main corpus with WordNet senses, SemCor (Miller et al., 1993), does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech.	0
978	16631	16631	S16-10	Introduction	9	13	3.0	1.0	This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora.	0
979	16632	16632	S16-10	Introduction	10	14	3.0	1.0	To address this limitation, in the DiMSUM 2016 shared task, 1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following Schneider and Smith, 2015), on multiple nontraditional genres of text.	1
980	16633	16633	S16-10	Introduction	11	15	3.0	1.0	By moving away from fine-grained sense inventories and lexicalized, language-specific 2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis.	0
981	16634	16634	S16-10	Introduction	12	16	4.0	1.0	We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres.	0
982	16635	16635	S16-10	Introduction	13	17	4.0	1.0	The integrated lexical semantic representation ( 2, 3) has been annotated in an extensive benchmark data set comprising several nontraditional domains ( 4).	0
983	16636	16636	S16-10	Introduction	14	18	4.0	1.0	Objective, controlled evaluation procedures ( 5) facilitate a comparison of the 9 systems submitted as part of the official task ( 6).	0
984	16637	16637	S16-10	Introduction	15	19	4.0	1.0	While the systems range in performance, all are below 60% in our composite evaluation, suggesting that further work is needed to make progress on this difficult task.	0
985	16913	16913	S16-11	Task Description	1	26	1.0	1.0	The Complex Word Identification task of SemEval 2016 invited participants to create systems that, given a sentence and a target word within it, can predict whether or not a non-native English speaker would be able to understand the meaning of the target word.	1
986	16914	16914	S16-11	Task Description	2	27	1.0	1.0	We chose non-native speakers as a target audience because, unlike second language learners and those with low literacy levels or conditions such as Aphasia and Dyslexia, non-native speakers of English have not yet been explicitly assessed with respect to their simplification needs.	0
987	16915	16915	S16-11	Task Description	3	28	2.0	1.0	In addition, the broad availability of such an audience makes data collection more feasible.	0
988	16916	16916	S16-11	Task Description	4	29	2.0	1.0	We have established main goals for the task:	0
989	16917	16917	S16-11	Task Description	5	30	3.0	1.0	1	0
990	16918	16918	S16-11	Task Description	6	31	3.0	1.0	To learn which words challenge non-native English speakers and to understand what their traits are.	0
991	16919	16919	S16-11	Task Description	7	32	4.0	1.0	2	0
992	16920	16920	S16-11	Task Description	8	33	4.0	1.0	To investigate how well one's individual vocabulary limitations can be predicted from the overall vocabulary limitations of others in the same category.	0
993	17092	17092	S16-12	Introduction	1	7	1.0	1.0	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007;	0
994	17093	17093	S16-12	Introduction	2	8	1.0	1.0	Verhagen et al., 2010;UzZaman et al., 2013).	0
995	17094	17094	S16-12	Introduction	3	9	1.0	1.0	Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations.	1
996	17095	17095	S16-12	Introduction	4	10	2.0	1.0	However, the Temp-Eval campaigns to date have focused primarily on in-document timelines derived from news articles.	0
997	17096	17096	S16-12	Introduction	5	11	2.0	1.0	In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; to broaden our understanding of the language of time beyond newswire expressions and structure.	0
998	17097	17097	S16-12	Introduction	6	12	2.0	1.0	Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation.	0
999	17098	17098	S16-12	Introduction	7	13	2.0	1.0	Participating systems are expected to take as input raw text, for example: April 23, 2014:	0
1000	17099	17099	S16-12	Introduction	8	14	3.0	1.0	The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
1001	17100	17100	S16-12	Introduction	9	15	3.0	1.0	The systems are then expected to output annotations over the text, for example, those shown in Figure 1.	0
1002	17101	17101	S16-12	Introduction	10	16	3.0	1.0	That is, the systems should identify the time expressions, event expressions, attributes of those expressions, and temporal relations between them.	0
1003	17102	17102	S16-12	Introduction	11	17	3.0	1.0	Clinical TempEval 2016 addressed one of the major challenges in Clinical TempEval 2015: data distribution.	0
1004	17103	17103	S16-12	Introduction	12	18	4.0	1.0	Because Clinical TempEval is based on real patient notes from the Mayo Clinic, participants go through a lengthy authorization process involving a data use agreement and an interview.	0
1005	17104	17104	S16-12	Introduction	13	19	4.0	1.0	For Clinical TempEval 2016, we streamlined this process and were able to authorize data access for more than twice as many participants as Clinical TempEval 2015.	0
1006	17105	17105	S16-12	Introduction	14	20	4.0	1.0	And since all the training and evaluation data distributed for Clinical TempEval 2015 was used as the training data for Clinical TempEval 2016, participants had more than a year to work on their systems.	0
1007	17106	17106	S16-12	Introduction	15	21	4.0	1.0	The result was that four times as many teams participated.	0
1008	17206	17206	S16-13	abstract	1	2	1.0	1.0	This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016.	0
1009	17207	17207	S16-13	abstract	2	3	2.0	1.0	This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them.	1
1010	17208	17208	S16-13	abstract	3	4	3.0	1.0	TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science.	0
1011	17209	17209	S16-13	abstract	4	5	4.0	1.0	A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations.	0
1012	17405	17405	S16-14	abstract	1	2	1.0	1.0	Manually constructed taxonomies provide a crucial resource for many NLP technologies, yet these resources are often limited in their lexical coverage due to their construction procedure.	0
1013	17406	17406	S16-14	abstract	2	3	2.0	1.0	While multiple approaches have been proposed to enrich such taxonomies with new concepts, these techniques are typically evaluated by measuring the accuracy at identifying relationships between words, e.g., that a dog is a canine, rather relationships between specific concepts.	0
1014	17407	17407	S16-14	abstract	3	4	2.0	1.0	Task 14 provides an evaluation framework for automatic taxonomy enrichment techniques by measuring the placement of a new concept into an existing taxonomy: Given a new word and its definition, systems were asked to attach or merge the concept into an existing WordNet concept.	1
1015	17408	17408	S16-14	abstract	4	5	3.0	1.0	Five teams submitted 13 systems to the task, all of which were able to improve over the random baseline system.	0
1016	17409	17409	S16-14	abstract	5	6	4.0	1.0	However, only one participating system outperformed the second, morecompetitive baseline that attaches a new term to the first word in its gloss with the appropriate part of speech, which indicates that techniques must be adapted to exploit the structure of glosses.	0
1017	17410	17410	S16-14	abstract	6	7	4.0	1.0	the new synset as a hyponym of S in the Word-Net's subsumption hierarchy.	0
1018	17609	17609	S17-1	Task Overview	1	40	1.0	1.0	STS is the assessment of pairs of sentences according to their degree of semantic similarity.	0
1019	17610	17610	S17-1	Task Overview	2	41	1.0	1.0	The task involves producing real-valued similarity scores for sentence pairs.	1
1020	17611	17611	S17-1	Task Overview	3	42	2.0	1.0	Performance is measured by the Pearson correlation of machine scores with human judgments.	0
1021	17612	17612	S17-1	Task Overview	4	43	2.0	1.0	The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence.	0
1022	17613	17613	S17-1	Task Overview	5	44	3.0	1.0	Intermediate values reflect interpretable levels of partial overlap in meaning.	0
1023	17614	17614	S17-1	Task Overview	6	45	3.0	1.0	The annotation scale is designed to be accessible by reasonable human judges without any formal expertise in linguistics.	0
1024	17615	17615	S17-1	Task Overview	7	46	4.0	1.0	Using reasonable human interpretations of natural language semantics was popularized by the related textual entailment task (Dagan et al., 2010).	0
1025	17616	17616	S17-1	Task Overview	8	47	4.0	1.0	The resulting annotations reflect both pragmatic and world knowledge and are more interpretable and useful within downstream systems.	0
1026	17849	17849	S17-2	abstract	1	2	1.0	1.0	This paper introduces a new task on Multilingual and Cross-lingual Semantic WordSimilarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish.	1
1027	17850	17850	S17-2	abstract	2	3	2.0	1.0	High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark).	0
1028	17851	17851	S17-2	abstract	3	4	2.0	1.0	These were used for semi-automatic construction of ten cross-lingual datasets.	0
1029	17852	17852	S17-2	abstract	4	5	3.0	1.0	17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2.	0
1030	17853	17853	S17-2	abstract	5	6	4.0	1.0	Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks.	0
1031	17854	17854	S17-2	abstract	6	7	4.0	1.0	More information can be found on the task website:	0
1032	18044	18044	S17-3	abstract	1	2	1.0	1.0	We describe SemEval2017 Task 3 on Community Question Answering.	0
1033	18045	18045	S17-3	abstract	2	3	2.0	1.0	This year, we reran the four subtasks from SemEval-2016: (A) Question-Comment Similarity, (B) Question-Question Similarity, (C) Question-External Comment Similarity, and (D) Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing.	1
1034	18046	18046	S17-3	abstract	3	4	2.0	1.0	Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums.	1
1035	18047	18047	S17-3	abstract	4	5	3.0	1.0	A total of 23 teams participated in the task, and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks A-D. Unfortunately, no teams participated in subtask E. A variety of approaches and features were used by the participating systems to address the different subtasks.	0
1036	18048	18048	S17-3	abstract	5	6	4.0	1.0	The best systems achieved an official score (MAP) of 88.43, 47.22,  15.46, and 61.16  in subtasks A, B, C, and D, respectively.	0
1037	18049	18049	S17-3	abstract	6	7	4.0	1.0	These scores are better than the baselines, especially for subtasks A-C.	0
1038	18720	18720	S17-5	abstract	1	2	1.0	1.0	"This paper discusses the ""Fine-Grained Sentiment Analysis on Financial Microblogs and News"" task as part of SemEval-2017, specifically under the ""Detecting sentiment, humour, and truth"" theme."	0
1039	18721	18721	S17-5	abstract	2	3	2.0	1.0	This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines.	0
1040	18722	18722	S17-5	abstract	3	4	3.0	1.0	The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks.	1
1041	18723	18723	S17-5	abstract	4	5	4.0	1.0	The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment.	0
1042	18724	18724	S17-5	abstract	5	6	4.0	1.0	This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2.	0
1043	19011	19011	S17-6	abstract	1	3	1.0	1.0	This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead.	0
1044	19012	19012	S17-6	abstract	2	4	2.0	1.0	The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the 'Hashtag Wars' segment of the TV show @midnight.	0
1045	19013	19013	S17-6	abstract	3	5	2.0	1.0	The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are.	1
1046	19014	19014	S17-6	abstract	4	6	3.0	1.0	7 teams participated in subtask A, and 5 teams participated in subtask B.	0
1047	19015	19015	S17-6	abstract	5	7	4.0	1.0	The best accuracy in subtask A was 0.675.	0
1048	19016	19016	S17-6	abstract	6	8	4.0	1.0	The best (lowest) rank edit distance for subtask B was 0.872.	0
1049	19262	19262	S17-7	abstract	1	2	1.0	1.0	A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect.	0
1050	19263	19263	S17-7	abstract	2	3	2.0	1.0	Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-percontext assumption.	0
1051	19264	19264	S17-7	abstract	3	4	3.0	1.0	This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns.	1
1052	19265	19265	S17-7	abstract	4	5	4.0	1.0	We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set.	0
1053	19266	19266	S17-7	abstract	5	6	4.0	1.0	Finally, we present an overview and discussion of the participating systems' methodologies, resources, and results.	0
1054	19517	19517	S17-8	Introduction and Motivation	1	7	1.0	1.0	Rumours are rife on the web.	0
1055	19518	19518	S17-8	Introduction and Motivation	2	8	1.0	1.0	False claims affect people's perceptions of events and their behaviour, sometimes in harmful ways.	0
1056	19519	19519	S17-8	Introduction and Motivation	3	9	1.0	1.0	With the increasing reliance on the Web -social media, in particularas a source of information and news updates by individuals, news professionals, and automated systems, the potential disruptive impact of rumours is further accentuated.	0
1057	19520	19520	S17-8	Introduction and Motivation	4	10	1.0	1.0	The task of analysing and determining veracity of social media content has been of recent interest to the field of natural language processing.	0
1058	19521	19521	S17-8	Introduction and Motivation	5	11	1.0	1.0	After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014;	0
1059	19522	19522	S17-8	Introduction and Motivation	6	12	2.0	1.0	Zhang et al., 2015;	0
1060	19523	19523	S17-8	Introduction and Motivation	7	13	2.0	1.0	Shao et al., 2016;Zubiaga et al., 2016b).	0
1061	19524	19524	S17-8	Introduction and Motivation	8	14	2.0	1.0	Veracity judgment can be decomposed intuitively in terms of a comparison between assertions made in -and entailments from -a candidate text, and external world knowledge.	0
1062	19525	19525	S17-8	Introduction and Motivation	9	15	2.0	1.0	Intermediate linguistic cues have also been shown to play a role.	0
1063	19526	19526	S17-8	Introduction and Motivation	10	16	2.0	1.0	Critically, based on recent work the task appears deeply nuanced and very challenging, while having important applications in, for example, journalism and disaster mitigation (Hermida, 2012;	0
1064	19527	19527	S17-8	Introduction and Motivation	11	17	3.0	1.0	Procter et al., 2013a;	0
1065	19528	19528	S17-8	Introduction and Motivation	12	18	3.0	1.0	Veil et al., 2011).	0
1066	19529	19529	S17-8	Introduction and Motivation	13	19	3.0	1.0	We propose a shared task where participants analyse rumours in the form of claims made in user-generated content, and where users respond to one another within conversations attempting to resolve the veracity of the rumour.	1
1067	19530	19530	S17-8	Introduction and Motivation	14	20	3.0	1.0	"We define a rumour as a ""circulating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient scepticism and/or anxiety so as to motivate finding out the actual truth"" (Zubiaga et al., 2015b)."	0
1068	19531	19531	S17-8	Introduction and Motivation	15	21	3.0	1.0	While breaking news unfold, gathering opinions and evidence from as many sources as possible as communities react becomes crucial to determine the veracity of rumours and consequently reduce the impact of the spread of misinformation.	0
1069	19532	19532	S17-8	Introduction and Motivation	16	22	4.0	1.0	Within this scenario where one needs to listen to, and assess the testimony of, different sources to make a final decision with respect to a rumour's veracity, we ran a task in SemEval consisting of two subtasks: (a) stance classification towards rumours, and (b) veracity classification.	0
1070	19533	19533	S17-8	Introduction and Motivation	17	23	4.0	1.0	Subtask	0
1071	19534	19534	S17-8	Introduction and Motivation	18	24	4.0	1.0	A corresponds to the core problem in crowd response analysis when using discourse around claims to verify or disprove them.	0
1072	19535	19535	S17-8	Introduction and Motivation	19	25	4.0	1.0	Subtask B corresponds to the AI-hard task of assessing directly whether or not a claim is false.	0
1073	19536	19536	S17-8	Introduction and Motivation	20	26	4.0	1.0	determine how other users in social media regard the rumour (Procter et al., 2013b).	0
1074	19537	19537	S17-8	Introduction and Motivation	21	27	4.0	1.0	We propose to tackle this analysis by looking at the conversation stemming from direct and nested replies to the tweet originating the rumour (source tweet).	0
1075	19665	19665	S17-9	title	1	1	4.0	1.0	SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation	1
1076	19666	19666	S17-9	abstract	1	2	1.0	1.0	In this report we summarize the results of the 2017 AMR SemEval shared task.	0
1077	19667	19667	S17-9	abstract	2	3	2.0	1.0	The task consisted of two separate yet related subtasks.	0
1078	19668	19668	S17-9	abstract	3	4	2.0	1.0	In the parsing subtask, participants were asked to produce Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the biomedical domain.	0
1079	19669	19669	S17-9	abstract	4	5	3.0	1.0	In the generation subtask, participants were asked to generate English sentences given AMR graphs in the news/forum domain.	0
1080	19670	19670	S17-9	abstract	5	6	4.0	1.0	A total of five sites participated in the parsing subtask, and four participated in the generation subtask.	0
1081	19671	19671	S17-9	abstract	6	7	4.0	1.0	Along with a description of the task and the participants' systems, we show various score ablations and some sample outputs.	0
1082	19854	19854	S17-10	abstract	1	3	1.0	1.0	We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials.	1
1083	19855	19855	S17-10	abstract	2	4	2.0	1.0	Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios.	0
1084	19856	19856	S17-10	abstract	3	5	3.0	1.0	We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.	0
1085	19857	19857	S17-10	abstract	4	6	4.0	1.0	Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction.	0
1086	19858	19858	S17-10	abstract	5	7	4.0	1.0	These tasks are related to the tasks of named entity recognition, named entity	0
1087	20036	20036	S17-11	abstract	1	2	2.0	1.0	This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language.	1
1088	20037	20037	S17-11	abstract	2	3	4.0	1.0	It aims to support the evaluation and evolution of the discussions surrounding the application natural language processing techniques within the context of end-user natural language programming, under scenarios of high lexical and semantic heterogeneity.	0
1089	20239	20239	S17-12	Introduction	1	7	1.0	1.0	The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007(Verhagen et al., , 2010UzZaman et al., 2013).	0
1090	20240	20240	S17-12	Introduction	2	8	1.0	1.0	In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures.	0
1091	20241	20241	S17-12	Introduction	3	9	2.0	1.0	In the Clinical TempEval shared tasks (Bethard et al., 2015(Bethard et al., , 2016, participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations.	1
1092	20242	20242	S17-12	Introduction	4	10	2.0	1.0	For example, Figure 1 shows the annotations that a system is expected to produce when given the text:	0
1093	20243	20243	S17-12	Introduction	5	11	2.0	1.0	April 23, 2014:	0
1094	20244	20244	S17-12	Introduction	6	12	3.0	1.0	The patient did not have any postoperative bleeding so we'll resume chemotherapy with a larger bolus on Friday even if there is slight nausea.	0
1095	20245	20245	S17-12	Introduction	7	13	3.0	1.0	Clinical TempEval 2017 introduced a new aspect to this problem: domain adaptation.	0
1096	20246	20246	S17-12	Introduction	8	14	4.0	1.0	Whereas in Clinical TempEval 2015 and 2016, systems were both trained and tested on notes from colon cancer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients.	0
1097	20247	20247	S17-12	Introduction	9	15	4.0	1.0	The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients.	0
1098	20248	20248	S17-12	Introduction	10	16	4.0	1.0	As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016.	0
1099	20341	20341	S18-1	1 Introduction	1	9	1.0	1.0	Emotions are central to language and thought.	0
1100	20342	20342	S18-1	1 Introduction	2	10	1.0	1.0	They are familiar and commonplace, yet they are complex and nuanced.	0
1101	20343	20343	S18-1	1 Introduction	3	11	1.0	1.0	Humans are known to perceive hundreds of different emotions.	0
1102	20344	20344	S18-1	1 Introduction	4	12	1.0	1.0	According to the basic emotion model (aka the categorical model) (Ekman, 1992;	0
1103	20345	20345	S18-1	1 Introduction	5	13	1.0	1.0	Plutchik, 1980;Parrot, 2001;Frijda, 1988), some emotions, such as joy, sadness, and fear, are more basic than others-physiologically, cognitively, and in terms of the mechanisms to express these emotions.	0
1104	20346	20346	S18-1	1 Introduction	6	14	1.0	1.0	Each of these emotions can be felt or expressed in varying intensities.	0
1105	20347	20347	S18-1	1 Introduction	7	15	1.0	1.0	For example, our utterances can convey that we are very angry, slightly sad, absolutely elated, etc.	0
1106	20348	20348	S18-1	1 Introduction	8	16	1.0	1.0	Here, intensity refers to the degree or amount of an emotion such as anger or sadness.	0
1107	20349	20349	S18-1	1 Introduction	9	17	1.0	1.0	1	0
1108	20350	20350	S18-1	1 Introduction	10	18	1.0	1.0	As per the valence-arousal-dominance (VAD) model (Russell, 1980(Russell, , 2003, emotions are points in a three-dimensional space of valence (positivenessnegativeness), arousal (active-passive), and dominance (dominant-submissive).	0
1109	20351	20351	S18-1	1 Introduction	11	19	2.0	1.0	We use the term affect to refer to various emotion-related categories such as joy, fear, valence, and arousal.	0
1110	20352	20352	S18-1	1 Introduction	12	20	2.0	1.0	Natural language applications in commerce, public health, disaster management, and public policy can benefit from knowing the affectual states of people-both the categories and the intensities of the emotions they feel.	0
1111	20353	20353	S18-1	1 Introduction	13	21	2.0	1.0	We thus present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks where automatic systems have to infer the affectual state of a person from their tweet.	1
1112	20354	20354	S18-1	1 Introduction	14	22	2.0	1.0	2	0
1113	20355	20355	S18-1	1 Introduction	15	23	2.0	1.0	We will refer to the author of a tweet as the tweeter.	0
1114	20356	20356	S18-1	1 Introduction	16	24	2.0	1.0	Some of the tasks are on the intensities of four basic emotions common to many proposals of basic emotions: anger, fear, joy, and sadness.	0
1115	20357	20357	S18-1	1 Introduction	17	25	2.0	1.0	Some of the tasks are on valence or sentiment intensity.	0
1116	20358	20358	S18-1	1 Introduction	18	26	2.0	1.0	Finally, we include an emotion classification task over eleven emotions commonly expressed in tweets.	0
1117	20359	20359	S18-1	1 Introduction	19	27	2.0	1.0	3 For each task, we provide separate training, development, and test datasets for English, Arabic, and Spanish tweets.	0
1118	20360	20360	S18-1	1 Introduction	20	28	2.0	1.0	The tasks are as follows:	0
1119	20361	20361	S18-1	1 Introduction	21	29	2.0	1.0	1. Emotion Intensity Regression (EI-reg):	0
1120	20362	20362	S18-1	1 Introduction	22	30	3.0	1.0	Given a tweet and an emotion E, determine the intensity of E that best represents the mental state of the tweeter-a real-valued score between 0 (least E) and 1 (most E);	0
1121	20363	20363	S18-1	1 Introduction	23	31	3.0	1.0	2. Emotion Intensity Ordinal Classification (EIoc):	0
1122	20364	20364	S18-1	1 Introduction	24	32	3.0	1.0	Given a tweet and an emotion E, classify the tweet into one of four ordinal classes of intensity of E that best represents the mental state of the tweeter; 3. Valence (Sentiment) Regression (V-reg):	0
1123	20365	20365	S18-1	1 Introduction	25	33	3.0	1.0	Given a tweet, determine the intensity of sentiment or valence (V) that best represents the mental state of the tweeter-a real-valued score between 0 (most negative) and 1 (most positive);	0
1124	20366	20366	S18-1	1 Introduction	26	34	3.0	1.0	4. Valence Ordinal Classification (V-oc):	0
1125	20367	20367	S18-1	1 Introduction	27	35	3.0	1.0	Given a tweet, classify it into one of seven ordinal classes, corresponding to various levels of positive and negative sentiment intensity, that best represents the mental state of the tweeter;	0
1126	20368	20368	S18-1	1 Introduction	28	36	3.0	1.0	5. Emotion Classification (E-c):	0
1127	20369	20369	S18-1	1 Introduction	29	37	3.0	1.0	Given a tweet, classify it as 'neutral or no emotion' or as one, or more, of eleven given emotions that best represent the mental state of the tweeter.	0
1128	20370	20370	S18-1	1 Introduction	30	38	3.0	1.0	Here, E refers to emotion, EI refers to emotion intensity, V refers to valence, reg refers to regression, oc refers to ordinal classification, c refers to classification.	0
1129	20371	20371	S18-1	1 Introduction	31	39	3.0	1.0	For each language, we create a large single textual dataset, subsets of which are annotated for many emotion (or affect) dimensions (from both the basic emotion model and the VAD model).	0
1130	20372	20372	S18-1	1 Introduction	32	40	3.0	1.0	For each emotion dimension, we annotate the data not just for coarse classes (such as anger or no anger) but also for fine-grained real-valued scores indicating the intensity of emotion.	0
1131	20373	20373	S18-1	1 Introduction	33	41	4.0	1.0	We use Best-Worst Scaling (BWS), a comparative annotation method, to address the limitations of traditional rating scale methods such as inter-and intraannotator inconsistency.	0
1132	20374	20374	S18-1	1 Introduction	34	42	4.0	1.0	We show that the finegrained intensity scores thus obtained are reliable (repeat annotations lead to similar scores).	0
1133	20375	20375	S18-1	1 Introduction	35	43	4.0	1.0	In total, about 700,000 annotations were obtained from about 22,000 English, Arabic, and Spanish tweets.	0
1134	20376	20376	S18-1	1 Introduction	36	44	4.0	1.0	Seventy-five teams (about 200 team members) participated in the shared task, making this the largest SemEval shared task to date.	0
1135	20377	20377	S18-1	1 Introduction	37	45	4.0	1.0	In total, 319 submissions were made to the 15 task-language pairs.	0
1136	20378	20378	S18-1	1 Introduction	38	46	4.0	1.0	Each team was allowed only one official submission for each task-language pair.	0
1137	20379	20379	S18-1	1 Introduction	39	47	4.0	1.0	We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful.	0
1138	20380	20380	S18-1	1 Introduction	40	48	4.0	1.0	We also analyze system predictions for consistent bias towards a particular race or gender using a corpus specifically compiled for that purpose.	0
1139	20381	20381	S18-1	1 Introduction	41	49	4.0	1.0	We find that a majority of systems consistently assign higher scores to sentences involving one race or gender.	0
1140	20382	20382	S18-1	1 Introduction	42	50	4.0	1.0	We also find that the bias may change depending on the specific affective dimension being predicted.	0
1141	20383	20383	S18-1	1 Introduction	43	51	4.0	1.0	All of the tweet data (labeled and unlabeled), annotation questionnaires, evaluation scripts, and the bias evaluation corpus are made freely available on the task website.	0
1142	20813	20813	S18-2	Task Description	1	56	1.0	2.0	Given a text message including an emoji, the emoji prediction task consists of predicting that emoji by relying exclusively on the textual content of that message.	1
1143	20814	20814	S18-2	Task Description	2	57	1.0	2.0	In particular, in this task we focused on the one emoji occurring inside tweets, thus relying on Twitter data.	0
1144	20815	20815	S18-2	Task Description	3	58	1.0	2.0	Last hike in our awesome camping weekend!	0
1145	20816	20816	S18-2	Task Description	4	59	2.0	2.0	The task is divided into two subtasks respectively dealing with the prediction of the emoji associated to English and Spanish tweets.	0
1146	20817	20817	S18-2	Task Description	5	60	2.0	2.0	The motivation for providing a multilingual setting stems from previous findings about the idiosyncrasy of use of emojis across languages (Barbieri et al., 2016b) (see Figure 3): one emoji may be used with completely different meanings depending not only on the language of the speaker, but also on regional dialects (Barbieri et al., 2016a).	0
1147	20818	20818	S18-2	Task Description	6	61	2.0	2.0	For each subtask we selected the tweets that included one of the twenty emojis that occur most frequently in the Twitter data we collected (Table 1).	0
1148	20819	20819	S18-2	Task Description	7	62	3.0	2.0	Therefore, the task can be viewed as a multilabel classification problem with twenty labels.	0
1149	20820	20820	S18-2	Task Description	8	63	3.0	2.0	Twitter datasets were shared among participants by providing a list of tweet IDs 4 or directly the 4 Participants were provided with a Javabased crawler (https://github.com/fra82/ twitter-crawler) to ease the download of the textual	0
1150	20821	20821	S18-2	Task Description	9	64	3.0	2.0	It's flipping hot out here!	0
1151	20822	20822	S18-2	Task Description	10	65	4.0	2.0	Iniciamos el nuevo ao con ilusin!	0
1152	20823	20823	S18-2	Task Description	11	66	4.0	2.0	Figure 3: Example of distinct use of the fire emoji across languages: the first tweet (English) comments on the torrid weather, while the second one (Spanish) exploits the same emoji to wish an happy new year ('We start the new year with enthusiasm!').	0
1153	20824	20824	S18-2	Task Description	12	67	4.0	2.0	text of each tweet.	0
1154	20825	20825	S18-2	Task Description	13	68	4.0	2.0	The last approach was adopted to share the test sets (more details are provided in Section 4).	0
1155	20904	20904	S18-3	Introduction	1	8	1.0	1.0	The development of the social web has stimulated the use of figurative and creative language, including irony, in public (Ghosh et al., 2015).	0
1156	20905	20905	S18-3	Introduction	2	9	1.0	1.0	From a philosophical/psychological perspective, discerning the mechanisms that underlie ironic speech improves our understanding of human reasoning and communication, and more and more, this interest in understanding irony also emerges in the machine learning community (Wallace, 2015).	0
1157	20906	20906	S18-3	Introduction	3	10	1.0	1.0	Although an unanimous definition of irony is still lacking in the literature, it is often identified as a trope whose actual meaning differs from what is literally enunciated.	0
1158	20907	20907	S18-3	Introduction	4	11	1.0	1.0	Due to its nature, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce human language.	0
1159	20908	20908	S18-3	Introduction	5	12	1.0	1.0	In fact, automatic irony detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis.	0
1160	20909	20909	S18-3	Introduction	6	13	1.0	1.0	Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)).	0
1161	20910	20910	S18-3	Introduction	7	14	1.0	1.0	However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets.	0
1162	20911	20911	S18-3	Introduction	8	15	2.0	1.0	The results revealed that, while sentiment classification performance on regular tweets reached up to F 1 = 0.71, scores on the ironic tweets varied between F 1 = 0.29 and F 1 = 0.57.	0
1163	20912	20912	S18-3	Introduction	9	16	2.0	1.0	In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012;	0
1164	20913	20913	S18-3	Introduction	10	17	2.0	1.0	Maynard and Greenwood, 2014;	0
1165	20914	20914	S18-3	Introduction	11	18	2.0	1.0	Ghosh and Veale, 2016).	0
1166	20915	20915	S18-3	Introduction	12	19	2.0	1.0	Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge.	0
1167	20916	20916	S18-3	Introduction	13	20	2.0	1.0	Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative.	0
1168	20917	20917	S18-3	Introduction	14	21	2.0	1.0	(1) I feel so blessed to get ocular migraines.	0
1169	20918	20918	S18-3	Introduction	15	22	2.0	1.0	(2)	0
1170	20919	20919	S18-3	Introduction	16	23	3.0	1.0	Go ahead drop me hate, I'm looking forward to it.	0
1171	20920	20920	S18-3	Introduction	17	24	3.0	1.0	"For human readers, it is clear that the author of example 1 does not feel blessed at all, which can be inferred from the contrast between the positive sentiment expression ""I feel so blessed"", and the negative connotation associated with getting ocular migraines."	0
1172	20921	20921	S18-3	Introduction	18	25	3.0	1.0	Although such connotative infor-mation is easily understood by most people, it is difficult to access by machines.	0
1173	20922	20922	S18-3	Introduction	19	26	3.0	1.0	Example 2 illustrates implicit cyberbullying; instances that typically lack explicit profane words and where the offense is often made through irony.	0
1174	20923	20923	S18-3	Introduction	20	27	3.0	1.0	"Similarly to example 1, a contrast can be perceived between a positive statement (""I'm looking forward to"") and a negative situation (i.e. experiencing hate)."	0
1175	20924	20924	S18-3	Introduction	21	28	3.0	1.0	To be able to interpret the above examples correctly, machines need, similarly to humans, to be aware that irony is used, and that the intended sentiment is opposite to what is literally enunciated.	0
1176	20925	20925	S18-3	Introduction	22	29	3.0	1.0	The irony detection task 1 we propose is formulated as follows: given a single post (i.e. a tweet), participants are challenged to automatically determine whether irony is used and which type of irony is expressed.	1
1177	20926	20926	S18-3	Introduction	23	30	4.0	1.0	We thus defined two subtasks:	0
1178	20927	20927	S18-3	Introduction	24	31	4.0	1.0		0
1179	20928	20928	S18-3	Introduction	25	32	4.0	1.0	Task A describes a binary irony classification task to define, for a given tweet, whether irony is expressed.	0
1180	20929	20929	S18-3	Introduction	26	33	4.0	1.0		0
1181	20930	20930	S18-3	Introduction	27	34	4.0	1.0	Task B describes a multiclass irony classification task to define whether it contains a specific type of irony (verbal irony by means of a polarity clash, situational irony, or another type of verbal irony, see further) or is not ironic.	0
1182	20931	20931	S18-3	Introduction	28	35	4.0	1.0	Concretely, participants should define which one out of four categories a tweet contains: ironic by clash, situational irony, other verbal irony or not ironic.	0
1183	20932	20932	S18-3	Introduction	29	36	4.0	1.0	It is important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location).	0
1184	20933	20933	S18-3	Introduction	30	37	4.0	1.0	Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised.	0
1185	21120	21120	S18-4	Task Description	1	32	1.0	1.0	Let a mention be a nominal that refers to a singular or a collective entity (e.g., she, mom, Judy), and an entity be the actual person that the mention refers to.	0
1186	21121	21121	S18-4	Task Description	2	33	1.0	1.0	Given a dialogue transcribed in text where all mentions are detected, the objective is to find the entity for each mention, who can be either active or passive in the dialogue.	1
1187	21122	21122	S18-4	Task Description	3	34	2.0	1.0	In Figure 1, entities such as Ross, Monica, and Joey are the active speakers of the dialogue, whereas Jack and Judy are not although they are passively mentioned as mom and dad in this context.	0
1188	21123	21123	S18-4	Task Description	4	35	2.0	1.0	Linking such mentions to their global entities demands inferred knowledge about the kinship from other dialogues, challenging crossdocument resolution.	0
1189	21124	21124	S18-4	Task Description	5	36	2.0	1.0	Thus, character identification can be viewed as an entity linking task that aims for holistic understanding in multiparty dialogue.	0
1190	21125	21125	S18-4	Task Description	6	37	3.0	1.0	Most of previous works on entity linking have focused on Wikification, which links named entity mentions to their relevant Wikipedia articles (Mihalcea and Csomai, 2007;	0
1191	21126	21126	S18-4	Task Description	7	38	3.0	1.0	Ratinov et al., 2011;Guo et al., 2013).	0
1192	21127	21127	S18-4	Task Description	8	39	4.0	1.0	Unlike Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom-piled information, which makes this task even more challenging.	0
1193	21128	21128	S18-4	Task Description	9	40	4.0	1.0	It is similar to coreference resolution in a sense that it groups mentions into entities, but distinguished because this task requires to identify each mention group as a known person.	0
1194	21129	21129	S18-4	Task Description	10	41	4.0	1.0	In Figure 1, coreference resolution would give a cluster of the four mentions, {mom, woman, I, I}; however, it would not identify that cluster to be the entity Judy, which in this case is not possible to identify without getting contexts from other dialogues.	0
1195	21261	21261	S18-5	abstract	1	2	1.0	1.0	This paper discusses SemEval-2018	0
1196	21262	21262	S18-5	abstract	2	3	1.0	1.0	Task 5: a referential quantification task of counting events and participants in local, long-tail news documents with high ambiguity.	1
1197	21263	21263	S18-5	abstract	3	4	2.0	1.0	The complexity of this task challenges systems to establish the meaning, reference and identity across documents.	0
1198	21264	21264	S18-5	abstract	4	5	2.0	1.0	The task consists of three subtasks and spans across three domains.	0
1199	21265	21265	S18-5	abstract	5	6	3.0	1.0	We detail the design of this referential quantification task, describe the participating systems, and present additional analysis to gain deeper insight into their performance.	0
1200	21266	21266	S18-5	abstract	6	7	3.0	1.0	70 Answer: 3 Question:	0
1201	21267	21267	S18-5	abstract	7	8	4.0	1.0	How many killing incidents happened in 2016 in Columbus, Mississippi?	0
1202	21268	21268	S18-5	abstract	8	9	4.0	1.0	Mississippi boy killed in gun accident Shooting suspect charged with domestic aggravated assault NEWLYWED ACCUSED OF SHOOTING NEW BRIDE Columbus Police investigating early morning shooting High Winds Play Role in 2-Alarm District Heights Apartment Fire input documents	0
1203	21516	21516	S18-6	Tasks	1	29	1.0	1.0	The ultimate goal of the shared task is to interpret time expressions, identifying appropriate intervals that can be placed on a timeline.	1
1204	21517	21517	S18-6	Tasks	2	30	1.0	1.0	Given a document, a system must identify the time entities by detecting the spans of characters and labeling them with the proper SCATE type.	1
1205	21518	21518	S18-6	Tasks	3	31	1.0	1.0	Examples of time entities and their corresponding types in Figure 1 would be (6, DAY-OF-MONTH), (Saturday, DAY-OF-WEEK), (March, MONTH-OF-YEAR) or (since, BETWEEN).	0
1206	21519	21519	S18-6	Tasks	4	32	1.0	1.0	Besides the time entities explicitly expressed in the text, implicit occurrences must also be identified, like the THIS and LAST time entities in Figure 1 that do not have any explicit triggers in the text.	0
1207	21520	21520	S18-6	Tasks	5	33	1.0	1.0	Once time entities have been identified, they should be linked together using the relations described in the SCATE schema.	0
1208	21521	21521	S18-6	Tasks	6	34	1.0	1.0	Following with the example in Figure 1, the time entity 6 should be linked as a SUB-INTERVAL of March, Saturday should be a REPEATING-INTERVAL of the time entity other, and so on.	0
1209	21522	21522	S18-6	Tasks	7	35	1.0	1.0	Finally, all the time entities must be completed with some additional properties needed for their interpretation.	0
1210	21523	21523	S18-6	Tasks	8	36	2.0	1.0	For example, the time entity other should have a VALUE of 2, the END-INTERVAL of since is the Document Creation Time, etc.	0
1211	21524	21524	S18-6	Tasks	9	37	2.0	1.0	Once again, the properties required by each time entity type are defined by the SCATE schema.	0
1212	21525	21525	S18-6	Tasks	10	38	2.0	1.0	1 Every resulting graph, composed of a set of linked time entities, represents a time expression that can be semantically interpreted.	0
1213	21526	21526	S18-6	Tasks	11	39	2.0	1.0	For this purpose, we provide a Scala library 2 that reads the graphs in Anafora XML format (Chen and Styler, 2013) and converts them into intervals on the timeline.	0
1214	21527	21527	S18-6	Tasks	12	40	2.0	1.0	An example of interpreting the time entities corresponding to the expression every Saturday since March 6 relative to an anchor time of April 21, 2017 is given in Figure 2.	0
1215	21528	21528	S18-6	Tasks	13	41	2.0	2.0	In this example, the values today and result store the entities that represent the time expressions April 21, 2017 and every Saturday since March 6 respectively.	0
1216	21529	21529	S18-6	Tasks	14	42	2.0	2.0	The Scala command on the right side interprets the latter and produces the corresponding time intervals.	0
1217	21530	21530	S18-6	Tasks	15	43	3.0	2.0	The task includes two evaluation methods, one for the parsing step, i.e. time entity identification scala&gt; for (Interval(start, end) &lt;-result.intervals) | println(start, end) (2017-03-11T00:00,2017-03-12T00:00) (2017-03-18T00:00,2017-03-19T00:00) (2017-03-25T00:00,2017-03-26T00:00) (2017-04-01T00:00,2017-04-02T00:00) (2017-04-08T00:00,2017-04-09T00:00) (2017-04-15T00:00,2017-04-16T00:00) (2017-04-22T00:00,2017-04-23T00:00) ... and linking, and one to score the resulting time intervals.	0
1218	21531	21531	S18-6	Tasks	16	44	3.0	2.0	For the later, we only consider time expressions that yield a finite set of bounded intervals, for example, last Monday.	0
1219	21532	21532	S18-6	Tasks	17	45	3.0	2.0	Time expressions that refer to an infinite set of intervals, like every month, are not considered in the interval-based part of the evaluation.	0
1220	21533	21533	S18-6	Tasks	18	46	3.0	2.0	Participants only need to produce Anafora outputs with parsed time entities; the interpretation is carried out by the evaluation system.	0
1221	21534	21534	S18-6	Tasks	19	47	3.0	2.0	The evaluation system is also able to obtain the intervals from timestamps in TimeML format.	0
1222	21535	21535	S18-6	Tasks	20	48	3.0	2.0	Thus, systems can be evaluated by both methods or just by the interval-based one, depending on the output format.	0
1223	21536	21536	S18-6	Tasks	21	49	3.0	2.0	In summary, the tasks offers two tracks:	0
1224	21537	21537	S18-6	Tasks	22	50	4.0	2.0	Track 1: Parse text to time entities.	0
1225	21538	21538	S18-6	Tasks	23	51	4.0	2.0	Systems must identify time entities in text and link them correctly to signal how they have to be composed.	0
1226	21539	21539	S18-6	Tasks	24	52	4.0	2.0	The output must be given in Anafora format.	0
1227	21540	21540	S18-6	Tasks	25	53	4.0	2.0	In this track, all time entities and relations of every time expression are evaluated.	0
1228	21541	21541	S18-6	Tasks	26	54	4.0	2.0	Track 2: Produce time intervals.	0
1229	21542	21542	S18-6	Tasks	27	55	4.0	2.0	Systems can participate through Track 1 or by providing TimeML annotations.	0
1230	21543	21543	S18-6	Tasks	28	56	4.0	2.0	In both cases, the intervals are inferred by our interpreter.	0
1231	21544	21544	S18-6	Tasks	29	57	4.0	2.0	In this track, only bounded time intervals are scored.	0
1232	21657	21657	S18-7	Introduction	1	7	1.0	1.0	One of the emerging trends of natural language technologies is their use for the humanities and sciences.	0
1233	21658	21658	S18-7	Introduction	2	8	1.0	1.0	Recent works in the semantic web (Osborne and Motta, 2015;	0
1234	21659	21659	S18-7	Introduction	3	9	2.0	1.0	Wolfram, 2016) and natural language processing (Tsai et al., 2013;	0
1235	21660	21660	S18-7	Introduction	4	10	2.0	1.0	Luan et al., 2017;	0
1236	21661	21661	S18-7	Introduction	5	11	2.0	1.0	Augenstein and Sgaard, 2017; aimed to improve the access to scientific literature, and in particular to respond to information needs that are currently beyond the capabilities of standard search engines.	0
1237	21662	21662	S18-7	Introduction	6	12	3.0	1.0	Such queries include finding all papers that address a problem in a specific way, or discovering the roots of a certain idea.	0
1238	21663	21663	S18-7	Introduction	7	13	3.0	1.0	This ambition involves the identification and classification of concepts, and the relations connecting them.	0
1239	21664	21664	S18-7	Introduction	8	14	4.0	1.0	The purpose of the task is to automatically identify relevant domain-specific semantic relations in a corpus of scientific publications.	1
1240	21665	21665	S18-7	Introduction	9	15	4.0	1.0	"In particular, we search for and classify relations that provide snippets of information such as ""a (new) method is proposed for a task"", or ""a phenomenon is found in a certain context"", or ""results of different experiments are compared to each other""."	0
1241	21666	21666	S18-7	Introduction	10	16	4.0	1.0	Identifying such semantic relations between domain-specific concepts allows us to detect research papers which deal with the same problem, or to track the evolution of results on a certain task.	0
1242	21830	21830	S18-8	abstract	1	2	1.0	1.0	This paper describes the SemEval 2018 shared task on semantic extraction from cybersecurity reports, which is introduced for the first time as a shared task on SemEval.	0
1243	21831	21831	S18-8	abstract	2	3	2.0	1.0	This task comprises four SubTasks done incrementally to predict the characteristics of a specific malware using cybersecurity reports.	1
1244	21832	21832	S18-8	abstract	3	4	3.0	1.0	To the best of our knowledge, we introduce the world's largest publicly available dataset of annotated malware reports in this task.	0
1245	21833	21833	S18-8	abstract	4	5	4.0	1.0	This task received in total 18 submissions from 9 participating teams.	0
1246	22070	22070	S18-9	abstract	1	2	1.0	1.0	This paper describes the SemEval 2018 Shared Task on Hypernym Discovery.	0
1247	22071	22071	S18-9	abstract	2	3	2.0	1.0	We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input.	0
1248	22072	22072	S18-9	abstract	3	4	2.0	1.0	Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus.	1
1249	22073	22073	S18-9	abstract	4	5	3.0	1.0	We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music).	0
1250	22074	22074	S18-9	abstract	5	6	3.0	1.0	Participants were allowed to compete in any or all of the subtasks.	0
1251	22075	22075	S18-9	abstract	6	7	4.0	1.0	Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks.	0
1252	22076	22076	S18-9	abstract	7	8	4.0	1.0	Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119.	0
1253	22301	22301	S18-10	Task description	1	12	1.0	1.0	A semantic model that has only been evaluated on similarity detection may very well fail to be of practical use for specific applications.	0
1254	22302	22302	S18-10	Task description	2	13	1.0	1.0	For example, word sense disambiguation could benefit greatly from representations that can model complex semantic relations.	0
1255	22303	22303	S18-10	Task description	3	14	1.0	1.0	This means that the evaluation of word representation models should not only be centered on semantic similarity and relatedness, and should include different, complementary tasks.	0
1256	22304	22304	S18-10	Task description	4	15	1.0	1.0	To fill this gap, we proposed a novel task of semantic difference detection as Task 10 of the SemEval 2018 workshop.	0
1257	22305	22305	S18-10	Task description	5	16	2.0	1.0	The goal of the systems in this case was to predict whether a word is a discriminative attribute between two other words.	1
1258	22306	22306	S18-10	Task description	6	17	2.0	1.0	For example, given the words apple and banana, is the word red a discriminative attribute?	0
1259	22307	22307	S18-10	Task description	7	18	2.0	1.0	Semantic difference is a ternary relation between two concepts (apple, banana) and a discriminative attribute (red) that characterizes the first concept but not the other.	0
1260	22308	22308	S18-10	Task description	8	19	2.0	1.0	By its nature, semantic difference detection is a binary classification task: given a triple apple,banana,red, the task is to determine whether it exemplifies a semantic difference or not.	0
1261	22309	22309	S18-10	Task description	9	20	3.0	1.0	In practice, when preparing the task, we started out with defining potential discriminative attributes as semantic features in the sense of (McRae et al., 2005): properties that people tend to think are important for a given concept.	0
1262	22310	22310	S18-10	Task description	10	21	3.0	1.0	McRae et al.'s features are expressed as phrases, but these phrases can usually be reconstructed from a single word (e.g. red as a feature of apple stands for the phrase is red, carpentry as a feature of hammer can be used as a shorthand of used for carpentry, etc.)	0
1263	22311	22311	S18-10	Task description	11	22	3.0	1.0	Given this general reconstructability, we have for simplicity used single words rather than phrases to represent features.	0
1264	22312	22312	S18-10	Task description	12	23	3.0	1.0	The same solution was also adopted in the feature norming studies by (Vinson and Vigliocco, 2008) and (Lenci et al., 2013).	0
1265	22313	22313	S18-10	Task description	13	24	4.0	1.0	Following McRae et al., we did not define discriminative features in purely logical but rather in psychological terms.	0
1266	22314	22314	S18-10	Task description	14	25	4.0	1.0	Accordingly, features are prototypical properties that subjects tend to associate to a certain concept.	0
1267	22315	22315	S18-10	Task description	15	26	4.0	1.0	For example, not all apples are red and some bananas are, but red tends to be judged as an important feature of apples and not of bananas.	0
1268	22316	22316	S18-10	Task description	16	27	4.0	1.0	We therefore fully trust human anno-tators in deciding what counts as a distinguishing attribute and what does not.	0
1269	22462	22462	S18-11	Introduction	1	7	1.0	1.0	Developing algorithms for understanding natural language is not trivial.	0
1270	22463	22463	S18-11	Introduction	2	8	1.0	1.0	Natural language comes with its own complexity and inherent ambiguities.	0
1271	22464	22464	S18-11	Introduction	3	9	1.0	1.0	Ambiguities can occur, for example, at the level of word meaning, syntactic structure, or semantic interpretation.	0
1272	22465	22465	S18-11	Introduction	4	10	1.0	1.0	Traditionally, Natural Language Understanding (NLU) systems have resolved ambiguities using information from the textual context (e.g. neighboring words and sentences), for example via distributional methods (Lenci, 2008).	0
1273	22466	22466	S18-11	Introduction	5	11	1.0	1.0	However, many times context may be absent or may lack sufficient information to resolve the ambiguity.	0
1274	22467	22467	S18-11	Introduction	6	12	1.0	1.0	In such cases, it would be beneficial to include commonsense knowledge about the world in an NLU system.	0
1275	22468	22468	S18-11	Introduction	7	13	1.0	1.0	For example, consider example (1).	0
1276	22469	22469	S18-11	Introduction	8	14	1.0	1.0	(1) The waitress brought Rachel's order.	0
1277	22470	22470	S18-11	Introduction	9	15	1.0	1.0	She ate the food with great pleasure.	0
1278	22471	22471	S18-11	Introduction	10	16	1.0	1.0	Looking at the example in isolation, the person eating the food could be either Rachel or the waitress.	0
1279	22472	22472	S18-11	Introduction	11	17	1.0	1.0	Using commonsense knowledge, or, more specifically, script knowledge about the RESTAU-RANT scenario, helps to resolve the referent of the pronoun: Rachel ordered the food.	0
1280	22473	22473	S18-11	Introduction	12	18	1.0	1.0	The person who orders the food is the customer.	0
1281	22474	22474	S18-11	Introduction	13	19	1.0	1.0	So Rachel should eat the food, she thus refers to Rachel.	0
1282	22475	22475	S18-11	Introduction	14	20	1.0	1.0	This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems.	0
1283	22476	22476	S18-11	Introduction	15	21	1.0	1.0	In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts.	0
1284	22477	22477	S18-11	Introduction	16	22	2.0	1.0	Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc.	0
1285	22478	22478	S18-11	Introduction	17	23	2.0	1.0	(Schank and Abelson, 1975).	0
1286	22479	22479	S18-11	Introduction	18	24	2.0	1.0	The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979;	0
1287	22480	22480	S18-11	Introduction	19	25	2.0	1.0	Schank, 1982;Modi et al., 2017).	0
1288	22481	22481	S18-11	Introduction	20	26	2.0	1.0	From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993).	0
1289	22482	22482	S18-11	Introduction	21	27	2.0	1.0	Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers.	0
1290	22483	22483	S18-11	Introduction	22	28	2.0	1.0	On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender.	0
1291	22484	22484	S18-11	Introduction	23	29	2.0	1.0	Because of this implicitness, learning script knowledge from texts is very challenging.	0
1292	22485	22485	S18-11	Introduction	24	30	2.0	1.0	There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge.	0
1293	22486	22486	S18-11	Introduction	25	31	2.0	1.0	An example is the InScript , which contains short and simple narratives, that very explicitly mention script events and participants.	0
1294	22487	22487	S18-11	Introduction	26	32	2.0	1.0	The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario.	0
1295	22488	22488	S18-11	Introduction	27	33	2.0	1.0	In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010;	0
1296	22489	22489	S18-11	Introduction	28	34	2.0	1.0	Wanzare et al., 2017), event prediction (namely, the narrative cloze task)	0
1297	22490	22490	S18-11	Introduction	29	35	2.0	1.0	Jurafsky, 2008, 2009;	0
1298	22491	22491	S18-11	Introduction	30	36	2.0	1.0	Rudinger et al., 2015b;Modi, 2016) or story completion (e.g. the story cloze task T	0
1299	22492	22492	S18-11	Introduction	31	37	2.0	1.0	It was a long day at work and I decided to stop at the gym before going home.	0
1300	22493	22493	S18-11	Introduction	32	38	3.0	1.0	I ran on the treadmill and lifted some weights.	0
1301	22494	22494	S18-11	Introduction	33	39	3.0	1.0	I decided I would also swim a few laps in the pool.	0
1302	22495	22495	S18-11	Introduction	34	40	3.0	1.0	Once I was done working out, I went in the locker room and stripped down and wrapped myself in a towel.	0
1303	22496	22496	S18-11	Introduction	35	41	3.0	1.0	I went into the sauna and turned on the heat.	0
1304	22497	22497	S18-11	Introduction	36	42	3.0	1.0	I let it get nice and steamy.	0
1305	22498	22498	S18-11	Introduction	37	43	3.0	1.0	I sat down and relaxed.	0
1306	22499	22499	S18-11	Introduction	38	44	3.0	1.0	I let my mind think about nothing but peaceful, happy thoughts.	0
1307	22500	22500	S18-11	Introduction	39	45	3.0	1.0	I stayed in there for only about ten minutes because it was so hot and steamy.	0
1308	22501	22501	S18-11	Introduction	40	46	3.0	1.0	When I got out, I turned the sauna off to save energy and took a cool shower.	0
1309	22502	22502	S18-11	Introduction	41	47	3.0	1.0	I got out of the shower and dried off.	0
1310	22503	22503	S18-11	Introduction	42	48	3.0	1.0	After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home.	0
1311	22504	22504	S18-11	Introduction	43	49	3.0	1.0	Q1	0
1312	22505	22505	S18-11	Introduction	44	50	3.0	1.0	Where did they sit inside the sauna?	0
1313	22506	22506	S18-11	Introduction	45	51	3.0	1.0	a. on the floor b. on a bench Q2 How long did they stay in the sauna?	0
1314	22507	22507	S18-11	Introduction	46	52	3.0	1.0	a. about ten minutes b. over thirty minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions.	0
1315	22508	22508	S18-11	Introduction	47	53	3.0	1.0	(Mostafazadeh et al., 2016)).	0
1316	22509	22509	S18-11	Introduction	48	54	4.0	1.0	These tasks test a system's ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks.	0
1317	22510	22510	S18-11	Introduction	49	55	4.0	1.0	Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension.	0
1318	22511	22511	S18-11	Introduction	50	56	4.0	1.0	The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text.	1
1319	22512	22512	S18-11	Introduction	51	57	4.0	1.0	In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text.	1
1320	22513	22513	S18-11	Introduction	52	58	4.0	1.0	In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts.	1
1321	22514	22514	S18-11	Introduction	53	59	4.0	1.0	For example, consider the short narrative in (1).	0
1322	22515	22515	S18-11	Introduction	54	60	4.0	1.0	For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text:	0
1323	22516	22516	S18-11	Introduction	55	61	4.0	1.0	Usually, people sit on benches inside a sauna, an information that is not given in the text.	0
1324	22517	22517	S18-11	Introduction	56	62	4.0	1.0	The dataset also comprises questions that can just be answered from the text, as the second question:	0
1325	22518	22518	S18-11	Introduction	57	63	4.0	1.0	The information about the duration of the stay is given literally in the text.	0
1326	22519	22519	S18-11	Introduction	58	64	4.0	1.0	The paper is organized as follows:	0
1327	22520	22520	S18-11	Introduction	59	65	4.0	1.0	In Section 2, we give an overview of other machine comprehension datasets.	0
1328	22521	22521	S18-11	Introduction	60	66	4.0	2.0	In Section 3, we describe the dataset used for our shared task.	0
1329	22522	22522	S18-11	Introduction	61	67	4.0	2.0	Section 4.2 gives details about the setup of our task.	0
1330	22523	22523	S18-11	Introduction	62	68	4.0	2.0	In Section 5, information about participating systems is given.	0
1331	22524	22524	S18-11	Introduction	63	69	4.0	2.0	Results are presented and discussed in Sections 6 and 8, respectively.	0
1332	22718	22718	S18-12	abstract	1	2	1.0	1.0	A natural language argument is composed of a claim as well as reasons given as premises for the claim.	0
1333	22719	22719	S18-12	abstract	2	3	1.0	1.0	The warrant explaining the reasoning is usually left implicit, as it is clear from the context and common sense.	0
1334	22720	22720	S18-12	abstract	3	4	2.0	1.0	This makes a comprehension of arguments easy for humans but hard for machines.	0
1335	22721	22721	S18-12	abstract	4	5	2.0	1.0	This paper summarizes the first shared task on argument reasoning comprehension.	0
1336	22722	22722	S18-12	abstract	5	6	3.0	1.0	Given a premise and a claim along with some topic information, the goal is to automatically identify the correct warrant among two candidates that are plausible and lexically close, but in fact imply opposite claims.	1
1337	22723	22723	S18-12	abstract	6	7	3.0	1.0	We describe the dataset with 1970 instances that we built for the task, and we outline the 21 computational approaches that participated, most of which used neural networks.	0
1338	22724	22724	S18-12	abstract	7	8	4.0	1.0	The results reveal the complexity of the task, with many approaches hardly improving over the random accuracy of  0.5.	0
1339	22725	22725	S18-12	abstract	8	9	4.0	1.0	Still, the best observed accuracy (0.712) underlines the principle feasibility of identifying warrants.	0
1340	22726	22726	S18-12	abstract	9	10	4.0	1.0	Our analysis indicates that an inclusion of external knowledge is key to reasoning comprehension.	0
1341	22988	22988	S19-1	title	1	1	4.0	1.0	SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA	1
1342	22989	22989	S19-1	abstract	1	2	1.0	1.0	We present the SemEval 2019 shared task on Universal Conceptual Cognitive Annotation (UCCA) parsing in English, German and French, and discuss the participating systems and results.	0
1343	22990	22990	S19-1	abstract	2	3	2.0	1.0	UCCA is a crosslinguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation.	0
1344	22991	22991	S19-1	abstract	3	4	3.0	1.0	UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units.	0
1345	22992	22992	S19-1	abstract	4	5	4.0	1.0	The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings.	0
1346	22993	22993	S19-1	abstract	5	6	4.0	1.0	Full results can be found in the task's website https://competitions. codalab.org/competitions/19160.	0
1347	23445	23445	S19-3	abstract	1	2	1.0	1.0	In this paper, we present the SemEval-2019 Task 3 -EmoContext: Contextual Emotion Detection in Text.	0
1348	23446	23446	S19-3	abstract	2	3	1.0	1.0	Lack of facial expressions and voice modulations make detecting emotions in text a challenging problem.	0
1349	23447	23447	S19-3	abstract	3	4	2.0	1.0	"For instance, as humans, on reading ""Why don't you ever text me!"" we can either interpret it as a sad or angry emotion and the same ambiguity exists for machines."	0
1350	23448	23448	S19-3	abstract	4	5	2.0	1.0	However, the context of dialogue can prove helpful in detection of the emotion.	0
1351	23449	23449	S19-3	abstract	5	6	2.0	1.0	In this task, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes -Happy, Sad, Angry and Others.	1
1352	23450	23450	S19-3	abstract	6	7	3.0	1.0	To facilitate the participation in this task, textual dialogues from user interaction with a conversational agent were taken and annotated for emotion classes after several data processing steps.	0
1353	23451	23451	S19-3	abstract	7	8	3.0	1.0	A training data set of 30160 dialogues, and two evaluation data sets, Test1 and Test2, containing 2755 and 5509 dialogues respectively were released to the participants.	0
1354	23452	23452	S19-3	abstract	8	9	4.0	1.0	A total of 311 teams made submissions to this task.	0
1355	23453	23453	S19-3	abstract	9	10	4.0	1.0	The final leader-board was evaluated on Test2 data set, and the highest ranked submission achieved 79.59 micro-averaged F1 score.	0
1356	23454	23454	S19-3	abstract	10	11	4.0	1.0	Our analysis of systems submitted to the task indicate that Bi-directional LSTM was the most common choice of neural architecture used, and most of the systems had the best performance for the Sad emotion class, and the worst for the Happy emotion class.	0
1357	23632	23632	S19-4	Task Definition	1	27	1.0	1.0	We define hyperpartisan news detection as follows:	0
1358	23633	23633	S19-4	Task Definition	2	28	2.0	1.0	Given the text and markup of an online news article, decide whether the article is hyperpartisan or not.	1
1359	23634	23634	S19-4	Task Definition	3	29	3.0	1.0	Hyperpartisan articles mimic the form of regular news articles, but are one-sided in the sense that opposing views are either ignored or fiercely attacked.	0
1360	23635	23635	S19-4	Task Definition	4	30	4.0	1.0	We deliberately disregard the distinction between left and right, since previous work has found that, in hyperpartisan form, both are more similar to each other in terms of style than either are to the mainstream (Potthast et al., 2018c).	0
1361	23636	23636	S19-4	Task Definition	5	31	4.0	1.0	The challenge of this task is to unveil the mimicking and to detect the hyperpartisan language, which may be distinguishable from regular news at the levels of style, syntax, semantics, and pragmatics.	0
1362	23862	23862	S19-5	Introduction	1	8	1.0	1.0	Hate Speech (HS) is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000).	0
1363	23863	23863	S19-5	Introduction	2	9	1.0	1.0	Given the huge amount of user-generated contents on the Web, and in particular on social media, the problem of detecting, and therefore possibly contrasting the HS diffusion, is becoming fundamental, for instance for fighting against misogyny and xenophobia.	0
1364	23864	23864	S19-5	Introduction	3	10	1.0	1.0	Some key aspects feature online HS, such as virality, or presumed anonymity, which distinguish it from offline communication and make it potentially also more dangerous and hurtful.	0
1365	23865	23865	S19-5	Introduction	4	11	1.0	1.0	Often hate speech fosters discrimination against particular categories and undermines equality, an everlasting issue for each civil society.	0
1366	23866	23866	S19-5	Introduction	5	12	1.0	1.0	Among the mainly targeted categories there are immigrants and women.	0
1367	23867	23867	S19-5	Introduction	6	13	2.0	1.0	For the first target, especially raised by refugee crisis and political changes occurred in the last few years, several governments and policy makers are currently trying to address it, making especially interesting the development of tools for the identification and monitoring such kind of hate .	0
1368	23868	23868	S19-5	Introduction	7	14	2.0	1.0	For the second one instead, hate against the female gender is a long-time and well-known form of discrimination (Manne, 2017).	0
1369	23869	23869	S19-5	Introduction	8	15	2.0	1.0	Both these forms of hate content impact on the development of society and may be confronted by developing tools that automatically detect them.	0
1370	23870	23870	S19-5	Introduction	9	16	2.0	1.0	A large number of academic events and shared tasks for different languages (i.e. English, Spanish, Italian, German, Mexican-Spanish, Hindi) took place in the very recent past which are centered on HS and related topics, thus reflecting the interest by the NLP community.	0
1371	23871	23871	S19-5	Introduction	10	17	2.0	1.0	Let us mention the first and second edition of the Workshop on Abusive Language 1 (Waseem et al., 2017), the First Workshop on Trolling, Aggression and Cyberbullying (Kumar et al., 2018), that also included a shared task on aggression identification, the tracks on Automatic Misogyny Identification (AMI) (Fersini et al., 2018b) and on Authorship and Aggressiveness Analysis (MEX-A3T) (Carmona et al., 2018) proposed at the 2018 edition of IberEval 2 , the GermEval Shared Task on the Identification of Offensive Language (Wiegand et al., 2018), and finally the Automatic Misogyny Identification task (AMI) (Fersini et al., 2018a) and the Hate Speech Detection task (HaSpeeDe)  at EVALITA 2018 3 for investigating respectively misogyny and HS in Italian.	0
1372	23872	23872	S19-5	Introduction	11	18	2.0	1.0	Hat	0
1373	23873	23873	S19-5	Introduction	12	19	3.0	1.0	Eval consists in detecting hateful contents in social media texts, specifically in Twitter's posts, against two targets: immigrants and women.	1
1374	23874	23874	S19-5	Introduction	13	20	3.0	1.0	Moreover, the task implements a multilingual perspective where data for two widespread languages, English and Spanish, are provided for training and testing participant systems.	0
1375	23875	23875	S19-5	Introduction	14	21	3.0	1.0	The motivations for organizing HatEval go beyond the advancement of the state of the art for HS detection for each of the involved languages and targets.	0
1376	23876	23876	S19-5	Introduction	15	22	3.0	1.0	The variety of targets of hate and languages provides a unique comparative setting, both with respect to the amount of data collected and annotated applying the same scheme, and with respect to the results achieved by participants training their systems on those data.	0
1377	23877	23877	S19-5	Introduction	16	23	3.0	1.0	Such comparative setting may help in shedding new light on the linguistic and communication behaviour against these targets, paving the way for the integration of HS detection tools in several application contexts.	0
1378	23878	23878	S19-5	Introduction	17	24	3.0	1.0	Moreover, the participation of a very large amount of research groups in this task (see Section 4) has improved the possibility of in-depth investigation of the involved phenomena.	0
1379	23879	23879	S19-5	Introduction	18	25	4.0	1.0	The paper is organized as follows.	0
1380	23880	23880	S19-5	Introduction	19	26	4.0	1.0	In the next section, the datasets released to the participants for training and testing the systems are described.	0
1381	23881	23881	S19-5	Introduction	20	27	4.0	1.0	Section 3 presents the two subtasks and the measures we exploited in the evaluation.	0
1382	23882	23882	S19-5	Introduction	21	28	4.0	1.0	Section 4 reports on approaches and results of the participant systems.	0
1383	23883	23883	S19-5	Introduction	22	29	4.0	1.0	In Section 5, a preliminary analysis of common errors in top-ranked systems is proposed.	0
1384	23884	23884	S19-5	Introduction	23	30	4.0	1.0	Section 6 concludes the paper.	0
1385	24220	24220	S19-6	Conclusion	1	158	1.0	4.0	We have described SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval).	0
1386	24221	24221	S19-6	Conclusion	2	159	1.0	4.0	The task used OLID (Zampieri et al., 2019), a dataset of English tweets annotated for offensive language use, following a three-level hierarchical schema that considers (i) whether a message is offensive or not (for subtask A), (ii) what is the type of the offensive message (for sub-task B), and (iii) who is the target of the offensive message (for sub-task C).	1
1387	24222	24222	S19-6	Conclusion	3	160	1.0	4.0	Overall, about 800 teams signed up for Of-fensEval, and 115 of them actually participated in at least one sub-task.	0
1388	24223	24223	S19-6	Conclusion	4	161	1.0	4.0	The evaluation results have shown that the best systems used ensembles and state-of-the-art deep learning models such as BERT.	0
1389	24224	24224	S19-6	Conclusion	5	162	2.0	4.0	Overall, both deep learning and traditional machine learning classifiers were widely used.	0
1390	24225	24225	S19-6	Conclusion	6	163	2.0	4.0	More details about the indvididual systems can be found in their respective system description papers, which are published in the SemEval-2019 proceedings.	0
1391	24226	24226	S19-6	Conclusion	7	164	2.0	4.0	A list with references to these publications can be found in Table 2; note, however, that only 50 of the 115 participating teams submitted a system description paper.	0
1392	24227	24227	S19-6	Conclusion	8	165	2.0	4.0	As is traditional for SemEval, we have made OLID publicly available to the research community beyond the SemEval competition, hoping to facilitate future research on this important topic.	0
1393	24228	24228	S19-6	Conclusion	9	166	2.0	4.0	In fact, the OLID dataset and the SemEval-2019 Task 6 competition setup have already been used in teaching curricula in universities in UK and USA.	0
1394	24229	24229	S19-6	Conclusion	10	167	3.0	4.0	For example, student competitions based on Offens	0
1395	24230	24230	S19-6	Conclusion	11	168	3.0	4.0	Eval using OLID have been organized as part of Natural Language Processing and Text Analytics courses in two universities in UK: Imperial College London and the University of Leeds.	0
1396	24231	24231	S19-6	Conclusion	12	169	3.0	4.0	System papers describing some of the students' work are publicly accessible 11 and have also been made available on arXiv.org (Cambray and Podsadowski, 2019;	0
1397	24232	24232	S19-6	Conclusion	13	170	3.0	4.0	Frisiani et al., 2019;	0
1398	24233	24233	S19-6	Conclusion	14	171	4.0	4.0	Ong, 2019;Sapora et al., 2019;Puiu and Brabete, 2019;	0
1399	24234	24234	S19-6	Conclusion	15	172	4.0	4.0	Uglow et al., 2019).	0
1400	24235	24235	S19-6	Conclusion	16	173	4.0	4.0	Similarly, a number of students in Linguistics and Computer Science at the University of Arizona in USA have been using OLID in their coursework.	0
1401	24236	24236	S19-6	Conclusion	17	174	4.0	4.0	In future work, we plan to increase the size of the OLID dataset, while addressing issues such as class imbalance and the small size for the test partition, particularly for sub-tasks B and C.	0
1402	24237	24237	S19-6	Conclusion	18	175	4.0	4.0	We would also like to expand the dataset and the task to other languages.	0
1403	24239	24239	S19-7	abstract	1	2	1.0	1.0	"Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of ""fake news"" has become a mainstream concern."	0
1404	24240	24240	S19-7	abstract	2	3	1.0	1.0	However automated support for rumour verification remains in its infancy.	0
1405	24241	24241	S19-7	abstract	3	4	2.0	1.0	It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase.	0
1406	24242	24242	S19-7	abstract	4	5	2.0	1.0	Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour's veracity.	0
1407	24243	24243	S19-7	abstract	5	6	3.0	1.0	As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity.	0
1408	24244	24244	S19-7	abstract	6	7	3.0	1.0	The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts.	0
1409	24245	24245	S19-7	abstract	7	8	4.0	1.0	There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants.	1
1410	24246	24246	S19-7	abstract	8	9	4.0	1.0	We received 22 system submissions (a 70% increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved.	0
1411	24458	24458	S19-8	Overview	1	10	1.0	1.0	The current coverage of the political landscape in both the press and in social media has led to an unprecedented situation.	0
1412	24459	24459	S19-8	Overview	2	11	1.0	1.0	Like never before, a statement in an interview, a press release, a blog note, or a tweet can spread almost instantaneously.	0
1413	24460	24460	S19-8	Overview	3	12	1.0	1.0	The speed of proliferation leaves little time for double-checking claims against the facts, which has proven critical in politics, e.g., during the 2016 presidential campaign in the USA, which was dominated by fake news in social media and by false claims.	0
1414	24461	24461	S19-8	Overview	4	13	1.0	1.0	Investigative journalists and volunteers have been working hard to get to the root of a claim and to present solid evidence in favor or against it.	0
1415	24462	24462	S19-8	Overview	5	14	1.0	1.0	Manual fact-checking is very time-consuming, and thus automatic methods have been proposed to speed-up the process, e.g., there has been work on checking the factuality/credibility of a claim, of a news article, or of an information source (Ba et al., 2016;	0
1416	24463	24463	S19-8	Overview	6	15	1.0	1.0	Zubiaga et al., 2016;	0
1417	24464	24464	S19-8	Overview	7	16	1.0	1.0	Ma et al., 2016;	0
1418	24465	24465	S19-8	Overview	8	17	2.0	1.0	Castillo et al., 2011;	0
1419	24466	24466	S19-8	Overview	9	18	2.0	1.0	Baly et al., 2018).	0
1420	24467	24467	S19-8	Overview	10	19	2.0	1.0	The process starts when a document is made public.	0
1421	24468	24468	S19-8	Overview	11	20	2.0	1.0	First, an intrinsic analysis is carried out in which check-worthy text fragments are identified.	0
1422	24469	24469	S19-8	Overview	12	21	2.0	1.0	Then, other documents that might support or rebut a claim in the document are retrieved from various sources.	0
1423	24470	24470	S19-8	Overview	13	22	2.0	1.0	Finally, by comparing a claim against the retrieved evidence, a system can determine whether the claim is likely true or likely false (or unsure, if no strong enough evidence either way could be found).	0
1424	24471	24471	S19-8	Overview	14	23	2.0	1.0	For instance, Ciampaglia et al. (2015) do this using a knowledge graph derived from Wikipedia.	0
1425	24472	24472	S19-8	Overview	15	24	2.0	1.0	The outcome could then be presented to a human expert for final judgement.	0
1426	24473	24473	S19-8	Overview	16	25	3.0	1.0	1 For our two subtasks, we explore factuality in the context of Community Question Answering (cQA) forums.	1
1427	24474	24474	S19-8	Overview	17	26	3.0	1.0	Forums such as StackOverflow, Yahoo!	0
1428	24475	24475	S19-8	Overview	18	27	3.0	1.0	Answers, and Quora are very popular these days, as they represent effective means for communities around particular topics to share information.	0
1429	24476	24476	S19-8	Overview	19	28	3.0	1.0	However, the information shared by the users is not always correct or accurate.	0
1430	24477	24477	S19-8	Overview	20	29	3.0	1.0	There are multiple factors explaining the presence of incorrect answers in cQA forums, e.g., misunderstanding of the question, ignorance or maliciousness of the responder.	0
1431	24478	24478	S19-8	Overview	21	30	3.0	1.0	Also, as a result of our dynamic world, the truth is time-sensitive: something that was true yesterday may be false today.	0
1432	24479	24479	S19-8	Overview	22	31	3.0	1.0	Moreover, forums are often barely moderated and thus lack systematic quality control.	0
1433	24480	24480	S19-8	Overview	23	32	3.0	1.0	Here we focus on checking the factuality of questions and answers in cQA forums.	0
1434	24481	24481	S19-8	Overview	24	33	4.0	1.0	This aspect was ignored in recent cQA tasks (Ishikawa et al., 2010;	0
1435	24482	24482	S19-8	Overview	25	34	4.0	1.0	Nakov et al., 2015	0
1436	24483	24483	S19-8	Overview	26	35	4.0	1.0	Nakov et al., , 2016a, where an answer is considered GOOD if it addresses the question, irrespective of its veracity, accuracy, etc.	0
1437	24484	24484	S19-8	Overview	27	36	4.0	1.0	Figure 1 presents an excerpt of an example from the Qatar Living Forum, with one question and three answers selected from a longer thread.	0
1438	24485	24485	S19-8	Overview	28	37	4.0	1.0	According to SemEval-2016 Task 3 (Nakov et al., 2016a), all three answers would be considered GOOD since they are formally answering the question.	0
1439	24486	24486	S19-8	Overview	29	38	4.0	1.0	Nevertheless, a 1 contains false information, while a 2 and a 3 are correct, as can be established from an official government website.	0
1440	24487	24487	S19-8	Overview	30	39	4.0	1.0	2 Checking the veracity of answers in a cQA forum is a hard problem, which requires putting together aspects of language understanding, modelling the context, integrating several information sources, uisng world knowledge and complex inference, among others.	0
1441	24488	24488	S19-8	Overview	31	40	4.0	1.0	Moreover, high-quality automatic fact-checking would offer better experience to users of cQA systems, e.g., the user could be presented with veracity scores, where low scores would warn the user not to completely trust the answer or to double-check it.	0
1442	24649	24649	S19-9	Introduction	1	8	1.0	1.0	State of the art opinion mining systems provide numerical summaries of sentiments and tend to overlook additional descriptive and potentially useful content present in the opinionated text.	0
1443	24650	24650	S19-9	Introduction	2	9	1.0	1.0	We stress that such content also encompass information like suggestions, tips, and advice, which is otherwise explicitly sought by the stakeholders.	0
1444	24651	24651	S19-9	Introduction	3	10	1.0	1.0	For example, hotel reviews often contain room tips, i.e., which room should be preferred in a hotel.	0
1445	24652	24652	S19-9	Introduction	4	11	1.0	1.0	Likewise, tips on restaurants, shops, sightseeing, etc. are also present within the hotel reviews.	0
1446	24653	24653	S19-9	Introduction	5	12	1.0	1.0	On the other hand, platforms like Tripadvisor 1 , which collect hotel and restaurant related opinions, request the reviewers to fill up the room tips section in addition to the hotel review.	0
1447	24654	24654	S19-9	Introduction	6	13	1.0	1.0	Likewise, sentences expressing advice, tips, and recommendations relating to a target entity can often be present in text available from different types of data sources, like blogs, microblogs, discussions, etc.	0
1448	24655	24655	S19-9	Introduction	7	14	1.0	1.0	Such sentences can be collectively referred to as suggestions.	0
1449	24656	24656	S19-9	Introduction	8	15	1.0	1.0	With the increasing availability of opinionated text, methods for automatic detection of suggestions can be employed for different use cases.	0
1450	24657	24657	S19-9	Introduction	9	16	1.0	1.0	Some example use cases are the extraction of suggestions for brand improvement, the extraction of tips and advice for customers, the extraction of the expressions of recommendations from unstructured data in order to aid recommender systems, or the summarisation of suggestion forums where suggestion providers often tend to provide context in their responses (Figure 1) which gets repetitive over a large number of responses relating to the same entity.	0
1451	24658	24658	S19-9	Introduction	10	17	2.0	1.0	The task of automatic identification of suggestions in a given text is referred to as suggestion mining (Brun and Hagege, 2013).	0
1452	24659	24659	S19-9	Introduction	11	18	2.0	1.0	Studies performed on suggestion mining have defined it as a sentence classification task, where class prediction has to be made on each sentence of a given text, classes being suggestion and non suggestion (Negi, 2016).	1
1453	24660	24660	S19-9	Introduction	12	19	2.0	1.0	State of the art opinion mining systems have mostly focused on identifying sentiment polarity of the text.	0
1454	24661	24661	S19-9	Introduction	13	20	2.0	1.0	Therefore, suggestion mining remains a very less explored problem as compared to sentiment analysis, specially in the context of recent advancements in neural network based approaches for feature learning and transfer learning.	0
1455	24662	24662	S19-9	Introduction	14	21	2.0	1.0	As suggestion mining is still an emerging research area, it lacks benchmark datasets and well defined annotation guidelines.	0
1456	24663	24663	S19-9	Introduction	15	22	2.0	1.0	A few early works were mostly rule based methods, mainly targeted towards the use case of extracting suggestions for product improvements (Brun and Hagege, 2013;	0
1457	24664	24664	S19-9	Introduction	16	23	2.0	1.0	Ramanand et al., 2010;Moghaddam, 2015).	0
1458	24665	24665	S19-9	Introduction	17	24	2.0	1.0	In our prior work, we performed early investigations on the problem definition and datasets, aiming for the statistical methods which also require benchmark train datasets in addition to the evaluation Figure 1: A post from the suggestion forum for Microsoft developers datasets (Negi and Buitelaar, 2015;.	0
1459	24666	24666	S19-9	Introduction	18	25	2.0	1.0	A few other works also evaluated statistical classifiers (Wicaksono and Myaeng, 2012;	0
1460	24667	24667	S19-9	Introduction	19	26	2.0	1.0	Dong et al., 2013), which employed mostly manually identified features, however only two other works (Wicaksono and Myaeng, 2012;	0
1461	24668	24668	S19-9	Introduction	20	27	3.0	1.0	Dong et al., 2013) provided their datasets.	0
1462	24669	24669	S19-9	Introduction	21	28	3.0	1.0	Suggestion mining still lacks well defined annotation guidelines, a multi-domain and cross-domain approach to the problem and benchmark datasets, which we address in our recent work (Negi et al., 2018).	0
1463	24670	24670	S19-9	Introduction	22	29	3.0	1.0	Therefore, we introduce this pilot shared task to disseminate suggestion mining benchmarks and evaluate state of the art methods for text classification on domain specific and cross domain training scenarios.	0
1464	24671	24671	S19-9	Introduction	23	30	3.0	1.0	The datasets released as a part of the shared task include the domains hotel reviews and software developers suggestion forum (see Table 1).	0
1465	24672	24672	S19-9	Introduction	24	31	3.0	1.0	Suggestion mining faces similar text processing challenges as other sentence or short text classification tasks related to opinion mining and subjectivity analysis, such as stance detection (Mohammad et al., 2016), or tweet sentiment classification (Rosenthal et al., 2015).	0
1466	24673	24673	S19-9	Introduction	25	32	3.0	1.0	Some of the observed challenges in suggestion mining are elaborated below:	0
1467	24674	24674	S19-9	Introduction	26	33	3.0	1.0	 Class imbalance: Usually, suggestions tend to appear sparsely among opinionated text, which leads to higher data annotation costs and results in a class distribution bias in the trained models.	0
1468	24675	24675	S19-9	Introduction	27	34	3.0	1.0		0
1469	24676	24676	S19-9	Introduction	28	35	3.0	1.0	Figurative expressions:	0
1470	24677	24677	S19-9	Introduction	29	36	3.0	1.0	Text from social media and other sources usually contains figurative use of language, which demands pragmatic understanding from the models.	0
1471	24678	24678	S19-9	Introduction	30	37	4.0	1.0	For example, 'Try asking for extra juice at breakfast -its 22 euros!!!!!' is more of a sarcasm than a suggestion.	0
1472	24679	24679	S19-9	Introduction	31	38	4.0	1.0	Therefore, a sentence framed as a typical suggestions may not always be a suggestion and vice versa.	0
1473	24680	24680	S19-9	Introduction	32	39	4.0	1.0	A variety of linguistic strategies used in suggestions also make this task interesting from a computational linguistics perspective and labeled datasets can be leveraged for linguistic studies as well.	0
1474	24681	24681	S19-9	Introduction	33	40	4.0	1.0	 Context dependency:	0
1475	24682	24682	S19-9	Introduction	34	41	4.0	1.0	In some cases, context plays a major role in determining whether a sentence is a suggestion or not.	0
1476	24683	24683	S19-9	Introduction	35	42	4.0	1.0	For example, 'There is a parking garage on the corner of the Forbes showroom.' can be labeled as a suggestion (for parking space) when it appears in a restaurant review and a human annotator gets to read the full review.	0
1477	24684	24684	S19-9	Introduction	36	43	4.0	1.0	However, the same sentence would not be labeled as a suggestion if the text is aimed to describe the surroundings of the Forbes showroom.	0
1478	24685	24685	S19-9	Introduction	37	44	4.0	1.0		0
1479	24686	24686	S19-9	Introduction	38	45	4.0	1.0	Long and complex sentences:	0
1480	24687	24687	S19-9	Introduction	39	46	4.0	1.0	Often, a suggestion is expressed in either one part of a sentence, or it is elaborated as a long sentence, like, 'I think that there should be a nice feature where you can be able to slide the status bar down and view all the push notifications that you got but you didn't view, just like	0
1481	24860	24860	S19-10	title	1	1	4.0	1.0	SemEval 2019 Task 10: Math Question Answering	1
1482	24861	24861	S19-10	abstract	1	2	1.0	1.0	We report on the SemEval 2019 task on math question answering.	0
1483	24862	24862	S19-10	abstract	2	3	2.0	1.0	We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions.	0
1484	24863	24863	S19-10	abstract	3	4	3.0	1.0	For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms.	0
1485	24864	24864	S19-10	abstract	4	5	4.0	1.0	Systems were evaluated based on the percentage of correctly answered questions.	0
1486	24865	24865	S19-10	abstract	5	6	4.0	1.0	The top system correctly answered 45% of the test questions, a considerable improvement over the 17% random guessing baseline.	0
1487	24995	24995	S19-12	abstract	1	2	1.0	1.0	We present the SemEval-2019	0
1488	24996	24996	S19-12	abstract	2	3	1.0	1.0	Task 12 which focuses on toponym resolution in scientific articles.	0
1489	24997	24997	S19-12	abstract	3	4	2.0	1.0	Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations.	1
1490	24998	24998	S19-12	abstract	4	5	2.0	1.0	We proposed three subtasks.	0
1491	24999	24999	S19-12	abstract	5	6	2.0	1.0	In Subtask 1, we asked participants to detect all toponyms in an article.	0
1492	25000	25000	S19-12	abstract	6	7	3.0	1.0	In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames.	0
1493	25001	25001	S19-12	abstract	7	8	3.0	1.0	In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms.	0
1494	25002	25002	S19-12	abstract	8	9	3.0	1.0	A total of 29 teams registered, and 8 teams submitted a system run.	0
1495	25003	25003	S19-12	abstract	9	10	4.0	1.0	We summarize the corpus and the tools created for the challenge.	0
1496	25004	25004	S19-12	abstract	10	11	4.0	1.0	They are freely available at https://competitions.codalab. org/competitions/19948.	0
1497	25005	25005	S19-12	abstract	11	12	4.0	1.0	We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.	0
1498	25182	25182	S20-1	Overview	1	7	1.0	1.0	Recent years have seen an exponentially rising interest in computational Lexical Semantic Change (LSC) detection (Tahmasebi et al., 2018;Kutuzov et al., 2018).	0
1499	25183	25183	S20-1	Overview	2	8	1.0	1.0	However, the field is lacking standard evaluation tasks and data.	0
1500	25184	25184	S20-1	Overview	3	9	1.0	1.0	Almost all papers differ in how the evaluation is performed and what factors are considered in the evaluation.	0
1501	25185	25185	S20-1	Overview	4	10	2.0	1.0	Very few are evaluated on a manually annotated diachronic corpus Perrone et al., 2019;	0
1502	25186	25186	S20-1	Overview	5	11	2.0	1.0	Schlechtweg et al., 2019, e.g.).	0
1503	25187	25187	S20-1	Overview	6	12	2.0	1.0	This puts a damper on the development of computational models for LSC, and is a barrier for high-quality, comparable results that can be used in follow-up tasks.	0
1504	25188	25188	S20-1	Overview	7	13	2.0	1.0	We report the results of the first SemEval shared task on Unsupervised LSC detection.	0
1505	25189	25189	S20-1	Overview	8	14	3.0	1.0	1	0
1506	25190	25190	S20-1	Overview	9	15	3.0	1.0	We introduce two related subtasks for computational LSC detection, which aim to identify the change in meaning of words over time using corpus data.	1
1507	25191	25191	S20-1	Overview	10	16	3.0	1.0	We provide a high-quality multilingual (English, German, Latin, Swedish) LSC gold standard relying on approximately 100,000 instances of human judgment.	0
1508	25192	25192	S20-1	Overview	11	17	3.0	1.0	For the first time, it is possible to compare the variety of proposed models on relatively solid grounds and across languages, and to put previously reached conclusions on trial.	0
1509	25193	25193	S20-1	Overview	12	18	4.0	1.0	We may now provide answers to questions concerning the performance of different types of semantic representations (such as token embeddings vs. type embeddings, and topic models vs. vector space models), alignment methods and change measures.	0
1510	25194	25194	S20-1	Overview	13	19	4.0	1.0	We provide a thorough analysis of the submitted results uncovering trends for models and opening perspectives for further improvements.	0
1511	25195	25195	S20-1	Overview	14	20	4.0	1.0	In addition to this, the CodaLab website will remain open to allow any reader to directly and easily compare their results to the participating systems.	0
1512	25196	25196	S20-1	Overview	15	21	4.0	1.0	We expect the long-term impact of the task to be significant, and hope to encourage the study of LSC in more languages than are currently studied, in particular less-resourced languages.	0
1513	25574	25574	S20-2	title	1	1	4.0	1.0	SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment	1
1514	25575	25575	S20-2	abstract	1	2	1.0	1.0	Lexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction.	0
1515	25576	25576	S20-2	abstract	2	3	2.0	1.0	Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries.	0
1516	25577	25577	S20-2	abstract	3	4	2.0	1.0	As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE.	0
1517	25578	25578	S20-2	abstract	4	5	3.0	1.0	The shared task spans three dimensions: (1) monolingual LE in multiple languages versus cross-lingual LE, (2) binary versus graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs).	0
1518	25579	25579	S20-2	abstract	5	6	3.0	1.0	We offered two different evaluation tracks: (a) distributional (Dist): for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b)	0
1519	25580	25580	S20-2	abstract	6	7	4.0	1.0	Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet).	0
1520	25581	25581	S20-2	abstract	7	8	4.0	1.0	In the Any track, we received system runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction.	0
1521	25760	25760	S20-3	abstract	1	2	1.0	1.0	This paper presents the Graded Word Similarity in Context (GWSC) task which asked participants to predict the effects of context on human perception of similarity in English, Croatian, Slovene and Finnish.	1
1522	25761	25761	S20-3	abstract	2	3	2.0	1.0	We received 15 submissions and 11 system description papers.	0
1523	25762	25762	S20-3	abstract	3	4	3.0	1.0	A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages.	0
1524	25763	25763	S20-3	abstract	4	5	4.0	1.0	Systems beat the baselines by significant margins, but few did well in more than one language or subtask.	0
1525	25764	25764	S20-3	abstract	5	6	4.0	1.0	Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect.	0
1526	26520	26520	S20-6	abstract	1	2	1.0	1.0	Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered.	0
1527	26521	26521	S20-6	abstract	2	3	2.0	1.0	In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a termdefinition pair corpus that reflects the complex reality of definitions in natural language.	1
1528	26522	26522	S20-6	abstract	3	4	3.0	1.0	Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner.	0
1529	26523	26523	S20-6	abstract	4	5	4.0	1.0	Deft	0
1530	26524	26524	S20-6	abstract	5	6	4.0	1.0	Eval involved 3 distinct subtasks: 1) Sentence classification, 2) sequence labeling, and 3) relation extraction.	0
1531	26700	26700	S20-7	Task Description	1	52	1.0	1.0	The objective of this shared task is to build systems for rating a humorous effect that is caused by small changes in text.	1
1532	26701	26701	S20-7	Task Description	2	53	2.0	1.0	To this end, we focus on humor obtained by applying micro-edits to news headlines.	1
1533	26702	26702	S20-7	Task Description	3	54	2.0	1.0	Editing headlines presents a unique opportunity for humor research since headlines convey substantial information using only a few words.	0
1534	26703	26703	S20-7	Task Description	4	55	3.0	2.0	This creates a rich background against which a micro-edit can lead to a humorous effect.	0
1535	26704	26704	S20-7	Task Description	5	56	3.0	2.0	With that data, a computational humor model can focus on the exact localized cause of the humorous effect in a short textual context.	0
1536	26705	26705	S20-7	Task Description	6	57	4.0	2.0	We split our task into two subtasks.	0
1537	26706	26706	S20-7	Task Description	7	58	4.0	2.0	The dataset statistics for these subtasks are shown in Table 2.	0
1538	26869	26869	S20-8	abstract	1	2	1.0	1.0	Information on social media comprises of various modalities such as textual, visual and audio.	0
1539	26870	26870	S20-8	abstract	2	3	1.0	1.0	NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media.	0
1540	26871	26871	S20-8	abstract	3	4	2.0	1.0	However, computational processing of Internet memes needs a hybrid approach.	0
1541	26872	26872	S20-8	abstract	4	5	2.0	1.0	The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content anymore.	0
1542	26873	26873	S20-8	abstract	5	6	3.0	1.0	To the best of our knowledge, there is not much attention towards meme emotion analysis.	0
1543	26874	26874	S20-8	abstract	6	7	3.0	1.0	The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes.	0
1544	26875	26875	S20-8	abstract	7	8	4.0	1.0	The task Memotion analysis released approx 10K annotated memes-with human annotated labels namely sentiment(positive, negative, neutral), type of emotion(sarcastic,funny,offensive, motivation) and their corresponding intensity.	0
1545	26876	26876	S20-8	abstract	8	9	4.0	1.0	The challenge consisted of three subtasks: sentiment (positive, negative, and neutral) analysis of memes, overall emotion (humor, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion.	1
1546	26877	26877	S20-8	abstract	9	10	4.0	1.0	The best performances achieved were F 1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks.	0
1547	27105	27105	S20-9	Task Description	1	61	1.0	1.0	Although code-mixing has received some attention recently, properly annotated data is still scarce.	0
1548	27106	27106	S20-9	Task Description	2	62	1.0	1.0	We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media.	1
1549	27107	27107	S20-9	Task Description	3	63	1.0	1.0	Each tweet is classified into one of the three polarity classes -Positive, Negative, Neutral.	0
1550	27108	27108	S20-9	Task Description	4	64	1.0	1.0	Each tweet also has word-level language marking.	0
1551	27109	27109	S20-9	Task Description	5	65	2.0	1.0	We release two datasets -Spanglish and Hinglish.	0
1552	27110	27110	S20-9	Task Description	6	66	2.0	1.0	We used CodaLab 4,5 to release the datasets and evaluate submissions.	0
1553	27111	27111	S20-9	Task Description	7	67	2.0	2.0	Initially, the participants had access only to train and validation data.	0
1554	27112	27112	S20-9	Task Description	8	68	2.0	2.0	They could check their system's performance on the validation set on a public leaderboard.	0
1555	27113	27113	S20-9	Task Description	9	69	3.0	2.0	Later, a previously unseen test set was released, and the performance on the test set was used to rank the participants.	0
1556	27114	27114	S20-9	Task Description	10	70	3.0	2.0	Only the first three submissions on the test set by each participant were considered, to avoid over-fitting on the test set.	0
1557	27115	27115	S20-9	Task Description	11	71	3.0	2.0	The ranking was done based on the best out of the three submissions.	0
1558	27116	27116	S20-9	Task Description	12	72	3.0	2.0	There was no distinction between constrained and unconstrained systems, but the participants were asked to report what additional resources they have used for each submitted run.	0
1559	27117	27117	S20-9	Task Description	13	73	4.0	2.0	We release 20k labeled tweets for Hinglish and  19k labeled tweets for Spanglish.	0
1560	27118	27118	S20-9	Task Description	14	74	4.0	2.0	In both the datasets, 6 in addition to the tweet level sentiment label, each tweet also has a word-level language label.	0
1561	27119	27119	S20-9	Task Description	15	75	4.0	2.0	The detailed distribution is provided in Table 1.	0
1562	27120	27120	S20-9	Task Description	16	76	4.0	2.0	Some annotated examples are provided in Table 2.	0
1563	27121	27121	S20-9	Task Description	17	77	4.0	2.0	Although this task focuses on sentiment analysis, the data has word-level language marking and can be used for other NLP tasks.	0
1564	27311	27311	S20-10	abstract	1	2	1.0	1.0	In this paper, we present the main findings and compare the results of SemEval-2020 Task 10, Emphasis Selection for Written Text in Visual Media.	0
1565	27312	27312	S20-10	abstract	2	3	1.0	1.0	The goal of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in textual content to enable automated design assistance in authoring.	1
1566	27313	27313	S20-10	abstract	3	4	2.0	1.0	The main focus is on short text instances for social media, with a variety of examples, from social media posts to inspirational quotes.	0
1567	27314	27314	S20-10	abstract	4	5	2.0	1.0	Participants were asked to model emphasis using plain text with no additional context from the user or other design considerations.	0
1568	27315	27315	S20-10	abstract	5	6	3.0	1.0	SemEval-2020 Emphasis Selection shared task attracted 197 participants in the early phase and a total of 31 teams made submissions to this task.	0
1569	27316	27316	S20-10	abstract	6	7	3.0	1.0	The highest-ranked submission achieved 0.823 Match m score.	0
1570	27317	27317	S20-10	abstract	7	8	4.0	1.0	The analysis of systems submitted to the task indicates that BERT and RoBERTa were the most common choice of pre-trained models used, and part of speech tag (POS) was the most useful feature.	0
1571	27318	27318	S20-10	abstract	8	9	4.0	1.0	Full results can be found on the task's website 1 .	0
1572	27479	27479	S20-11	title	1	1	4.0	1.0	SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles	1
1573	27480	27480	S20-11	abstract	1	2	1.0	1.0	We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles.	1
1574	27481	27481	S20-11	abstract	2	3	2.0	1.0	The task featured two subtasks.	0
1575	27482	27482	S20-11	abstract	3	4	2.0	1.0	Subtask SI is about Span Identification: given a plain-text document, spot the specific text fragments containing propaganda.	0
1576	27483	27483	S20-11	abstract	4	5	3.0	1.0	Subtask TC is about Technique Classification: given a specific text fragment, in the context of a full document, determine the propaganda technique it uses, choosing from an inventory of 14 possible propaganda techniques.	0
1577	27484	27484	S20-11	abstract	5	6	3.0	1.0	The task attracted a large number of participants: 250 teams signed up to participate and 44 made a submission on the test set.	0
1578	27485	27485	S20-11	abstract	6	7	4.0	1.0	In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used.	0
1579	27486	27486	S20-11	abstract	7	8	4.0	1.0	For both subtasks, the best systems used pre-trained Transformers and ensembles.	0
1580	27797	27797	S20-11	Conclusion and Future Work	1	319	1.0	4.0	We have described SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles.	1
1581	27798	27798	S20-11	Conclusion and Future Work	2	320	2.0	4.0	The task attracted the interest of a number of researchers: 250 teams signed up to participate, and 44 made submissions on the test dataset.	0
1582	27799	27799	S20-11	Conclusion and Future Work	3	321	2.0	4.0	We received 35 and 31 submissions for subtask SI and subtask TC, respectively.	0
1583	27800	27800	S20-11	Conclusion and Future Work	4	322	3.0	4.0	Overall, subtask SI (segment identification) was easier and all systems managed to improve over the baseline.	0
1584	27801	27801	S20-11	Conclusion and Future Work	5	323	3.0	4.0	However, subtask TC (technique classification) proved to be much more challenging, and some teams could not improve over our baseline.	0
1585	27802	27802	S20-11	Conclusion and Future Work	6	324	4.0	4.0	In future work, we plan to extend the dataset to cover more examples as well as more propaganda techniques.	0
1586	27803	27803	S20-11	Conclusion and Future Work	7	325	4.0	4.0	We further plan to develop similar datasets for other languages.	0
1587	28133	28133	S21-1	Introduction	1	7	1.0	1.0	The occurrence of an unknown word in a sentence can adversely affect its comprehension by readers.	0
1588	28134	28134	S21-1	Introduction	2	8	1.0	1.0	Either they give up, misinterpret, or plough on without understanding.	0
1589	28135	28135	S21-1	Introduction	3	9	1.0	1.0	A committed reader may take the time to look up a word and expand their vocabulary, but even in this case they must leave the text, undermining their concentration.	0
1590	28136	28136	S21-1	Introduction	4	10	1.0	1.0	The natural language processing solution is to identify candidate words in a text that may be too difficult for a reader (Shardlow, 2013;	0
1591	28137	28137	S21-1	Introduction	5	11	1.0	1.0	Paetzold and Specia, 2016a).	0
1592	28138	28138	S21-1	Introduction	6	12	1.0	1.0	Each potential word is assigned a judgment by a system to determine if it was deemed 'complex' or not.	0
1593	28139	28139	S21-1	Introduction	7	13	1.0	1.0	These scores indicate which words are likely to cause problems for a reader.	0
1594	28140	28140	S21-1	Introduction	8	14	1.0	1.0	The words that are identified as problematic can be the subject of numerous types of intervention, such as direct replacement in the setting of lexical simplification (Gooding and Kochmar, 2019), or extra information being given in the context of explanation generation .	0
1595	28141	28141	S21-1	Introduction	9	15	1.0	1.0	Whereas previous solutions to this task have typically considered the Complex Word Identification (CWI) task (Paetzold and Specia, 2016a;	0
1596	28142	28142	S21-1	Introduction	10	16	1.0	1.0	Yimam et al., 2018) in which a binary judgment of a word's complexity is given (i.e., is a word complex or not?), we instead focus on the Lexical Complexity Prediction (LCP) task (Shardlow et al., 2020) in which a value is assigned from a continuous scale to identify a word's complexity (i.e., how complex is this word?).	1
1597	28143	28143	S21-1	Introduction	11	17	1.0	1.0	We ask multiple annotators to give a judgment on each instance in our corpus and take the average prediction as our complexity label.	0
1598	28144	28144	S21-1	Introduction	12	18	1.0	1.0	The former task (CWI) forces each user to make a subjective judgment about the nature of the word that models their personal vocabulary.	0
1599	28145	28145	S21-1	Introduction	13	19	1.0	1.0	Many factors may affect the annotator's judgment including their education level, first language, specialism or familiarity with the text at hand.	0
1600	28146	28146	S21-1	Introduction	14	20	1.0	1.0	The annotators may also disagree on the level of difficulty at which to label a word as complex.	0
1601	28147	28147	S21-1	Introduction	15	21	1.0	1.0	One annotator may label every word they feel is above average difficulty, another may label words that they feel unfamiliar with, but understand from the context, whereas another annotator may only label those words that they find totally incomprehensible, even in context.	0
1602	28148	28148	S21-1	Introduction	16	22	1.0	1.0	Our introduction of the LCP task seeks to address this annotator confusion by giving annotators a Likert scale to provide their judgments.	0
1603	28149	28149	S21-1	Introduction	17	23	1.0	1.0	Whilst annotators must still give a subjective judgment depending on their own understanding, familiarity and vocabulary -they do so in a way that better captures the meaning behind each judgment they have given.	0
1604	28150	28150	S21-1	Introduction	18	24	1.0	1.0	By aggregating these judgments we have developed a dataset that contains continuous labels in the range of 0-1 for each instance.	0
1605	28151	28151	S21-1	Introduction	19	25	2.0	1.0	This means that rather than a system predicting whether a word is complex or not (0 or 1), instead a system must now predict where, on our continuous scale, a word falls (0-1).	0
1606	28152	28152	S21-1	Introduction	20	26	2.0	1.0	Consider the following sentence taken from a biomedical source, where the target word 'observation' has been highlighted:	0
1607	28153	28153	S21-1	Introduction	21	27	2.0	1.0	(1) The observation of unequal expression leads to a number of questions.	0
1608	28154	28154	S21-1	Introduction	22	28	2.0	1.0	In the binary annotation setting of CWI some annotators may rightly consider this term non-complex, whereas others may rightly consider it to be complex.	0
1609	28155	28155	S21-1	Introduction	23	29	2.0	1.0	Whilst the meaning of the word is reasonably clear to someone with scientific training, the context in which it is used is unfamiliar for a lay reader and will likely lead to them considering it complex.	0
1610	28156	28156	S21-1	Introduction	24	30	2.0	1.0	In our new LCP setting, we are able to ask annotators to mark the word on a scale from very easy to very difficult.	0
1611	28157	28157	S21-1	Introduction	25	31	2.0	1.0	Each user can give their subjective interpretation on this scale indicating how difficult they found the word.	0
1612	28158	28158	S21-1	Introduction	26	32	2.0	1.0	Whilst annotators will inevitably disagree (some finding it more or less difficult), this is captured and quantified as part of our annotations, with a word of this type likely to lead to a medium complexity value.	0
1613	28159	28159	S21-1	Introduction	27	33	2.0	1.0	LCP is useful as part of the wider task of lexical simplification (Devlin and Tait, 1998), where it can be used to both identify candidate words for simplification (Shardlow, 2013) and rank potential words as replacements . LCP is also relevant to the field of readability assessment, where knowing the proportion of complex words in a text helps to identify the overall complexity of the text (Dale and Chall., 1948).	0
1614	28160	28160	S21-1	Introduction	28	34	2.0	1.0	This paper presents SemEval-2021 Task 1: Lexical Complexity Prediction.	0
1615	28161	28161	S21-1	Introduction	29	35	2.0	1.0	In this task we developed a new dataset for complexity prediction based on the previously published CompLex dataset.	0
1616	28162	28162	S21-1	Introduction	30	36	2.0	1.0	Our dataset covers 10,800 instances spanning 3 genres and containing unigrams and bigrams as targets for complexity prediction.	0
1617	28163	28163	S21-1	Introduction	31	37	2.0	1.0	We solicited participants in our task and released a trial, training and test split in accordance with the SemEval schedule.	0
1618	28164	28164	S21-1	Introduction	32	38	2.0	1.0	We accepted submissions in two separate Sub-tasks, the first being single words only and the second taking single words and multi-word expressions (modelled by our bigrams).	0
1619	28165	28165	S21-1	Introduction	33	39	2.0	1.0	In total 55 teams participated across the two Sub-tasks.	0
1620	28166	28166	S21-1	Introduction	34	40	2.0	1.0	The rest of this paper is structured as folllows:	0
1621	28167	28167	S21-1	Introduction	35	41	2.0	1.0	In Section 2 we discuss the previous two iterations of the CWI task.	0
1622	28168	28168	S21-1	Introduction	36	42	2.0	1.0	In Section 3, we present the CompLex 2.0 dataset that we have used for our task, including the methodology we used to produce trial, test and training splits.	0
1623	28169	28169	S21-1	Introduction	37	43	2.0	1.0	In Section 5, we show the results of the participating systems and compare the features that were used by each system.	0
1624	28170	28170	S21-1	Introduction	38	44	3.0	1.0	We finally discuss the nature of LCP in Section 7 and give concluding remarks in Section 8 2 Related Tasks CWI 2016 at SemEval	0
1625	28171	28171	S21-1	Introduction	39	45	3.0	1.0	The CWI shared task was organized at SemEval 2016 (Paetzold and Specia, 2016a).	0
1626	28172	28172	S21-1	Introduction	40	46	3.0	1.0	The CWI 2016 organizers introduced a new CWI dataset and reported the results of 42 CWI systems developed by 21 teams.	0
1627	28173	28173	S21-1	Introduction	41	47	3.0	1.0	Words in their dataset were considered complex if they were difficult to understand for non-native English speakers according to a binary labelling protocol.	0
1628	28174	28174	S21-1	Introduction	42	48	3.0	1.0	A word was considered complex if at least one of the annotators found it to be difficult.	0
1629	28175	28175	S21-1	Introduction	43	49	3.0	1.0	The training dataset consisted of 2,237 instances, each labelled by 20 annotators and the test dataset had 88,221 instances, each labelled by 1 annotator (Paetzold and Specia, 2016a).	0
1630	28176	28176	S21-1	Introduction	44	50	3.0	1.0	The participating systems leveraged lexical features (Choubey and Pateria, 2016;	0
1631	28177	28177	S21-1	Introduction	45	51	3.0	1.0	Bingel et al., 2016;Quijada and Medero, 2016) and word embeddings (Kuru, 2016;S.	0
1632	28178	28178	S21-1	Introduction	46	52	3.0	1.0	P et al., 2016;	0
1633	28179	28179	S21-1	Introduction	47	53	3.0	1.0	Gillin, 2016), as well as finding that frequency features, such as those taken from Wikipedia (Konkol, 2016;	0
1634	28180	28180	S21-1	Introduction	48	54	3.0	1.0	Wrbel, 2016) were useful.	0
1635	28181	28181	S21-1	Introduction	49	55	3.0	1.0	Systems used binary classifiers such as SVMs (Kuru, 2016;S.	0
1636	28182	28182	S21-1	Introduction	50	56	3.0	1.0	P et al., 2016;	0
1637	28183	28183	S21-1	Introduction	51	57	3.0	1.0	Choubey and Pateria, 2016), Decision Trees (Choubey and Pateria, 2016;	0
1638	28184	28184	S21-1	Introduction	52	58	3.0	1.0	Quijada and Medero, 2016;Malmasi et al., 2016), Random Forests (Ronzano et al., 2016	0
1639	28185	28185	S21-1	Introduction	53	59	3.0	1.0	Brooke et al., 2016;	0
1640	28186	28186	S21-1	Introduction	54	60	3.0	1.0	Mukherjee et al., 2016) and thresholdbased metrics (Kauchak, 2016;	0
1641	28187	28187	S21-1	Introduction	55	61	3.0	1.0	Wrbel, 2016) to predict the complexity labels.	0
1642	28188	28188	S21-1	Introduction	56	62	3.0	1.0	The winning system made use of threshold-based methods and features extracted from Simple Wikipedia (Paetzold and Specia, 2016b).	0
1643	28189	28189	S21-1	Introduction	57	63	4.0	1.0	A post-competition analysis (Zampieri et al., 2017) with oracle and ensemble methods showed that most systems performed poorly due mostly to the way in which the data was annotated and the the small size of the training dataset.	0
1644	28190	28190	S21-1	Introduction	58	64	4.0	2.0	CWI 2018 at BEA	0
1645	28191	28191	S21-1	Introduction	59	65	4.0	2.0	The second CWI Shared Task was organized at the BEA workshop 2018 (Yimam et al., 2018).	0
1646	28192	28192	S21-1	Introduction	60	66	4.0	2.0	Unlike the first task, this second task had two objectives.	0
1647	28193	28193	S21-1	Introduction	61	67	4.0	2.0	The first objective was the binary complex or non-complex classification of target words.	0
1648	28194	28194	S21-1	Introduction	62	68	4.0	2.0	The second objective was regression or probabilistic classification in which 13 teams were asked to assign the probability of a target word being considered complex by a set of language learners.	0
1649	28195	28195	S21-1	Introduction	63	69	4.0	2.0	A major difference in this second task was that datasets of differing genres: (TEXT GENRES) as well as English, German and Spanish datasets for monolingual speakers and a French dataset for multilingual speakers were provided (Yimam et al., 2018).	0
1650	28196	28196	S21-1	Introduction	64	70	4.0	2.0	Similar to 2016, systems made use of a variety of lexical features including word length (Wani et al., 2018;	0
1651	28197	28197	S21-1	Introduction	65	71	4.0	2.0	De Hertog and Tack, 2018;	0
1652	28198	28198	S21-1	Introduction	66	72	4.0	2.0	AbuRa'ed and Saggion, 2018;Hartmann and dos Santos, 2018;	0
1653	28199	28199	S21-1	Introduction	67	73	4.0	2.0	Alfter and Piln, 2018;Kajiwara and Komachi, 2018), frequency (De Hertog and Tack, 2018;	0
1654	28200	28200	S21-1	Introduction	68	74	4.0	2.0	Aroyehun et al., 2018;	0
1655	28201	28201	S21-1	Introduction	69	75	4.0	2.0	Alfter and Piln, 2018;Kajiwara and Komachi, 2018), N-gram features (Gooding and Kochmar, 2018;	0
1656	28202	28202	S21-1	Introduction	70	76	4.0	2.0	Popovi, 2018;Hartmann and dos Santos, 2018;	0
1657	28203	28203	S21-1	Introduction	71	77	4.0	2.0	Alfter and Piln, 2018;Butnaru and Ionescu, 2018) and word embeddings (De Hertog and Tack, 2018;	0
1658	28204	28204	S21-1	Introduction	72	78	4.0	2.0	AbuRa'ed and Saggion, 2018;Aroyehun et al., 2018;Butnaru and Ionescu, 2018).	0
1659	28205	28205	S21-1	Introduction	73	79	4.0	2.0	A variety of classifiers were used ranging from traditional machine learning classifiers (Gooding and Kochmar, 2018;	0
1660	28206	28206	S21-1	Introduction	74	80	4.0	2.0	Popovi, 2018;AbuRa'ed andSaggion, 2018), to Neural Networks (De Hertog andAroyehun et al., 2018).	0
1661	28207	28207	S21-1	Introduction	75	81	4.0	2.0	The winning system made use of Adaboost with WordNet features, POS tags, dependency parsing relations and psycholinguistic features (Gooding and Kochmar, 2018).	0
1662	28382	28382	S21-2	abstract	1	2	1.0	1.0	In this paper, we introduce the first SemEval task on Multilingual and Cross-Lingual Wordin-Context disambiguation (MCL-WiC).	0
1663	28383	28383	S21-2	abstract	2	3	2.0	1.0	This task allows the largely under-investigated inherent ability of systems to discriminate between word senses within and across languages to be evaluated, dropping the requirement of a fixed sense inventory.	0
1664	28384	28384	S21-2	abstract	3	4	2.0	1.0	Framed as a binary classification, our task is divided into two parts.	0
1665	28385	28385	S21-2	abstract	4	5	3.0	1.0	In the multilingual sub-task, participating systems are required to determine whether two target words, each occurring in a different context within the same language, express the same meaning or not.	1
1666	28386	28386	S21-2	abstract	5	6	3.0	1.0	Instead, in the crosslingual part, systems are asked to perform the task in a cross-lingual scenario, in which the two target words and their corresponding contexts are provided in two different languages.	1
1667	28387	28387	S21-2	abstract	6	7	4.0	1.0	We illustrate our task, as well as the construction of our manually-created dataset including five languages, namely Arabic, Chinese, English, French and Russian, and the results of the participating systems.	0
1668	28388	28388	S21-2	abstract	7	8	4.0	1.0	Datasets and results are available at: https://github.com/ SapienzaNLP/mcl-wic.	0
1669	28617	28617	S21-4	abstract	1	2	1.0	1.0	This paper introduces the SemEval-2021 shared task 4: Reading Comprehension of Abstract Meaning (ReCAM).	0
1670	28618	28618	S21-4	abstract	2	3	1.0	1.0	This shared task is designed to help evaluate the ability of machines in representing and understanding abstract concepts.	0
1671	28619	28619	S21-4	abstract	3	4	2.0	1.0	Given a passage and the corresponding question, a participating system is expected to choose the correct answer from five candidates of abstract concepts in a cloze-style machine reading comprehension setup.	1
1672	28620	28620	S21-4	abstract	4	5	2.0	1.0	Based on two typical definitions of abstractness, i.e., the imperceptibility and nonspecificity, our task provides three subtasks to evaluate the participating models.	0
1673	28621	28621	S21-4	abstract	5	6	3.0	1.0	Specifically, Subtask 1 aims to evaluate how well a system can model concepts that cannot be directly perceived in the physical world.	0
1674	28622	28622	S21-4	abstract	6	7	3.0	1.0	Subtask 2 focuses on models' ability in comprehending nonspecific concepts located high in a hypernym hierarchy given the context of a passage.	0
1675	28623	28623	S21-4	abstract	7	8	4.0	1.0	Subtask 3 aims to provide some insights into models' generalizability over the two types of abstractness.	0
1676	28624	28624	S21-4	abstract	8	9	4.0	1.0	During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2.	0
1677	28625	28625	S21-4	abstract	9	10	4.0	1.0	The participating teams additionally made 29 submissions to Subtask 3.	0
1678	28884	28884	S21-5	Introduction	1	8	1.0	1.0	Discussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a).	0
1679	28885	28885	S21-5	Introduction	2	9	1.0	1.0	Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017;	0
1680	28886	28886	S21-5	Introduction	3	10	1.0	1.0	Pavlopoulos et al., 2017;Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible.	0
1681	28887	28887	S21-5	Introduction	4	11	1.0	1.0	In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic.	0
1682	28888	28888	S21-5	Introduction	5	12	2.0	1.0	This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems.	0
1683	28889	28889	S21-5	Introduction	6	13	2.0	1.0	As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible.	1
1684	28890	28890	S21-5	Introduction	7	14	2.0	1.0	Note that a post may include no toxic span and still be marked as toxic.	0
1685	28891	28891	S21-5	Introduction	8	15	2.0	1.0	On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts.	0
1686	28892	28892	S21-5	Introduction	9	16	3.0	1.0	We provided a dataset of English posts with gold annotations of toxic spans, and evaluated participating systems on a held-out test subset using character-based F1.	0
1687	28893	28893	S21-5	Introduction	10	17	3.0	1.0	The task could be addressed as supervised sequence labeling, training on the provided posts with gold toxic spans.	0
1688	28894	28894	S21-5	Introduction	11	18	3.0	1.0	It could also be treated as rationale extraction (Li et al., 2016;	0
1689	28895	28895	S21-5	Introduction	12	19	3.0	1.0	Ribeiro et al., 2016), using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations.	0
1690	28896	28896	S21-5	Introduction	13	20	4.0	1.0	There were almost 500 individual participants, and 36 out of the 92 teams that were formed submitted reports and results that we survey here.	0
1691	28897	28897	S21-5	Introduction	14	21	4.0	1.0	Most teams adopted the supervised sequence labeling approach.	0
1692	28898	28898	S21-5	Introduction	15	22	4.0	1.0	Hence, there is still scope for further work on the rationale extraction approach.	0
1693	28899	28899	S21-5	Introduction	16	23	4.0	1.0	We also discuss other possible improvements in the definition and data of the task.	0
1694	29120	29120	S21-6	abstract	1	2	1.0	1.0	We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems.	0
1695	29121	29121	S21-6	abstract	2	3	2.0	1.0	The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image.	1
1696	29122	29122	S21-6	abstract	3	4	3.0	1.0	It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set.	0
1697	29123	29123	S21-6	abstract	4	5	4.0	1.0	The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image.	0
1698	29124	29124	S21-6	abstract	5	6	4.0	1.0	Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.	0
1699	29341	29341	S21-7	abstract	1	2	1.0	1.0	SemEval 2021	0
1700	29342	29342	S21-7	abstract	2	3	1.0	1.0	Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection.	0
1701	29343	29343	S21-7	abstract	3	4	2.0	1.0	We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70.	0
1702	29344	29344	S21-7	abstract	4	5	2.0	1.0	Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold.	1
1703	29345	29345	S21-7	abstract	5	6	3.0	1.0	The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models.	0
1704	29346	29346	S21-7	abstract	6	7	3.0	1.0	Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training.	0
1705	29347	29347	S21-7	abstract	7	8	4.0	1.0	The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task.	0
1706	29348	29348	S21-7	abstract	8	9	4.0	1.0	We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems.	0
1707	29639	29639	S21-8	Task Description	1	50	1.0	1.0	Meas	0
1708	29640	29640	S21-8	Task Description	2	51	1.0	1.0	Eval is an entity recognition and semantic relation extraction task focused on finding counts and measurements, attributes of those quantities, and additional information including measured entities, properties, and measurement contexts.	1
1709	29641	29641	S21-8	Task Description	3	52	1.0	1.0	Meas	0
1710	29642	29642	S21-8	Task Description	4	53	1.0	1.0	MeasEval is composed of five sub-tasks that cover span extraction, classification, and relation extraction, including cross-sentence relations.	1
1711	29643	29643	S21-8	Task Description	5	54	1.0	1.0	Given a paragraph from a scientific text:	0
1712	29644	29644	S21-8	Task Description	6	55	2.0	1.0		0
1713	29645	29645	S21-8	Task Description	7	56	2.0	1.0	For each paragraph of text, identify all spans containing quantities (e.g. 12 kg).	0
1714	29646	29646	S21-8	Task Description	8	57	2.0	1.0	Quantities are treated as strings, and are not converted or normalized.	0
1715	29647	29647	S21-8	Task Description	9	58	2.0	1.0		0
1716	29648	29648	S21-8	Task Description	10	59	2.0	1.0	For each identified Quantity, identify the Unit of Measurement (e.g. kg), if one exists.	0
1717	29649	29649	S21-8	Task Description	11	60	3.0	1.0	For each Quantity classify additional value Modifiers (e.g. count, range, approximate, mean, etc.) that apply to the Quantity.	0
1718	29650	29650	S21-8	Task Description	12	61	3.0	2.0		0
1719	29651	29651	S21-8	Task Description	13	62	3.0	2.0	For each identified Quantity, identify the Measured Entity (e.g. bed inventory) it applies to (if one exists) and mark its span.	0
1720	29652	29652	S21-8	Task Description	14	63	3.0	2.0	If an associated Measured Property (e.g. concentration) also exists, identify it and mark its span.	0
1721	29653	29653	S21-8	Task Description	15	64	3.0	2.0		0
1722	29654	29654	S21-8	Task Description	16	65	4.0	2.0	Identify and mark the span of any Qualifier (e.g. after incubation) that is needed to record additional related context to either validate or understand each identified Quantity.	0
1723	29655	29655	S21-8	Task Description	17	66	4.0	2.0	 Identify relationships between Quantity, Measured Entity, Measured Property, and Qualifier spans using the HasQuantity, HasProperty, and Qualifies relation types.	0
1724	29656	29656	S21-8	Task Description	18	67	4.0	2.0	More detailed definitions can be found be reviewing the MeasEval Annotation Guidelines.	0
1725	29657	29657	S21-8	Task Description	19	68	4.0	2.0	5	0
1726	29658	29658	S21-8	Task Description	20	69	4.0	2.0	We describe each of the elements to be extracted in more detail in the next section.	0
1727	29659	29659	S21-8	Task Description	21	70	4.0	2.0	4 Annotated Data	0
1728	29834	29834	S21-9	abstract	1	2	1.0	1.0	Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells.	0
1729	29835	29835	S21-9	abstract	2	3	1.0	1.0	In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS).	0
1730	29836	29836	S21-9	abstract	3	4	2.0	1.0	Our dataset contains 981 manuallygenerated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations.	0
1731	29837	29837	S21-9	abstract	4	5	2.0	1.0	SEM-TAB-FACTS featured two sub-tasks.	0
1732	29838	29838	S21-9	abstract	5	6	3.0	1.0	In subtask A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table.	1
1733	29839	29839	S21-9	abstract	6	7	3.0	1.0	In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement.	1
1734	29840	29840	S21-9	abstract	7	8	4.0	1.0	69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B.	0
1735	29841	29841	S21-9	abstract	8	9	4.0	1.0	We present our results and main findings from the competition.	0
1736	30019	30019	S21-10	Introduction	1	9	1.0	1.0	Data sharing restrictions are common in NLP datasets.	0
1737	30020	30020	S21-10	Introduction	2	10	1.0	1.0	For example, Twitter policies do not allow sharing of tweet text, though tweet IDs may be shared.	0
1738	30021	30021	S21-10	Introduction	3	11	1.0	1.0	The situation is even more common in clinical NLP, where patient health information must be protected, and annotations over health text, when released at all, often require the signing of complex data use agreements.	0
1739	30022	30022	S21-10	Introduction	4	12	1.0	1.0	The Source-Free Domain Adaptation shared task presents a new framework that asks participants to develop semantic annotation systems in the face of data sharing constraints.	0
1740	30023	30023	S21-10	Introduction	5	13	1.0	1.0	A participant's goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed.	0
1741	30024	30024	S21-10	Introduction	6	14	1.0	1.0	Instead of annotated training data, participants are given a model trained on the annotations.	0
1742	30025	30025	S21-10	Introduction	7	15	1.0	1.0	Then, given unlabeled target domain data, they are asked to make predictions.	0
1743	30026	30026	S21-10	Introduction	8	16	1.0	1.0	This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016;	0
1744	30027	30027	S21-10	Introduction	9	17	2.0	1.0	Ziser and Reichart, 2017;	0
1745	30028	30028	S21-10	Introduction	10	18	2.0	1.0	Saito et al., 2017;Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum III, 2007;	0
1746	30029	30029	S21-10	Introduction	11	19	2.0	1.0	Xia et al., 2013;	0
1747	30030	30030	S21-10	Introduction	12	20	2.0	1.0	Kim et al., 2016;	0
1748	30031	30031	S21-10	Introduction	13	21	2.0	1.0	Peng and Dredze, 2017).	0
1749	30032	30032	S21-10	Introduction	14	22	2.0	1.0	Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition.	0
1750	30033	30033	S21-10	Introduction	15	23	2.0	1.0	These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem.	0
1751	30034	30034	S21-10	Introduction	16	24	2.0	1.0	Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowing us to annotate data in a new domain.	0
1752	30035	30035	S21-10	Introduction	17	25	3.0	1.0	Negation detection is the task of identifying negation cues in text.	0
1753	30036	30036	S21-10	Introduction	18	26	3.0	1.0	This task has been widely studied by previous work (Chapman et al., 2007(Chapman et al., , 2001; Harkema et al., 2009;	0
1754	30037	30037	S21-10	Introduction	19	27	3.0	1.0	Sohn et al., 2012) including the development of a variety of datasets (Uzuner et al., 2011;Mehrabi et al., 2015).	0
1755	30038	30038	S21-10	Introduction	20	28	3.0	1.0	However, there are still large performance losses in the cross-domain setting (Wu et al., 2014).	0
1756	30039	30039	S21-10	Introduction	21	29	3.0	1.0	"For negation detection, we provided a ""spanin-context"" classification model, fine-tuned on instances of the SHARP Seed dataset of Mayo Clinic clinical notes, which the organizers have access to but cannot currently be distributed."	0
1757	30040	30040	S21-10	Introduction	22	30	3.0	1.0	(Models were approved to be distributed, as the data is deidentified.)	0
1758	30041	30041	S21-10	Introduction	23	31	3.0	1.0	In the SHARP data, clinical events are marked with a boolean polarity indicator, with values of either asserted or negated.	0
1759	30042	30042	S21-10	Introduction	24	32	3.0	1.0	As development   data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners Health-Care.	0
1760	30043	30043	S21-10	Introduction	25	33	4.0	1.0	The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016).	0
1761	30044	30044	S21-10	Introduction	26	34	4.0	1.0	Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013).	0
1762	30045	30045	S21-10	Introduction	27	35	4.0	1.0	For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in Sem-Eval 2018 Task 6 (Laparra et al., 2018).	0
1763	30046	30046	S21-10	Introduction	28	36	4.0	1.0	As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019).	0
1764	30047	30047	S21-10	Introduction	29	37	4.0	1.0	For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary.	0
1765	30048	30048	S21-10	Introduction	30	38	4.0	1.0	(Models were approved to be distributed, as the data is deidentified.)	0
1766	30049	30049	S21-10	Introduction	31	39	4.0	1.0	The development data was the annotated news portion of the SemEval 2018 Task 6 data whose source text is from the freely available TimeBank.	0
1767	30050	30050	S21-10	Introduction	32	40	4.0	1.0	For evaluation, we used a set of annotated documents extracted from food security warning systems.	0
1768	30051	30051	S21-10	Introduction	33	41	4.0	1.0	The main impact of this task is to drive the NLP community to address the serious challenges of data sharing constraints by designing new domain adaptation algorithms that allow source data and target data to remain separate, rather than assuming they can be shared freely with each other.	1
1769	30289	30289	S21-11	Task Description	1	103	1.0	2.0	Our comprehensive NCG Shared Task formalism was as follows.	0
1770	30290	30290	S21-11	Task Description	2	104	1.0	2.0	Given a scholarly article A in plaintext format, the goal was to extract (1) a set of contribution sentences C sent = {C sent 1 , ..., C sent N }, (2) a set of scientific knowledge terms and predicates from C sent referred to as entities E = {e 1 , ..., e N }, and (3) to organize the entities E as a set of (subject,predicate,object) triple statements T = {t 1 , ..., t N } toward KG building organized under three or more of the 12 total IUs.	1
1771	30291	30291	S21-11	Task Description	3	105	1.0	2.0	Task Evaluation Phases.	0
1772	30292	30292	S21-11	Task Description	4	106	1.0	2.0	The task comprised three evaluation phases, thereby enabling detailed system evaluations.	0
1773	30293	30293	S21-11	Task Description	5	107	1.0	2.0	Evaluation Phase 1: End-to-end Pipeline.	0
1774	30294	30294	S21-11	Task Description	6	108	1.0	2.0	In this phase, systems were tested for the comprehensive end-to-end KG building task described in the formalism above.	0
1775	30295	30295	S21-11	Task Description	7	109	1.0	2.0	Given a test set of articles A in plaintext format, the participating systems were expected to return: (1) a set of contribution sentences C sent , (2) a set of scientific knowledge terms and predicates from C sent , i.e. entities E, and (3) the entities in E organized in a set of triple statements T toward KG building.	0
1776	30296	30296	S21-11	Task Description	8	110	2.0	2.0	System outputs were evaluated for the three aspects and overall.	0
1777	30297	30297	S21-11	Task Description	9	111	2.0	2.0	Evaluation Phase 2, Part 1: Phrases and Triples.	0
1778	30298	30298	S21-11	Task Description	10	112	2.0	2.0	In this phase, systems were tested only for their capacity to extract phrases and organize them as triples.	0
1779	30299	30299	S21-11	Task Description	11	113	2.0	2.0	Given a test set of articles A in plain-text format and contribution sentences C sent from each article, each system was expected to return: (1) the entities E, and (2) the set of triple statements T .	0
1780	30300	30300	S21-11	Task Description	12	114	2.0	2.0	Evaluation Phase 2, Part 2: Triples.	0
1781	30301	30301	S21-11	Task Description	13	115	2.0	2.0	In this phase, systems were tested only for the triples formation task.	0
1782	30302	30302	S21-11	Task Description	14	116	2.0	2.0	Thus, given gold entities E for the set of C sent , systems were expected to form triple statements T .	0
1783	30303	30303	S21-11	Task Description	15	117	3.0	2.0	In the Evaluation phases that lasted from Jan 10 till Feb 1, 2021, we provided the participants with masked versions of the test set based on the current evaluation phase.	0
1784	30304	30304	S21-11	Task Description	16	118	3.0	2.0	The test set annotations in each phase were uploaded to CodaLab and were not available to the participants.	0
1785	30305	30305	S21-11	Task Description	17	119	3.0	2.0	To obtain results, the participants were expected to upload their system outputs to Codalab where they were automatically evaluated by our script and reference data stored on the platform.	0
1786	30306	30306	S21-11	Task Description	18	120	3.0	2.0	In each evaluation phase, teams were restricted to make only 10 submissions and only one result, i.e. the top-scoring result, was shown on the leaderboard.	0
1787	30307	30307	S21-11	Task Description	19	121	3.0	2.0	Before the task began, our participants were onboarded via our task website https://ncg-task.github.io/.	0
1788	30308	30308	S21-11	Task Description	20	122	3.0	2.0	Further, participants were encouraged to discuss their task-related questions via our task Google groups page at https://groups.google.com/forum/#! forum/ncg-task-semeval-2021.	0
1789	30309	30309	S21-11	Task Description	21	123	3.0	2.0	The NCG Data Collection of Articles	0
1790	30310	30310	S21-11	Task Description	22	124	4.0	2.0	Our base collection of scholarly articles was downloaded from the publicly available leaderboard of tasks in AI called https://paperswithcode.com/.	0
1791	30311	30311	S21-11	Task Description	23	125	4.0	2.0	While paperswithcode predominantly represents the NLP and Computer Vision research fields in AI, we restricted ourselves just to its NLP papers.	0
1792	30312	30312	S21-11	Task Description	24	126	4.0	2.0	From their overall collection of articles, the tasks and articles in our final data were randomly selected.	0
1793	30313	30313	S21-11	Task Description	25	127	4.0	2.0	The raw articles' pdfs needed to undergo a two-step preprocessing before the annotation task.	0
1794	30314	30314	S21-11	Task Description	26	128	4.0	2.0	1) For pdf-to-text conversion, the GROBID parser (GRO, 2008(GRO, -2020 was applied; following which, 2) for plaintext pre-processing in terms of tokenization and sentence splitting, the Stanza toolkit (Qi et al., 2020) was used.	0
1795	30315	30315	S21-11	Task Description	27	129	4.0	2.0	The resulting pre-processed articles could then be annotated in plaintext format.	0
1796	30316	30316	S21-11	Task Description	28	130	4.0	2.0	Note, our data consists of articles in English.	0
1797	30477	30477	S21-12	abstract	1	2	1.0	1.0	Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision.	0
1798	30478	30478	S21-12	abstract	2	3	2.0	1.0	However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization.	0
1799	30479	30479	S21-12	abstract	3	4	3.0	1.0	The aim of the SemEval-2021 shared task on Learning with Disagreements (L --D ) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images.	1
1800	30480	30480	S21-12	abstract	4	5	4.0	1.0	In this paper we describe the shared task and its results.	0
1801	30899	30899	W18-2409	Shared Task Description	1	36	1.0	2.0	As in previous editions of the workshop series, the shared task in NEWS 2018 consists of developing machine transliteration systems in one or more of the specified language pairs.	0
1802	30900	30900	W18-2409	Shared Task Description	2	37	1.0	2.0	Each language pair of the shared task consists of a source and a target language, implicitly specifying the transliteration direction.	0
1803	30901	30901	W18-2409	Shared Task Description	3	38	1.0	2.0	Training and development data in each of the language pairs was made available to all registered participants for developing their transliteration systems.	0
1804	30902	30902	W18-2409	Shared Task Description	4	39	2.0	2.0	At the evaluation time, hand-crafted test sets of source names were released to the participants, who were required to produce a ranked list of transliteration candidates in the target language for each source name.	1
1805	30903	30903	W18-2409	Shared Task Description	5	40	2.0	2.0	The system outputs were tested against their corresponding reference sets (which may include multiple correct transliterations for some source names).	0
1806	30904	30904	W18-2409	Shared Task Description	6	41	2.0	2.0	The performance of a system is quantified using multiple metrics (defined in Section 3).	0
1807	30905	30905	W18-2409	Shared Task Description	7	42	3.0	2.0	In this edition of the workshop, only standard runs (restricted to the train and development data provided) were considered.	0
1808	30906	30906	W18-2409	Shared Task Description	8	43	3.0	2.0	No other data or linguistic resources were allowed for standard runs.	0
1809	30907	30907	W18-2409	Shared Task Description	9	44	3.0	2.0	This ensures parity between systems and enables meaningful comparison of performance of various algorithmic approaches in a given language pair.	0
1810	30908	30908	W18-2409	Shared Task Description	10	45	4.0	2.0	Participants were allowed to submit one or more standard runs for each task they participated in.	0
1811	30909	30909	W18-2409	Shared Task Description	11	46	4.0	2.0	"If more than one standard runs were submitted, it was required to select one as the ""primary"" run by publishing it into the leaderboard."	0
1812	30910	30910	W18-2409	Shared Task Description	12	47	4.0	2.0	The primary runs are the ones used to compare results across different systems.	0
1813	30911	30911	W18-2409	Shared Task Description	13	48	4.0	2.0	The NEWS 2018 Shared Task was run on Co-daLab (http://codalab.org/).	0
1814	31000	31000	W18-3219	abstract	1	2	1.0	1.0	In the third shared task of the Computational Approaches to Linguistic Code-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on code-switched social-media data.	1
1815	31001	31001	W18-3219	abstract	2	3	2.0	1.0	We divide the shared task into two competitions based on the English-Spanish (ENG-SPA) and Modern Standard Arabic-Egyptian (MSA-EGY) language pairs.	1
1816	31002	31002	W18-3219	abstract	3	4	2.0	1.0	We use Twitter data and 9 entity types to establish a new dataset for code-switched NER benchmarks.	1
1817	31003	31003	W18-3219	abstract	4	5	3.0	1.0	In addition to the CS phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process.	0
1818	31004	31004	W18-3219	abstract	5	6	4.0	1.0	As a result, the best scores of the competitions are 63.76% and 71.61% for ENG-SPA and MSA-EGY, respectively.	0
1819	31005	31005	W18-3219	abstract	6	7	4.0	1.0	We present the scores of 9 participants and discuss the most common challenges among submissions.	0
1820	31398	31398	W18-3706	abstract	1	2	1.0	1.0	This paper presents the NLPTEA 2018 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as foreign language.	1
1821	31399	31399	W18-3706	abstract	2	3	2.0	1.0	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
1822	31400	31400	W18-3706	abstract	3	4	3.0	1.0	Of the 20 teams registered for this shared task, 13 teams developed the system and submitted a total of 32 runs.	0
1823	31401	31401	W18-3706	abstract	4	5	4.0	1.0	Progress in system performances was obviously, reaching F1 of 36.12% in position level and 25.27% in correction level.	0
1824	31402	31402	W18-3706	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
1825	31666	31666	W19-3801	Task	1	22	1.0	1.0	The goal of our shared task was to encourage research in gender-fair models for NLP by providing a well-defined task that is known to be sensitive to gender bias and an evaluation procedure addressing this issue.	0
1826	31667	31667	W19-3801	Task	2	23	1.0	1.0	We chose the GAP resolution task (Webster et al., 2018), which measures the ability of systems to resolve gendered pronoun reference from real-world contexts in a gender-fair way.	1
1827	31668	31668	W19-3801	Task	3	24	2.0	1.0	Specifically, GAP asks systems to resolve a target personal pronoun to one of two names, or neither name.	0
1828	31669	31669	W19-3801	Task	4	25	2.0	1.0	For instance, a perfect resolver would resolve that she refers to Fujisawa and not to Mari Motohashi in the Wikipedia excerpt:	0
1829	31670	31670	W19-3801	Task	5	26	2.0	1.0	(1)	0
1830	31671	31671	W19-3801	Task	6	27	3.0	1.0	In May, Fujisawa joined Mari Motohashi's rink as the team's skip, moving back from Karuizawa to Kitami where she had spent her junior days.	0
1831	31672	31672	W19-3801	Task	7	28	3.0	2.0	The original GAP challenge encourages fairness by balancing its datasets by the gender of the pronoun, as well as using disaggregated evaluation with separate scores for masculine and feminine examples.	0
1832	31673	31673	W19-3801	Task	8	29	4.0	2.0	To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.	0
1833	31674	31674	W19-3801	Task	9	30	4.0	2.0	1	0
1834	31675	31675	W19-3801	Task	10	31	4.0	2.0	The competition was run on Kaggle 2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support.	0
1835	31776	31776	W19-4107	Task	1	23	1.0	1.0	This task pushed the state-of-the-art in goaloriented dialogue systems in four directions deemed necessary for practical automated agents, using two new datasets.	0
1836	31777	31777	W19-4107	Task	2	24	1.0	1.0	We sidestepped the challenge of evaluating generated utterances by formulating the problem as next utterance selection, as proposed by Lowe et al. (2015).	0
1837	31778	31778	W19-4107	Task	3	25	2.0	1.0	At test time, participants were provided with partial conversations, each paired with a set of utterances that could be the next utterance in the conversation.	1
1838	31779	31779	W19-4107	Task	4	26	2.0	1.0	Systems needed to rank these options, with the goal of placing the true utterance first.	1
1839	31780	31780	W19-4107	Task	5	27	3.0	1.0	Prior work used sets of 2 or 10 utterances.	0
1840	31781	31781	W19-4107	Task	6	28	3.0	1.0	We make the task harder by expanding the size of the sets, and considered several advanced variations: Subtask 1 100 candidates, including 1 correct option.	0
1841	31782	31782	W19-4107	Task	7	29	4.0	1.0	Subtask 2 120,000 candidates, including 1 correct option (Ubuntu data only).	0
1842	31783	31783	W19-4107	Task	8	30	4.0	1.0	Subtask 3 100 candidates, including 1-5 correct options that are paraphrases (Advising data only).	0
1843	31784	31784	W19-4107	Task	9	31	4.0	1.0	Subtask 4 100 candidates, including 0-1 correct options.	0
1844	31893	31893	W19-4226	Introduction	1	7	1.0	1.0	While producing a sentence, humans combine various types of knowledge to produce fluent outputvarious shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology.	0
1845	31894	31894	W19-4226	Introduction	2	8	1.0	1.0	Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear.	0
1846	31895	31895	W19-4226	Introduction	3	9	1.0	1.0	Automatic systems must also consider these constraints when constructing or processing language.	0
1847	31896	31896	W19-4226	Introduction	4	10	2.0	1.0	Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology.	0
1848	31897	31897	W19-4226	Introduction	5	11	2.0	1.0	Many languages implement large inflectional paradigms that mark both function and content words with a varying levels of morphosyntactic information.	0
1849	31898	31898	W19-4226	Introduction	6	12	2.0	1.0	* Now at Google For instance, Romanian verb forms inflect for person, number, tense, mood, and voice; meanwhile, Archi verbs can take on thousands of forms (Kibrik, 1998).	0
1850	31899	31899	W19-4226	Introduction	7	13	3.0	1.0	Such complex paradigms produce large inventories of words, all of which must be producible by a realistic system, even though a large percentage of them will never be observed over billions of lines of linguistic input.	0
1851	31900	31900	W19-4226	Introduction	8	14	3.0	1.0	Compounding the issue, good inflectional systems often require large amounts of supervised training data, which is infeasible in many of the world's languages.	0
1852	31901	31901	W19-4226	Introduction	9	15	3.0	1.0	This year's shared task is concentrated on encouraging the construction of strong morphological systems that perform two related but different inflectional tasks.	0
1853	31902	31902	W19-4226	Introduction	10	16	4.0	1.0	The first task asks participants to create morphological inflectors for a large number of under-resourced languages, encouraging systems that use highly-resourced, related languages as a cross-lingual training signal.	1
1854	31903	31903	W19-4226	Introduction	11	17	4.0	1.0	The second task welcomes submissions that invert this operation in light of contextual information: Given an unannotated sentence, lemmatize each word, and tag them with a morphosyntactic description.	1
1855	31904	31904	W19-4226	Introduction	12	18	4.0	1.0	Both of these tasks extend upon previous morphological competitions, and the best submitted systems now represent the state of the art in their respective tasks.	0
1856	32104	32104	W19-4622	Task Description	1	35	4.0	2.0	The MADAR Shared Task included two subtasks: the MADAR Travel Domain Dialect Identification subtask, and the MADAR Twitter User Dialect Identification subtask.	1
1857	32196	32196	W19-5302	abstract	1	2	1.0	1.0	This paper presents the results of the WMT19 Metrics Shared Task.	0
1858	32197	32197	W19-5302	abstract	2	3	2.0	1.0	Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics.	1
1859	32198	32198	W19-5302	abstract	3	4	2.0	1.0	"13 research groups submitted 24 metrics, 10 of which are reference-less ""metrics"" and constitute submissions to the joint task with WMT19 Quality Estimation Task, ""QE as a Metric""."	0
1860	32199	32199	W19-5302	abstract	4	5	3.0	1.0	In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF).	0
1861	32200	32200	W19-5302	abstract	5	6	4.0	1.0	Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality.	0
1862	32201	32201	W19-5302	abstract	6	7	4.0	1.0	This year, we use direct assessment (DA) as our only form of manual evaluation.	0
1863	32489	32489	W19-5404	Introduction	1	5	1.0	1.0	Machine Translation (MT) has experienced significant advances in recent years thanks to improvements in modeling, and in particular neural models (Bahdanau et al., 2015;	0
1864	32490	32490	W19-5404	Introduction	2	6	1.0	1.0	Gehring et al., 2016;Vaswani et al., 2017).	0
1865	32491	32491	W19-5404	Introduction	3	7	1.0	1.0	Unfortunately, today's neural machine translation models, perform poorly on low-resource language pairs, for which clean, parallel training data is high-quality training data is lacking, by definition (Koehn and Knowles, 2017).	0
1866	32492	32492	W19-5404	Introduction	4	8	1.0	1.0	Improving performance on low resource language pairs is very impactful considering that these languages are spoken by a large fraction of the world population.	0
1867	32493	32493	W19-5404	Introduction	5	9	2.0	1.0	This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base.	0
1868	32494	32494	W19-5404	Introduction	6	10	2.0	1.0	In face of the scarcity of clean parallel data, learning to translate from any multilingual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl 1 ) is an important option.	0
1869	32495	32495	W19-5404	Introduction	7	11	2.0	1.0	1 http://www.paracrawl.eu/	0
1870	32496	32496	W19-5404	Introduction	8	12	2.0	1.0	Recently, there is an increased interest in the filtering of noisy parallel corpora to increase the amount of data that can be used to train translation systems .	0
1871	32497	32497	W19-5404	Introduction	9	13	3.0	1.0	While the state-of-the-art methods that use NMT models have proven effective in mining parallel sentences (Junczys-Dowmunt, 2018) for high-resource languages, their effectiveness has not been tested in low-resource languages.	0
1872	32498	32498	W19-5404	Introduction	10	14	3.0	1.0	The implications of low availability of training data for parallel-scoring methods is not known yet.	0
1873	32499	32499	W19-5404	Introduction	11	15	3.0	1.0	The Shared Task on Parallel Corpus Filtering at the Conference for Machine Translation (WMT 2019) was organized to promote research to learning from noisy data more viable for low-resource languages.	0
1874	32500	32500	W19-5404	Introduction	12	16	3.0	1.0	Compared to last year's edition , we only provide about 50-60 million word noisy parallel data, as opposed to 1 billion words.	0
1875	32501	32501	W19-5404	Introduction	13	17	4.0	1.0	We also provide only a few million words of clean parallel data of varying quality, instead of over 100 million words of high-quality parallel data.	0
1876	32502	32502	W19-5404	Introduction	14	18	4.0	1.0	Participants developed methods to filter web-crawled Nepali-English and Sinhala-English parallel corpora by assigning a quality score for each sentence pair.	1
1877	32503	32503	W19-5404	Introduction	15	19	4.0	1.0	These scores are used to filter the web crawled corpora down to fixed sizes (1 million and 5 million English words), trained statistical and neural machine translation systems on these subsets, and measured their quality with the BLEU score on a test set of multi-domain Wikipedia content .	0
1878	32504	32504	W19-5404	Introduction	16	20	4.0	1.0	This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes and the average sentence length of sub-selected data.	0
1879	33100	33100	2020.sigmorphon-1.2	abstract	1	2	1.0	1.0	We describe the design and findings of the SIGMORPHON 2020 shared task on multilingual grapheme-to-phoneme conversion.	0
1880	33101	33101	2020.sigmorphon-1.2	abstract	2	3	2.0	1.0	Participants were asked to submit systems which consume a sequence of graphemes then emit output a sequence of phonemes representing the pronunciation of that grapheme sequence in one of fifteen languages.	1
1881	33102	33102	2020.sigmorphon-1.2	abstract	3	4	3.0	1.0	Nine teams submitted a total of 23 systems, at best achieving an 18% relative reduction in word error rate (macro-averaged over languages), versus strong neural sequence-to-sequence baselines.	0
1882	33103	33103	2020.sigmorphon-1.2	abstract	4	5	4.0	1.0	To facilitate error analysis, we publicly release the complete outputs for all systems-a first for the SIGMORPHON workshop.	0
1883	33293	33293	2020.sigmorphon-1.3	abstract	1	2	1.0	1.0	In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology.	0
1884	33294	33294	2020.sigmorphon-1.3	abstract	2	3	1.0	1.0	Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma.	1
1885	33295	33295	2020.sigmorphon-1.3	abstract	3	4	2.0	1.0	In order to simulate a realistic use case, we first released data for 5 development languages.	0
1886	33296	33296	2020.sigmorphon-1.3	abstract	4	5	2.0	1.0	However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline.	0
1887	33297	33297	2020.sigmorphon-1.3	abstract	5	6	3.0	1.0	We provided a modular baseline system, which is a pipeline of 4 components.	0
1888	33298	33298	2020.sigmorphon-1.3	abstract	6	7	3.0	1.0	3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages.	0
1889	33299	33299	2020.sigmorphon-1.3	abstract	7	8	4.0	1.0	Only on 3 languages did a submitted system obtain the best results.	0
1890	33300	33300	2020.sigmorphon-1.3	abstract	8	9	4.0	1.0	This shows that unsupervised morphological paradigm completion is still largely unsolved.	0
1891	33301	33301	2020.sigmorphon-1.3	abstract	9	10	4.0	1.0	We present an analysis here, so that this shared task will ground further research on the topic.	0
1892	33525	33525	2021.case-1.23	abstract	1	2	1.0	1.0	This paper describes the Shared Task on Fine-grained Event Classification in News-like Text Snippets.	0
1893	33526	33526	2021.case-1.23	abstract	2	3	2.0	1.0	The Shared Task is divided into three subtasks: (a) classification of text snippets reporting socio-political events (25 classes) for which vast amount of training data exists, although exhibiting different structure and style vis-a-vis test data, (b) enhancement to a generalized zero-shot learning problem, where 3 additional event types were introduced in advance, but without any training data ('unseen' classes), and (c) further extension, which introduced 2 additional event types, announced shortly prior to the evaluation phase.	1
1894	33527	33527	2021.case-1.23	abstract	3	4	3.0	1.0	The reported Shared Task focuses on classification of events in English texts and is organized as part of the Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), co-located with the ACL-IJCNLP 2021 Conference.	0
1895	33528	33528	2021.case-1.23	abstract	4	5	4.0	1.0	Four teams participated in the task.	0
1896	33529	33529	2021.case-1.23	abstract	5	6	4.0	1.0	Best performing systems for the three aforementioned subtasks achieved 83.9%, 79.7% and 77.1% weighted F 1 scores respectively.	0
1897	33718	33718	2021.dialdoc-1.1	abstract	1	3	1.0	1.0	We present the results of Shared Task at Workshop Dial	0
1898	33719	33719	2021.dialdoc-1.1	abstract	2	4	2.0	1.0	Doc 2021 that is focused on document-grounded dialogue and conversational question answering.	0
1899	33720	33720	2021.dialdoc-1.1	abstract	3	5	2.0	1.0	The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that can identify the most relevant knowledge in the associated document for generating agent responses in natural language.	1
1900	33721	33721	2021.dialdoc-1.1	abstract	4	6	3.0	1.0	It includes two subtasks on predicting agent responses: the first subtask is to predict the grounding text span in the given document for next agent response; the second subtask is to generate agent response in natural language given the context.	1
1901	33722	33722	2021.dialdoc-1.1	abstract	5	7	3.0	1.0	Many submissions outperform baseline significantly.	0
1902	33723	33723	2021.dialdoc-1.1	abstract	6	8	4.0	1.0	For the first task, the best-performing system achieved 67.1 Exact Match and 76.3 F1.	0
1903	33724	33724	2021.dialdoc-1.1	abstract	7	9	4.0	1.0	For the second subtask, the best system achieved 41.1 SacreBLEU and highest rank by human evaluation.	0
1904	33880	33880	2021.sigmorphon-1.13	Task definition	1	46	4.0	1.0	In this task, participants were provided with a collection of words and their pronunciations, and then scored on their ability to predict the pronunciation of a set of unseen words.	1
1905	34029	34029	2021.unimplicit-1.4	Introduction	1	5	1.0	1.0	The goal of this shared task is to evaluate the ability of NLP systems to detect whether a sentence from an instructional text requires clarification.	0
1906	34030	34030	2021.unimplicit-1.4	Introduction	2	6	1.0	1.0	Such clarifications can be critical to ensure that instructions are clear enough to be followed and the desired goal can be reached.	0
1907	34031	34031	2021.unimplicit-1.4	Introduction	3	7	2.0	1.0	We set up this task as a binary classification task, in which systems have to predict whether a given sentence in context requires clarification.	1
1908	34032	34032	2021.unimplicit-1.4	Introduction	4	8	2.0	1.0	Our data is based on texts for which revision histories exist, making it possible to identify (a) sentences that received edits which made the sentence more precise, and (b) sentences that remained unchanged over multiple text revisions.	0
1909	34033	34033	2021.unimplicit-1.4	Introduction	5	9	3.0	1.0	The task of predicting revision requirements in instructional texts was originally proposed by Bhat et al. (2020), who attempted to predict whether a given sentence will be edited according to an article's revision history.	0
1910	34034	34034	2021.unimplicit-1.4	Introduction	6	10	3.0	1.0	The shared task follows this setup, with two critical differences:	0
1911	34035	34035	2021.unimplicit-1.4	Introduction	7	11	4.0	1.0	First, we apply a set of rules to identify a subset of edits that provide clarifying information.	0
1912	34036	34036	2021.unimplicit-1.4	Introduction	8	12	4.0	1.0	This makes it possible to focus mainly on those edits that are related to implicit and underspecified language, excluding grammar corrections and other edit types.	0
1913	34037	34037	2021.unimplicit-1.4	Introduction	9	13	4.0	1.0	Since the need for such edits may depend on discourse context, a second difference is that we provide context for each sentence to be classified (see Table 1).	0
1914	34135	34135	W00-0726	abstract	1	2	2.0	1.0	We describe the CoNLL-2000 shared task: dividing text into syntactically related nonoverlapping groups of words, so-called text chunking.	1
1915	34136	34136	W00-0726	abstract	2	3	4.0	1.0	We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.	0
1916	34264	34264	W01-0708	Task description	1	12	1.0	1.0	Defining clause boundaries is not trivial (Leffa, 1998).	0
1917	34265	34265	W01-0708	Task description	2	13	1.0	1.0	In this task, the gold standard clause segmentation is provided by the Penn Treebank (Marcus et al., 1993).	0
1918	34266	34266	W01-0708	Task description	3	14	1.0	1.0	The guidelines of the Penn Treebank describe in detail how sentences are segmented into clauses (Bies et al., 1995).	0
1919	34267	34267	W01-0708	Task description	4	15	2.0	1.0	Here is an example of a sentence and its clauses obtained from Wall Street Journal section 15 of the Penn Treebank (Marcus et al., 1993):	0
1920	34268	34268	W01-0708	Task description	5	16	2.0	1.0	(S Coach them in (S-NOM handling complaints) (SBAR-PRP so that (S they can resolve problems immediately) )	0
1921	34269	34269	W01-0708	Task description	6	17	2.0	1.0	. )	0
1922	34270	34270	W01-0708	Task description	7	18	3.0	1.0	The clauses of this sentence have been enclosed between brackets.	0
1923	34271	34271	W01-0708	Task description	8	19	3.0	1.0	A tag next to the open bracket denotes the type of the clause.	0
1924	34272	34272	W01-0708	Task description	9	20	3.0	1.0	In the CoNLL-2001 shared task, the goal is to identify clauses in text.	0
1925	34273	34273	W01-0708	Task description	10	21	4.0	1.0	Since clauses can be embedded in each other, this task is considerably more difficult than last year's task, recognizing non-embedded text chunks.	0
1926	34274	34274	W01-0708	Task description	11	22	4.0	1.0	For that reason, we have disregarded type and function information of the clauses: every clause has been tagged with S rather than with an elaborate tag such as SBAR-PRP.	0
1927	34275	34275	W01-0708	Task description	12	23	4.0	1.0	Furthermore, the shared task has been divided in three parts: identifying clause starts, recognizing clause ends and finding complete clauses.	1
1928	34276	34276	W01-0708	Task description	13	24	4.0	1.0	The results obtained for the first two parts can be used in the third part of the task.	0
1929	34360	34360	W02-2024	Introduction	1	4	1.0	1.0	Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.	0
1930	34361	34361	W02-2024	Introduction	2	5	1.0	1.0	Example:	0
1931	34362	34362	W02-2024	Introduction	3	6	2.0	1.0	PER	0
1932	34363	34363	W02-2024	Introduction	4	7	2.0	1.0	Wol ] , currently a journalist in LOC Argentina ] , played with PER Del Bosque ] in the nal years of the seventies in ORG Real Madrid ] .	0
1933	34364	34364	W02-2024	Introduction	5	8	2.0	1.0	This sentence contains four named entities:	0
1934	34365	34365	W02-2024	Introduction	6	9	3.0	1.0	Wol and Del Bosque are persons, Argentina is a location and Real Madrid is a organization.	0
1935	34366	34366	W02-2024	Introduction	7	10	3.0	1.0	The shared task of CoNLL-2002 concerns language-independent named entity recognition.	0
1936	34367	34367	W02-2024	Introduction	8	11	3.0	1.0	We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.	0
1937	34368	34368	W02-2024	Introduction	9	12	4.0	1.0	The participants of the shared task have been o ered training and test data for two European languages: Spanish and Dutch.	1
1938	34369	34369	W02-2024	Introduction	10	13	4.0	1.0	They have used the data for developing a named-entity recognition system that includes a machine learning component.	1
1939	34370	34370	W02-2024	Introduction	11	14	4.0	1.0	The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.	0
1940	34449	34449	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	1	4	1.0	1.0	Named entity recognition is an important task of information extraction systems.	0
1941	34450	34450	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	2	5	1.0	1.0	There has been a lot of work on named entity recognition, especially for English (see Borthwick (1999) for an overview).	0
1942	34451	34451	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	3	6	2.0	1.0	The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition.	0
1943	34452	34452	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	4	7	2.0	1.0	They have also produced a scheme for entity annotation (Chinchor et al., 1999).	0
1944	34453	34453	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	5	8	2.0	1.0	More recently, there have been other system development competitions which dealt with different languages (IREX and CoNLL-2002).	0
1945	34454	34454	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	6	9	3.0	1.0	The shared task of CoNLL-2003 concerns language-independent named entity recognition.	0
1946	34455	34455	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	7	10	3.0	1.0	We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.	0
1947	34456	34456	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	8	11	3.0	1.0	The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).	0
1948	34457	34457	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	9	12	4.0	1.0	The participants of the 2003 shared task have been offered training and test data for two other European languages: English and German.	1
1949	34458	34458	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	10	13	4.0	1.0	They have used the data for developing a named-entity recognition system that includes a machine learning component.	1
1950	34459	34459	W03-0419	This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.	11	14	4.0	1.0	The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data.	0
1951	34579	34579	W04-2412	Introduction	1	4	1.0	1.0	In recent years there has been an increasing interest in semantic parsing of natural language, which is becoming a key issue in Information Extraction, Question Answering, Summarization, and, in general, in all NLP applications requiring some kind of semantic interpretation.	0
1952	34580	34580	W04-2412	Introduction	2	5	1.0	1.0	The shared task of CoNLL-2004 1 concerns the recognition of semantic roles, for the English language.	0
1953	34581	34581	W04-2412	Introduction	3	6	1.0	1.0	We will refer to it as Semantic Role Labeling (SRL).	0
1954	34582	34582	W04-2412	Introduction	4	7	1.0	1.0	Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.	1
1955	34583	34583	W04-2412	Introduction	5	8	1.0	1.0	In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be extracted (see Figure 1 for a detailed example).	1
1956	34584	34584	W04-2412	Introduction	6	9	1.0	1.0	Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.	0
1957	34585	34585	W04-2412	Introduction	7	10	1.0	1.0	Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels.	0
1958	34586	34586	W04-2412	Introduction	8	11	2.0	1.0	Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments.	0
1959	34587	34587	W04-2412	Introduction	9	12	2.0	1.0	Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002;	0
1960	34588	34588	W04-2412	Introduction	10	13	2.0	1.0	Gildea and Palmer, 2002;Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003;	0
1961	34589	34589	W04-2412	Introduction	11	14	2.0	1.0	Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003;	0
1962	34590	34590	W04-2412	Introduction	12	15	2.0	1.0	Pradhan et al., 2003a;Pradhan et al., 2003b).	0
1963	34591	34591	W04-2412	Introduction	13	16	2.0	1.0	There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.	0
1964	34592	34592	W04-2412	Introduction	14	17	2.0	1.0	For instance, in (Pradhan et al., 2003a;	0
1965	34593	34593	W04-2412	Introduction	15	18	2.0	1.0	Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks.	0
1966	34594	34594	W04-2412	Introduction	16	19	3.0	1.0	Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001).	0
1967	34595	34595	W04-2412	Introduction	17	20	3.0	1.0	In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate-argument structures.	0
1968	34596	34596	W04-2412	Introduction	18	21	3.0	1.0	It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates.	0
1969	34597	34597	W04-2412	Introduction	19	22	3.0	1.0	A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations.	0
1970	34598	34598	W04-2412	Introduction	20	23	3.0	1.0	To date, the best results reported on the PropBank correspond to a F 1 measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information (Pradhan et al., 2003b).	0
1971	34599	34599	W04-2412	Introduction	21	24	3.0	1.0	This performance drops to 77 when a real parser is used instead.	0
1972	34600	34600	W04-2412	Introduction	22	25	3.0	1.0	Comparatively, the best SRL system based solely on shallow syntactic information (Pradhan et al., 2003a) performs more than 15 points below.	0
1973	34601	34601	W04-2412	Introduction	23	26	3.0	1.0	Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task.	0
1974	34602	34602	W04-2412	Introduction	24	27	4.0	1.0	The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases.	0
1975	34603	34603	W04-2412	Introduction	25	28	4.0	1.0	The annotations provided for the development of systems include, apart from the argument boundaries and role labels, the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.	0
1976	34604	34604	W04-2412	Introduction	26	29	4.0	1.0	The rest of the paper is organized as follows.	0
1977	34605	34605	W04-2412	Introduction	27	30	4.0	1.0	Section 2 describes the general setting of the task.	0
1978	34606	34606	W04-2412	Introduction	28	31	4.0	1.0	Section 3 provides a detailed description of training, development and test data.	0
1979	34607	34607	W04-2412	Introduction	29	32	4.0	1.0	Participant systems are described and compared in section 4.	0
1980	34608	34608	W04-2412	Introduction	30	33	4.0	1.0	In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.	0
1981	34609	34609	W04-2412	Introduction	31	34	4.0	1.0	Finally, section 5 concludes.	0
1982	34826	34826	W05-0620	Introduction	1	4	1.0	1.0	In the few last years there has been an increasing interest in shallow semantic parsing of natural language, which is becoming an important component in all kind of NLP applications.	0
1983	34827	34827	W05-0620	Introduction	2	5	1.0	1.0	As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and comparative evaluation.	0
1984	34828	34828	W05-0620	Introduction	3	6	1.0	1.0	Given a sentence, the task consists of analyzing the propositions expressed by some target verbs of the sentence.	1
1985	34829	34829	W05-0620	Introduction	4	7	1.0	1.0	In particular, for each target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized.	1
1986	34830	34830	W05-0620	Introduction	5	8	1.0	1.0	Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc.	0
1987	34831	34831	W05-0620	Introduction	6	9	1.0	1.0	Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information.	0
1988	34832	34832	W05-0620	Introduction	7	10	1.0	1.0	In (Carreras and Mrquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004.	0
1989	34833	34833	W05-0620	Introduction	8	11	1.0	1.0	Ten systems contributed to the task, which was evaluated using the PropBank corpus .	0
1990	34834	34834	W05-0620	Introduction	9	12	2.0	1.0	The best results were around 70 in F 1 measure.	0
1991	34835	34835	W05-0620	Introduction	10	13	2.0	1.0	Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F 1 slightly over 79).	0
1992	34836	34836	W05-0620	Introduction	11	14	2.0	1.0	In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004).	0
1993	34837	34837	W05-0620	Introduction	12	15	2.0	1.0	Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001).	0
1994	34838	34838	W05-0620	Introduction	13	16	2.0	1.0	From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004;	0
1995	34839	34839	W05-0620	Introduction	14	17	2.0	1.0	Moschitti, 2004;Xue and Palmer, 2004;Pradhan et al., 2005a).	0
1996	34840	34840	W05-0620	Introduction	15	18	2.0	1.0	Following last year's initiative, the CoNLL-2005 shared task 1 will concern again the recognition of semantic roles for the English language.	0
1997	34841	34841	W05-0620	Introduction	16	19	2.0	1.0	Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are:	0
1998	34842	34842	W05-0620	Introduction	17	20	2.0	1.0		0
1999	34843	34843	W05-0620	Introduction	18	21	3.0	1.0	Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task.	0
2000	34844	34844	W05-0620	Introduction	19	22	3.0	1.0	The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks, clauses, and named entities.	0
2001	34845	34845	W05-0620	Introduction	20	23	3.0	1.0		0
2002	34846	34846	W05-0620	Introduction	21	24	3.0	1.0	The training corpus has been substantially enlarged.	0
2003	34847	34847	W05-0620	Introduction	22	25	3.0	1.0	This allows to test the scalability of learning-based SRL systems to big datasets and to compute learning curves to see how much data is necessary to train.	0
2004	34848	34848	W05-0620	Introduction	23	26	3.0	1.0	Again, we concentrate on the PropBank corpus , which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate-argument structures.	0
2005	34849	34849	W05-0620	Introduction	24	27	3.0	1.0		0
2006	34850	34850	W05-0620	Introduction	25	28	3.0	1.0	In order to test the robustness of the presented systems, a cross-corpora evaluation is performed using a fresh test set from the Brown corpus.	0
2007	34851	34851	W05-0620	Introduction	26	29	4.0	1.0	Regarding evaluation, two different settings were devised depending if the systems use the information strictly contained in the training data (closed challenge) or they make use of external sources of information and/or tools (open challenge).	0
2008	34852	34852	W05-0620	Introduction	27	30	4.0	1.0	The closed setting allows to compare systems under strict conditions, while the open setting aimed at exploring the contributions of other sources of information and the limits of the current learning-based systems on the SRL task.	0
2009	34853	34853	W05-0620	Introduction	28	31	4.0	1.0	At the end, all 19 systems took part in the closed challenge and none of them in the open challenge.	0
2010	34854	34854	W05-0620	Introduction	29	32	4.0	1.0	The rest of the paper is organized as follows.	0
2011	34855	34855	W05-0620	Introduction	30	33	4.0	1.0	Section 2 describes the general setting of the task.	0
2012	34856	34856	W05-0620	Introduction	31	34	4.0	1.0	Section 3 provides a detailed description of training, development and test data.	0
2013	34857	34857	W05-0620	Introduction	32	35	4.0	1.0	Participant systems are described and compared in section 4.	0
2014	34858	34858	W05-0620	Introduction	33	36	4.0	1.0	In particular, information about learning techniques, SRL strategies, and feature development is provided, together with performance results on the development and test sets.	0
2015	34859	34859	W05-0620	Introduction	34	37	4.0	1.0	Finally, section 5 concludes.	0
2016	35128	35128	D07-1096	Introduction	1	6	1.0	1.0	Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999,2000), clause identification (2001), named entity recognition (2002,2003), and semantic role labeling (2004,2005).	0
2017	35129	35129	D07-1096	Introduction	2	7	1.0	1.0	In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006).	0
2018	35130	35130	D07-1096	Introduction	3	8	2.0	1.0	In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence.	1
2019	35131	35131	D07-1096	Introduction	4	9	2.0	1.0	This defines a dependency graph, where the nodes are the words of the input sentence and the arcs are the binary relations from head to dependent.	0
2020	35132	35132	D07-1096	Introduction	5	10	2.0	1.0	Often, but not always, it is assumed that all words except one have a syntactic head, which means that the graph will be a tree with the single independent word as the root.	0
2021	35133	35133	D07-1096	Introduction	6	11	3.0	1.0	In labeled dependency parsing, we additionally require the parser to assign a specific type (or label) to each dependency relation holding between a head word and a dependent word.	0
2022	35134	35134	D07-1096	Introduction	7	12	3.0	1.0	In this year's shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation.	0
2023	35135	35135	D07-1096	Introduction	8	13	3.0	1.0	The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where the task was to use machine learning to adapt a parser for a single language to a new domain.	0
2024	35136	35136	D07-1096	Introduction	9	14	4.0	1.0	In total, test results were submitted for twenty-three systems in the multilingual track, and ten systems in the domain adaptation track (six of which also participated in the multilingual track).	0
2025	35137	35137	D07-1096	Introduction	10	15	4.0	1.0	Not everyone submitted papers describing their system, and some papers describe more than one system (or the same system in both tracks), which explains why there are only (!) twenty-one papers in the proceedings.	0
2026	35138	35138	D07-1096	Introduction	11	16	4.0	1.0	In this paper, we provide task definitions for the two tracks (section 2), describe data sets extracted from available treebanks (section 3), report results for all systems in both tracks (section 4), give an overview of approaches used (section 5), provide a first analysis of the results (section 6), and conclude with some future directions (section 7).	0
2027	35516	35516	W08-2121	Introduction	1	7	1.0	1.0	In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English).	0
2028	35517	35517	W08-2121	Introduction	2	8	1.0	1.0	In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.	0
2029	35518	35518	W08-2121	Introduction	3	9	1.0	1.0	The CoNLL-2008 shared task 1 proposes a unified dependency-based c 2008.	0
2030	35519	35519	W08-2121	Introduction	4	10	1.0	1.0	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).	0
2031	35520	35520	W08-2121	Introduction	5	11	1.0	1.0	Some rights reserved.	0
2032	35521	35521	W08-2121	Introduction	6	12	1.0	1.0	1 http://www.yr-bcn.es/conll2008	0
2033	35522	35522	W08-2121	Introduction	7	13	2.0	1.0	formalism, which models both syntactic dependencies and semantic roles.	0
2034	35523	35523	W08-2121	Introduction	8	14	2.0	1.0	Using this formalism, this shared task merges both the task of syntactic dependency parsing and the task of identifying semantic arguments and labeling them with semantic roles.	0
2035	35524	35524	W08-2121	Introduction	9	15	2.0	1.0	Conceptually, the 2008 shared task can be divided into three subtasks: (i) parsing of syntactic dependencies, (ii) identification and disambiguation of semantic predicates, and (iii) identification of arguments and assignment of semantic roles for each predicate.	1
2036	35525	35525	W08-2121	Introduction	10	16	2.0	1.0	Several objectives were addressed in this shared task:	0
2037	35526	35526	W08-2121	Introduction	11	17	2.0	1.0	 SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies.	0
2038	35527	35527	W08-2121	Introduction	12	18	2.0	1.0	While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates.	0
2039	35528	35528	W08-2121	Introduction	13	19	2.0	1.0	 Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks.	0
2040	35529	35529	W08-2121	Introduction	14	20	3.0	1.0	For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations.	0
2041	35530	35530	W08-2121	Introduction	15	21	3.0	1.0		0
2042	35531	35531	W08-2121	Introduction	16	22	3.0	1.0	A practical framework is provided for the joint learning of syntactic and semantic dependencies.	0
2043	35532	35532	W08-2121	Introduction	17	23	3.0	1.0	Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly setting.	0
2044	35533	35533	W08-2121	Introduction	18	24	3.0	1.0	The evaluation is separated into two different challenges: a closed challenge, where systems have to be trained strictly with information contained in the given training corpus, and an open challenge, where systems can be developed making use of any kind of external tools and resources.	0
2045	35534	35534	W08-2121	Introduction	19	25	3.0	1.0	The participants could submit results in either one or both challenges.	0
2046	35535	35535	W08-2121	Introduction	20	26	4.0	1.0	This paper is organized as follows.	0
2047	35536	35536	W08-2121	Introduction	21	27	4.0	1.0	Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges.	0
2048	35537	35537	W08-2121	Introduction	22	28	4.0	1.0	Section 3 introduces the corpora used and our constituent-to-dependency conversion procedure.	0
2049	35538	35538	W08-2121	Introduction	23	29	4.0	1.0	Section 4 summarizes the results of the submitted systems.	0
2050	35539	35539	W08-2121	Introduction	24	30	4.0	1.0	Section 5 discusses the approaches implemented by participants.	0
2051	35540	35540	W08-2121	Introduction	25	31	4.0	1.0	Section 6 analyzes the results using additional non-official evaluation measures.	0
2052	35541	35541	W08-2121	Introduction	26	32	4.0	1.0	Section 7 concludes the paper.	0
2053	35863	35863	W09-1201	Introduction	1	6	1.0	1.0	"Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) launches a competitive, open ""Shared Task""."	0
2054	35864	35864	W09-1201	Introduction	2	7	1.0	1.0	"A common (""shared"") task is defined and datasets are provided for its participants."	0
2055	35865	35865	W09-1201	Introduction	3	8	1.0	1.0	In 2004 and 2005, the shared tasks were dedicated to semantic role labeling (SRL) in a monolingual setting (English).	0
2056	35866	35866	W09-1201	Introduction	4	9	1.0	1.0	In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.	0
2057	35867	35867	W09-1201	Introduction	5	10	1.0	1.0	In 2008, the shared task (Surdeanu et al., 2008) used a unified dependencybased formalism, which modeled both syntactic dependencies and semantic roles for English.	0
2058	35868	35868	W09-1201	Introduction	6	11	1.0	1.0	The CoNLL-2009 Shared Task has built on the 2008 results by providing data for six more languages (Catalan, Chinese, Czech, German, Japanese and Spanish) in addition to the original English 1 .	0
2059	35869	35869	W09-1201	Introduction	7	12	2.0	1.0	It has thus naturally extended the path taken by the five most recent CoNLL shared tasks.	0
2060	35870	35870	W09-1201	Introduction	8	13	2.0	1.0	As in 2008, the CoNLL-2009 shared task combined dependency parsing and the task of identifying and labeling semantic arguments of verbs (and other parts of speech whenever available).	1
2061	35871	35871	W09-1201	Introduction	9	14	2.0	1.0	Participants had to choose from two tasks:	0
2062	35872	35872	W09-1201	Introduction	10	15	2.0	1.0		0
2063	35873	35873	W09-1201	Introduction	11	16	2.0	1.0	Joint task (syntactic dependency parsing and semantic role labeling), or	0
2064	35874	35874	W09-1201	Introduction	12	17	2.0	1.0	 SRL-only task (syntactic dependency parses have been provided by the organizers, using state-of-the art parsers for the individual languages).	0
2065	35875	35875	W09-1201	Introduction	13	18	3.0	1.0	In contrast to the previous year, the evaluation data indicated which words were to be dealt with (for the SRL task).	0
2066	35876	35876	W09-1201	Introduction	14	19	3.0	1.0	In other words, (predicate) disambiguation was still part of the task, whereas the identification of argument-bearing words was not.	0
2067	35877	35877	W09-1201	Introduction	15	20	3.0	1.0	This decision was made to compensate for the significant differences between languages and between the annotation schemes used.	0
2068	35878	35878	W09-1201	Introduction	16	21	3.0	1.0	"The ""closed"" and ""open"" challenges have been kept from last year as well; participants could have chosen one or both."	0
2069	35879	35879	W09-1201	Introduction	17	22	3.0	1.0	In the closed challenge, systems had to be trained strictly with information contained in the given training corpus; in the open challenge, systems could have been developed making use of any kind of external tools and resources.	0
2070	35880	35880	W09-1201	Introduction	18	23	3.0	1.0	This paper is organized as follows.	0
2071	35881	35881	W09-1201	Introduction	19	24	4.0	1.0	Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges.	0
2072	35882	35882	W09-1201	Introduction	20	25	4.0	1.0	A substantial portion of the paper (Section 3) is devoted to the description of the conversion and development of the data sets in the additional languages.	0
2073	35883	35883	W09-1201	Introduction	21	26	4.0	1.0	Section 4 shows the main results of the submitted systems in the Joint and SRL-only tasks.	0
2074	35884	35884	W09-1201	Introduction	22	27	4.0	1.0	Section 5 summarizes the approaches implemented by participants.	0
2075	35885	35885	W09-1201	Introduction	23	28	4.0	1.0	Section 6 concludes the paper.	0
2076	35886	35886	W09-1201	Introduction	24	29	4.0	1.0	In all sections, we will mention some of the differences between last year's and this year's tasks while keeping the text self-contained whenever possible; for details and observations on the English data, please refer to the overview paper of the CoNLL-2008 Shared Task (Surdeanu et al., 2008) and to the references mentioned in the sections describing the other languages.	0
2077	36304	36304	W10-3001	Task Definitions	1	105	4.0	2.0	Two uncertainty detection tasks (sentence classification and in-sentence hedge scope detection) in two domains (biological publications and Wikipedia articles) with three types of submissions (closed, cross and open) were given to the participants of the CoNLL-2010 Shared Task.	1
2078	36604	36604	W11-1901	Task Description	1	183	1.0	2.0	The CoNLL-2011 shared task was based on the English portion of the OntoNotes 4.0 data.	1
2079	36605	36605	W11-1901	Task Description	2	184	2.0	2.0	The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains.	1
2080	36606	36606	W11-1901	Task Description	3	185	3.0	2.0	The target coreference decisions could be made using automatically predicted information on the other structural layers including the parses, semantic roles, word senses, and named entities.	0
2081	36607	36607	W11-1901	Task Description	4	186	4.0	2.0	As is customary for CoNLL tasks, there were two tracks, closed and open.	0
2082	36608	36608	W11-1901	Task Description	5	187	4.0	2.0	For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data.	0
2083	37215	37215	W12-4501	CoNLL-2012 Coreference Task	1	284	1.0	2.0	The CoNLL-2012 shared task was held across all three languages -English, Chinese and Arabicof the OntoNotes v5.0 data.	1
2084	37216	37216	W12-4501	CoNLL-2012 Coreference Task	2	285	1.0	2.0	The task was to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains.	1
2085	37217	37217	W12-4501	CoNLL-2012 Coreference Task	3	286	1.0	2.0	The coreference decisions had to be made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities.	0
2086	37218	37218	W12-4501	CoNLL-2012 Coreference Task	4	287	1.0	2.0	Given various factors, such as the lack of resources and state-of-the-art tools, and time constraints, we could not provide some layers of information for the Chinese and Arabic portion of the data.	0
2087	37219	37219	W12-4501	CoNLL-2012 Coreference Task	5	288	2.0	2.0	The three languages are from quite different language families.	0
2088	37220	37220	W12-4501	CoNLL-2012 Coreference Task	6	289	2.0	2.0	The morphology of these languages is quite different.	0
2089	37221	37221	W12-4501	CoNLL-2012 Coreference Task	7	290	2.0	2.0	Arabic has a complex morphology, English has limited morphology, whereas Chinese has very little morphology.	0
2090	37222	37222	W12-4501	CoNLL-2012 Coreference Task	8	291	2.0	2.0	English word segmentation amounts to rule-based tokenization, and is close to perfect.	0
2091	37223	37223	W12-4501	CoNLL-2012 Coreference Task	9	292	3.0	2.0	In the case of Chinese and Arabic, although the tokenization/segmentation is not as good as English, the accuracies are in the high 90s.	0
2092	37224	37224	W12-4501	CoNLL-2012 Coreference Task	10	293	3.0	2.0	Syntactically, there are many dropped subjects and objects in Arabic and Chinese, whereas English is not a pro-drop language.	0
2093	37225	37225	W12-4501	CoNLL-2012 Coreference Task	11	294	3.0	2.0	Another difference is the amount of resources available for each language.	0
2094	37226	37226	W12-4501	CoNLL-2012 Coreference Task	12	295	3.0	2.0	English has probably the most resources at its disposal, whereas Chinese and Arabic lack significantly -Arabic more so than Chinese.	0
2095	37227	37227	W12-4501	CoNLL-2012 Coreference Task	13	296	4.0	2.0	Given this fact, plus the fact that the CoNLL format cannot handle multiple segmentations, and that it would complicate scoring since we are using exact token boundaries (as discussed later in Section 4.5), we decided to allow the use of gold, treebank segmentation for all languages.	0
2096	37228	37228	W12-4501	CoNLL-2012 Coreference Task	14	297	4.0	2.0	In the case of Chinese, the words themselves are lemmas, so no additional information needs to be provided.	0
2097	37229	37229	W12-4501	CoNLL-2012 Coreference Task	15	298	4.0	2.0	For Arabic, by default written text is unvocalised, so we decided to also provide correct, gold standard lemmas, along with the correct vocalized version of the tokens.	0
2098	37230	37230	W12-4501	CoNLL-2012 Coreference Task	16	299	4.0	2.0	Table 2 lists which layers were available and quality of the provided layers (when provided.)	0
2099	37231	37231	W12-4501	CoNLL-2012 Coreference Task	17	300	4.0	2.0	 	0
2100	37769	37769	W13-3601	Introduction	1	5	1.0	1.0	Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL-2013).	0
2101	37770	37770	W13-3601	Introduction	2	6	1.0	1.0	In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay.	1
2102	37771	37771	W13-3601	Introduction	3	7	1.0	1.0	This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011;	0
2103	37772	37772	W13-3601	Introduction	4	8	2.0	1.0	Dale et al., 2012).	0
2104	37773	37773	W13-3601	Introduction	5	9	2.0	1.0	In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.	0
2105	37774	37774	W13-3601	Introduction	6	10	2.0	1.0	This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.	0
2106	37775	37775	W13-3601	Introduction	7	11	2.0	1.0	Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.	0
2107	37776	37776	W13-3601	Introduction	8	12	3.0	1.0	The CoNLL-2013 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.	0
2108	37777	37777	W13-3601	Introduction	9	13	3.0	1.0	This overview paper contains a detailed description of the shared task, and is organized as follows.	0
2109	37778	37778	W13-3601	Introduction	10	14	3.0	1.0	Section 2 provides the task definition.	0
2110	37779	37779	W13-3601	Introduction	11	15	3.0	1.0	Section 3 describes the annotated training data provided and the blind test data.	0
2111	37780	37780	W13-3601	Introduction	12	16	4.0	1.0	Section 4 describes the evaluation metric and the scorer.	0
2112	37781	37781	W13-3601	Introduction	13	17	4.0	1.0	Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.	0
2113	37782	37782	W13-3601	Introduction	14	18	4.0	1.0	Section 6 presents the results of the shared task.	0
2114	37783	37783	W13-3601	Introduction	15	19	4.0	1.0	Section 7 concludes the paper.	0
2115	37960	37960	W14-1701	Introduction	1	8	1.0	1.0	Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014(CoNLL-2014.	0
2116	37961	37961	W14-1701	Introduction	2	9	1.0	1.0	In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay.	1
2117	37962	37962	W14-1701	Introduction	3	10	1.0	1.0	This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011;	0
2118	37963	37963	W14-1701	Introduction	4	11	2.0	1.0	Dale et al., 2012), and a CoNLL shared task on grammatical error correction organized in 2013 .	0
2119	37964	37964	W14-1701	Introduction	5	12	2.0	1.0	In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application.	0
2120	37965	37965	W14-1701	Introduction	6	13	2.0	1.0	This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed.	0
2121	37966	37966	W14-1701	Introduction	7	14	2.0	1.0	Also, tackling this task has farreaching impact, since it is estimated that hundreds of millions of people worldwide are learning English and they benefit directly from an automated grammar checker.	0
2122	37967	37967	W14-1701	Introduction	8	15	3.0	1.0	The CoNLL-2014 shared task provides a forum for participating teams to work on the same grammatical error correction task, with evaluation on the same blind test set using the same evaluation metric and scorer.	0
2123	37968	37968	W14-1701	Introduction	9	16	3.0	1.0	This overview paper contains a detailed description of the shared task, and is organized as follows.	0
2124	37969	37969	W14-1701	Introduction	10	17	3.0	1.0	Section 2 provides the task definition.	0
2125	37970	37970	W14-1701	Introduction	11	18	3.0	1.0	Section 3 describes the annotated training data provided and the blind test data.	0
2126	37971	37971	W14-1701	Introduction	12	19	4.0	1.0	Section 4 describes the evaluation metric and the scorer.	0
2127	37972	37972	W14-1701	Introduction	13	20	4.0	1.0	Section 5 lists the participating teams and outlines the approaches to grammatical error correction used by the teams.	0
2128	37973	37973	W14-1701	Introduction	14	21	4.0	1.0	Section 6 presents the results of the shared task, including a discussion on cross annotator comparison.	0
2129	37974	37974	W14-1701	Introduction	15	22	4.0	1.0	Section 7 concludes the paper.	0
2130	38460	38460	K16-2001	Task Definition	1	31	1.0	1.0	The goal of the shared task on shallow discourse parsing is to detect and categorize individual discourse relations.	0
2131	38461	38461	K16-2001	Task Definition	2	32	1.0	1.0	Specifically, given a newswire article as input, a participating system is asked to return the set of discourse relations it can identify in the text.	1
2132	38462	38462	K16-2001	Task Definition	3	33	1.0	1.0	A discourse relation is defined as a relation taking two abstract objects (events, states, facts, or propositions) as arguments (Prasad et al., 2008;	1
2133	38463	38463	K16-2001	Task Definition	4	34	2.0	1.0	Prasad et al., 2014).	0
2134	38464	38464	K16-2001	Task Definition	5	35	2.0	1.0	Discourse relations may be expressed with explicit connectives like because, however, but, or implicitly inferred between two argument spans interpretable as abstract objects.	0
2135	38465	38465	K16-2001	Task Definition	6	36	2.0	1.0	In the current version of the PDTB, only adjacent spans are considered.	0
2136	38466	38466	K16-2001	Task Definition	7	37	3.0	1.0	Each discourse relation is labeled with a sense selected from a sense hierarchy.	0
2137	38467	38467	K16-2001	Task Definition	8	38	3.0	1.0	Its argument spans may be sentences, clauses, or in some rare cases, noun phrases.	0
2138	38468	38468	K16-2001	Task Definition	9	39	3.0	1.0	To detect a discourse relation, a participating system needs to:	0
2139	38469	38469	K16-2001	Task Definition	10	40	4.0	1.0	1	0
2140	38470	38470	K16-2001	Task Definition	11	41	4.0	1.0	"Identify the text span of an explicit discourse connective, if present, or the po-sition between adjacent sentences as the proxy site of an implicit discourse relation; 2. Identify the two text spans that serve as arguments to the relation; 3. Label the arguments as Arg1 or Arg2, as appropriate; 4. Predict the sense of the discourse relation (e.g., ""Cause"", ""Condition"", ""Contrast"")."	0
2141	38471	38471	K16-2001	Task Definition	12	42	4.0	1.0	A full system that outputs all four components of the discourse relations usually comprises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system.	0
2142	38472	38472	K16-2001	Task Definition	13	43	4.0	1.0	This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments.	0
2143	38701	38701	K17-3001	Introduction	1	6	1.0	1.0	Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular.	0
2144	38702	38702	K17-3001	Introduction	2	7	1.0	1.0	For the first time dependency treebanks in more than ten languages were available for learning parsers.	0
2145	38703	38703	K17-3001	Introduction	3	8	1.0	1.0	Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications.	0
2146	38704	38704	K17-3001	Introduction	4	9	2.0	1.0	While the two tasks (Buchholz and Marsi, 2006;	0
2147	38705	38705	K17-3001	Introduction	5	10	2.0	1.0	Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to-kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible.	0
2148	38706	38706	K17-3001	Introduction	6	11	2.0	1.0	CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues.	0
2149	38707	38707	K17-3001	Introduction	7	12	3.0	1.0	1	0
2150	38708	38708	K17-3001	Introduction	8	13	3.0	1.0	The focus of the 2017 task was learning syntactic dependency parsers that can work in a realworld setting, starting from raw text, and that can work over many typologically different languages, even surprise languages for which there is little or no training data, by exploiting a common syntactic annotation standard.	0
2151	38709	38709	K17-3001	Introduction	9	14	3.0	1.0	This task has been made possible by the Universal Dependencies initiative (UD) (Nivre et al., 2016), which has developed treebanks for 50+ languages with crosslinguistically consistent annotation and recoverability of the original raw texts.	0
2152	38710	38710	K17-3001	Introduction	10	15	4.0	1.0	Participating systems had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.	1
2153	38711	38711	K17-3001	Introduction	11	16	4.0	1.0	No gold-standard annotation (tokenization, sentence segmentation, lemmas, morphology) was available in the input text.	0
2154	38712	38712	K17-3001	Introduction	12	17	4.0	1.0	However, teams wishing to concentrate just on parsing were able to use segmentation and morphology predicted by the baseline UDPipe system (Straka et al., 2016).	0
2155	39001	39001	K18-2001	Introduction	1	8	1.0	1.0	The 2017 CoNLL shared task on universal dependency parsing  picked up the thread from the influential shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006;	0
2156	39002	39002	K18-2001	Introduction	2	9	1.0	1.0	Nivre et al., 2007) and evolved it in two ways: (1) the parsing process started from raw text rather than gold standard tokenization and part-of-speech tagging, and (2) the syntactic representations were consistent across languages thanks to the Universal Dependencies framework (Nivre et al., 2016).	0
2157	39003	39003	K18-2001	Introduction	3	10	2.0	1.0	The 2018 CoNLL shared task on universal dependency parsing starts from the same premises but adds a focus on morphological analysis as well as data from new languages.	0
2158	39004	39004	K18-2001	Introduction	4	11	2.0	1.0	Like last year, participating systems minimally had to find labeled syntactic dependencies between words, i.e., a syntactic head for each word, and a label classifying the type of the dependency relation.	1
2159	39005	39005	K18-2001	Introduction	5	12	3.0	1.0	In addition, this year's task featured new metrics that also scored a system's capacity to predict a morphological analysis of each word, including a part-of-speech tag, morphological features, and a lemma.	1
2160	39006	39006	K18-2001	Introduction	6	13	3.0	1.0	Regardless of metric, the assumption was that the input should be raw text, with no gold-standard word or sentence segmentation, and no gold-standard morphological annotation.	0
2161	39007	39007	K18-2001	Introduction	7	14	4.0	1.0	However, for teams who wanted to concentrate on one or more subtasks, segmentation and morphology predicted by the baseline UDPipe system (Straka et al., 2016) was made available just like last year.	0
2162	39008	39008	K18-2001	Introduction	8	15	4.0	1.0	There are eight new languages this year: Afrikaans, Armenian, Breton, Faroese, Naija, Old French, Serbian, and Thai; see Section 2 for more details.	0
2163	39009	39009	K18-2001	Introduction	9	16	4.0	1.0	The two new evaluation metrics are described in Section 3.	0
2164	39255	39255	K18-3001	Introduction	1	10	1.0	1.0	Some of a word's syntactic and semantic properties are expressed on the word form through a process termed morphological inflection.	0
2165	39256	39256	K18-3001	Introduction	2	11	1.0	1.0	For example, each English count noun has both singular and plural forms (robot/robots, process/processes), known as the inflected forms of the noun.	0
2166	39257	39257	K18-3001	Introduction	3	12	1.0	1.0	Some languages display little inflection, while others possess a proliferation of forms.	0
2167	39258	39258	K18-3001	Introduction	4	13	1.0	1.0	A Polish verb can have nearly 100 inflected forms and an Archi verb has thousands (Kibrik, 1998).	0
2168	39259	39259	K18-3001	Introduction	5	14	2.0	1.0	Natural language processing systems must be able to analyze and generate these inflected forms.	0
2169	39260	39260	K18-3001	Introduction	6	15	2.0	1.0	Fortunately, inflected forms tend to be systematically related to one another.	0
2170	39261	39261	K18-3001	Introduction	7	16	2.0	1.0	This is why English  example maps a lemma and inflection to an inflected form, The inflection is a bundle of morphosyntactic features.	0
2171	39262	39262	K18-3001	Introduction	8	17	2.0	1.0	Note that inflected forms (and lemmata) can encompass multiple words.	0
2172	39263	39263	K18-3001	Introduction	9	18	3.0	1.0	In the test data, the last column (the inflected form) must be predicted by the system.	0
2173	39264	39264	K18-3001	Introduction	10	19	3.0	1.0	speakers can usually predict the singular form from the plural and vice versa, even for words they have never seen before: given a novel noun wug, an English speaker knows that the plural is wugs.	0
2174	39265	39265	K18-3001	Introduction	11	20	3.0	1.0	We conducted a competition on generating inflected forms.	0
2175	39266	39266	K18-3001	Introduction	12	21	3.0	1.0	"This ""shared task"" consisted of two separate scenarios."	0
2176	39267	39267	K18-3001	Introduction	13	22	4.0	1.0	In Task 1, participating systems must inflect word forms based on labeled examples.	1
2177	39268	39268	K18-3001	Introduction	14	23	4.0	1.0	In English, an example of inflection is the conversion of a citation form 1 run to its present participle, running.	1
2178	39269	39269	K18-3001	Introduction	15	24	4.0	1.0	The system is provided with the source form and the morphosyntactic description (MSD) of the target form, and must generate the actual target form.	1
2179	39270	39270	K18-3001	Introduction	16	25	4.0	1.0	Task 2 is a harder version of Task 1, where the system must infer the appropriate MSD from a sentential context.	1
2180	39271	39271	K18-3001	Introduction	17	26	4.0	1.0	This is essentially a cloze task, asking participants to provide the correct form of a lemma in context.	0
2181	39561	39561	K19-2001	Background and Motivation	1	6	1.0	1.0	All things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, 'classic' (discrete and hierarchically structured) semantic representations will continue to play an important role in 'making sense' of natural language.	0
2182	39562	39562	K19-2001	Background and Motivation	2	7	1.0	1.0	While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surface syntax, and in particular for the representation of semantic structure.	0
2183	39563	39563	K19-2001	Background and Motivation	3	8	1.0	1.0	The 2019 Conference on Computational Language Learning (CoNLL) hosts a shared task (or 'system bake-off') on Cross-Framework Meaning Representation Parsing (MRP 2019).	0
2184	39564	39564	K19-2001	Background and Motivation	4	9	1.0	1.0	The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning.	0
2185	39565	39565	K19-2001	Background and Motivation	5	10	2.0	1.0	For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup.	0
2186	39566	39566	K19-2001	Background and Motivation	6	11	2.0	1.0	Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see 3 below)which all encode core predicate-argument structure, among other things-in the same implementation.	1
2187	39567	39567	K19-2001	Background and Motivation	7	12	2.0	1.0	Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel.	1
2188	39568	39568	K19-2001	Background and Motivation	8	13	2.0	1.0	Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required.	0
2189	39569	39569	K19-2001	Background and Motivation	9	14	3.0	1.0	Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017;	0
2190	39570	39570	K19-2001	Background and Motivation	10	15	3.0	1.0	Hershcovich et al., 2018;or Stanovsky and Dagan, 2018).	0
2191	39571	39571	K19-2001	Background and Motivation	11	16	3.0	1.0	Training and evaluation data were provided for all five frameworks.	0
2192	39572	39572	K19-2001	Background and Motivation	12	17	3.0	1.0	The task design aims to reduce framework-specific 'balkanization' in the field of meaning representation parsing.	0
2193	39573	39573	K19-2001	Background and Motivation	13	18	4.0	1.0	Its contributions include (a) a unifying formal model over different semantic graph banks ( 2), (b) uniform representations and scoring ( 4 and 6), (c) contrastive evaluation across frameworks ( 5), and (d) increased cross-fertilization via transfer and multi-task learning ( 7).	0
2194	39574	39574	K19-2001	Background and Motivation	14	19	4.0	1.0	Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 May, 2016;	0
2195	39575	39575	K19-2001	Background and Motivation	15	20	4.0	1.0	May and Priyadarshi, 2017;.	0
2196	39576	39576	K19-2001	Background and Motivation	16	21	4.0	1.0	Owing to the scarcity of semantic anno-tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being.	0
2197	40360	40360	2021.dravidianlangtech-1.15	Task	1	61	1.0	3.0	The shared task was hosted on Codalab.	0
2198	40361	40361	2021.dravidianlangtech-1.15	Task	2	62	2.0	3.0	Four translation sub-tasks were organized as a part of this task: English to Tamil, English to Malayalam, English to Telugu and Tamil to Telugu.	1
2199	40362	40362	2021.dravidianlangtech-1.15	Task	3	63	2.0	3.0	Participants were given a choice to participate in the sub-tasks they wanted to.	0
2200	40363	40363	2021.dravidianlangtech-1.15	Task	4	64	3.0	3.0	Training, development and test datasets of parallel sentences for each language pair were provided to all the participants.	0
2201	40364	40364	2021.dravidianlangtech-1.15	Task	5	65	3.0	3.0	The task was to train/develop machine translation systems for the given languages.	0
2202	40365	40365	2021.dravidianlangtech-1.15	Task	6	66	4.0	3.0	For evaluation, the participants translated the test data using the translation systems and the results were submitted to the organizers of the workshop.	0
2203	40366	40366	2021.dravidianlangtech-1.15	Task	7	67	4.0	3.0	The submissions were evaluated by comparing them with the gold standard test set translations available, for which BLEU scores were used as the metric to rank the participants and subsequently the results were returned to the participants.	0
2204	40438	40438	2021.dravidianlangtech-1.16	Task Description	1	39	1.0	2.0	"The goal of the ""Troll Meme Classification in Tamil"" shared task was to classify if a given meme is a ""troll"" or ""not-troll"" based on the image and text associated with the meme in the Tamil language."	1
2205	40439	40439	2021.dravidianlangtech-1.16	Task Description	2	40	1.0	2.0	The text from the meme is written in either the Tamil grammar and English lexicon or English grammar and Tamil lexicon.	0
2206	40440	40440	2021.dravidianlangtech-1.16	Task Description	3	41	1.0	2.0	However, for consistency, we transcripted the text in Latin.	0
2207	40441	40441	2021.dravidianlangtech-1.16	Task Description	4	42	2.0	2.0	Troll meme is a meme, which consists of offensive text and non-offensive images, offensive images with non-offensive text, sarcastically offensive text with non-offensive images, or sarcastic images with offensive text to provoke, distract and has digressive or off-topic content with intend to demean or offend particular people, group or race, otherwise, a not-troll meme .	0
2208	40442	40442	2021.dravidianlangtech-1.16	Task Description	5	43	2.0	2.0	Figure 1 shows examples of a troll and nottroll meme from the TamilMeme dataset.	0
2209	40443	40443	2021.dravidianlangtech-1.16	Task Description	6	44	2.0	2.0	"Example 1 is a troll meme targeted towards the potato chip brand called ""Lays""."	0
2210	40444	40444	2021.dravidianlangtech-1.16	Task Description	7	45	3.0	2.0	"In this example, an image is harmless with just a picture of the potato chips  packet, but the translation of the text is ""If you buy one packet air, then 5 chips free"" which is offensive for the brand."	0
2211	40445	40445	2021.dravidianlangtech-1.16	Task Description	8	46	3.0	2.0	"The translation of Example 2 would be ""Sorry my friend (girl)""."	0
2212	40446	40446	2021.dravidianlangtech-1.16	Task Description	9	47	3.0	2.0	This example does not contain any provoking or offensive image or text and hence, it is a not-troll meme.	0
2213	40447	40447	2021.dravidianlangtech-1.16	Task Description	10	48	4.0	2.0	Previously, we developed this TamilMemes dataset and treated the task of identifying a troll meme as an image classification problem.	0
2214	40448	40448	2021.dravidianlangtech-1.16	Task Description	11	49	4.0	2.0	Since the text associated with the meme acts as a context of the image, we enhanced our TamilMemes dataset by providing the text as a separate modality for the shared task.	0
2215	40449	40449	2021.dravidianlangtech-1.16	Task Description	12	50	4.0	2.0	We expected our participant to approach the task in a multimodal way.	0
2216	40543	40543	2021.dravidianlangtech-1.17	Task Description	1	40	1.0	2.0	Offensive language identification for Dravidian languages at different levels of complexity were developed following the work of (Zampieri et al., 2019).	1
2217	40544	40544	2021.dravidianlangtech-1.17	Task Description	2	41	1.0	2.0	It was customized to our annotation method from three-level hierarchical annotation schema.	0
2218	40545	40545	2021.dravidianlangtech-1.17	Task Description	3	42	1.0	2.0	A new category Not in intended language was added to include comments written in a language other than the Dravidian languages.	0
2219	40546	40546	2021.dravidianlangtech-1.17	Task Description	4	43	1.0	2.0	Annotations decision for offensive language categories were split into six labels to simplify the annotation process.	0
2220	40547	40547	2021.dravidianlangtech-1.17	Task Description	5	44	1.0	2.0	 Not Offensive: Comment/post does not have offence, obscenity, swearing, or profanity.	0
2221	40548	40548	2021.dravidianlangtech-1.17	Task Description	6	45	1.0	2.0	 Offensive Untargeted: Comment/post have offence, obscenity, swearing, or profanity not directed towards any target.	0
2222	40549	40549	2021.dravidianlangtech-1.17	Task Description	7	46	2.0	2.0	These are the comments/posts which have inadmissible language without targeting anyone.	0
2223	40550	40550	2021.dravidianlangtech-1.17	Task Description	8	47	2.0	2.0	 Offensive Targeted Individual: Comment/post have offence, obscenity, swearing, or profanity which targets an individual.	0
2224	40551	40551	2021.dravidianlangtech-1.17	Task Description	9	48	2.0	2.0	 Offensive Targeted Group: Comment/post have offence, obscenity, swearing, or profanity which targets a group or a community.	0
2225	40552	40552	2021.dravidianlangtech-1.17	Task Description	10	49	2.0	2.0	 Offensive Targeted Other: Comment/post have offence, obscenity, swearing, or profanity which does not belong to any of the previous two classes.	0
2226	40553	40553	2021.dravidianlangtech-1.17	Task Description	11	50	2.0	2.0		0
2227	40554	40554	2021.dravidianlangtech-1.17	Task Description	12	51	2.0	2.0	Not in indented language:	0
2228	40555	40555	2021.dravidianlangtech-1.17	Task Description	13	52	2.0	2.0	If the comment is not in the intended language.	0
2229	40556	40556	2021.dravidianlangtech-1.17	Task Description	14	53	3.0	2.0	For example, in the Malayalam task, if the sentence does not contain Malayalam written in Malayalam script or Latin script, then it is not Malayalam.	0
2230	40557	40557	2021.dravidianlangtech-1.17	Task Description	15	54	3.0	2.0	Sample of not-offensive comments in our dataset provided for the participants is given below with the corresponding English glosses..	0
2231	40558	40558	2021.dravidianlangtech-1.17	Task Description	16	55	3.0	2.0	This is an example of Offensive Targeted Individual.	0
2232	40559	40559	2021.dravidianlangtech-1.17	Task Description	17	56	3.0	2.0	Mr. Ghibran is a popular music composer in Tamil films, and the author tries to insult him.	0
2233	40560	40560	2021.dravidianlangtech-1.17	Task Description	18	57	3.0	2.0	 Malayalam-English: Verupikkal manju ullathu ozhichal baki ellam super aanu ee padam.	0
2234	40561	40561	2021.dravidianlangtech-1.17	Task Description	19	58	3.0	2.0	"English: ""Except the presence of disgusting Manju, everything is super in this movie"""	0
2235	40562	40562	2021.dravidianlangtech-1.17	Task Description	20	59	3.0	2.0	This is an example of Offensive Targeted Individual.	0
2236	40563	40563	2021.dravidianlangtech-1.17	Task Description	21	60	4.0	2.0	Mrs. Manju Warrier is a popular actress in South Indian films, and the author tries to discredit her by clearly mentioning that her presence is disgusting.	0
2237	40564	40564	2021.dravidianlangtech-1.17	Task Description	22	61	4.0	2.0	 Malayalam-English: Nalla trailer nu okke keri dislike adikunne ethelum thanthayillathavanmar aayirikum.	0
2238	40565	40565	2021.dravidianlangtech-1.17	Task Description	23	62	4.0	2.0	poyi chavinedey...	0
2239	40566	40566	2021.dravidianlangtech-1.17	Task Description	24	63	4.0	2.0	"English: ""Those who dislike any trailers will probably be assholes."	0
2240	40567	40567	2021.dravidianlangtech-1.17	Task Description	25	64	4.0	2.0	"Go to hell..."""	0
2241	40568	40568	2021.dravidianlangtech-1.17	Task Description	26	65	4.0	2.0	This is an example of Offensive Untargeted comment.	0
2242	40569	40569	2021.dravidianlangtech-1.17	Task Description	27	66	4.0	2.0	This text have inadmissible language without targeting anyone.	0
2243	40664	40664	2021.ltedi-1.8	Introduction	1	9	1.0	1.0	In the recent years, there has been an exponential rise in the number of studies focusing on the detection and management of hate speech and offensive language in social media (Kumar et al., 2018;	0
2244	40665	40665	2021.ltedi-1.8	Introduction	2	10	1.0	1.0	Mandl et al., 2020;Zampieri et al., 2020).	0
2245	40666	40666	2021.ltedi-1.8	Introduction	3	11	1.0	1.0	However, this has led to controlling user expression instead of improving user experience.	0
2246	40667	40667	2021.ltedi-1.8	Introduction	4	12	1.0	1.0	(Ghanghor et al., 2021a;	0
2247	40668	40668	2021.ltedi-1.8	Introduction	5	13	1.0	1.0	Hegde et al., 2021;Yasaswini et al., 2021).	0
2248	40669	40669	2021.ltedi-1.8	Introduction	6	14	1.0	1.0	This has also resulted in putting barriers on modes of expression of different groups of people resulting in the violation of the principles of Equality, Diversity, and Inclusion (EDI).	0
2249	40670	40670	2021.ltedi-1.8	Introduction	7	15	2.0	1.0	For instance, some NLP systems have classified the comments made in African American English as offensive language without accommodating the linguistic features peculiar to the dialect.	0
2250	40671	40671	2021.ltedi-1.8	Introduction	8	16	2.0	1.0	Hence there is a need for a shift in the approaches taken to handle hate speech without compromising on the principles of EDI.	0
2251	40672	40672	2021.ltedi-1.8	Introduction	9	17	2.0	1.0	We propose to shift the prevailing research direction in Natural Language Processing from controlling negativity (curbing hate speech etc.) to encouraging positivity (promoting hope speech).	0
2252	40673	40673	2021.ltedi-1.8	Introduction	10	18	2.0	1.0	Hope speech is any expression that is positive, encouraging, supportive, and / or inspires promise of the future (Chakravarthi, 2020a).	0
2253	40674	40674	2021.ltedi-1.8	Introduction	11	19	2.0	1.0	For the shared task organised in this connection, participants were provided with development, training and test dataset in English as well as in two under-resourced languages -Tamil and Malayalam.	0
2254	40675	40675	2021.ltedi-1.8	Introduction	12	20	2.0	1.0	Tamil (ISO 639-3: tam) belongs to the Dravidian language family and is widely spoken in the southern state of Tamil Nadu in India, Sri Lanka, Malaysia and Singapore (Krishnamurti, 2003;	0
2255	40676	40676	2021.ltedi-1.8	Introduction	13	21	3.0	1.0	Kolipakam et al., 2018;Chakravarthi, 2020b;Mahesan, 2019, 2020a,b). Malayalam (ISO 639-3: mal) also belongs to the Dravidian language family and is spoken in the Indian state of Kerala and the Union Territories of Lakshdweep and Puducherry (Chakravarthi et al., 2020).	0
2256	40677	40677	2021.ltedi-1.8	Introduction	14	22	3.0	1.0	Both Tamil and Malayalam have their own scripts which are alphasyllabaries like other Indic scripts i.e. they are partially alphabetic and partially syllabic.	0
2257	40678	40678	2021.ltedi-1.8	Introduction	15	23	3.0	1.0	The Tamil language was written using Tamili, Vattezhuthu, Chola, Pallava and Chola-Pallava scripts at different points in history.	0
2258	40679	40679	2021.ltedi-1.8	Introduction	16	24	3.0	1.0	The modern Tamil script descended from the Chola-Pallava script that was conceived around the 6th century CE (Srinivasan, 2019).	0
2259	40680	40680	2021.ltedi-1.8	Introduction	17	25	3.0	1.0	Malayalam was first written with Vattezhuthu script that evolved from Tamili script around 4-5th century CE (Mahadevan, 2003).	0
2260	40681	40681	2021.ltedi-1.8	Introduction	18	26	3.0	1.0	Modern Malayalam is written using the Vattezhuttu alphabets extended with symbols from the Grantha script to accomodate non-native Sanskrit sounds (Krishnamurti, 2003).	0
2261	40682	40682	2021.ltedi-1.8	Introduction	19	27	4.0	1.0	Although they have their own scripts, the social media comments in these languages are often written in Latin script as it is easy to input.	0
2262	40683	40683	2021.ltedi-1.8	Introduction	20	28	4.0	1.0	Our dataset for the hope speech for EDI shared task was created from user-generated content in Tamil, Malayalam, and English (Chakravarthi and Muralidaran, 2021).	0
2263	40684	40684	2021.ltedi-1.8	Introduction	21	29	4.0	1.0	The user-generated comments in our dataset for Tamil and Malayalam were codemixed (Chakravarthi, 2020a).	0
2264	40685	40685	2021.ltedi-1.8	Introduction	22	30	4.0	1.0	We proposed a comment/post level classification task.	0
2265	40686	40686	2021.ltedi-1.8	Introduction	23	31	4.0	1.0	The goal for the participants was to classify a given Youtube comment into either 'Hope speech', 'Non hope speech' or 'Not Tamil / Not Malayalam / Not English'.	1
2266	40687	40687	2021.ltedi-1.8	Introduction	24	32	4.0	1.0	Our CodaLab website will remain open to allow researchers to access the data and build upon this work.	0
2267	40805	40805	2021.wanlp-1.28	abstract	1	2	1.0	1.0	We present the findings and results of the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021).	0
2268	40806	40806	2021.wanlp-1.28	abstract	2	3	2.0	1.0	This Shared Task includes four subtasks: country-level Modern Standard Arabic (MSA) identification (Subtask 1.1), country-level dialect identification (Subtask 1.2), province-level MSA identification (Subtask 2.1), and province-level sub-dialect identification (Subtask 2.2).	1
2269	40807	40807	2021.wanlp-1.28	abstract	3	4	3.0	1.0	The shared task dataset covers a total of 100 provinces from 21 Arab countries, collected from the Twitter domain.	0
2270	40808	40808	2021.wanlp-1.28	abstract	4	5	4.0	1.0	A total of 53 teams from 23 countries registered to participate in the tasks, thus reflecting the interest of the community in this area.	0
2271	40809	40809	2021.wanlp-1.28	abstract	5	6	4.0	1.0	We received 16 submissions for Subtask 1.1 from five teams, 27 submissions for Subtask 1.2 from eight teams, 12 submissions for Subtask 2.1 from four teams, and 13 Submissions for subtask 2.2 from four teams.	0
2272	41112	41112	2021.wanlp-1.36	Tasks Description	1	118	1.0	3.0	The shared task on sarcasm detection and sentiment analysis in Arabic contains two subtasks as follows:	1
2273	41113	41113	2021.wanlp-1.36	Tasks Description	2	119	2.0	3.0	 Sarcasm Detection (subtask 1): the goal is to identify whether a tweet is sarcastic or not.	1
2274	41114	41114	2021.wanlp-1.36	Tasks Description	3	120	2.0	3.0	 Sentiment Analysis (subtask 2): the goal is to classify the tweet to one of the sentiment classes: positive, negative or neutral.	1
2275	41115	41115	2021.wanlp-1.36	Tasks Description	4	121	3.0	3.0	The data for both subtasks was provided as train/test split without a specific development set.	0
2276	41116	41116	2021.wanlp-1.36	Tasks Description	5	122	3.0	3.0	Table 1 shows the statistics of the two sets.	0
2277	41117	41117	2021.wanlp-1.36	Tasks Description	6	123	4.0	3.0	The training set consists of 12,548 tweets, while the testing set consists of 3,000 tweets.	0
2278	41118	41118	2021.wanlp-1.36	Tasks Description	7	124	4.0	3.0	The participants had access to the tweets' text and the dialect label during the testing phase.	0
2279	41171	41171	2021.wassa-1.10	Introduction	1	9	1.0	1.0	It is important to be able to analyze empathy and emotion in natural languages.	0
2280	41172	41172	2021.wassa-1.10	Introduction	2	10	1.0	1.0	Emotion classification in natural languages has been studied over two decades and many applications successfully used emotion as their major components.	0
2281	41173	41173	2021.wassa-1.10	Introduction	3	11	1.0	1.0	Empathy utterances can be emotional, therefore, examining emotion in text-based empathy possibly has a major impact on predicting empathy.	0
2282	41174	41174	2021.wassa-1.10	Introduction	4	12	1.0	1.0	Analyzing text-based empathy and emotion have different applications; empathy is a crucial component in applications such as empathic AI agents, effective gesturing of robots, and mental health, emotion has natural language applications such as commerce, public health, and disaster management.	0
2283	41175	41175	2021.wassa-1.10	Introduction	5	13	1.0	1.0	In this paper, we present the WASSA 2021 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories.	0
2284	41176	41176	2021.wassa-1.10	Introduction	6	14	1.0	1.0	This shared task included two individual tasks where teams develop models to predict emotions and empathy in essays in which people expressed their empathy and distress in reaction to news articles in which an individual, group of people or nature was harmed.	0
2285	41177	41177	2021.wassa-1.10	Introduction	7	15	1.0	1.0	Additionally, the dataset also included the demographic information of the authors of the essays such as age, gender, ethnicity, income, and education level, and personality information (details of the collection of the dataset is provided in section 3).	0
2286	41178	41178	2021.wassa-1.10	Introduction	8	16	1.0	1.0	Optionally, we suggested that the teams could also use emotion labels when modeling empathy to learn more about the impact of emotions on empathy.	0
2287	41179	41179	2021.wassa-1.10	Introduction	9	17	2.0	1.0	The shared task consisted of two tracks:	1
2288	41180	41180	2021.wassa-1.10	Introduction	10	18	2.0	1.0	"1. Predicting Empathy (EMP): the formulation of this track is to predict the Batson empathic concern (""feeling for someone"") and personal distress (""suffering with someone"") using the essay, personality information, demographic information, and emotion."	1
2289	41181	41181	2021.wassa-1.10	Introduction	11	19	2.0	1.0	2. Emotion Label Prediction (EMO): the formulation of this track is to predict emotion tags (sadness, joy, disgust, surprise, anger, or fear), taken from Ekman's six basic emotions (Ekman, 1971), plus no-emotion tag for essays.	1
2290	41182	41182	2021.wassa-1.10	Introduction	12	20	2.0	1.0	In this setting personality and demographic information as well as empathy and distress scores were also made available and optional to use.	0
2291	41183	41183	2021.wassa-1.10	Introduction	13	21	2.0	1.0	For both tasks, an identical train-dev-test split was provided.	0
2292	41184	41184	2021.wassa-1.10	Introduction	14	22	2.0	1.0	The dataset consists of essays that were collected from participants, who had read disturbing news articles about a person, a group of people, or painful situations.	0
2293	41185	41185	2021.wassa-1.10	Introduction	15	23	2.0	1.0	Empathy, distress, demographic, and personality information was taken from the original work by Buechel et al. (2018).	0
2294	41186	41186	2021.wassa-1.10	Introduction	16	24	2.0	1.0	They used Batson's Empathic Concern -Personal Distress Scale (Batson et al., 1987), i.e, rating 6 items for empathy (i.e., warm, tender, sympathetic, softhearted, moved, compassionate) and 8 items for distress (i.e., worried, upset, troubled, perturbed, grieved, disturbed, alarmed, distressed) using a 7point scale for each of these items (detailed information can be found in the Appendix section of the original paper).	0
2295	41187	41187	2021.wassa-1.10	Introduction	17	25	2.0	1.0	Regarding emotion, all data was annotated with the six basic Ekman emotions (sadness, joy, disgust, surprise, anger, or fear).	0
2296	41188	41188	2021.wassa-1.10	Introduction	18	26	3.0	1.0	Five teams participated in this shared task, three participated in both tracks, and each time one additional team participated in either the EMP or EMO track.	0
2297	41189	41189	2021.wassa-1.10	Introduction	19	27	3.0	1.0	During the evaluation phase, every team was allowed to submit their results until a certain deadline, after which the final submission was taken into consideration for the ranking.	0
2298	41190	41190	2021.wassa-1.10	Introduction	20	28	3.0	1.0	The best result for the empathy prediction track was an average Pearson correlation of 0.545 and the best macro F1-score for the emotion track amounted to 55%.	0
2299	41191	41191	2021.wassa-1.10	Introduction	21	29	3.0	1.0	All tasks were designed in CodaLab 1 and the teams were allowed to submit one official result during evaluation phase and several ones during the training phase.	0
2300	41192	41192	2021.wassa-1.10	Introduction	22	30	3.0	1.0	In the remainder of this paper we first review related work (Section 2), after which we introduce the dataset used for both tracks (Section 3).	0
2301	41193	41193	2021.wassa-1.10	Introduction	23	31	3.0	1.0	The shared task is presented in Section 4 and the official results in Section 5.	0
2302	41194	41194	2021.wassa-1.10	Introduction	24	32	3.0	1.0	A discussion of the different systems participating in both tracks is presented in Section 6 and we conclude our work in Section 7.	0
2303	41195	41195	2021.wassa-1.10	Introduction	25	33	3.0	1.0	1	0
2304	41196	41196	2021.wassa-1.10	Introduction	26	34	4.0	1.0	Task descriptions, datasets, and results are designed in CodaLab https://competitions.codalab.org/ competitions/28713 2 Related Work Emotion has been studied for two decades and a large body of works have provided insights and remarkable findings.	0
2305	41197	41197	2021.wassa-1.10	Introduction	27	35	4.0	1.0	In contrast, detecting and predicting empathy and distress in text is a growing field and there is little work on the correlation and relatedness of emotion, empathy, and distress.	0
2306	41198	41198	2021.wassa-1.10	Introduction	28	36	4.0	1.0	This shared task is designed to study the modeling of empathy and distress and the correlation among them.	0
2307	41199	41199	2021.wassa-1.10	Introduction	29	37	4.0	1.0	In the literature empathy is considered towards negative events, however, recent studies suggest that people's joyful emotions towards positive events can be termed as positive empathy (Morelli et al., 2015).	0
2308	41200	41200	2021.wassa-1.10	Introduction	30	38	4.0	1.0	The psychological theory distinguishes two separate constructs for distress and empathy; distress is a self-focused, negative affective state (suffering with someone), and empathy is a warm, tender, and compassionate state (feeling for someone).	0
2309	41201	41201	2021.wassa-1.10	Introduction	31	39	4.0	1.0	To quantify empathy and distress, studies present different approaches, the most popular one is Batson's Empathic Concern -Personal Distress Scale (Batson et al., 1987), which is used to obtain empathy and distress scores for each essay in this dataset.	0
2310	41202	41202	2021.wassa-1.10	Introduction	32	40	4.0	1.0	To annotate emotions in text, classical studies in NLP suggest categorical tagsets, and most studies are focused on basic emotion models that are suggested by psychological emotion models.	0
2311	41203	41203	2021.wassa-1.10	Introduction	33	41	4.0	1.0	The most popular ones are the Ekman 6 basic emotions (Ekman, 1971), the Plutchik 8 basic emotions (Plutchik, 1984), and 4 basic emotions (Frijda, 1988).	0
2312	41204	41204	2021.wassa-1.10	Introduction	34	42	4.0	1.0	We opted for the Ekman emotions, because this model is well adopted in different downstream NLP tasks of which emotion is a component, and it is most suited to the dataset we aim to study in this shared task.	0
2313	41524	41524	W18-5501	Task Description	1	24	2.0	1.0	Candidate systems for the FEVER shared task were given a sentence of unknown veracity called a claim.	1
2314	41525	41525	W18-5501	Task Description	2	25	4.0	1.0	The systems must identify suitable evidence from Wikipedia at the sentence level and Claim: The Rodney King riots took place in the most populous county in the USA.	1
2315	41707	41707	W18-6206	abstract	1	2	1.0	1.0	Past shared tasks on emotions use data with both overt expressions of emotions (I am so happy to see you!) as well as subtle expressions where the emotions have to be inferred, for instance from event descriptions.	0
2316	41708	41708	W18-6206	abstract	2	3	1.0	1.0	Further, most datasets do not focus on the cause or the stimulus of the emotion.	0
2317	41709	41709	W18-6206	abstract	3	4	2.0	1.0	Here, for the first time, we propose a shared task where systems have to predict the emotions in a large automatically labeled dataset of tweets without access to words denoting emotions.	1
2318	41710	41710	W18-6206	abstract	4	5	2.0	1.0	Based on this intention, we call this the Implicit Emotion Shared Task (IEST) because the systems have to infer the emotion mostly from the context.	0
2319	41711	41711	W18-6206	abstract	5	6	2.0	1.0	Every tweet has an occurrence of an explicit emotion word that is masked.	0
2320	41712	41712	W18-6206	abstract	6	7	3.0	1.0	The tweets are collected in a manner such that they are likely to include a description of the cause of the emotion -the stimulus.	0
2321	41713	41713	W18-6206	abstract	7	8	3.0	1.0	Altogether, 30 teams submitted results which range from macro F 1 scores of 21 % to 71 %.	0
2322	41714	41714	W18-6206	abstract	8	9	4.0	1.0	The baseline (Max-Ent bag of words and bigrams) obtains an F 1 score of 60 % which was available to the participants during the development phase.	0
2323	41715	41715	W18-6206	abstract	9	10	4.0	1.0	A study with human annotators suggests that automatic methods outperform human predictions, possibly by honing into subtle textual clues not used by humans.	0
2324	41716	41716	W18-6206	abstract	10	11	4.0	1.0	Corpora, resources, and results are available at the shared task website at http://implicitemotions.wassa2018.com.	0
2325	41911	41911	W18-6402	abstract	1	2	1.0	1.0	We present the results from the third shared task on multimodal machine translation.	0
2326	41912	41912	W18-6402	abstract	2	3	2.0	1.0	In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech.	1
2327	41913	41913	W18-6402	abstract	3	4	2.0	1.0	The image can be used in addition to (or instead of) the source sentence.	0
2328	41914	41914	W18-6402	abstract	4	5	3.0	1.0	This year the task was extended with a third target language (Czech) and a new test set.	0
2329	41915	41915	W18-6402	abstract	5	6	3.0	1.0	In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech.	0
2330	41916	41916	W18-6402	abstract	6	7	4.0	1.0	Seven teams submitted 45 different systems to the two variants of the task.	0
2331	41917	41917	W18-6402	abstract	7	8	4.0	1.0	Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive.	0
2332	42173	42173	D19-5309	abstract	1	3	1.0	1.0	While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited.	0
2333	42174	42174	D19-5309	abstract	2	4	2.0	1.0	The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semistructured tables.	1
2334	42175	42175	D19-5309	abstract	3	5	2.0	1.0	"Each explanation contains between 1 and 16 interconnected facts that form an ""explanation graph"" spanning core scientific knowledge and detailed world knowledge."	0
2335	42176	42176	D19-5309	abstract	4	6	3.0	1.0	It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multihop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus.	0
2336	42177	42177	D19-5309	abstract	5	7	4.0	1.0	The top-performing system achieved a mean average precision (MAP) of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model.	0
2337	42178	42178	D19-5309	abstract	6	8	4.0	1.0	Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multihop reasoning.	0
2338	42434	42434	D19-5701	Introduction	1	12	1.0	1.0	Efficient access to mentions of drugs, medications and chemical entities contained in clinical texts, scientific articles, patents or even the web is a pressing need shared by biomedical researchers and clinicians .	0
2339	42435	42435	D19-5701	Introduction	2	13	1.0	1.0	Biomedical text mining is one of the most prolific application domains of natural language processing technologies (Zweigenbaum et al., 2007).	0
2340	42436	42436	D19-5701	Introduction	3	14	1.0	1.0	The recognition of pharmaceutical drugs/chemical entities is a critical step required for the subsequent detection of relations with other biomedically relevant entities such as genes/proteins, diseases or adverse reactions (Vazquez et al., 2011).	0
2341	42437	42437	D19-5701	Introduction	4	15	1.0	1.0	Text mining and information extraction systems were published that tried to find protein-drug relations (including ligand-protein interactions and pharmacogenomics information), medication-related al-lergies, chemical metabolic reactions, drug-drug interactions (Herrero-Zazo et al., 2013), diseasedrug relations, as well as drug safety-related issues.	0
2342	42438	42438	D19-5701	Introduction	5	16	2.0	1.0	The correct identification of drug mentions is also needed for other complex relation types like drug dosage recognition, duration of medical treatments or drug repurposing.	0
2343	42439	42439	D19-5701	Introduction	6	17	2.0	1.0	The importance of chemical and drug name recognition motivated several-shared tasks in the past, such as the CHEMDNER tracks (Krallinger et al., 2015) or the i2b2 medication challenge (Uzuner et al., 2010b,a), with a considerable number of participants and impact (Doan et al., 2010;	0
2344	42440	42440	D19-5701	Introduction	7	18	2.0	1.0	Yang, 2010).	0
2345	42441	42441	D19-5701	Introduction	8	19	2.0	1.0	Currently, most of the biomedical and clinical NLP research, is done on English documents, while only few tasks were carried out using non-English texts, or were multilingual.	0
2346	42442	42442	D19-5701	Introduction	9	20	3.0	1.0	Nonetheless, it is important to highlight that there is a considerable amount of biomedically relevant content published in other languages than English, and particularly clinical texts are entirely written in the native language of each country.	0
2347	42443	42443	D19-5701	Introduction	10	21	3.0	1.0	Spanish is a language spoken by more than 572 million people in the world today, either as a native, second or foreign language.	0
2348	42444	42444	D19-5701	Introduction	11	22	3.0	1.0	It is the second language in the world by number of native speakers with more than 477 million people.	0
2349	42445	42445	D19-5701	Introduction	12	23	3.0	1.0	According to results derived from WHO statistics, just in Spain there are over 180 thousand practicing physicians, more than 247 thousand nursing and midwifery personnel or 55 thousand pharmaceutical personnel.	0
2350	42446	42446	D19-5701	Introduction	13	24	4.0	1.0	These facts, and the extrapolation to other Spanish speaking countries explains why a considerable subset of the PubMed database records corresponds to Spanish medical articles.	0
2351	42447	42447	D19-5701	Introduction	14	25	4.0	1.0	Moreover, PubMed does only contain a part of the medical literature originally published in Spanish, which is also stored in other resources such as MEDES, SciELO, IBECS or CUIDEN.	0
2352	42448	42448	D19-5701	Introduction	15	26	4.0	1.0	Following the outline of previous chemical/drug NER efforts, in particular the BioCreative CHEMDNER tracks, we have carried out the first task on chemical and drug mention recognition from Spanish medical texts, namely from a corpus of Spanish clinical case studies.	0
2353	42449	42449	D19-5701	Introduction	16	27	4.0	1.0	Thus, this track addressed the automatic extraction of chemical, drug, gene/protein mentions from clinical case studies written in Spanish.	1
2354	42450	42450	D19-5701	Introduction	17	28	4.0	1.0	The main aim was to promote the development of named entity recognition tools of practical relevance, that is, chemi-cal and drug mentions in non-English content, determining the current-state-of-the art, identifying challenges and comparing the strategies and results to those published for English data.	0
2355	42761	42761	D19-5729	Introduction and Motivation	1	6	1.0	1.0	The breadth of brain research is too expansive to be effectively curated without computational tools especially involving machine learning models.	0
2356	42762	42762	D19-5729	Introduction and Motivation	2	7	1.0	1.0	"For example, a Pubmed search for ""Brain"" on August 12, 2019, revealed 854,612 articles 1 . More specifically, an August 12, 2019 search for the single mental illness diagnosis of ""depression"" revealed 530,519 articles 2 ."	0
2357	42763	42763	D19-5729	Introduction and Motivation	3	8	1.0	1.0	And a search for anxiety revealed 224,305 articles 3 .	0
2358	42764	42764	D19-5729	Introduction and Motivation	4	9	1.0	1.0	It is not possible for researchers to functionally analyze all of the critical data patterns both within a single diagnosis or across diagnoses that could be revealed by those articles.	0
2359	42765	42765	D19-5729	Introduction and Motivation	5	10	1.0	1.0	The challenge of curating brain research has been further complicated by the National Institute of Mental Health's adoption of the Research Domain Criteria (RDoC) [6].	0
2360	42766	42766	D19-5729	Introduction and Motivation	6	11	1.0	1.0	"Since 1952, the Diagnostic and Statistical Manual of Mental Disorders 1 Pubmed search for Brain conducted on August 12, 2019 2 Pubmed search for depression conducted on August 12, 2019 3 Pubmed search for anxiety conducted on August 12, 2019 and International Classification of Diseases [5] (popularly known as DSM and ICD, respectively), have ""reigned supreme"" as the single ""overarching model of psychiatric classification"" [14]."	0
2361	42767	42767	D19-5729	Introduction and Motivation	7	12	1.0	1.0	That supremacy began to crumble in 2010 when the National Institute of Mental Health launched the RDoC initiative, an alternate framework to conceptually organize and direct biological research on mental disorders [1].	0
2362	42768	42768	D19-5729	Introduction and Motivation	8	13	1.0	1.0	"The RDoC initiative intends ""to foster integration not only of psychological and biological measures but also of the psychological and biological constructs those measures measure"" [13]."	0
2363	42769	42769	D19-5729	Introduction and Motivation	9	14	1.0	1.0	The RDoC initiative has fostered significant debate among brain health researchers.	0
2364	42770	42770	D19-5729	Introduction and Motivation	10	15	1.0	1.0	It has also created a significant categorization challengespecifically how to curate articles completed under the DSM-ICD criteria so their data can be incorporated into the RDoC model.	0
2365	42771	42771	D19-5729	Introduction and Motivation	11	16	2.0	1.0	Brain science cannot afford to lose critical insights from the numerous articles on different sides of the categorization divide.	0
2366	42772	42772	D19-5729	Introduction and Motivation	12	17	2.0	1.0	Hence, it is vital that all existing and future biomedical literature related to brain research is correctly categorized with respect to the RDoC terminology in addition to DSM-ICD models.	0
2367	42773	42773	D19-5729	Introduction and Motivation	13	18	2.0	1.0	However, manual curation of brain research articles using RDoC terminology by human annotators can be highly resource-consuming due to several reasons.	0
2368	42774	42774	D19-5729	Introduction and Motivation	14	19	2.0	1.0	RDoC framework is comprehensive and complex.	0
2369	42775	42775	D19-5729	Introduction and Motivation	15	20	2.0	1.0	It is made up six major domains of human functioning, which is further broken down to multiple constructs that comprise different aspects of the overall range of functions 4 .	0
2370	42776	42776	D19-5729	Introduction and Motivation	16	21	2.0	1.0	The RDoC matrix helps describe these constructs using several units of analysis such as molecules and circuits.	0
2371	42777	42777	D19-5729	Introduction and Motivation	17	22	2.0	1.0	On top of this, the rate of publication of biomedical literature (and by extension brain re-search related literature) is growing at an exponential rate [10].	0
2372	42778	42778	D19-5729	Introduction and Motivation	18	23	2.0	1.0	This means that the gap between annotated versus unannotated articles will continue to grow at an alarming rate unless more efficient means of automated annotation is developed soon.	0
2373	42779	42779	D19-5729	Introduction and Motivation	19	24	2.0	1.0	In order to invite text mining teams around the world to develop informatics models for RDoC, we introduced the RDoC Task 5 at this years'	0
2374	42780	42780	D19-5729	Introduction and Motivation	20	25	2.0	1.0	BioNLP-OST 2019 workshop 6 . RDoC task is a combination of two subtasks focusing on a subset of RDoC constructs: (a) Task 1 (RDoC-IR) -retrieving PubMed Abstracts related to RDoC constructs, and (b) Task 2 (RDoC-SE) -extracting the most relevant sentence for a given RDoC construct from a known relevant abstract.	1
2375	42781	42781	D19-5729	Introduction and Motivation	21	26	2.0	1.0	Both these tasks represent two very important steps of the typical triage process [10], which are finding the articles related to RDoC constructs and then extracting a specific snippet of information that is useful for curation or downstream tasks such as automatic text summarization [15].	0
2376	42782	42782	D19-5729	Introduction and Motivation	22	27	3.0	1.0	There have been several shared tasks on text mining from biomedical literature and clinical notes in the last decade [19,12] as well as a few shared tasks related to mental health topics ( [4,18,22,21,30]).	0
2377	42783	42783	D19-5729	Introduction and Motivation	23	28	3.0	1.0	CLPsych 2015 Shared Task [4] focused on identifying depression and PTSD users from twitter data, while the same task from the following year (i.e. CLPsych 2016 Shared Task [18]) revolved around classifying the severity of peer support forum posts.	0
2378	42784	42784	D19-5729	Introduction and Motivation	24	29	3.0	1.0	One of the i2b2 7 challenges from 2011 focused on the sentiment analysis of suicide notes [22,21].	0
2379	42785	42785	D19-5729	Introduction and Motivation	25	30	3.0	1.0	"In 2017, Uzuner et al. introduced the ""The RDoC for Psychiatry"" challenge, which was composed of three tracks: de-identification of mental health records [28], determination of symptom severity from a psychiatric evaluation of a patient) related to one of the RDoC domains) [9], and the use of mental health records released through the challenge for answering novel questions [32,29,7]."	0
2380	42786	42786	D19-5729	Introduction and Motivation	26	31	3.0	1.0	In contrast, the RDoC task is a combination of information retrieval and sentence extraction from Biomedical literature related to RDoC constructs.	0
2381	42787	42787	D19-5729	Introduction and Motivation	27	32	3.0	1.0	To generate benchmark data for the RDoC task, three annotators were used to curate the goldstandard datasets.	0
2382	42788	42788	D19-5729	Introduction and Motivation	28	33	3.0	1.0	The registration for the RDoC Task opened in March of 2019.	0
2383	42789	42789	D19-5729	Introduction and Motivation	29	34	3.0	1.0	Over 30 teams around the world registered for the two tasks.	0
2384	42790	42790	D19-5729	Introduction and Motivation	30	35	3.0	1.0	Training data in two batches were released in the month of April.	0
2385	42791	42791	D19-5729	Introduction and Motivation	31	36	3.0	1.0	Test data, again in two batches, were released in June.	0
2386	42792	42792	D19-5729	Introduction and Motivation	32	37	4.0	1.0	The participants were asked to submit their final predictions by June 19.	0
2387	42793	42793	D19-5729	Introduction and Motivation	33	38	4.0	1.0	Eventually, 4 and 5 groups each competed in Tasks 1 and 2, respectively.	0
2388	42794	42794	D19-5729	Introduction and Motivation	34	39	4.0	1.0	The final results were made public immediately after the submission deadline.	0
2389	42795	42795	D19-5729	Introduction and Motivation	35	40	4.0	1.0	Two (out of four) and four (out of five) teams each outperformed the baseline methods in task 1 and 2, respectively.	0
2390	42796	42796	D19-5729	Introduction and Motivation	36	41	4.0	1.0	The increase in performance over the baselines were more noticeable in task 2 suggesting that information retrieval for RDoC task may be more challenging.	0
2391	42797	42797	D19-5729	Introduction and Motivation	37	42	4.0	1.0	There was quite a lot of variation across the several RDoC constructs used for the tasks suggesting that the complexity of different constructs may hinder certain models and construct-specific methods or models may be a requirement in the future.	0
2392	42798	42798	D19-5729	Introduction and Motivation	38	43	4.0	1.0	Overall observations from the RDoC Task highlights the need for more sophisticated method development.	0
2393	42799	42799	D19-5729	Introduction and Motivation	39	44	4.0	1.0	The rest of the paper is organized as follows.	0
2394	42800	42800	D19-5729	Introduction and Motivation	40	45	4.0	1.0	Section 2 describes the benchmark or goldstandard data preparation process, development of training and test sets, submission requirements, baseline methods used by the organizers, and the performance measures used for the evaluation.	0
2395	42801	42801	D19-5729	Introduction and Motivation	41	46	4.0	1.0	Section 3 presents and discusses the overall results for the two tasks.	0
2396	42802	42802	D19-5729	Introduction and Motivation	42	47	4.0	1.0	Finally, Section 4 summarizes the task findings as well as describes the potential future work.	0
2397	43173	43173	2020.sigtyp-1.1	Task Description	1	36	1.0	1.0	The SIGTYP 2020 shared task is concerned with predicting typological features from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013).	1
2398	43174	43174	2020.sigtyp-1.1	Task Description	2	37	2.0	1.0	For the task, participants were invited to build systems to predict features for languages unseen at training time.	0
2399	43175	43175	2020.sigtyp-1.1	Task Description	3	38	2.0	1.0	The shared task consisted of two subtasks: 1) the constrained setting, for which only the provided training data may be used; 2) the unconstrained setting, for which training data may be extended with any external source of information (e.g. pre-trained embeddings, additional text, etc.)	0
2400	43176	43176	2020.sigtyp-1.1	Task Description	4	39	3.0	1.0	Data Format	0
2401	43177	43177	2020.sigtyp-1.1	Task Description	5	40	3.0	1.0	For each instance, the following information is provided: the language code, name, latitude, longitude, genus, family, country code, and features.	0
2402	43178	43178	2020.sigtyp-1.1	Task Description	6	41	4.0	1.0	At training time, both the feature names and feature values are given, while at test time, submitted systems are required to fill values for the requested features.	0
2403	43179	43179	2020.sigtyp-1.1	Task Description	7	42	4.0	1.0	An example of a test instance is given in Table 1.	0
2404	43307	43307	2020.wmt-1.3	abstract	1	2	1.0	1.0	We report the results of the first edition of the WMT shared task on Chat Translation.	0
2405	43308	43308	2020.wmt-1.3	abstract	2	3	2.0	1.0	The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer).	1
2406	43309	43309	2020.wmt-1.3	abstract	3	4	2.0	1.0	This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical.	0
2407	43310	43310	2020.wmt-1.3	abstract	4	5	3.0	1.0	Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns.	0
2408	43311	43311	2020.wmt-1.3	abstract	5	6	3.0	1.0	We received 14 submissions from 6 participating teams, all of them covering both directions, i.e. En	0
2409	43312	43312	2020.wmt-1.3	abstract	6	7	4.0	1.0	De for agent utterances and DeEn for customer messages.	0
2410	43313	43313	2020.wmt-1.3	abstract	7	8	4.0	1.0	We used automatic metrics (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments to evaluate the agent translations.	0
2411	43591	43591	2020.wnut-1.33	Summary	1	95	1.0	4.0	In this paper, we presented a shared task for consisting of two sub-tasks: named entity recognition and relation extraction from the wet lab protocols.	1
2412	43592	43592	2020.wnut-1.33	Summary	2	96	2.0	4.0	We described the task setup and datasets details, and also outlined the approach taken by the participating systems.	0
2413	43593	43593	2020.wnut-1.33	Summary	3	97	3.0	4.0	The shared task included larger and improvised dataset compared to the prior literature (Kulkarni et al., 2018).	0
2414	43594	43594	2020.wnut-1.33	Summary	4	98	4.0	4.0	This improvised dataset enables us to draw stronger conclusions about the true potential of different approaches.	0
2415	43595	43595	2020.wnut-1.33	Summary	5	99	4.0	4.0	It also facilitates us in analyzing the results of the participating systems, which aids us in suggesting potential research directions for both future shared tasks and noisy text processing in user generated lab protocols.	0
2416	43601	43601	2020.wnut-1.41	Introduction	1	6	1.0	1.0	As of late-September 2020, the COVID-19 Coronavirus pandemic has led to about 1M deaths and 33M infected patients from 213 countries and territories, creating fear and panic for people all around the world.	0
2417	43602	43602	2020.wnut-1.41	Introduction	2	7	1.0	1.0	1 Recently, much attention has been paid to building monitoring systems (e.g. The Johns Hopkins Coronavirus Dashboard) to track the development of the pandemic and to provide users the information related to the virus, 2 e.g. any new suspicious/confirmed cases near/in the users' regions.	0
2418	43603	43603	2020.wnut-1.41	Introduction	3	8	1.0	1.0	"It is worth noting that most of the ""official"" sources used in the tracking tools are not frequently kept up to date with the current pandemic situation, e.g. WHO updates the pandemic information only once a day."	0
2419	43604	43604	2020.wnut-1.41	Introduction	4	9	1.0	1.0	Those monitoring systems thus use social network data, e.g. from Twit-ter, as a real-time alternative source for updating the pandemic information, generally by crowdsourcing or searching for related information manually.	0
2420	43605	43605	2020.wnut-1.41	Introduction	5	10	2.0	1.0	However, the pandemic has been spreading rapidly; we observe a massive amount of data on social networks, e.g. about 3.5M of COVID-19 English Tweets posted daily on the Twitter platform (Lamsal, 2020) in which the majority are uninformative.	0
2421	43606	43606	2020.wnut-1.41	Introduction	6	11	2.0	1.0	Thus, it is important to be able to select the informative Tweets (e.g. COVID-19 Tweets related to new cases or suspicious cases) for downstream applications.	0
2422	43607	43607	2020.wnut-1.41	Introduction	7	12	2.0	1.0	However, manual approaches to identify the informative Tweets require significant human efforts, do not scale with rapid developments, and are costly.	0
2423	43608	43608	2020.wnut-1.41	Introduction	8	13	2.0	1.0	To help handle the problem, we propose a shared task which is to automatically identify whether a COVID-19 English Tweet is informative or not.	0
2424	43609	43609	2020.wnut-1.41	Introduction	9	14	3.0	1.0	Our task is defined as a binary classification problem: Given an English Tweet related to COVID-19, decide whether it should be classified as INFORMATIVE or UNINFORMATIVE.	1
2425	43610	43610	2020.wnut-1.41	Introduction	10	15	3.0	1.0	Here, informative Tweets provide information about suspected, confirmed, recovered and death cases as well as the location or travel history of the cases.	1
2426	43611	43611	2020.wnut-1.41	Introduction	11	16	3.0	1.0	The goals of our shared task are: (i)	0
2427	43612	43612	2020.wnut-1.41	Introduction	12	17	3.0	1.0	To develop a language processing task that potentially impacts research and downstream applications, and (ii)	0
2428	43613	43613	2020.wnut-1.41	Introduction	13	18	4.0	1.0	To provide the research community with a new dataset for identifying informative COVID-19 English Tweets.	0
2429	43614	43614	2020.wnut-1.41	Introduction	14	19	4.0	1.0	To achieve the goals, we manually construct a dataset of 10K COVID-19 English Tweets with INFORMATIVE and UNIN-FORMATIVE labels.	0
2430	43615	43615	2020.wnut-1.41	Introduction	15	20	4.0	1.0	We believe that the dataset and systems developed for our task will be beneficial for the development of COVID-19 monitoring systems.	0
2431	43616	43616	2020.wnut-1.41	Introduction	16	21	4.0	1.0	All practical information, data download links and the final evaluation results can be found at the CodaLab website of our shared task: https://competitions.codalab. org/competitions/25845.	0
2432	43617	43617	2020.wnut-1.41	Introduction	17	22	4.0	1.0	2 The WNUT-2020 Task 2 dataset	0
2433	43704	43704	W18-0507	Task Description	1	12	1.0	1.0	The goal of the CWI shared task of 2018 is to predict which words challenge non-native speakers based on the annotations collected from both native and non-native speakers.	1
2434	43705	43705	W18-0507	Task Description	2	13	1.0	1.0	To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity.	0
2435	43706	43706	W18-0507	Task Description	3	14	1.0	1.0	One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation.	0
2436	43707	43707	W18-0507	Task Description	4	15	1.0	1.0	More information about the data collection is presented in Section 3.	0
2437	43708	43708	W18-0507	Task Description	5	16	2.0	1.0	Given the multilingual dataset provided, the CWI challenge was divided into four tracks:	0
2438	43709	43709	W18-0507	Task Description	6	17	2.0	1.0	 English monolingual CWI;	0
2439	43710	43710	W18-0507	Task Description	7	18	2.0	1.0	 German monolingual CWI;	0
2440	43711	43711	W18-0507	Task Description	8	19	2.0	1.0	 Spanish monolingual CWI; and	0
2441	43712	43712	W18-0507	Task Description	9	20	2.0	1.0	 Multilingual CWI with a French test set.	0
2442	43713	43713	W18-0507	Task Description	10	21	3.0	1.0	For the first three tracks, participants were provided with training and testing data for the same language.	0
2443	43714	43714	W18-0507	Task Description	11	22	3.0	1.0	For French, participants were provided only with a French test set and no French training data.	0
2444	43715	43715	W18-0507	Task Description	12	23	3.0	1.0	In the CWI 2016, the task was cast as binary classification.	0
2445	43716	43716	W18-0507	Task Description	13	24	3.0	1.0	To be able to capture complexity as a continuum, in our CWI 2018 shared task, we additionally included a probabilistic classification task.	0
2446	43717	43717	W18-0507	Task Description	14	25	4.0	1.0	The two tasks are summarized as follows:	0
2447	43718	43718	W18-0507	Task Description	15	26	4.0	1.0		0
2448	43719	43719	W18-0507	Task Description	16	27	4.0	1.0	Binary classification task: Participants were asked to label the target words in context as complex (1) or simple (0).	0
2449	43720	43720	W18-0507	Task Description	17	28	4.0	1.0	 Probabilistic classification task: Participants were asked to assign the probability of target words in context being complex.	0
2450	43721	43721	W18-0507	Task Description	18	29	4.0	1.0	Participants were free to choose the task/track combinations they would like to participate in.	0
2451	43894	43894	W18-0604	Introduction	1	7	1.0	1.0	The ability to accurately predict current and future psychological health could be transformative in providing more personalized and efficient mental health care.	0
2452	43895	43895	W18-0604	Introduction	2	8	1.0	1.0	Currently, the mental health care industry is strained and overworked, and many conditions are on the rise among certain populations.	0
2453	43896	43896	W18-0604	Introduction	3	9	1.0	1.0	For example, suicide rates are climbing among veterans (USDVA, 2016) and youths (CDC, 2017).	0
2454	43897	43897	W18-0604	Introduction	4	10	1.0	1.0	Data-driven linguistic analysis offers a particularly attractive complement or alternative to traditional risk assessments, particularly in a clinical setting.	0
2455	43898	43898	W18-0604	Introduction	5	11	1.0	1.0	Language analysis is often relatively fast and easy to conduct at scale.	0
2456	43899	43899	W18-0604	Introduction	6	12	1.0	1.0	Further, whereas traditional risk assessments are typically limited to capturing one or a few psychological factors, language analysis has the advantage of being theoretically unlimited in what it can capture.	0
2457	43900	43900	W18-0604	Introduction	7	13	2.0	1.0	By evaluating the relationship between linguistic markers and lifetime health outcomes, such research may provide benefits for intake assessment, monitoring, and preventative care.	0
2458	43901	43901	W18-0604	Introduction	8	14	2.0	1.0	Computational linguistics has now shown strong potential for aiding in mental health assessment and treatment.	0
2459	43902	43902	W18-0604	Introduction	9	15	2.0	1.0	With few exceptions (e.g. De Choudhury et al. (2016), Sadeque et al. (2016)), work thus far from the NLP community has focused on predicting current mental health from language, and most exceptions have still only looked at the short-term future.	0
2460	43903	43903	W18-0604	Introduction	10	16	2.0	1.0	While such research is valuable, predictions about the long-term future can aid with another class of applications: the understanding of early life markers and development of preventative care.	0
2461	43904	43904	W18-0604	Introduction	11	17	2.0	1.0	Here we describe the CLPsych 2018 shared task, the purpose of which is to evaluate multiple methods for analyzing linguistic markers as a signal for current and future psychological outcomes (i.e. risk assessment).	1
2462	43905	43905	W18-0604	Introduction	12	18	2.0	1.0	We present three tasks centered around this goal:	1
2463	43906	43906	W18-0604	Introduction	13	19	2.0	1.0	Task A focuses on cross-sectional psychological health at age 11, based on essays written at childhood.	1
2464	43907	43907	W18-0604	Introduction	14	20	3.0	1.0	Task B uses these childhood essays to measure psychological distress across multiple life stages.	1
2465	43908	43908	W18-0604	Introduction	15	21	3.0	1.0	Finally, the Innovation Challenge seeks to predict language used forty years in the future.	1
2466	43909	43909	W18-0604	Introduction	16	22	3.0	1.0	The data for this work comes from the National Child Development Study (Power and Elliott, 2006), a unique British study which follows a single, nationally-representative cohort of individuals over a sixty-year period starting at birth.	0
2467	43910	43910	W18-0604	Introduction	17	23	3.0	1.0	The data available to shared task participants includes over ten thousand anonymized childhood essays, measures of psychological health taken at regular intervals, and adult writing at age 50, all collected as part of the NCDS study.	0
2468	43911	43911	W18-0604	Introduction	18	24	3.0	1.0	Related Work.	0
2469	43912	43912	W18-0604	Introduction	19	25	3.0	1.0	Relatively little work has been done on future mental health predictions.	0
2470	43913	43913	W18-0604	Introduction	20	26	4.0	1.0	De Choudhury et al. (2013) examine depression in individuals by analyzing social media signals up to a year in advance of its reported onset.	0
2471	43914	43914	W18-0604	Introduction	21	27	4.0	1.0	Similarly, De Choudhury et al. (2016) aims to identify individuals who are likely to engage in suicidal ideation in the future.	0
2472	43915	43915	W18-0604	Introduction	22	28	4.0	1.0	Sadeque et al. (2016) predict whether posters on a mental health forum will leave the forum within a particular (one, six, or twelve month) time frame.	0
2473	43916	43916	W18-0604	Introduction	23	29	4.0	1.0	In addition to these cases, some have used temporal information within cross-sectional analyses.	0
2474	43917	43917	W18-0604	Introduction	24	30	4.0	1.0	Zirikly et al. (2016), for example, use timestamp data to help classify the severity levels of posts to a mental health forum.	0
2475	43918	43918	W18-0604	Introduction	25	31	4.0	1.0	Loveys et al. (2017) explore mental health within the context of micropatterns, or sequences of posts occurring within a small time frame.	0
2476	43919	43919	W18-0604	Introduction	26	32	4.0	1.0	The goal of this shared task is to predict mental health not only at the time of writing, but years or decades into the future.	0
2477	44093	44093	W19-3003	abstract	1	2	1.0	1.0	The shared task for the 2019 Workshop on Computational Linguistics and Clinical Psychology (CLPsych'19) introduced an assessment of suicide risk based on social media postings, using data from Reddit to identify users at no, low, moderate, or severe risk.	1
2478	44094	44094	W19-3003	abstract	2	3	2.0	1.0	Two variations of the task focused on users whose posts to the r/Suicide	1
2479	44095	44095	W19-3003	abstract	3	4	3.0	1.0	Watch subreddit indicated they might be at risk; a third task looked at screening users based only on their more everyday (non-Suicide	1
2480	44096	44096	W19-3003	abstract	4	5	4.0	1.0	Watch) posts.	0
2481	44097	44097	W19-3003	abstract	5	6	4.0	1.0	We received submissions from 15 different teams, and the results provide progress and insight into the value of language signal in helping to predict risk level.	0
2482	44282	44282	2021.americasnlp-1.23	title	1	1	4.0	1.0	Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas	1
2483	44283	44283	2021.americasnlp-1.23	abstract	1	2	1.0	1.0	This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas.	1
2484	44284	44284	2021.americasnlp-1.23	abstract	2	3	2.0	1.0	The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages.	0
2485	44285	44285	2021.americasnlp-1.23	abstract	3	4	2.0	1.0	Overall, 8 teams participated with a total of 214 submissions.	0
2486	44286	44286	2021.americasnlp-1.23	abstract	4	5	3.0	1.0	We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets.	0
2487	44287	44287	2021.americasnlp-1.23	abstract	5	6	3.0	1.0	An official baseline trained on this data was also provided.	0
2488	44288	44288	2021.americasnlp-1.23	abstract	6	7	4.0	1.0	Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline.	0
2489	44289	44289	2021.americasnlp-1.23	abstract	7	8	4.0	1.0	The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.	0
2490	44555	44555	2021.bionlp-1.8	abstract	1	2	1.0	1.0	The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on summarization for medical text: (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings.	1
2491	44556	44556	2021.bionlp-1.8	abstract	2	3	2.0	1.0	Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of models developed and tested on the shared and external datasets.	0
2492	44557	44557	2021.bionlp-1.8	abstract	3	4	3.0	1.0	In this paper, we describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures.	0
2493	44558	44558	2021.bionlp-1.8	abstract	4	5	4.0	1.0	We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation.	0
2494	44748	44748	2021.cmcl-1.7	abstract	1	2	1.0	1.0	Eye-tracking data from reading represent an important resource for both linguistics and natural language processing.	0
2495	44749	44749	2021.cmcl-1.7	abstract	2	3	2.0	1.0	The ability to accurately model gaze features is crucial to advance our understanding of language processing.	0
2496	44750	44750	2021.cmcl-1.7	abstract	3	4	2.0	1.0	This paper describes the Shared Task on Eye-Tracking Data Prediction, jointly organized with the eleventh edition of the Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2021).	0
2497	44751	44751	2021.cmcl-1.7	abstract	4	5	3.0	1.0	The goal of the task is to predict 5 different token-level eyetracking metrics from the Zurich Cognitive Language Processing Corpus (ZuCo).	1
2498	44752	44752	2021.cmcl-1.7	abstract	5	6	3.0	1.0	Eyetracking data were recorded during natural reading of English sentences.	0
2499	44753	44753	2021.cmcl-1.7	abstract	6	7	4.0	1.0	In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches.	0
2500	44754	44754	2021.cmcl-1.7	abstract	7	8	4.0	1.0	The winning system used a range of linguistic and psychometric features in a gradient boosting framework.	0
2501	44879	44879	2021.smm4h-1.3	Task Description	1	27	2.0	1.0	Shared Task Goal	0
2502	44880	44880	2021.smm4h-1.3	Task Description	2	28	4.0	1.0	ProfNER focuses on the automatic recognition of professions and working status mentions on Twitter posts related to the COVID-19 pandemic in Spanish.	1
2503	45060	45060	2021.textgraphs-1.17	Task Description	1	78	2.0	3.0	Following the previous editions of the shared task, we frame explanation generation as a ranking problem.	0
2504	45061	45061	2021.textgraphs-1.17	Task Description	2	79	3.0	3.0	Specifically, for a given science question, a model is supplied both the question and correct answer text, and must then selectively rank all the atomic scientific and world knowledge facts in the knowledge base such that those that were labelled as most relevant to building an explanation by a human annotator are ranked the highest.	1
2505	45062	45062	2021.textgraphs-1.17	Task Description	3	80	4.0	3.0	Additional details on the ranking problem are described in the 2019 shared task summary paper (Jansen and Ustalov, 2019).	0
2506	45124	45124	I17-4001	abstract	1	2	1.0	1.0	This paper presents the IJCNLP 2017 shared task for Chinese grammatical error diagnosis (CGED) which seeks to identify grammatical error types and their range of occurrence within sentences written by learners of Chinese as foreign language.	1
2507	45125	45125	I17-4001	abstract	2	3	2.0	1.0	We describe the task definition, data preparation, performance metrics, and evaluation results.	0
2508	45126	45126	I17-4001	abstract	3	4	3.0	1.0	Of the 13 teams registered for this shared task, 5 teams developed the system and submitted a total of 13 runs.	0
2509	45127	45127	I17-4001	abstract	4	5	4.0	1.0	We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection.	0
2510	45128	45128	I17-4001	abstract	5	6	4.0	1.0	All data sets with gold standards and scoring scripts are made publicly available to researchers.	0
2511	45525	45525	I17-4005	Introduction	1	7	1.0	1.0	One critical but challenging problem in natural language understanding (NLU) is to develop a question answering(QA) system which could consistently understand and correctly answer general questions about the world.	0
2512	45526	45526	I17-4005	Introduction	2	8	1.0	1.0	"""Multi-choice Question Answering in Exams""(MCQA) is a typical question answering task that aims to test how accurately the participant QA systems could answer the questions in exams."	1
2513	45527	45527	I17-4005	Introduction	3	9	1.0	1.0	All questions in this competition come from real examinations.	0
2514	45528	45528	I17-4005	Introduction	4	10	1.0	1.0	We collected multiple choice questions from several curriculums, such as Biology, History, Life-Science, with a restrain that all questions are limited in the elementary and middle school level.	0
2515	45529	45529	I17-4005	Introduction	5	11	2.0	1.0	For every question, four answer candidates are provided,  where each of them may be a word, a value, a phrase or even a sentence.	0
2516	45530	45530	I17-4005	Introduction	6	12	2.0	1.0	The participant QA systems are required to select the best one from these four candidates.	0
2517	45531	45531	I17-4005	Introduction	7	13	2.0	1.0	Fig 1 is an example.	0
2518	45532	45532	I17-4005	Introduction	8	14	2.0	1.0	To answer these questions, participants could utilize any public toolkits and any resources on the Web, but manually annotation is not permitted.	0
2519	45533	45533	I17-4005	Introduction	9	15	2.0	1.0	As for the knowledge resources, we encourage participants to utilize any resource on Internet, including softwares, toolboxes, and all kinds of corpora.	0
2520	45534	45534	I17-4005	Introduction	10	16	3.0	1.0	Meanwhile, we also provide a dump of Wikipedia 2 and a collection of related Baidu Baike Corpus 3 under a specific license.	0
2521	45535	45535	I17-4005	Introduction	11	17	3.0	1.0	These corpora and released questions are all provided in the XML format, which will be explained in section 2.2.	0
2522	45536	45536	I17-4005	Introduction	12	18	3.0	1.0	Main characteristics of our task are as follow:	0
2523	45537	45537	I17-4005	Introduction	13	19	3.0	1.0		0
2524	45538	45538	I17-4005	Introduction	14	20	3.0	1.0	All the questions are from real word examinations.	0
2525	45539	45539	I17-4005	Introduction	15	21	4.0	1.0		0
2526	45540	45540	I17-4005	Introduction	16	22	4.0	1.0	Most of questions require considerable inference ability.	0
2527	45541	45541	I17-4005	Introduction	17	23	4.0	1.0		0
2528	45542	45542	I17-4005	Introduction	18	24	4.0	1.0	Some questions require a deep understanding of context.	0
2529	45543	45543	I17-4005	Introduction	19	25	4.0	1.0	 Questions from different categories have different characteristics, which makes it harder for a model to have a good performance on all kinds of questions.	0
2530	45544	45544	I17-4005	Introduction	20	26	4.0	1.0		0
2531	45545	45545	I17-4005	Introduction	21	27	2.0	1.0	It concentrates only on the textual content, as questions with figures and tables are all filtered out.	0
2532	45674	45674	S12-1035	Task description	1	33	1.0	1.0	The *SEM 2012 Shared Task 2 was dedicated to resolving the scope and focus of negation (Task 1 and 2 respectively).	1
2533	45675	45675	S12-1035	Task description	2	34	2.0	1.0	Participants were allowed to engage in any combination of tasks and submit at most two runs per task.	0
2534	45676	45676	S12-1035	Task description	3	35	2.0	1.0	A pilot task combining scope and focus detection was initially planned, but was cancelled due to lack of participation.	0
2535	45677	45677	S12-1035	Task description	4	36	3.0	1.0	We received a total of 14 runs, 12 for scope detection (7 closed, 5 open) and 2 for focus detection (0 closed, 2 open).	0
2536	45678	45678	S12-1035	Task description	5	37	3.0	1.0	Submissions fall into two tracks:	0
2537	45679	45679	S12-1035	Task description	6	38	4.0	1.0	Regardless of the track, teams were allowed to submit their final results on the test set using a system trained on both the training and development sets.	0
2538	45680	45680	S12-1035	Task description	7	39	4.0	1.0	The data format is the same as in several previous CoNLL Shared Tasks (Surdeanu et al., 2008).	0
2539	45681	45681	S12-1035	Task description	8	40	4.0	1.0	Sentences are separated by a blank line.	0
2540	45682	45682	S12-1035	Task description	9	41	1.0	1.0	Each sentence consists of a sequence of tokens, and a new line is used for each token.	0
2541	45872	45872	S13-1004	abstract	1	2	2.0	1.0	In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar.	1
2542	45873	45873	S13-1004	abstract	2	3	2.0	1.0	This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).	0
2543	45874	45874	S13-1004	abstract	3	4	3.0	1.0	CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets.	0
2544	45875	45875	S13-1004	abstract	4	5	3.0	1.0	TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description.	0
2545	45876	45876	S13-1004	abstract	5	6	4.0	1.0	Several types of similarity have been defined, including similar author, similar time period or similar location.	0
2546	45877	45877	S13-1004	abstract	6	7	4.0	1.0	The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%.	0
2547	45878	45878	S13-1004	abstract	7	8	1.0	1.0	The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.	0
2548	46217	46217	W11-1803	abstract	1	2	3.0	1.0	This paper presents the preparation, resources, results and analysis of the Epigenetics and Post-translational Modifications (EPI) task, a main task of the BioNLP Shared Task 2011.	0
2549	46218	46218	W11-1803	abstract	2	3	4.0	1.0	The task concerns the extraction of detailed representations of 14 protein and DNA modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances.	1
2550	46219	46219	W11-1803	abstract	3	4	1.0	1.0	Seven teams submitted final results to the EPI task in the shared task, with the highest-performing system achieving 53% F-score in the full task and 69% F-score in the extraction of a simplified set of core event arguments.	0
2551	46370	46370	W11-1804	Introduction	1	6	1.0	1.0	The Infectious Diseases (ID) task of the BioNLP Shared Task 2011	0
2552	46371	46371	W11-1804	Introduction	2	7	1.0	1.0	(Kim et al., 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases.	1
2553	46372	46372	W11-1804	Introduction	3	8	2.0	1.0	The primary target of the task is event extraction , broadly following the task setup of the BioNLP'09 Shared Task (BioNLP ST'09) .	0
2554	46373	46373	W11-1804	Introduction	4	9	2.0	1.0	The task concentrates on the specific domain of two-component systems (TCSs, or two-component regulatory systems), a mechanism widely used by bacteria to sense and respond to the environment (Thomason and Kay, 2000).	0
2555	46374	46374	W11-1804	Introduction	5	10	2.0	1.0	Typical TCSs consist of two proteins, a membrane-associated sensor kinase and a cytoplasmic response regulator.	0
2556	46375	46375	W11-1804	Introduction	6	11	3.0	1.0	The sensor kinase monitors changes in the environment while the response regulator mediates an adaptive response, usually through differential expression of target genes (Mascher et al., 2006).	0
2557	46376	46376	W11-1804	Introduction	7	12	3.0	1.0	TCSs have many functions, but those of particular interest for infectious disease researchers include virulence, response to antibiotics, quorum sensing, and bacterial cell attachment (Krell et al., 2010).	0
2558	46377	46377	W11-1804	Introduction	8	13	3.0	1.0	Not all TCS functions are well known: in some cases, TCSs are involved in metabolic processes that are difficult to precisely characterize (Wang et al., 2010).	0
2559	46378	46378	W11-1804	Introduction	9	14	4.0	1.0	TCSs are of interest also as drugs designed to disrupt TCSs may reduce the virulence of bacteria without killing it, thus avoiding the potential selective pressure of antibiotics lethal to some pathogenic bacteria (Gotoh et al., 2010).	0
2560	46379	46379	W11-1804	Introduction	10	15	4.0	1.0	Information extraction techniques may support better understanding of these fundamental systems by identifying and structuring the molecular processes underlying two component signaling.	0
2561	46380	46380	W11-1804	Introduction	11	16	4.0	1.0	The ID task seeks to address these opportunities by adapting the BioNLP ST'09 event extraction model to domain scientific publications.	0
2562	46381	46381	W11-1804	Introduction	12	17	4.0	1.0	This model was originally introduced to represent biomolecular events relating to transcription factors in human blood cells, and its adaptation to a domain that centrally concerns both bacteria and their hosts involves a variety of novel aspects, such as events concerning whole organisms, the chemical environment of bacteria, prokaryote-specific concepts (e.g. regulons as elements of gene expression), as well as the effects of biomolecules on larger-scale processes involving hosts such as virulence.	0
